text
"let us explain intuitively why (3) is an effective encoding scheme for a sparse vector h. first, it is commonly known that f h is non-sparse and its mass is somewhat evenly spread over all its components. the random phases of x by design are of critical importance. they \"scramble\" f h component wisely and break the delicate relationships among f h's components; as a result, in contrast to the sparse"
"as shown in fig. 6(b), the sdlc drive generates highfrequency line current compared to the conventional drive. this is due to the resonant effects of the small dc-link capacitor with the line impedance. in fact, the dc-link current and the line current are influenced by a) the rectified voltage of the diode rectifier and b) the loop impedance of the system (grid impedance and dc-link capacitor). thus, the current harmonics are generated at the resonant frequency of the system in which the loop impedance is decreased."
"as shown in fig. 6(c), the ei drives control the dc-link current to a constant value, proportional to the load power and hence the line current is almost a square-wave at different load power levels. the dc-link current (i rect ) is controlled in the continuous conduction mode (ccm), and therefore the grid current will be a rectangular waveform. in order to avoid having the dc-link current in the discontinuous conduction mode (dcm), the ei parameters (e.g., inductor size and switching frequency) should be designed in accordance to the partial load condition [cit] . notably, as the dc-link current is controlled based on the load power, the input current total harmonic distortion (thd) and the power factor (pf) have become independent of the load profile."
"cs performance is measured by the number of measurements required for stable recovery. to compare the proposed sensing schemes with the well-established gaussian random sensing, we conduct numerical simulations and show its results in figure 2 . we compare three types of cs encoding matrices: the i.i.d. gaussian random complex matrix, and the circulant random complex matrices corresponding to x of types (i) and (ii) above, respectively. in addition, 1 minimization is compared to our proposed algorithm cs-ofdm, which is detailed in the next section. the simulations results show that the random convolutions of both types perform as well as the gaussian random sensing matrix under 1 minimization, and our algorithm cs-ofdm further improves the performance by half of a magnitude."
"sdn is a relatively recent networking paradigm that facilitates new innovative and flexible networking environments. sdns provide an opportunity for network administrators and service providers to add new service-specific functionality for monitoring and management in order to improve the qos/qoe for their subscribers [cit] . currently, the most common protocol for communication between the controller and the data plane is openflow, and many network vendors already implement it in their components."
"as mentioned above, a motor drive system with a small dc-link capacitor (sdlc) has some advantages compared to a conventional motor drive system with a large dc-link capacitor and inductor [cit] . from a design point of view, the conventional drives have a large size dc or ac choke in order to reduce the line current harmonics emissions as well as a large dc capacitor to control and reduce dc-link voltage fluctuation. these dc-link components increase the cost of the conventional drives compared to the sdlc drives. however, a comprehensive analysis is required to investigate the power quality of a distribution network with different penetrations of power electronics systems like the sdlc systems."
"harmonic emissions of three-phase diode rectifiers figure 6. simulated input current (i a ) waveforms of (a) conventional drive, (b) sdlc drive, (c) ei drive systems, at different output power levels following system parameters in table 1 (with rated output power of 10 kw)."
(ii) transmission delay: the time it takes to push all the bits of the packet onto the link. (iii) propagation delay: the time it takes for the packet to reach the next hop on the path. (iv) queueing delay: the time the packet spends in routing queues. this paper defines delay to equate to the sum of these delays.
"which is convex and has polynomial time algorithms. if y has no noise, both (4) and (5) can recover h exactly given enough measurements, but (5) requires more measurements than (4)."
"it is well known that voip is extremely sensitive to network conditions [cit], and thus, is one of the most difficult traffic types (in addition to video, remote surgery, control room traffic) for which to ensure a high quality of service (qos). the quality of a voip call can degrade if there is loss, delay, or jitter on the network links between two end users. traditional methods of voip qos assessment include the mean opinion score (mos) -a metric used to grade the end-to-end quality of a call. however, simply knowing that the call quality is bad is not sufficient to enable dynamic improvement to the qos. more granular monitoring is needed to locate the root cause of the quality degradation."
"three phase power converters with low cost diode rectifiers are still widely used in motor drive systems to regulate motor speed in different residential, commercial and industrial applications [cit] . the diode rectifiers can achieve lower power losses, but may significantly increase current harmonics due to their non-linear effects. the current harmonics may result in low power quality, resonances, and finally stability issues of the distribution networks. therefore, a number of harmonic mitigation techniques have been developed for different applications such as passive filters [cit], multi-pulse transformer based rectifiers [cit] and active harmonic filtering techniques [cit] . these harmonic mitigation solutions can increase the overall system cost and volume or complicate the entire control system but are required due to international regulations."
"generally speaking, problem (4) is np-hard and is impossible to solve even for moderate n . a common alternative is its 1 relaxation model with the same constraints."
in this analysis it is assumed that only one drive is connected to an ideal voltage source -a non-distorted three-phase balanced system -as shown in fig. 3 . the total line inductance of each phase is assumed as l g and nonlinear effects of other loads connected to the same pcc are not considered.
"as a result of pulse width modulation (pwm) process, the rear-end inverter produces pulsating three-phase pole voltages v x (t) (12), where it is composed of a dc offset value, baseband harmonics and carrier group harmonic,"
"in a typical distribution network system, a number of power electronics converters including motor drive systems may be connected to a low-voltage distribution network in a commercial or an industrial segment. in order to analyze harmonics and power quality issues of the motor drives topologies, first a sdlc motor drive connected to a low-voltage distribution system is considered. it is assumed that ''n'' numbers of sdlc drives are connected in parallel at a pcc as shown in fig. 10 . interconnection impedances between the drives volume 5, 2017 are assumed to be negligible and the low-voltage distribution transformer is defined as the major inductive impedance of the system at the grid side."
"as shown in fig. 3, one of the main problems of sdlc motor drives is the resonant frequency generated by the dclink capacitor and the line inductance (l g ). the line inductance of a low-voltage distribution network is mainly defined by the size and the type of the step-down transformer and feeders. thus, the resonant frequency of the system depends on the grid parameters and its configuration -line impedance value and a number of nonlinear loads -and the drives parameters. the resonant frequency may appear at any frequency from 150 hz to 3 khz in a typical low-voltage distribution network [cit] . the main focus of this paper is to analyze two power quality issues of a low-voltage distribution network with grid connected three-phase motor drive systems based on fig. 1(b) :"
"the rest of this paper is organized as follows: section ii reviews the general ofdm system model and sets up the channel estimation formulation. section iii relates channel estimation to cs and present the proposed pilot design. in section iv, the estimator based on iterative support detection and limited-support least-squares are introduced. section v give the simulation results. finally, section vi concludes this work."
"the voip imos module sends the computed imos value to the report collection module, from where it can be requested by the sdn controller. the voip imos module can be enabled or disabled by the sdn controller. this means that, in an sdn network, imos-enabled switches can inter-operate with nonimos-enabled switches. the newly added module does not interfere with the default operation of an openflow switch."
"on the gui side, each switch displays additional information related to the voip calls passing through it. to retrieve this information from the switches, a set of new classes were added to floodlight. these classes extend the statistic class and extract the information available in the report collection module of the switches. thus, through the floodlight's gui, information about a voip call (e.g., source and destination ip, call id, rtp port, delay, jitter, loss, and imos) is displayed."
"increasing the use of distributed energy sources and modern power converter technologies such as active front end (afe) converters in distribution networks causes harmonic emissions over the frequency range of 2-9 khz, which has become an important power quality issue [cit] . thus, there is a serious action supported by international standardization organizations to consider compatibility level for this new frequency range of 2-9 khz [cit] . although, this is a very important area, there are very limited information available about the distortion and measurement techniques in this frequency range. in recent years few articles have been published, which mainly cover the distortion and emissions of different equipment in this frequency range [cit] ."
"as it can be seen from the simulation results shown in fig. 5, the load profile has less effect on the harmonics emissions of the conventional drive due to the fact that the resonant frequency is placed close to the 3rd harmonic (150 hz) with a narrow-band characteristic. thus, other current harmonics around the resonant frequency are not affected significantly. moreover, the sdlc drives may affect power quality figure 5. impedance characteristics of conventional, sdlc and ei drives at two output power levels using the parameters given in table 1 ."
"our current work is limited to estimating a signal h-vector. while our work is based on similar random convolution techniques, we have proposed to use a pair of high-speed source and low-speed receiver for the novel goal of high resolution channel estimation. furthermore, we apply a novel algorithm for the channel response recovery based on iterative support detection and limited-support least-squares, which is described in details in section iv below."
"two fields in particular are useful to our proposal: the rtp [cit] timestamp field that carries information about the current session clock of the voip call, and the ntp timestamp field that carries information about the local 'wall clock' time that corresponds to the session clock. the voip session clock is a counter increased with every sample taken by the codec. typically codecs sample the audio signal at a fixed rate (e.g. 8 khz for g711), and the timestamp included in the rtp header reflects the sampling instant of the first byte in that rtp packet."
"openflow is an open standard, that allows researchers and developers to add network functionality and run experiments, without needing to know the internal workings of hardware network devices. of switches (ofss) are composed of one or more flow tables. the of controller (ofc) imposes policies on the switch flows. flows contain a set of packet fields that are used to match flows, and a set of actions (e.g. send-outport, modify-field or drop) used to process the packet. the of protocol consists of several messages used to define the behaviour of the network. an ofs has several flow tables; when a packet is received, it enters the of processing pipeline as described in the following paragraph."
"if a packet arriving at the switch matches an entry in the first flow table, a look-up is performed for the corresponding instruction: (i) the packet can be modified with an 'applyaction' instruction. (ii) the action set (initially empty) can be updated with a 'write-action' or 'clear-action' instruction. (iii) the metadata is updated with a 'write-metadata' instruction. (iv) the packet can be sent to a following table with a 'goto' instruction. (v) if there is no 'goto' instruction, the action set is executed and the packet leaves the switch. (vi) if the packet doesn't match any entry in the table, it is sent to the controller or dropped. figure 1 illustrates a typical enterprise voip service architecture. it consists of a cloud pbx hosted by the enterprise voip service provider. small and medium enterprises (smes) subscribing to this service are connected to the pbx via business-grade broadband links to the internet. the office infrastructure can vary and it depends on the number of employees. larger offices such as the main office typically will have many ip phones connected to a data switch. traffic will flow through the switch, then the voip gateway, and finally leave the premises via the asymmetric digital subscriber line"
"vi. conclusions efficient ofdm channel estimation will drive ofdm to carry the future of wireless networking. a great opportunity for high-efficiency ofdm channel estimation is lent by the sparse nature of channel response. riding on the recent development of cs, we propose a design of probing pilots with random phases, which preserves the information of channel response during the convolution and down-sampling processes, and a sparse recovery algorithm, which returns the channel response in high snr. these benefits translate to the high resolution of channel estimation, the lower speed of the receiver adc, as well as shorter probing times. in this paper, the presentation is limited to an idealized ofdm model, intuitive explanations, and simulated experiments. in the future, we will formalize the work with rigorous theorems and fuse it into more realistic ofdm frameworks. the results presented here hint a high efficiency improvement for ofmd in practice."
"instead of using a generic algorithm for (5), we design an algorithm to exploit the ofdm system features, including the special structure of h and noisy measurements y. at the same time, we maintain its simplicity to achieve low complexity and match with easy hardware implementation."
"three-phase diode rectifiers are still commonly utilized in many applications such as adjustable speed drive systems, which may be one of the major sources of harmonic emission in the distribution networks. therefore, in this paper, the harmonic distortion generated by three different topologies utilized in the three-phase diode rectifier based systems have been analyzed and compared for 0-9 khz frequency range. this paper elaborates mathematical expressions and modelings of multi-drive systems with respect to their power quality and resonant frequency issues and the analyses have been verified by several tests. this paper is structured as follows. section ii describes the frequency ranges of harmonic emissions and their importance. comprehensive mathematical harmonic analysis of grid connected drives are presented in section iii. section iv is dedicated to experimental results in order to demonstrate the effectiveness of the developed analysis. finally, conclusions are drawn in section v."
"the current harmonics generated by these three topologies have been analyzed based on time-domain simulations. as it is shown in fig. 7, the electronic inductor is not influenced by the load profile and has lower line current harmonic emission for the frequency range of 0-2 khz. although the current harmonics (in percentage) generated by the conventional drive are higher in magnitude at 1 kw compare to 10 kw load power but the harmonic reductions have a same trend from the 5th order up to the 37th for both power levels. this is consistent with the impedance characteristic of the conventional drive as shown in fig. 5 . on the other hand, the sdlc drive has different current harmonic performances at both low and high power levels. this is due to the low damping performance of the sdlc drive at the low power operation, around its resonant frequency. volume 5, 2017 f"
"orthogonal frequency division multiplexing (ofdm) has been widely applied in wireless communication systems, because it transmits at a high rate, achieves high bandwidth efficiency, and is robust to multipath fading and delay [cit] . ofdm applications can be found in digital television and audio broadcasting, wireless networking, and broadband internet access. current ofdm based wlan standards (such as ieee802.11a/g) use variations of qam schemes for subcarrier modulations which require a coherent detection at the ofdm receiver and consequently requires an accurate (or near accurate) estimation of channel state information (csi). the structure of ofdm signal makes it difficult to balance complexity and performance in channel estimation. the design principles for channel estimators are to reduce the computational complexity and bandwidth overhead while maintaining sufficient estimation accuracy."
"so far, the above three-phase drives have been analyzed with respect to the resonant issues and the line current harmonic emissions at the grid side for the frequency range of 0-2 khz. as the dc-link sides of these drives are connected to a motor drive, hence the high-frequency harmonic emissions of the whole system need to be analyzed for the frequency range of 2 khz and above."
"a) the resonance effect due to the dc-link capacitor of a drive b) line current harmonics emissions affecting the power quality of a grid at pcc it is important to emphasize that the main goal of this research work is to highlight power quality issues of threephase diode rectifiers with different front-end topologies and configurations. it has been reported by many authors that sdlc motor drives have dc-link voltage stability issues. different active and passive damping methods including different modulation techniques have been addressed to reduce the dc-link voltage fluctuation and the resonance due to the small capacitance in the dc link [cit] . therefore, the main focus of this paper is not on the motor control side of the motor drive. on the other hand, it is assumed that the sdlc motor drive operates properly at the load side while its line current and dc-link effects will be analyzed at the grid side."
"the ei topology has a very high impedance due to the current control of the dc link. however, as it is shown in fig. 4(a), its input impedance seen from the diode rectifier side needs to be calculated. the closed-loop input impedance of the boost converter based on its small-signal model (fig. 4(b) ) is defined as given below:"
", where the guard band and pilot signal will be removed. for pilot assisted ofdm channel estimation, we shall design the pilots x (and thus x) and recover h from the measurements y (or, equivalently y)."
"power electronics system is a key technology for distribution networks, which can transfer electrical power from renewable energy sources to grids or generate regulated frequency and/or voltage for different loads such as variable speed drives and battery chargers. new demands for a) cost and size reduction, b) performance and quality improvement and c) flexibility on power management have promoted power electronics applications extensively in industrial, commercial and residential sectors such as in transportation, utility and home appliances in the recent years."
"according to the impedance models shown in fig. 5, it is expected that the sdlc drive has a better harmonic performance (up to 540 hz) at the system level compared to the conventional drive. this is due to the fact that the system impedance value is higher than the conventional drive for this frequency range. therefore, other nonlinear loads connected to the same pcc do not resonate significantly with the sdlc drives."
"in order to determine voip qos at intermediate nodes in a sdn network, each of switch must be able to compute the imos metric for any voip call. thus, the of nodes have been upgraded with adjustments to the existing of report collection module and the inclusion of our imos metric module. the architecture of our proposed openflow node is depicted in figure 3 . the flow table module identifies the voip packets, using rules set by the controller, and forwards a copy of the voip packets to the voip imos module. two different types of packets are tracked by the voip imos module: sip packets, which are used to establish a voip session, and the actual voip packets with are carried over rtp."
"taking into account the condition in (5), following equations are found from fig. 4(c) : notably the current controller gains are given in table 2 . these parameters are calculated for the hysteresis current control [cit] . the relationship between the input current and the inductor current can be obtained from fig. 4 (b) as given below:î"
"several other tests have been carried out for the conventional drives -a single and five units -at the same power levels and the results have been analyzed and compared with the sdlc drives. fig. 14(a) shows that the low order line current harmonics of the sdlc drive (the 5th and the 7th) are much lower than the conventional drive while the high order harmonics of the sdlc drive are higher in magnitude. fig. 14(b) shows the effects of the resonant frequency on the line current harmonics. in fact, when the number of drives is increased, the line current harmonics and consequently the voltage harmonics will be affected accordingly."
"of grids as other non-linear loads connected to the same pcc will be influenced by the resonant frequency. the electronic inductor is controlled to behave like an infinite inductor. in this respect, a very high damping factor can be achieved due to the high dc-link impedance."
"as the ei drive has a controlled current source at the dclink side, hence its line current is almost square wave based on the conduction angles of the three-phase diode rectifier. therefore, at the system level, when few of them are connected in parallel, the current harmonics are not changed in percentage. in fig. 15, the test results show that the line current is almost square-wave and current harmonic magnitudes and thd i values are not changed for few drives in parallel operating at the same total power. fig. 16 shows the test results of three different drives for the frequency range of 2-9 khz based on the system parameters given in table 1 . these test results show that the sdlc drive cannot suppress high-frequency current harmonics generated by the load -inverter at the dc-link side -while the conventional drive has a better performance. however, the ei drive has the best harmonic performance for this frequency range as the ei generate a high impedance at the rectifier side and can buffer the inverter noise significantly. as discussed in the previous section, the ei circuit at the dc-link side generates ripple current and high-frequency noise, depending on its system parameters such as the inductor size and type and the switching frequency and transient times. although the main focus of the paper is on the frequency range of 0-9 khz, in order to clarify this issue, additional high-frequency tests have been performed for the ei drive for the frequency range of 10-150 khz as shown in fig. 17 . it is obvious that highfrequency noise generated by a power electronics system may damage sensitive measuring devices like a harmonic receiver. therefore, in order to reduce high-frequency noise level, an electromagnetic interference (emi) filter has been placed at the input side of the converter in order to measure the high-frequency noise (above 9 khz) generated by the ei drive."
"at the system level, the resonant frequency is decreased when the number of the drives is increased. as given below, the resonant frequency (f o s ) of the system with ''n'' drives is reduced by the factor of 1/ √ n, while the damping factor (ξ s ) is increased by a factor of √ n. this may affect specific harmonic orders at a new resonant frequency -depending on the grid impedance and the number of units. this harmonic issue is not considered at the unit level."
"the e-model was initially designed to be used by endnodes, where the voip packets originate or terminate. all metrics needed by the model are easily obtained at endnodes, however this is different at intermediate nodes. more specifically, the delay is much harder to measure or to estimate at intermediate points, as the classical estimation using the rtt of the rtcp packets becomes unusable in this situation."
"at the resonant frequency (f o ), the impedance value is decreased in magnitude and the damping factor (ξ ) depends on the load power and the converter parameters."
"finally, we compared the end-to-end mos values with imos values obtained at an intermediate of switch when network conditions are dynamic. we varied the network delay, and figure 10 shows that the imos is reported to be slightly higher than the end-to-end mos. this is due to the fact that the imos captures the quality at an intermediate point and does not account for the factors degrading the quality of the call between that point and the receiver of the call. this result validates the usefulness of the proposed solution. researchers and engineers can leverage our solution to observe how voip quality degrades along a communication line, and eventually pin-point the node or link that caused the highest quality drop. figure 10 shows that between the intermediate points where the imos is collected and the receiver of the call, there has been a 0.5 mos decrease. this paper details an approach to managing enterprise voip services using software defined networking. various different technologies, used to create a cost-effective prototype for research purposes, were discussed. the paper includes a comprehensive description of advanced voip call quality monitoring in sdn, implemented using of and floodlight. to date, several different novel corrective mechanisms have been developed and validated on the testbed, which are currently the subject of further investigation and development. the future work for the proposed voip aware sdn includes the implementation of network reconfiguration such as a cac mechanism that can prevent congestion on a network node caused by too many active calls, and to investigate how our solution scales with higher volume of voip traffic, and what adjustments are needed to consider voip traffic over tcp rather than udp."
we present a solution to this critical issue that enables intermediate points to estimate the mos of voip conversations. our solution is based on the rtcp reports sent periodically by the call participants. the report contains useful information that conversation parties use to obtain an estimation of their peers' perceived quality and is also used to keep the session clock in sync.
"the delay estimation detailed in section iv is based on the assumption that the codec sampling rate is constant. however, this is not the case for variable bitrate codecs; thus, we proposed an apparent codec rate adjustment. figures 5 and 6 show the accuracy of the delay measurement when a variable bitrate voip call is transported on a communication link that delays all packets by 50 millisecond. when rate adjustment is not used, the jitter over-imposed on the delay measurements is high, as can be seen in figure 5 . the jitter diminishes ( figure 6 ) when rate adjustment is used, however some artifacts are still present, but with limited impact on the imos."
"some channel estimation schemes proposed in literature are based on pilots, which form the reference signal used by both the transmitter and the receiver. this approach has two main challenges: (i) the design of pilots; and (ii) the design of an efficient estimation algorithm (i.e., the estimator)."
this section discusses the modifications made to both of and the floodlight controller to support our voip monitoring solution: (i) new modules were added to the of switch to support the imos metric (ii) modifications were made to the floodlight controller to support voip monitoring on the switches and to display the voip imos on the gui.
"in this paper, grid connected three-phase motor drive systems with different front-end topologies have been analyzed at a unit and a system level. the analysis shows that the resonant frequencies of the single and the multi-drive systems are affected by the size of the dc-link components and the grid configuration. the resonant frequency of the sdlc drive is higher than the conventional and ei drives due to the small dc-link capacitor, which has a large impact on line current harmonics emissions and consequently on power quality of the system. one of the main factors which can control the power quality and current harmonics emissions is the damping factor of the system, which depends on the drive characteristics and operating mode. the simulations, analyses and test results show that the single and multi-sdlc drives operating at high power levels have a better harmonic performance at a broad range of frequency while at partial power their performances depend on the grid configuration, drive parameters and load profile. on the other hand, the conventional and the ei drives have consistent harmonic emissions for the frequency range of 0-2 khz. harmonic emissions of these three different drives have been analyzed and tested for the frequency range of 2-9 khz and the results show that the sdlc drives have less capability to buffer high-frequency noise generated by the inverter at the motor side. emi filter types and configurations including all capacitive couplings within a drive have a big impact on the circulating commonmode and differential-mode currents. thus to improve the overall harmonic performances of the drives for the highfrequency range of 2-9 khz and above, the whole system and layout need to be optimized with respect to all converter design factors and applications."
"in order to compare the system characteristic of the conventional drive with the sdlc drive, the same analysis has been carried out and the results are given in equations (3) and (4) based on the equivalent impedances of the system illustrated in fig. 3(b) ."
"as a sensing problem, ofdm channel estimation can benefit from the emerging technique of compressive sensing (cs), which acquires and reconstructs a signal from fewer samples than what is dictated by the nyquist-shannon sampling theorem, mainly by utilizing the signal's sparse or compressible property. the field has exploded since the pioneering work by donoho [cit] and candes, romberg and tao [cit] . the main idea is to encode a sparse signal by taking its \"incoherent\" linear projections and recover the signal through algorithms such as 1 minimization. to maximize the benefits of cs for ofdm channel estimation, one shall skillfully perform the cs encoding and decoding steps, which are precisely the two focuses of this paper: the designs of pilots and estimator, respectively."
"harmonics have short and long term effects on grids, and also on grid connected electronics and power electronics equipment such as malfunction, failure and losses. these issues reduce the reliability, lifetime and efficiency of the electricity networks. as shown in fig. 2, there are no general regulations and compatibility levels for harmonics within the frequency range of 2-150 khz to protect all electricity networks and grid connected equipment. the international electro-technical commission (iec), technical committee 77a (tc 77a) -the world leading authority to prepare technical documents for international standards -has requested international experts to define standardization for harmonics within the frequency range of 2-150 khz [cit] . the new challenging issues of future grids are related to this new frequency range and they are classified as: 1) generation of high-frequency harmonics, 2) creation of new resonance frequencies and 3) strong harmonic interactions between different types of power electronic systems. hence the utility companies, renewable energy and power electronics manufactures have been facing new challenges to solve these harmonic issues and define proper harmonics standards and regulations for grid connected electronics and power electronics equipment."
"the tests were run on a machine equipped with an intel i7-3610qm@2.30ghz processor, 6gb of ram, running ubuntu 14.04 64bits, and were repeated 5 times to ensure repeatability and stability of the observed metrics."
"in order to simplify the configuration and analyze the multi-sdlc drives at a system level, the grid voltage is assumed to be an ideal voltage source -balanced with no distortion -and only the sdlc drives at a same power level are connected to the grid. similar to the above analysis, the equivalent circuit of each drive (refer to fig. 3 ) is considered for a multi-drive analysis and the system configuration is shown in fig. 10 . as the drives are connected in parallel, the equivalent impedance model of the whole system is shown in fig. 10(c) . the load power level of each drive is modeled as a resistor (r load ) connected in parallel across the dc-link capacitor. thus, the total load power and the dc-link capacitor of the multi-drive system are modeled as r load /n and nc dc−small, respectively."
"for each call, sip information and rtp stream information is extracted. sip packets provide information about the voip sessions, such as: call-id, codec, rate, and call start time. rtp packets identify the voip streams running through the of switch. for each voip stream identified using the ssrc field in the rtp header, the packet loss, jitter, and delay are use to compute the imos."
"note that the proposed sampling ↓ ω (f −1 diag(x)f ) is very different from partial fourier sampling ↓ ω f . the latter requires a random ω to avoid the aliasing artifacts in the recovery but the former, with random-phased x, permits both random and uniform ω. below we numerically demonstrate its encoding efficiency."
"in order to study the harmonic emissions of the multi drives at the system level and compare it with the single sdlc drive, additional time-domain simulations have been carried out and the results are shown in fig. 12. it is also important to analyze and compare the damping factors of the sdlc drives at different output power levels. as shown in fig. 12(a), a single sdlc motor drive is simulated in time domain at different power levels (1, 2, 3, 10, 20, 30 and 100 kw) and the are decreased around the resonant frequency due to better damping factors. thus, the performance of the sdlc drive can be improved significantly at a full power level compared to a partial load power. fig. 12(b) shows the harmonic performances of two systems, a) a single sdlc drive at 10 kw with a resonant frequency at 1816 hz and b) 10 sdlc drives each at 1 kw with a resonant frequency around 574 hz. it is obvious that for the same total power of 10 kw, the performance of the multi drives are significantly affected at the system level due to shifting of the resonant frequency and poor damping."
"this experiment shows that negative delay values can be obtained when the codec rate is under-adjusted. figure 7 shows (1) that there are many negative samples when a delay of 10 milliseconds is affecting every voip packet. this also has a negative effect on the imos estimation. when codec rate adjustment is enabled (figure 8 ) some negative values can still appear, however their impact on the imos estimation is minimal when compared to the previous case."
"in this section, we perform numerical simulations to illustrate the performance of the proposed cs-ofdm algorithm for high resolution ofdm channel estimation. we focus on the mean square error (mse) of channel estimation as well as the multipath delay detection when channel profile and signal to noise ratio (snr) changes."
"we investigated the accuracy of our delay estimation at intermediate points against the actual delay that affects voip packets in a range from 0 to 100 milliseconds. figure 9 depicts the results; the green line represents the estimated delay. it can be seen that the difference between the actual delay and the estimated one decreases when the actual delay increases. this is a good result since the delay has significant impact on the imos when its value is high. the estimation error at small delay values can only lead to false positives, which is beneficial if the scope of using the imos is, for example, cac."
"in the following sections, three-phase diode rectifiers with a) conventional (cnv), b) sdlc and c) electronic inductor (ei) connected to motor drives are analyzed at a unit (single drive) and at a system level (multi-drives) with different configurations and load profiles. the harmonic analyses have been considered for two frequency ranges: 0-2 and 2-9 khz."
"although, in all power electronics system an emi filter is required to reduce conduced emission noise below a certain limit, hence it is not easy to compare high-frequency noise emission of all drives with one emi filter. on the other hand, the emi filter of a power converter needs to be optimized with respect to its configuration, topology, operating mode and application. in a motor drive system, common-mode and differential-mode noises are generated due to different couplings and pwm strategies. the electronic inductor generates mainly a differential mode noise however its high impedance at the dc-link side can affect the overall performance of the system including the emi filter. thus, fig. 17 shows only the noise emission of the harmonics generated by the whole system and it is compared with the conventional drive with the same emi filter."
"hosted enterprise voip (voice over ip) services are becoming an extremely popular choice for small to medium sized enterprises. in this model, the phone service is outsourced to a third party service provider, and is typically delivered over the public internet. enterprise voip services provide organisations with advanced features and functionality such as private branch exchange (pbx) capabilities, and conference calling."
"another upgrade to the floodlight controller is the ability to enable and disable the imos metric computation functionality on the switches. a new of message was defined, which is sent by the floodlight controller when the metric needs to be activated or deactivated. for the functionality mentioned above, the messages to query or send instructions to the switches were created using the of vendor message format that allows the communication of custom of messages between the controller and the switches."
"main drawbacks of power electronic systems are low (below 2 khz) and/or high (above 2 khz) frequency harmonics emissions. the utilization of power electronics systems has been increased significantly in many applications such as motor drive systems, roof-top solar inverters and compact fluorescent lamps, which can inject high-frequency and high-energy harmonics within the frequency range of 2-150 khz in the distribution networks."
"the remainder of the paper is organised as follows: section ii discusses a selection of the related work in the literature. section iii presents an overview of the pertinent technical details of the technologies employed in this work. section iv details the imos implementation in of to enable advanced voip monitoring. section v describes the modifications made to both the of switches and the floodlight controller during the construction of the testbed. section vi presents a series of experiments and results used to validate the new functionality implemented in of. finally, section vii concludes this paper."
"as mentioned in section iii, the of controller is the sdn component that imposes different forwarding policies on the of switches. in our test-bed, the floodlight controller is used as it provides an intuitive web gui for visualising the network."
"the main focus of this section is to analyze the resonant effect of the dc-link capacitor with the line impedance and consequently the line current harmonic emissions generated by the drive. fig. 3 (a) shows the equivalent impedances of the systems with the conventional drive during each conduction period of the three-phase diode rectifier. at any instant of time, two input voltage sources are connected to the dc-link of the drive through two diodes. thus the whole system can be modeled as an rlc circuit where 2l g is the grid inductance during the conduction period of the diode rectifier, c dc−cnv is the dc-link capacitor, l dc−cnv is the inductor of each positive and negative dc-link branch and r load is an equivalent resistor to model the load power. therefore, the impedance of the system seen from the grid side is calculated for the conventional drive and the result is given in equation (1):"
"besides the continuous efforts toward employing and utilizing the industrial motor drives based on the line commutated front-end diode rectifiers with ac and/or dc passive chokes (in conventional and sdlc drives), substantial interests are also devoted to suitable cost-effective active methods which can be combined with the existing motor drives. meanwhile, employing an electronic inductor (ei) placed at the dc-link stage can emulate an active front-end along with the diode rectifier, as it is shown in fig. 1(a) . the main idea behind using ei is to replace the bulky dc-link inductor with a relatively small inductor incorporated with a dc-dc converter to behave like an ideal infinite inductor, which can significantly improve the drives input current quality. an acceptable compromise between the dc-link inductor size and the switching frequency of the converter will determine the operating mode of the ei [cit] . moreover, applying a multi-pulse pattern modulation technique on the dc-dc converter results in a selective harmonic mitigation of the drive's input current [cit] . this issue will be more valuable at multi-drive applications, where implementing a suitable modulation scheme with respect to the number of drives will enhance the grid power quality at the pcc."
"in order to analyze the resonant effects in details, the three different topologies have been simulated based on a 10 kw drive and the results are shown in fig. 6 and fig. 7 . when the sdlc drive operates at a high power, the impedance characteristic of the drive is improved at the resonant frequency due to a better damping factor. this means that the sdlc drive operating at high power may not inject significant current harmonics at its resonant frequency. as shown in fig. 5, at 1 kw power level, the loop impedance of the sdlc drive is decreased significantly at the resonant frequency due to poor damping factor. therefore, high current harmonic emission is expected for the drive at this power level."
"where r 0 is the signal to noise ratio; i s is the impairment that occurs more or less simultaneously with the voice signal (e.g., speech volume too loud); i d is the impairment due to a delay or echo effect; i e ef f is the impairment due to low bitrate codecs and packet-losses; and a is the advantage factor that allows factoring in the user's wiliness to accept lower call quality for ease of service access (e.g. calls from remote locations). in a study done after the proposal of the e-model, clark [cit] has shown that, for ip-based calls, most of the telephone line specific terms can be reduced to their default values, thus simplifying the e-model to encompass only ip network specific parameters such as packet delay, loss, and jitter. the resulting equation is:"
"the fundamental principles, which led to the harmonic standards can be noted as: a) avoid system and load damage as well as disruption due to high harmonic levels b) reduced losses to an acceptable level c) a proper and an economical mitigation solution for both manufacturers and utility companies according to the existing iec regulations, there are two main frequency ranges for which equipment require to comply with harmonic emission limits: 0-2 khz and above 150 khz. currently, there is not any general regulation to cover all products within the frequency range of 2-150 khz. due to increasing a number of grid connected high-frequency power electronics converters such as motor drive systems, solar inverters and single-phase converters with pfc systems, different disturbances have been reported in distribution networks [cit] . thus, the following frequency ranges cover almost all frequency ranges: noitemsep"
"as is widely perceived, cs favors fully random matrices, which admit stable recovery from fewest measurements (in terms of order of magnitude), but c is structured and thus much less \"random\". this factor seemingly suggests that c would be not favored by cs. nevertheless, carefully designed circulant matrices can deliver the same optimal cs performance."
"intercepting at least one rtcp [cit] packet, will enable an intermediate node to estimate the delay that affected any rtp packet from its generation until it was intercepted in the intermediate node. this process is depicted in figure 2, and the delay of any rtp voip packet can be obtained using (1). rt cp nt p is the wall clock time when the rtcp packet originated; rt p t s is the session clock timestamp found in rtp packets following the rtcp packet; arrival nt p is the wall clock time when the rtp packet was intercepted; and finally, codec rate is the voip codec sampling rate."
conventional power electronics products have affected the quality of power networks due to significant injection of low order harmonics (mainly below 2 khz). power factor correction (pfc) circuits have been proposed for single-phase power electronic products in order to improve line current quality and power factor. the line current of a single-phase converter with a pfc system has very low current distortion below 2 khz but can inject harmonics at high frequencies. this technology cannot be used in a three-phase power electronics systems to completely eliminate the most important low order harmonics -the 5th and the 7th orders -but it can mitigate the harmonics below a certain level defined by standards such as iec61000-3-2 or iec61000-3-12 [cit] .
"actions such as extracting large buffers of data from frequently used system calls or other functions require considerable overhead to process, and thus must be handled in the most efficient manner. when a guest vm-exits to the hypervisor, it is in a suspended state on the cpu core which has exited. when the hypervisor takes too long to do its processing during the exit, a noticeable lag can be seen by users in the guest vm. since kvmi hooks kvm to gain execution in vmx root mode, it is also running in this window during the vmexit. kvmi has two interfaces for users to interact with, a set of character devices, and a sysfs tree. in both cases, the data given to the user is stored inside a kernel buffer. when the data is captured by kvmi, it performs a copy from the vm's buffer into the kernel buffer and immediately lets the guest resume. no other communication takes place during the exit. with both character device and sysfs interfaces, the user requests the data at his or her leisure, and it is copied to them outside of vmx root mode. no ring switching or vm transitions occur other than the one required vm-exit and vm-enter (which would have happened even without the hooking). with this method, kvmi is able to move data from the vm to the host with minimal overhead."
"the accuracy of our proposed drr algorithm was determined by comparing our algorithm's estimated coordinates with actual gps coordinates. table 1 shows that the maximum test error of the six test cases is 28.02m and the minimum test error is 14.20m. the average positioning error is 19.86m. it is evident from table 1 that for case 1 and case 2, the radar scanning range does have some impact on the positioning accuracy to a certain extent and gives an accuracy difference of 5.64m between the case 1 and the case 2. case 3 has better positioning accuracy than case 4. the reason is that for case 4 the radar works in pulse expansion mode, which provides the ability to increase the duration of the transmit pulse and helps maximize the energy on targets. it means that this mode often provides larger target sizes on the chart plotter but also reduces the edge accuracy of the coastline and some reference buildings. in addition, this mode also introduces some interferences. some small moving objects, such as birds and ships, are overlaid with each other to form a pattern similar to the coastline. these two factors reduce the localization accuracy of the algorithm on case 4. in case 5, there are large number of boats around the radar. however, these objects do not have large impact on the positioning accuracy because the boats are scattered all around the radar and some of the boats complement each other during the minimal distance search process. this indicates that the algorithm proposed in this paper has some anti-interference abilities against the obstacles around the radar. unlike in other cases, the images in case 6 is derived from a camera shot rather than a screen shot. the fisheye effect of the lens and the tilt of the shooting angle can cause the camera photo to be deformed, resulting in a reduction in positioning accuracy. so, the image is subjected to fisheye correction and keystone correction before applying the positioning algorithm. the position error after correction is reduced from the original 32.06m to 21.65m. it should be noted that case 4 and case 6 were selected to evaluate the performance of the algorithm in a variety of environments. in the actual positioning scenario, the radar should not be set to a special operating mode or a camera would not be selected as an input to the algorithm."
"notice that the circuitry for tx and rx is assumed to be identical for all nodes. in what follows, we neglect the steady energy consumption (e elec ) and consider only dynamic energy consumption caused by amplifiers in the tx circuitry averaged over some time period. we however point out that the results of this paper can be easily extended to capture the steady energy consumption, which may strongly influence the optimization results if the sensor nodes can be switched off after computing a function value. in such cases, it may be beneficial to prefer larger clusters when the steady energy consumption increases."
"the national institute for standards and technology (nist) defines cloud computing as \"a model for enabling ubiquitous, convenient, on demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction\" [cit] . a few businesses have emerged as leaders as cloud computing has become increasingly mature and available. amazon, google and microsoft have demonstrated support through the promotion, encouragement, adoption, and leadership of cloud computing, building a foundation for recent paradigm shifts. the paradigm will continue to evolve as the cloud becomes more pervasive."
"there have been many inquiries into the ability of forensic practitioners to conduct the science of digital forensics in cloud-based infrastructure and the ability for the current tools and techniques of digital forensics to operate in the cloud. the cloud infrastructure -with its distributed processing, storage, and resources -can be extremely complex because storage capacities can grow geometrically. before understanding the applicability of current digital forensics practices to the cloud, we must construct a common understanding of digital forensics."
"output from classification and metadata extraction are output to agnostically-formatted log files that may then be ingested by security information and event management (siem) systems and/or logging/analysis engines. these data are correlated against log data output from the kvmi character devices may be ingested by an analytic engine to provide a rich set of analytics. when coupled with network-based data introspection, a full view of historical and current state of vms and their interactions with other entities (either in the cloud ecosystem or external) may be captured and inspected. a high level diagram depicting a notional infrastructure for kvmi and network forensic tools in a cloud and analytic environment is shown in fig. 1 . the architecture is fluid and agnostic to be supported on one or many compute host servers of varying architectures."
"since the uav industry has a relatively long history and its development is fast, many studies have been conducted on uav navigation under gps denial environment. main methods include inertial guidance based on inertial measurement unit, positioning based on aviation image registration, and lidarbased positioning. the inertial guidance is generally used together with image registration due to reduced cumulative errors [cit], and image registration requires some reference elements to implement, such as artificial markers [cit], intersection [cit] or some known location markers [cit] . some studies have also used elevation together with 2d-image as inputs for 3d-based registration [cit] . in addition to image registration, ins accuracy can also be improved by fusing the velocity and altitude information provided by the dynamic image [cit] . inertial guidance can also be used in conjunction with gps to improve the gps accuracy [cit] but can also deal with short-time gps failures [cit] . experiments show that the error of vehicle positioning using ins is about 3.2±1.1% of the distance traveled [cit], which also shows that independent ins can only be used for short-range positioning. for a uav, positioning methods based on image registration have significant advantages because of the following reasons: 1) top view: images obtained by the uav have a consistent top view perspective with the satellite. therefore, it is easy to register the uav images with satellite images such as google earth images; 2) wider range: wide range of ground images can be obtained from a top view angle, meaning that the extraction of registration features is much easier. for example, lakes, coastline, and road intersections are good registration feature points. optical flow sensors instead of cameras are used in some studies to assist the process of uav positioning. this solution has a lower cost. lidar-based methods provide high accuracy and better environmental adaptability. in addition to accuracy, in the dusk environment, lidar's physical recognition capability is much higher than the camera [cit] . therefore, they are widely used in indoor uav navigation, pedestrian [cit], grass, road, vehicles and other objects detection [cit] . in addition, together with the simultaneous localization and this work is licensed under a creative commons attribution 3.0 license. for more information, see http://creativecommons.org/licenses/by/3.0/ mapping (slam) technology, adaptive uav navigation can be achieved [cit] . because of these advantages, current ugvs mainly use this kind of lidar-based methods to achieve high-accuracy environmental data, thus achieving autopilot. of course, in order to obtain both color and depth information, lidar and camera are usually used in combination [cit] . the disadvantage of this lidar-based methods is that the sensor used in this method is very expensive. furthermore, its measuring range is generally small, which is not suitable for the detection of long-distance marine landmarks [cit] ."
"in this paper, we presented a novel algorithm for the joint optimization of convergence and energy consumption for consensus algorithms in wireless sensor networks. the proposed algorithm takes into account distance dependent transmit energies and clusters the network nodes according to user-defined cluster sizes that may depend on application and site specific characteristics. by incorporating a regularization term, we investigated the trade-off between convergence speed and energy consumption. this is achieved by a series of numerical simulations of a temperature monitoring application under the assumption of noiseless communication links. the examples show that the naive choice of a single cluster containing all nodes can be outperformed in terms of network energy consumption if a certain excess error can be tolerated."
"u nmanned systems such as unmanned ground vehicles (ugvs), unmanned aerial vehicles (uavs) and usvs rely on information from gps and this information is an important reference for the automated operations of these unmanned systems. any disturbance or denial of gps signals can seriously affect the operations of these unmanned systems. the automatic identification system, electronic chart display, navigation system and information system of these unmanned vehicles are all dependent on healthy operation of the gps. gps denial can not only lead to malfunction in the operation of these unmanned vehicles, but also threatens the safety of their surroundings. recent advancements in sophisticated hacking techniques and the availability of highly efficient and computationally powerful chipsets also pose a threat to safe operation of these unmanned systems. some experiments have shown that a 1.58w gps jamming device can shield all the gps signals on a wide range of sea area and this jamming technology has been used by military [cit], a us uav was captured by iran using gps spoofing [cit] . north korean gps jamming, directed into south korea, most likely had precipitated a sequence of events that led to some mistakes made by the south korean drone operators and eventually led to a crash [cit] ."
"iv. conclusion and future work drr algorithm, which can obtain the real-time position of a usv through image registration between a radar image and a satellite image, is proposed in this paper to solve the positioning problem of a usv under gps denial environment. features of radar images with a horizontal viewing angle and satellite image with a vertical viewing angle are analyzed in this paper. the coastline is adopted as the image registration feature for the two kinds of images. a coastline feature extraction method based on edge gray features, which is suitable for both radar and satellite images, is also given. the method can eliminate the interference caused by boats around the radar to some extent, achieving more accurate feature extraction. in addition, we have proved that the dr distance can be used as an indicator of the measure of an image registration method. compared with the common hausdorff distance, with the dr distance as an indicator of the image registration, the registration speed can be significantly improved, especially in cases with relatively large point set size. we have also evaluated the positioning accuracy of the proposed algorithm using six typical application scenarios. the test results show that, in the six test cases, the maximum positioning error is 28.02m, and the average positioning accuracy is 19.86m. we have also implemented a 1200s-continuous test mission out of the portsmouth harbor, the test results show that the maximum positioning error is 20.81m for the entire trip."
"kvmi is implemented as a single loadable kernel module for linux. it begins by locating the kvm and kvm intel kernel modules in kernel-space memory. upon finding them, it hooks code in kvm's exit handler, redirecting execution into its own exit handler. when kvmi encounters a new vm, it determines its operating system and adds the system of interest to a set for further introspection. vms then run until an operation within the guest causes them to vm-exit, which then passes control to kvmi's vm-exit handler routine, providing the foundation to understand the dynamic behavior of actors within the virtual machine, introspect without introducing artifacts into the running system, and allow full control over guest system."
"2) results: as mentioned in the description of kvmi, the kvmi kernel module is attached to the kvm hypervisor; hence, its existence is not visible from inside the guest (unlike agent-based solutions). thus, the only indicator of visibility from inside the guest might be through timing analysis. for the experiment of concurrently downloading a pdf file with 1, 10 and 25 vms on a host, the time in millions of cpu ticks for the download are show in the box plots below. each download was run 30 times on each vm instance."
"to address these constraints, a vmi tool was envisioned to provide the cloud forensic capabilities while having as few of these limitations as possible. the kernel-based virtual machine introspection (kvmi) tool was developed for the kernel-based virtual machine (kvm) hypervisor on intel's x86 architecture [cit] . to meet performance, scoping and usecase demands, the follow tenets were applied to kvmi:"
"as can be seen in the plots in fig. 2, there is negligible difference in the time ticks between the downloads (growing more consistent with the greater number of samples). also, it should be noted that the sysfs process was also able to extract the file before it reaches the disk encryption process for ntfs; md5sums were also taken to show the pdf extracted was the same downloaded."
"the authors of [14, anon] describe a method for conducting networking monitoring at the application layer, to provide greater visibility and correlation of network traffic to vmi. their solution is implemented in c-code for the open-source virtual switch, open vswitch. traffic classification leverages several libraries, provided through a continuously updated catalog of thousands (˜2500) of plugins and signatures covering human-initiated and iot protocols/applications. metadata is extracted for http (request, servers, uris, mime types), dns (hosts, queries, servers), smtp (mailfrom, header), kerberos (login, server), ldap (hostname), with goals to include ospf and bgp data."
this work was supported by the german research foundation (dfg) under grants sta 864/3-2 and by the german ministry for education and research (bmbf) under grant 01bu1224.
"the cloud has been leveraged for many applications by many different industries. despite its popularity, cloud technologies are still not well understood and are open for research and development [cit] . the security implications of cloud computing is a critical topic requiring additional research. from a forensic perspective, numerous questions arise on how to analyze the cloud using traditional digital forensics techniques [cit] . for instance, during a traditional digital forensic examination, all files on the storage media are examined along with the entire file system structure. however, this is not a practical model for cloud infrastructure, as the elasticity and ephemerality of pooled storage make pinpointing data blocks cumbersome. this difficulty is exacerbated in networked systems by the scale with which computing resources are spread over diverse administrative and geopolitical domains. cloud is able to combine numerous heterogeneous resources (hardware platforms, storage back ends, file systems) that may be geographically distributed. the idiosyncrasies in cloud have caused a paradigm shift in digital forensics; however, tools and techniques still do not exist to help forensic practitioners cope with these issues. and while many research areas enumerate these challenges, open literature has not made significant headway to address the issues them or provide solutions."
"step 2: broadcast result. fig. 1 . two-step approach to average consensus by choosing different weights for a regularization term that takes into account the energy costs. the paper is organized as follows: we first introduce the system model and the cost model. this is followed by the problem statement. in section iv, we reformulate the original problem of maximizing the convergence rate to take into account the energy consumption for transmission. this problem is solved using some standard optimization tools. section vi presents some simulations, while the paper is completed by some conclusions and open problems."
"2) inappropriate radar scanning range: different radar scanning ranges may result in different coastline feature points obtained from radar images. a large radar scanning range can cover greater coastline area and feature points, but will introduce excessive interference, such as boats, navigation marks along with lose greater coastline details. a small radar scanning range can get more coastline details, but the obtained coastline range is limited."
"the notion of the virtual machine sitting on top of a lightweight hypervisor is a relatively new paradigm for forensic practitioners. however, it is a near-ubiquitous certainty for most iaas infrastructures. traditional forensic techniques, based on assumptions that the file-system was directly interacting with the hardware through an abstraction, afforded the forensic practitioner the assumption that there was nothing controlling the application below the file-system. this is not the case when using virtualized technologies. hypervisors have the ability to transparently monitor, introspect and interact with the guest in a non-intrusive fashion. there are four key areas that monitoring and collecting data from the hypervisor would assist in alleviating:"
"we also extract the coastline which is located in the range of activity of the usv from satellite images. for this we initially set up several virtual radar positions (vrp) in satellite image and then extract the coastline shape from different angles using these vrps. finally, we combine all the coastline points extracted by the vrps to obtain the entire coastline feature set. the vrps are selected ensuring that the shape of each part of the coastline is covered. the coastline feature, extracted using satellite images for portsmouth in united kingdom is shown in fig. 4(b) . to cover the entire range of usv's mobile activity, we used multiple vrps to extract the coastline of the region."
"the coastline feature, extracted from radar and satellite images, is used as the registration feature. this coastline registration feature should satisfy equation (1), where (x, y) is a coastline point from the radar image, while (x g, y g ) is a point corresponding to the coastline from the satellite image. s x and s y are scale transformation factors each in both directions. θ is the rotational transformation factor between the two images. b x and b y are translation transformation factors each in different coordinate directions. an objective of image registration process is to determine the optimal translation factors, rotational factor and scale factors with minimal complexity."
"as shown in fig. 9, this paper compares the trajectory of dvl-based dead reckoning algorithm and radar based drr algorithm. it can be seen from the figure that the positioning error of the dead reckoning algorithm increases with the distance traveled, but the results of the drr algorithm are positioning accuracy comparison between dvl based dead reckoning algorithm and radar based drr algorithm. stable in the whole process. it can be seen from fig. 10, after the 1200s-sailing task, the error of adopting dead reckoning algorithm is 243.57m. in contrast, the radar-based drr algorithm has a maximum error of 20.81 m and an average error of 9.77 m over the course of travel. the comparison shows that the drr algorithm has the advantage over the dead reckoning algorithm in long-distance ship positioning. there are two reasons for the better accuracy: firstly, the radar setting in this test takes into account both the scanning range and the measurement accuracy, which are the two factors influencing the positioning accuracy. secondly, the images used for positioning are derived directly from the radar system, which is high resolution and has no deformation."
"virtual machine introspection (vmi) is a technique used to monitor the runtime state of a system-level virtual machine. the runtime state can be may include processor registers, memory, disk, network, and any other hardware-level events [cit] . a review of research literature and current vmi technologies exposed a number of limitations and trade-offs in vmi approaches [cit], including: the use of in-guest agents; kernel to user space transitions (dramatically slowing down processing); vmi tool pre-configuration requirements; hypervisor version lock-in or source code patching; reliance on operating system (os) symbols; limited processor features due to hypervisor (even if the hardware could do more)."
"however, incorporating cloud infrastructure into a company's network may alter its threat surface and appear contrary to security and privacy controls implemented for boundary protection. cloud computing presents the risk of shared computing resources among multiple tenants on the same physical hardware; there is a need to have strict software isolation in order to prevent one tenant's software from compromising another tenant. for iaas, a lack of proper virtual machine (vm) separation severely elevates this risk. for data protection, providers and administrators must ensure only authorized users have access to their data, and that their data is protected at rest, then sufficiently isolated and permanently erased during data sanitization. when security incidents occur in violation of risk-reduction controls, the challenges involved in the cloud incident response and forensics begin to manifest [cit] ."
"the current challenge is most hypervisors do not expose a useful application programming interface (api) at a sufficient level to do transparent, fine-grained and customizable introspection. scalable vm instrumentation and introspection at an in-depth level requires fast handling of events, as well as direct access to vm state. furthermore, deep introspection benefits greatly from the ability to gather data from the hardware during the vm's exit to the hypervisor. all of this requires identical access to the system as the hypervisor itself; improper use of this ability could easily cause system instability. it is for this reason we believe that the hypervisor developers have been hesitant to grant this much control through their apis."
"in our method, we convert a typical radar image into a grayscale image as shown in fig. 2 . the concentric circles in the figure are uniformly-spaced, which are centered at the coordinates of radar as p 1 (x r, y r ). gray blocks around the radar represent different obstacles, such as boats, buildings or trees along the coastline or on land, etc. in this figure, we take the radar signal feature which has an angle α with the direction of north as an example. the yellow dotted line in the fig. 2 is an auxiliary line in that direction. it can be seen that the auxiliary line has 9 points from p 1 to p 9, where p 1 is the radar and p 2, p 3 are boat projections in the horizontal direction, whereas p 4, p 5, p 7, p 9 are intersection points of the auxiliary line and uniformly-spaced lines. p 6, p 8 are the projections of onshore obstacles in the horizontal direction, and p 6 (x c, y c ) can be considered as a coastline point in the direction α. in other words, we can extract the coastline points in this direction according to the gray scale values of pixels of the auxiliary line."
"of particular interest for our subsequent work will be scenarios involving noisy communications links and time-varying measurement objectives. this can be achieved by using adaptive subgradient methods that harness interference for computation [cit] . also, by using more knowledge about the target application and leveraging ideas from compressed sensing, the number of required measurements in the network can be reduced. in turn, this will result in additional savings in energy consumption and introduce new degrees of freedom to the tradeoff between convergence speed and estimation accuracy on one hand, and energy consumption and network robustness on the other hand."
"in many wireless (sensor) applications, nodes cooperate for some common goal. one example is the localization of an acoustic source using a number of geographically distributed microphones that are equipped with wireless communication capabilities. in fire alarm networks, for instance, a number of wireless sensor nodes may be used to monitor maximum and average temperature values. in such applications, the goal is therefore not to share local measurements among network nodes but rather to compute one or multiple functions of these measurements (e.g., the maximum function or a weighted sum). the functions to be computed depend on the targeted application."
"our proposed drr algorithm uses the coastline as the registration feature and inaccuracies in obtaining the will result is poor accuracy in determining the position. hence, we evaluate the factors that can lead to inaccuracies in coastline extraction from radar and satellite images."
"to alleviate some of the issues of media, disk, data temporality, location and ownership in the cloud infrastructure, providers leverage logging to detail the events that occur in their domains. these logs are normally comprised of: (1) audit logs that may correlate services to operating systems, (2) security logs that may attempt to connect users to broad actions, and (3) application logs that highlight cloud application activity. however, these logs often suffer from the semanticgap problem due to the lack visibility into the vm, where the events take place. the common denominator in any iaas-based environment is the hypervisor. is the our belief that many of the deficiencies with cloud forensics may be addressed by tapping the hypervisor for vm introspection (vmi); that is, uncovering forensic artifacts at the virtual machine manager (vmm) layer. while the \"use of logs in hypervisors is not well understood and presents a significant challenge to cloud forensics\" [cit], it is our conjecture that such logging may be the successful path forward. this is the core of our research, and is outlined in the following sections."
"as shown in fig. 4(a), the shape of the coastline (highlighted in yellow) around the usv can be obtained using the above method. the figure reveals that most of the yellow coastline points are a close match. note that radar cannot identify all parts of the coastline in range as it is only lineof-sight and cannot look around corners. errors are possible when the coastline is not correctly detected by the yellow lines. in fig. 4 (a), marked by red ellipses, distant coastlines are detected along with other boats. these error points will affect the image registration accuracy, so the image registration algorithm should be robust against outliers, e.g. it should be capable of removing lone yellow dots."
"as systems and devices become virtualized and deployed in the cloud, the hypervisor becomes an increasingly appropriate place to collect performance data, system state, system landscape, function calls, transaction traces, and other characteristics. we propose a method by which an introspection application may be coupled with a hypervisor, in order to \"reach into\" the vm with minimal intrusiveness to collect data critical to the reconstruction of events, files, and operations. such a capability is required to take advantage of the hypervisor as an instrumentation platform and to integrate that data with more traditional collection mechanisms."
"since there are two sets of images, one from the radar and the other from satellite, with different viewing angles, the image registration methods cannot be applied as used in uav. fig.1(a) is a satellite image and provides the top-view. all the objects on the sea or on land within the range of this satellite image range are shown in fig. 1(a) . fig. 1(b) is a radar image, in which the usv is at the center. the distances between the usv and surrounding obstacles are also provided in the figure. also that in fig. 1(b), width of the red shaded area in a certain direction reflects the horizontal projection of an obstacle in that direction. this information cannot be inferred from satellite image. however, the radar image cannot provide any information about objects behind the obstacles. hence, in order to determine the usv's position based on radar and satellite images, we propose a fast image registration method between radar image with a horizontal viewing angle and satellite image with a vertical viewing angle. the method we proposed is called dimensionality reduction registration (drr) and this method can be used for real-time applications. each pixel on a satellite image map has its own known coordinates. therefore, the coordinates of the radar can be calculated by establishing a registration between a satellite image and a radar image. in order to carry out image registration between the two kinds of images, we extract their common features. as shown in fig. 1 (c), the coastline is the intersection line of the satellite image plane and the radar image plane. therefore, it can be used as an image registration feature."
"with respect to both computing speed and positioning accuracy, the drr algorithm proposed in this paper can meet the real-time positioning requirements of an operational usv. the performance of our proposed algorithm may further be enhanced by: a) carrying out tests in more application scenarios, such as fast-moving boats, to improve the universality of the algorithm, b) integrating with methods which can quickly determine approximate locations, such as dead reckoning, to narrow satellite image area to be registered to improve the efficiency of image registration, c) evaluating the positioning performance of the algorithm under different coastline scenarios, d) combining with other sensors such as lidar or camera to improve the in harbor positioning accuracy."
"there are many parallels between the steps in incident handling and the forensics phases of the incident response life cycle. with the focus on creating forensic artifacts that are actionable, there are limited tools, methods and approaches that enable the collection and preservation of forensic evidence in the cloud. both approaches and disciplines face similar challenges in the need to interact with the system in a transparent, non-intrusive fashion. in order to contain, collect and analyze evidence, both areas are demanding that advancements and tools be written to aid in their approaches."
"the distance 'd' between the feature points of the radar image and the satellite image is shown in fig. 5 . the distance varies significantly with varying position of the center of the radar image. the search process looks for the best possible minimum value for d and as the search range is gradually approaching the best match point, the distance value gradually reduces and eventually reaches its minimum value at the best match point. we take this best match point as the pixel position of the radar. since each pixel of the satellite image has its corresponding gps coordinates, after obtaining the best match point, we can calculate the coordinates of the radar, and ultimately achieve the goal of determining the position of usv based on radar signals."
"in the domain of computer networking, virtual switching was devised a means to support networking of virtual machines on a single compute node, or host. a virtual switch is essentially a kernel process executed on the host, often in collusion with a hypervisor, to provide virtual interfaces (ethernet segments) to virtual machines and switching/forwarding logic between interfaces. the applications derived from virtual switching can be as simple as bridges, performing layer-2 forwarding operations, or as complex as multilayer forwarding and routing functions and protocols, as well as supporting newer approaches to networking, such as software-defined networking and network function virtualization."
"the concept of a vm serviced by a lightweight hypervisor is a relatively new paradigm for forensic practitioners. traditional forensic techniques, based on assumptions that the filesystem was directly interacting with the hardware through an abstraction, afforded the forensic practitioner the assumption that there was nothing controlling the application below the file-system. this is not the case when using virtualized technologies. hypervisors may have the ability to covertly monitor, introspect and interact with the guest in a transparent fashion. as mentioned in previous sections, the problems of storage and collection of actionable data are exhausting."
in order to improve the positioning accuracy of the algorithm the following aspects need to be considered: 1. pulse width and other parameters of radar should be set so that the contours of the coastlines are clear; 2. the scanning range and display range of the radar should be set according to the ship's distance from the coastline; 3. the radar image used for positioning should from screen-shot rather than camera-shot.
"with the advent of cloud computing and virtualization, special care needs to be taken by the data center, cloud service provider, and the cloud architect to ensure the tenant's (intellectual) property is secure. cloud computing changes the relationship between the computer hardware and the operating system that manages and controls it. focusing on the added virtual layer is not enough. with the kvmi we can look at the hypervisor and ways to more tightly secure it."
"2) results: this particular experiment makes use of kvmi to introspect on guest vms, and network forensic tools (as described above) to correlate guest data to network data. the results of the experiment largely focus on log data to navigate the attack in realtime and identify the actions done on the target vm. by logging the cloud compute host, virtual machine name/id, and ip addresses, the vm in multitenancy can be quickly identified. kvmi includes data pulled form windows apis, with parameters. the collection of guess and network data address the semantic-gap problem of pulling context from the guest to the host."
"to start the experiment, an administrator logs into a vm (.82) and adds a share using domain admin credentials. he then visits a phishing website hosted on the attacker's machine (\"attack.com\", .2). this is shown in a network forensic log of fig. 3 as a dns request to the dns/domain controller server at .66. the attacker compromises the vm (.82) using a silverlight exploit through a xap file and runs a bind meterpreter on port 2222 (fig. 4) . the attacker then starts a new process, notepad.exe, and migrates to the process so if the user closes iexplore.exe it won't close the meterpreter session (fig. 5) . the attacker then uploads a binary and executes it. the binary is seen in fig. 6, and also from the guest to the host for further inspection. the attacker then exfils a file from the compromised vm's desktop to the attacker machine, we see this process started by a walk of the directory tree in fig. 7 . the attacker then collects the local sam hashes on the machine and passwords located in memory (kerberos, msv, and ssp passwords). as this information is transferred back to the attacker machine, high entropy uris are seen in the dpi log ( fig. 8) over a meterpreter bound port 2222. using the new found credentials, the attacker logs into the domain controller (dc) (.66). on the dc the attacker again collects passwords and domain password hashes. hashed uris are shown traversing port 2222 from the ad to the attacker server, as well as exfil communication from the ad to the attacker over port 3333 (fig. 9) ."
"the purpose of this experiment is to help identify anomalous guest processes or to identify \"stealthy\" malware (process hiding techniques). these are both techniques that could be used by a malicious user's attempt to hide their actions through covert means. the process information would then be correlated to network traffic supporting c2-like operations. the underlying concept here represents a typical drive-bydownload attack against a vm to include exploitation and pivoting."
"the final use-case examines the situation wherein an internet-connected node might be used a listening-post or a botnet drone waiting c2 commands. the underlying concept involves a targeted vm that is conscripted, running both legitimate and non-legitimate traffic/services."
"2) results: using the kvmi sockets monitoring feature, the vm making connections and the endpoints (ips) to which connections are made can be identified. what's novel is the binding of the network connection to the requesting application. as can be seen in fig. 10, the vm (host process id 0xc27) can be seen making connections to ip .33 over port 80, with the process iexplorer.exe (internet explorer)."
"digital forensics, as a scientific discipline, is concerned with the collection, analysis and interpretation of digital data connected to a computer security incident, as well crimes that involves a digital device that may store electronic information. practitioners have attempted to provide some formalization to the field by defining a five-phase process: 1) identification of an incident from its source(s) and determine its type. 2) acquisition of evidence from various sources. 3) preservation of the state of evidential data. 4) analysis of evidential data, reconstructing fragments and drawing conclusions. 5) reporting of results and conclusions about the evidence. this standard unifies many of the previous forensic protocols and provides an abstraction to the process that is not focused on a particular tool or technology, nor is bound to a specific class of cyber-crimes. during a forensic examination, all files (e.g., storage, log files), memory, and external media are examined along with the entire file system structure to locate forensic artifacts. however, each of the phases provides unique challenges and opportunities for investigators as cases and artifacts are situated in the cloud. numerous papers have discussed the challenges to current hard-disk-based forensics approaches [cit] . to date, the predominantly focus has been on challenges and solutions in in the network forensics subdiscipline (which focuses on forensics of network traffic, rather than hard disk forensics). there is a gap in the ability to conduct the preservation and analysis of hard-disk forensics in the cloud. efforts have been made [cit], but do not address the area and do not provide a significant improvement or methodology in this area."
"the registration process of feature points for the radar image and for satellite image is a search process for the center of the radar image. in this search process, we first find an indicator to measure the effect of the registration and based on this measure we determine the optimal registration solution. hausdorff distance is usually taken as the indicator of the image registration [cit] and is represented using equation (2) the distances of all the pair of points between set a and set b needs to be calculated. therefore, this method is unsuitable for scenarios which have high real-time requirements, or image registration with high resolutions. for the application in this paper, all the possible scenarios and areas that a usv may cover, needs the hausdorff distance to be calculated. since the area covered by a satellite image to be registered will be very large, it will be difficult to meet the real-time operational requirements of a usv to determine the position under gps denial if this indicator is used as the registration indicator for radar and satellite images."
"in order to increase the processing speed, the radar is set to north up mode. in this way, we can obtain a radar image whose orientation is consistent with that of the satellite's image. before image registration, the distance resolution of the radar image and the satellite's image is adjusted to the same value."
"our approach proposes a new view to the issue through the use of virtual machine introspection. section ii provides a foundational background on cloud computing, digital forensics, and incident response. section iii covers the challenges that arise for forensics for cloud environments. section iv describes our methodology and approach to addressing those challenges. section v describes our hypervisor-based introspection tool for cloud forensics. our experimental analysis for our introspection tool is highlighted in section vi, while section vii concludes this paper and suggests future work."
"where h ik (t), n i (t) and s k (·) denote the corresponding channel coefficient from node k to cluster head, receiver-side noise and transmit signal of node k, respectively. this setting will be referred to as noisy mac. assume, nodes can estimate the channel to the cluster head by some wake up pilot symbols. then, to compute the average within cluster c i, every node needs to invert it's own channel, which yields transmit signals of the form"
"in order to further verify the performance of drr, this paper implemented an experiment in which the ship navigated out of the portsmouth harbor. the ship's speed was set at 5 knots, the state of the ocean was choppy, and the mission lasted 1,200 seconds. the ship was equipped with radar, doppler velocity log (dvl), gps and radar was used to scan the coastline. dvl was used to measure the speed over the ground and gps was used to provide reference. the advantage of dvl over ins is that the ins can only measure the motion of the ship relative to the water flow, while the dvl can measure the motion of the ship relative to the seafloor. radar operates in normal mode with a scanning radius of 3 nm. the output frequency of the radar image is 15s and the dvl and gps are 1s."
"a user on a vm would visit a \"malicious\" website, that would then exploit a browser vulnerability, providing the attacker privileged control of the virtual machine. at this time, the attack would then pivot to other machines in the network, using metasploit to gain passwords."
"the notion and risk of acquisition changes within virtualized environments. without physical queues and devices, it may no longer be feasible to physically protect against contamination of the machine through a \"write-blocker.\" in traditional networks, the analyst physically removes the drive to create a bit-to-bit image of the device. in the cloud, analysts may be bound to the network that the vm is on. as a consequence, investigations are more dependent on the surrounding infrastructure than physical machines."
"first is the need to process entire large storage pools used by the iaas infrastructure. storage solutions in the cloud are varied. to support a variety of formats, e.g., fiber chanel, ethernet iscsi, and a variety of file system types, raw data can be petabytes in size. the vms often exist in some \"sharded\" (striped) fashion on the file system. current forensic tools are unable to collect data from a large data volume in a timely fashion, nor can the companies who host these services afford to take the storage system offline in order to gather forensically sound evidence from the underlying file systems. if one were to move processing to the hypervisor to gather all the file system artifacts -then all the io is decoded, saved and archived before being written to the distributed file system. second, the ephemerality of the guests and of cloud computing is a challenging problem, raising many issues regarding the lifetime of a particular device. the lifetime is no longer years or months, but rather it is weeks at best. storage issues are one of the greatest challenges of cloud computing; as demand for resources increases, the cloud provider's ability to store all of a particular user's information for weeks, or even months, becomes economically unfeasible. as space is reclaimed, forensic evidence is lost. as an alternative, an emerging enterprise trend is to have users use transient clients and to store user profiles at a separate location. but, as virtual machines are cleaned and reimaged, all potential for evidence residing on the original virtual machine is lost. some iaas cloud platforms such as openstack and amazon ec2 have mapping knowledge of where guests are deployed. given that information, it is possible to do targeted collection by the hypervisor, making it possible to collect artifacts from a guest while it is still running. it is possible to get information regarding file i/o, memory, processes, network connections as well as traceability of the actions on the system. the third area is the elasticity of the collection methods and processing of the data. by collecting data from individual hosts, the approach scales with the cloud. no longer does forensic analysis or artifact collection focus on a single host; every host that may have information can assist in the collection and processing of the forensic artifacts."
"3) too many interference factors around the radar: if there are many boats surrounding the radar, these boats will block the measurement of coastline. this may result in errors in coastline feature points obtained from the radar image."
"a light weight agent was created that could download files to the guest through a web interface, and then saved them to disk. this would mimic a variety of content being shared (such as child pornographic images, sensitive proprietary information, etc.); typical of what would be transferred and accessed through the cloud. to conduct the experiment, the same file was used for download in experiments consisting of 1, 10 and 25 virtual machines on a single host. time ticks were counted during each of the downloads to identify time differences between baseline (that is, without kvmi extracting the file) and with the kvmi sysfs functionality enabled."
"the gray values of each pixel of the auxiliary line are shown in fig. 3 . with the help of the gray values, we can obtain the size of obstacles from this figure and we can further identify different objects. since the distance between p 2 and p 3 is small, it indicates that the obstacle is a small object, such as boats or navigation buoy. since the distance between p 6 and p 8 is large, it indicates that the size of the object is big. such big obstacles have high probability of being land. it can be seen from the fig. 3 that different intersection points have different gray values. the gray values of the pixel points representing uniformly-spaced lines are low and the gray values of the points representing large land obstacles are high, while the gray values of the points representing small obstacles such as boats fall in between. based on this information, we can obtain the coastline position p 6 (x c, y c ) for the angle α."
now we are in a position to extend the optimization problem in (11) to incorporate the additional energy cost. the problem of interest -called the joint convergence/expected lifetime optimization problem -takes the form
"to evaluate the efficacy and applicability of the kvmi tool for cloud forensics, we identified three areas or usecases for experimentation and analysis. the first involves the reconstruction of files placed or executed on targeted machines, to be used as forensic evidence. the second involves monitoring and gathering intelligence for attacks in progress, to include network traffic. the final experiment involves the ability to collect general vmi and network data for historical purposes, in a multitenant environment. the results of the usecase testing elucidate the strengths and weaknesses in each situations, and possible means for improvement. the testing environment was comprised of the following hardware and software elements:"
"and the consumed energy for a certain realization of the sequence of activated clusters. the results are averaged over 10 3 runs. for the simulations, we also assume that the nodes have expert knowledge of the current estimation error to terminate communication, and therefore also dynamic energy consumption, once the error falls below a threshold of 10 −1 ."
"another avenue we're pursing is extending kvmi for general cloud security requirements. since the hypervisor is the means through which cloud is managed, it security professionals are concerned it may be leveraged as a vector to present attacks or unauthorized access to the virtual systems. since kvmi is decoupled from the hypervisor, we are interested in using kvmi to detect, stifle or block attacks."
"fourth is forensic collection and time correlation of the guest artifacts make it hard to prove the provenance of the artifacts. by collecting the artifacts from the hypervisor, it is possible to independently verify all the logs, access and interactions from the guest and create a forensic timeline of the event that is grounded with a trusted time source."
"coastline extraction errors, whether they come from radar images or satellite images, will eventually lead to decreased positioning accuracy. however, these two situations could be treated differently. in order to provide real-time positioning information, a usv needs to process radar images in real time. however, the coastline feature points from satellite images do not need real-time processing. satellite images are pre-processed off-line and can be stored in the system. therefore, human intervention can be introduced into the off-line processing of the coastline feature from satellite images, such as removing obstacles and adjusting the position of the actual coastline, to obtain higher accuracy. feature extraction of radar images is completely dependent on the algorithm itself. and for this we evaluate the factors affecting the feature extraction of radar images. fig. 8 shows six different test cases used for evaluation of position determination using feature extraction from radar images. these six cases are from four different locations and related dataset is collected from open-sources [cit] ."
"each vm of interest is dynamically analyzed to determine offsets of key structures in memory. this is done in multiple ways, including walking exports of portable executable (pe) files, disassembling code, and simple recognition of data in relation to other objects. it also utilizes vm exits for things like control-register access, model specific register (msr) access, cpuid, and timer related exits. kvmi also keeps track of each virtual cpu separately, and links them to their respective vm. for breakpoints, it uses permissions in intel's extendedpage-tables (ept) to trap on read, write, or execution on arbitrary sized chunks of memory. it leverages the monitortrap-flag (mtf) bit for single stepping. kvmi produces logbased output through ring buffered character devices on the host's device file system -and may receive select input through this method as well."
"in case 1 and case 2, the boat is located at the same place in yaquina bay. the difference between the two cases is that both cases use different radar scan ranges. in case3 and case4, the boat is located in hillsdale lake with the boat at different positions and radar mode is also different for both cases. in case 5, the boat is located in sawyer cove, and there are high number of boats around the radar onboard the boat. in case 6, the boat is located in galveston channel. in this case the radar image does not come directly from the radar system but comes from a camera shooting the radar screen, which is different from previous cases. since the radar images come from different devices with different settings, we process the images before image registration. in this processing, we only leaving the obstacle information scanned by the radar, as shown in fig. 4 . the orientation and plotting scale of the images are also labeled based on the information provided by the images. in practical applications, the algorithm calibration need only to be performed only once assuming the radar settings are not changed."
"some of the most attractive benefits for cloud computing involve a subscriber's ability to receive services from a broker or provider, and expand their requirements at scale; the burden of scaling is placed on the broker or provider and becomes transparent to the user. coupled with this is the economic perk that subscribers need only pay for what they use (i.e., a payas-you-go), forgoing the operations and maintenance costs that would normally accompany an on-premises data center."
"2) system security: the cloud service providers should also perform suspicious activities monitoring to eliminate unauthorized or nefarious access to the virtual systems. although, this type of monitoring is an important security feature of kvmi, it can also be a means of full access to the virtual systems of the cloud. another key to mitigating mishandling of the kvmi or the cloud is to insure the logical cloud stores are segregated and the data is isolated thoroughly."
"4) non-real time update: satellite images are not updated in real time; in fact, these images may be updated once every few months or even years. therefore, if there are any moving objects such as boats, etc. in the satellite images, some errors may be introduced as the positions of these objects may have changed over time 5) varying tidal level: the coastline features of a satellite image may be different from the coastline feature of a radar image under varying conditions of tide. this is because some coastline features may disappear under a high tide such as on a low-slope beaches."
"the purpose of this experiment is to verify the extraction of various forensic artifacts from the system without adversely affecting the guest and without guest detection of the introspection. the underlying concept addresses intellectual property theft, child pornography, etc."
"informally, digital forensics is defined as \"the collection of techniques and tools used to find evidence in a computer.\" it is often considered a science due to its systematic, technological approach toward inspecting a computer system and its contents. its aim is to locate and preserve electronic evidence for use in criminal investigations. digital forensic investigations require a level of expertise and rigorous methodology that exceed standard data collection and preservation routinely performed by system administration personnel."
"there are several challenges that arise when conducting digital forensics and incident response in the cloud. in our paper, we have discussed the challenges, current shortcomings, and proposed a unique approach and tools to meet those challenges. while we have made headway in developing our methodology and technology, there are still difficult problems and areas for improvement we're pursuing, such as: (1) extending kvmi for other platforms and various operating systems; (2) furthering kvmi's capability to make on-the-fly modifications to guest execution, such targeted encryption key extraction, or making certain suspicious actions trigger enhanced introspection; (3) further decouple kvmi from kvm, in both its memory accessing ability, and general execution. we are also in discussion with commercial hypervisor companies to extend the kvmi capability to their hypervisors."
"therefore, images obtained by cameras are mainly used for obstacle detection at a close range [cit] and for usv control in narrow inland waterways [cit] by detecting the coastline [cit] . so this method also has the potential to replace sonar to achieve low-cost obstacle marking [cit] radar is a good supplementary positioning means for manned boats with higher measuring accuracy and its signals are less subjected to variable weather conditions. it has been studied as a tool for ugv positioning many years ago. the method is that a millimeter-wave radar was used to measure the position of a fixed radar beacon and to estimate the position of the vehicle [cit] . since the cost of the radar-beacon positioning method is relatively high for automotive vehicles and difficult to implement, it is not widely used [cit] . at present, millimeter-wave radar is mainly used to detect obstacles within 400m for ugv. radar is very common in marine application, and even now can be found on many small yachts. the reason is that it is a useful electronic aid, and has position-fixing and collision-avoidance capabilities [cit] . in addition, s or x wave radars for the marine application can provide a measurement distance of more than 150 km, which can meet the positioning requirements of offshore usvs, compared to lidar, millimeterwave radar and cameras, which provide measurement distances of less than 400m. therefore, mariners can determine the position of a boat roughly by combining radar images with sea charts. in this paper, we propose the use of radar and satellite images to determine the feature space from the two sets of images. the radar images provide a horizontal view while the satellite images provide vertical view as shown in fig. 1, we use the conjunction of the two views, horizontal and vertical, to determine the usv's position under gps denial environment. section ii provides detailed methodology employed in the paper. results and discussion of results are provided in section iii followed by conclusion in section iv."
"1) personnel security: today with companies, governments and organizations choose to host services and store information on the cloud (both public and private), the physical access to their digital property will be inevitably lost. because of this risk, the possibility of data being exposed to attack is higher. the biggest threat to sensitive data will possibly come from individuals or groups inside the data center. therefore it should be put on the cloud services provider to secure the system, software and data through background checks of data center personnel. access to the kvmi application should also be restricted and controlled based on detailed roles of the individual."
"the notion of forensically sound images is also a challenge, particularly source images. will the service provider have to store the image/backup on their system until the case is resolved to ensure image integrity/attribution? or, will a hash of the vm compared to the other vm be enough to satisfy the requirement of producing the source evidence in the court? such circumstances will leave the analyst dependent on the backup strategies of the service provider, which may vary from cloud to cloud, and may not always be viable."
"the performance of drr algorithm is evaluated using two metrics: a) computational complexity and b) positioning accuracy. since the speed of image registration is mainly determined by the efficiency of the distance calculation, therefore we compare the computing performance of the dr distance algorithm and hausdorff distance algorithm using different number of feature points to evaluate the computational complexity of the proposed algorithm. we evaluate the positioning accuracy of our proposed algorithm by using six typical scenarios as test cases to evaluate and analyze the potential affecting factors."
"most hypervisor platforms allow some interaction through an api. they can range from simple things like querying the power status of vms, to more complex things like viewing or modifying register state inside a guest. in all cases, a considerable amount of overhead is incurred. the apis for hypervisors like xen or vmware require ring switches and transfers between the hypervisor and a special vm (dom0 or secure virtual machine (svm) respectively). this effectively separates the actual hypervisor kernel from bugs in the vmi code; however, it also causes any introspection data to travel far from the hypervisor kernel before it reaches the vmi code. additionally, it enables hypervisor authors to decide what data is relevant to the vmi code. this separation is advantageous for the hypervisor, but at best neutral for the vmi code, and only if it is able to get all the data that it needs. the vmi code can request additional information, but this requires even more context switching, and is still limited to what the hypervisor will allow it to request. the vmi code will not be running in vmx root mode and thus does not have the ability to use virtual machine extensions directly. in some cases, apis to do certain tasks don't exist, and thus the only way to do some types of introspection is to patch or hook the kvm kernel code and obtain vmx root privileges. for example, it is not possible to ask kvm to enable the mtf (monitor trap flag) functionality of intel hardware virtualization to single-step a guest."
"the reasoning flow for the query answer is illustrated in table 6 . from this case, it can be seen that two-step reasoning provides different results than the simultaneous reasoning of bynowlife: two-step reasoning returns telkomsel while simultaneous reasoning by bynowlife returns indocement. this is because through two-step reasoning, the bn system cannot calculate probability values for the indocement node because it does not exist in the bn document."
"ontology enrichment based on probability values from bns, as shown in case #1: the investment problem and case #2: social crm in higher education; 2."
"in the next experiment, the qam modulator is used to compare the pts and slm papr reduction technique. similar result is seen as the oqpsk modulator shown. the pts is performing superior result over the slm and the original ofdm signal. the main difference is observed that the qam reduces the better papr value as compared to the oqpsk value in the pts scheme."
create a data property named hasprobvalueof for the individual and set its data property value equal to the node's probability value. for each parent of the individual:
"we have implemented the bynowlife framework into an application software prototype. the application was developed using c#. it uses owlapi for .net libraries as an ontology reader and writer, pellet as a logical reasoner, and structural modeling, inference, and learning engine (smile) as its probabilistic reasoner. we built a bynowlife library package that contains three main modules: (1) a module to handle queries and their execution results, (2) a module that controls and coordinates logical and probabilistic reasoning and (3) a morpher/transformer module that translates owl/rdf into xdsl (and vice versa). examples of cases solved using this technique are given below."
despite these advances there is still an important hurdle to be taken which concerns the development of scalable purification methods of these lv vector pseudotypes before they can be routinely used in preclinical and clinical research. first achievements
ensure that the ontology contains an object property with the name objpropinfluences and that the individual ind has the object property in relation to individual nod.
"thus this review, based on publically available sources, presents the actual state of the art of production means for lv vectors, providing information on advantages and short comings of actual protocols (or methods) and devices as well as on maximal manufacturing levels achievable (titer, total vector quantity) and finishing with a perspective of what should come next."
"foreign subnode: an individual in an ontology that has a parent in both the ontology and a bn, but does not itself exist in the bn."
"investment problem case -scenario 2. in case #1 concerning an investment problem (see section 4.1), there was another individual in the ontology named \"indocement\" (as depicted in figure 11 ), but in the probabilistic knowledge base, no indocement node was found (see figure 4) . the resulting reasoning flow from the query in section 4.1 is shown in table 5 . using the bynowlife framework, the system will create the indocement node in a probabilistic knowledge base and then calculate its probability value using a parameter learning algorithm as follows ( figure 12 ). using the bynowlife framework, the system will create the indocement node in a probabilistic knowledge base and then calculate its probability value using a parameter learning algorithm as follows. the reasoning flow for the query answer is illustrated in table 6 . investment #2-the calculation of probability values using the bynowlife reasoning technique."
"of the features of previous frameworks, there are three aspects that have never before been successfully implemented in past research: ontology enrichment based on bn knowledge bases, the adjustment of bn structures through structural and parameter learning based on ontology knowledge bases and support for an integrated probabilistic clause in sparql query format. this study accomplished these three goals in the form of an enhanced framework, as described in the following sections."
"a downstream recovery process should provide a product with the desired concentration, purity, and the other quality attributes at minimal costs. 98 this statement is true for all large-scale manufacturing processes, but often not for small-scale purification protocols destined for research purposes."
"below is the algorithm's process based on the steps described above (algorithm 2). description of the functions used in the mergeonto algorithm, where m is the number of input nodes in the ontology document and n is the number of input nodes in the bn document, is given in table 3 ."
"this framework consists of three main parts: the application, the reasoner and the knowledge base. the application can either be independent software or a software agent written in a common programming language, such as c/c++, java, php, or python. the reasoner involves two components: (1) the logical reasoner, assigned to perform logical reasoning based on sroiq specifications in owl 2 dl and (2) the probabilistic reasoner, assigned to perform probabilistic reasoning for the knowledge base in the form of a bn, oobn, or dynamic bn (dbn). the knowledge base contains knowledge of two forms: ontology knowledge in owl/rdf format and bn knowledge in xml of decision systems laboratory (xdsl) format [cit] . in the knowledge base, there is a component named the morpher that conducts data transformation from standardised ontologies (owl/rdf) to xdsl (and vice versa). the morpher also serves to enrich the ontology with the probability values of the nodes and links in the bn. the probability values of the nodes in the bn are translated into axioms in the ontology and links are translated into relations. this framework allows the application to query the knowledge base in sparql format for logical reasoning and the hasprobvalueof special property for probabilistic reasoning. bynowlife was developed not only as a theoretical foundation in combining logical with probabilistic knowledge bases but also a technical implementation that can be tested with various test cases. online technical implementation of this framework can be found on the bynowlife website [cit] . the system requires input in the form of logical knowledge-based files (.owl) and probabilistic knowledge-based files (.xdsl). after both files are uploaded and the knowledge bases have been integrated, the user can query to communicate with the system. users can also download the resulting .owl and .xdsl files from the system if needed. figure 2 shows a screenshot of the website's content."
"social crm is crm combined with the power of social media to generate new approaches in maintaining and enhancing relationships with customers. a social crm model contains semantic relationships between stakeholders and factors that must be understood thoroughly to identify the roles of each stakeholder and factor in the business process. the stakeholders are the university (including the units inside), prospective students, current students and alumni. their semantic relationships have consequences for logical reasoning."
"three-plasmid systems a rev-independent vector technology was only successfully implemented for the eiav system in which only three plasmids (two helper plasmids coding for the gag-pol and the env functions and the tv plasmid) are required 37 (table 1 ). in the past, virxsys has developed a two-plasmid system for the treatment of hiv infection using a conditionally replicating lv vector. 38 this system is characterized by the assembly of all helper functions (gag-pol, rev, tat, and vsv-g) on one plasmid. 39 due to the use of full ltrs, this system is tat-dependent and therefore by definition a second-generation lv vector system. the maintenance of full ltrs is retained to allow the transcription of the antisense against the hiv envelope gene 39 only in the hiv-infected target cells that express tat. though such a production system is easier to produce and less expensive in its application and leads to higher vector titer than the three or four plasmid systems, 39 the presence of all helper genes located on one plasmid might be a concern with respect to the formation of rcls. however, none of the vector lots or of the transduced cell products were shown to contain rcls. 40"
"it can be proven that probabilistic reasoning in bn followed by logical reasoning in o is a subset of probabilistic and logical reasoning in o', which is a subset of logical reasoning in o'. because probabilistic reasoning in bn followed by logical reasoning in o is a subset of probabilistic and logical reasoning in o', there are cases where queries cannot be answered except by combining these knowledge bases and querying them. this occurs, for example, when there is an individual that does not exist in the probabilistic knowledge base but exists in the logical knowledge base while the individual is an entity that is taken into account during probabilistic reasoning."
"the conventional reasoning query process is done in two steps. the first step is to perform probabilistic reasoning within the bn knowledge base. the second step, based on the reasoning in the first step, is reasoning within the ontology knowledge base; this returns the final result of the reasoning. the reasoning query process on ontology-based systems that utilize bynowlife is completed in just one step by sending logical and probabilistic reasoning queries in sparql format. this is because bynowlife transforms bns into ontologies (through the merging process) automatically before reasoning is performed on the ontology knowledge base. it can be proven that the conventional two-step process of reasoning is a subset of bynowlife's reasoning process; that is, the reasoning done by bynowlife encompasses conventional reasoning."
"in the signal scrambling techniques, the input message blocks are encoded by multiply the phase sequence vector to reduce the papr efficiently without introducing the distortion. the encoded message can easily be decoded by the help of same phase vector sequence, although the signal become complex but it maintains the bit error rate. there are two signal scrambling techniques are dissected in this paper namely partial transmit sequence and selected mapping [cit] ."
"harvesting is performed via one simple medium exchange and in some cases the number of harvests has been increased to augment the final vector quantity. however, in the case of preclinical and clinical large-scale productions frequent harvest cycles are not practical, wherefore in most of the cases one to at most three harvests are performed. 42, 43, [cit] based on the number of 10-stack culture devices and the number of harvests, traditionally, the harvest volumes range between 20 and 52 l 42,52,54 using 12-24 cf-10 devices for a single campaign production schedule."
below is the intended algorithm based on the steps described above (algorithm 1). a description of the functions used in mergebn algorithm above is given in table 1 . table 1 . description of the functions used in the mergebn algorithm.
"investment problem case-scenario 2. in case #1 concerning an investment problem (see section 4.1), there was another individual in the ontology named \"indocement\" (as depicted in figure 11 ), but in the probabilistic knowledge base, no indocement node was found (see figure 4) . the resulting reasoning flow from the query in section 4.1 is shown in table 5 ."
"the morpher will create individuals that did not previously exist in the owl document; in this case, income is created as a generic type. the generic type is a subclass of thing. the morpher will then create a special property called hasprobvalue for every individual in the system. the reasoner will set probabilistic values in the xdsl document based on the data in the owl document, perform probabilistic reasoning and then return the result to the owl document by updating its respective property values. the result of running the program is shown in figure 5 and figure 6 . in the xdsl document, there are rules that the fiscal_policy influences stock prices and that the fluctuation of stock prices will affect investors' incomes, as shown in figure 4 . this structure was visualized using genie modeler [cit] software."
"ecdysone-inducible system. the ecdysone-inducible system, based on the insect hormone ecdysone analog ponasterone a, was employed in alternative to the tetracycline-regulated technology. 83 a ponasterone a-responsive 293t-based cell line was generated in which the expression of gag, pol, rev and vsv-g genes was placed under control of an inducible ecdysone promoter. this cell line consistently produced second-generation pseudotyped lentiviral vector stocks that after concentration gave titers up to 10 8 tu/ml. 83 similarly, the 293-rev/gag/pol i cell line was obtained by introducing hiv rev and gag/pol genes, each under the control of separate ecdysone inducible promoters, into 293 cells in the continuous presence, during selection, of the specific hiv protease inhibitor, saquinavir, to further control the cytotoxic effect of hiv protease. the 293-rev/gag/pol i cells released within 48 hours post-induction, high amounts of hiv gag/pol particles (about 10 µg p24/ml) that could package and transduce third-generation hiv vectors to high titers. 84 constitutive packaging cells constitutive (or continuous, as also referred) packaging cells with high productivity rate are no doubts more difficult than inducible cells to be obtained. in fact, the toxicity of the vector genes renders unfeasible either the use of the pantropic vsv-g envelope or the selection of p24gag highly expressing cells. therefore, this category includes packaging cells carrying only envelopes different from vsv-g and usually producing lower amount of p24gag as compared with inducible cells."
"in conclusion, large-scale production of lv using transient transfection of suspension cells is feasible and shows promising productivity. however, further developments are required before transferring this technology to industrial manufacturing. the major bottleneck in urge of optimization is the transfection procedure itself that requires massive amounts of plasmid dna and consequently makes the production process extremely costly. an industrial-friendly transfection technology consuming less dna and increasing the percentage of producing cells will be the key element to render such a process profitable for industries in the future."
"bayesowl [cit] was the first framework to attempt to integrate ontology with bn. it was created with the primary goal of representing the bn in resource description framework (rdf)/owl notation in a simple structure to be used as the ontology knowledge base. alongside general nodes, bayesowl generates bridge and intersection nodes to facilitate the modeling of relations between classes. reasoning is done using the decomposed iterative proportional fitting procedure (d-ipfp) algorithm [cit] to calculate the probability values for each node in the system. in addition to including a graphical user interface (gui) for direct use, bayesowl was also designed for software developers by providing application programming interface (api) in the form of java packages. this is notable because it provided an interface for software developers to create their own ontology-based applications capable of representing probability values in its owl knowledge base. however, bayesowl does not have query support in sparql format as a query standard in its knowledge base. in addition, bayesowl does not feature a blending process or \"symbiotic mutualism\" between ontologies and bns, so it does not offer ontology enrichment or bn network structural adjustment through structural and parameter learning."
"bynowlife was developed as a framework using different concepts or paradigms from existing frameworks that integrate bns with ontologies. existing integration frameworks only focus on how to represent probabilistic information by using ontology notation and perform reasoning with it. bynowlife was created not only to represent probabilistic information in ontology notation and perform reasoning with it but also to enhance integration through ontology enrichment and structural and parameter learning mechanisms. with bynowlife, ontology is enriched with information from probabilistic structures and values in the bn and, conversely, the bn structure is augmented with information contained in the ontology through structural learning. the determination of the probability values of new nodes added to the system and those related to them is accomplished through a parameter learning mechanism."
generate files named bndatafile containing sample data with the suspect node node for each node in the bn that is not included in fixed nodes. perform parameter learning based on the generated sample data inside bndatafile for each node in the bn that is not included in fixednodes using em algorithm. the result is stored in bnresultfile.
"lack of this property may disgrace the communication. the ofdm modulation technique is used in 802.11g which is known as wireless (wlan). one of the ieee standards, 802.11 is a wireless networking transmission methods that is commonly used today in 802.11a, 802.11b, 802.11g and 802.11 versions to ease wireless connectivity with throughput to up to 54 mbit/s using the same 2.4 ghz band in various places or organization such as abode, office and commercial purpose [cit] . with some of the vantage with ofdm system (which has many narrow band signals in the time domain) has one major issue that is its large peak-toaverage power ratio which leads to cause inter-modulation distortion and out-of-band radiation due to nonlinear function of power amplifier. this amplifier must be work in its linear region to combat distortion, out-of-band noise and bit error rate degradation. non-linearity leads to low spectral efficiency and high amount of dissipation of power which is limited for use in many wireless applications. this is the reason why papr must be reduced for application used in ofdm system. therefore, in order to find a way so as to abase high papr effectively, it is prominent to reduce the papr of an ofdm signal. with previous years researchers has been working on various schemes giving an attempt to reduce the papr in orthogonal frequency division multiplexing (ofdm) system. these schemes can be further classified into two prominent types which are signal distortion and signal scrambling techniques. one of the common schemes include in signal scrambling techniques are selected mapping (slm) scheme\" [cit] \" and partial transmit sequence (pts) schemes\" [cit], [10, [cit] \". among all these schemes, the slm and pts schemes have been considered the most efficiently schemes due to its high papr reduction performance without incurring additional signal distortion. the rest of this paper is organized as follows. section ii, manifest ofdm signal and papr formulation. section iii gives summary description of papr reduction techniques. section vi shows simulated experimental results. further, section v comes with the conclusion."
"another issue with pei is the absence of analytical method to detect and quantify this molecule in process or in the purified vector preparation. therefore, it is unclear whether pei remains copurified with the vector, to which extent and if this can be detrimental or not for the vector infectivity and stability."
"in fig.3 the comparison of pts, slm and the original ofdm signal by using oqpsk modulator has been shown. it is resulted that the pts schemes perform the better performance as compared to slm and original ofdm signal at the parameter are listed in the table 2."
"as a suspension culture, the cells can be expanded in different kinds of vessels: shake flasks, glass bioreactors, stainless steel bioreactors, wave bags, and disposable stirred tank. in the case of hek293t cells, expansion, transfection, and lentivirus production have been demonstrated at 50-l scale in single-use bioreactors. 63 to transfect suspension cells, dna precipitation using calcium phosphate is expected to be less effective because of continuous culture stirring. therefore, other transfection agents like cationic polymers are used most of the time. linear 25-kda pei appears to induce the highest transfection efficiency in 293t and 293-ebna1 cells, leading to 75% of transfected cells using a green fluorescent protein (gfp) reporter plasmid. 66 the same form of pei is used in the patent from marceau and gasmi 63 for the production of lv and leads to 90% of gfp-positive cells. however, in this example, the transfection efficiency was measured 48 hours post-transfection. therefore, the gfp signal might also result from cell transduction by the neosynthesized lentiviral vector."
ensure that the ontology contains a data property with the name dataprophasprobvalue and that the individual ind has the data property with the value prob.
"the concept of ontology and bn integration was developed based on the need for a framework able to integrate logical and probabilistic reasoning simultaneously to exploit integration. the integration algorithms were developed to integrate bns into ontologies and vice versa, as shown in section 3.3. the major benefits obtained from the proposed concept are as follows:"
"in web ontology language (owl), reasoning in ontologies is supported using a sub-language that accommodates a description logic (dl)-based reasoning called owl dl [cit] . owl dl is the most important owl sub-language because it supports maximum expressiveness while still providing computational completeness and decidability [cit] . the fundamental modeling concept in dl is an axiom, a logical statement related to roles and/or concepts. the mechanism of reasoning using such logic is referred to as logical reasoning."
"the tv plasmid is the only genetic material transferred to the target cells and consists of the lv backbone containing the transgene expression cassette flanked by cis-acting elements required for encapsidation, reverse transcription, and integration. in view of biosafety improvement, self-inactivating (sin)-lv vectors have been developed, for which a deletion has been introduced into the u3 element of the 3′ltr. 23, 24 this type of vector loses the transcriptional capacity of the viral ltr once transferred to the target cells minimizing the risk of emergence of replication competent recombinants and avoiding problems linked to promoter interference."
in the case of stable inducible producer cell lines (tet-off induction system) 50-l wave reactor cultures (working volume: 25 l) could produce about 138 l of vector-containing cell culture supernatant in a continuous process (harvest period: 3 to 6-8 days postinduction) ( 56 and see below).
these are the main reasons why there is a clear transition from the use of mlv to lv vectors though the overall manufacturing conditions for lv vectors have not yet reached their maximal potential and the level of those used for mlv vectors.
"as lv gene therapy will be soon a routine treatment not only for rare genetic, but also for acquired diseases, i.e., haematological malignancies and infectious diseases as hiv infection, for which large number of patients are expected to be treated, the implementation of scalable vector production protocols is becoming urgent to satisfy the demand not only of academic and hospital institutions, but also of the industry, which is moving very rapidly towards this type of medicinal products."
"remark 1. from (5) and (9), we observe that the total power drawn from all txs' voltage sources depends on the mutual inductance m n0 between the coils of each tx n and the rx, but it is independent of the mutual inductance m nk between any pair of tx coils."
"in addition to the quality control requirements, as for all biotech products produced with continuous cell lines, lv vector batches as well as the producer cells at the end of vector production have to be analyzed for the absence of rcls which might be generated via homologous recombination. despite the fact that for the current protocol of the lv production, that uses split genome packaging constructs with little or no sequence overlap between the vector components, the unlikely generation of rcls has never been observed, the absence of rcls in every batch intended for clinical use has to be indeed proven using very sensitive methods. 114, 115 in this context, the availability of stable producer cell lines will provide an increased safety level since production can be performed starting from well-defined quality controlled cell banks and end of production cell lines could be tested much easier for absence of rcls."
"a data property named hasprobvalueof. create a rule or an axiom so that the object property influences_ is an inverse of the object property influenced_by_, and vice versa. create an individual with the same name as the node as an instance of class t; a.3."
"the bngetfirsttemplatenode function works by comparing the individuals in a set of siblings in the ontology that are most similar (i.e. has the same set of parents) to the individual being investigated in the bn. this assumption is the most likely to find the template node of a class in the ontology that its instance to be created in the bn. if no class is similar to the class being investigated, the bngetfirsttemplatenode function returns null and the class being investigated will not be created in the bn. the maximum complexity of the bngetfirsttemplatenode algorithm is o(n 2 ), where n is the number of nodes in the bn document. ontogetsiblings(node) get all siblings of node. m"
"define x ii h, m mm h, and b n b n b h n . thus, (p1) can be equivalently rewritten as the following sdp problem"
"such related studies discuss the transformation process of an overall ontology knowledge base into the form of a bn or oobn. bynowlife has a different approach; it uses the markov blanket [cit] principle. for reasoning in a case, only related nodes are necessary to answer a given query. the transformation of the ontology is only carried out on individuals belonging to the markov blanket of nodes contained in the bn."
"several advances have been achieved in the last years in the stable packaging field: (i) the use of integrating vectors rather than plasmids to deliver vector genes; (ii) the codon optimization of the packaging genes to destroy homologous regions to reduce the probability of rcl formation and ψ-gag recombination; it is worth mentioning, however, that rcl formation has never been reported, at the best of our knowledge, even with noncodon optimized genes; (iii) the development of constitutive packaging cells which are simpler and safer than inducible cells both in the upstream and downstream process; (iv) the development of the csin-lv to simplify the integration of the tv even though it is only applicable with an inducible system; (v) the application of rmce approach to the lv system. major contribution of the rmce technology is expected though in the integration of the tv to allow the switch of different transgenes in the same selected locus."
"the adjustment of bn structure through structural and parameter learning, as shown in the two cases, which are discussed clearly in section 4; and 3."
"the table 6 shows the papr values of the pts, slm and the original ofdm signal. it is shows that the papr value of pts is lower among the other two schemes slm and original ofdm signal at modulation order 'm' sub-band 'sb' and oversampling factor 'l' by using bpsk modulator. the table 7 shown below listed the different papr values at different sub-bands using slm papr reduction technique in bpsk modulator. at sub-band 32 it shows lowest papr value that is 6.37db and the highest papr value of 7.83 at sub-band 512."
"in today's era of modernization where wireless communications are maturing day by day, orthogonal frequency division multiplexing technique has become the most significant assets which is used worldwide. it is pervasive from wireless local area network (802.11g), worldwide interoperability for microwave access (wimax), to the long term evolution (lte) system, and digital video broadcasting (dvb-t, dvb-t2) [cit] . the ofdm is a digital modulation technique that upholds high bit rate transmission so it is used in high speed video and audio communication with eradication of inter symbol interference (isi) and inter channel interference (ici). it can accommodate bountiful number of user and increase the spectral efficiency of the system. the orthogonal signals are required for favoring orthogonal frequency division multiplexing system. this concept is used to separately demodulate overlapping carriers. orthogonality is a podium to expeditiously transmit information signals via a familiar channel without any interference."
"in the next experiment, the effect of sub-bands is analyzed on papr values while using qam modulator. here pts technique is used while varying the sub-band and keeping all the parameter constant namely modulation order 'm' and the oversampling factor 'l' shown in the fig 5. as shown in the table 4 given below, the papr performance of pts using qam modulator is analyzed at different sub-bands. according to that as we increased the number of sub-bands papr is increasing simultaneously."
"it can be proven that probabilistic reasoning in bn followed by logical reasoning in o is a subset of probabilistic and logical reasoning in o', which is a subset of logical reasoning in o'. because probabilistic reasoning in bn followed by logical reasoning in o is a subset of probabilistic and logical reasoning in o', there are cases where queries cannot be answered except by combining these knowledge bases and querying them. this occurs, for example, when there is an individual that does not exist in the probabilistic knowledge base but exists in the logical knowledge base while the individual is an entity that is taken into account during probabilistic reasoning."
"in the final experiment, the bit error rate is calculated between the transmitted and the pts signal because it is very important to preserve the bit error rate of the signal while reducing the papr. it is found that the bit error rate is almost the same so it is proved that the pts reduces the papr efficiently without degrading the bit error rate shown in fig.10 . the similar experiment is also done in the slm scheme by using the bpsk modulator and it also give the same result as the pts schemes that is the bit error rate is almost equal to the original transmitted signal in case of slm slight increased in es/no value shown in fig.11 ."
"however, the type of required reasoning is not always purely logical (true or false, 1 or 0) but can also be probabilistic (the degree of certainty or probability that an event will happen, represented by a value between 0 and 1). bayesian networks (bns) have been chosen by many previous researchers as models for managing probabilistic reasoning in ontologies [cit] . this is because in addition to its ability to perform probabilistic reasoning with prior knowledge, a bn has a graphical structure like that supported by ontological representation in owl."
"in future work, we want to enhance the template class selection technique as implemented in the bngetfirsttemplatenode function. currently, the function's implementation is only based on the similarity of the direct descendant class to the class being investigated. we must consider the next steps to be taken when the template node is derived from the indirect descendant class or when the function returns several alternative template nodes because, in an ontology, an individual can be derived from several classes at once. the template class selection technique will also become more complex by including not only parental similarities but also the similarities of children in determining the template class."
"production in bioreactors requires the expansion of the producer cells in suspension. several cell lines used for lentivirus production (293t, 293ft, and 293sf-3f6) have been described to be prone to adaptation to suspension culture in chemically defined media (freestyle 293 and f17, invitrogen, carlsbad, ca; hyqsfm4transfx293, hyclone, logan, ut). [cit] these cells grow readily in suspension with no need for microcarriers rendering thus their culture and expansion much easier than for adherently growing cells. in addition, the absence of bovine serum and animal origin components in the culture media is the most suitable situation for clinical manufacturing as this decreases the risk of contamination by adventitious agents."
"the conventional reasoning query process is done in two steps. the first step is to perform probabilistic reasoning within the bn knowledge base. the second step, based on the reasoning in the first step, is reasoning within the ontology knowledge base; this returns the final result of the reasoning. the reasoning query process on ontology-based systems that utilize bynowlife is completed in just one step by sending logical and probabilistic reasoning queries in sparql format. this is because bynowlife transforms bns into ontologies (through the merging process) automatically before reasoning is performed on the ontology knowledge base. it can be proven that the conventional two-step process of reasoning is a subset of bynowlife's reasoning process; that is, the reasoning done by bynowlife encompasses conventional reasoning."
"without those perspectives of improvement, stable producer cell lines will represent a much more affordable production system once they will be available for lv production."
"the prototype lv vector system is based on hiv-1, a very well-studied human pathogen virus. besides hiv-1, other lentiviruses have also been developed as gene transfer vectors (tvs) but most of them have not yet reached the clinical study stage, such as hiv-2 (ref. 14) simian immunodeficiency viruses, 15 or nonprimate lentiviruses including feline immunodeficiency virus, 16 bovine immunodeficiency virus 17 or caprine arthritis-encephalitis virus. 18 only equine infectious anemia virus (eiav)-based vectors 19 have been developed up to clinical use."
"ensure that the ontology contains object properties with the names objpropinfluences and objpropinfluencedby, then create a rule that objpropinfluences is an inverse of objpropinfluencedby (and vice versa)."
"in this section, firstly the graphical user interface (gui) is created in matlab simulation tool and then writes the code under each button to calculate the papr values. there are two way to take a input data firstly the recorded voice or secondly to load the save voice file. the performance evaluation and comparison of pts & slm papr reduction technique by using different modulator oqpsk, qam and bpsk has been done. the various parameter used in simulation, namely transmitted data, papr reduction technique, modulator, modulator order 'm' and sub-band 'sb' is listed in the table 1."
"obviously the standard transfection including the suspension culture-based transfection protocols is insufficient for providing the lv vector quantities required for the future routine use of lv vectors, and can only be considered as intermediate solution. the final solution for the production of large vector lots will be the implementation of stable producer cell lines cultured under suspension conditions allowing in principle unlimited scalability."
"in the partial transmit sequence (pts), the transmitted data block is divided in small parts before applying to the ifft block, the small data sub-blocks are multiplied by the weighted phase vector and the resultant data block have the minimum papr value as compared to the input data block, the small divided data block is given by the equation."
"uika is a university that cares about its customers and pays special attention to its implementation of social crm. it allocates funds for various activities at annual budget meetings, including those supporting the implementation of social crm. departments or units directly involved in the implementation of social crm at uika include the human resources (hr) department, the public relations (pr) department, the computer and information systems (cis) unit, the academic administration (aa) bureau, the administration and financial resources (afr) bureau and the quality assurance (qa) unit. figure 7 shows a simplified model that illustrates the semantic relationships between entities at uika. each department/unit has its own budget for supporting the successful implementation of social crm. other information related to csfs that determines the success of social crm implementation at uika is illustrated in figure 8 . suppose that we want to answer the following queries: (1) display units that handle csfs and their budget allocations for the successful implementation of social crm-in this case, we can only query the owl document for a result; (2) list three factors of csfs that most influence the successful implementation of social crm-in this this case, we can only query the xdsl document for a result; and (3) list three units that handle csfs and their budget allocations, which are prioritized to address the successful implementation of social crm-in this case, we need to query both the owl and xdsl documents simultaneously for a result. an example of sparql syntax for query no. 3 is as follows:"
"i.e., all txs carry the same in-phase current which can be adjusted. in particular, we compare the wpt performance for the case with txs' constraints on the peak voltage and current, and for the case without txs' constraints, respectively. we define the efficiency of wpt as the ratio of the delivered load power β 0 to the total tx power p, i.e., η β 0 p . fig. 3 plots the total tx power p and the efficiency η versus the delivered load power β 0 . all curves show the feasible and optimal values. for the case without txs' constraints, we observe that the wpt efficiencies by using magnetic beamforming and by using the benchmark are 87.1%"
"the different value of papr at different sub-bands 'sb' is shown in the table 5 and it is analyzed that the papr is increasing simultaneously as the number of sub-bands is increased. now the next experiment is performed by using the binary phase shift key modulator (bpsk) again the pts technique perform the better papr reduction as compared to other techniques, the only thing is observed that the papr values in bpsk is slightly higher as compared with the other two modulator. the ccdf vs. papr curve shown in the fig.7 demonstrate the comparison between the pts, slm and the original signal."
"in this paper, the two very efficient papr reduction scheme pts and slm are used under three modulator namely qam, oqpsk and bpsk by varying different parameter in wlan. the pts scheme performs much better than slm in papr reduction technique, the graph and the tables supports the statement. the three modulators are used in the experiment out of which the qam is performing better than papr reduction in comparison with oqpsk & bpsk. the subbands play a very important role, as the number of sub-bands is increased the papr is increased simultaneously because the number of ifft operation is increased proportionally. the variation in papr values is greater in qam as compared to oqpsk and bpsk by varying sub-bands shown in fig.5, 6 & 8. the modulation order 'm' is also very important parameter, as the modulation order is increased the papr value starts decreasing step by step shown in fig.9 . finally the bit error rate comparison has shown in fig.10 between the original transmitted signal and the pts shows that the pts reduces papr very effectively without degrading the bit error rate. similarly, fig. 11 shows that the slm technique reduces the papr efficiently while keeping almost the same bit error rate to original transmitted signal."
"the following section compares the production processes that have been described in the literature from bench scale to industrial manufacturing. the comparison focuses mainly on the technical methods developed by the producers. it would not be appropriate to discuss the performance and productivity of each process since the differences in lentivirus systems, transgenes of interest and titration methods do not allow objective comparisons. nevertheless, the titers that were reported by the authors are given as an indication in this review."
"now in the next experiment, the same experiment is performed through slm scheme using oqpsk modulator by keeping the entire parameters constant and varying the subband (sb).the papr vs. ccdf curve shown in the fig. 6 demonstrates that slm perform better as compared to original ofdm signal and the papr in increasing with increasing number of sub-bands."
"the morpher will create individuals that did not previously exist in the owl document; in this case, income is created as a generic type. the generic type is a subclass of thing. the morpher will then create a special property called hasprobvalue for every individual in the system. the reasoner will set probabilistic values in the xdsl document based on the data in the owl document, perform probabilistic reasoning and then return the result to the owl document by updating its respective property values. the result of running the program is shown in figure 5 and figure 6 ."
"after that, the time domain data sub-blocks is multiplied by the weighted phase vector 'p' which is given as, now the divided data sub-block and phase factor are required to combined together to create a set of candidates which having the lowest value of papr in decibel."
"in terms of productivity in suspension culture, the published data indicate that lv titers are similar to the values obtained in adherent cells. the titers achieved in the bulk harvest or culture supernatants are in the range of 10 7 to 10 8 infectious genomes (ig)/ml or tu/ml [cit] while the productivity of adherent cell systems varies from 10 6 to 10 8 tu/ml 69 (table 1) . however, the comparison is rather difficult since a limited number of comparative studies has been reported using the same vector and because of differing analytical procedures."
"various studies related to the integration of probabilistic information into ontology have been carried out and applied to various areas of research such as image classification [cit], knowledge repositories for website evaluation methods [cit], and decision support systems for terrorist identification [cit] . probabilistic ontology has great potential as a knowledge base that is able to address challenges of logical and probabilistic reasoning simultaneously. owl dl and its variations have been standardised by the world wide web consortium (w3c) for logical reasoning in ontology [cit] . however, w3c has not yet established a standard for probabilistic reasoning in ontology. previous studies have attempted to fill this gap by combining bns with ontologies to make probabilistic reasoning available for use in ontology, but these have mainly focused on how to represent probabilistic information in owl notation. for future systems or those that are newly built, the process of establishing an ontological knowledge base containing probabilistic information from the ground up may not be a problem. for systems that already have ontologies and bn knowledge bases, it is unnecessary to rewrite the probabilistic information contained in the bn into the owl; it will require significant effort, especially if the bn consists of thousands of nodes and relations. in cases such as this, a machine capable of transforming the information contained in the bn into owl and vice versa is needed."
"although vsv-g is endowed with the indisputable quality of entering any type of cells either in resting or stimulated conditions, 116 its application is highly preferable in ex vivo rather in vivo gene therapy to avoid the risk of possible toxicity in transduced nontarget cells. in fact, even if specific promoters or specific mirna target sequences 117 can be included in the tv to control transgene expression, the risk of possible genotoxicity derived by lv integration in nontarget cells cannot be prevented by these strategies. furthermore, if directly administrated to the blood stream (intravenous administration) vsv-g pseudotyped vectors are sensible to degradation by human complement; however, this fact does not preclude the administration of vsv-g pseudotyped lv vectors and efficient transduction of target cells in the case of localized delivery (such as treatment of disorders of the central nervous system 12 or of the liver, i.e., for the treatment of haemophilia). 118 to alleviate most of the drawbacks associated with the use of vsv-g pseudotyped lv vectors, alternative envelopes complementresistant, such as the rd114-tr and rd114-pr, have already been tested in stable systems, 89, 92, 107, 119 whereas other excellent candidates studied only in transient systems are awaiting to be validated also in stable packaging cell lines. among these, the most promising include the baboon endogenous retrovirus glycoprotein, belonging to the same beta-retroviruses family of rd114. baboon endogenous retrovirus has recently been shown to transduce at high efficiency resting hscs. 120 yet, a scfv derived from a specific monoclonal antibody against the cd133 molecule has been developed as lv pseudotyping agent to preferentially transduce a population of human hematopoietic stem cells with high proliferative potential in vitro and multilineage engraftment in vivo. 121 finally, mutants of the measles virus hemagglutinin (h) and fusion (f) glycoproteins h/f have been shown to efficiently transduce quiescent t and b lymphocytes in the presence of high concentrations of measles virus antibody positive human serum. 122 although alternative envelopes have much higher target specificity than vsv-g, it has to be kept in mind that they are not entirely specific because all cells expressing the specific receptor recognized by the env protein can be transduced, signifying the requirement for their careful evaluation for direct in vivo administration."
"as shown in the table 3 given below, out of the three techniques used for the comparison of papr, pts shows the best performance among the three using qam. the papr values and the parameters used in this experiment are listed in the table."
"related studies on representing probabilistic information in ontological notation and reasoning with it are more focused on the rules of representation than on the transformation of existing bn knowledge bases. consequently, an assumption arises that the inclusion of probabilistic information in ontologies must be done from the beginning when designing the ontology itself. in practice, however, the domain engineer and the knowledge engineer have roles in preparing the knowledge base. domain engineers struggle with the concept structure of a domain along with its logical rules, and the knowledge engineer examines causality models of inter-node relationships in a problem domain. the domain engineer's work results in a logical knowledge base (ontology), while the knowledge engineer's work results in a probabilistic knowledge base (bn). a union of the two types of knowledge bases through transforming existing knowledge bases so that the processing time is much faster is a more efficient solution than having to redesign them from the beginning."
vector production at large scale when moving towards clinical trials larger vector lots are required signifying that the production method has to be scaled up. this can either be performed by using a scale-out approach (addition of supplementary production units) or by the use of suspension cultures which are characterized by much better scalability than when using adherent cell cultures. both approaches will be presented in the following:
"a. get s ← siblings of f. if s is not empty, then follow the next steps: a.1. get a t ← selected template node from one member of s. the selection rule of t as a template node is described in the bngetfirsttemplatenode function. a.2."
"applications up to a couple of meters. with the short-range wpt technology reaching the stage of commercial use, the mid-range wpt technology has been gathering momentum in the last decade."
"we have developed an algorithm, called mergeonto, to merge the information contained in an ontology into a bn. the following are some of the terms used in this algorithm:"
"where p av is the average power of the signals in time domain, and x (t) 2 is the maximum peak of the signal [cit] ."
set maximum likelihood estimation (mle) values for each probability value of the node based on the siblings list s using moving average or sliding window algorithms.
"several issues still remain to be addressed: the majority of stable systems were developed using the gfp marker and only a few have used therapeutic genes. it is necessary to collect more data on a larger number of therapeutic genes to understand how the expression of the transgene can influence cell productivity. furthermore, are there alternatives to hek293t cells as starting material? is autotransduction a safety concern during lv manufacturing? would the removal of the lv envelope receptor by the plasma membrane of the producer cells prevent either autotransduction or vector aggregation without affecting the performance of the producer cells?"
"where c are classes or concepts, i are individuals, p are properties attached to a class or individual, r are inter-class or inter-individual relationships and x are axioms. bn is a bayesian network:"
"the block diagram of the partial transmit sequence (pts) is shown in the given below fig.2 . the objective is to optimally combine the 'v' sub-blocks to obtain the time domain ofdm signals with the lowest papr\" [cit] \"."
"an investor has shares in the following companies: mandiri, kpc and telkomsel. mandiri is a bank, kpc is a mining company and telkomsel is a telecommunications company. each stock has a price property. however, a government policy affects the fluctuation of stock prices. the latest condition of the case, as represented in two documents (owl and xdsl), can be seen in figures 3 and 4 below."
"remark 3. as a convex problem, (p1−sdr) can be polynomially solved via an interior-point method [cit], to arbitrary accuracy. the optimal solution to (p1) is directly obtained from the optimal solution to (p1−sdr), without any postprocessing required."
"large-scale vector production using adherently grown cells large-scale productions of lv are mostly a direct scale-up (scaleout) of the small-scale production methods by augmenting the culture surface via addition of supplementary culture/production units. the productions are essentially performed in parallel cultures using large numbers of multitray systems (cell factories (cf) (cf-10 ( figure 2) ) or cell stacks (cs)). because of easier manipulation, 10 stack devices (cs-10) are preferable, though, in principle, 40 stack devices could equally be used, requiring nevertheless a specific handling system due to the elevated weight of the cf-40 stacks. furthermore, gas exchange as well as medium layers may not be identical for all plates and growth control using microscopy is very 42 or in a semiclosed mode 43 providing improved safety for the operator, the environment as well as for the final product."
"vector production using suspension cultures although transfection of adherent cells is the gold standard methodology for producing lv, this system is rather limited in scalability. for industrial manufacturing, cell culture in large bioreactors is usually the most convenient approach."
"in order to concentrate the bulk product (supernatant or an intermediate product), tff systems are employed allowing also diafiltration for buffer exchange and formulation. at small scale or at later stages of an industrial downstream processing protocol centrifuge-based disposable devices 42, 45, 69, 100, 102, 105 and at a larger scale hollow fibre 42, 52, 55, 107 or flat membrane cartridges 56, 106, 108 are used. tff devices have also been applied for the concentration and diafiltration of lv vectors pseudotyped with other than vsv-g envelope proteins. 59, 69, 109 depending on the therapeutic indication (i.e., in vivo administration or ex vivo application), the concentration factor will vary significantly. for instance, in the case of intracerebral administration of lv, as exemplified by the parkinson's disease treatment prosavin from oxford biomedica, 12 the vector has to be highly concentrated as the injectable volume in brain is extremely small. in this specific case, a double tff has been implemented to achieve titers above 10 8 tu/ml. 81 as a consequence, the tff was the ultimate step of the purification process since any additional procedure like polishing or 0.2-µm filtration would have resulted in significant loss or dilution of the final product."
"we have other ideas about this, motivated by the belief that incorporating a bn into an ontology and vice versa should be more than just a transformation process; it should also benefit them both. this paper presents a concept called the bayesian network and owl integration framework (or bynowlife), a novel approach to integrating bns with owl by providing an interface for probabilistic reasoning through sparql queries. to the best of our knowledge, this work is the first to integrate bns into owl and vice versa, and exploit the integration process. the main contributions of this research are as follows: (1) ontology enrichment based on bn knowledge bases; (2) bn structural adjustment through structural learning based on ontology knowledge bases; and (3) support for an integrated probabilistic clause in the sparql query format. this paper is organized as follows: section 2 outlines previous work related to the merging of ontologies and bns. section 3 describes bynowlife as the proposed approach as well as the translation rules and algorithms used. section 4 presents the experiment as an implementation framework using two validation cases to demonstrate the feasibility and effectiveness of our approach. section 5 details the reasoning proof, while section 6 discusses the results of the experiment and important aspects of the framework. section 7 concludes the study, and section 8 suggests several next steps for future work."
"as we phase rotated data blocks, the rotated ofdm data blocks represents similar information which are unmodified ofdm data blocks, provided with known phase sequence. in fig. 1 block diagram of the slm technique is shown. in this frequency domain signal is converted into the time domain x (u), by the help of ifft. the signal comes from ifft block is in time domain having the lowest papr value and same information as the original ofdm signal. the ofdm signal having lowest papr value in decibel would be selected for making high power amplifier (hpa) worked in the linear region. the actual signal is recovered by the help of side information at the receiving end. the side information is the knowledge about the phase sequence used at the transmitting end to reduce the papr. the two main draw backs of slm technique are, first is the side information and another is the phase sequence. for improving papr performance there is a need of increasing the number of phase sequence which increases the complexity of the system. as the number of phase sequence increased the size of side information is also increased proportionally as a result spectral efficiency decrease. these disadvantages are overcome by using other technique called partial transmit"
"the ofdm signal are transform by a number of independent sub carriers, which introduce the high peak to average power (papr) when these sub carriers are combined constructively. the various frequency signal produces a high peak power when added up in a same phase. generally, the papr of the ofdm signal is known as the ratio of the maximum power to its average power of the signal."
"calculate the probability values of n by looking for the center of distribution (e.g., using a moving windows average algorithm) based on the probability value distribution of nodes in s."
"considering vector production, hek293 or hek293t cells (see below) are transfected with four plasmids encoding the gag-pol genes, the rev gene, the vsv-g envelope gene, as well as a sin lv tv plasmid with an internal promoter for transgene expression (figure 1) ."
"this case illustrates the decision-making process in the investment in social crm development in a higher education institution, namely bogor ibn khaldun university (universitas ibn khaldun bogor"
the framework and its implementation in prototype form have been demonstrated in the investment problem and social crm for higher education cases. the findings from these scenarios show that this framework efficiently performs both logical and probabilistic reasoning simultaneously.
"lv vectors have been used successfully in clinical trials, in a first instance for the treatment of rare diseases, in particular, of primary immunodeficiencies 7, 8 and in neurodegenerative storage diseases. 9, 10 however, their application for the treatment of more frequent genetic and acquired diseases, including treatment of β-thalassemia, 11 parkinson's disease, 12 and chimeric antigen receptor-based immunotherapy of cancer, 13 has been assessed in clinics with exciting outcomes. this means that manufacturing technology becomes a critical issue in view of the implementation of these novel therapies for routine use."
"remark 2. theorem 1 implies that for the case of identical tx resistances, the optimal current magnitude of each tx n is proportional to the mutual inductance m 0n between the rx and tx n."
"compared with transient production, stable lv manufacturing is the best option for gene therapy to reduce the production costs and increase the overall safety and reproducibility, which are, currently, the major limitations discouraging the application of the transient method to the large scale. the implementation of stable systems for clinic trials first and ultimately for gene therapy product commercialization represents, therefore, a mandatory milestone to reduce the manufacturing cost and to enhance quality and safety of lv. [cit] s, when the first packaging cells were generated, various construction strategies have been developed differing, essentially, for the method chosen to introduce the vector encoding genes, i.e., plasmids versus integrating vectors, and the nature of the pseudotyping envelopes utilized. the latter has been one of the most important elements guiding towards the development of a larger number of inducible rather than constitutive packaging cells. as the heterologous vsv-g envelope, the most widely utilized envelope for lv, is highly cytotoxic, and its transcription must be induced only at the time of lv production to prevent packaging cells death. furthermore, the expression of gag and pol genes of hiv is associated to cytotoxic and cytostatic effects and therefore high expression of these genes must also be regulated only during the production phase. the first and more frequently utilized inducible systems are the tet-on and tet-off systems, which are based on the addition or removal, respectively, of the tetracycline/doxycycline antibiotic in the culture medium to trigger gene transcription through the tetracycline response element (tre). when alternative nontoxic envelopes were found suitable for lv pseudotyping, constitutive packaging cells were later generated. this section will treat separately inducible and constitutive packaging cells derived from both hiv-1 and eiav lentiviruses, 3, 19 whose main features are summarized in table 2 ."
"(p1) is a complex-valued non-convex qcqp problem [cit] . although solving such a problem is nontrivial in general [cit], we obtain the optimal solution to (p1) in section iv."
"for analytical convenience, we treat the complex currents i n 's flowing through txs as design parameters 1, which are adjusted to realize magnetic beamforming."
"all large-scale production protocols were developed for hiv-1 42, 52, 54, 61 and eiav 37, 55 -based lv vectors. very recently a semi-\"large-scale\" lv vector production system based on the use of hollow fibers has been presented. 62 the hollow fibers are seeded with hek293t cells, which are then transfected with three plasmids after attachment for 24 hours. the advantage consists in the fact that it is a closed, fully automated culture system with an lv yield equivalent to three cf-10 stacks. this, however, requires that several parallel systems have to be set up for real larger scale productions. table 1 presents details of the available large-scale vector productions."
"we propose a framework for systems using ontologies as their knowledge bases and requiring both logical and probabilistic reasoning abilities simultaneously. we call it bynowlife, which stands for bayesian network and owl integration framework. it is shown in figure 1 ."
"here, n represents a set of nodes, v represents a set of vertices (links) and ρ represents the set of conditional probability values of every node in the bn."
"as well known, the rank constraint in (19e) is non-convex [cit] . by ignoring the rank-one constraint in (19e), we obtain the sdr of (p1−sdp), denoted by (p1−sdr), which is convex."
"the ontogetfirsttemplateclass function works by comparing the nodes in the bn that are most similar (i.e., has the same set of parents and children) to the node being investigated in the ontology. this assumption is the most likely to find a template class of a node in the bn that its instance to be created in the ontology. a template class is required to create an individual because, in an ontology, an individual must be an instance of a class. if no node is similar to the node being investigated, the ontogetfirsttemplateclass function returns a generic class, which is a subclass of owl:thing by default. the maximum complexity of the ontogetfirsttemplateclass algorithm is o(n 2 ), where n is the number of nodes in the bn document. the complexity analysis of the mergebn algorithm where n is the number of input nodes in the bn document is explained in table 2 . step 1 + step 2 + step 3"
"the basic algorithm of adams-bashforth-moulton prediction-correction method is shown in equations (11) and (13), where the coefficients are shown in equations (12) and (14) . although the calculation of the fractional time domain analysis method is more complicated than the frequency domain method, the order of the calculus operator can calculate the step change smaller, which is more conducive to analyze the dynamics of the fractional system. now redefine the mathematical model of the fractional-order unified chaotic system:"
"using the same approach as in (54), but altering the wrapping from [−π, π] to [0, π], we can write the wrapped elevation pdf, f w (θ), on [0, π] as"
"the algorithm is further optimized according to the adomian decomposition method [cit] . the fractional-order unified chaotic system is taken as an example to decompose the nonlinear part. according to the given conditions, the following equation can be obtained:"
"for any decreasing function, g(x), such that the integral in (67) exists. using (65)-(67), the error term, ula, in the approximation of (35) for a ula can be bounded as"
"use tables 2 and 3 62.840 0.2500 0.0025 table 3 . capacitance value in q-order fractional unit circuit. the circuit diagram of the fractional-order unified chaotic system is shown in figure 12 . for fractional-order unified chaotic system, the linear part is composed of basic operational amplifier circuit, such as reverse proportional amplifier, reverse adder, subtracter, in-phase proportional amplifier, reverse integrator, etc., and the nonlinear term is represented by multiplier. the parameters of each component are as follows:"
"where f w (θ) is the wrapped pdf of θ on [0, π]. proof: the proof is given in appendix e. at present no further convenient analysis for h v (z 1, z 2 ) appears tractable and we leave the representation in lemma 3 as a single finite range numeric integral. substituting the results of lemmas 1, 2 and 3 in (30), (31) and (32), respectively, gives κ which completes the derivation of (20) and (26) . as for the ula, we note that summation over n can be truncated with very few terms as the ψ(n) coefficients decay very rapidly. in appendix f we prove the convergence of the series in lemmas 1, 2, and 3 and also derive bounds on the truncation error of these series. this allows us to select the number of terms in the truncated series to guarantee any desired accuracy."
"the sinr expressions in (20) and (26) both require κ which can be computed as follows for the three antenna layouts discussed in sec. ii. substituting the steering vectors for the ula into (21) and defining θ, φ as a generic pair of ray angles andφ as a second azimuth angle independent of φ gives"
"the authors declare no conflict of interest. figures 15 and 16 show that under the same control law, the response system of the fractional-order unified chaotic system is synchronized with the drive system, and has good control performance."
"f is the frobenius norm. hence, κ can be interpreted both as the interference power between two rays and also as the normalized squared frobenius norm of the channel correlation matrix. this allows the sinr results in proposition 1 and proposition 2 to be interpreted in a familiar way as functions of system size, snr, channel power and channel correlation. we immediately see the adverse effect of correlation on snr/sinr as both snr zf l and sinr mmse l are decreasing functions of κ. we note that channel hardening has been observed in a recent measurement campaign [cit] where it was found that measured indoor channels do experience channel hardening but more slowly than i.i.d. rayleigh channels. this experimental work supports the approximations used in (17) and (24) which partly depend on channel hardening. the results in sec. vii also align with [cit] as it is shown that both κ and sinr performance improve for wider angular spreads, where i.i.d. rayleigh can be seen as an extreme case."
"equations (20), (26) and (27) are powerful tools for interpreting the behavior of ray-based massive mimo channels. note that (52) in appendix b demonstrates that κ has the alternative expression,"
"the distribution of the ray angles affects the mean sinr only through the term κ. hence, we discuss the impact of κ in more detail. the link between κ and the actual angular pdf is not obvious as κ has a relatively complex formulation. nevertheless, the trends can be inferred from the interpretation of κ in (21) as the interaction power between two distinct and independent rays. for small numbers of antennas and uras, traditional thinking applies. angular pdfs with larger variance (wider angular spread) spread the rays, reducing the chance of two rays being similar and hence reducing interference. therefore, increased angular spread is beneficial for small numbers of antennas. for large numbers of antennas in a ula the situation is different and more complex. substituting the ula steering vector from (3) into (21), gives"
"note that the sectoral antennas envisaged for massive mimo have finite angular range radiation patterns in both the azimuth and elevation domains [cit] . hence, radiation near end fire is substantially attenuated and this will reduce the visibility of these unexpected trends."
"the basic algorithm of adams-bashforth-moulton prediction-correction method is shown in equations (11) and (13), where the coefficients are shown in equations (12) and (14) . although the calculation of the fractional time domain analysis method is more complicated than the frequency domain method, the order of the calculus operator can calculate the step change smaller, which is more conducive to analyze the dynamics of the fractional system. now redefine the mathematical model of the fractional-order unified chaotic system:"
"note that any desired accuracy in (35) can be obtained by choosing n as in appendix f. next, we consider the hura where the expectation required in (31) is of the form"
"based on these problems, in this paper, different methods are used to solve the numerical solution of the fractional-order unified chaotic system, and the phase space diagram of the corresponding order is obtained. then, the circuit simulation diagram of the chaotic system is designed. finally, three control laws are obtained by the active control method, and the control and synchronization of the fractional-order unified chaotic system are completed."
"breaking up the integral in (62) into real and imaginary terms and using [48, equations 6.681.8 and 6.681.9] gives the desired result in (37) ."
. this is denoted as property 1 and is used in various proofs in sec. iii and the appendices. this follows from the independence of h i and h j and the following results for the ray coefficients:
we examine a single-cell massive mimo system where a bs with m electronically steerable antenna elements configured as a ula or ura serves l single-antenna users within a single resource block. we consider ul transmission assuming perfect csi at the bs. 1
"the above three methods are directly calculated in the time domain, avoiding the approximation error caused by transforming time domain into frequency domain and improving the accuracy compared with the calling function method."
"in practical engineering application, there are many algorithms for solving fractional differential equations. the most commonly used is time domain and complex frequency domain transformation method. by solving transfer function 1 s q, its expanded form is obtained and then transformed to time domain for solution. in reference [cit], the expansion of 1 s q is determined by bode graphic approximation method, and the 1 s q expansion of q from 0.1 to 0.9 is derived in reference [cit] . generally, when designing a fractional-order chaotic circuit, the corresponding integer order chaotic circuit should be designed firstly, and the capacitance in the integer circuit is replaced by a fractional-order module to form a fractional-order chaotic circuit [cit] ."
"in practical engineering application, there are many algorithms for solving fractional differential equations. the most commonly used is time domain and complex frequency domain transformation method. by solving transfer function, its expanded form is obtained and then transformed to time domain for solution. in reference [cit], the expansion of is determined by bode graphic approximation method, and the expansion of q from 0.1 to 0.9 is derived in reference [cit] . generally, when designing a fractional-order chaotic circuit, the corresponding integer order chaotic circuit should be designed firstly, and the capacitance in the integer circuit is replaced by a fractional-order module to form a fractional-order chaotic circuit [cit] ."
"c,s is affected by the spread. however, this term is small relative to lβ (l)2 and mβ (l) . hence, the effect of the spread of the ray powers is usually dominated by the terms involving the total power, β (l), especially when l and/or m are large (see [cit] )."
"it can be seen from the above calculation process that the improved adomian decomposition method has no gamma function in the process of decomposition, so the decomposition process is simpler than the adomian decomposition method, which it is more convenient in the programming of the program and has little calculation amount."
"it can be seen from figure 13 that the circuit simulation diagram of the fractional-order unified chaotic system is basically consistent with the phase space diagram obtained by the numerical solution of the system, and it is verified that the 0.9-order fractional-order unified chaotic system is exists. it can be seen from figure 13 that the circuit simulation diagram of the fractional-order unified chaotic system is basically consistent with the phase space diagram obtained by the numerical solution of the system, and it is verified that the 0.9-order fractional-order unified chaotic system is exists."
"author contributions: g.l., x.z., and h.y. conceived the idea and research theme. x.z. designed and performed the experiments. g.l., x.z., and h.y. analyzed the experimental results. g.l., x.z., and h.y. wrote and revised the paper."
the simulation circuit diagram of the fractional-order unified chaotic system is built in the multisim 14.1 version. the simulation result is shown in figure 13 . figure 13 .
"where, u 1, u 2, u 3 are the control input, v 1, v 2, v 3 are the linear functions of x 1, x 2, x 3 . get the following formula:"
"the algorithm is further optimized according to the adomian decomposition method [cit] . the fractional-order unified chaotic system is taken as an example to decompose the nonlinear part. according to the given conditions, the following equation can be obtained:"
"author contributions: g.l., x.z., and h.y. conceived the idea and research theme. x.z. designed and performed the experiments. g.l., x.z., and h.y. analyzed the experimental results. g.l., x.z., and h.y. wrote and revised the paper."
"also shown in fig. 5 is the unusual end-fire effect for a ula discussed in fig. 1 . for very large angle spreads (plotted in green), κ is actually higher than for scenario 2 as the wide angular spread gives rise to large interference at end-fire. the inset also shows that for large m there is a cross-over and the widest angular spread has a higher value of κ even compared to scenario 1. fig. 6 plots the global mean sinr for mmse processing as a function of m . the y-axis is therefore sinr"
"where e θ,φ,φ [·] refers to the expectation over the ray angles and the phases of the ray coefficients. hence, we derive expressions for e θ,φ,φ [sinr l ] for zf and mmse processing. in the following, we omit the expectation operator subscripts for a more compact notation. with this definition of expectation, we note that the channel model in (1) satisfies the second order properties:"
"p erformance analysis of linear transceiver techniques for conventional and massive multi-user multiple-input multiple-output (mu-mimo) systems is well advanced for simple statistical channel models [cit] . early work on independent, identically distributed rayleigh fading channels has been extended to a wide range of more complex and realistic channel models. for example, results are now emerging on complex, heterogeneous, correlated ricean channels for both uplink (ul) and downlink (dl) systems employing maximum ratio combining (mrc) [cit], zero-forcing (zf) [cit] and minimum mean-squared error (mmse) combining [cit] . in contrast, the literature on performance analysis for ray-based channels is very sparse. in this paper, we use the phrase ray-based to denote a wide class of channel models where a user's channel is broken down into a finite set of rays denoting the different multipath components in the channel, as a result of the transmitted waveform interacting with objects in the environment forming different propagation processes. specific properties of the ray angles can be modeled by statistical distributions obtained from propagation measurements. broadly speaking, this covers many of the models described as spatial or directional."
"we also note that in the high snr regime, we have ρm β (k) /(1 + ρm β (k) ) ≈ 1 and with this approximation we have the simplified version of (26) given by"
"mmse processing: the trends shown by (26) are the same as zf for m, ρ and the ray powers: asymptotic linear growth in m, ρ and β (l) . for mmse also, large values of κ reduce the sinr."
"in this paper, different numerical methods of fractional calculus are used to analyze the fractional-order unified chaotic system, that is the adams prediction-correction method, the adomian decomposition method and the improved adomian decomposition method. secondly, 0.9-order fractional-order unified chaotic circuit simulation diagram is designed. finally, the control and synchronization of the fractional-order unified chaotic system is completed by the active control method."
use tables 2 and 3 to calculate the cell resistance and capacitance values for the fractional property. the system function [cit] of the equivalent circuit between a and b can be expressed as:
"ray-based models have several advantages over the simplified statistical models: (1) they are more physically motivated; (2) they have a direct link to the antenna layout at both link ends, and to the propagation environment; (3) they are applicable over a wide range of frequency bands. for example, recent ray-based measurements are used to characterize the channel at 2.53 ghz [cit] and at 28 ghz and 73 ghz [cit] . for these reasons, ray-based channels form the basis of many standardized models used to evaluate the performance of the upcoming new radio systems [cit] ."
"such a methodology was developed in our conference paper [cit] for uniform linear arrays (ulas) using zf or mrc processing leading to accurate approximations for the zf signalto-noise-ratio (snr). in this work, we extend this approach considerably to handle mmse processing and full-dimension processing with uniform rectangular arrays (uras). we also investigate the fp convergence and the underlying spatial correlation structure of ray-based channels. we develop a novel methodology to analyze zf and mmse in ul systems for ulas and uras and an extremely wide range of angular distributions. this includes all commonly used ray models, such as those containing clusters of rays and angles with wrapped gaussian and laplacian distributions. to this end, unlike previous studies in the literature, our work acts as a unified framework for the performance analysis of massive mimo systems with ray-based propagation. in particular, we make the following contributions:"
"where ψ c (n) and ψ s (n) are the cfs of the central cluster angles and subray offsets. from lemma 1, we have the exact solution"
"the vast body of papers in the massive mimo literature consider statistical channel models for the performance characterization. our paper moves the state-of-the-art by innovating analytical solutions for the more physically motivated ray-based channel models. we have derived accurate expressions for zf snr and mmse sinr, and the resulting spectral efficiencies for ray-based channel models. the results, applicable to different antenna configurations and parameter distributions, offer important insights into the effects of the propagation environment. we have demonstrated an improvement in performance when moving from vura to hura to ula antenna configurations, and a corresponding increase in the robustness to propagation scenarios. we have also investigated the impact of different array configurations and system parameters on the rate of convergence to favorable propagation. finally, we evaluated the spatial correlation properties of ray-based models."
"in this paper, different numerical methods of fractional calculus are used to analyze the fractional-order unified chaotic system, that is the adams prediction-correction method, the adomian decomposition method and the improved adomian decomposition method. secondly, 0.9-order fractional-order unified chaotic circuit simulation diagram is designed. finally, the control and synchronization of the fractional-order unified chaotic system is completed by the active control method."
"it can be seen from the above calculation process that the improved adomian decomposition method has no gamma function in the process of decomposition, so the decomposition process is simpler than the adomian decomposition method, which it is more convenient in the programming of the program and has little calculation amount."
"based on these problems, in this paper, different methods are used to solve the numerical solution of the fractional-order unified chaotic system, and the phase space diagram of the corresponding order is obtained. then, the circuit simulation diagram of the chaotic system is designed. finally, three control laws are obtained by the active control method, and the control and synchronization of the fractional-order unified chaotic system are completed."
"in terms of accuracy, we note that replacing h h l h l /m with its large system limit, q l, will be most effective if l and κ are not large. we know that κ primarily controls the interference power, so smaller κ will reduce the off-diagonals in h h l h l /m and help convergence to the diagonal q l . also, smaller l reduces the number of off-diagonal terms giving a smaller aggregate deviation from q l . convergence depends on large m which further justifies the desirability of small κ for increasing approximation accuracy. this argument also applies to the other approximation steps in proposition 1 and is numerically evaluated in sec. vii."
"to analyse the rate of convergence of analytical results to the true expectation κ, we define ula, hura and vura as the error resulting from the truncation of the infinite sums in (33), (37) and (40), respectively. in order to bound these error terms we require the following well-known results"
"rdf2vec also includes edge labels into the walks and the embedding procedure. we also noticed a positive effect including the edge labels whenever they are traversed by algorithm 2 global rdf vector space embedding paint with a weight equal to the amount of paint. because the summation and additions of the label weights might lead to a skew in the values, we normalize each bcv in the co-occurence matrix by removing the value on the diagonal and scaling the remaining values such that their sum is 1. this operation led to improvements in the results and hence we adopted this technique for the overall algorithm. the pseudo code of the global rdf vector space embedding algorithm can be found in algorithm 2."
"software testing produces information that is non-trivial to manage and communicate. the flow of information in software testing is built up of a number of feedback loops with an origin in software development activities. geographical, social, cultural, and temporal distances can have an impact on the loops (see section iii-b). six key factors at the organizations developing embedded systems affect the flow of information in software testing: how they conduct testing and trouble shooting, communication, processes, technology, artifacts, as well as how it is organized (see section iii-c)."
based on the ethical guidelines proposed by the swedish research council and the centre of research ethics and bioethics at uppsala university [cit] we took several steps to ensure we fulfilled ethical responsibilities as researchers.
"one organization required code reviews prior to the start of testing, but colleagues were not always available for reviews and therefore the start of testing could be postponed. when it comes to the development of safety critical software, participants judged that testing is cumbersome to perform for each code change in isolation -this implies that testing is postponed until several major changes have been implemented. another participant mentioned that a possible misconception in their organization is that testing can only be performed when the target hardware is available which is a mindset that delays the flow of information and feedback cycles for a certain time."
"for a qualitative study as ours, internal validity translates to how credible the results and the analysis are. there was a non-random selection of the interviewees -so one could think of selection bias as a validity threat. however, since our focus is on embedded systems development, we had to select relevant companies and random selection was not an option. we were also careful in selecting a variety of roles to interview to reduce the threat of selection bias. this study is performed in the context of embedded software organizations in sweden. our general perception is that the overall communication style in swedish embedded software organizations is informal and this might have impacted our analysis. thus our results should be interpreted with keeping the context in mind. finally, as with the majority of interview studies, the credibility of our results would have improved by conducting a much larger set of interviews, owing to availability of time and availability of industrial participants."
"while rdf data is graph shaped by nature, most traditional data mining and machine learning software expect data to be in propositional form. hence, to be used in machine learning and data mining pipelines, rdf data needs to be transformed to propositional feature vectors."
"finally, we conclude with an open problem for the random graph. recall that g(n, p) denotes the erdős-rényi random graph on n vertices with probability p."
"a potential advantage of mupps is to further discretize the shape manifold to increase the number of searching paths such as these between two existing paths t i and t j . furthermore, we used a mask on the whole heart for the path ranking. it may be more accurate to rank the path based on the similarity within certain local region such as for the myocardium. the segmentation of the whole heart can then be considered as several segmentations of a number of local regions and different rankings can be used for different regions [cit] . finally, the results of mupps contain a set of deformation fields which are the transformations between the atlas and unseen image plus certain random errors. by averaging these fields, we should obtain a better estimation of the registration ground truth for the applications where the spatial mapping is of the interest. we will address these in future work."
"in particular, we designed a process for anonymizing and keeping the obtained data safe, keeping strict restrictions in space and time on the raw interview files, and anonymized the transcripts such that aggregated claims could not be traced back to individual organizations or interviewees. further, we ensured that informed consent was obtained for all participants."
"we can then define, for each r-crh k, an r-linear form which we can also call g k (p). it is easy to prove, for a hereditary property h of r-graphs, that there is a family f (h ) of r-crhs such that"
"in the case if a non-inertial nonlinear element     f u t, can be represented by a taylor series or a polynom, the macromodel in the form of a structural diagram (fig. 1) is represented by volterra serries (1) [cit] . computational experiments. let's consider the case when the linear part is determined by the semi-integral link, and the nonlinear part has the form:"
"to support the thematic analysis we implemented python scripts performing raw analyses on the spreadsheet (between step (4) and (5) in figure 1 ). the script summarized demographic information, ranked theme and subtheme combinations, extracted interview fragments from all themes and subtheme combinations, and summarized challenges and approaches. [cit] pages. example of how a scripted extraction of interview snippets by iterating over these reports and generating ideas for challenges, a candidate list of twelve challenges was initially generated. this list was reviewed along with reading the raw data. iterating over this list during a workshop, we arrived at a final list of seven main challenges. using a similar approach, we arrived at five perceived good approaches from the interviewees. all this resulted in the final report."
the aim of the work is to develop an effective method for solving inverse problems of dynamics of nonlinear objects with distributed parameters by solving volterra polynomial integral equations of the first kind.
"in the faster cases, testing typically involves some form of automation that enables test execution without human involvement. alternatively, testing may be supported by a nightly build solution that compiles the new software, deploys it to the test system, and allows the test team to start testing as soon as they come into the office at the start of business on the work days. one of the studied organizations did automated system level testing where code changes were automatically tested nightly. here the role of the test team was more focused on developing and maintaining the test framework and the test environment."
"besides these statistics, we also use pagerank [cit] computed for the entities in the knowledge graph [cit] . this pagerank is computed based on links between the wikipedia articles representing the respective entities. when using the pagerank computed for dbpedia, not each node has a value assigned, as only entities which have a corresponding wikipedia page are accounted for in the pagerank computation. examples of nodes which do not have a pagerank include dbpedia types or categories, like http://dbpedia.org/ontology/place and http://dbpedia.org/ resource/category:central_europe. therefore, we assigned a fixed page-rank to all nodes which are not entities. we chose a value of 0.2, which is roughly the median pagerank in the non-normalized page rank values we used."
"there are well established practices in industrial software engineering and software testing, described by e.g. sommerville's book on software engineering [cit] and istqb syllabi [cit] . much of what we have found overlaps with or builds upon already well established practices. however, in addition to the already mentioned key findings, we have also discovered that: (i) much of the test results where tests pass are never used. (ii) processes such as reviews may slow down the feedback loops. (iii) the test results may be pushed back to developers, or pulled back by developers, or both. (iv) a test report written by a human is not always seen as an important document. (v) testers may spend a significant amount of time investigating a failing test before filing an issue out of fear of reporting a false positive."
"considering the staffing aspects, our findings indicate that under-staffing, staff circulation and scarce resources assigned for testing are heavily impacting the flow of information in software testing. team composition with low-ability profiles and knowledge levels was considered an important negative aspect. in addition, it seems that some organizations are not having teams with enough testers while others are facing the opposite organizational side of having more testers than developers. some respondents mentioned that many testers do not possess any meaningful education in software testing, and that in some cases developers are working as testers without much test-related knowledge."
"all five organizations develop embedded systems. three of the organizations work in the domain of safety-critical vehicular systems, one of which do this as a consulting service. the remaining two produce communication equipment. the organizations vary greatly in size and geographic distribution. the largest has more than 50 000 employees over many sites in and outside of sweden. two have between 10 000 and 50 000 employees over a few sites in sweden, europe, and also in asia. one has between 100 and 500 employees in a few offices in the same building as well as a second office about an hour by car away from the first. the smallest has between 1 and 5 employees and does consulting at a few different sites. all organization's products and services target an international market. the vehicle organizations develop software according to the v-model, and at least parts of their product development adheres to safety standards. the communication equipment organizations are agile, one follows scrum, the other kanban. both communication organizations have invested heavily in test automation. one of the vehicle companies is undergoing a transition to test automation and has started doing nightly builds and some of their projects have partial automated testing. the second vehicle organization has automation for some test levels, in some projects. the final vehicle organization has very mixed usage of automation. the communication organizations make extensive use of floss (free, libre and open source software) tools for source code management, automatic builds, continuous integration, collaborative code reviews and so on."
"some organizations have implemented test results exploration solutions, but extending or maintaining these is not a priority. these solutions are considered immature and lack important features such as filtering. thus, a number of participants saw shortcomings related to poor feedback intertwined with the lack of traceability between these tools and the use of several different views and perspectives for searching feedback information. volume 7, 2019"
"in the graph case, it is trivial to derive ed m (1) using symmetrization. in addition, the classification of p-core crgs established by marchant and thomason [cit] allow for a trivial proof of the asymptotic erdős-stone-simonovits result [cit] ."
question 5 are there hereditary properties of graphs for which the edit distance function cannot be determined from the g functions of a finite number of crgs?
"software testing is typically divided in levels, where pieces of source code at a low level are tested using unit tests, with the purpose of testing one part of the code in isolation. when units are being tested together we typically talk about integration testing, with the purpose of verifying for example interfaces between units. a next step is system testing, where the whole system is being tested. here the focus of the testing is to verify that the system as a whole can accomplish its task. during acceptance testing customers or external tester are often involved to validate the software with respect to user needs, typically in a customer setting. when going up in test level more hardware is needed, and greater feedback loops are required. in figure 2 this is illustrated with feedback loops of increasing size when going to the right of the diagram."
"we first ask about powers of cycles and the questions left open in section 5.4. other metrics on the space of graphs are of interest, as we discussed in section 6."
"propagation registration is used to register t i (a) in mupps or mr i in maps to the unseen image to achieve the segmentation. this process includes three steps [cit] : the global affine registration, larm using the local regions defined by the labels in the segmentation, and the ffd registration."
"as indicated in figure 2, test leads, project managers and requirements engineers are important for the software testing process. their interactions are diverse and occur with many roles in an organization through many contact interfaces. in other cases, customers or other external parties perform testing and have access to the full hardware system, but their test result feedback can be very slow."
"-volterra kernel, makes it possible to simplify nonlinear mathematical models by converting them to a quasi-linear type [cit] . it also should be noted that the applying of volterra series will allow to solve the problems of modeling dynamic objects with lumped and distributed parameters based on the same means, because for these objects, mathematical models will differ only in the form of kernels [cit] ."
"interviews were conducted face-to-face, and recorded with an off-line digital voice recorder. the interviewees were given a lot of freedom to express their thoughts and explain topics not covered by the instrument. the majority of the interviews were conducted by two researchers where one was the ''driver'' and the other researchers made clarifying questions, kept track of time and made sure no question was forgotten. we got a total of about 17 hours of audio material, (2) in figure 1 . the audio material was not stored on servers or emailed."
"consequently, to build a glove model a word-word co-occurrence matrix is first built, which contains for each word how often other words occur in its context. model parameters then include the size of the context window, whether to distinguish left context from right context, as well as a weighting functions to weight the contribution of each word co-occurrence -e.g., a decreasing weighting function, where word pairs that are d words apart contribute 1/d to the total co-occurrence count."
"research with good construct validity investigates the phenomenon that the researchers intended to study -a misunderstood question is an example of a threat to construct validity. the design of our instrument was based on research questions, and was thoroughly reviewed before use (by running pilot interviews and evaluating the instrument in workshops). we designed a process for managing and anonymizing the interview audio files and transcripts, and by presenting it to the interviewees we reduced the threat of evaluation apprehension, and reactivity, where interviewees alter their behavior in the presence of others. information flow in software testing is an area of research that intertwines with other processes of software development, therefore, we had to ask interviewees a large number of questions, not necessarily focused on communication of test results and the information flow per se. this resulted in a large amount of data to analyze for understanding the full spectrum of software testing and sometimes it was difficult (in our analysis) to primarily focus on the flow."
"this simplified version of pagerank can run into a number of problems, namely some pages may have a zero out degree (so called dangling nodes) and there could be groups of pages which form closed cycles. in the first case, pagerank (i.e., random walkers) will get lost from the graph and any node linking directly or indirectly to a zero out-degree node will get a pagerank of zero. in the second case walkers will get trapped and the pages in the cycles will accumulate all pagerank. to amend these problems, the above equation is adapted to include parts which ensure that when a walk ends up in a dangling node, it will continue from another node selected from a distribution v, called the teleportation distribution. further, to avoid ending in a cycle, a random jump is also performed with probability α to a node selected from the same distribution. usually, v is chosen to be a uniform distribution, making each node equally likely to be the target of the jump. however, in the case of personalized page rank the distribution is degenerate as the target of these random jumps is always the node for which the rank vector is computed (which we called the focus node). in effect, the personalized pagerank vector indicated the importance of nodes from the perspective of the focus node."
"predicate frequency for each predicate in the dataset, we count the number of times the predicate occurs (only occurrences as a predicate are counted). object frequency for each resource in the dataset, we count the number of times it occurs as the object of a triple. predicate-object frequency for each pair of a predicate and an object in the dataset, we count the number of times there is a statement with this predicate and object."
"in maps, good quality images, such as mr data with high signal-to-noise ratios and minimal artifacts, are selected as atlases for propagation to maximize the registration accuracy [cit] . however, clinical cardiac mr images often contain strong artifacts, in particular the high resolution volumetric data, due to the effects from complex heart motions and long acquisition. therefore, the registration of these images may have large errors or even fail. furthermore, constructing an atlas with good image quality may need a large amount of training data and may be practically expensive in terms of manual image processing and data acquisition. therefore, in this work we propose to build a good quality atlas from limited quality images and use the multiple path propagation and segmentation (mupps) strategy to achieve a result of multiple classifiers."
"all organizations that were part of this study perform some form of integration-or system-level testing in a dedicated test environment (at (5) in figure 2 ), typically in one or several test labs. the test environment can be built up in racks, or on wagons or walls in dedicated rooms. peripheral hardware to support testing in these labs include voltage generators, signal generators, vehicle displays, or other subsystems. in addition to undertaking manual or automated testing in these labs, dedicated test teams (4) are sometimes responsible for the test environment as well as for the test automation framework."
"most of the studied companies use different types of coverage information, typically based on different artifacts: when exploring test results, traceability between artifacts allows a developer to easily obtain more information on the result of a failed test by exploring other types of artifacts, such as revision numbers, log files, test case description, requirements, issues in an issue tracker, details on the test system used for executing the test, and the test case source code. one developer, emphasized the importance of traceability in the following way: ''i think it's good that we can connect our issues to the requirements [but] we cannot [connect] our issues to the source code in a good way.'' by reporting and test reporting aspects we both mean making an actual report, and reporting in general. for safety critical development a formal test report in accordance with the relevant safety standard was very important, other organizations saw little value in such test reports. a test results database is a good starting point for a report, and our interviewees argued that an automated summary would be excellent, and that most parts of a report should be automated. but in reality this is not the case. typically a test leader writes a report, and rather few people find value in the test report. in addition: test reports age fast, act as a slow feedback loop, and instead talking face-to-face or using issues were two central ways to communicate around test results."
"the thematic analysis of the interview transcripts identified six themes. the themes and their most important subthemes are summarized in table 1 . many subthemes are important to more than one theme, such as collaboration between roles that is important to both communication and organization. the themes are: 1) testing and troubleshooting: a prerequisite for using and communicating information about test results is testing, and this theme covers how the organizations conduct testing. 2) communication: the flow of information in software testing is directly related to communication, and an important theme in our interviews was of course the ways in which the interviewees communicated with different stakeholders. 3) processes: the theme of processes has an important impact on the flow of information: different development models prescribe different communication patterns, and enforced reviews turn out to be possible bottlenecks for the information flow."
"in this paper, we propose a new multi-classifier method, the multiple path propagation and segmentation (mupps), for the whole heart segmentation of cardiac mri. we showed that mupps achieved better mean dice scores than the standard maps, 0.895 vs 0.836 before the fusion and 0.911 vs 0.905 after the fusion. to minimize the number of propagation paths in mupps for a segmentation case, we studied three path ranking schemes. the best scheme was to rank the path using the image similarity of the transformed atlas and unseen image after an affine registration. the segmentation framework of mupps achieved 0.911 ± 0.016 mean dice score, a 0.015 gain of accuracy, using eleven highest ranked paths and the fusion techniques. in particular for the segmentation of myocardium, a gain of 0.03 was reported. the whole heart segmentation error of rms surface distance was 1.13±0.248 (mm). note that the shape manifold in mupps is different from the linear space formed from the principal components in the statistical shape model [cit] . also, it only needs about 10 images to construct a with good quality [cit] . hence, for a new image mr n+1, only one off-line registration is needed to compute the new path t n+1 and no reconstruction of a is required."
"glove was designed for creating dense word vectors (also known as word embeddings) from natural language texts, which have been recently used with much success in a plethora of natural language processing tasks. glove follows a distributional semantic view of word meaning in context, which basically relies on the assumption that 'words which are similar in meaning occur in similar contexts' [cit] -i.e., meaning can be derived from the context (i.e., the surrounding words) of the word in a large corpus of text."
"software is created, enhanced or corrected by a team of software developers (shown as (1) in figure 2) . modified software is often tested locally first at the developers desk (2) -either by running unit level or integration level tests, often without target devices, or only a part of a device. the overall information flow diagram for a typical software testing process, as described by our interviewees. the development team (1) produces new software, or software with changes. this is tested in a short and fast loop locally at the developers desk (2), or using some special hardware in a slower feedback loop (3) . if the software is tested in a test environment (5) then there might be gates (6) slowing this process down and the need for a dedicated test team (4) . testing produces test results in a test results database (7) . sometimes the test results are pushed (8) back to the developers, sometimes they need to pull to get access to it (again: 8), and oftentimes the test results is never used (''void''). peripheral, but important, roles are requirements engineers (req.), project managers (pm), and test leads (tl). this is a fast feedback loop, initiated by the developer, when the developer wants to run it, and with the developer as the recipient of test results."
"there are seven main challenges for the flow of information in software testing: comprehending the objectives and details of testing, root cause identification, poor feedback, postponed testing, poor artifacts and traceability, poor tools and test infrastructure, and distances (see section iii-d). finally, we identified five good approaches for enhancing the flow of information in software testing: close collaboration between roles, fast feedback, custom test report automation, test results visualization, and the use of suitable tools and frameworks (see section iii-e)."
"recently, vector space embeddings have been proposed as a means to create lowdimensional feature vector representations of nodes in an rdf graphs. inspired by techniques from nlp, such as word2vec [cit], they train neural networks for automatically learning the mapping of rdf nodes to feature vectors. vector space embeddings have been shown to outperform traditional methods for creating propositional feature vectors from rdf [cit], e.g., in tasks like content-based recommender systems [cit] ."
"the nodes in ascending in-degree add all nodes to q indeg repeat while g has a node n with out-degree 0 do add n to order, remove n from g, remove n from q indeg end while if g is not empty then"
"we have conducted an interview study of information flow in software testing of embedded systems in five organizations in sweden. the results presented in this paper are based on semi-structured interviews with twelve practitioners with an average work experience of more than fourteen years, and thematic analysis to identify major themes, challenges and good approaches. based on this analysis we describe the flow of information, and show how this communication process is built up of a number of feedback loops originating in software development activities, typically connecting test activities, test artifacts, test results databases and issue trackers with teams of developers, testers and managers (as described in figure 2 ). these loops are negatively affected by increased geographical, social, cultural or temporal distances. we discovered themes that impact this flow: how organizations conduct testing and trouble shooting, communication, processes, technology, artifacts, and the organization of the company (summarized in table 1 )."
"calculating entity similarity lies at the heart of knowledge-rich approaches to computing semantic similarity, a fundamental task in natural language processing and information retrieval [cit] . as previously mentioned, in the feature embedding space semantically similar entities appear close to each other in the feature space. therefore, the problem of calculating the similarity between two instances is a matter of calculating the distance between two instances in the given feature space. to do so, we use the standard cosine similarity measure, which is applied on the vectors of the entities."
"the research showed that deep learning achieved promising results for segmentation and classification of skin lesions. however, there is a huge difference in availability of ground truths with respect to segmentation and classification in public skin lesion datasets. this is mainly because, the expert annotation for segmentation ground truths is very expensive and laborious when compared with classification labels. [cit] competition, only a total number of 3694 images were available for the segmentation challenge in comparison to 11720 images in the classification challenge [cit] . the expert annotations used in segmentation tasks are not always very accurate which could affect the performance of segmentation algorithms. [cit] testing set are shown in fig. 2 . other issues regarding the expert annotation are, some experts tend to draw a very precise outer boundary of skin lesions while others draw loose outer boundary to represent skin lesions for segmentation ground truths as shown in fig. 3 . depending on whether the expert annotations precise or loose for the skin lesion dataset, it is very difficult for any deep learning algorithm to produce accurate segmentation results. for precise boundary representation of skin lesions, the algorithm needs to have high specificity score whereas for loose representation, high sensitivity score is desirable. in this paper, we addressed these issues by developing three fully automatic cnn-based ensemble methods to suit both precise and loose lesion boundary segmentation. [cit] testing set and another publicly available dataset, the ph2 dataset."
"given the importance of context in empirical studies [cit], we start this section with an overview of the organizations and interviewees. the central parts of this section covers the results of the thematic analysis, and thus the answers to our research questions."
"often a test lead will write a test report and make a high level investigation of all produced results. in those cases where safety-critical software was being developed, a formal test report was always written. for the interviewee in the 'other' category that works with functional safety, the test report was tremendously important and these reports had to strictly follow the domain specific safety standard. in more agile contexts, the role of the test report was questioned. in particular, when a test report was being produced the information in it was typically late, in the sense that it was not useful internally as the results had already reached the developers through other channels. however, several of the interviewees mentioned that the test report could be important for customers."
"because of timing issues and other non-functional aspects, the software testing of embedded systems has to be conducted on target hardware at some point in the development process [cit] . this will have a significant impact on unit testing, if done on target, as software uploads will be a limiting factor. [cit] identified strategies for combining unit level testing on both target and host, as well as proposed new strategies for test-driven development of embedded systems, in particular they discuss hardware mocking. in addition to testing on host and on target, the use of simulators and emulators are common and allow higher level testing without access to hardware. [cit] developed xemu which extends the common qemu hardware simulator [cit] for mutation based testing of embedded systems. the importance of simulation was also emphasized by web-based survey with more than 1200 respondents as the most important design technique for future designs of embedded systems [cit] . with respect to the flow of information in software testing, we could not only expect faster feedback loops with increased simulation, testing would also be able to start earlier in the development process. an earlier start of the testing could reduce the impact of the gate at step (6) in figure 2 . there is, however, a cost involved in developing and maintaining such tools."
"overview of the method followed in this interview study: (1) preparing and validating the instrument, (2) conducting interviews, (3) transcribing interviews, and (4) qualitative analysis with thematic analysis that includes scripting of raw reports which were (5) interpreted when reporting the results."
"the most frequently discussed subtheme in this entire study was collaboration between roles. notably, walking over to a colleague to discuss test results is a standard approach in all organizations. this face-to-face communication occurs between most roles. understanding issues is not always easy, and several interviewees emphasized that it is very important that they are written in such a way that they are understood. collaboration seems to help locating issues faster, and is thus of great help during root cause analysis. there is often an informal communication between roles before filing an issue. according to one project manager, a tester finding a problem should first talk to a developer to get the details needed, and then file an issue. this way the developer will understand the issue once it reaches him or her. a consequence is also that distance is an important factor."
"both communication equipment organizations utilized web-based portals to allow interactive visual exploration of test results. these could display different aspects of testing, provide links to log files, and produce plots to reveal patterns in the test outcomes. in the test results exploration portals, the ability to filter results was deemed important, as well as being able to look at several log files at the same time in some way, and also to show coverage and coverage gaps. however, a test results exploration portal requires time and effort for development and maintenance. a prerequisite for visualizing trends is a test results database (also discussed in the theme on artifacts)."
"to reduce surface reflection [cit] . in common use for the last 20 years, dermoscopy has improved the diagnosis rate over visual inspection alone [cit] . the abcd criteria help non-dermatologists screen skin lesions to differentiate common benign melanocytic naevi (naevi) from melanoma [cit] . fig. 1 illustrates the abcd rules for skin lesion diagnosis, where: 1) a: asymmetry property checks whether two halves of the skin lesion match or not in terms of colour, shape, edges. the skin lesions are divided into two halves based on long axis and short axis as shown in fig. 1 ."
"all vehicle organizations used requirements management tools to store requirements, test cases and the traceability between them. these tools could also store test results in the form of test records -typically an entry in a database saying pass or fail, with a trace to the version of the software being used. from this tool reports could be exported for humans to read. the communication equipment organizations instead used web-based portals where test results could be explored. one of them used plots of trends to visualize aspects of the testing -such as trends over time, or over test system. the other used a portal with many tabs to present results. both used links to rapidly provide log files from the test framework to give developers the details they needed. further, one of the vehicle organizations had the habit of gathering the team around visualizations exported from a requirements management tool. however, it should be noted that most of the data generated during testing is never used or looked at, in particular when tests pass. one reason for this is the rapid pace at which new, replacing, test results are generated."
"for research, the findings in this paper are important as they bring ''tacit knowledge'' from industrial practice into academia. by knowing that the information flow in software testing is not a trivial process, further research is made possible. we also provide a model for what we know about the information flow inner-workings in organizations developing real-world embedded software. we invite other researchers to add on, revisit in other contexts, and possibly revise or reject, this model of the overall flow of test information."
"whenever testing is performed, it inherently produces test results (step (7) in figure 2 ). in case of a failure, it is common for testers to have a face-to-face dialogue with the developers. this dialogue may lead to the failure being understood and resolved directly, or to the issue being filed in an issue tracker. some organizations said that they both had a faceto-face conversation and filed an issue. one tester said that a considerable amount of effort (i.e., several hours of work) was sometimes needed before filing the issue -this was done out of fear of filing a false negative issue (i.e., issues that are not related to the software under test)."
"in order for the testing to commence, the software modified by the development team (1) must reach the test environment (5). the time required for this transition (6) varies considerably among our studied organizations. the slower cases typically involve checkpoints in the testing or development process. one organization did not allow code to reach the main repository, hence blocking it from testing, until the code volume 7, 2019 change had been reviewed -a process that could take days depending on availability of colleagues. for safety critical software, or when manual testing is done, it is common to bundle many code changes into one test session -because the test sessions require much resources such as manual test effort. this means that testing of a code change will be put on hold until a sufficient number of other code changes have been done. in such cases, the feedback loop can be delayed for months."
"in a complex test environment the root cause of a failure may be located in the software under test, the test framework, hardware components in the test environment, or be caused by shortcomings in the test case. this challenge is related to issues the participants are facing when understanding faults that could also originate from poor or untestable requirements or from problems in the simulators used. many interviewees mention that this is a challenge, and some talk about strategies on how to pinpoint the location of an issue (e.g. use different exceptions for different root causes), and how hard it is to perform such actions. a project manager mentioned that this problem is worst in the early stages of software development. one of the developers we interviewed expressed that this challenge is primarily an issue for testers: ''you make these kind of checks. an experienced tester, have already done that before coming to me as an implementer.'' . equally important, too much information in test reporting can also be a problem, in particular when the information is unstructured or when it cannot be furthered filtered in an overview. none of the interviewees discussed strategies on how to actually perform logging, and it seems that more logging is implemented when needed, in an ad-hoc manner."
"2) ph2 dataset ph2 dataset has 200 images in which 160 images are naevus (atypical naevus and common naevus), and 40 images are of melanoma [cit] . in this dataset, the ground truths represent the precise and true boundaries of skin lesion (high specificity), as shown in the fig. 5 . [cit] segmentation training set."
"in the case when the linear part is determined by the semi-inertial link, the macro-model in the form of the integro-degree volterra series has the form:"
"the default pagerank and bca algorithm assume that a random walker will follow the out edges of a node with equal likelihood. however, one can also create a setup in which given out edges are more likely than others. for bca, this possibility was already hinted in the original paper [cit], but not elaborated much further. this so called biasing can be accomplished by taking into account the out edge weights when distributing the paint over them."
"in table 2 and 3, we present the performance of our proposed method with other trained fully convolutional networks. we trained fcns, deeplabv3+, [cit] testing set. since ensemble-s method compares and picks the smaller area, it performed best in the specificity category with a score of 97.94% by outperforming other algorithms overall and also for each lesion type in table 2 . hence, ensemble-s achieved the best position to perform well for evaluation metric specificity as per our claim. for sensitivity, ensemble-a and ensemble-l performed the best among other algorithms with a score of 89.93% and 88.70%. hence, we proved our claim of designing these algorithms for high sensitivity. ensemble-a is marginally better in the accuracy than other methods. particularly, in naevus category, deeplabv3+ performed better than ensemble-a with a fine margin."
"in this section we describe the testing process from the perspective of the overall information flow diagram described in figure 2 . this is based on a synthesis of the information provided by the interviewees and represents an abstract and inclusive view of the testing processes used within the studied organizations. in particular, if a test-related information flow has been expressed by any of the interviewees, it is captured in some form in the diagram. this does not necessarily mean that any particular information flow in the diagram is present in all studied organizations. numbers within parenthesis refer to process steps indicated in the diagram."
"some of the mitigations to the challenges described are already known to industry. [cit] . for example, it covers a generic test automation architecture, as well as the potential risks when implementing and using test automation. we would like to highlight the chapter on reporting and metrics as these are topics that occurred frequently in our thematic analysis and results. this chapter recommends to: (i) measure benefits of automation (costs are easily seen), (ii) visualize results, (iii) create and store logs from both the system under test and the test framework, and (iv) generate reports after test sessions. this is an indication that this syllabus could be relevant to the information flow in software testing, both for an academic and an industrial perspective."
"we used two types of ensemble methods called ensemble-add and ensemble-comparison. first of all, if there is no prediction from deeplabv3+, the ensemble methods pick up the prediction of mask r-cnn and vice versa. then, ensemble-add combines the results of both mask r-cnn and deeplabv3+ to produce the final segmentation mask. ensemble-comparison-large picks the larger segmented area by comparing the number of pixels in the output of both methods. on contrary, ensemble-comparison-small picks the smaller area from the output. the ensemble methods are illustrated in fig. 11 where (a) shows ensemble-add; (b) shows ensemble-comparison-large; and (c) represents ensemble-comparison-small. for convenience, we used the abbreviation as ensemble-a for ensemble-add, ensemble-l for ensemble-comparison-large, and ensemble-s for ensemble-comparison-small. ideally, ensemble-s is designed for performing well in specificity i.e. to have precise segmentation masks whereas ensemble-a and ensemble-l are designed for high sensitivity."
"information flow can be defined by a distributed system made up of agents and the behavioral and structural rules and relationships between them. information flow is important when a synergy between humans and software systems is required for a work flow [cit] . in the daily work of software developers and testers, there is a need to make many small decisions on what to implement, correct or test next, but also decisions on a project level, e.g. if the software has been tested enough, has a good quality, or can be released anytime soon. oftentimes, test results are needed to make these decisions, and this information can be seen as flowing between humans and/or software systems."
"mask r-cnn is a recent deep learning architecture to provide instance segmentation i.e. identifying object outlines at the pixel level [cit] . mask r-cnn is inspired by the faster r-cnn for object detection by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition [cit] . the framework is mask r-cnn is detailed in fig. 9 . we fine-tuned a pre-trained mask r-cnn with resnet-inceptionv2 model (henceforth mask r-cnn) on ms-coco dataset for a single class as skin lesion for this experiment [cit] . in some cases, mask r-cnn generated more than one output due to the generation of 2k proposals from the region proposal network (rpn) in the initial stage of mask r-cnn. to limit the single output mask of highest confidence per image, we set the value of top n proposals to 1 from rpn at test time."
"visualization of test results may serve as an important instrument in understanding the test execution by using different feature and characteristic trends over time as well as a general performance plot in form of a dashboard. one participant mentioned that even the use of colors for visualizing the results of testing can show where the errors manifest and how acute the problems are by representing the severity and the importance of the information using gradients of representative colors. for example, red is used for fail, yellow for inconclusive results. another participant encouraged the use of an overview of the results that is used to examine in-depth each level of information needed to discover the location of a fault. these visualizations were thought as a good basis for fostering the communication around diagrams and visual information instead of using textual results which can be difficult to understand."
"as with any metric, we may take a property of graphs h (that is, a set of graphs), and compute the distance of a graph from that property:"
institute for mathematics and its applications (ima). this paper was written while the author was a long-term visitor at the ima and would not have been possible without their resources and generous support.
"we have essentially two types of metrics, those assigned to nodes, and those assigned to edges. the predicate frequency and predicate-object frequency, as well as the inverses of these, can be directly used as weights for edges. therefore, we call these weighting methods edge-centric. in the case of predicate frequency each predicate edge with that label is assigned the weight in question. in the case of predicate-object frequency, each predicate edge which ends in a given object gets assigned the predicate-object frequency. we also use the inverse metrics, where not the absolute frequency is assigned, but its multiplicative inverse."
"the macro model in the form of the integro-degree volterra series, providing that the linear part is determined by the link of delay, has the form:"
"the participants stated that for strengthening the flow of information, a better use of tools and frameworks is needed to improve the continuous exchange of information between different tools and people throughout the development process. one participant mentioned that the use of code collaboration tools for reviews and static analysis could help improve the flow of information, while another participant said that tools and artifacts should be linked and test results must be showed inside the ide, for allowing developers to test in their own sandbox. in addition, recording signal values, and re-running test cases can help in enhancing the flow of information when the sut is located in another location than the main team(s)."
"this section discusses the publicly available skin lesion datasets, the preparation of the ground truth, the proposed ensemble methods, and the performance measures to validate our results."
"among the studied organizations, some use simulators that could be used at a developers desk, allowing up to 90% of the system-level tests cases to be executed locally. one organization has test systems built up of network topologies at the desk of most developers, thus allowing them to run sophisticated test cases locally."
axenovich and martin have investigated natural generalizations of the edit distance problem. the paper [cit] addressed editing matrices (section 7.1 below). the paper [cit] addressed both editing the edges of multicolorings of a complete graph (section 7.2) and editing the edges of a directed graph (section 7.3).
"for objects with distributed parameters the following basic typical irrational and transcendental links can be distinguished, they are defined by the operator     a x t : semi-integral, semi-inertial, delaying and damping (or semi-delaying) [cit] . in"
"testing and troubleshooting covers aspects of performing testing, regardless of level or degree of automation. we also consider the environment in which the testing is performed, as well as troubleshooting. this was the most important theme in terms of how frequently it appeared in the interviews."
"the isic challenge dataset comprised of dermoscopic skin lesion images taken by different dermatoscopes and camera devices all over the world. hence, it is important to perform pre-processing for colour normalization and illumination with colour constancy algorithm [cit] . we processed the datasets with shades of gray algorithm [cit] as shown in fig. 7 ."
"for theorem 31(d), the constructions are due to füredi [cit] to address a related zarankiewicz problem. if q is a prime power such that t − 1 divides q − 1, then there exists a graph on 2(q 2 − 1)/(t − 1) vertices that is q-regular with no copy of k 2,t and no triangle. this is enough to ensure that k 2,t does not map to the corresponding crg. by (5), this construction gives"
"we trained all the networks on a gpu machine with the following specification: (1) hardware: cpu -intel i7-6700 @ 4.00ghz, gpu -nvidia titan x 12gb, ram -32gb ddr5 (2) software: tensor-flow."
conclusions. the computational experiments have shown that the developed regularization method for solving volterra polynomial integral equations of the first kind on the basis of on the introduction of a differential regularization operator allows obtaining high accuracy in regenerating of signals at the input of nonlinear dynamic objects with distributed parameters in the presence of noise interference.
"proof. by symmetry, it is sufficient to prove (a). for (a)(i) we observe that, by theorem 9(a), all edges are white and it is easy to see that the optimum weight vector in equation (6) is constant. thus, all vertices have the same weight and the result follows."
"software testing includes not only the creation, execution and evaluation of test cases, but also the process of communicating on test cases and their results between human beings, or between humans and machines. this includes faceto-face discussions, writing standards-compliant test reports, reading system logs, as well as making decisions based on test results visualizations. there is a broad industrial interest"
"test execution in the studied organizations encompasses manual testing as well as automated testing. all studied organizations do some form of manual testing. there seems to be three main approaches for this; the most traditional approach is typically done by a tester in a test lab by following a step-by-step test specification, while pushing buttons or setting signals using control panels and interfaces. organizations that perform testing like this typically store the results as test records in a protocol, a database, or both. the second approach is risk-based exploratory testing, where a risk analysis has been completed to identify the test scope, and manual testing is performed to cover different risks. at one organization this type of testing was conducted, but not really sanctioned. the third approach is manual testing where experience guides the testing, but there is no formal planning, no test specification, and sometimes no artifact produced by testing."
"where f (x ij ) is a weighting function on co-occurrence counts of word j in the context of word i (x ij ), w i are word vectors,w j context vectors and b i and b j biases. the intuition behind this cost function is the following one. each summand of the summation represents the amount of error attributed to a count x ij in the co-occurrence matrix. the error consists of a weighing function f, to dampen the effect of very large co-occurrence counts, and a squared error factor. the squared error factor will become smaller when the dot product of word vectors becomes closer to the logarithm of the probability that the words co-occur. or turned the other way, when two words co-occur often, their vectors' dot product will be relatively high, meaning that the vectors are more similar to make the error factor smaller. the logarithm also causes that ratios of co-occurrence probabilities are associated with differences of vectors. as a result, the embedding contains information useful for determining analogies."
"so, even if we add unbiased random walks to the list of weighting strategies, we retain 12 unique ones, each with their own characteristics. these strategies, which we further elaborated upon in our earlier work [cit], are:"
"deeplabv3+ is one of the best performing semantic segmentation networks achieving the test set performance of 89.0% and 82.1% [cit] and cityscapes datasets respectively [cit] . deeplabv3+ is an encoder-decoder network which makes the use of cnn called xception-65 with atrous convolution layers to get the coarse score map and then, conditional random field is used to produce final output as shown in fig. 8 . to train deeplabv3+ [cit] and adjusted the final output of 21 classes to a single class for segmentation of skin lesion [cit] . it assigns semantic label lesion to every pixel in a dermoscopic image."
"in the case of melanoma, it is likely to have an asymmetrical appearance. 2) b: border property. it defines whether the edges of skin lesion are smooth, well-defined or otherwise. in the case of melanoma, edges are likely to be uneven, blurry and jagged. 3) c: colour property. the colour in a melanoma varies from one area to another, and it often has varying shades of tan, brown, red, and black. 4) d: diameter property. it measures the approximate diameter of the skin lesion. the diameter of a melanoma is generally greater than 6mm (the size of a pencil eraser). end-to-end computerised solutions that can produce accurate segmentation of skin lesions irrespective of types of skin lesions are highly desirable to mirror the clinical abcd rule. for segmentation of medical imaging, jaccard similarity index (jsi), specificity and sensitivity are deemed as important performance measures for methods. hence, computerised methods need to achieve high scores in these performance metrics."
"we interviewed twelve practitioners, three women and nine men, that we grouped as three developers, four testers, three managers and two 'others'. this division is not trivial as an interviewee was for example a developer specialist, while another was developer of an automated test framework. all of the interviewed women were testers. the interviewees have between 2 and 35 years of experience with an average of 14.4 years. eight had at least 10 years of work experience. typical activities for the developers were to write, analyze and implement requirements. they also did design, and one lead a group for process-improvements. they could also have an architect role and no longer write code. three of the testers were involved in work on test frameworks, either in a leading role (with responsibilities such as coaching and planning), or with implementation. testers mentioned that they participate in meetings with test leaders or projects, write test specifications, write test reports, perform manual testing and implement test automation focusing on the test framework part so that the test cases can use the framework. the project manager interviewees included a line manager, a project manager and a program manager. they were responsible for teams from 10 to more than 100 members. their activities were on following up on and leading work on a software release, making sure deadlines are reached, and some had responsibilities on making sure that people are being recruited, developed and are feeling well. of the others, the first had a high-level architect role that moved between development teams. the second was a specialist in functional safety working with safety assessment and requirements, safety management, and education. functional safety, architecture and requirements are not all in the domain of one given role in an organization, the responsibilities of the different roles had a considerable overlap, in particular with respect to requirements engineering that was the responsibility of several roles."
"we found that the team structure is an important factor impacting the flow of information in software testing. team design choices usually involve a separation between the development and the test teams. it seems that development teams are typically responsible for unit-level testing in addition to programming and software development. on the other hand, test teams can be categorized into three different types: (i) a test framework team responsible for developing and testing the software used in the test framework, (ii) a team dealing with test equipment such as the test environment, test systems and simulators, and (iii) a traditional test team in which testers are usually performing manual testing. we found that most organizations had a clear separation between the test framework team and the development team(s), and that this separation was clearer in the vehicular organizations. the interviewees involved in safety development had an even stronger focus on clearly separating testing and development roles."
"we used basic image processing methods, i.e. morphological operations to fill the region and remove unnecessary artefacts of the results as illustrated in fig. 10 . these issues were only countered by deeplabv3+ as in the case of mask r-cnn, we have not had these issues. hence, post-processing is only used for the semantic segmentation methods like fcns and deeplabv3+."
"an intuitive version of the bca algorithm is as follows (for full details, see [cit] ). to compute the ppr vector p (b) for a focus node b, we start by injecting a unit amount of paint, representing the walkers in the standard personal pagerank computation, to b. from this paint an α-portion is retained and added to the value for b in p (b) . the remaining (1 − α)-portion is distributed uniformly over the out-links. this retain-and-distribute process is then repeated recursively for all nodes which got paint injected. when a node has a zero out degree, the outgoing paint is discarded."
"unlike the first models for rdf vector space embeddings, which are based on paths, walks, or kernels, and therefore rely on local patterns, in this paper we present an approach in that exploits global patterns for creating vector space embeddings, inspired by the global vectors (glove) [cit] approach for learning vector space embeddings for words from a text corpus. we show that using the glove approach on the same data as the older rdf2vec approach does not improve the created embeddings. however, this approach is able to incorporate larger portions of the graph, without substantially increasing the computational time, leading to comparable results. the main contributions of this paper are this new embedding approach and an approach to approximate all-pairs personalized pagerank (ppr) computation, which is used to efficiently compute such embeddings."
"standard software engineering tools may facilitate the process of testing, e.g. (i) requirements and traceability management tools such as ibm doors [cit], (ii) bug/issue tracking tools such as mantis bug tracker [cit], and (iii) build and test results dashboard tools such as jenkins [cit] . however, the flow of information in software testing cannot be seen as a tooling issue alone. some studies have found that important challenges in software development are related to people and not tools [cit] . while studies presenting such problems and solutions exist in the wide scope of software engineering, it is unknown to what extent these tools are contributing to improving the information flow in practice."
the bound in corollary 18(a)(ii) can be approached by a crg on black vertices where the gray edges induce a blow-up of k t−1 .
"from our observations, test results are reaching developers (step (8) in figure 2 ) when information is pushed back. this happens by allocating an issue directly to a developer (i.e., using an automated system producing notifications) or when a tester is reaching out to the developer. another way for the test result to reach developers is for the information to be pulled by curiosity, or discipline, in the daily work of a developer. the pulled information is often extracted from an interactive environment (i.e., a portal or a requirement management system used for showing an overview of the information as well as visualizations). the result of a failed test could reach a developer weeks or months after the developer performed the code change that triggered the test execution."
"future work could investigate the information flow in contexts other than embedded software organizations, use a larger sample, if possible in additional countries, in order to strengthen our understanding, identify more challenges and helpful approaches. in a recent literature study of literature studies, garousi and mäntylä, [cit], found that there is a ''need for secondary studies in the areas of test-environment development and setup, test results evaluation and reporting.'' secondary studies on test reporting, as well as on communication within the context of software testing, could widen and extend the collective understanding of the flow of information in software testing. finally, we would like to see implementation of tools for improving the information flow in software testing: from introduction of visualizations, to how to make test results communication feedback loops faster, as well as more general exploration tools of all aspects of test results. perhaps simplifications when going from test verdict via root cause analysis and source code exploration to discussions with colleagues on possible fixes for issues. robert feldt is currently a professor of software engineering with the chalmers university of technology, gothenburg, where he is also a part of the software engineering division, department of computer science and engineering. he is also a part-time professor of software engineering with the blekinge institute of technology, karlskrona, sweden. he has broad research interests, but focuses on software testing, requirements engineering, psychological and social aspects, as well as agile development methods/practices. he was one of the pioneers in the search-based software engineering field. he has a general interest in applying artificial intelligence and machine learning and has tried to get more research focused on human aspects -behavioral software engineering. [cit], he has been the co-editor-in-chief of the empirical software engineering (emse) journal. volume 7, 2019"
"although it is difficult to compute the edit distance function for general hereditary properties, we can estimate the function through a variety of techniques. first, we can use the clique spectrum and (9) to construct an upper bound."
"computing pagerank (and also the ppr variant) is reasonably scalable. however, as we need to compute ppr for each individual node in turn, in order to build the cooccurrence matrix, the rapidly becomes too expensive. moreover, the pagerank algorithm assigns a value to all nodes in the graph. if we computed the co-occurrence matrix this way, we would end up with a very large (in our experiments below this would become around 500 tb) dense matrix with many small values, which have little to no impact on the later training. hence, we designed a faster, approximate all-pairs ppr computation method, which results in a sparse matrix. this algorithm is based on an approximate ppr method which we will introduce next."
"this basic algorithm can be improved by choosing the order in which nodes considered for the retain-and-distribute. it is more efficient to select nodes with a larger amount of paint fist. to achieve this, a max priority queue, with the amount of paint as priorities is maintained. in principle, the queue could contain an entry for each node involved in each distribute step. however, it is more efficient to merge the separate wet paint amounts into one entry. hence, the queue must allow efficient finding and updating of elements. finally, when the amount of paint to be distributed becomes negligible (i.e., less than the parameter ) it gets discarded, making the resulting rank vector sparse. all these improvements are described in more detail in the bca paper [cit] . one more technique described in the same paper is reuse of bookmark-coloring vectors (bcv -the equivalent to the pagerank vector) for the computation of other bcvs. this is analyzed further for the case of hubs (i.e., nodes which correspond to a subset of important pages). the bcv is precomputed for these pages and whenever the retain-and-distribute process forwards paint to a page p (h) in the hub, the amount is multiplied with the bcv corresponding to p (h) and added to p (b) . this optimization makes sense when many bcvs have to be computed, which is also the case for the co-occurrence matrix. however, since we are interested in computing the bcv for all nodes, further enhancements are possible, as we will discuss in the following subsection."
"in general, the problem of restoration can be reduced to the volterra polynomial integral equations of the 1st kind (1). in the linear variant, we will have a classical form of the integral of volterra equation of the 1 st kind with one kernel 1 k [cit] . the invalidity of signal restoration problem requires the applying of indirect approaches to the solving of integral equations of the first kind. the most effective approach to solving these problems is the appliance of regularization methods, the application of which will allow obtaining the sustainable solutions to the problem of restoration [cit] ."
"the article deals with the method of signal restoration at the input of a nonlinear dynamic object with distributed parameters. to describe these objects, a universal mathematical model in the form of a volterra integro-degree series has been chosen. the problem of signal restoration is reduced to the problem of solving the volterra polynomial equation of the first kind. the numerical implementation of such models is suggested to be carried out using quadrature methods, in particular, the method of trapezoids. in order to increase the stability of the solution in the presence of noise interference in the input data, it is suggested to use the differential regularization operator, which allows the incorrectly set task to be transformed into a class of correct ones. the possibility of applying such an approach is studied in solving the volterra polynomial integral equation of the second order type, which describes nonlinear dynamic objects with quadratic nonlinearity. the computational formulas for solving this type of equations are given in the article. the received nonlinear second-order algebraic equations after approximation of the initial equation by integral sums are solved by iterative methods with initial approximation in the form of a precalculated radical. the developed algorithms are implemented as software modules in the matlab, with the help of which a number of computational experiments have been carried out. as an example, non-linear dynamic objects that contain static non-linearity of the second order and dynamic links that are typical for objects with distributed parameters have been chosen. such links are: a semiintegral link, an attenuation link (semi-delay) and a semi-inertial link. on the basis of applying the equivalent transformations, the macromodels of objects with distributed parameters have been obtained in the form of a polynomial integral volterra equation of the i kind of the second order with the kernels that describe the above-mentioned components. the results of the computational experiments, presented in the form of graphs, showed that the suggested approach can be effectively used in restoration of signals at the input of nonlinear dynamic objects with distributed parameters."
"у статті розглянуто метод відновлення сигналу на вході неліній-ного динамічного об'єкта з розподіленими параметрами. для опису даних об'єктів обрано універсальну математичну модель у вигляді ін-тегро-степеневого ряду вольтерри. задача відновлення сигналу зво-диться до задачі розв'язування поліноміального рівняння вольтерри першого роду. чисельну реалізацію таких моделей пропонується здійснювати з використанням квадратурних методів, зокрема, методу трапецій. для збільшення стійкості розв'язку при наявності шумових завад у вхідних даних запропоновано використання диференціального регуляризаційного оператора, який дозволяє некоректно поставлену задачу перевести у клас коректних. можливість застосовування тако-го підходу досліджено при розв'язуванні поліноміального інтеграль-ного рівняння вольтерри і роду другого порядку, яке описує нелінійні динамічні об'єкти із квадратичною нелінійністю. в статті наведено обчислювальні формули для розв'язання даного типу рівнянь. отри-мані нелінійні алгебраїчні рівняння другого порядку після апрокси-мації вихідного рівняння інтегральними сумами розв'язуються ітера-ційними методами із початковим наближенням у вигляді попередньо обчисленого кореня. розроблені алгоритми реалізовано у вигляді про-грамних модулів в середовищі matlab, за допомогою яких проведено ряд обчислювальних експериментів. як приклад, вибрано нелінійні динамічні об'єкти, які містять статичну нелінійність другого порядку та динамічні ланки, які є типовими для об'єктів із розподіленими па-раметрами. такими ланками є: напівінтегральна ланка, ланка затухан-ня (напівзапізнення) та напівінерційна ланка. на основі застосування еквівалентних перетворень отримано макромоделі об'єктів з розподі-леними параметрами у вигляді поліноміального інтегрального рів-няння вольтерри і роду другого порядку із ядрами, які описують вка-зані вище ланки. результати обчислювальних експериментів, які на-ведено у вигляді графіків, показали, що запропонований підхід може ефективно використовуватись при відновленні сигналів на вході нелі-нійних динамічних об'єктів із розподіленими параметрами."
"to test the robustness of our method and cross-dataset performance, we evaluate our proposed algorithms on ph2 dataset. [cit] testing set where as in ph2 dataset, ensemble-s achieved best scores in ph2 dataset except sensitivity in which ensemble-a performed best, as shown in table 4 . this again proved our claim of performance of our ensemble-s method for high specificity and ensemble-a for high sensitivity. in ph2 dataset, the expert annotations are very precisely drawn for outer boundary of skin lesions, hence, that is why ensemble-s performed the best on ph2 dataset."
"introduction. while solving the problem of signal restoration at the input of nonlinear dynamical systems, there is usually a need to solve volterra nonlinear equations of the first kind [cit] . applying of integro-degree volterra series, which is a universal mathematical model for describing nonlinear dynamical system [cit],"
"in addition to testing, requirements engineering is an important part of embedded system development. requirements represents a major cost driver when developing embedded systems [cit] . to ensure functional safety, some embedded systems are developed according to standards such as iec 61508 or iso 26262 [cit] . the processes and people used for requirements engineering vary widely depending on the application domain and the organization developing and using requirements during development. in many companies, people in different roles (e.g., developers, testers, experts, architects) are involved in this iterative process and have overlapping goals, but also use different perspectives [cit] ."
"two recent papers on agile communication practices discuss approaches and barriers for agile communication. the barriers for effective knowledge sharing were categorized as team factors, process factors and contextual factors, and include: distances, lack of social skills, stakeholder neglect of nonfunctional requirements, product owner lack of sharing client feedback, inadequate planning, insufficient documentation, as well as lack of high-quality collaboration techniques and processes [cit] . proposed approaches for enhanced communication in a global outsourced agile development included the use of good tools that can replace face-to-face communication when teams are distributed, as well as the use of tools for social networking and continuous integration. the authors also propose to establish trust and a one-team mentality to overcome cultural distances [cit] . this is in line with our findings."
"in contrast, the object frequency, and also the used pagerank metric, assign a numeric score to each node in the graph. therefore, we call weighting approaches based on them node-centric. to obtain a weight for the edges, we either push the weight down, meaning that the number assigned to a node is used as the weight of all in edges, or we split the number down, meaning that the weight is divided by the number of in edges and then assigned to all these edges. if split is not mentioned explicitly in node centric weighting strategies, then it is a push down strategy."
"in addition, by virtue of k being p-core, the vector x * has no zero entries and x * is unique for any fixed labeling of the vertices of k."
"note that uniform weights are equivalent to using object frequency with splitting the weights. to see why this holds true, we have to follow the steps which will be taken. first, each node gets assigned the amount of times it is used as an object. this number is equal to the number of in edges to the node. then, this number is split over the in edges, i.e., each in edge gets assigned the number 1. finally, this weight is normalized, assigning to each out link a uniform weight. hence, this strategy would result in the same walks as using unbiased random walks over the graph."
"they showed that the limit exists, based on work by alekseev [cit] (see also [cit] ). thomason [cit] compiled these results to show the relationship to the edit distance function:"
"there is a matrix associated with a crg called m k (p) that plays a role similar to the role the adjacency matrix does for graphs. we can use this matrix to help define the functions f k and g k, that are essential for understanding edit distance."
"in mupps, the propagation of an atlas to unseen image can be done through a number of different paths to resemble the multiple classifier strategy. a set of propagation results are then available for fusion using the available methods. however, the number of available paths from the training subjects can be large, leading to the problem of scale. three path selection strategies are hence investigated and an efficient method is proposed."
"a number of participants saw that postponing testing is a challenge to effective flow of information. if the testing process does not start early, it means that the communication of test results cannot start and the results cannot be transformed into actions. we identified four separate causes for postponing testing: (i) unclear test entry criteria, (ii) tight coupling between hardware and software, (iii) scheduling of testing after software development ends, and (iv) ''the crunch'' occurring when development runs late and testing time needs to be dramatically reduced."
"studies with good reliability are independent of the researchers that performed them. we have been careful in describing the method used for conducting this study and also made the instrument available [cit], making it easier for researchers and practitioners to understand the details of this study. this threat has also been addressed by involving multiple researchers and iterating over proposed results between researchers. that said, it is most likely inevitable for our preconceptions and cultural and professional backgrounds to have affected the data collection as well as the synthesis processes, and consequently also the results. we thus welcome complementary work on this topic in different settings and by authors with contrasting backgrounds."
"structural and algorithmic method of obtaining solutions. let's consider a dynamic object with non-inertial nonlinearity, which is described by the structural scheme represented in fig. 1 . on a structure diagram"
"many works have shown the applicability of registration-based atlas propagation for the automatic segmentation, particularly in brain mr segmentation, where the multi-atlas propagation and segmentation (maps) has gained its wide popularity in recent years [cit] . in maps, each atlas is a combination of an intensity image and a label image. the intensity image is used for the registration process to map the space of the atlas to the unseen image, while the label image contains the segmentation information for propagation to the unseen image. the label image can also have prior knowledge such as the shape of the heart to assist the registration process [cit] ."
"we evaluated the performance of the segmentation algorithms by using jaccard similarity index (jsi). in addition, we report our findings in dice similarity coefficient (dice), sensitivity, specificity, accuracy and matthew correlation coefficient (mcc) [cit] . sensitivity is defined in eq (1), where tp is true positives and fn is false negatives. a high sensitivity (close to 1.0) indicates good performance in segmentation which implies all the lesions were segmented successfully. on the other hand, specificity (as in eq. (2)) indicates the proportion of true negatives (tn) of the non-lesions. a high specificity indicates the capability of a method in not segmenting the non-lesions. accuracy in segmentation methods report the percent of pixels in the image which were correctly classified as in eq. (3). jsi and dice is a measure of how similar both prediction and ground truth are, by measuring of how many tp found and penalising for the fp that the method found, as in eq. (4) and (5) respectively. mcc has a range of −1 (completely wrong binary classifier) to 1 (completely right binary classifier). this is a suitable measurement for the performance assessment of our segmentation algorithms based on binary classification (lesion versus non-lesions), as in eq. (6)."
"the method introduced in the previous subsection speeds up the computation of individual ppr computations. now, the observation leading to reuse of bcvs for pages in a hub can be adapted to our setting. the main point is that the computation of the bcv of node b can reuse the bcv of nodes reachable through its out links. especially, it is beneficial if the bcv of nodes one hop away have already been computed. adopting this viewpoint, we say that computation of the bcv of the node b depends on the bcv computation of all one-hop reachable nodes and hence b is a dependent of these nodes. now, what we want to achieve is that we only compute the bcv for nodes once the bcvs of all its dependent nodes have been computed. however, this will not always be feasible as the graphs contains cycles. hence, we want to quickly find an ordering of nodes, such that we can likely reuse as many bcv computations as possible. to achieve this we break cycles and in that case compute the bcv for the node at which we break without being able to count on all dependents being available. we choose the node for breaking the cycle to be the one with the highest in-degree as that one is likely to cause most reuse and break multiple cycles at once. the pseudocode of the algorithm is shown in algorithm 1, the actual implementation also includes a couple of indexes and bitmaps to speed up the computation. now, with the order determined, we can compute each bcv, reusing many previously computed values."
the main contribution of this work includes: (1) propose mupps and demonstrate its superiority over standard maps; (2) investigate propagation path selection and the effect on accuracy and efficiency.
we designed these end-to-end ensemble segmentation methods to combine mask r-cnn and deeplabv3+ with pre-processing and post-processing methods to produce accurate lesion segmentation as shown in fig. 6 . this section describes each stage of our proposed ensemble method.
"tions of the rotational motion of a rigid body in the rodrigues hamilton parameters. in this paper, this approach was used to solve the main problems of controlling the angular motion of a spacecraft: stabilization problems and terminal control problems. the article may be useful to developers of spacecraft attitude control systems."
"the international skin imaging collaboration (isic) is a driving force by providing the digital skin lesion image datasets with expert annotations from around the world for automated cad solutions for the diagnosis of melanoma and other cancers. this community also organises yearly skin lesion challenges to attract wider participation of researchers to improve the diagnosis of cad algorithms and spread awareness regarding skin cancer [cit] segmentation competition category consists of 2750 [cit] images in the training set, 150 in the validation set and 600 in the testing set. fig. 4 is an example of loosely drawn boundary by experts to label ground truths in the dataset. hence the algorithms need to achieve high sensitivity to perform well in this testing set. [cit] was conducted last year, they did not share the ground truth of their testing set. [cit] ."
"the given problem is suggested to be solved by replacing the integrals in (3) quadrature formulas, which allows to obtain a number of advantages, in particular, the simplicity of implementation and high stability of computational algorithms due to the regularizing properties of choosing the quadrature step [cit] . applying to (3) the method of trapezoids and difference formula of the first kind [cit], the following will b received:"
"completed interviews were added into an on-line spreadsheet. a list of themes, subthemes and codes were used to get consistency in the merged interviews. this sheet grew to almost 10 000 rows of interview text, and was later exported as a comma-separated-values (csv) text file to support partial automated analysis through scripting. about 17.9% of the rows of the spreadsheet were coded. next followed a period of iterations where the sets of themes and subthemes were reduced. for example, many subthemes on the lines of ''manager tester communication'' and ''developer tester collaboration'' were merged into the more general subtheme ''collaboration between roles''. this period ended with a workshop where the number of themes was reduced to 6. after a few iterations, the initial 357 subthemes were reduced to 86. there were about 1550 unique codes."
in this section we present good approaches for improving the information flow in software testing mentioned by the interviewees (summarized in table 3 ). some aspects of them are also discussed in relation to existing research in the discussion section.
"finally, our results show that more research on the flow of information and communication in testing is needed and that practitioners need to take it more clearly into account; it is not enough to improve testing itself unless its results are clearly communicated so they can be acted upon."
"for two matrices a and b of the same dimensions, we say that dist (a, b) is the number of positions in which a and b differ; i.e., it is the matrix hamming distance."
"the results of the thematic analysis show that software testing produces information that is non-trivial to manage and communicate. the flow of information in software testing is built up of a number of feedback loops initiated by software development activities. as illustrated in figure 2 distances may have an impact on the loops. six key factors at the organizations developing embedded systems affect the flow of information in software testing: how testing and trouble shooting is conducted, communication, processes, technology, artifacts, as well as how the companies are organized (see table 1 ). there are seven main challenges for the flow of information in software testing: comprehending the objectives and details of testing, root cause identification, poor feedback, postponed testing, poor artifacts and traceability, poor tools and test infrastructure, and distances (see table 2 ). finally, we identified five good approaches for enhancing the flow of information in software testing: close collaboration between roles, fast feedback, custom test report automation, test results visualization, and the use of suitable tools and frameworks (see table 3 )."
"tools are used in the creation, execution and handling of artifacts and are used by developers, test engineers and project managers to coordinate and communicate test results. thus, interviewees saw tool support as an important area supporting the flow of information and it was mentioned both as an area of improvement, and as an important aspect enhancing the use of test results. some interviewees gave examples of when tools are helpful for communicating information about testing: tools for traceability, for pushing notifications (such as sending an email when an issue in an issue tracker has been resolved), or for following code changes through the stages in the build and test phase. it is also common to use tools for exporting test results from one format to another for further analysis. however, interviewees felt that too many tools were needed and that tool interoperability was low. another technological aspect mentioned by interviewees was related to the use of test environments. this aspect refers to the equipment used for the execution of tests. most of the studied organizations used several co-located test environments (e.g., test rigs with several embedded systems, or a software simulation environment on a pc) or globally distributed environments supporting different teams. test environments occurred in the interviews as a major factor impacting software testing. the interviewees perceived that these environments require a significant effort for configuration. in one case, the interviewee was frustrated with the test environment's instability and unavailability. when these issues do not occur, the availability of a test environment (e.g, developers testing locally using their own machines) speeds up the feedback cycles as well as reduces the number of involved persons. test environments typically use simulators to simulate the environment around the system and its dynamic behavior (e.g., signal and traffic generators). further, simulators were used when destructive tests are needed to not damage the hardware. one organization used a software test environment to support testing performed by developers at their own desks. these simulators are developed and maintained by a separate team focusing on simulators and test environments. interviewees mentioned that testing on the target hardware typically results in difficulties in using test automation, which results in a slower feedback loop that might postpone testing: ''and, people don't tend to want to write [ complexity is a major technological aspect influencing the flow of information, and affects the system under testincluding the differences in the supported hardware targets, requirements on forwards or backwards compatibility with respect to both software and hardware combinations. our interviewees also mentioned that the sheer number of communication protocols and their combinations, system configurations and code dependencies can directly influence software testing. we found evidence that complexity is rarely taken into account when testing and this can cause negative emotions and impact team productivity."
"our interviewees also mentioned distance between teams (or distribution of teams) as an important attribute affecting the flow of information in software testing. for example, the organizations had teams working together in different cities, different countries, and even in different continents. it seems that communication in large organizations between teams not located together (even if the distance is relatively short) negatively impacts testing. the interviewees mentioned that the organizational distance and the coordination between large teams can affect the flow of information in software testing. the communication links between team members are scattered and divided throughout the organization: '' we often work pretty much independently, so you don't have so much in common with others. and then you don't have the natural group, that you probably should have [in the agile process].'' another interviewee noticed that the use of different development processes in other parts of the organization was confusing. several interviewees mentioned that a way of overcoming this type of organizational distances is to use different tools and techniques such as filming a test session in the test environment."
"it can be difficult to differentiate benign lesions from skin cancers. skin cancer specialists examine their patients' skin lesions using visual inspection aided by hand-held dermoscopy. they may capture digital close-up (macroscopic) and dermoscopic (microscopic) images. dermoscopy is a means to examine the skin using a bright light and magnification, and employs either polarisation or immersion fluid the associate editor coordinating the review of this manuscript and approving it for publication was haiyong zheng ."
"we recruited a convenience sample of individuals affiliated with organizations in the embedded system domain. using a stratified design to ensure experience and specialization diversity, we selected individuals from each of the following groups: developers, testers and managers. we later added the category 'other' for very experienced interviewees that did not fit into any of these three roles. the interviewees were selected from a diverse set of organizations, again by using a convenience sample. we recruited individuals that the authors knew themselves or through one or more contact persons in a given organization, or by searching for suitable individuals in corporate address books."
      (4)i x t                                            
"in the embedded systems domain, software failures in communication equipment can lead to isolation of nodes in a vehicle or a plant, in turn leading to delays, loss of productivity, or even loss of life in extreme cases. the software in embedded systems needs to be of high quality, and software testing is the standard method for detecting shortcomings in quality. an important aspect of testing embedded systems is to conduct some of testing on real hardware [cit] and a common approach to support this is to provide a test environment with devices. [cit] found that, because software uploads to target are slow, a highly iterative testing process such as test-driven development, can be hard to implement, so some testing could be run on the host rather than the target. furthermore, testing on real hardware opens up for questions on how the test environment has changed since a function was tested last, or how the changed software best reaches the test system."
"text coding, step (4) in figure 1, started with all five authors coding one and the same interview to build consensus on the procedure. during a full day co-located workshop, we discussed the method for the qualitative analysis. we made a number of decisions: (i) to use the braun and clarke method, (ii) codes should preferably have four words or less -themes and sub-themes should preferably have fewer, and (iii) as not all sentences in the transcript were on the topic of information flow in software testing, we left the decision to each individual researcher whether to code, or not code, each fragment of an interview. each remaining interview was coded independently by two authors in different combinations in order to encourage diversity in the coding. next the first author merged the two codings of each interview into one. below is a coding example of an interview excerpt:"
"in practice projects are delayed, and when organizations experience tight deadlines, testers face time pressure: '' [what] you don't trifle with is the development time, but you trifle with test time.'' as a consequence there may be very poor information flow in the later stages of a project."
"illes-seifert and paech [cit] investigate the role of communication, documentation and experience during system testing. they found that ''most communication during the testing process occurs with the requirements engineer and the project manager followed by the developer.'' this is a type of pattern we had expected to be able to find in our interview data, but we cannot confirm or reject their finding. however, we note that communication patterns like this one could be of importance to the practice of software testing."
"we identified that a review process, and the practice of bundling several software changes before testing can both have a negative impact on the pace at which software is delivered from developers to the test environment. two papers on continuous practices in the embedded systems domain has identified additional factors to this challenge: having a timeconsuming or complicated delivery process, difficulties with configuration management of the test environment, and also human factors such as team structure, or not seeing value in delivering often, [cit] ."
"since theorem 9 establishes that every 1/2-core crg is a gray edge crg (that is, of the form k(r, s)) we can compute ed h (1/2) in terms of χ b (h ). combining this with other basic facts, we obtain theorem 13."
"in other words, the normalized edge weights are directly interpreted as the probability to follow a particular edge. to obtain these edge weights, we make use of the following statistics computed from the rdf data:"
"the function c h (p) is not necessarily concave down, however ed h (p) is. concavity is a key tool in finding the elusive lower bounds on the edit distance function which can then be used to compute lower bounds for c h (p)."
"we identified several challenges ( table 2 ) that need to be considered: comprehending the objectives and details of testing, root cause identification, poor feedback, postponed testing, poor artifacts and traceability, poor tools and test infrastructure, and distances. in addition, we highlight approaches (table 3) positively influencing the flow of information. close collaboration and communication between different roles was perceived as a good way to enhance the information flow. fast feedback, custom report generation, and use of suitable tools were seen as critical approaches in dealing with test results in an efficient manner. our study also indicates that visualization of test results can be used for fostering communication around testing."
"in earlier work on rdf graph embeddings (specifically rdf2vec [cit] ), symmetric windows were used on top of generated random walks, which include both node and edge labels. these symmetric windows have the focus word in the middle and the context of the word is both before and after it. this means that the context of a node b consists of the nodes it can reach by following edges, as well as the nodes which can reach b. what this means is that the result rdf2vec would be the same, independently of whether the original walks would be performed forward or backward. inspired by this, we investigated the effect of creating the co-occurence matrix as the sum of the normal ppr matrix as described above and the ppr matrix of the graph with all edges reversed. since a positive effect on the embeddings was obtained (at least for the tasks we used in the evaluation) we chose to use this approach."
"in this paper, we have introduced a novel approach for generating embeddings of rdf graphs, which exploits global instead of local patterns. we have shown that it is possible to outperform local graph embeddings techniques, in particular on document similarity. for most other tasks similar performance can be obtained."
"the cut metric is useful for comparing random graphs. although two typical graphs selected according to g(n, p) have edit distance close to 2p(1 − p), their d distance is o(1/n)."
"the problem of regenerating a signal that is altered by a nonlinear dynamic object is incorrect, and using of classical methods for solving such problems in the presence of noise interference in signals does not allow obtaining stable solutions with the necessary accuracy. it is suggested to use a differential regularization operator of first kind dx dt , where  -a parameter of regularization. in this case the solution of volterra polynomial equation is reduced to solving of the following integro-differential equation:"
"there is a lack of evidence on how software testing can be analyzed from an information flow perspective. the body of knowledge on the flow information in software testing is limited, in particular with regards to industrial evidence. in this paper we explore the information flow and the factors influencing it by interviewing experienced industrial practitioners and analyzing interview transcripts with thematic analysis. the practitioners were sampled from five different organizations developing embedded software in order to get a more diverse set of results."
"in table 3, we reported the performance of algorithms in terms of dice, jsi, and mcc. ensemble-a method outperformed all the participating segmentation algorithms for each skin lesion type."
"in the topic of test communication: industry standards such as iso/iec/ieee 29119-3 [cit] prescribe formats for test reports and types of other test documentation. many agile practices are also prescribing formats for test reporting, such as daily stand-up meetings. in addition, the international software testing qualifications board (istqb) syllabus on test automation prescribe approaches to logging and storing test results [cit] ."
"in order to test complex embedded systems in the first place, a sophisticated test environment is needed. another technological aspect of the testrelated information flow is how it can be enhanced with tools. 5) artifacts: requirements, issues, test cases, and also coverage are important aspects of a software development process, and how an organization handles these are of importance to the information flow. 6) organization: two important organizational aspects that have an impact on the flow of information are team structure and the distances between stakeholders. in the coming sections we present the themes and the most important subthemes (with the latter boldfaced in the text)."
"the co-occurrence matrix for textual data is obtained by linearly scanning through the text and counting the occurrence of context words in the context of each word. however, the graph which we use as input data does not have a linear structure. this problem has been worked around in the past by performing random walks starting from each of the nodes in the graph. recording the paths of these walks results in a linear sequence of node (and optionally edge) labels, which can then, in turn, be used as a pseudo-text to train a model. this approach is, for example, used in node2vec [cit] and rdf2vec [cit] . however, in these approaches, the trained model is different from the glove model and it does not use the co-occurrence counts, but rather trains a neural network on the individual context windows directly. in the case of glove, only the counts are needed and hence we are looking for a method to obtain these without generating the random walks explicitly. a possible solution would be to perform a breadth-first search of a certain depth starting from each node in turn, and take all reachable nodes as the context of each start node. given these kinds of contexts, one could then straightforwardly apply glove's co-occurrence weighting and assign a lower weight to co-occurrence counts of nodes which are further away from the focus node. however, this simple approach is problematic in that: a) there could be nodes reachable through multiple paths at different levels, b) there could be loops in the graph, making a walk pass through the same node multiple times, and c) if there is a node with many context nodes at level d, but only few ones at level d−1, then the ones at level d will dominate the closer ones in the co-occurrence matrix as there are that many of them."
"goyal and yap [cit] . this was a very first attempt to perform multi-class segmentation to distinguish melanocytic naevus, melanoma and seborrhoeic keratosis rather than a single class of skin lesion."
"the software is also commonly tested in target devices (3) . sometimes this is done at the desk of the developers, or in a lab setting at some distance from the developers desk. when done locally, the test setup is typically smaller, perhaps containing one unit or subunit of an embedded system, and not a full system. this is a slower feedback loop than (2), but it is still initiated by the developer and with that developer as the recipient of the test results."
"the lifetime of a test case from its creation to its retirement (i.e., the process of adding/removing test cases) may influence how test results are communicated. several interviewees mentioned that test cases can be added in an ad hoc manner or in a structured strategy based on requirements or using risk analysis (e.g., hazard analysis and a fault tree analysis). in the organizations with good traceability between requirements and test cases, the retirement of a test case seems relatively easy; if a test case had its linked requirements removed then there is no reason for the existence of the test case and its results. in two of the studied companies, the interviewees instead mentioned that test cases are rarely removed and that their test scope grows in an uncontrolled way. many interviewees mentioned that grouping of test cases for particular areas was a common way of assigning the responsibility of dealing with these test cases during their lifetime to a certain developer. there was also a need for areatailored test reports."
"the cardiac mri sequence used in the experiment was the balanced steady state free precession (b-ssfp) for whole heart imaging, at the end diastolic phase. the sequence was implemented on a 1.5t clinical scanner (philips healthcare, best, the netherlands) equipped with 32 independent receive channels. a fat saturation and t2 preparation pulses were used to null fat and to increase the contrast between blood and cardiac muscle. a free breathing scan was realized by enabling one navigator beam before data acquisition for each cardiac phase."
"the crg that corresponds to the p/(1 + 4p) part of the function has 5 white vertices, 2 nonadjacent white edges and the remaining 8 edges gray. see figure 4 ."
"path selection. fig. 4 plots the mean dice scores of the vote, staple, sba fusion results of mupps using the three different registration schemes for path ranking: the after affine, after larm, and after nonrigid. all the three fusion methods agreed that the path selection strategy using the after affine was better than the other two. the best results of the after affine scheme were achieved when using the best 11-13 ranked paths for propagation and segmentation, while the other two both needed more, about 20, cases. hence, we needed n sr, which was 20 in our experiment, affine registration for path selection and ranking and 11-13 propagation registration to achieve the best result using the after affine. also, affine registration was the fast one among the three selection registration schemes. it could be achieved within one minute while the nonrigid registration, larm plus ffds, might need over one hour in our experiment. it may seem counterintuitive that the after affine performed better than the after nonrigid, as the image similarity after nonrigid registration should be more accurate in evaluating the segmentation accuracy and thus in ranking the paths. this could be an inconclusive result from the specific test dataset, as the difference between the after nonrigid and the other two on 11-13 of fig. 3 (right) was small. this could also be the indication that the \"good\" segmentations ranked by after nonrigid might tend to have similar bias, resulting in the gain of accuracy by fusing them not being significant, compared to the fusion results of \"bad\" segmentations which nevertheless have unrelated random errors. fig. 5 and table 1 provide the mean dice scores of local regions and the whole heart. the vote u, staple u, and sba u in table 1 are the fusion results of mupps using the after affine registration to select eleven highest ranked paths for propagation. the accuracy improvement using fusion was evident, as all the three fusion techniques gained at least 0.016, about one standard deviation, in whole category. the gains in local regions were different. the myocardium, which resembles a thin cup-shaped sheet and normally reports a worst segmentation result in the five regions, achieved the most significant improvement, about 0.03 after using the fusion. by contrast, the left ventricle had the least improvement, less 0.01, and other categories had less than 0.02. we therefore conclude that it can be more beneficial to use mupps and fusion techniques in the segmentation of thin, sheet-shaped objects where one propagation path tends to have certain random errors in the result. note that the conclusion from the surface distance as error measure was similar, as the mean root mean squared surface distance of mupps without fusion, and the fusion results using vote, staple, and sba were 1.29±0.319, 1.13±0.275, 1.25 ± 0.346, and 1.13 ± 0.248 (mm), respectively. only that staple fusion seemed to be disfavored in this error measure."
"in fact, we have a more general way of computing the edit distance function if the matrix m k (p) is block diagonal matrix, where the blocks correspond to a crg notion of components."
"visual reporting (i.e., using visualizations to support or replace reporting) was used at most organizations. it was suggested that visualizations supports communication. for example, organizations could use a practice where they gather development teams around a visualization of test results. a common visualization displayed the number of found, open or corrected issues. one vehicle organization used plots showing coverage in terms of system requirements, broken down to show requirements without test cases, as well as requirements with untested, failing, and passed test cases respectively. most organizations used color codes to separate passing from failing results, here red means fail, and pass could be green or blue. the color yellow or orange was sometimes used -it could mean test not executed, or failure due to shortcomings in the test environment. aspects with lacking visualization support included non-functional characteristics (such as memory usage), test environment details such as cable connection endpoints in the test system, and various aggregated views such as test results only relevant to a specific functional area, project or customer."
in this study we have explored the information flow in software testing by conducting and analyzing interviews with twelve experienced practitioners in the embedded software industry. while much of research and even practice of software testing is focused on improving the testing itself a main conclusion from our study is that large gains can be had by better using and communicating around the testing that already happens. in this section we put these results in context of related work.
"following our previous work [cit], we apply twelve different strategies for assigning these weights to the edges of the graph. these weights will then in turn bias the random walks on the graph. in particular, when a walk arrives in a vertex v with out edges v o1,...v od, then the walk will follow edge v ol with a probability computed by"
"the difficulty in extending the theory of the edit distance in graphs to hypergraphs is in proving that (17) is, in fact, an equality. the above definition of the r-crh would not seem to be adequate to capture the subtleties of hypergraphs. consider the common example of a hypergraph whose 3edges are cyclic triangles in an underlying random tournament. see, e.g., the survey of hypergraph turán theory by keevash [cit] . this hypergraph has no copy of the tetrahedron k 3 4 but crossing triples would be gray in any 3-crh that models it. strong hypergraph regularity was developed in the 3-uniform case by frankl and rödl [cit] and then for the general r-uniform case by gowers [cit], rödl and skokan [cit] and nagle, rödl and schacht [cit] . in these formulations, the notion of how overlapping hyperedges interact is captured by structures known as complexes. the structure of complexes inherent in strong hypergraph regularity would seem to be necessary in order to define r-crhs and colored homomorphisms in order for the existence of a particular induced hypergraph to be determined."
"cancers of the skin are the most common form of malignancy in humans [cit] . the most common malignant skin lesions are melanoma, squamous cell carcinoma and basal cell carcinoma. [cit], 96,480 new cases will be diagnosed with melanoma and more than 7,000 people will die from the disease in the united states [cit] . early detection of melanoma can save lives."
"there are many approaches to qualitative data analysis; we decided to follow thematic analysis as described by braun and clarke [cit] . we saw this approach as comprehensible, suitable for the type of data we had, and it allowed for one sentence of the transcript to be coded as belonging to several themes. the terminology used in thematic analysis partially overlaps with the one in grounded theory (e.g., [cit] ) and content analysis (e.g., [cit] ). important concepts are:"
"with these increased distances come an increased need for the use of tools to bridge the distances. one participant mentioned video-recording test sessions or using a tool to access a test system remotely. however, the use of tools can negatively impact the flow of information. the challenge is that tools may have compatibility distances, may not always work well, and manual traceability between tools is quite common. sometimes tools are used in ways they are not designed for, such as using an issue tracker as a to-do list manager, because ''it's easier for everyone to keep all the things in the same place instead of having fifteen different places to look [in] .''"
"based on our findings, organizations developing embedded software may want to tailor their communication processes to improve the information flow and feedback loops. tools and frameworks for test result reporting and communication should take these challenges and approaches into account. for example, when developing frameworks for automated test reporting one should try to address the challenges of understanding the objectives and details of a test result and how testing was conducted by providing all the needed instructions and details; one idea would be to provide a framework suggesting that a particular failure was due to errors in the test environment, simulators, test case itself, test framework, software or hardware under test. successfully implementing this kind of suggestion system could save a lot of time in properly using and communicating test results."
"selection registration is used to register the unseen image to the atlas intensity image and then compute the similarity between them to rank the path or atlas for propagation selection. in the multiple classifier strategy, the number of available atlases is normally greater than that of atlases needed to achieve the best result [cit] . therefore, the selection registration is also required to be computationally efficient compared to the propagation registration. three registration schemes will be investigated in this work:"
"at numerical realization of the received model at the point of zero of kennel are singular, therefore instead of zero point the value of h modeling step is taken. with the help of the developed software assets, a number of computational experiments have been conducted. in fig. 2 a signal with noise interference is shown, on the basis of which the input signal is restored. in fig. 3 the accurate and restored signals are provided."
"therefore, the development of new methods for solving volterra polynomial integral equations of the 1 st kind based on regularization algorithms for restoration of input signals of nonlinear dynamic objects, both with lumped and distributed parameters, is an urgent task."
"there are a number of interesting consequences arising from the study of these hereditary properties. first, we note that the crg that gives the portion of the function in theorem 31(b) corresponding to (1 + 7p)/15 results from a strongly-regular graph construction."
"if conjecture 2 is false, then it implies that the structure of random graphs and the behavior of editing induced graphs is quite complex and very unexpected."
"magnetic resonance imaging (mri) has become a routine modality for the determination of patient cardiac morphology. the extraction of this morphological information can be important for the development of new clinical applications as well as the planning and guidance of cardiac interventional procedures. manual delineation is labor intensive and subject to inter-and intra-observer variability. therefore, it is highly desirable to develop an automatic technique for whole heart segmentation of cardiac magnetic resonance (mr) images. however, automating this process is complicated by the limited quality of acquired images and large shape variation of the heart between subjects."
the second step of our framework aims to enrich the representation of both ks and twitter documents using information about the entities and concepts mentioned in these documents.
"in the case of dbpedia, we sparql 23 queried for all resources whose categories (dcterms:subject) and sub-categories (skos:narrower) are similar to the topic of interest. the final dbpedia dataset comprised 9,465 articles 24 . while the majority of these articles belong to a single topic, less than 1.% of them are annotated with 3,4,5,6,7 or 9 topics (as shown in figure 4) . for querying the freebase ks we used the freebase text service api 25 . the final freebase dataset comprises 16,915 articles 26, where the majority belong to a single topic (as shown in figure 4) . from the returned resources, we kept each resource's abstract or title to build the annotated dataset for the given topic."
"each of these approaches has its advantages and disadvantages. the first approach determines the efficiency of production management in terms of the final result, but in this case, the direct contribution of the management system to obtaining this result remains uncertain. another approach is more objective, but a problem is how to allocate from the general result the contribution of the management system. therefore, researchers propose to determine the contribution of management to ensuring the production efficiency not by using the general results, but by determining the qualitative impact of namely management on this efficiency (pawłowski, 2009; [cit] ."
"the first proposed in our original framework ( [cit] ), resource meta-graph, exploits semantic information about an entity's ks resource. the second is the category meta-graph which exploits the semantic information extracted from the wikipedia categories to which an entity belongs. this second graph can be effectively considered as a subset of the first one, as it groups similar entities belonging to the same topic under the same label. the category meta-graph thus categorises entities into more granular taxonomies."
"1. as a result of the theoretical search and empirical confirmation of its results, it has been established that an adequate system of its assessment is needed to ensure the effectiveness of the production management system. the results of production management are not so obvious and, as a rule, are not directly evaluated as the results of the production itself. therefore, for their system evaluation it is advisable to apply a criterion model based on a two-dimensional and indirect methodological approach. it should have a generalized, unified format and define the principle of constructing modified subsystems of indicators and relevant indexes."
"the internal effectiveness shows how the satisfaction of certain social needs is reflected in the dynamics of achieving its own goals of the socio-economic system and its individual groups of participants [cit] . the basis of the internal efficiency of the control object is formed by the production results (used by 92.3% of the surveyed management subjects), as well as the level of use of its production potential (used by 59.4% of respondents). in this regard, in order to assess the internal efficiency of a management object, it is proposed to use the generalized criterion of production efficiency (k3), which is formed on the basis of: a) the level of efficiency of the productive resources use; b) the level of labour productivity."
"source topic classification this section details the results obtained for the single-source tc case. in particular, it compares and contrasts the results reported in our previous work for the resource meta-graph ( [cit] ) with the results obtained for the category meta-graph introduced in this paper."
"2. combining the evidence about the semantic features from multiple, linked ks taxonomies (t w(dbks + f bks )) is beneficial for tc, showing a significant improvement over approaches considering a single ks (t w(dbks ), t w( f bks ))."
"as indicated in equation 2, the computation of the specificity value is independent of the entity e and differs according to the ks graph from which it is derived 17 . higher specificity values indicate that the property p occurs frequently in resources of the given class cls."
"the methodological basis for the research of the production management efficiency assessment, as a complex socio-economic phenomenon, envisaged the use of different approaches, particularly: structural-functional (to analyze the functioning of various components of the management system), system-synergetic (to substantiate the systemic interaction of various management levels), informational and value-based (to assess the performance of the expected results). within these approaches, after putting forward the hypothesis about the appropriateness of applying a criterion assessment, the critical aspects of the existing management system were identified through the analysis of online sources, statistical materials and primary documents of enterprises, media reports, a review of scientific literature on the problem of management efficiency. the empirical confirmation of certain abstract constructions and the hypothesis was obtained by means of a sociological survey, statistical processing of the received answers, their analysis and generalization. the employees of local departments of the state executive power and bodies of local selfgovernment, heads of enterprises and their subdivisions in zhytomyr region (ukraine) were the respondents of the survey. the statistical sample included 250 respondents. the survey was anonymous using a standardized questionnaire containing empirical indicators. in this way, information on the practical experience concerning subjects of evaluation has been collected."
"the conducted bibliographic review convinces that the theory and practice of management formed an understanding of determining the effectiveness of management based on the result-cost principle (аinabiek, 2015; [cit] . in this case, the priority is given to so-called \"target\" efficiency, the highest value of which is characterized by a complete coincidence of the result with the goal set [cit] . but such an approach does not allow the full disclosing the essence of the economic category \"efficiency\", since it focuses solely on determining the ultimate performance and does not include an evaluation of the management activities through which it has been achieved."
"after solving the discrete optimization problem defined by (2) with numerical methods, it is possible to find out that 1) there exists a unique ne for our system 2) at the ne there are 7 sus on channel 1, 15 sus on channel 2 and 28 sus on channel 3."
"the real-time classification of tweets is important since they act as social sensors revealing emerging events occurring in the world. in this work we investigated the use of semantic concept graphs of linked knowledge sources for topic classification of social media posts. we demonstrated the feasibility of this approach by implementing classification models that make use of semantic graph structures in multiple knowledge sources (dbpedia and freebase). in particular, we introduced and evaluated various semantic features derived from two distinct concept graphs (resource-meta graph and category-meta graph) of these kss, and showed that they can help to build accurate topic classifiers of tweets."
"in addition, we observe that in all the three datasets the number of unique categories is higher than the number of unique classes. this indicates that the datasets are more diverse in terms of categories than in terms of classes."
"2. the usefulness of these entropy based measures varies among different topics and tc scenarios. however, the property-based dispersion measures achieved best correlation values in both single-source and cross-source tc scenarios 30 ."
"following the path traced by this paper, we plan to further explore payoff computation techniques and to apply them to distributed scenarios such as the one offered by imitationbased cognitive radios. especially, at the actual stage we are attempting to study the spectrum access game with estimate errors, in order to analytically show the convergence of the derived dynamics."
this augmentation strategy (f a2 ) aims to further improve the generalization of a tc by exploiting the subsumption relation among classes within the dbpedia or freebase ontologies.
"the goal of the feature weighting strategies is to capture the importance of the semantic features for a given topic, based on the structure of the ks ontologies."
"following the concept generalisation process, in the resource meta-graph the number of unique dbclass classes reduced by 76%, the number of unique yagoclass classes reduced by 92%, and the number of unique f bclass classes by 88%. while in the category meta-graph the number of unique dbcat classes reduced by 42%."
"for other kss, which are not part of the lod cloud (e.g. wikidata 33 ), the proposed framework could still be applied provided that a mapping between the dbpedia ks and the newly explored ks exists. a possible future direction could be to utilise the data from dbpedia, and derive contextual information about entities from the semantic meta-graph of the new ks."
"given the vocabulary differences between kss and tweets, one of the challenges faced by these models is the frequent usage of grammatically incorrect english in microposts. due to the restricted size of short messages, entities such as country names (e.g. nkorea) are often abbreviated, as in the following tweet: \"nkorea prepared nuclear weapons holy war south official tells state media usa\". these irregularities mean that current annotation services (including opencalais api) will ignore these entities, and therefore no semantic information will be exploited for these entities by the tc system. a possible solution to address these challenges is to apply lexical normalisers especially developed for tweets ( [cit] ) to normalise these words to standard english terms."
"email address: a.varga@dcs.shef.ac.uk (andrea varga) 1 http://irevolution.net/2013/07/07/twitter-political-polarization-egypt/ however, the classification of such messages poses unique challenges, due to the special characteristics of the messages i) the limited length of microposts (up to 140 characters), restricting the contextual information necessary to effectively understand and classify them; ii) the noisy lexical nature of microposts, where new terminology and jargon emerges as different events are discussed; iii) the large topical coverage of micropost."
"in our context, we introduce this measure, as it allows to capture the semantic ambiguity and uninformativeness of a topic based on the entities mentioned in the documents and the ks structure 19 . that is, entities that are evenly distributed over multiple ks concepts/categories will have high entropy and thus topics mentioning these entities are less focused (more ambiguous) in the subject(s) they discuss."
"in this paper we have discussed the problem of opportunistic spectrum access in csma/ca-based cognitive radio networks where the secondary users aim at load-balancing the achieved throughput. we have demonstrated that this is possible if each su adopts the proportional imitation-based spectrum access policy, which is totally distributed and relies on solely local interactions amongst users."
"6. entity-category entropy (entity-category entropy): in an analogy with the entity-class entropy, we computed this measure for each topic, by considering the category bags for each entity mentioned in a topic based on the extracted category meta-graphs. in this case, low entropy indicates that the topic is less ambiguous, consisting of entities belonging to few categories, while high entropy refers to higher ambiguity at the level of entities."
"2. the conceptual basis for the content and forms of assessing the production management efficiency is its definition in two dimensions: 1) the efficiency, expressed in its final results (strategic efficiency) which has a leading role and 2) the efficiency of the management process (tactical efficiency). this approach allows assessing not only the effectiveness of management, but also the means by which this performance has been achieved."
"for e.g. in freebase the fbont:/crime/crime accuser class is derived from a very generic fbont:/common/topic class, while another related class type fbont:/crime/convicted criminal extends the fbont:/people/person class. in the case of the category structure of wikipedia, we also note that the category tree is not a strict taxonomy and does not always contain an is-a relationship ( [cit] ). given that for both semantic concept graphs we applied concept generalisation strategies, this mismatch can affect the generalisation of the patterns learned by our topic classifier, in that entities which should be considered together might belong to different entity types. one possible solution to overcome this problem could be to perform a cross-consistency validation, by investigating the overlapping properties between the entities assigned to the same entity classes, and consider the most likely entity classes ( [cit] )."
"these results are confirmed by fig. 6, which displays one realization of ideal-pisap, nepct-pisap and gpct-pisap. in particular, the convergence trends in terms of number of sus per channel and average utility per user per channel are depicted. looking at the three plots, it is easy to notice that the one induced by ideal-pisap is less noisy than the ones induced by nepct-pisap and gpct-pisap. not enough, in the latter case the curves even intersect several times. this is due to the fact that larger estimate errors (gpct-pisap high shorttime throughput variations can be regarded as errors around the average throughput) produce greater amounts of revision opportunities 3 both in the convergence and the stability phase."
"the complexity of choosing criteria and indicators for evaluating the management efficiency is mainly due to the specifics of management activities those don't have direct metrics (аinabiek, 2015; [cit] . in this regard, the assessment of the management efficiency is often carried out on the basis of determining the efficiency of object itself but not its management in particularly. however, the equation of the concepts of \"management efficiency\" and \"efficiency of production\" is incorrect. an analysis of the managerial practice shows that even when applying an effective management mechanism, it is not always possible to achieve high results and goals set because of force majeure circumstances that cannot be predicted in advance."
"in the first stage of our framework, data collection, data from both twitter and kss is retrieved. the twitter dataset comprises a set of topically annotated microposts. conversely, the kss dataset is build from a set of articles relevant to a given topic extracted from multiple, linked lod kss. this study considers two linked kss (from lod), namely dbpedia (db) and freebase (fb), which are applied both independently and as a merged ks. therefore we consider three cross-source scenarios for the use of these ks articles: i) db -from dbpedia only; ii) fb -from freebase only; and iii) db-fb -from both dbpedia and freebase."
"for example, for barack obama these features would be yago:presidentsoftheunitedstates, fbont:/book/author, yago:livingpeople, and dbowl:person. our main intuition is that the relevance of an entity to a given topic could be inferred from an entity's class type. for example, the class yago:presidentsoftheunitedstates could be considered more relevant to the topic violence, than the class yago:singer."
proof: the nash equilibria are the maximizers of the potential function defined by (2) . there is at least one maximizer since p can only take a finite set of values.
"ensuring a high level of efficiency of the management process is possible only if it is achieved at each stage: at goal-setting (making management decisions), at the stage of achieving the goals and at the feedback stage (каpelushyna, 2016; [cit] ) . the effectiveness of the management process depends on its direct participants (the object and subject of management), their internal organization, potential capabilities and direct actions. the optimal functioning of all structural elements of the management process, as one of the determining factors of management efficiency, has been noted by 23.1% of respondents. that's why the subsystem for assessing the efficiency of the management process should include an assessment of the process each stage, as well as an assessment of the management system organization and production itself. an assessment of the organizational level of the management system can determine its ability to meet the needs of the business environment. production management must adequately respond to environmental signals in relation to public needs and achieve their goals. the assessment of the management organization should be characterized by criteria that most fully determine its internal structure and reflect its essence. these include: a) the optimality level of the organizational structure of management; b) the level of security of the system of the competent personnel management; c) the organizational level of managerial labour. these criteria are generalized by the level of management system organization (k4). importantly, evaluating the effectiveness of management, 45.5% of respondents take into account the level of the management system organization."
"for evaluating the tc framework for the different singlesource and cross-source scenarios, we took the commonly used one-vs-all approach ( [cit] ). in this approach we decompose the multi-label problem into multiple independent binary classification problems."
this strategy (f a1 ) augments the initial lexical features (e.g bow and boe features) of the datasets with additional semantic information extracted for the entities appearing in them.
"also for the cri topic, we observe, that the fb + t w single ks classifier using bow features performed better than the db + t w single ks classifier. however, when looking at the results obtained for the boe features, we observe the opposite trend, the db+t w performed better than the fb+t w. an explanation for this could be that a relatively large number (3,377) of articles do not contain any entity, and thus are not semantically enriched."
"considering the results obtained for the single-source tc (t w(dbks+fbks)) case, the class-property entropy yields the best correlation value, over 60% for all three topics. among the class entropy, category entropy, property entropy and entity entropy measures, however, the property entropy values were found to be the best. as opposed to the cross-source case, among the class dispersion measures, the category entropy values were higher than the entity-class entropy values. for the category dispersion measures, the category entropy values were higher than the entity-category entropy values in two out of three topics. these results indicate that in the single-source case analysing a single semantic feature (e.g. p, cls or cat) can provide a good estimate of the performance of the topic classifier. in the case of the cross-source scenario the representation of the topics seems to be more complex, requiring the modelling of the entropy of two semantic features (in our case in the form of conditional entropy values). nonetheless, among the property dispersion values, the best results were obtained by the class-property entropy values."
"conversely, the generality measure captures the specialisation of a property p to a given class cls, by computing the property's frequency within other semantically related classes r (cls). the generality measure of a property p to a class cls in a ks graph g ks is defined, as follows:"
"once a semantic meta graph has been constructed for a given entity, three main features can be derived from it: class, category and property features. among these features the class and category features are particular to a semantic meta-graph: class being extracted from the resource meta-graph, and category being derived from the category meta-graph; while the property features are common to both meta-graphs."
our experiments make use of a twitter dataset and two kss datasets previously introduced in ( [cit] ). these datasets belong to the emergency response (er) and violence detection (vd) domains. this section provides a brief description of these datasets.
"in our experiments we employed three different single-source tw classifiers. these classifiers make use of a single ks ontol-ogy: tw(dbks) and tw(fbks); and the combined ks ontologies: tw(dbks+fbks). in particular, in the case of the resource meta-graph, dbks denotes the dbowl +yago ontologies, while in the case of the category meta-graph, dbks stands for the dbcat ontology. these classifiers are evaluated against several baseline models, as presented in table 6 ."
"we also compared these results with the content-based similarity measures studied in our previous work ( [cit] ). that is, we computed the (χ 2 ) −1 measure between the training and test datasets for the db+ fb+t w and t w(dbks+fbks) classifiers using bow and boe features, and correlated these values with the performance of these classifiers. according to these results, in the single-source case the best correlation values obtained were: 21% (boe) for disacc, 58% (bow) for cri, and 23% (boe) for war; while in the cross-source case, these values were 14% (bow) for disacc, 45% (boe) for cri, and 20% (boe) for war. as we observe, our novel entropy based similarity measures (entity-property entropy for cross-source tc and classproperty entropy for single-source tc) achieve better correlation with the performance of the topic classifier, showing the usefulness of incorporating semantic features from kss for enhancing the representation of a topic."
"the second stage topic classification involves the creation of statistical tc models, which incorporate the various semantic features obtained in the context modelling step. in this stage we investigate two different scenarios: the twitter only scenario in which we build a topic classifier on twitter data only, and the cross-source tc case where we make use of the information from multiple linked kss. this allows us to analyse which concept graph provides better semantic features for tc, and also whether the role of the semantic features differ according to the tc scenarios. in particular, we investigate whether the same semantic features which account for modelling the specificity of the topic in the twitter only scenario, serve the same role in the cross-source scenarios."
"in summary, these different ks ontologies (i.e. dbowl, yago, fbont) provide a rich source of semantic information about entities in many domains and across topics. further, in these kss each entity resource is related to different ontological classes or concepts which can provide additional contextual information for that resource. this contextual information allows us to exploit various semantic structures of these resources."
"having the documents selected from both ks and twitter, a simple bow representation is employed for modelling the content of these documents. this allows these datasets to be represented based on what is discussed in these documents. in order to capture the importance of each word mentioned in these documents, the tf-idf weighting schema is applied."
"criterion of estimation of external efficiency of production consequently, in order to assess the external effectiveness of the management object, it is expedient to use as a criterion the degree of satisfaction of social, group, individual needs and interests (к2). it gives an opportunity to evaluate the degree of interest of certain consumer groups, business entities and society as a whole in the functioning and development of a particular production. it is intended to target the management system to the needs and problems of society, and to reflect the degree of their resolution. in practice, this direction of evaluating the efficiency of management is carried out on the basis of continuous monitoring of market share, which was indicated by 67.1% of respondents."
"in this paper, sus implement dcf as a random access protocol to avoid collisions. this yields: the contention window has been set according to the dsss schema parameters [cit] where p(n i ) denotes the successful transmission probability with n i sus on the same channel i. note that the channel is here assumed perfectly fair. that means, by fixing n i in (1), e[p(n i )] takes the same value for all individuals on channel i."
"cognitive radio [cit], with its capability to flexibly configure its transmission parameters, has emerged in recent years as a promising paradigm to enable more efficient spectrum utilization. spectrum access models in cognitive radio networks can be classified into three categories, namely exclusive use (or operator sharing), commons and shared use of primary licensed spectrum [cit] . in the last model, unlicensed secondary users (su) are allowed to access the spectrum of licensed primary users (pu) in an opportunistic way. in this case, a well-designed spectrum access mechanism is crucial to achieve efficient spectrum usage."
"in order to assess the relevance of a semantic feature type to the performance of a topic classifier, we analysed these metrics by considering the following cases: figure 6 presents the pearson correlation values obtained for each topic. the correlation was calculated between the entity difference scores and the performance of the cross-source (db + fb + t w) and single-source (t w(dbks+fbks)) classifiers in terms of f1 measure obtained using 80% of tw data for training (in addition to the ks data), and 20% tw data for test."
"1. the performance of a topic classifier can be accurately assessed following the proposed entropy-based measures. these measures when applied over a particular topic's concept graphs generated from multiple linked kss, outperform previous content based similarity measures derived from the sole text content."
"the choice of criteria and indicators for assessment of production management efficiency is the most critical aspect of this problem. this is confirmed by the experience of the subjects of management at all levels. in particular, within the framework of the conducted survey, 17.7% of employees of local power executive bodies and 13.1% of heads of enterprises and their units confirmed the lack of a wellfounded system of criteria for assessing the efficiency of management in their practice. this fact, in turn, generates a one-sided approach and the bias of the mentioned evaluation or even its absence at all."
"the quality of the feedback in the management process considerably depends on the quality of the information transmitted through the communicative network of the socio-economic system, and on the system for evaluating this process results. unfortunately, in evaluative practice, the feedback quality according to the survey is taken into account only by 28.2% of managers, and its importance, as an important factor in determining the efficiency of management, is understood only by 17.9% of respondents. the effectiveness of the feedback phase can be determined on the basis of the following criteria: a) the level of optimality of the evaluation procedure; b) the competence level of the evaluation subject; c) the level of information provision of the management process. the level of completeness and reliability of the achieved results assessment (k8) can be the generalized criterion for the listed in this group. it allows us to determine how far the feedback provides performance of the functions of assessing the effectiveness of management."
"further, we noticed that the coverage of entities is lower in the freebase than in dbpedia. for example from the total number of entities extracted by opencalais a large proportion (40%) of the entities were not found in the freebase ks, while in the case of dbpedia 35% of the entities were not assigned any uri. regardless of this, an improvement in f1 measure was obtained for both semantic graphs when combining the two linked kss. this thus indicates that the two linked kss complement each other well. in one hand, freebase brings its strength in content coverage for the topics, while dbpedia brings useful semantic evidence about the entities which are covered ( [cit] )."
"while the w-freq (semantic feature frequency) weighting function depends on the occurrences of features in a particular document, other generalised weighting information can be derived from a ks's semantic structure to characterise a semantic meta-graph. to derive a weighted semantic meta-graph the following w-sg weighting strategy is proposed."
"the main means of knowledge about the management process is its modeling, that is, the identification of the main stages and essential characteristics of this process, their reflection in a certain form and the establishment of links between them. schematically, the process of production management is presented in figure 1, which allows demonstrating its structural elements in interaction and highlighting the main stages: goal setting, its achievement and feedback."
"in order to achieve this, two main steps are first performed: (i) entity extraction -employing the opencalais 12 and zemanta 13 services for extracting the named entities in the documents; and (ii) semantic mapping -where the obtained named entities are mapped to their ks resource counterpart if it exists 14 ."
"from fig. 4 and fig. 5 one can further infer that a greater estimate error does not result in a slower convergence. regardless of the iteration block length in fact, nepct-pisap, gpct-pisap and ideal-pisap always converge within 125, 40 and 100 iterations respectively. with respect to the time however, that means that iteration block size is in direct ratio fairness convergence of pisap based on gpct for different iteration block sizes. each curve represents an average over 100 independent realizations to convergence speed. hence gpct-pisap convergence time can be calculated as 40 * p u slot length * block length, which in our case is 2s, 6s and 20s for iteration blocks of 100, 300 and 1000 pu-slots respectively. with a similar procedure we can calculate nepct-pisap convergence time, which takes the values of 6.25s, 18.75s and 62.5s for iteration blocks of 100, 300 and 1000 pu-slots respectively."
"in this section we conduct extensive simulations to evaluate the performance of nepct-pisap and gpct-pisap in terms of fairness, convergence speed and switching cost. in addition, the underlying techniques will be compared to the ideal case, referred to in the following as ideal-pisap, where the payoffs are perfectly known by the sus at each iteration."
"in particular, our framework proposes novel weighting strategies for the explored semantic graphs. these weights can further be used to filter on ks semantic features relevant to a micropost. this feature selection strategy also largely differs from state-of-the-art feature selection techniques [cit] ) used in text classification, as they typically make use of the scores obtained for the features based on the text content only (e.g. occurrences of a feature in training positive-and negativeclass training examples separately)."
"8. class-property entropy (class-property entropy): we measured this by taking the property bag for each class appearing in each topic derived from the resource metagraphs. in this context, low entropy indicates that a topic is less ambiguous, few properties spanning over multiple classes, while high entropy reveals high property diversity. the corresponding measure is defined as follows:"
the pre-processing steps for generating lexical features (i.e. bow) included: i) removal of stopwords; ii) transformation of each word to lowercase iii) stemming each word using the lovins stemmer ( [cit] ).
"we compared the performance of the proposed framework against several baseline models corresponding to state-of-theart approaches for tc. these baseline models employ three baseline features namely: bow, boe, part-of-speech (pos). these features are typical baseline features for tc and have been evaluated in previous work ( [cit] ). in addition, a new baseline feature set (bag-of-concepts (boc)) is also introduced. the boc feature set consists of a collection of opencalaisderived semantic classes which are assigned to entities. as opposed to the semantic classes from ks semantic graphs, classes derived with the opencalais service represent more generic concepts."
"to address the above research questions, we present an approach which facilitates the exploitation of multiple semantic meta-graphs from linked kss for tc of microposts. in particular, in contrast to our previous work ( [cit] ), in this paper our main focus is to understand the differences between the different semantic concept graphs, and present a comparative evaluation of these graphs at different stages of our three-stage approach. the main stages of our approach can be summarised as follows: i) context modelling; ii) topic classification and iii) topic similarity analysis."
"finally, in the third set of experiments, we look at the roles of the semantic features in predicting the performance of a topic classifier. for this reason we proposed and compared various entropy-based measures using the semantic features which characterise a topic. we then correlated these entropy-based measures with the performance of svm topic classifiers. in this case we investigate the questions of can we predict the performance of a topic classifier? which topic similarity measure provides a better estimate on the performance of a topic classifier?"
"the management process is considered to be started after the transfer of the decision by the management subject and its perception by the object of management. if this command is not accepted, the management process cannot pass the complete cycle. therefore, to achieve the efficiency of the management process, first of all, it is necessary to ensure the manageability of the object, without which it is impossible to implement management decisions. it is provided by the subject through the managerial influence on the management object on the basis of the scientific principles observance, implementation of managerial functions, through the optimal use of forms, methods, levers and instruments of managerial influence [cit] . management influence is primarily mediated through the management mechanism. the subsystem of criteria and indicators for evaluating its efficiency should be based on the evaluation of each its element (каpelushyna, 2016) ."
"the examined entropy-based measures make use of the enhanced representation of topics exploiting contextual information from semantic concept graphs about concepts from linked kss. this new representation led us to induce a new semantic feature space for a topic consisting of semantic features extracted from the semantic meta-graphs of multiple linked kss. our results on both single-source and cross-source scenarios show that this new semantic representation can be useful for providing a good estimate on the performance of a tc, achieving correlation values over 60% on the single-source scenario and over 70% on the cross-source scenario."
k3.1 − level of efficiency of the productive resources use; k3.2 − level of labour productivity; к4.1− optimality level of the organizational structure of management; к4.2 − level of security of the system of the competent personnel management; k4.3 -organizational level of managerial labour; k5.1 − level of the production structure optimality; k5.2 − level of production potential; k5.3− level of labour management within the production process; k6.1 − level of efficiency of the decision making mechanism; k6.2 − level of information provision of the management process; к7.1 − level of observance of principles and laws of management; к7.2 − level of operability and quality of management functions implementation; к7.3 − level of efficiency of administrative influence application methods; к7.4 − level of optimality of the management subjects' functioning style; k8.1− level of optimality of the evaluation procedure; k8.2 − competence level of the evaluation subject; k8.3 − level of information provision of the management process. the subsystem of criteria for evaluating the efficiency of management by results and goals
"one of the main factors which influence the performance of our approach, is the performance of the named entity recogniser (ner) used to extract the named entities from short text messages. in this paper we employed one of the most popular entity recognisers (opencalais and zemanta) for this purpose. although there have been several ner available ( [cit] ) for extracting entities from textual data, these approaches were built on newswire corpora, and therefore to date it is not well understood which provides the best performance on microposts. our future work will thus concentrate in evaluating our framework using other ners ( [cit] )."
"through addressing the question are there differences in the roles (generalisation patterns) of the concept graphs in the different topics and tc scenarios?, we compared the usefulness of the semantic features for two different scenarios: the crosssource scenario utilising ks data and the single-source scenario utilising only twitter data. our results in this respect revealed different roles (generalisation patterns) for the features derived from the two concept graphs. in particular, we found that some features from the category meta-graph were better used to encode the specificity of a topic, achieving the best performance in the single-source case, however, when considering the crosssource scenario other features derived from the resource metagraph were found to be better. nonetheless, despite the different roles of the features, our topic classifier exploiting multiple linked kss achieved significant results over the baseline models."
"the extraordinary complexity of calculating such models complicates their application in practice and increases the cost of evaluation. in this regard, minimizing their constituent elements and the relative simplicity of the calculation should become an important requirement for economic and mathematical models of production management efficiency. this requirement can be met by using the generalized criteria proposed for assessing the effectiveness of management."
"next, our second set of analyses aim to investigate whether there are any differences in the roles (generalisation patterns) of semantic features derived from the two semantic concept graphs in tc. in this case, we address the research question are there differences in the roles of the concept graphs in the different tc scenarios?"
the rest of the paper is structured as follows. section iii presents the system model followed by the formulation of the spectrum access game. section iv describes and analyzes distributed techniques for payoff calculation. section v discusses a natural enhancement to our work. section vi presents simulations results to evaluate the performance of the proposed techniques. section vii concludes the paper.
"following this process, different semantic meta-graphs are exploited from the different kss, and a set of semantic features derived, leveraging the rich semantic information about entities described in these semantic meta-graphs. this stage comprises two steps: (i) semantic meta-graph construction and (ii) semantic feature creation. in the following subsections we discuss each of these steps."
the described method for topic classification uses supervised svm machine learning models to detect the topic of microposts. these models make use of the lexical (bow) and semantic features extracted from different semantic meta-graphs.
"in the previous section we compared the overall performance of a topic classifier using semantic features derived from two semantic meta-graphs (resource meta-graph and category metagraph). in this section, we continue our discussion focusing on the differences in roles of these semantic features in different tc scenarios."
"4. the proposed criteria model for assessing the production management system efficiency can serve as a methodological basis for the further development of economic and mathematical modeling. taking into consideration the complexity of the management efficiency as a socio-economic phenomenon, its models must be objectively multicomponent (multicriteria)."
"the final stage of the framework aims to build supervised topic classifiers corresponding to the different cross-source scenarios, which make use of the generated ks semantic features. the support vector machine (svm) ( [cit] ) with polynomial kernel was selected as a base classifier, which we detail in section 8."
"7. entity-property entropy (entity-property entropy): similarly, we took the property bag for each entity mentioned in a topic based on the extracted ks graphs. in this context, low entropy indicates that the topic is less ambiguous, consisting of entities being associated to few properties, while high entropy refers to higher ambiguity at the level of entities."
"our distributed approach is based on imitation and more specifically on a recently proposed algorithm [cit] called pisap. we first give details on pisap, standing for proportional imitation-based spectrum access policies. the core idea behind pisap is the following: at each iteration, each su randomly selects another su; if the payoff (i.e. the throughput) of the selected su is higher than its own payoff, the su imitates the strategy of the selected su at the next iteration with a probability proportional to the payoff difference multiplicated by the switching rate."
"designing such topic similarity measures can be extremely important for a cross-source topic classifier, as they could help in providing an estimation of usefulness of a ks graph to previously unseen lexical data. one such example could be, the application of our model to a different genre, longer posts e.g. blogposts or facebook comments. another situation could be, in building a topic classifier for a new topic (e.g. politics), in which case we want to have an a priori estimate of the similarity between ks data and twitter data."
"unfortunately, current approaches still present some limitations. the majority of the approaches model the entities using very generic concept types. for example, in the case of the entity obama, the generic class person is considered. when detecting microposts related to the war topic, however, a more fine grained categorisation of this entity, such as president of united states (presidents of the united states), could be more useful."
"1. the semantic features derived from the resource metagraph and category meta-graph exhibit different roles (generalisation patterns) in the different tc scenarios. the class features derived from the resource meta-graph exhibit better generalisation patterns in the cross-source setting, while the category features derived from the category meta-graph are better suited to encode the specificity of a topic in a single-source setting 2. despite the differences in roles of the semantic features derived from the two semantic meta-graphs, incorporating semantic features from both semantic graphs is beneficial for tc, achieving performance superior to previous approaches utilising lexical features."
"the current framework employed two large coverage lod kss for demonstrating the usefulness of structured data in the tc task. however, lod contains many other kss interlinked with dbpedia, such as geonames 31 or musicbrainz 32 . a new lod ks can easily be integrated into the current framework, by exploiting the data (if available) for training a topic classifier, and the semantic information present in the ks's ontology as additional semantic features."
"overall, our approach demonstrated that semantic metagraphs derived from linked kss: i) provide useful semantic features helpful in accurately detecting topics in microposts ii) can be used as a measure for predicting the accuracy of a topic classifier."
"contrasting the results for all three topics, we observe, that the biggest overall improvement was achieved for the cri topic using the resource meta-graph. in particular, the db+fb+t w achieved an improvement of 31.4% over fb+t w. for the case of the category meta-graph, the db + fb + t w achieved an improvement of 30.9% over fb + t w."
"we have shown that computing payoffs (i.e., throughputs) can be problematic, and we have analyzed the performance of two payoff computation methodologies relying on csma/ca, namely the neighbors estimate-based payoff computation technique (nepct), relying on the estimate of the number of sus settling in the same channel at each iteration, and the greedy payoff computation technique (gpct), relying on the measured instantaneous throughput. we have simulated a scenario where the underlying techniques are used in conjunction with pisap and we found out that, although estimate inaccuracy affects negatively the nash equilibrium exactness reached by means of nepct-pisap, the latter achieves high fairness values even for very small iteration block sizes and clearly outperforms gpct-pisap in terms of both fairness and switching cost."
"we now discuss the issues and findings from each stage. 30 we also mention here that the inconsistencies for the entropy values (achieving both positive and negative correlations for a given entropy measure) may be the result of many different factors, such as the noisy lexical nature of microposts or the distributional differences between the ks and tw datasets in term of entities. in order to understand these variations, a more in-depth analysis would need to be conducted, which we aim to investigate in the future."
"the basis of a criteria model for assessing the efficiency the production management system is the assessment by management results and achieved goals set, which are mediated in the indicators of the production final results. it includes three main directions: 1) the assessment of the achieved goals; 2) the assessment of external production efficiency; 3) the assessment of the internal efficiency of production. another subsystem of the model contains an assessment of the operational efficiency of all structural elements of the management process and their optimal combination. it covers the following directions of assessment: 1) the assessment of the management system organization; 2) the assessment of the production organization; 3) the assessment of the efficiency of the goal-setting phase; 4) the assessment of the efficiency of the managerial influence; 5) the evaluation of the efficiency of the feedback phase. some criteria of the presented model can act as indicators for the assessment at the highest level of complexity."
the context modelling stage enriches the text using different concept abstraction techniques. for this reason we extract various semantic features about entities appearing in the text from two distinct concept graphs built from linked kss.
"in this section, we present the system model of our work, followed by the game formulation of the spectrum access problem, which serves as the basis of the analysis presented in subsequent sections."
"furthermore our goal is also to understand which semantic feature contributes to the performance of a topic classifier. for this reason we propose an algorithm for automatic estimation of accuracy loss of a topic classifier. we introduce novel topic similarity measures, which in contrast to our previous contentbased similarity measures ( [cit] ), aim to measure the similarity between the ks documents and microposts at a conceptual level, considering the enriched representation of these documents."
"similarly, we computed the category property entropy for each topic. in this context, low entropy indicates that a topic is less ambiguous, few properties spanning over multiple categories, while high entropy reveals high property diversity. the corresponding measure is defined as followed:"
where n x ( f ) is the number of times feature f appears in all the semantic meta-graphs associated to document x; and f is the semantic feature's vocabulary. this weighting function captures the relative importance of a document's semantic features against the rest of the corpus; while the normalisation prevents bias toward longer documents.
"where n(r (cls) is the number of resources whose type is either cls or a specialisation of cls's parent classes. this measure captures the relative generalisation of a property p to a broader set of specialised sibling classes derived from cls, and its computation is independent of the entity e. in this case the generality of property dbowl:leader given the class yago:president for the db graph is computed as:"
"by exploring the research question how does the performance of a topic classifier vary using different concept graphs?, we found that although both semantic concept graphs contain useful information for tc of tweets, the best overall performance was achieved by the features derived from the resource meta-graph. more importantly, for both concept graphs, we obtained a significant improvement over previous approaches using only lexical features derived from the single twitter dataset content."
"the above provisions allow raising the question of disclosure of the essence of the production management efficiency in two dimensions: 1) the efficiency which is expressed in final results of economic activities (for company management systemin the final results of production) and 2) the management process efficiency. these two dimensions are in a close dialectical relationship, provided that the leading role is strategic efficiency. this conclusion was confirmed empirically by means of a survey, which found that 62.8% of respondents supported the proposed methodology. because we use \"two dimensions\" methodological approach, the system of criteria for evaluating the efficiency of production management will also consist of two subsystems: 1) a subsystem of criteria for evaluating the management system by results and objectives; and 2) a subsystem of criteria for evaluating management process that more closely reflects the figure 2 ."
"looking at the performance of the baseline models, we observe that the best performance was achieved by the boe features, which performed better than the boc and bow features. further, the pos features did not improve on the baseline model using only bow features. an explanation for this could be that the language in tweets is quite complex, and exhibits less regularity than longer texts used from kss (ks abstracts)."
"for generating the semantic features we used the boe features derived from a document. for each entity feature we looked for its resource representation in both kss (dbpedia and freebase). using these ks resources we then generated different semantic meta-graphs (i.e. g db and g fb ) as indicated in subsection 4.2.1. in addition, for generating the semantic meta-graphs we disregarded properties which contained general information about an entity. examples of such properties from dbpedia include: rdfs:comment, abstract, wikipageexternallink, and from freebase include: type/object."
"despite of the accuracy gain obtained with the p and cls features for the db+fb+t w classifier, an interesting observation about these results is however, that the semantic features do not always improve upon the baseline models. for instance in the case of db + fb topic classifier, the results are comparable or slightly worst than those obtained by the bow feature set ignoring semantic augmentation. an explanation for this could be that the distribution of entities in the db and fb datasets may slightly be different to the one in twitter. further given that these classifiers do not make use of any microposts data, this mismatch provides challenges for the topic classifier. a possible reason for this could be the level of ambiguity of the entities in the different datasets. in order to capture the differences between the datasets and provide an estimation on the usefulness of the different semantic features, the reminder of the reader, we employed a set of topic similarity measures which we will examine in subsection 8.3."
"we now turn to the analysis of the switching cost, i.e., the global number of channel switches. due to the drastic cost of changing frequencies in current wireless devices in terms of delay, packet loss and protocol overhead, an efficient channel access policy should avoid frequently channel switching, unless necessarily. fig. 7 shows the nepct-pisap and gpct-pisap switching cost trend as a function of the iteration t for different iteration block lengths. ideal-pisap trend is also plotted by for performance comparison. we observe that nepct-pisap clearly outperforms gpct-pisap for any iteration block size. the slope of the curved induced by nepct-pisap are in fact noticeably less steep than the ones induced by gpct-pisap. as a consequence, if for instance we fix the iteration block length to 100 pu-slots, after 500 iterations the charged switching cost with nepct-pisap and gpct-pisap is approximately 50 and 300 respectively."
"there is little work on classifying blogposts into topics ( [cit] ). husby and barbosa [cit] demonstrated that selecting data from freebase using distant supervision, in addition to incorporating features about named entities is beneficial for tc."
we start our analysis by assessing the usefulness of the different semantic meta-graphs in both single-source tc (section 8.1.1) and cross-source tc (section 8.1.2) scenarios.
"in this paper we thus present an extension of our tc framework [cit], which exploits this new semantic graph, called category meta-graph, providing a more fine grained classification of concepts based on their topics. we introduce a set of novel semantic features derived from this graph, and present a comparative evaluation against those obtained from the resource metagraph."
we classify the related approaches into two main strands: research pertaining to generic topic detection approaches and the use of kss for linking topics to microposts.
"the linked open data (lod) cloud 2 consists of a large number of interlinked kss, covering a range of different topics. among these kss, dbpedia 3 ( [cit] ) and freebase 4 [cit] ) constitute some of the largest datasets built in a collaborative manner. the main advantages of these kss are: i) they provide plentiful amount of data on a growing number of topics, ii) they contain factual information about a large number of entities, covering these topics. this semantic information is also structured according to their own ks ontology."
"the final stage topic similarity analysis uses the enhanced representation of the documents (in both the kss and twitter) following context modelling to provide an estimate on the performance of the topic classifier on new, unseen micropost data. this allows us to analyse which semantic concept graph is better suited to measure the topic similarity between ks documents and microposts for tc. in this stage, we also investigate whether this novel representation of the documents provides a better measure for topic similarity than our previous content based statistical measures ( [cit] )."
"we propose two throughput computation methodologies: 1) greedy payoff computation technique (gpct): the sus simply associate their payoff to their measured instantaneous normalized throughput, i.e. they divide at each iteration the number of packets being successfully transmitted by the iteration time t . 2) neighbors estimate-based payoff computation technique (nepct): the sus exchange information about estimated average throughput. this can be accomplished if at each iteration t each su j settling in channel i computes his own payoff in the following manner:"
"comparing the different enrichment strategies, we observed similar trends for both resource meta-graph and category metagraph. the best enrichment that consistently improved over the baseline for both concept graphs was the w-sg for p, indicating that encoding the specificity of a property for each semantic concept graph is beneficial for tc. for the w-freq features, however, we found that in the case of the resource meta-graph, the semantic augmentation by feature frequency (cls(w-freq)) and by generalisation (parent(cls)(w-freq)) ( table 7, column 8) worked consistently better than the baseline models. however, in the case of the category meta-graph, the performance of the cat(w-freq) and parent(cat)(w-freq) were only comparable to those of the baseline models."
"the aim of the study is to justify the methodological approach to constructing a criterion model for assessing the efficiency of the production management system, which would allow objectively and comprehensively to implement the assessment of the management efficiency and to ensure the quality of feedback in the management process."
"in addition, we explore the role of the semantic features derived from these different semantic graphs in the representation of a topic. for example, looking at the micropost in figure 1, and the semantic representation of the resource barack obama in figure 2, we can observe that the semantic features derived about obama's resource (barack obama) can be indicative of the topic war. our aim is thus to investigate whether the different semantic structures of a ks can aid in identifying which semantic features are more representative of this topic. for this reason, section 5 introduces different metrics for analysing different semantic feature-based topic similarity measures."
"the degree of achievement of the set goals is a key feature of an effective management system and its determining characteristic [cit] . this direction of assessment takes into account the target aspect of management efficiency. therefore, the success of achieving the objectives is proposed to use as a generalized criterion of assessing the effectiveness of the management system in this direction (к1). it characterizes the strategic aspect of management effectiveness. in practice, it is the degree of achievement of the goals set in the plans and programs of production development, is used by 71.2% of the polled managers. also 31.4% of respondents take into account the number of eliminated deficiencies and solved problems in the current period."
"1. semantic meta-graphs (both resource meta-graph and category meta-graph) built from kss contain useful semantic features about entities for tc. in particular, incorporating semantic features about properties (p) using our novel class-property co-occurrence weighting schema (w-sg) proved a significant improvement over previous stateof-the-art approaches."
"we define an iteration t as a block of pu-slots of fixed duration t during which the sus don't change their strategy. at the end of each iteration, sus obtain a payoff which corresponds to the achieved throughput. we assume that such information is sent in a specific field of the packet header such that it can be seized by any su in the system."
"the remainder of this paper is structured as follows: section 2 provides an overview of the employed linked kss, and explains their relevance for tc of microposts; section 3 presents the related work on tc. section 4 then provides an overview of the original tc framework employed in this work, and describes its extension for the newly introduced category meta-graph. section 5 continues by introducing a novel set of topic similarity measures. next, section 6, presents the gold standard datasets used in our experiments, and section 7 describes the baseline models employed. after that, in section 8 a comparative evaluation of the extended tc framework and new topic similarity measures are discussed. this is followed by a discussion on the shortcomings of our approach and possible future extensions in section 9. finally, conclusions are drawn in section 10."
"for example for the obama entity, considering the dbowl:leader property and yago:president class, the specificity value of dbowl:leader in the dbpedia graph g db is computed as follows:"
"1. topic-class bag entropy (class entropy) : we took the class bag for each topic derived from the resource metagraphs and measured the entropy of that class bag, capturing the dispersion of classes used for a particular topic. in this context, low entropy indicates a focused topic, while high entropy indicates an unfocused topic, which is more random in the subjects that this topic discusses. we define this measure as follows:"
"in the previous section we presented different semantic features extracted from two semantic meta-graphs, which can be used to enhance the representation of documents with additional contextual information. while feature expansion can be beneficial in some cases, in others it rather undermines the performance of a topic classifier. in order to understand the relevance of the proposed semantic features to the performance of a topic classifier, in this section we study different topic similarity (also called domain similarity [cit] ) measures which can provide an estimate of the usefulness of these structures for tc."
"looking at the performance of the baseline models, we observe a different trend compared to the t w only scenario. the syntactic classes provided by the pos taggers, in this crosssource scenario, were found to be more beneficial, compared to the bow cases. while for the boe and boc features, we did not obtain an improvement on the baseline bow features. an explanation for this could be that the entities which appear in the t w dataset could be quite different from the entities appearing in the ks data for each topic, in which case exploiting the semantic information from kss seems to be more beneficial."
"these figures show that in the cross-source (db + fb + t w) scenario, the entity-property entropy yields the best correlation scores, over 70% in two out of three topics. when looking at the values obtained for class entropy, category entropy, property entropy and entity entropy measures, we observe, that the class entropy showed the highest correlation values with the performance of the cross-source topic classifiers. for the disacc and war these values were higher than 54%, however, for the cri topic the correlation values were 11%. when examining the class dispersion measures, we see that the entity-class entropy showed higher correlation than class entropy. in the case of the category dispersion values, for some topics (e.g. disacc) the entity-category entropy was found to be better, while for others (e.g. war) the category entropy was more beneficial. moreover, among the property dispersion values the entity-property entropy values showed the highest correlation values."
in the following we present an overview of the weighting strategies applied for the different semantic features derived from the resource meta-graph and category meta-graph:
"in this section we present a series of experiments to evaluate the tc framework and topic similarity measures using the two semantic meta-graphs introduced in subsection 4.2.1. in particular, these experiments aim to compare and contrast the results obtained for the resource meta-graph (used in our previous experiments [cit] ) with the results obtained for the newly introduced category meta-graph. 28 the motivation behind the selection of these discriminative models, is that they correspond to typical baseline methods used in cross-source (multi-source) learning ( [cit] ). previous approaches on single-source tc are not directly comparable with our current setting and results. for instance, the majority of the generative lda based approaches (e.g. twitterlda) were trained on unlabelled data only using simple bow features. however our approach exploits labelled data from kss, and further focuses on the evaluation of the usefulness of different semantic features for tc."
"for incorporating the presented ks semantic features into a topic classifier, this framework employs different weighting strategies for the semantic features and feature combinations, as well as different semantic augmentation strategies for extending the initial feature spaces of both ks and twitter documents. in this section we review these two strategies -originally proposed in ( [cit] ) -and present their adoption for the category meta-graph."
"where n p (r(cls)) is the number of times property p appears in all resources of type cls in the ks graph g ks, and n(r(cls) is the number of resources of type c in g ks . this measure captures the probability of the property p being assigned to an entity resource of type cls."
"while employing the p features have been shown to provide a positive gain over the baseline features for most of the topics, the usefulness of the semantic features and augmentation strategies merely depend on a number of factors. for instance, one of the factors which influences the performance of a tc classifier is the number of entities identified in a micropost. for instance, in the case of the war topic, a higher number of entities have been extracted than for the other two topics. this can explain the higher gain achieved for this topic, resulted from a larger number of microposts being enriched. further, the lower performance achieved by the cls features, could be due to the level of ambiguity (measured as cls/ent) of the cls features and their discriminative power for a given topic. looking at the table 5, it can be observed that there are a larger number of property features defined in kss for an entity (prop/ent) than for a class (cls/ent, f bcls/ent). this allows the incorporation of very fine grained information into tc, which indeed seems to improve the performance of the classifier upon the baseline features. in order to capture these factors and provide an insight into the usefulness of these features for topic classification, the reminder of the reader, we employed a set of topic similarity measures which we will evaluate in subsection 8.3."
"in conclusion, considering the results obtained for both single-source and cross-source scenarios for the various semantic features derived from the three ks graphs, our findings are as follows:"
"it is impossible to achieve a higher level of management efficiency if the management object is not well organized. in this regard, the organizational level of the management object (the production system as a participant in the management process) should be evaluated by analogy with the assessment of the organizational level of the management system according to the following criteria: a) the level of the production structure optimality; b) the level of production potential; c) the level of labour management within the production process. the level of production organization is a generalized criterion for assessing the organization of the management object in relation to the above presented criteria (k5). this is supported by management practice, particularly 51.3% of managers use this criterion, and 20.5% of them take into account the number of failures in the functioning of the management object due to errors in its organization. the goal-setting phase is very important for the management process, because the mission, strategy, tactics, goals and objectives of the operation and development of the control object are developed. the goal-setting process is relatively independent, which determines the peculiarities of its evaluation. like any economic process, the stage of goal-setting generates its own results and has its own implementation mechanism [cit] . therefore, the main criteria for assessing its effectiveness are: a) the level of efficiency of the decision making mechanism; b) the level of information provision of the management process; c) the level of observance of principles and laws of management. the level of decisions taken quality (k6) is generalizable in relation to other criteria of this group. the majority (67.1%) of the polled managers pointed out its importance, and 74.4% of respondents consider the number of implemented ideas that brought positive results."
these measures will be examined in section 8 by correlating them with the performance of different topic classifiers. our approach was evaluated in the emergency response and violence detection domains. the following section introduces the datasets in which the proposed framework and topic similarity metrics were tested.
", where p(cls j ) denotes the conditional probability of a concept cls j, within the topic's concept bag cls t . dispersion of the entities in each class. that is, low entropy indicates that the topic is less ambiguous, consisting of entities belonging to few classes, while high entropy refers to higher ambiguity at the level of entities."
"to evaluate the usefulness of exploiting this new category meta-graph for both tc and topic similarity, we present an extensive analyses of our extended framework using a ground truth data in the emergency response (er) and violence detection (vd) domains."
"these feature spaces were also reduced by considering only the top 5 entity classes and top 5 properties derived from the different ks graphs for each opencalais' entity type (e.g. person). the same strategy was used for reducing the number of category features. the statistics of the lexical and semantic features derived for these datasets is summarised in table 5. 23 http://www.w3.org/tr/rdf-sparql-query/ 24 for each given topic (e.g. cri) then the number of positive dbpedia instances is 1,000, and the number of negative dbpedia documents equals to 8,465. 25 freebase, http://frebase.org 26 for each given topic (e.g. cri) then the number of positive freebase instances is 1,000 and the number of negative freebase instances is 15,915. comparing the statistics obtained for the resource metagraph and category meta-graph, we observe that the frequency of dbcat categories are generally higher than those of dbowl and yago classes. in addition, the average number of distinct categories for an entity (cat/ent) double the number of distinct classes per entities (cls/ent). this indicates that the categories form much larger clusters than the classes."
the following section introduces a set of metrics we proposed for analysing the relevance of these semantic features to the performance of a topic classifier.
"currently, the economic science has formed two main approaches to determining the efficiency of production management. the first one is based on simple logic: if production is productive, profitable, then the management system can be assessed as efficient. another approach to determining the efficiency of production management involves an assessment of the direct contribution of the management to the results of production [cit] ."
"3. each subsystem involves the use of appropriate assessment criteria grouped in specific areas of evaluation. each of the directions has a certain generalized criterion. the empirical verification of the evaluation criteria revealed the basis for the correction and strengthening of individual components of the model, which are often underestimated in practice. in this way, the proposed criterion model acquires a comprehensive coverage and a systematic approach to assessing the efficiency of production management."
"further, the use of fine grained information in kss provided by the category meta-graph has been exploited for many other problems, such as document classification [cit], entity disambiguation [cit], and semantic relatedness [cit], and shown that it carries rich semantic information. however, to date no study has been conducted to investigate the usefulness of this category meta-graph graph structure for tc."
"the closest task to our multi-label topic classification task is topic detection which aims to assign a set of topics or labels to a given micropost. recent approaches for topic detection on twitter stream can be classified into: descriptive characterisation of microposts, topic models, and classification models."
"in addition, these methods model the content of text using simple 1-gram (unigram) features. a possible extension of our approach could be to incorporate other ngram features into these models also, for e.g. 2-grams or a combination of 1-grams with 2-grams ( [cit] )."
"in our work, each su j is modeled as a rational decision maker, aiming at load-balancing the total system throughput. the instantaneous throughput it can achieve in terms of packets per second, denoted as t j, can be expressed as a function of µ sj and n sj, where s j denotes the channel which j chooses, n sj denotes the number of neighbors on the same channel s j . the expected value of t j, which has to be intended as the long-term throughput when t is very large, can be written as:"
"in this paper, we focus on the generic model of cognitive networks consisting of multiple frequency channels, each characterized by a channel availability probability determined by the activity of pus on it. in such model, from the individual su's perspective, a challenging problem is to compete (or coordinate) with other sus in order to opportunistically access the unused spectrum of pus to maximize its own payoff; at the system level, a crucial research issue is to design efficient spectrum access protocols achieving optimal spectrum usage."
"higher generality values indicate that a property spans over multiple classes, and is less specific to a given class cls. these two measures (generality and specificity) of a property p to a given class cls are combined as follows:"
"the previous sections analysed the benefit of using semantic features derived from ks graphs for the topic classification task in both single-source and cross-source scenarios. these results have also shown that there is variation in the performance levels between topics. this suggests that differences between the ks and twitter datasets affects the performance levels. in order to understand these variations, we analysed the relevance of these semantic features for the representation of a given topic. for this reason, we computed the entropy difference values between the training and test datasets for each topic as introduced in section 5."
"as depicted in figure 3, our framework makes use of multiple linked kss (from lod) for tc of microposts. the main stages of this framework can be summarised as follows: 1) dataset collection and content modelling 2) context modelling 2.1) dataset enrichment 2.2) semantic feature derivation from different semantic meta-graphs"
"topic classification this section continues with the description of the results obtained for the cross-source tc case. in particular, it compares and contrasts the results reported in our previous work for the resource meta-graph ( [cit] ) with the results obtained for the category meta-graph introduced in this paper."
"the efficiency of managerial influence at the stage of attaining the set goals should be evaluated, in our opinion, according to the following criteria:: a) the level of observance of principles and laws of management; b) the level of operability and quality of management functions implementation; c) the level of efficiency of administrative influence application methods; d) the level of optimality of the management subjects' functioning style. at the same time, the effectiveness of the management mechanism, first of all, is manifested in the level of the management object control (k7), which is determined by the results of achieving the goals, and therefore is generalizing with respect to other criteria of this group. the high level of ruling the object of management provides the most complete and qualitative achievement of the set goals, which will be confirmed by such indicators as high final production results."
"to provide context for our motivating example, we first present statistics of the kss used in this paper (i.e. dbpedia and freebase), summarised in table 1. the first ks, dbpedia (dbks), is derived from wikipedia 5 . in dbpedia each resource is harvested from a wikipedia article which is semantically structured into a set of dbpedia (dbowl) 6 and yago2 (yago) 7 ontologies, with the provision of links to external knowledge sources such as freebase, opencyc 8, and umbel 9 . the wikipedia articles are furthermore grouped into categories, which are represented using the skos vocabulary 10 . the dbpedia dump version 3.8 classifies 2.35 million resources into dbpedia's ontology classes (dbowl). these classes comprises 359 distinct classes, and 740,000 skos categories (dbcat), which form a subsumption hierarchy and are described by 1,820 different properties. conversely, the yago ontology ( [cit] ) is a much bigger and fine grained ontology. it contains over 447 million facts about 9.8 million entities which are classified into 365,372 classes, and 104 manually defined properties. in contrast to dbpedia, freebase (fbks) is a large online knowledge base which users can edit in a similar manner to wikipedia. in freebase, resources are also harvested from multiple sources such as wikipedia, chefmoz, nndb and musicbrainz 11 along with data individually contributed by users. these resources are semantically structured into freebase's own ontologies (fbont), which consist of 1,450 classes and more than 7,000 unique properties."
"the assessment of the external effectiveness of the object of management is based on the establishment of the completeness of meeting the public needs, which form the goals of production, since the needs are primary in relation to the goals [cit]"
a positive correlation indicates that the performance increases as the entropy difference decreases (the distributions are more similar); while a negative correlation indicates that the performance increases as the divergence increases (the distributions are less similar).
"we tackle the problem of opportunistic spectrum access in csma/ca-based cognitive radio networks from an evolutionary game theoretic angle. the motivation of applying evolutionary game theory in the study of the spectrum access problem is twofold. first, it is a powerful tool to study the interaction among players and the system dynamic in terms of population. stemmed from classic game theory and darwins evolution theory, it can explicitly capture the fundamental relationship among competition, cooperation and communication, three crucial elements in the design of any spectrum access protocols in cognitive radio networks. second, evolutionary game theory provides a theoretic tool for the design of distributed channel access protocols based on local information which is particularly suited in decentralized environments as cognitive radio networks."
"where p is the fixed packet length (made up of the physical header, the mac header and the payload) and δ is the propagation delay."
"the presented semantic meta-graphs (both resource metagraph and category meta-graph) are capable of providing contextual information about concepts in short text. our method for tc makes use of various semantic features that are constructed from these semantic meta-graphs. by extracting the named entities we were able to enhance the lexical feature space of a topic classifier with additional contextual information about these concepts. in addition, our approach takes into account the information about concepts (e.g. resource type-hierarchies, resource properties) present in multiple semantic concept graphs of multiple linked kss."
"following this approach, each tc system was evaluated using 5-fold cross-validation. the training dataset for the t w tc system consisted of 80% of the original twitter data. for the ks classifier the training set consisted of the full ks data. for the ks + t w classifier the full ks data was combined with 80% of twitter data. using weka software 29 we applied different discriminative classifiers including the maximum entropy, perceptron and support vector machine. after comparing the results of these classifiers we found the svm with polynomial kernel to be that which achieved the best results. therefore, in this paper we only report results for the svm classifier."
"moreover, when building the cross-source topic classifiers, our models still require a large number of annotated tweets to outperform the single-source twitter models. previous research on cross-domain (cross-source) learning has also shown that outperforming the target (twitter) classifier is extremely difficult for many text classification tasks ( [cit] ). in order to further increase the effectiveness and robustness of the current model, our future work in this direction will thus focus on investigating unsupervised multi-source adaptation models which require less annotation from twitter ( [cit] )."
"the duration of one pu-slots is fixed. a small initial part of it, of fixed length as well, is used by the sus for sensing the presence of the pu. it is assumed that if a packet is partially transmitted at the end of the pu-slot, the transmission can be reactivated, without loss of information, at the first pu-slot which is senses non-occupied by the pu."
"we let the sus access the selected channels at each iteration through the distributed coordination function (dcf), which is a carrier sense multiple access with collision avoidance (csma/ca) scheme and binary slotted exponential back-off 2 . the related random backoff algorithm (defined in the ieee 802.11 dcf) is designed to give each host a fair chance of obtaining the channel under contention."
"these insights have provoked our final question can we predict the performance of a topic classifier? to address this question, we introduced and evaluated various entropy-based measures defined over these semantic concept graphs and showed that the performance of a topic classifier can be predicted with reasonably high accuracy using the property dispersion entropy measures. further, we showed a significant improvement over previous content-based lexical similarity measures proposed for tc."
this weighting function computes how specific and how general a property is to a given class or category based on a set of semantically related resources derived from a ks's graph.
"the achievement of a higher level of production efficiency directly depends on the qualitative influence of the management on the production process. therefore, the organization's management seeks how to ensure, first of all, its own efficiency, which is determined by external factors, as well as the optimal functioning of all structural elements of management system itself. the assessment of production management efficiency is one such element. it enables the feedback control of the management process and adapts the organization to changes in the external and internal environment."
"social media posts, and in particular microposts collected from twitter, have been found to contain useful information for many applications including disaster detection [cit], seasonal mood level changes [cit], tracking influenza rates [cit], box-office revenue forecast [cit], political elections [cit], stock market prediction [cit], etc. [cit], microposts were found to provide early warning signals of violent events; such events were reported much faster than traditional media sources 1 . the real-time identification of such events could be extremely important to authorities and emergency professionals by allowing such parties to immediately respond."
"consider the two micropost examples displayed in figure 1 . in these examples, the entity obama is mentioned in two different contexts, each of them corresponding to different roles this entity plays (for e.g. president in microposts (1); and husband in micropost (2)). in such cases, the role of this entity is defined by the contextual information provided on the content of each microposts. in this paper, our main goal is to exploit this semantic contextual information about entities. in particular, we study different semantic graph structures defined in linked kss, and provide a comparative evaluation of their usefulness for tc of microposts. section 4 introduces our approach and the semantic metagraphs exploited in this work."
"where cost(w) represents the resource consumption and quanterr(w) represents the detection error caused by quantization error. fore w (i,j) and fore ref (i,j) were, respectively, the detected foreground pixel value when adopting fixedpoint and floating-point data format. rate thresh was the threshold value of detection success rate, which was defined as"
"for an accurate evaluation towards the computational consumption of the ct algorithm, we rewrote the program on eclipse with c language using no dependent libraries like opencv. the algorithm program ran on a pc (intel core i7-4712mq, 8 gb ddr3 ram, windows 7 64-bit) and a zynq embedded system (arm cortex-a9 dual-core 667 mhz, 512 mb ddr3 ram, linux 3.2.16), respectively. the time consumption data of the ct algorithm at different resolutions was as shown in table 1 . because the ct algorithm adopted the random haar-like feature model and the dimension of feature vectors to be processed by the classifier was static, the real-time performance of ct has little relationship with the image resolution. without program optimization, the processes of integral image calculation, compressive sensing and naïve bayesian classifier cost 20.4%, 11.9% and 64.7% of the whole time when running on the computer, respectively. the corresponding percentages when running on the arm were 17.8%, 18.2% and 60.0%, respectively. the process of compressive sensing contains a great amount of floating-point multiplication and add operations. the naïve bayesian classifier mainly involves exponent and logarithm operations, which are equivalent to floating-point multiplication operations according to taylor series. the pure software solution on arm could only provide a processing rate of 6.0 fps on average, which could not meet the requirements of robotic applications. hence, hardware accelerating measures shall be taken on these time-consuming processes to ensure realtime performance."
"similar to lasso catheter detection, the pattern of blob positions can be used to distinguish the cs catheter from other catheters or guidewires. cs catheter detection should be applied after lasso catheter ring detection, if both catheters appear in the x-ray image. as all cs catheters used in our clinical cases contain 10 electrodes and electrodes are nearly evenly spaced, a cost function is developed as following:"
"though the detection and tracking system designed in the section 'zynq-7000 soc-based low-power real-time tracking system' was able to meet the functional requirements of our robots, it had obvious drawbacks in fpga resource utilization rate for two reasons. on the one hand, the resource consumption of an fpga-based image ps is large in nature due to the high data volume of a digital image. on the other hand, customized accelerators used in the system were designed with high-level languages. limited by existing compilation techniques, this design method consumes much more resources in exchange for reliability and user friendliness. table 4 shows the fpga resource utilization of the two customized accelerators without optimization. because other essential logic modules of the system also consumed a few fpga resources, these two accelerators cannot be deployed on the pl at the same time. moreover, the stability and the performance of an fpga system may decrease if using more than 70% of the total logic resources. to reduce resource and power consumption, techniques including word length optimization and dynamic reconfiguration were adopted."
"to the best of our knowledge, this is the first embedded design to implement subtle tracking algorithms on a single soc for robotic applications. moreover, the proposed heterogeneous computing architecture provides a feasible solution for mobile vision systems. the design techniques presented in this article, including hardwaresoftware co-development, word length optimization and reconfigurable customized accelerators, may promote the practical use of state-of-the-art tracking algorithms like tld and mil."
"zynq combines advantages of arm and fpga and overcomes the communication problem between the two sections. thus, it provides a feasible solution for control and processing systems of small-scale autonomous mobile robots. 28 however, as far as we know, few studies were made to fully tap its potential in robotic vision. focusing on the application problem of the amphibious spherical robot, a zynq-7000 soc was used to construct the robotic tracking system in this article. a sophisticated heterogeneous computing system architecture, which took full advantages of its characteristics, was proposed to ensure the real-time performance of the vision system. moreover, optimization methods including dynamic reconfiguration and word length optimization were designed to further reduce the power consumption and to enhance the system flexibility."
"in this article, a low-power real-time detection and tracking system was designed and implemented for our amphibious spherical robot. given the unique mechanical structure and the specialized application scenarios of the robot, a novel soc-based heterogeneous computing architecture was proposed for implementations of gaussian background model-based detection and ct algorithms. under the presented architecture, the main part of visual algorithms was realized as software programs running on the arm subsystem, while compute-intensive processes were realized as hardware accelerators running on the fpga subsystem. moreover, dynamic reconfiguration and word length optimization were adopted to improve the versatility, adaptability and resource efficiency of the proposed system. experimental results confirmed that the proposed system had advantages of lightweight, low power consumption and good real-time performance, which was capable of meeting application requirements of our amphibious spherical robot. its good real-time performance could also meet future demands of the robot in biological monitoring and multitarget tracking."
"to determine intraobserver variation, the same clinical expert was asked to perform the manual annotation again on 400 randomly selected x-ray images. the error distances between the first annotation and second one are 0.19 ae 0.09 mm and the maximum error is 0.36 mm. therefore, the result indicates that there were no significant intraobserver differences. to determine interobserver variation, the second clinical expert was invited to perform the manual annotation on the same randomly selected dataset. the error distances between the first expert's annotation and the second expert's annotation are 0.22 ae 0.11 mm and the maximum error is 0.43 mm. both annotations were used as the gold standard and two sets of average error distances between a detected object and one of corresponding goldstandard objects were calculated. the p-values of t-tests were calculated for two sets of average error distances. the pvalues was 0.24, which indicates that there were no significant interobserver differences."
"besides, the c/cþþ language for the ps development and the verilog hdl (vhdl) for the pl development have become de facto standards in hardware-software codevelopment, thus programs under this architecture had great portability. and the ps can be used either alone or in conjunction with the pl. so lower power consumption can be obtained by switching unused devices or logics into low-power mode."
the tip detection precision is also evaluated. the error is defined as the distance between the detected guidewire or catheter tip position and manual annotated tip position. the error is 0.57 ae 0.38 mm.
"moreover, high-speed advanced extendable interface (axi) buses provide low latency data exchange channels between the two sections, which makes it efficient to transfer partially processed data inside the heterogeneous system. axi ports connecting the ps and the pl, including four gp ports, four high-performance (hp) ports and an accelerator coherency port (acp), provide a data transfer rate of up to 8 gbps. according to different characteristics of these axi ports, they can be used to connect different instantiating intellectual property (ip) cores for various applications. as shown in figure 2, the ps usually accesses configuration registers of peripherals on the pl through axi-gp ports in low-speed applications such as motor control. axi-hp ports are suitable for high-speed applications in which an ip core needs to access the double data rate (ddr) random access memory (ram), for example, video stream processing or image acquisition. because the axi-acp port is able to access the ddr and supports coherency with the cpu cache, it can be used for applications needing a shared work spaces between the software and the hardware, such as interactive image processing."
the lasso body detection has the highest detections error and highest missing rate. this is because the main body of lasso catheter is more frequently overlapped with other catheters than the cs catheter.
"on the other hand, the pl consumes a much higher percentage of dynamic power than the ps. so resources of the pl in working mode also shall be reduced for lower power consumption. as to the proposed system, the detection subsystem and the tracking subsystem did not work simultaneously because of the detection-then-tracking workflow."
"in order to evaluate the performance of knn classifier, both false positive (fp) rate and true negative (tn) rate are calculated among all test images (4726 images) which are from treatment of chd. fp is defined as the image artifacts are classified as wire objects and tn is defined as the wire objects are labeled as image artifacts. the results show that tn rate is 4.1% and fp rate is 7.4%. some artifacts are classified as wire objects because they are overlapped with a wire object or they are from contrast agent injection (fig. 13) ."
"however, if none of detected wire objects satisfy those two conditions, the unused line segments from centerline segmentation algorithm will be searched. if one of them meets the two conditions, the wire path algorithm will be used to trace the path from the lasso ring to one of image edges."
"a multiscale vessel enhancement filter 29 is used to enhance the visibility of wire-like structures in the x-ray images. it is based on the idea of approximating wire-like objects, such as tubular or cylindrical structures. 29 in order to detect wire-likes structures, the vessel filter algorithm finds the local coordinate system aligned with the wire and use the curvature in these directions (x or y axis) to classify different structures. this involves the following five steps:"
"according to the analysis results in the section 'analysis and evaluation on compressive tracking algorithm', acceleration mechanisms were designed for the processes of compressive sensing and naïve bayesian classifier to achieve real-time performance. the process of compressive sensing is actually a sparse matrix multiplication operation. thus, it can be speed up with the advanced single instruction, multiple data (simd) or 'neon' engine, which is a floating-point coprocessor extension to the ps. because the neon engine supports 16-channel paralleled multiply-add operations, the calculation process could be greatly accelerated and the cpu load could be decreased. the process of naïve bayesian classifier can be denoted as"
"as shown in figure 1, the amphibious spherical robot consisted of a waterproof hemispheric upper hull (diameter: 250 mm), in which electronic devices and scientific instruments were installed, and two openable quarter-sphere lower shells (diameter: 266 mm). in the land mode, the robot walked with four legs. in the underwater mode, it swam with water jets. [cit], an improved version of the amphibious spherical robot was proposed using 3d printing technology and adding sensors including gyroscopes, accelerometer, global position system and cameras. 12 different from most existing mobile robots or autonomous underwater vehicles, the robot was able to work in complex and narrow environments like coral reefs and pipelines. 13, 14 due to the unique mechanical structure and the specialized application scenarios, designing a tracking system for our amphibious spherical robot was a challenging task. first, the load space of the robot was very narrow and was designed as enclosed for waterproofing. thus, a high-speed computer or workstation, which is usually in large size and generates a great deal of heat, was not suitable for this small-scale mobile robot. second, the robot was powered by lithium batteries, the total capacity of which was 4800 mah. so the power consumption of its robotic vision system shall be considered to ensure enough work range. third, future applications of the robot include biological monitoring and multi-robot collaboration. the robotic vision system may need to track multiple targets or a high-speed target like fish. therefore, the real-time performance of the tracking system shall be especially considered. besides, precision and effectiveness of the adopted tracking algorithm should be acceptable to meet the requirements of robotic applications like visual servoing. to address issues mentioned above, a low-power real-time tracking system build upon embedded processors was essential for applications of our amphibious spherical robot."
"the heterogeneous computing architecture of the lowpower real-time tracking system proposed in this article was as shown in figure 3 . the major parts of software and digital hardware of this system were integrated on a single xilinx zynq-7000 soc. the software concerning system control and serial processes of algorithms was running on the ps which provided an embedded linux environment. the hardware, including the image acquisition logic, the customized image accelerator logic and other digital interfaces or logics, was deployed on the pl."
step #3 select a random solution around the searching center and calculate corresponding successful rate. w try w center þrangeá(rand-0.5) rate try 1-quanterr (w try )
"hybrid or heterogeneous systems integrated advantages of multiple solutions and have been widely used in realtime vision applications in recent years. in general, cpu-gpu is the most popular hybrid solution in the field of computer vision for its excellent performance. 19 however, most cpu-gpu heterogeneous systems are fabricated as computers or workstations, which have high power consumption and are not suitable for small-scale mobile robots. moreover, programs of vision algorithm shall be carefully optimized to use cpu and gpu simultaneously for a high utilization rate. besides, the program portability problem between some types of nvidia gpus still exists."
"to overcome the high-computing load problem, the mainstream solution of real-time visual tracking systems is implementing algorithms with graphic processor units (gpus), 15 digital signal processors (dsps), 16 fpgas 17 or application-specific integrated circuits (asics). 18 however, tracking systems built upon a single specific purpose processor or pl device have drawbacks in real-time performance, developing difficulties, cost and extendability, respectively. that limited their applications in batterypowered or multiple functional platforms like autonomous mobile robots."
"the eq. (6) is very efficient to compute as the computational cost of dot product of two vectors is very low (compared with curve fitting or curve derivative calculations). the computation of local curvatures was only carried out in the region of two end points for each line segment. pt 1 and pt 2 should not be neighbor image pixels along line segment as curvature computation could be distracted by incorrectly identified endpoints which arise from image noise. instead, there should be a gap of 3 or 4 image pixels between pt 1 and pt 2 ."
"in order to recognize artifacts, a k-nearest neighbor (knn) algorithm is used. instead of applying knn on the whole image to detect artifacts, a localized knn image classifier is developed to process only wire-like objects, which are obtained from the previous centerline segmentation step. first, 500 sample images of artifacts (negative data) and 500 sample images of target objects (positive data) are manually located and labeled from training image dataset. the orientation vectors are computed and sample images are organized around the detected main axis. then, the sample images were flipped along the x and y axis to create a total of 4000 sample images for both positive and negative data. this not only increases the number of training samples, but also solves the asymmetrical problem."
"the major computational load of the proposed detection framework is the vessel enhancement filter algorithm. the software of the proposed framework was developed using opencv and the performance was evaluated on an intel core i7 2.9 ghz laptop with a single-threaded cpu implementation. the low level image processing functions of opencv are accelerated by using the intel ipp library which provides high performance low-level routines for imaging. currently, a frame rate of 15 fps is achieved for cs catheter detection, guidewire, and guiding catheter detection without the knn classifier. a frame of 12 fps is achieved for simultaneous detection of cs and lasso catheters. the frame rate is reduced to 11 fps for guidewire and guiding catheter detection with the knn classifier."
"real-time performance test results of the tracking subsystem were as shown in table 3 . image sequences at the resolution of 320 â 240 and 640 â 480 were entered into the tracking subsystem; 4.3 times speed-up was achieved on the process of compressive sensing using neon engine; 65.2 times speed-up was achieved on the process of naïve bayesian classifier using the fpga-based customized accelerator. the processes of naïve bayesian classifier cost only 15.1% of the whole time when adopting the heterogeneous computing technology. a tracking rate of up to 76.5 fps was achieved with neon and the accelerator at the resolution of 320 â 240. for the reason that the image resolution does not have great effects on the computational consumption of the ct algorithm, a tracking rate of 46.5 fps was achieved at the resolution of 640 â 480. although it decreased by 40.4%, it still met the real-time performance demands of the robot."
"to reduce development time and ensure hardware reliability, accelerators used in the proposed system were implemented with vivado hls tools. the processes in the foreground detection stage mainly concerned pixel-based processing, which is data independent and easy to be parallelized on the fpga. besides, vivado hls provides some commonly used functions for image processing, such as erode and dilate. these functions usually performed better than self-designed ip cores because they have been highly optimized by xilinx inc. in accordance with the hardware platform. thus, operations in the foreground detection stage, the erode operation and the dilate operation were realized in the accelerator of gaussian background model, and other operations in the target marking stage were realized as programs running on the ps. figure 4 shows the major workflow of the detection subsystem. after booting up, the detection program would do initialization works including loading kernel modules, resetting hardware, and so on. later, preprocessed grey images were buffered in the ddr3 ram and then transferred to the gaussian background model accelerator. the computed binary image was returned to the program for potential moving object detection. if an eligible moving object was found, then it would be specified as the target to be tracked. finally, the fpga area of the accelerator was reconfigured and the tracking subsystem was launched. figure 5 shows the primary structure of the accelerator of gaussian background model. the 320 â 240 8-bit grey images were serially read into the ip core from an axistream port. data of an image was buffered into a slice of block ram (bram) through an axi-stream first in first out (fifo). after receiving an image, the online gaussian background model computation process was started. to reduce resource consumption and background noise, the original image was resized to 160 â 120 before executing pixel-based foreground detection. the resize, erode and dilate functions were realized on the basis of the video function library provided by vivado hls. finally, the computed binary image was sent out to an axi-stream port. the multipliers and adders inside the detection loop were highly parallelized. 2.7 times speedup was achieved under this heterogeneous architecture, as shown in table 2 ."
"visual tracking is an active research topic in the field of computer vision with robotic applications ranging from visual servoing, automatic navigation and robot-human interaction. given the initial state (e.g., position and scale) of a specific target in the first frame of a video or an image sequence, a visual tracker seeks to estimate the states of the target in the subsequent frames. some state-of-the-art tracking algorithms including tracking-learning-detection (tld), 1 multiple instance learning (mil), 2 structured output tracking with kernels (stuck) 3 and l1 tracker using accelerated proximal gradient (l1apg) 4 have been proposed in recent years. although numerous tracking algorithms have been proposed, 5 it still remains a very challenging task to design a low-power real-time tracking system for mobile robotic applications. [cit] on the one hand, most embedded processors for mobile robotic applications have a relatively weaker computational ability compared with multicore central processing unit (cpu) in workstations, which leads to difficulties for real-time image processing. on the other hand, visual tracking algorithms have to process images successively, which covers processes of image preprocessing, appearance modelling, motion estimating, target locating and model updating. 5 consequently, it results in a great number of compute-extensive operations."
". some of failed classifications are illustrated in fig. 12 . the proposed detection framework has been tested on 102 image sequences (10,624 images). the remaining 610 images are used as training data for finding optimal weights and training knn image classifier. among 63 clinical cases, 42 cases are cardiac electrophysiology (ep) interventions and remaining cases are cardiac catheterization procedures for treating congenital heart defects (chd). 3521 x-ray images from ep cases contain one cs catheter and 2377 images contain one cs catheter and one lasso catheter. remaining test images (4726 images) are from treatment of chd and images contain maximum two target wire objects such as guiding catheters or guidewires."
"after reading the (nþ1)-th frame, the detector tried to judge whether a pixel belonged to a moving target or the static foreground with [ row,col,n, s row,col,n ]. in the target marking stage, erode, dilate, connected region analysis and other morphological image processing methods were used to extract potential moving object from the foreground image. finally, the object larger than a threshold value would be marked as the target to be tracked."
"to establish ground truth for evaluation, electrode catheters, guiding catheters or guidewires are manually annotated by a clinical expert. an annotated object starts from the edge of the image and ends at its tip. for lasso catheter ring, the manual annotation is a closed curve. they are used as the ground truth for accuracy tests. the detection precision is defined as the average of shortest distances from points on a detected object to the corresponding annotated object. for the technique to be acceptable in clinical practice, failed detections are considered to be the ones where any points on the detected object has larger errors than a preset threshold (e.g., a threshold of 2.5 mm (around 6 image pixels) 24 is used in this evaluation), which is the average diameter of guidewires or catheters) and it also corresponds to the size of the smallest target structures for cardiac catheterization procedures."
"the wire path reconstruction algorithm used in this paper assumed that the catheters and guidewires are relatively rigid so that the path created from multiple line segments (section 2.b) is smooth. the proposed algorithm probably will not work on very flexible wires such as pacing wires in cardiac resynchronization therapy. however, the main applications for the computational framework is for motion compensation and 2d to 3d registration. both applications require rigid objects to work with."
"according to the desired functions of the system in different working stages, the reconfigurable regions of the pl can be configured as corresponding devices or peripherals."
"if the x-ray image contains a lasso catheter, the pattern of blob (electrode) positions from the vessel enhancement filter can be used to distinguish the lasso catheter ring from other catheters or guidewires. the physical shape of the lasso catheter ring (fig. 8) is a closed circular curve. when it is perspectively projected onto a 2d imaging plane by the x-ray system, the projected shape becomes a circle or an ellipse. under some particular view orientations, the ellipse can collapse to a line segment. to accommodate different shapes of lasso ring, blob grouping and ring object selecting based on a cost function were developed."
"in our system, an ov7670 coms camera was adopted and configured to capture 320 â 240 16-bit rgb images at 30 fps. in the image preprocess module, acquired colour images were converted into 320 â 240 8-bit grey images and were then transferred to the ddr3 ram. the whole working process of the system was divided into two stages: the detection stage and the tracking stage. in the detection stage, the gaussian background model-based detection program was running on the ps to sense moving object entering the field of view. and the accelerator of gaussian background model was programmed to the reconfigurable area of the pl. the detector would mark the target to be tracked once it found an eligible moving object. after that, the reconfiguration operation would be executed to program the accelerator of the naïve bayes classifier to the reconfigurable area. then, the ct program would be launched. in the tracking stage, the ct program was running on the ps to successively locate the target specified in the detection stage. the partial bit stream files and linux driver modules to be used in the reconfiguration operation between two stages were stored and managed in the file system of the ps. compared with conventional heterogeneous system architecture for image processing like cpu-gpu and dsp-fpga, the presented architecture in this article has two remarkable properties."
"the accelerator of naïve bayes classifier was also implemented using vivado hls tools. figure 7 (b) shows its primary structure. in the proposed system, the dimension of feature vectors m was set to 50 to ensure better discriminative characteristics. the sampling radius g was set to 15, which led to a candidate patch number of up to 768. figure 7 (a) shows the format of data stream transferred from the ps to the accelerator. classifier parameters, the number of candidate patches and feature vectors were serially received from an axi-stream port and then buffered into a slice of bram via an axi-stream fifo. a threestage pipeline was constructed in the loop for classifier response calculation. the layout of arithmetic units was designed to be symmetrical to realize parallel computing. finally, the computed maximum classifier response and the corresponding patch number were sent out to an axistream port."
"furthermore, in cardiac catheterization procedures for treating congenital heart defects, the first step is to push a guiding catheter toward the area to be treated, which therefore, a minimum energy method is used to search a smooth path for the completed catheter or guidewire. the cost function for the minimum energy method is defined as:"
"the image acquisition logic was composed of a camera interface module controlling the camera, an axi-dma module transferring acquired images to the ddr3 ram through an axi-hp port and an image preprocess module completing image enhancement operations. the customized image accelerator logic was centred on a reconfigurable image accelerator, which is an fpga-based digital circuit executing specific time-consuming operations of image processing algorithms. the image accelerator can be reprogrammed online by the ps through the processor configuration access port (pcap). two axi-dma modules were used to realize bidirectional data transfer between the accelerator and the ddr3 ram through the axi-acp port."
"where c is the distance between pt 1 and pt 2 . angle a can be computed from angle b, which is defined as"
"after blob grouping, principle component analysis (pca) is used to compute the center of the estimated enclosing circle and two principle vectors. the first principle vector is the direction of semimajor axis of the ellipse. the second vector is the direction of semiminor axis [ fig. 9(a) ]. to estimate the radius of semimajor axis, all blob points are projected onto the first principle vector and the radius is half of the distance between two end points. the radius of semiminor axis is estimated as the maximum distance between blob points and their projected points on the semimajor axis. to remove outliers, all blob points are converted into the polar coordinate system by using the center as the reference point. any blob point with a sudden large change in radius will be removed. after removing outlier blobs, pca as well as ellipse equation are recalculated."
"switching functions of the system, while the partial reconfiguration only reprograms related logic components and does not break work of others. in comparison, the partial reconfiguration has more advantages in efficiency and is suitable for the proposed architecture. accelerators used in the proposed system were packed into ip cores with the same ports to realize dynamical partial reconfiguration, as shown in figure 3 . four steps were executed to realize reconfiguration between the detection and tracking stages. first, after specifying the target to be tracked, the program backed up related information into the ddr3 ram. second, components connecting to the accelerator were temporarily set to idle state in case of time sequence problems. three, the partial bit stream file was written to the pl through the pcap interface. finally, components connecting to the accelerator were restarted to recovery work. table 6 shows experimental results of dynamical reconfiguration. two banks of the pl were allocated to the black box, in which accelerators would be loaded. other components like axi-dma were deployed outside the black box. because of the size of the bit stream file, the full reconfiguration time was nearly twice larger than the partial reconfiguration time and the image acquisition interval, which may lead to tracking failure. thus, partial reconfiguration was more practical for fpga-based realtime vision applications."
"in comparison with the ps, functional modules running on the pl exist in the form of digital circuits, which leads to two properties. on the one hand, the dynamical reconfigurable property of the pl can be utilized to improve the adaptability of the heterogeneous computing system."
two metrics are used to evaluate the precision of the proposed system. the first metric is the success rate of the benchmark sequences. the success rate of a frame is defined as
"this paper presents and validates a novel and real-time catheter and guidewire detection framework. the proposed framework does not require any user interaction or prior models. the novelty of proposed framework is the combination of blob detection and wire detection without adding extra computational workload. combining the electrode (blob) pattern and wire pattern makes the framework robust against other wire-like objects or image artifacts. therefore, it can efficiently and robustly work on low-dose x-ray images. the framework also enables the fully automatic recognition of cs and lasso catheters based on the pattern of electrodes. as the proposed framework detects both electrode positions and the entire length of a catheter, it opens up more clinical applications. for example, the wire body of a cs catheter can be used for both automatic 2d to 3d registration 10 and motion compensation 9 in image guided applications for cardiac interventional procedures. both electrode points and the wire body of a detected catheter could be also used together in the accurate 3d reconstruction of the detected catheter. electrode points are used as key points for phase matching in biplane image sequences. 33 the biplane image sequences are the two image sequences acquired in different acquisition angles. phase matching is to match the cardiac motion and respiratory motion phases, which can largely reduce 3d reconstruction errors. the accurate 3d models of catheters could provide enhanced functionality during procedures for guidance and also for postprocedural analysis."
"the system proposed in this article also had some inevitable drawbacks. the detection and tracking precision of the proposed system was directly determined by the adopted vision algorithms. thus, the detection subsystem could only process videos with static background. the tracking subsystem had the drift problem which limited its precision and would finally lead to tracking failures. consequently, the detection and tracking results in the experiment section were not entirely accurate. another problem was that the study in this article mainly aimed at the design and implementation of the robotic vision system. robotic application functions like visual servoing and autonomous navigation was not realized yet. our future study will try to improve the tracking precision using state-of-the-art theoretical tools like the conventional neural network. we will also focus on robotic applications and intelligent functions of the amphibious spherical robot. table 9 . principles of gaussian background model-based detection."
"as all catheters or guidewires are inserted from the blood vessels in arms, legs or necks into the heart, wire path reconstruction algorithm could start with the longest line segment which is close to the image boundaries. then, the algorithm computes the energy costs (eq. (7)) for all line segments within the maximum searching distance and the one with the lowest energy cost is selected. repeat the previous step until no more line segments are within the maximum searching distance or stopping criterion is triggered. the stopping criterion is cosa\\0, which means that the wire path is going to make a sharp turn (a [ 90 ). the wire path reconstruction algorithm can be repeated several times in order to extract multiple wire objects. examples of path reconstruction for electrode catheters and guidewires are shown in fig. 17."
"where rc is the radius of the semimajor axis of the detected lasso ring. r short is the radius of the semiminor axis. rc is the average radius of lasso ring, which is computed from training image dataset (section 2.c), in which all lasso rings have been manually annotated. n is the number of blobs found in the blob group and n la is the expected number of electrodes. in eq. (8), when the lasso ring collapses to a line segment, r short rc % 0. in this special case, the cost function has two parts. the first part is to test whether the size of detected ring is close to the average radius or not. the second part is to compute the density of blobs on the line segment. the collapsed lasso ring has higher density of blobs. finally, the ring-like object with the lowest score is selected as the detected lasso catheter ring."
"first, the communication between the cpu and the coprocessor or the accelerator is completed through onchip buses rather than external interfaces like pcie, which leads to lower data latency and a more concise structure. thus, the processing capability and the stability of the embedded system are ensured, which is meaningful in applications of autonomous robots."
"to compute the local curvature efficiently, an approximation method was developed. in mathematics, the local curvature is defined as 1/r, where r is the radius of an approximation circle which is fitted with the curve. as shown in fig. 6, the approximation circle touches the curve on the point where the local curvature calculation is required."
"the proposed system measures 118 â 98 â 45 mm and weighs 125 g. an agilent 34410a multimeter controlled by c# programs was used to evaluate its average power consumption by continuously measuring the current and voltage value. test results show that the total power consumption was around 2.99 w. considering that the battery capacity of our spherical robot was 4800 mah, it can work in detection or tracking mode for no less than 8 h."
"guiding catheters have larger errors and higher missing rates when the knn image classifier was not integrated into the detection framework. this is due to lower image contrast for guiding catheters compared with guidewires as guiding catheters are mainly made from plastic materials and guidewires are made from high density metal wires. therefore, under x-ray, guiding catheters have lower image contrast. the wire path reconstruction algorithm was sometimes distracted from surrounding image artifacts (fig. 16 gives an example). however, after the knn local image classifier removed majority of image artifacts, guiding catheter, and guidewires has similar results."
the rest of the article is organized as follows. an overview on our amphibious spherical robot and existing lowpower real-time visual tracking solutions is provided in the section 'related works and application requirements'. ct algorithm is analysed and evaluated in the section 'analysis and evaluation on compressive tracking algorithm'. design details of the proposed detection and tracking system are elaborated in the section 'zynq-7000 soc-based low-power real-time tracking system'. optimization design of the system including word length optimization and dynamical reconfiguration is described in the section 'optimization design of the proposed visual tracking system'. experiments and evaluations on the proposed system are conducted in the section 'experimental results and discussions'. section 'conclusions' describes conclusions and follow-up research works. an appendix with the principles of the ct algorithm and the gaussian background model is also included.
"if the remaining wire body of lasso catheter needs to be detected, existing detected wire objects which were found by wire path reconstruction algorithm are checked first. two conditions must be met if the wire is detected as the body of lasso catheter. (a) one of its end points must be connected to the lasso ring. (b) if one end point connects to the ring, its tangent vector v end ! must be aligned with the tangent vector v la ! from the lasso ring. it can be calculated as"
"although the vessel enhancement filter can enhance and detect wire-like objects, it could still be affected by some image artifacts, such as calcium deposits and rib bone boundaries (fig. 10) . for cs catheter and lasso ring detection, electrodes on the wire provide reliable information to distinguish target objects from image artifacts. for guidewires or guiding catheter, it is necessary to recognize as many artifacts as possible so that the wire path reconstruction algorithm will not be distracted by image artifacts."
"however, as far as we know, most ct-based tracking systems were running on personal computers (pcs) or workstations. 31 because the computational consumption of the ct algorithm is still too large for most embedded platforms, there were no related application cases of mobile robots."
"in order to reduce computational complexity, and achieve real-time detection speed, after applying the vessel enhancement filter, the image is binarized using otsu's method. 30 otsu's method is a nonparametrized and adaptive algorithm as it automatically determines the thresholding level based on minimizing the intra-class variance. otsu's method has been used together with vessel enhancement filter for coronary sinus segmentation on x-ray images. 31 a thinning algorithm 32 is applied to the binarized image and the results are one-pixel-wide skeletons. those skeletons are the centerlines of wire-like objects and they have to be broken down into line segments before wire path reconstruction. the first step of segmentation is to find the branch points and end points of the skeleton. a branch point is the pixel which has more than two neighbors of skeleton pixels in connected 8-neighbours. end points only have one neighbor [ fig. 3(a) ]. figure 3 presents the flow chart for the centerline segmentation algorithm."
"in the other hand, cs catheter has higher detection errors and higher missing rate but the successful rate is also higher if compared with the lasso catheter detection. the failed cases often happen when only part of cs catheter was visible within the x-ray images. the detection framework completely missed the cs catheter and detected the wrong catheter. the other failed cases are caused by detecting the wrong part of catheter when the cs catheter is overlapped with other catheters. two examples of successful detection are shown in figs. 14 and 15 gives two failed examples."
"data exchange between the ip cores was completed through axi4-stream buses. the ps controlled the work mode of these ip cores by accessing control register banks via axi-gp ports. the axi ports used for dma transfers of the image acquisition and the image accelerator were separated to avoid bandwidth competition. considering that the data exchange or interactive operations between the software and the accelerator may be frequent, the axi-acp port was assigned to connecting the customized image accelerator logic."
step 2) aligning the orthogonal coordinate system with the local features in the image by forming and decomposing the 2 9 2 hessian matrix at every image pixel. the hessian matrix h x;s consists of second order derivatives that contain information about the local curvature. h x;s is defined such as:
"the size of sample images is 20 9 40 pixels. figure 11 gives some examples. there are some bended wire images in positive data, which gives some flexibilities for knn to recognize bended wires or catheters. all positive and negative sample images are filtered by sobel filter and normalized so that they all have the same value of the average intensity. this step is to prevent the wrong classification which is caused by the difference in the average intensity between two images. sober filter is used to enhance the edges, which are the important feature to distinguish wires and artifacts. the wires has two strong edges in both sides. on the other hand, artifacts often have just one strong edge and the other edge is weak (see fig. 11 )."
"the proposed framework can be also used for detecting guidewires and guiding catheters. however, the electrode pattern might not be found in the target object. in order to distinguish between the wire-like target objects and image artifacts, a localized machine learning algorithm (knn classifier) is used. as the knn classifier has low fp and tn rates, it can reliably identify image artifacts and reduce the influence of artifacts for wire path reconstruction algorithm. however, there could be other real wire objects such as metal wires left by the open-heart surgery ( fig. 11) or ecg cables. the knn classifier cannot identify them as artifacts as those wires has smooth edges on the both sides after applying sobel filter. however, those wires only distract the path reconstruction algorithm when they are closely overlapping with the target wire object."
"moreover, localizing guidewires and catheters can facilitates optimal collimation. 11 collimators control the field of view by using thick blades to block part of x-ray radiation leaving the source to the patient. currently, collimation is controlled manually by human operator and it causes interruption to the clinical work-flow. detecting locations of guidewires or catheters can automatically narrow down the region of interest [ fig. 1(b) gives an example]. therefore, real-time catheter and guidewire detection, is essential for many image guided applications, such as motion compensation, 2d3d image registration, and x-ray dose control using collimation. nonetheless, developing an accurate and robust real-time detection method is a challenging task as the majority of x-ray fluoroscopic images are low quality."
"the third part of eq. (3) is to minimize the con dist distance which is between two end points from two line segments. similarly, the fourth part of eq. (3) is to minimize the dev dist distance which is between an end point and its projected point on the tangent vector. as shown in fig. 7, v (3) is the maximum distance for searching candidate line segments. in order to accommodate the low-contrast guiding catheters or guidewires, the maximum searching distance is set to 10% of image width or image height (whichever is longer). an example of path reconstruction of the low-contrast wire guiding catheter is shown in fig. 17(e) . to find out the optimal weight parameters (w 1"
"the proposed computational framework is divided into four steps: path reconstruction. (d) object classification. for detecting and recognizing electrode catheters, an object classifier using electrode (blob) positions is added after the third step. for detecting guidewires or catheters without electrodes, a knn classifier is added into the third step."
where curv1 is the local curvature in the end point ept 1 in line segment a and curv2 is the local curvature in the end point ept 2 in line segment b (fig. 5) .
"second, partial reconfiguration is adopted to dynamically switch functions of coprocessors or accelerators deployed on the fpga, which results in superiority in two aspects. on the one hand, the adaptive capacity of the system to ever-changing tasks is extended because the ps can easily reprogram the coprocessor online to meet requirements of different tasks or work stages. on the other hand, available fpga resources are extended by multiplexing in time domain, which reduces power consumption and system cost."
"where roi t is the tracked bounding box, roi g is the ground truth bounding box and area(á) denotes the number of pixels in the region. if the score is larger than the given threshold (0.5 in this article) in a frame, it counts as a success. the second metric is the centre location error which is the euclidean distance between the central points of the tracked bounding box and the ground truth bounding box. experimental results on precision are as shown in table 7, which verified that the proposed system provided an acceptable detection and tracking precision."
"the tracking subsystem was designed to successively determine the bounding box of a target, the initial state of which was specified by the detection subsystem. the ct algorithm was adopted in this subsystem as mentioned in the section 'heterogeneous architecture of the real-time tracking system'."
"(1) in the precision test phase, the proposed system was tested with two benchmark image sequences, namely bike (320 â 240, 119 frames) and walking (768 â 576, 140 frames). the image sequences were stored in the file system of the proposed system and were read by the implemented visual algorithms. the detection and tracking results were compared with the counterpart of the original matlab programs of gaussian background model and ct. figures 10 and 11 show the detection and tracking results, respectively. in the detection mode, the down-sampling process, the dilate operation and the erode operation eliminated the detection noise caused by background disturbances. then, the contour and position of the moving target was located by analysing the connected region. in the tracking mode, the tracking subsystem was configured by dynamic reconfiguration. then, the ct tracker tracked the target with a rectangle until the target was lost."
"to verify the validation of the proposed heterogeneous system, an myirz-turn core board carrying zynq-7000 soc (xc7z020) and an ov7670 camera was adopted to implement the detection and tracking system elaborated in the sections 'zynq-7000 soc-based low-power real-time tracking system' and 'optimization design of the proposed visual tracking system'. three phases of experiments were conducted to test the detection and tracking precision, real-time performance and power consumption of the tracking system."
"overall, the detection errors for guidewires and guiding catheters are 0.62 ae 0.48 and successful rate is 83.5%. the individual errors for guidewires or guiding catheters were given in table ii . similar to catheter detection results, both raw results and results after removing failed cases were calculated. the results with or without a local knn image classifier were also calculated."
"in conclusion, a fully automatic detection framework is presented for the detection of the electrode catheter as well as guiding catheters and guidewires in real time. the framework not only detects the target wire objects, but also the electrode positions of catheters. this could lead to wider clinical applications."
"where v 2 r m represents the compressed feature vector of a candidate patch and pos, neg, pos and neg represent parameters of the classifier. because the calculating process mainly concerns exponent and logarithm, so a customized accelerator is more suitable to speed up the process. thus, the function of the naïve bayesian classifier was packed into an ip core in the proposed system. figure 6 shows the major workflow of the tracking subsystem. after getting the initial state of a specified target from the detection subsystem, the tracking subsystem would launch and do initialization works. after sampling candidate patches from an acquired grey image and calculating the integral image, the tracking program would call the neon engine to complete the process of compressive sensing. then, feature vectors of candidate patches were sent to the naïve bayesian classifier accelerator deployed on the pl. according to the output of the classifier accelerator, the target position can be located and would be used for updating classifier parameters later."
"in this paper, we propose a fully automatic catheter and guidewire detection framework. two object classifiers were used in the detection framework. the first classifier is based on blob detection and is embedded in the vessel enhancement filter with no additional computational cost. this classifier is used to target electrode catheters. in addition to recognize the type of electrode catheter based on the electrode pattern, the classifier also reduces the influence of wire-like artifacts. the second classifier uses a k-nearest neighbor (knn) algorithm to distinguish between target wire objects and image artifacts. it is used for detecting guidewires or catheters without electrodes. the computational costs of both classifiers are low and they can be used in real-time applications."
"as a by-product of the vessel enhancement filter, electrode (blob) positions can be detected as image pixels where the ratio between two eigenvalues is close to 1.0 (k 1 % k 2 ). enabling blob detection adds no additional computational cost for the filter as the eigenvalues of hessian matrix h x;s are always computed inside the vessel enhancement filter. an example of blob detection can be found in fig. 2 ."
"cardiac catheterization procedures are routinely carried out under x-ray fluoroscopic guidance to diagnose and treat heart diseases such as atrial fibrillation, congenital heart defects, coronary artery diseases, and more. the procedures generally involve catheters and guidewires, which are visible in the x-ray images as they are made from high density materials. however, soft tissues are hardly visible under xray. to overcome this problem, static (3d) roadmaps can be overlaid onto x-ray images to add anatomical information. 3d roadmaps are the anatomical models of soft issues which can be generated from preprocedural computed tomography (ct) images, [cit] magnetic resonance (mr) images, 4, 5 or rotational x-ray angiography (rxa) images. [cit] however, the accuracy of 3d roadmap guiding systems rely on the accuracy of: (a) respiratory and cardiac motion compensation, and (b) registration between the 2d x-ray images and the 3d roadmap. localizing catheters and guidewires provides more information to help increasing the accuracy of both motion compensation and 2d3d registration. for example, during cardiac electrophysiological (ep) procedures, localizing electrode catheters such as coronary sinus (cs) catheters and lasso catheters, which are routinely used, could be employed to correct respiratory motion. 9 furthermore, the cs catheter in the x-ray image could be used as a reference to increase the overlaying accuracy of the 3d roadmap ( fig. 1(a) gives an example). the applications of real-time detection of catheters and guidewires are not limited to ep procedures. in cardiac catheterization procedures for treating congenital heart defects, 3d heart models together with blood vessels models could be automatically registered with 2d x-ray images if the locations of catheters or guidewires are detected. 10 those soft tissue models could then be overlaid onto live x-ray images to aid device deployment in the targeted location."
"currently, the proposed framework has achieved the frame rate of up to 15 fps. this frame rate is considered as real-time for cardiovascular intervention procedures as the average maximum frame rate for modern intervention x-ray systems is 15 fps. in the low dose x-ray image setting, the frame rate will drop to average 7.5 fps to reduce x-ray radiation doses. therefore, our detection framework is sufficient for using in real-time imaging applications. furthermore, if the detected models of catheters or guidewires could be fed into a template-based tracking method, it not only can produce an even faster tracking method but also can largely increase the accuracy and successful rate compared with the original detection framework. because the proposed frame work does not require any user interaction or prior models, it could autonomously detect and recognize all wire objects if knowledge about catheters and guidewires such as the number of electrodes, catheter tip size, guiding catheter diameter and etc. are passed onto the detection framework. this could lead to a context-aware detection framework. for example, it can automatically recognize the stage of procedures as in the different stage of procedure different catheters are used."
filtering performance of sample 2: (a) dsm before filtering; (b) true dtm generated using reference ground points; (c) extracted dtm generated using the filtering results.
"for algorithmic tractability for designing beamformers for message sharing, a sum power constraint over 7 bss is adopted so that the average power spectral density at each bs antenna is -27dbm/hz. for the pure message sharing scheme, we adopt the design methodology of section iii-a, namely fixing cooperation cluster size for each user, picking the bss according to channel strength, and using the wmmse approach [cit] for designing beamformers. the backhaul capacity is calculated once the user rates are determined. for compression and hybrid schemes, we adopt the design methodology of section v, except the initial network-wide beamformers are chosen using the wmmse approach with full cooperation over 7 cells. fig. 4 shows the cumulatively distribution function (cdf) of the user rates for the three schemes. in the simulation, weighted sum rate maximization is used as the optimization objective with weights updated according to proportional fairness criterion. it can be clearly seen that both the pure compression and the hybrid schemes significantly outperform the message-sharing scheme. in particular, the hybrid scheme with 350mbps backhaul achieves about the same user rates as the message-sharing scheme with 862mbps, which represents a saving in backhaul capacity by about 60%. further, the hybrid scheme is also seen to outperform the pure compressionbased scheme, improving the rate of the 50th percentile user by about 10% at the same backhaul."
"to find the appropriate regularization constant α, we can set it heuristically depending on snr, or solve (p2) for different α's and pick the one that maximizes the weighted sum rate."
"research in statistical machine translation has begun to turn to semantics. effective semantics-based translation systems pose a crucial need for a practical cross-lingual semantic representation. one such schema, abstract meaning representation (amr; [cit] ), has attracted attention for its simplicity and expressive power. amr represents the meaning of a sentence as a directed graph over concepts representing entities, events, and properties like names or quantities. concepts are represented by nodes and are connected by edges representing relations-roles or attributes. figure 1 shows an example of the amr annotation format, which is optimized for text entry rather than human comprehension."
"1: design fixed network-wide beamformers using, for example, the regularized zero-forcing approach; 2: assuming pure compression, optimize the quantization noise level in each backhaul link, obtain the user rates; 3: use algorithm 2 to select users for message sharing."
"in a second set of simulations, we consider a larger network with 19 cells wrapper around, 3 sectors per cell, and 10 users randomly located in each sector. the central 7 bss (i.e., the central 21 sectors) form a cooperation cluster; the outof-cluster interference produced by the rest of bss is taken into account. here, we impose a more realistic per-bs power constraint equivalent to -27dbm/hz over 10mhz, and use regularized zero-forcing with per-bs power constraint to find the initial beamformers in compression and hybrid designs. fig. 5 shows the average per-cell sum rate of the proposed hybrid scheme as compared to the compression based scheme as a function of average per-cell backhaul capacity. as can be seen from the figure, the hybrid scheme improves backhaul utilization as compared to the compression scheme. the improvement is prominent when the backhaul capacity is small and the gap decreases as the backhaul capacity increases. the maximum achievable rate with infinite backhaul capacity using regularized zero-forcing beamforming and the no-cooperation baseline are also plotted for reference. it can be seen that at an operating point of 85mbps per-cell sum rate, which is about 90% of the full cooperation rate, the hybrid scheme requires a backhaul of 150mbps, while the pure compression scheme requires 180mbps. thus, the hybrid scheme achieves a saving of about 20% in backhaul capacity requirement."
"noise. every site's attractiveness always has a noise component, representing sources of transient error. the noise term is computed according to equation 3, taken from the cognitive architecture act-r [cit], where n is a parameter that we tune (table 1), and r is a random value between 0 and 1."
"the flow chart of the proposed method is shown in fig. 1 . the outliers are first detected and removed since their abnormal elevation values will affect the subsequent extraction of ground points. to reduce the intervention and release human resources, this paper applies multi-scale 89368 volume 7, 2019 morphological operations to extract and label the training sets automatically. then, a svm model is built using several calculated features. according to the svm model, the candidate points can be classified as ground points and nonground points. the key for active learning is to define the oracle. in this paper, the oracle is set as a sigmoid function of the residuals from the candidate points to the fitted surface generated using ground points in the training sets. in the iteration, some points from the candidate ground points and non-ground points are selected and labeled as ground points and non-ground points separately according to the oracle. subsequently, the newly added train samples update the training sets. meanwhile, these points should be removed from the candidate points. as a consequence, the svm model can be revised iteratively using the updated training sets. the remaining candidate points are then reclassified using the updated svm model. the above-mentioned process is iterated until termination test condition is met. considering the errors caused by the point-based classification, the filtering results are further optimized using a slope based method. the proposed method is mainly composed of the following five steps."
"caching sessions. in a caching session, a 'virtual bird' makes as many caches as the real birds did in the corresponding experiment. every time it must decide where to cache, it chooses the site with the highest 'cache attractiveness' c k, as determined by equation 4 . here, o k stands for the influence of onlooker aversion, while i k refers back to the inhibition of return of equation 2, and noise to the transient error of equation 3 ."
"although the traditional supervising learning algorithm can obtain good filtering performance, they need huge amounts of labeled training samples. the labeling process is generally time-consuming and needs much human intervention. as a result, the automation of these algorithms is low-level."
"to compare our simulations to the behavior of real birds, we use the same statistical tests as in the empirical works [cit] . alpha is set at 0.05, and all tests are two-tailed. our simulations are implemented in a java program, cogcor, which is included in the supporting information, model code s1. conceptually, the program consists of a setup model, a simulator model, and a cognitive model. the setup model keeps track of the state of the 'physical world' in the original experiments: what ice cube trays are available, how many worms are cached where, and so on. the simulator model runs the experiments: it ensures that the cognitive model and the setup model are initialized, that the right number of caching and recovery sessions are conducted, and that data is collected for further analysis. the cognitive model is the 'virtual bird'; it consists of behavioral rules (figure 3), which determine how decisions are made, and memory chunks, on which decisions are based."
"all caching and recovery events are explicitly encoded in memory, in chunks. a chunk's type refers to whether it was a caching or recovery event, and its location refers to the associated cache site. in the case of a recovery event, a chunk also records success, which refers to whether or not a cache was actually found. thus, a chunk is a memory of a particular kind of experience -caching in a site, successfully recovering there, or unsuccessfully recovering there. every time a 'virtual bird' experiences one of these events, it creates the appropriate chunk, and encodes it in memory. if the appropriate chunk already exists, it receives an update instead. these updates help determine its activation, or 'memory strength'. a chunk h's activation a h depends on its recency and frequency of use [cit], as specified by equation 1, where t j represents the elapsed time t since use j of chunk h, and d is a decay parameter (table 1) . this equation is adapted from act-r, a computational model designed to study human cognition [cit] ."
"for simplicity, we fix the network beamformers for precoding the user signals over the multiple bss. we describe here an approach based on regularized zero-forcing beamforming. the beamformers can also be chosen in different ways, for example using the zero-forcing or the wmmse approach."
"amrica offers the novel ability to align amr annotations of bitext. this is useful for analyzing 37 amr annotation differences across languages, and for analyzing translation systems that use amr as an intermediate representation. the alignment is more difficult than in the monolingual case, since nodes in amrs are labeled in the language of the sentence they annotate. amrica extends the smatch alignment algorithm to account for this difficulty."
"this paper adopts three accuracy indexes including type i error, type ii error and total error to access the filtering effect of the proposed method. type i error also referred to as omission error is the percentage of ground points misclassified as non-ground points. type ii error also called as commission error is the percentage of non-ground points accepted as ground points. total error is percentage of all the misclassified points. a confusion matrix towards these three kinds of errors is tabulated in tab. 3. type i, type ii and total figure 7. filtering performance of sample 1: (a) dsm before filtering; (b) true dtm generated using reference ground points; (c) extracted dtm generated using the filtering results."
"for each of these options, it estimates the 'attractiveness' of caching or recovering there. in caching sessions, it estimates 'cache attractiveness', in recovery sessions, it estimates 'recovery attractiveness'; these will be explained further in coming sections. however, both types of attractiveness always contain at least two components, namely, inhibition of return, which helps the 'virtual bird' avoid revisiting the same sites with the same purpose, and noise. inhibition of return. to calculate the inhibition of return i k associated with a site k, a 'virtual bird' checks whether any chunks l exist of the current session's type, that refer to the same site. if any such chunks l exist, the effect of inhibition of return, i k, associated with site k is equal to equation 2, where ga l is the sum of the activations of all such chunks l. thus, the stronger a 'virtual bird's' memories of having cached in a particular location, the lower its tendency to cache there again; similarly, the stronger its memories of having recovered in a particular location, the lower its tendency to recover there again."
"this paper adopts three publicly available datasets provided by the isprs to test the performance of the proposed method [cit] . these three datasets are acquired using an optech altm laser scanner and the point space is 1 m-1.5 m. these three datasets are selected since they include different terrain features and filtering challenges. thus, it is helpful to test the filtering performance in different terrain environments. as shown in fig. 6 (a), the terrain slope varies greatly in the first dataset (sample 1) and there is some low vegetation on the slope. moreover, some parts of the buildings are attached to the terrains. these points are easy to be misclassified as ground points. in the second dataset (sample 2), the main filtering challenges are complex buildings as shown in fig. 6 (b) . how to remove larger objects (such as the buildings) and smaller objects (such as cars and pedestrians) simultaneously is a filtering difficulty. in the third dataset (sample 3), it is difficult for most filtering algorithms to discriminate the attached objects (such as the bridge in the fig. 6 (c) ) from the terrains, since their elevations are close to each other. thus, these datasets are representative to test the filtering performance in different terrain environments."
"let s k be the message signal for user k, assumed to be complex gaussian with zero-mean and unit variance. let the normalized beamforming vector from all the bss to user k be"
"the 'virtual birds' have two sets of behavioral rules: one for caching sessions, one for recovery sessions. however, in each case, they must repeatedly decide where to cache or recover, and this process is always the same. first, a 'virtual bird' evaluates all its possible options -all the discrete ice cube tray sections that are on offer."
an independent data stream is transmitted from the central processor to each user. let x l be the signal transmitted by bs l. the received signal at user k can be written as
"recovery sessions. in a recovery session, a 'virtual bird' continues to recover until it has recovered all its caches. in reality, this is usually not the case; recovery percentages between 39% and 73% have been reported [cit], and another study mentions that 'a few items were often cached and not recovered' [cit] . furthermore, in some cases, the birds recovered relatively more of their 'watched' caches than their 'in private' ones [cit] . however, without a specific theory of scrub jay motivation -why they cache as much as they do, why they eat as much as they do -having the 'virtual birds' always recover everything seemed simplest."
"in figs. 7-9, (a) is the digital surface model (dsm) generated using point clouds in each sample; (b) is the true digital terrain model (dtm) generated using accurate ground points selected manually; (c) is the extracted dtm generated using the filtered ground points. it can be seen from the comparison that the filtering results are close to the reference ground points. the proposed method can achieve good filtering performances in all the three samples. as shown in fig. 7(a), there is much low vegetation on the slope. after filtering, both (c) ). in sample 2, some small objects are mixed with large buildings as shown in fig. 8 (a) . comparing with the true ground points (fig. 8 (b) ), the filtering results still contains some commission errors as shown in fig. 8 (c) . these commission errors are mainly caused by cars or pedestrians on the street since their elevations are close to the ones of terrain. in sample 3, the filtering challenge is the attached object (bridge) as shown in fig. 9 (a) . the filtered dtm is very close to the reference dtm (fig. 9 (b) and fig. 9 (c) ). it can be concluded that the proposed method achieves the best filtering performance towards sample 3."
"in the message-sharing based cooperation scheme, the backhaul links are exclusively used to carry user messages. the advantage of such an approach is that bss get clean messages which they can use for joint encoding. however, the backhaul capacity constraint limits the cooperation cluster size for each user. in compression based scheme, the precoding operation is exclusively performed at the central processor. the main advantage of such an approach is that, since the central processor has access to all the user data, it can form a joint precoding vector using all the user messages, thus achieving full bs cooperation. additionally, the bss can now be completely oblivious of the user codebooks as the burden of preprocessing is shifted from the bss to the central processor. however, since the precoded signals are compressed, we pay a price in the form of quantization noise."
"over the last decade, the social cognition of corvids -the extended family of crows -has been the subject of much scientific attention. experiments have shown, for instance, that clark's nutcrackers can use human cues to find food [cit], that pinyon jays can reason about social hierarchies [cit], and that rooks can cooperate to obtain rewards [cit] . most impressive are the behaviors that ravens [cit] and scrub jays [cit] display in the context of caching. like most corvids, these species hide food under ground, saving it for later. however, items may be stolen by conspecifics that saw the caching occur. this could create an incentive for the birds to be sensitive to the visual perspectives of others [cit], and many results appear to confirm that they are. when pilfering, if two ravens are present at a caching event, the more subordinate one pilfers faster if the cache site was within the dominant one's field of vision, and thus likely to be stolen, than if the cache site was not [cit] . similarly, when a raven is shown two cache sites in front of a competitor, it first raids the cache that the competitor also had a line of sight to, and only then the other [cit] . when caching, corvids bury most of their items far away from onlookers, and behind barriers [cit] . furthermore, scrub jays often re-cache their worms later, when they are in private, if they were forced to cache in the presence of others [cit] ."
"this paper studies the downlink transmission in cran where the bss are connected to a cloud processor with finite capacity backhaul links. we propose a hybrid strategy that combines compression-based signaling and message sharing, which results in better utilization of the backhaul. we propose a design procedure for selecting the appropriate users for message sharing, and an efficient method for the optimization of the quantization noise levels for the compressed part of the signals. numerical simulations confirm the performance gains of the proposed strategy over the existing approaches."
"this paper extracts geometric features to build a svm model, since these features are easily accessible [cit] . the geometric features include two parts. one part is calculated based on a local three-dimensional structure covariance tensor, while the other part is achieved based on elevation values of neighboring points."
"for human analysis, we believe it is easier to visualize the amr graph. we present amrica, a sys-(b / be-located-at-91 :li 4 :arg1 (i / i) :arg2 (c / country :name (n / name :op1 \"new\" :op2 \"zealand\")) :time (w / week :quant 2 :time (p / past)))"
"dally, emery and clayton [cit] investigated whether scrub jays could take into account the proximity of another bird, as well as its social status. to test this, the birds were allowed to cache in three different conditions: either in front of a dominant onlooker, in front of a subordinate onlooker, or in private, with an opaque partition separating them from a neighbor. in each case, they were offered two ice cube trays to cache in, one near the adjacent bird, and one farther away. it was found that the cachers preferred to cache in the far tray when watched -either by a dominant or by a subordinate -but that they showed no preference when alone. furthermore, during the recovery session, they re-cached a greater proportion of their worms in the 'dominant onlooker' condition than in either of the other two. when they had been watched, the birds also seemed to re-cache specifically from the 'near' tray ( figure 2a, 2b ), although the sample size was too small for statistical analysis."
"if we instead send the message for, say user k, directly, the signal that needs to be compressed now has smaller powerp l −p l,k . so to compress it to within the same quantization noise level q l, approximately log"
"bits are needed instead. now, the backhaul capacity required to send the message of user k to bs l is just its achievable rate, namely, r k . thus, message sharing is beneficial for user k on bs l whenever r k is less than the saving in the quantization bits, or equivalently"
"the onlooker aversion component o k causes the 'virtual birds' to avoid caching in the 'near' tray in experiment 2, where they are watched by dominant and subordinate conspecifics [cit] . as we assume that increased distance is a preference that the scrub jays have learned before the experiment, we incorporate it directly into the model. therefore, o k is equal to oa d for sites in the 'near' tray in the 'dominant onlooker' condition, to oa s for the same sites in the subordinate onlooker condition, and to zero otherwise ( table 1) . the settings oa d and oa s are tuned so that approximately 25% of the caches end up in the 'near' tray in each condition, as found in the empirical data [cit] . to capture our assumption that scrub jays are stressed by having to cache in front of a conspecific, and that stress causes them to re-cache [cit], watched 'virtual birds' can immediately recover a cache they have just created, with odds cr w . thus, every time a 'virtual bird' caches in front of an onlooker, it has a small chance of immediately recovering its worm. we set cr w to 0.6, which is tuned to our first experiment. there, it causes the 'virtual birds' to re-cache worms an average of 1.29 times when they are watched; this is very close to the empirical average of 1.2 [cit] . furthermore, for experiment 2, we assume that dominant onlookers evoke more stress than subordinate onlookers, and that this translates into more stress; therefore we set the respective recaching odds cr d and cr s to 0.8 and 0.4 (table 1) ."
"where z p is the height value of point p. z max (·) is the maximum height value of the neighboring point sets kn (p), whereas z min (·) is the minimum height value."
"one beneficial aspect of simulation models is that they can generate empirical predictions that can be tested easily [cit] . we list four. first, we predict that birds that re-cache more during the recovery session must have also re-cached more during the caching session. after all, it is this re-caching during the caching session that causes the 'virtual birds' to experience memory confusion at recovery, and it is the memory confusion that, through stress, causes the re-caching; thus, without increased recaching during the caching session, our whole explanation breaks down. for the vast majority of experiments, its presence or absence is not reported [cit] . second, we predict that in a recovery session, the birds start re-caching only after a number of recovery failures. if that is not the case, then it cannot be stress from recovery failures that causes re-caching. third, we predict that scrub jays must always re-cache proportionally more from emptier trays. so, if scrub jays were forced to cache more worms in one tray than in another, they should also re-cache more from the emptier tray. finally, we predict that any cause of stress should produce enhanced re-caching, irrespective of the social context. for instance, if some of a bird's caches were removed by an experimenter before its tray was returned, then we predict that it should re-cache more. although experiments that include cache removal have frequently been done [cit], whether this causes the scrub jays to re-cache at recovery is not reported."
"the main objective of this paper is to show that in a practical cran setting with finite backhaul capacity, instead of pure compression or pure message-sharing, a hybrid scheme that combines the two can bring significant benefit to the overall system performance. this paper proposes an approach where the central processor directly sends messages for some of the users to the bs along with the compressed version of rest of the precoded signal. the intuition is that it is beneficial, in terms of backhaul capacity utilization, to send clean message for strong users while compressing rest of the interference canceling signals. to quantify the benefit of this hybrid strategy, this paper also proposes convex optimization based method for optimizing the quantization noise levels for the compressed part of the precoded signal."
". a key observation is that the resulting optimization problem (p3) becomes convex in c l (assuming fixed p k and w k ), which allows efficient numerical solution. the proof of concavity is omitted here for brevity."
"this occurred because we made our 'virtual bird' more stressed by dominant onlookers than by subordinate ones, and this caused it to re-cache more during the caching session with the dominant onlooker. this increased re-caching during the caching session caused its memory to be more confused during the recovery session, which in turn caused it to experience more recovery failures. in our model, such recovery failures cause stress, and stress causes recaching, so this caused the 'virtual bird' to re-cache more during the recovery session as well. furthermore, like the real birds, the 'virtual bird' re-cached proportionally more from the 'near' tray when it had been watched (figure 2a, 2b ), but not when it had been in private ( figure 2c ). this was due to the fact that it avoided the proximity of others at caching, which we assume to have been learned from daily life. as a consequence, the likelihood of successfully recovering from the 'near' tray was statistically smaller, because there were fewer worms in it. therefore, the 'virtual bird' experienced more recovery failures in the 'near' tray, which caused a higher level of stress to be associated with that tray, and thus more re-caching."
"to address the filtering challenges mentioned above, this paper presents an active learning filtering method. in the proposed method, the initial training samples can be selected and labeled automatically using morphological operators. then, a support vector machine (svm) model can be built and updated iteratively according to the active learning strategy. the filtering accuracy improved gradually and the filtering results turned better and better. the remainder of this paper is organized as follows. section 2 elaborates the steps of the proposed method, including removing outliers, obtaining and labeling the initial training samples, point cloud features extraction, oracle design and samples selection, and classification results optimization. in section 3, the experimental tests are undertaken to evaluate the proposed method and a detailed analysis is made towards the experimental results. finally, conclusions are drawn at end of this paper."
"the point clouds generally contain noisy points due to the influence of the instrument itself or the external environment. these noisy points can be categorized into low and high outliers as shown in fig. 2 . the low outliers normally originate from multi-path and errors in the laser range finder and thus do not belong to the landscape. the high outliers are always elevated points that are generally generated by birds and low flying aircraft [cit] . the existence of noisy points will always bring about some negative effects, including the following: (1) the quality of dtm generation may be affected by the noisy points, especially the low outliers, since most of the filtering algorithms always assume that the lowest points in the local areas must belong to ground; (2) the rendering of point cloud based on elevation will be influenced due to the maximal or minimal elevations of outliers; and (3) mass of noisy point will incur low three-dimensional model reconstruction quality and decrease the degree of automation. hence, outliers should first be removed."
"note that the training samples mentioned in tab. 2 include two parts, one is the initial training samples obtained at subsection b, the other is the added training samples generated iteratively based on the principles referred to subsection d. the training sets and the testing sets are two different components. the training sets do not overlap the testing sets."
"building on this intuition, this paper proposes an approach where a part of backhaul capacity is used to send direct messages for some users (for whom the bss are better off receiving messages directly, instead of their contributions in the compressed precoded signals) and the remaining backhaul capacity is used to carry the compressed signal that combines the contributions from the rest of the users. typically, each bs receives direct messages for the strong users and compressed precoded signals combining messages of the rest of the weak users in the network. each bs then combines the direct messages with the decompressed signal, and transmits the resulting precoded signal on its antenna. note that the appropriate beamforming coefficients are assumed to be available at both the cloud processor and at the bss. this hybrid scheme is illustrated in fig. 3 ."
"amr annotators and researchers are still exploring how to achieve high interannotator agreement . so it is useful to visualize a pair of amrs in a way that highlights their disagreement, as in figure 3 . amrica shows in black those nodes and edges which are shared between the annotations. elements that differ are red if they appear in one amr and blue if they appear in the other. this feature can also be used to explore output from an automatic amr parser in order to diagnose errors."
"cannot achieve satisfactory accuracy in complicated terrain environments. to solve this problem, some improved methods have been proposed to enhance the robustness of this kind of approach by making the slope thresholds adaptive to abrupt terrains [cit] ."
"to solve this problem, this paper adopts multi-scale morphological operations to obtain and label the initial train samples automatically. the morphological filtering generally involves two basic operations, including morphological dilation and morphological erosion. the morphological dilation d selects the highest elevation value of all the points within the filtering window, while the morphological erosion e selects the lowest elevation value of all the points within the filtering window, given as (2) [cit] :"
"the raw point clouds are irregularly distributed. to fast locate one point's neighbors, this paper adopts k-dimensional tree to organize the point clouds [cit] . to compute the features for a point p, its neighboring point sets kn (p) are first obtained. then, the local three-dimensional structure covariance tensor cov p can be calculated according to (4) :"
"the question of which subset of bss should serve each user is in general nontrivial. for comparison purpose, this paper uses the following common heuristics for evaluating the achievable rates using the message-sharing scheme, wherein each user forms a cooperating cluster consisting of s bss with the strongest channels. under a fixed bs cooperation structure, locally optimal beamformers for maximizing the weighted sum rate subject to bs power constraints can be found using the weighted minimum mean square error (wmmse) approach [cit] . the total amount of backhaul required to support this message-sharing scheme can be calculated based on the achieved user rates multiplied by the number of bss serving each user."
filtering performance of sample 3: (a) dsm before filtering; (b) true dtm generated using reference ground points; (c) extracted dtm generated using the filtering results.
"the proposed method involves five main steps, including removing outliers, obtaining and labeling the initial training samples, point cloud features extraction, oracle design and samples selection, and classification results optimization. among these five steps, obtaining and labeling the initial training samples and point cloud features extraction are mandatory steps, whereas the other three steps can be seen as optimization steps. thus, for analyzing contributions of different pipeline components, this paper conducted the following ablation study:"
"amr distinguishes between variable and constant nodes. variable nodes, like i in figure 1, represent entities and events, and may have multiple incoming and outgoing edges. constant nodes, like 2 in figure 1, participate in exactly one relation, making them leaves of a single parent variable. smatch compares a pair of amrs that have each been decomposed into three kinds of relationships:"
"the other part of geometric features can be calculated according to elevations of neighboring points. these features include vertical range, height below and height above given as (11-13)."
"we now describe a design methodology for the hybrid compression and message-sharing strategy. the optimization of the hybrid strategy involves the choice of beamforming vector w k, power p k, the quantization noise levels q l, and more importantly the decision of which users should participate in message sharing and which users in compression. to make the overall problem tractable, in this paper, we fix the networkwide beamformers throughout, and begin the design process with an optimized pure compression scheme. at each iteration of the algorithm, we strategically select the most suitable user for message sharing, then re-optimize the quantization noise levels for the remaining compressed part. we continue this procedure until no additional users can benefit from message sharing instead of being included in the compressed signal."
"this paper restricts attention to linear precoding strategies, but we mention here, as related work, that possibilities exist for performing nonlinear precoding based on dirty-paper coding [cit], and for using lattice-coding based strategy based on compute-and-forward [cit] for the downlink cran. ii. system model this paper considers a downlink cran with l singleantenna bss serving k single-antenna remote users. each of the l bss is connected to a central processor with a capacitylimited digital backhaul link. a sum capacity constraint is imposed so that the total capacity over the l backhaul links is limited to c bits per channel use. the sum-capacity backhaul constraint is adopted here for convenience. it can model the scenario where the backhaul is implemented in a shared (e.g., wireless) medium."
"message sharing refers to the cooperation scheme in which the central processor distributes the actual message of each user to its cooperating bss through the backhaul links. each bs then forms a precoded signal based on all the user messages available to it, as shown in fig. 1 ."
"figure 1: amr for \"i've been in new zealand the past two weeks.\" [cit] tem for visualizing amrs in three conditions. first, amrica can display amrs as in figure 2 . second, amrica can visualize differences between aligned amrs of a sentence, enabling users to diagnose differences in multiple annotations or between an annotation and an automatic amr parse (section 2). finally, to aid researchers studying crosslingual semantics, amrica can visualize differences between the amr of a sentence and that of its translation (section 3) using a novel cross-lingual extension to smatch . the amrica code and a tutorial are publicly available. 1"
"for the purpose of computing the activations of chunks, time is measured in steps. every cache or recovery event counts as one step, and time outside of the experimental sessions is not considered. although real animals definitely experience memory loss over time, our approach still seems reasonable: the recovery accuracy of western scrub jays appears to decrease only after retention intervals of several days [cit], not the several hours used in these experiments."
"algorithm 2 summarizes the user selection process for message sharing based on the criterion (12). we use a greedy approach to look for the user which can provide the best improvement in backhaul utilization, then continue the process until no more users would result in further improvement."
"in active learning, the learner should actively query an oracle outside the learner to get the labels for the samples. thus, to obtain a good classification result, it is necessary to set the oracle correctly and appropriately. in this paper, the oracle is set to a sigmoid function of the residual between the point and the fitted surface given as (14)"
"the experiments that we simulated consisted of one caching session and one recovery session, in which the scrub jays were tested individually, in their home cages [cit] . in a caching session, the scrub jays were given a bowl of worms, and ice cube trays to cache in. after caching, both were removed. the birds were left without any food until the recovery session, several hours later. at that point, they were mildly hungry, and their trays were returned to them. for each experiment, we ran one hundred simulations at the original sample size, and we created as many 'virtual birds' as there were real birds. we averaged the results for each run, and calculated the discrepancy with the empirical data. further analysis was done on the run closest to the average discrepancy. the model has three adjustable parameters (table s1 ): d, governing how quickly memories decay, n, specifying the noise in their likelihood of recall, and st, determining the stress threshold at which recovering 'virtual birds' re-cache. they were kept constant across experiments, and set so that the performance of the 'virtual birds' matched that of the real birds. our results were robust, and depended only mildly on the model's exact parameter values; see figure s1, table s2, and text s1."
"although our model is built upon the observation that western scrub jays often cache more when conspecifics are present [cit], it is still an open question why they do so, especially given that other species of corvid usually show the opposite pattern [cit] . clark's nutcrackers, for instance, have been shown to cache less when being watched than when not [cit] . clary and kelly [cit] speculate that this species difference might be due to the fact that scrub jays are more social than clark's nutcrackers, and thus might consider the task to be cooperative. however, it seems unlikely that this is the case; if scrub jays consider the task to be cooperative, it is difficult to explain why they prefer to cache far away from conspecifics [cit] . alternatively, one could argue that for social birds it is not always feasible to inhibit caching until alone; they might have to settle for 'compensating' future cache theft, rather than avoiding it. whether birds employ one strategy or the other -'cache more while watched' or 'cache less while watched' -might also depend on individual experience, and the specific situation. this would explain why both enhanced [cit] and reduced [cit] caching have been found for eurasian jays, and corresponds well with the observation that ravens gradually acquire some of their 'cache protection techniques' during development [cit] ."
"the segmentation-based approaches have been proved to perform better than some other methods such as the surface-based method and the ptd method, since the segmented results can provide additional geometric, texture and other features [cit] . nevertheless, these methods cannot achieve satisfactory filtering results in forested areas as little laser pulses can pretreat tree crowns to reach ground. moreover, the filtering performance deeply depends on the segmentation accuracy [cit] ."
"to align amrs, we use the public implementation of smatch . 2 since it also forms the basis for our cross-lingual visualization, we briefly review it here."
"emery and clayton [cit] investigated whether scrub jays change their re-caching behavior in response to being watched at caching. this was tested in two different ways. first, the birds were given three trials 'with onlooker' and three trials 'in private'. on 'with onlooker' trials, the cachers could see a bird in an adjacent cage; on 'in private' trials, the cachers could not. at recovery, they were always in private. the result was that the scrub jays re-cached a significantly larger proportion of their worms on the 'with onlooker' trials than the 'in private' ones ( figure 1a ). in a second setup, the same birds received two trials where they could cache in two trays, one after the other. while caching in one of the trays, there was a conspecific present; while caching in the other, there was not. several hours later, both trays were returned simultaneously, and the birds could recover in private. it was found that they re-cached a significantly larger proportion of their worms from the 'with onlooker' tray than the 'in private' tray ( figure 1b) ."
"we now improve upon the initial user rates obtained with pure compression by allowing the messages for a subset of users to be sent to bss directly through the backhaul links. to select users for direct data transfer, we compare, for each user, the backhaul capacity required for sending its message directly, with the reduction in backhaul in compressing the rest of the signal if that user is dropped from compression."
"4 demonstration script amrica makes amrs accessible for data exploration. we will demonstrate all three capabilities outlined above, allowing participants to visually explore amrs using graphics much like those in figures 2, 3, and 4, which were produced by amrica. we will then demonstrate how amrica can be used to generate a preliminary alignment for bitext figure 5 : [cit] . the node-to-node alignment of the highlighted nodes is computed using the node-to-word, word-to-word, and node-to-word alignments indicated by green dashed lines. amrs, which can be corrected by hand to provide training data or a gold standard alignment."
"every time a 'virtual bird' must decide where to recover, it chooses the site with the highest 'recovery attractiveness' r k, as determined by equation 5 . here, f k is a cache relocation effect, which helps the 'virtual bird' recover at sites where it has previously cached, while the inhibition of return i k and noise are the same as described previously, in equations 2 and 3 respectively."
"in the compression based scheme, the functionality of precoding is completely migrated to the central processor, as shown in fig. 2 . the central processor performs joint encoding of the user messages and forms the analog signals intended to be transmitted by the bss' antennas. as the precoded signals are analog, they need to be compressed before they can be forwarded to the corresponding bss through the finite-capacity backhaul links. compression introduces quantization noises."
"with regards to our model extensions related to stress, we assume that stress causes increased caching and, as a special case, increased re-caching. this is inspired by work explicitly linking stress to caching [cit], based on the observation that birds cache more when faced with poor habitat quality [cit] and light body weight [cit] . along the same lines, we interpret it as a sign of stress that scrub jays often cache [cit] and re-cache [cit] more when watched. such enhanced caching in the presence of conspecifics has also been reported for eurasian jays [cit], although the opposite pattern has also been found, for both eurasian jays [cit] and other species of corvid [cit] . here, we focus on scrub jays, and thus, to mimic their behavior, we make our 'virtual bird' re-cache more when it is watched. we also make it re-cache more if the spectator is dominant than if it is subordinate, as we assume that the former evokes more stress. furthermore, we posit that, in recovery sessions, finding caches missing is a source of stress, and this, accordingly, makes our 'virtual bird' re-cache more."
"elmqvist [cit] first proposed an energy-optimization-based filtering algorithm in form of active shape model. energy of the model is a weighted combination of internal force and external force. the minimization of the energy function is processed iteratively until the step size of the model is less than a threshold. [cit] developed a semi-global filtering (sgf) algorithm by defining a novel energy function that is composed of a data term and a regularization term. ural and shan [cit] presented a min-cut based filtering (mbf) algorithm on the basis of an energy function, which considers both local and global features. to minimize the energy function, a minimum cut optimization algorithm was applied. the test reveals that an overall filtering accuracy of 91.3% can be achieved for the isprs test areas. similarly, [cit] also extracted ground points progressively through energy minimization using graph cuts. in this method, the energy function and graph model encode both point-wise closeness and pairwise smoothness."
"given the setup, this paper seeks to find the optimal encoding and transmission schemes at the central processor and at the bss that maximize the weighted sum rate of the overall network. for convenience, fixed user scheduling is assumed in this paper. in addition, perfect csi is assumed to be available both at the central processor and at all the bss."
"this paper proposes a hybrid compression and messagesharing strategy in which the precoding operation is split between the central processor and the bss. the rationale is that as the desired precoded signal typically consists of both strong and weak users, it may be beneficial to send clean messages for the strong users, rather than including them as a part of the signal to be compressed. in so doing, the power of the signal that needs to be compressed can be lowered, and the required number of compression bits reduced."
"the cache relocation effect f k associated with a site k depends on the existence of a cache chunk o referring to the same site. then, the cache relocation effect associated with site k is equal to that chunk's current activation a o (equation 1). thus, the stronger a 'virtual bird's' memory of having cached somewhere, the more attractive it finds it to recover there. if a 'virtual bird' recovers a worm, it needs to decide whether to re-cache it. to do so, it first calculates the safety risk s k associated with the site k where it just found its worm. this safety risk depends on its previous recovery experiences with site k's tray, but only on those recovery experiences directed at its actual cache sites or their neighbors. this is in line with our previous work [cit], where we show that scrub jays probably do not learn from all their recovery attempts, but only from those that are directed at sites where they have cached."
"the re-caching behavior of our 'virtual birds' was similar to that of the scrub jays. however, the 'virtual birds' lacked 'theory of mind', and had only a single behavioral rule that was assumed to be due to prior learning: a preference for caching far away from conspecifics. in the recovery session, the 'virtual birds' did not remember who had watched them, nor how close cache sites had been to an onlooker, nor whether they had been watched at all. nevertheless, they displayed various behaviors typically interpreted as indicators of 'cache protection': they re-cached more after being watched than after being alone, they re-cached more after caching with a dominant conspecific than after caching with a subordinate one, and they re-cached a larger proportion of the worms cached closer to an onlooker than of the worms cached farther away. to summarize, these results can be explained as follows: the more the 'virtual bird' was stressed at caching, the more it re-cached during the caching session, and the more its memory was confused later, during the recovery session. the more its memory was confused at recovery, the more often it expected to find worms in sites that were empty; the more it experienced such recovery failures, the more stressed it was, and the more it recached. similarly, the less it had cached in a particular tray, the lower its likelihood of successfully recovering there; the more it failed, the more stressed it was, and the more it re-cached."
"in order to visualize the improvement in network utility, we fix the total backhaul capacity to be 150mbps and 90mbps and plot the cdf of user rates of the compression and the hybrid schemes in fig. 6 . the hybrid scheme is seen to improve over the pure compression scheme mostly for high-rate users."
"this criterion is used to select users for message sharing. once a user is selected for message sharing, we re-optimize the quantization noise levels for the compressed part of the signals for each bs again by solving (p3) with a modified total backhaul constraint and modified power constraint. note that the modified backhaul capacity constraint depends on the rate of the selected user, which is a function of the quantization noise levels to be optimized. hence, we need to iteratively solve (p3) assuming fixed rate for that user from the previous iteration, then update the rate and continue the process until the rate converges. note also that the new quantization noise levels obtained from re-solving (p3) also affect the power constraint. however, such effects are small and can be neglected."
"to better show the generalization of the presented approach, two different datasets used in practice other than the widely used isprs data [cit] were tested. the new datasets are located within the city of luoding, china, characterized by modern architecture with low and high-storey buildings as shown in fig. 11 (a) and (b) . the datasets were obtained by an als50 scanner with an average point density of 2.76 points/m 2 . in terms of the first datasets (fig. 11a), the type i, type ii and total errors are 8.34%, 2.99%, and 4.81%, respectively. in terms of the second datasets (fig. 11b), the type i, type ii and total errors are 8.76%, 6.19%, and 7.39%, respectively. graphical illustrations of the type i and type ii error distributions of the two datasets are shown in fig. 12 (a) and (b) . from the error calculation results and distributions, it can be found that the proposed method performed well on the airborne lidar datasets (fig. 11 a and b) even though the datasets contained a large number of points with high point density."
"considering the errors caused by the point-based classification, this paper uses a slope-based method to optimize the classification results. as shown in fig. 5 (a), it is a digital terrain model (dtm) generated using the ground points classified by the svm model. to fine-tune the filtering results, this paper first divides the filtering results into grids as shown in fig. 5 (b) . subsequently, the lowest point in each grid is selected to be a ground seed. all the ground seeds are used to interpolate a fitted surface according to the rbf interpolator as shown in fig. 5 (c) . moreover, the fitted elevationẑ i for each point can be calculated. according to the fitted surface, the slope gradient in x and y directions s u i, s v i can also be obtained. the points that satisfy (15) are determined as nonground points and removed."
"the two explanations that have been offered for these results are the subject of intense debate [cit] . first, the birds could be reasoning about the mental states of their competitors [cit] . a scrub jay might infer that other birds intend to steal its worms, and that if others see it caching they will know where its worms are. furthermore, a scrub jay could realize that caching far away from onlookers makes it difficult for them to see its caches, and that re-caching when alone will ensure that they no longer know the locations of its items. according to this hypothesis, scrub jays thus have some elements of a 'theory of mind'. alternatively, the birds could be applying behavioral rules that they have learned previously, from experience in daily life [cit] . for instance, through cache interruptions, ravens could learn the rule 'cache far away from onlookers' [cit] : the nearer conspecifics are, the greater the likelihood that one of them will try to take the food the cacher is trying to bury. in this way, the birds could learn that the proximity of conspecifics implies cache loss, and should therefore be avoided. however, they might also learn rules that are more complex; for example, they might associate 'a specific competitor's line of sight in the past' with 'a general feeling of unease' regarding a particular cache site."
"where abs (·) means the absolute value, t 1 is the threshold. in this paper, t 1 is set to 5, since this constant is able to detect abnormal characteristic values successfully while protecting details from being flattened effectively. here, the recovered point clouds data x, y,ẑ can be obtained by inverse transformation according to the mapping relationship r. comparing with the difference between the observed elevation z and the recovered elevationẑ, the points with larger elevation differences are detected as outliers and removed. volume 7, 2019"
"airborne lidar filtering is a critical step in point cloud postprocessing applications. to solve the problems of low filtering accuracy in complex terrain environments and excessive human intervention, this paper proposed a filtering algorithm based on active learning. in this method, multi-scale morphological filtering methods are adopted to obtain and label the initial training samples. this paper then applies the active learning strategy to add training samples gradually and update the svm model progressively. finally, this paper adopts the slope-based method to optimize the classification results. the proposed method has been tested using the three datasets provided by the isprs. the experimental results showed that the total errors of the proposed method were smaller. this indicates that the method can achieve good filtering performances in different terrain environments. moreover, the average total error of the proposed method is the smallest when comparing with that of other ten famous filtering methods. in terms of average type i and type ii errors, the proposed method can balance these two types of errors. thus, the proposed method can filter out non-ground points as more as possible while preserving terrain details. overall, this paper realizes the automatic classification of point clouds without manually selecting training samples and labeling. this feature greatly reduces the human intervention and improves the degree of algorithm automation. furthermore, the proposed method can adapt to different terrain environments and achieve good filtering performances."
the design of the pure-compression strategy can now be stated as a weighted sum rate maximization problem over the transmit beamformers and the quantization noise levels as follows:
"where z i is the observed elevation,ẑ i is the corresponding fitted elevation, te is a constant that is set to 0.3 in this paper, since 0.3 m is appropriate to discriminate low vegetation from grounds."
"other experiments on the social cognition of scrub jays can also be interpreted within our framework. for instance, the fact that scrub jays use shadows [cit] and barriers [cit] to protect their caches can be captured by a slight rephrasing of the rule 'prefer to cache far away from onlookers' to the more general version 'prefer to cache where onlookers are difficult to see', and this can also be assumed to have been previously learned. then, like in our current simulations, selective re-caching from 'riskier' trays would be due to the stress caused by those trays containing fewer worms. other results require additional rules to be added to our model. for instance, scrub jays seem to take into account which caches have been seen by which onlookers [cit] . thus, it seems that they do remember who was present during caching, unlike our 'virtual birds'. however, this does not imply that they have 'theory of mind'. instead, it could be that the onlooker's presence triggers the subject's memory of the stress it felt during caching, and that this causes it to re-cache the associated caches, without any specific intent to prevent those worms from being stolen by that onlooker. with regards to scrub jays, one final result to consider is the fact that only experienced pilferers appear to re-cache. this has been described as a case of experience projection, 'it takes a thief to know a thief' [cit] . an alternative explanation is that scrub jays usually do not feel threatened by onlookers in neighboring cages, because such onlookers cannot actually reach their worms. in that case, maybe only birds that have pilfered find it stressful to be watched by an adjacent bird, because only they have experienced that trays can be moved between cages. thus, our hypothesis that stress drives re-caching can also be extended to account for this result."
"where w is the filtering window, (x i, y i, z i ) is the point within the filtering window. the morphological opening o is achieved by applying morphological erosion followed by dilation given as (3):"
"the filtering results of the above-mentioned three ablation studies are shown in tabs. 7-9. comparing with the three types of errors listed in tabs. 4-6, it can be found that all the errors of the three samples turn larger without the optimization steps. these include removing outliers, oracle design and samples selection, and classification results optimization. from tab. 7, it can be seen that the total errors of the three samples are slightly larger without outlier removal. it can be concluded that the outliers indeed have influence on the filtering results. therefore, removing outliers first can improve the filtering performance. the filtering results in tab. 8 are obtained using the initial training samples. it can be found that the proposed method does not perform well in these samples, especially for sample 1, whose total error is almost twice of that of using oracle design and samples selection. in this paper, the step of oracle design and samples selection is to acquire more labeled samples iteratively. in so doing, the svm model can be revised from coarse to better. therefore, this step is crucial for the proposed method. from tab. 9, it is easy to see that the proposed method is prone to achieve unbalanced type i and type ii errors without the step of classification results optimization. for instance, the type i error of sample 2 is 0.1%, while its type ii error is 30.65%. the type ii error of sample 3 is 0.03%, whereas its type i error volume 7, 2019 note that in these two figures there are some white components that are generated since these parts are data gaps. is 16.14%. as a result, all the total errors of the three samples are larger. this is because the svm classification used in this paper is based on the point primitive. it is very errorprone when calculating geometric features for each point. hence, the step of classification results optimization is very important to achieve good filtering performances."
"we now present the performance of the classification algorithms on both cpu and gpu. the euclidean distance computation resulted in computations three times faster on the gpu than the cpu (table 3 ). the sorting was also almost four times faster on the gpu. the sorting algorithm was based on the cuda sdk sorting networks, which used a bitonic sorter to construct a network using a parallel algorithm. the input data to the sorting algorithm was considered a batch and was divided into fixed size arrays such that the array-length was of size 2 n . in our case, the array-length and the batch-size referred to the number of the training, and test samples, respectively. in order to tailor the data accordingly, 256 data samples were used instead of 261, by removing 5 wbcs, while not changing the amount of rbcs and cancer cells. reducing the number of wbcs, however, did not affect the classification of cancer cells. the sorting algorithm was tested with 25%, 50%, and 75% of training data that pertained to 64, 128, and 192 of array-length, respectively. in case of 75%, training data were increased to 256 samples from 192 samples by padding zeros in order to make it an integral power of 2. the data were sorted in ascending order after the padded zeros, which were grouped together at the beginning of the sorted array. the decision-making step was then implemented only on the cpu due to branch divergence, which resulted in performance degradation on the gpu."
"the system was evaluated using three different biological assays describing several profiles of rbcs, wbcs and cancer cells. the data were collected from the micropore experimental setup with a pore diameter of 12 micrometers [cit] . these bio-sets consisted of current values in microamperes sampled at a rate of 2.2 microseconds. the raw data were around 10 gb owing to 360 million current values. after pre-fetching the data into main memory and pre-processing the raw data into integers, the total amount of data shrank down to 2.88 gb, resulting in 68% reduction of the total acquired raw data. the resulting data could easily fit in the available 6 gb main memory and therefore avoided the system to page."
"the micropores are an example of biomems devices that can detect human cells at the finer granularity from a biological assay (blood samples) and provide efficient, cheaper, and highly sensitive alternatives. the pore used for experiments had 12-micrometer radius and was made in 200 nanometer thin membrane that translocated single human cell at a given time. in order to achieve maximum throughput, the flow rate was optimized. increasing flow rate (such as 20 microliter per minute) missed some of the translocation events, while decreasing it to 5 microliter per minute reduced the throughput significantly. an optimal throughput was achieved at 10 microliter per minute. in addition to that, sampling frequency of the micropore was an important factor to determine the desired output. higher sampling rates induced lot of noise in the output current from the micropore and suppressed many translocation events, while lower sampling rates resulted in a more stable baseline current with a higher signal to noise ratio, but the system missed faster translocation events mainly caused by smaller sized cells, such as rbcs. an optimized sampling rate was achieved at 2.5 microseconds."
"this module took input from the pulse-detector as shown in fig. 1(c) . input included the detected pulses in the data. featureextractor computed the important features of the detected pulses, such as width, amplitude, slopes, and statistics, which were then used by the next module on gpu for further pattern classification as shown in fig. 1(d) . the pulse width and amplitude alone were insufficient to visualize the clusters from different cell types due to the overlap seen in 2d scatter plots. therefore, features pertaining to the morphology (width, amplitude), geometry (falling slope and rising slope) as well as statistics (mean and standard deviation) per pulse were computed. statistical feature, such as the average of mean and standard deviation of each pulse were also computed to facilitate visualization in 3d."
"the system comprised of a data pre-processor to format the raw data; the pulse-detector to detect pulses in the formatted data; the feature-extractor to compute useful features of the detected pulses followed by the pulse-classifier that classified the pulses based on their features; and finally, the visualization module to analyze the results as scatter plots."
"in this case, the cancer cluster is classified accurately. however, half of the input data (detected pulses) were used as training samples, while the other half were used as test data (fig. 6 )."
"this paper describes a novel system-level design that detects pulses from the collected raw data of solid-state micropores, computes useful features of the detected pulses, and finally, classifies unknown samples based on the learned knowledge. the results can be readily used by physicians/scientists to infer useful information for disease diagnosis."
"the speedup of 3-4x achieved from machine learning algorithm on gpu was fascinating. however, the experiments were performed on a powerful intel core i7-based cpu coupled with low-end quadro-based gpu with limited memory and number of cores as compared to the state-of-the-art server-class gpus, such as fermi [cit] and kepler [cit] . furthermore, the detected pulses were just a few kilobytes of the acquired measurements of a typical blood sample and were significantly smaller as compared to the gpu memory, which was a value of a few hundreds of megabytes. such a small amount of data did not significantly populate the global memory of gpu hardware, and thus, the overhead of data transfer was greater than the computation itself. significant working set that fit in gpu memory will result in even better speedup. detection of diseases, in particular, cancer is an extremely time-sensitive matter. state of the art technologies and research have heavily focused on the early detection of cancer to quickly and efficiently locate the disease, and limit its effects. to aid this cause, this technique is able to increase the speed of data analysis by three to four folds for detection of cancer cells, which in turn allows an earlier diagnosis and better outcomes for the therapy. this technique is efficient, costeffective, and does not require any state of the art technology. it is simple and easy to set-up."
"the experimental setup consisted of an intel core i7 for a gpu and cpu setup. cuda, pthreads, and written code were involved in analyzing the data that were attained by running the biological assays through the solid-state micropore."
"different threshold values resulted in different number of detected cell types. if the threshold was kept too close to the baseline, large numbers of cells were detected. in contrast, if the threshold was away from the baseline, fewer cells of each type were detected. smaller pulses constituting noisy pulses (false positives) were detected in addition to the actual pulses when threshold was closer to the baseline. unfortunately, the number of false positives was large in the case of rbcs due to their smaller dimensions-closely resembling to the actual pulses, which made it challenging to discriminate these from the real pulses. however, in case of cancer cells, due to their large dimensions, the discrimination from noisy pulses was significant and therefore very convenient for the threshold detector. conversely, when the threshold shifted away from the baseline, no noisy pulses were detected; however, it missed useful pulses (false-negative) due to their smaller amplitude. false negatives were frequent in the case of rbcs due to smaller sized pulses, and rare in the case of cancer cells due to their significantly larger dimensions. the detected pulses from different cell types were delivered to the feature extractor module to compute pulse features."
"empirically, pulses with a width larger than four samples (approximately nine microseconds) and amplitude larger than 1000 nanoamperes were recorded in the evaluation. [cit] samples to achieve a better detection of pulses. this reduced the pre-processed data to a total of 261 pulses-- fig. 3 . the baseline shift and the follow up of moving average filtering are also shown in fig. 3(a) . the onsets of the raw data attained are displayed in fig. 4(a), (b), and (c) which capture finer details of each remarkable pulse. clearly, cancer cell pulses were larger when compared to wbcs and rbcs. rbc pulses were fluctuating and unstable as compared to the cancer and wbc pulses. the two edges emerging out in the rbc pulses led to an insight that rbcs were clumped together when passed through the micropore-most probably due to their smaller size [cit] ."
"originally aimed for gaming, graphics processing units (gpus) have evolved as accelerators into a gamut of compute-intensive scientific applications including bioinformatics and biomedical signal processing [cit] . the gpu is what translates binary data from the central processing unit (cpu) and converts it into a picture. the tiny dots of an image displayed on a monitor are called pixels. the gpu decides how to use the pixels on a monitor to create an image to reflect the binary data sent to the system. these highly parallel architectures are becoming pervasive toward embarrassingly parallel applications due to their massively parallel architectures. gpus host clustered cores called streaming processors (sps), which are further grouped into streaming multiprocessors (sms). there are varied memory spaces available that range from slower off-chip global memory to the faster on-chip shared memory. on-chip shared memory is faster than off-chip memory, but typically smaller. gpus are good at doing many tasks at the same time. programmers optimize memory accesses to global memory either by coalescing memory accesses to global memory or by exploiting the shared memory to reduce the off-chip memory overhead. in parallel, the k-nearest neighbor algorithm is used to analyze the data. this technique, in particular, is used because it is a simple analysis tool. however, it is also very effective in this experiment as it delivers results and the necessary classification required for these studies."
"this algorithm is primarily organized into three steps. initially, the euclidean distance between each test sample to every training sample is calculated, which yields an n x m matrix, where n is the total number of test samples, and m is the entire number of training samples. next, all the training samples are sorted with respect to each test sample in order to achieve a row-wise sorted matrix. finally, the test sample is assigned using a class-based approach on the maximum proportion in a set of k selected training samples [cit] . these steps are shown in fig. 2, separated by synchronization barriers that guarantee the completion of previous steps before proceeding to the next step. in the following, we present mathematical formulation of the aforementioned steps."
"with the given hardware and cuda capability 1.1, the execution configuration could have a maximum of 512 threads per block. in order to achieve sorting required by k-nearest neighbor technique, we used cuda sdk sorting. quick sort implementation was used for cpu-based counter execution [cit] . built-in cuda timer and event functions, i.e., cudaeventcreate(), cudaeventrecord() and cudaeventsynchronize() were used to measure the execution of kernel functions within the machinelearning algorithm. these routines ensured that previously issued cuda events were recorded on the gpu. finally, cuda syncthreads() was used for synchronization across all thread blocks."
"the programmers can harness the underlying massive parallel architecture of the gpus through parallel programming tools. nvidia has designed compute unified device architecture (cuda) [cit] that is used in most of the nvidia gpus, including fermi [cit] and kepler [cit] . the cuda enables programmers to launch a compute-intensive kernel on gpu and harness a massive amount of computational power for the written programs to determine color, texture, and lighting of each pixel on the screen. furthermore, cuda provides abstractions to implement parallelism from coarser to finer granularity, in addition to the task parallelism. cuda follows single program multiple data (spmd) programming style. this enables programmers to launch compute-and data-parallel kernels onto gpu. cuda thread is the smallest unit of computation on a typical gpu. to run very efficiently on a gpu, there needs to be many hundreds of threads; the more threads, the better. the cuda is well known for its impressive number crunching and handling of large datasets. a kernel is primarily composed of a grid of cuda threads that is organized into warps, which are further combined into blocks and altogether the blocks form a grid. the threads would all be executing the same function but with different datasets. the execution configuration for a given thread launch ranges from 64 to 512 threads per block. threads within a block have access to the common shared memory, which is otherwise not shared across blocks. if needed, threads within a block can also perform barrier synchronization [cit] ."
"additionally, the features of the detected pulses such as pulse-width, pulse-amplitude, mean, standard deviation, falling slope, and rising slope were computed from the recorded samples per pulse. the width, amplitude, mean, standard deviation, falling slope, and rising slope are graphically defined in fig. 3(d) . pulse-width, pulse-amplitude, and the statistical feature (combined mean and standard deviation) were found statistically significant features in separating out the clusters, and therefore, were used for further analysis of different cluster types. the next section shows the statistical significance of the distinguishing features."
"the implementation of the algorithms for detecting and classifying biological targets from the raw data is described first. the gpu architecture and programming model, as well as the design of the classification algorithms, system components, and experimental setup are discussed in section 2. section 3 elaborates the results. finally, summary and future directions are concluded in section 4."
"once the distances were computed from each test sample to every training sample, an n x m distance matrix was achieved, such that d ij in the distance matrix was the distance between i th test sample to the j th training sample as shown below:"
"diseases such as cancer can be fully cured, if detected and treated at early stages. the traditional methods like magnetic resonance imaging (mri) and cytology are intrusive and are not done as part of screening for cancer. the current methods cannot decode the type of cancer in a sample, such as liver cancer or a brain tumor. however, nanoscale biotech devices, for instance nanopores [cit] and micropores [cit] enable the translocation of biological targets, such as dna and human cells in a biological assay at finer granularity. the coulter counter was patented to detect small particles using resistive-pulse sensing with microscopic tubes, and as the analytes pass through the microchannels, dips in the electrical current are recorded [cit] . this is the basis of electrical detection in micropores and nanopores. nanopores provide the ability to detect one molecule at a time [cit] and the ability to separate individual polymers [cit] . however, nanometer-scale pores have previously had limitations including, fluctuation in pore currents [cit], limited residence time of the molecule in the nanopore, nanopore clogging, and poor biomarker selectivity [cit] ."
"each row was sorted in the ascending order with the closest training samples d i1 next to its respective test sample i, and subsequently, the farthest training sample d im at the end of the row, as given below."
"once the features for each pulse were computed, these were fed to the pulse classifier to classify the pulses into their respective clusters. the pulse classifier used k-nearest neighbor machine-learning technique to detect cancer clusters and to deliver results to the visualization module as shown in fig. 1(e) ."
"nanopores have applications such as rapid detection and characterization of molecules, while micropores are widely used for separating cells [cit] . the sensors in this paper are minuscule channels made in thin silicon membranes. their output is a current signal that is measured in micro-and nanoamperes. research shows that cancer cells are softer and deform more readily than their healthy counterparts because of their elastic behavior [cit] . such behavior of diseased and healthy cells is recorded as varying patterns (pulses) in the output signal when passed through these devices [cit] . the pulses occur at different scales and amplitudes due to the varying size and physical properties of human cells-stiffness and viscosity. nevertheless, the data collected from such sensors suffer from a large amount of raw data riddled with sensor artifacts. in situ translocation of a characteristic biological assay (0.5 milliliter of a blood sample) results in 10 gb of raw data. the commercial software tools used for the analysis of raw data are limited to smaller datasets, and even a well-trained technician has to spend innumerable hours to process and analyze the data from a simple biological assay. the ability to address an increasing amount of raw data arising from bio-nano devices lies in machine-learning approaches for an effective detection and classification of biological targets in order to accomplish high-quality decision making."
"various computational methods are used to analyze the data attained from nanometer-scale devices, and significant work is focused on the applications of supervised machine-learning algorithms [8, [cit] in order to classify important patterns in gene expression data [cit] . simple threshold based on peakdetection algorithms can detect useful patterns in the raw data emerging from ecg and mass spectroscopy [cit] . a threshold is based on local minimum/maximum, mean, standard deviation, energy or entropy [cit] . these strategies motivate the design of machine learning approaches for the effective detection and classification of biological targets in the raw data collected from bio-nano sensors."
"the acquired raw data was composed of current values in microamperes sampled at a few microseconds apart with a micropore-based electrical measurement setup. the data preprocessor efficiently read data from the storage device into the system. the acquired raw data was further converted into integers instead of floating-point values for efficient processing and to avoid round-off errors. it was then ready for the pulsedetector as shown with labels (a) and (b) in fig. 1 . double buffering [cit] was used in the system to overlap the reading latency from the slower storage device with the computation [cit] . such configuration used two buffers and two threads, that is, the producer and consumer. the producer fetched data from the storage, while the consumer processed the data in the buffer in parallel. while the producer received data chunk k+1 into one buffer at time t+1, the consumer was processing data chunk k in another buffer at time t+1 in which the producer at time t received the data. this resulted in a sampling speed 1.6 times faster than the naive implementation where the data were first collected into the main memory from the storage device, and then computed. the data were transferred to the pulse-detector module for the detection of cells."
"during our experiments, buffers of size 200,000 samples were found optimal for our problem [cit] . the integer size on 64 bit intel machine running linux os with gcc complier was 4 bytes. the time and current values in integers per sample contributed to 8 bytes per sample, and overall results were limited to 1.6 mb per buffer."
"the k-nearest neighbor grouped the overall data into the training and testing samples, and based on the training samples, the test samples were classified. the pulse-classifier could be executed either on the cpu or on the gpu, as selected by the user. the gpu used multiple threads in order to run machinelearning algorithm in parallel, as explained below. the test samples (n) were classified using k-nearest neighbor classification technique based on different percentages of training samples (m). generally, a larger proportion of training samples resulted in better classification of the test samples."
"it was found that the average pulse amplitude of the cancer cells was larger than that of wbcs and rbcs as shown in table 1 . a similar trend was also observed for the average translocation time, and statistical feature. however, in the case of falling slope, it was observed that wbcs' average was greater when compared to the cancer cells, and similarly, in the case of rising slope, a greater average of rbcs was observed than that of cancer cells. such behavior of slopes, although useful for biophysical insights, did not make them useful for distinguishing the three types of human cells."
"the experimental setup consisted of an intel core i7-based compute node with nvidia quadro fx 580 and 6 gb of main memory installed on it. the cpu had 4 cores clocked at 1.6 ghz. the gpu has 512 mb global memory, 64 kb constant memory, 16 kb per block shared memory and 8k registers. moreover, the gpu contained 32 cores, i.e., 4 multiprocessors (mp), with 8 cores per mp, clocked at 1.12 ghz. the system used 64 bit linux ubuntu 10.04.4 lts, kernel version 2.6.32."
"the high-level detail of the system modules is provided in fig. 1 . the system is composed of several software compo-nents, including a data pre-processor, pulse-detector, featureextractor, pulse-classifier, and visualization. the pre-processor component converts raw data into the appropriate format for efficient processing. subsequently, the pulse-detector component detects pulses in the pre-processed data. the featureextractor component computes important pulse features followed by the pulse-classifier that classifies the detected pulses into their respective groups based on their distinguishing features. finally, the visualization module, which is extensively used in biomedical applications, produces 3d scatter plots."
"an approach is presented to detect and classify pulses in the raw data collected from healthy and diseased cells using a machine-learning technique. the useful features were computed from the detected pulses, which were used to classify cancer clusters from the wbc and rbc clusters. the system detected cancer cells with an accuracy of 70% from a typical biological assay, which were then completely separated out from other cell clusters using their features. in addition to that, all cell types were accurately classified when training data size was increased to 80%. using a gpu for the classification of cancer cells resulted in computations four times faster than those using a cpu. statistical feature computed as the average of mean and standard deviation for each pulse was used as a third metric, in addition to the pulse width and amplitude. these features completely separated out the cancer cell cluster from those of rbcs and wbcs. it, however, left the latter two clusters overlapped. this helped in labeling and detecting cancer clusters from mixed cell types with different proportions of training samples. the k-nearest neighbor approach detected all clusters with high accuracy when the right proportion of training samples and the right choice of the neighborhood size were chosen. this work lays the foundation for automated detection and classification of biological targets that can further be used to infer useful information at even smaller scales. nanopore data can be analyzed for disease diagnosis by using the same fundamental methods as described in this work."
"this module detected the translocation patterns of red blood cells (rbcs), white blood cells (wbcs), and cancer cells in the form of pulses passed through a micropore, as shown in fig. 1(c) . the technique used by the detector was based on the movingaverage technique. this technique worked on the pre-processed integer data and avoided pre-processing steps like smoothing, and thus, eliminated subjectivity to some extent. the size of sampling window was critical to use the moving-average filtering. increasing the size of the sampling window increased smoothing of the target data; however, it missed the useful pulses (false negatives) that were smaller. on the other hand, decreasing the size of the sampling window resulted in limited smoothing but detected noisy pulses as useful pulses (false positives). averages of the sampling window were computed further to compare subsequent data samples to it. in case the next data sample was smaller than the computed average, the sample was recorded as candidate pulse. finally, the subsequent data samples that were less than the threshold were recorded only as a pulse if the number of these samples were within the acceptable range as explained previously [cit] ."
"the computed proportions from all participating classes were used to compute the maximum proportion, such that for each test sample, max j c ij i"
"the unprecedented amount of data generated from the biosensors is increasing to capture the biophysical characteristics at ever-finer granularities. future micro/nanodevice designs are expected to include arrays to measure behaviors at extremely high resolution, and at sampling intervals of only a few nanoseconds. therefore, this is more daunting to find useful insights into such data in real time. this inspires the need for advanced resource management mechanisms, including advanced i/o techniques, such as pre-fetching and multiple buffering coupled with advanced gpu-based setup. kepler-based gpu setup with a significantly larger number of cores, an improved bandwidth between the cores, larger on- chip memory, and more control than its ancestors seems promising in accelerating such computations. nanoscale devices including nanopores can be the next step in this integrated machine learning technique. additionally, the use of advanced, though compute-intensive machine-learning algorithms, such as artificial neural networks, and supportvector machines, can play a significant role to unveil the type of cancer in the target tissue."
"visualization presents statistical information effectively in abstract form [cit] . the results were organized into 3d scatter plots for pattern analysis of the biological targets. by visual inspection, width, amplitude, mean, and standard deviation were found the best for discriminating the clusters of rbc, wbc, and cancer. however, humans can better visualize up to three dimensions, and the task becomes tedious beyond three dimensions due to the increased number of features leading to the curse of dimensionality. therefore, mean and standard deviation were combined as a statistical feature to be used as a third dimensional feature to the width and amplitude, and was found to completely separate out the cancer cells from wbcs and rbcs."
"the system was implemented using c and cuda with 349 lines of code for moving-average filtering, 53 for feature extraction, and 600 for k-nearest neighbor algorithm."
"to evaluate the proposed weighting schemes experiments were conducted using a projection of the cattle tracking system (cts) database in operation in great britain (gb). the proposed weighting schemes were incorporated into weighted variations and extensions of the well established gspan frequent subgraph mining algorithm [cit] . for comparison purposes we also derived variation of gspan, extgspan; that did not feature weightings, but could handle directed graphs, self-cycles and multiple links. results from the experiments are presented in the following sub-sections."
"results for the different performance metrics are presented in tables 4, 5 for the core and extended validation datasets, respectively, and generally demonstrate high to very high performance in line with results obtained from other case studies [cit] . repeatability and reproducibility were defined as agreement of within-run and between-run replicates, and evaluate concordance between runs of the bioinformatics workflow on either the same ngs dataset, or a different ngs dataset that was generated for the same sample and/or library, respectively. repeatability was always 100% for all assays for both the core and extended validation datasets. although certain components of the workflow employ heuristics to accelerate the computation, they do not appear to have an effect on the final results. reproducibility could only be evaluated for the core validation dataset, and was also found to be very high reaching values of 100, 99.65, and 99.50% for the resistance gene characterization, sequence typing, and serogroup determination assays, respectively. the small number of discrepancies between sequencing runs could be traced back to differences between runs a and b/c, whereas results of runs b and c were always much more concordant, as illustrated for the sequence typing assay in figure 2 . this could potentially indicate a library preparation effect because runs b and c share the same libraries, sequencing instrument, and operator, but the difference is too small to make any definitive observations and could also be explained by stochastic variation or the lower quality of some samples in sequencing run a (see the section \"results\"). accuracy and precision were defined as the likelihoods that results are correct and truly present, respectively. sensitivity and specificity were defined as the likelihoods that a result will be correctly picked up when present, and not falsely be picked up when not present, respectively. these four performance metrics all require the classification of results of the different bioinformatics assays as either tp, fn, tn, or fp, for which assay-specific definitions were formulated ( table 3) . classification also requires to use a reference that represents the best possible approximation of the \"truth, \" for which we used either a database or tool standard (see above). [cit], where both standards were used without discrimination for the validation of the bioinformatics component, but for which we consider differentiation relevant to indicate whether performance is evaluated on high-quality genotypic information or rather widely used and adopted bioinformatics tools. note that our implementation of accuracy for bioinformatics assays also differs from other studies where this was defined as all correct results divided by all results [cit], whereas we adopted the more classically employed definition of all correct classifications (tp + tn) divided by all classifications (tp+fn+tn+fp) [cit], thereby also incorporating results from the negative controls into the calculation of this metric. additionally, we introduced precision as a performance metric rather than using this term to refer to both repeatability and reproducibility [cit] . this metric is of particular interest because it does not incorporate tn and is more informative than the sensitivity for bioinformatics assays for which an imbalance between the number of positive and negative classes exists, such as is for instance the case for all resistance gene characterization assays. these different performance metrics therefore all provide complementary information, for which it can be helpful to consider that \"accuracy and precision provide insight into completeness, whereas sensitivity and specificity measure completeness\" [cit] ."
"snm is directed at the identification of patterns within social networks networks. the nature of the patterns can take many forms, a common type of snm is the identification of clusters of frequently \"corresponding\" nodes. snm is usually applied in a static context, i.e. a \"snap shot\" of the network is taken to which snm is then applied. in this paper we are interested in applying snm techniques to sequences of such snap shots where each snap shot is time stamped in someway. we refer to these sequences as longitudinal social networks, in that the sequences may be compared to longitudinal data collections such as those found in medical applications [cit] . more specifically we are interested in finding frequently occurring subgraphs in longitudinal social network data."
"# all schemas are extracted from https://pubmlst.org [cit] . a detailed overview of all their source links is provided in supplementary table s3 . * fhbp is typed twice: for the vaccine targets as the whole gene (nucleotide), and for the fhbp schema as the whole gene (nucleotide) but also as a dna fragment (nucleotide) and seven different peptides corresponding to, among others, the different protein segments."
"kv, nr, and sd conceived and designed this study. kv supervised the project. sd supervised the data generation. bb constructed the bioinformatics workflow and performed all bioinformatics analysis. rw, qf, and jv contributed toward the algorithmic implementation of the bioinformatics workflow. p-jc, wm, and sb collected and isolated dna of n. meningitidis samples to be used for the validation, and provided specialist feedback on the required functionalities of the bioinformatics workflow. bb and kv conceived the validation strategy, for which sd provided input and feedback. bb and kv analyzed the validation results, and wrote the draft for the manuscript. all authors aided in interpretation of the results and writing of the final manuscript."
"the data analysis bottleneck in particular represents a serious obstacle because it typically consists out of a stepwise process that is complex and cumbersome for non-experts. an overview of data analysis tools that can be used for capacity building was recently published by the engage consortium, which aims to establish the ngs ability for genomic analysis in europe [cit] . many of these tools still require substantial expertise because they are only available using the command line on linux, but a subset is also available as web-based platforms with a user-friendly interface open to the scientific community. for instance, an entire suite of tools for pathogen characterization through wgs data has been made available by the center for genomic epidemiology 1 hosted at the technical university of denmark allowing, among others, assembly, serotyping, virulence detection, plasmid replicon detection, mlst, and phylogenetic clustering [cit], and is frequently used by the different enforcement laboratories in europe (garcía [cit] ) . pubmlst 2 is another popular web-based platform that maintains databases with sequence typing information and schemas for a wide variety of pathogens, and can be queried with wgs data [cit] . some resources have also been developed tailored specifically toward certain pathogens, such as neisseriabase as a platform for comparative genomics of neisseria meningitidis [cit] . while these resources are most definitely useful, they do have some disadvantages. several databases and tools typically still need to be combined manually, whereas an integrated approach encompassing all required analytical steps is preferred for a public health laboratory [cit] . in addition, a set of standardized tools and guidelines is not defined yet, limiting the collaboration and reproducibility between different nrcs and nrls that all have their own way of analyzing wgs data [cit] . many of these resources are also lacking traceability. database versions, and tool parameters and versions, can be missing in the output or change without notice, making it hard to compare and exchange results with other laboratories. systematic international collaboration between different nrcs and nrls across multiple years is, however, only possible when a standardized workflow is used. the time between submitting data to the webserver and receiving results varies and could be a limiting factor in emergency cases, unless the application is locally installed."
"we reported a validation strategy focusing specifically on the bioinformatics analysis for clinical microbiological isolate wgs data, demonstrating generally high to very high performance, highlighting the added value and feasibility of employing wgs with the aim of being integrated into routine use under a quality system in an applied public health setting. our validation strategy can be extended to other bioinformatics assays, wgs workflow components (e.g., library preparation, etc.), different wgs workflows and/or sequencing technologies, and pathogens. several similar endeavors are currently being undertaken for other pathogens of interest for other belgian nrcs and nrls, and will in the future help to narrow the gap between the widely acclaimed success of wgs in research environments and its practical implementation in applied settings."
where v (g i ) is the set of nodes in transaction graph g i and v (g) is the set of nodes in the subgraph g. observe that w t (g) satisfies:
the rest of this paper is structured as follows. some previous work and a problem definition are presented in sects 2 and 3 respectively. three proposed weighting schemes to achieve longitudinal snm are presented in sect 4. the suggested weighting schemes are evaluated and compared in sect 5. some conclusions and main findings are presented in sect 6.
"profiles are obtained from pubmlst using the rest api and are automatically pulled in-house and updated on a weekly basis (the date of the last database update is always included in the output). serogroup profiles are available on pubmlst for the following 10 serogroups: a, b, c, e, h, l, w135, x, y, and z. genotypic profiles for other serogroups are not available and can hence not be detected by the assay. the serogroups are assigned to categories based on the number and type of hits (see supplementary table s2 ) for the corresponding schemas. the first category contains serogroups for which all loci are found as perfect hits. in the second category, all loci are found as perfect or imperfect identity hits. in the third category, loci resistance genes 9 9 0"
"the cattle tracking system (cts) database in operation in great britain (gb), which forms the focus of the research described in this paper, is maintained by the department for environment, food and rural affairs (defra) as part of the rapid analysis and detection of animal-related risks (radar) initiative 6 . the cts database records all cattle movements in gb, each record describes the movement of a single animal, identified by a unique id number, between two holding locations (e.g. agriculture holdings, markets, etc). social network can be extracted from this database such that each node represents a geographical location and the links the number of animals moved between locations. by considering the time stamps associated with movements, temporal sequences of networks can be extracted (i.e. longitudinal social networks). for the experimental analysis three distinct longitudinal social network data sets were extracted from the cts database using data from 1 [cit] to 31 [cit] . the first two data sets, derbyshire 7 and lancashire 8 . the third data set represented gb in its entirety. the data divided into 7-day \"episodes\" (there is a 6-day movement restriction that applies to agriculture holdings in gb), giving longitudinal sequences of 52 episodes. the links were annotated with a weight, indicating the number of animals that were moved, and a label, indicating the type of movement (e.g. farmtofarm, farmtomarket, etc). some statistics for each of the data sets is presented in table 1 . note that the gb database is significantly larger than the lancashire data set, which in turn was larger than the derbyshire data set. note that all the graphs featured \"self-cycles\" and \"multiple links\"."
"the reportable range for all three databases corresponds to their respective gene content, and performance was evaluated at the level of the gene. for repeatability and reproducibility, replicates were considered to be in agreement when a gene (also including imperfect hits) was detected or absent in withinrun and between-run replicates, respectively. for the database standard, no metrics could be calculated as no such associated information is available for neither the core nor extended validation datasets. for the tool standard, no accompanying reference tool exists for the arg-annot database, for which performance metrics could therefore not be calculated. for the card database, resistance gene identifier (rgi) 4.0.3 [cit] was used as the tool standard. the program was executed on all assemblies using the following settings: database version 1.1.8, input type parameter set to \"contig, \" alignment tool set to blast (all other settings were left at their default values). loose hits, hits that covered less than 60% of the query sequence or with less than 90% sequence identity, and hits that aligned to the protein variant model were afterward excluded. for the resfinder database, the online web service 6 was used as the reference tool, by analyzing all assemblies using the following settings: 90% identity threshold, 60% minimum length, and all antimicrobial resistance databases selected. for the ndaro database, amrfinder (alpha version) was used as the tool standard (excluding hits covering less than 60% of the target gene). the following definitions for classification were used: tp as genes detected by both our workflow and the tool standard; fn as genes missed by our workflow but reported by the tool standard; fp as genes detected by our workflow but not reported by the tool standard; and tn as genes not detected by both our workflow and the tool standard."
"experiments (not shown) using extgspan and the gb data set failed to produce any results (because of memory errors) unless the support threshold was set to 30% or above, a threshold at which only one node size subgraphs are discovered. thus it was not possible to conduct any meaningful comparison between the weighted frequent subgraph mining algorithms and a non-weighted approach using the gb data set."
"as a proof-of-concept, we focus here on n. meningitidis, a gram-negative bacterium responsible for invasive meningococcal disease, causing symptoms such as meningitis, septicemia, pneumonia, septic arthritis, and occasionally inflammatory heart disorders. the belgian nrc neisseria analyses approximately 100-130 strains per year, and traditionally employed the following molecular techniques for pathogen surveillance: species identification by real-time polymerase chain reaction (qpcr); matrix assisted laser desorption/ionization; or biochemistry to verify that the infection is n. meningitidis and not another pathogen also causing bacterial meningitis such as streptococcus pneumonia or l. monocytogenes; serogroup determination by slide agglutination or qpcr of the capsule genes; drug susceptibility and antibiotics resistance testing by determining the minimum inhibitory concentration on plated samples; and subtyping by pcr followed by sanger sequencing of several loci of interest such as for instance the classic seven mlst genes [cit] and vaccine candidates such as factor h-binding protein (fhbp) [cit] . the rapid progression, high fatality rate, and frequent complications render n. meningitidis an important public health priority in belgium and an ideal candidate to investigate the feasibility of using wgs while effectively mitigating the data analysis bottleneck. at the same time, the strict requirement for iso15189 accreditation, which deals with the quality and competence of medical laboratories for all employed tests and is demanded by the belgian national stakeholder (the national institute for health and disability insurance), renders it an ideal proof-of-concept to investigate how to validate the bioinformatics workflow."
this section provides the necessary longitudinal social network representation and mining definitions definition 1. a longitudinal social network is comprised of a sequence of graphs
"moreover, most nrcs and nrls operate according to a strict quality system that requires an extensive validation to demonstrate that methods are \"fit-for-purpose, \" thereby fulfilling the task for which the method was developed in order to produce high-quality results, which is also important to obtain accreditation [cit] . for classical typing methods, the process of validation is typically dependent upon the exact type of analysis and the (often limited) number of wellcharacterized samples. a standardized approach to validate wgs for routine use in public health laboratories for microbiological applications is not available yet and still under development by the international organization for standardization (iso) in the working group \"wgs for typing and genomic characterization\" (iso tc34-sc9-wg25) [cit] . although this working group is expected to lead to an official standard in the next few years, many nrcs and nrls already face the need for validation at the current moment, as evidenced by many recent case studies that describe the validation of components of the wgs workflow. [cit] presented the validation of an end-to-end wgs workflow for source tracking of listeria monocytogenes and salmonella enterica. [cit] reported the validation of a wgs workflow for the identification and characterization of shiga toxin-producing escherichia coli (stec) focusing on standardization between different public health agencies. [cit] described external quality assessment options for wgs. [cit] documented the validation of wgs for a commercial solution for stec. [cit] reported the validation of wgs for outbreak detection and clustering of stec. [cit] detailed an entire modular template for the validation of the wgs process not limited to certain species but generally applicable for a public health microbiology laboratory. such case studies help to propel the implementation of wgs for clinical microbiology, but the comprehensive validation of the underlying bioinformatics analysis has not been documented yet. this is however of paramount importance as bioinformatics analysis is inherently part of the evaluation of every step of the entire wgs workflow going from sample isolation, dna extraction, library preparation, sequencing, to the actual bioinformatics assays. it is therefore imperative to thoroughly validate this step before the other levels of the wgs workflow are evaluated [cit] . the bioinformatics analysis acts as the \"most common denominator\" between these different steps, allowing to compare and evaluate their performance. an exhaustive validation of the bioinformatics analysis for wgs for clinical and/or public health microbiology has, however, not yet been described, and is not an easy task because classical performance metrics cannot be directly applied to bioinformatics analyses, and it is often not possible to obtain a realistic 'gold standard' for systematic evaluation [cit] ."
"definition 14. the share of a subgraph g, denoted as sh(g), is the ratio of the graph weight of g with respect to n g to the total weight of n g . thus:"
"the datasets supporting the conclusions of this study have been deposited in the ncbi sra under accession number srp137803 (in-house sequenced data), zenodo (http://doi.org/ 10.5281/zenodo.1575931) (results of all bioinformatics analysis for both the core and extended validation datasets), and are included within this manuscript and its supplementary files (results of the validation for both the core and extended validation datasets of all bioinformatics assays)."
"to demonstrate that the the utility of the subgraphs that have been discovered this sub-section briefly discusses an application of the approach. the frequent subgraphs identified by the above mining algorithms can be further used to construct a sequential database where each item of the sequence is a frequent subgraph. formally, each sequential transaction is extracted using the following identity (8) ."
"where s t,t+ts represents the sequence of frequent subgraphs in a ts time period, f i represents a frequent subgraph and f s(g t ) represents all frequent subgraphs of the graph g t, where g t represents the graph at a time instance t. using the amw-gspan algorithm as an example; the output when applied to the gb data set, with a support of 10%, was utilized to create a sequential data collection with a time-step value of 1. a sequential mining algorithm, prefixspan [cit], was then applied to this database with a support threshold of 60%. one example of the maximal-size sequential patterns is displayed in fig. 4 . the patterns in the figure indicate these four subgraphs occurred together on 32 occasions out of 52 in the order showed in the figure. in the figure, each node denotes the location of the agriculture holding, and the number next to the node denotes the unique identification number. it can be seen that g1, g3, and g4 are all subgraphs of g2. all the movements are pointed to the location \"266329912\", and g1 always occurs before g2, while g3 and g4 always occur after g2. if a smaller support threshold of 30% was used, a longer sequence consisting of 9 patterns were extracted as illustrated in fig. 5 . figure 5 features additional movements to those given in fig. 4, and includes new locations. in the figure, the movement was still centered on the location \"266329912\", however two new locations \"266329843\", and \"266309364\" were added into the sequence, and each pattern in the figure contains either one of them or both."
"most research work in frequent subgraph mining [cit], assumes each discovered frequent subgraph is equally important. because of this, a lot of redundant and repetitive frequent patterns exist in the resultant set. in addition, if the size of the graphs is substantial and the minimum support is low, a typical frequent subgraph mining task will not terminate within a reasonable period of time due to the exponential computation incurred by subgraph isomorphism testing. if we put emphasis on differentiating each discovered frequent subgraph by their importance as defined by the user, or as derived from the application domain, a reduced computational complexity can be achieved without compromising the effectiveness of the discovery process."
"when a weighting function is integrated into the process of mining weighted frequent subgraphs, the well-known anti-monotone property 5, which is often used to prune the search space of the patterns, is not satisfied anymore. there are two general solutions to this dilemma: (i) design a weighting function that keeps the property; (ii) utilize some heuristics to reduce the computation incurred by not maintaining the property."
"we describe here the first exhaustive validation of a bioinformatics workflow for microbiological isolate wgs data, extensively documenting the performance of different bioinformatics assays at the genotypic level by means of a set of traditional performance metrics with corresponding definitions and formulas that were adapted specifically for wgs data. the wgs workflow was evaluated both on a set of sequenced reference samples and collected public data generated by means of the illumina sequencing platforms, and demonstrates high performance. our validation strategy can serve as a basis to validate other bioinformatics workflows that employ wgs data, irrespective of their targeted pathogen, and illustrates the feasibility of employing wgs as an alternative to traditional molecular techniques for a relatively small-scale laboratory in a public health context."
"the extended validation dataset consisted out of 64 n. meningitidis samples selected from publicly available ngs data. this additional dataset was collected to evaluate our bioinformatics workflow on data coming from different laboratories, as is often the case in real-world applications. in this extended dataset, we included additional strains of serogroups y and w135, which are underrepresented in the global collection of 107 n. meningitidis samples maintained at the university of oxford, and which are currently causing epidemics in both the united states and europe [cit] (see also the section \"discussion\"). additionally, the majority of samples in the extended validation dataset were generated by means of the hiseq instrument in contrast to the miseq used for the core validation dataset, and read lengths for this dataset were consequently typically shorter. an overview of these samples with their corresponding ncbi sra accession numbers is available in supplementary table s20 ."
"options are left at their default values. post-trimming data quality reports are then generated using fastqc to obtain an overview of processed read quality. afterward, processed paired-end reads are de novo assembled using spades 3.10.0 [cit] . orphaned reads resulting from trimming (i.e., reads where only one read of the pair survived) are provided to the assembler as unpaired reads. the \"-careful\" option is used. all other options are left at their default values (kmers are chosen automatically based on the maximum read length). assembly statistics such as n50 and number of contigs are calculated with quast 4.4 [cit] ) using default settings. the processed reads are then mapped back onto the assembled contigs using bowtie2 2.3.0 [cit] with the following settings: \"-sensitive, \" \"-end-to-end, \" and \"-phred33\" (all other options are left at their default values). the mapped reads are used to estimate the coverage by calculating the median of the per position depth values reported by samtools depth 1.3.1 [cit] ) using default settings (spades by default also maps reads back to the assembly but reports coverage in terms of kmer coverage). lastly, several quality metrics are checked to determine whether data quality are sufficient before proceedings toward the actual bioinformatics assays. threshold values for these quality metrics were set based on the quality ranges observed during validation by selecting more and less stringent values for metrics exhibiting less and more variation between samples/runs, respectively. an overview of all quality metrics and their corresponding warning and failure thresholds is provided in table 1 ."
"several relevant databases for sequence typing hosted by the pubmlst platform 4 [cit] are employed for genotypic sequence typing ( table 2 ). all sequences and profiles are obtained using the rest api [cit] and are automatically pulled in-house and updated on a weekly basis (the date of the last database update is always included in the output). for every database, loci are typed separately by aligning the assembled contigs against all allele sequences of that locus using blastn and blastx for nucleotide and protein sequences, respectively [cit] ). filtering and best hit identification are performed as described previously for resistance gene characterization. if multiple exact matches exist, the longest one is reported. for protein sequences, alignment statistics are calculated based on the translated sequences. the different possibilities for \"hit types\" and their corresponding color codes used in the output are detailed in supplementary table s2 . if sequence type definitions are available and the detected allele combination matches a known sequence type, this is reported along with associated metadata in the output (for classic mlst: corresponding clonal complex; for rplf: genospecies and associated comments). for rpob (rifampicin resistance) and pena (penicillin resistance), the phenotypically tested susceptibility to the corresponding antibiotics of strains carrying that particular allele is also retrieved from pubmlst and included in the output."
"because access to the required bioinformatics expertise and/or resources remain obstacles for many nrcs and other laboratories from smaller and/or less developed countries [cit], our bioinformatics workflow has been made available as a \"pushbutton\" pipeline (compatible with all data generated through the illumina technology) accessible free-of-charge at the public galaxy instance of our institute 8 for non-profit and academic usage by this target audience. nevertheless, properly evaluating performance through a validation strategy as described in this study is paramount when using this resource to ensure highquality results are obtained on a set of samples that contain a representative (sub)set of genetic variation typically encountered in the population under investigation. although both the larger volume and genetic diversity of samples expected to be analyzed by nrcs and other laboratories from larger and/or more developed countries implies that this resource will not scale well with their requirements, our \"push-button\" implementation can still be used by the latter as a showcase to demonstrate how the bioinformatics workflow was locally implemented and made available to specialists of the belgian nrc neisseria, since they employ the workflow similarly through an in-house instance of galaxy (the volume of samples in belgium is currently not high enough to motivate automated workflow execution nor does it present issues with scalability when using the galaxy framework). a training video is also available as a tutorial for using this resource (see supplementary material)."
"the aw approach is founded on two elements to restrict the growth of the search space: (i) a graph distance measure, and (ii) a weighting ratio. for a subgraph g to be frequent both elements must be greater than specified user thresholds. the graph distance measure is calculated using an appropriately defined support weighting function, w (g). this is defined as follows. let g be a candidate pattern for a database"
"serogroup determination is based on the genotypic sequence typing of the capsule loci [cit] . serogroup per base sequence content difference between at and gc frequencies averaged at every read position. since primer artifacts can cause fluctuations at the start of reads due to the non-random nature of enzymatic tagmentation when the nextera xt protocol is used for library preparation, the first 20 bases are not included in this test. as fluctuations can also exist at the end of reads caused by the low abundance of very long reads because of read trimming, the 0.5% longest reads are similarly excluded 3 6"
"definition 10 leads to an alternative pruning strategy which, may be used as part of any frequent subgraph mining algorithms. during the candidate selection phase, the mining will keep track of the weighted support and weighting ratio of all candidates and discard all those candidates that do not satisfy at least one of (c1) and (c2)."
"examples of research work on applying frequent subgraphs to analyzing social networks include: community pattern mining [cit], targeted advertising [cit], and structural prediction [cit] . frequent subgraph mining [cit] entails two significant overheads: candidate set generation and (sub)graph isomorphism checking. however, these overheads are exacerbated when the size of the graph data is substantial and the support threshold is low. weighted frequent subgraph mining [cit] advocates the use of weighted support counts to identify weighted frequent subgraphs. hence, the \"computational burden\" of subgraph mining can be considerably alleviated by generating a set of weighted frequent subgraphs."
"social network data is often substantial, usually comprising many nodes and links. consequently, longitudinal social network data tends to be even more substantial, typically an order of magnitude relative to the number of time stamps to be considered. standard frequent subgraph mining algorithms, such as gspan [cit], are thus unable to process such large longitudinal networks in a realistic manner. in recognition that some links (and/or nodes) may be considered to be more important than others, in this paper weighted longitudinal social network mining is proposed. weightings can be applied in a number of manners, three weighting schemes are proposed and evaluated in this paper."
"the following considerations should be taken into account for our validation strategy. first, all bioinformatics assays are at the level of the genotype, which does not necessarily correspond with the phenotype. it is for instance well documented that antibiotics resistance genes are not always expressed, and therefore not always result in the corresponding resistant phenotype [cit] . the same has been witnessed for the serotype [cit] . only systematic and long-term monitoring of genotype versus phenotype relationships will allow to evaluate their concordance [cit], but in the genomic era, a paradigm shift should be envisaged where currently existing methods for pathogen typing based on the phenotype are phased out by pathogen characterization based on the genotype, despite issues for backward compatibility with traditional methods [cit] . second, some performance metrics could easily be misinterpreted or even manipulated in certain contexts. for instance, the resistance characterization assay for the resfinder database had a perfect accuracy of 100% for both the core and extended validation datasets for the tool standard, but only very few tp existed because n. meningitidis typically only harbors a limited number of antibiotics resistance genes [cit] . this implies that most database genes will never be detected but still contribute to the overall accuracy through an overwhelming number of tn compared to tp. populating any database with irrelevant genes would therefore artificially increase the accuracy of any such assay, and only properly considering and evaluating other metrics such as precision and sensitivity can help to put this in its proper context [cit] . third, evaluation by means of a database standard is preferred over a tool standard because the former contains high-quality genotypic information, but the current reality is that such databases are still scarce and/or incomplete. the database standard employed for n. meningitidis still contained some missing data, since both our workflow and the reference tool managed to identify several loci for which no identifier was stored in the database standard. additionally, it always remains a possibility for any biobank that mutations are introduced over time due to microevolutionary events caused by freezing, thawing, and repeated cultivations of the reference collection leading to differences [cit] . this highlights that the construction of \"gold standard\" databases will greatly benefit the scientific community by providing a high-quality reference standard to which results of different bioinformatics methods and workflows can be compared against, for which notable efforts are already ongoing within the microbial community such as spearheaded by the global microbiological identifier initiative, and which are expected to aid future validation efforts [cit] . fourth, the current validation strategy is at the level of the isolate, and does not consider any phylogenomic analysis comprising many isolates. the validation of such a bioinformatics assay, however, represents an additional layer of complexity and constitutes an entire study on its own, first requiring consensus in the scientific community about how wgs workflows for isolates can be validated, to which our study represents a significant contribution."
"the bioinformatics workflow was implemented in collaboration with the experts from the belgian nrc neisseria to ensure it complied with the needs of its actual end users by providing a \"push-button\" pipeline solution. on the frontend, the bioinformatics workflow was integrated as a standalone tool into a local instance of the galaxy workflow management system [cit] to ensure a user-friendly interface that only requires uploading the data and selecting the desired bioinformatics assays (the data pre-processing and quality control are always executed by default). an illustration of the input interface is provided in supplementary figure s1 . the bioinformatics workflow is compatible with data from all illumina sequencing platforms (other sequencing technologies are not supported). threshold values for some quality metrics such as the sequence length distribution (table 1) are dynamically adapted based on the detected sequence length."
"genotypic antimicrobial resistance is detected by identifying resistance genes with nucleotide blast+ 2.6.0 [cit] using default values against four widely used resistance gene databases: arg-annot [cit], card [cit], resfinder [cit], and ndaro 3 . these databases are automatically pulled in-house and updated on a weekly basis (the date of the last database update is always included in the output). first, hits that cover less than 60% of, or have less than 90% identity to the subject, are removed. second, overlapping hits (i.e., hits located on the same contig with at least one base overlap) are grouped into clusters. [cit] . the different possibilities for \"hit types\" and their corresponding color codes used in the output are detailed in supplementary table s1 . visualizations of pairwise alignments are extracted from the blast output generated with the pair-wise output format (\"-outfmt 1\")."
"from the above it can be easily inferred that the function w (g), as defined by (2), satisfies the anti-monotone property. therefore, if a k-subgraph candidate is not frequent, then any of its (k + 1)-supergraph candidates can be safely pruned from this branch in the search space lattice during the k +1 candidate generation process. it should be noted, however, that the approach will tend to bias large transaction graphs over smaller transaction graphs, thus is best applied to graph sets where the individual graphs are of a similar size."
"where w i can be user defined or calculated by some weighting methods. in this paper, a weighting method to generate the node weight, is introduced as follows:"
"despite being a well-established research method, the use of whole-genome sequencing (wgs) for routine molecular typing and pathogen characterization remains a substantial challenge due to the required bioinformatics resources and/or expertise. moreover, many national reference laboratories and centers, as well as other laboratories working under a quality system, require extensive validation to demonstrate that employed methods are \"fit-for-purpose\" and provide high-quality results. a harmonized framework with guidelines for the validation of wgs workflows does currently, however, not exist yet, despite several recent case studies highlighting the urgent need thereof. we present a validation strategy focusing specifically on the exhaustive characterization of the bioinformatics analysis of a wgs workflow designed to replace conventionally employed molecular typing methods for microbial isolates in a representative smallscale laboratory, using the pathogen neisseria meningitidis as a proof-of-concept. we adapted several classically employed performance metrics specifically toward three different bioinformatics assays: resistance gene characterization (based on the arg-annot, resfinder, card, and ndaro databases), several commonly employed typing schemas (including, among others, core genome multilocus sequence typing), and serogroup determination. we analyzed a core validation dataset of 67 well-characterized samples typed by means of classical genotypic and/or phenotypic methods that were sequenced in-house, allowing to evaluate repeatability, reproducibility, accuracy, precision, sensitivity, and specificity of the different bioinformatics assays. we also analyzed an extended validation dataset composed of publicly available wgs data for 64 samples by comparing results of the different bioinformatics assays against results obtained from commonly used bioinformatics tools. we demonstrate high performance,"
"in supplementary table s19, and results in a perfect accuracy, precision, sensitivity, and specificity of 100%. table 5 presents results for the extended validation dataset for the serogroup determination assay. within-run replicates were always fully consistent, resulting in a repeatability of 100%. reproducibility could not be assessed as no betweenrun replicates were available. results for the database standard are presented in supplementary table s26 . our workflow identified the same serogroup as the database standard for 92.19% of samples that were classified as tp. all other samples were classified as fn. values for tn and fp were put at 100 and 0% of samples, respectively, to reflect that the database standard represents the \"truth\" (see also the section \"discussion\"). the ensuing confusion matrix is presented in supplementary table s30 and results in an accuracy, precision, sensitivity, and specificity of 96.09, 100, 92.19, and 100%, respectively. results for the tool standard are presented in supplementary table s27 . our workflow identified the same serogroup as the tool standard for 100% of samples that were classified as tp, and no samples were classified as fn. no capsule loci were detected by our workflow nor the reference tool when challenged with the serogroup schema of l. monocytogenes resulting in values for tn and fp of 100 and 0% of samples, respectively. the ensuing confusion matrix is presented in supplementary table s31, and results in a perfect accuracy, precision, sensitivity, and specificity of 100%. results for both the database and tool reference were therefore in line with results observed for the core validation dataset (table 4) . figure 2 for a brief explanation). the upper left graph depicts the percentage of concordant cgmlst loci, i.e., where our workflow identified the same allele as the tool standard, which were classified as tps. note that the ordinate starts at 98% instead of 0% to enable illustrating the results more clearly. all other cases were classified as fns, and encompass two categories. first, the upper right graph depicts the percentage of cgmlst loci for which our workflow identified multiple perfect hits, of which at least one corresponded to the tool standard but was reported differently. second, the lower left graph depicts the percentage of cgmlst loci for which our workflow detected a different allele compared to the tool standard. most fns are therefore explained by a different manner of handling multiple perfect hits, and only a small minority are due to an actual mismatch between our workflow and the tool standard. furthermore, upon closer inspection, these mismatches were due to an artifact of the reference tool used for the tool standard that has been resolved in the meantime (see supplementary figure s2 ). see also supplementary table s11 for detailed values for all samples and runs."
"a supplementary manuscript containing all supplementary figures and tables is available as \"bogaerts_neisseria_ supplementarymaterial.docx.\" a supplementary video providing a tutorial for employing the \"push-button\" pipeline instance of our bioinformatics workflow is also available at zenodo 7 ."
"resistance gene characterization assay table s15, and results in a perfect accuracy, precision, sensitivity, and specificity of 100%. for the card database, within-run replicates were always fully consistent, resulting in a repeatability of 100%. four genes were detected in several samples as perfect hits for all runs: fara, farb, mtrc, and mtrd. an imperfect hit, covering 71.79% of ermf with 99.83% identity, was found for sample z1073 in run b only. another imperfect hit, covering 71.79% of mtrr with 99.83% identity, was detected in runs b and c but not in run a for sample z4242. as these were the only four between-run replicates not in agreement on a total of 158,187 comparisons (i.e., 787 database genes times 67 samples times 3 runs), the reproducibility rounds up to 100%. performance metrics could not be calculated for the database standard because no such information was available. for the tool standard, all detected genes were always fully consistent with results of the rgi tool, with the exception of mtre and aac(2 ). mtre was detected in all samples across all runs by our workflow but never by rgi, and was therefore classified as a fp in all samples. this was most likely because the mtre sequence displayed very high nucleotide identity (∼94%) with the database variant, but had some frameshift mutations. since rgi predicts open reading frames followed by protein alignment table s15, and results in an accuracy, precision, sensitivity, and specificity of 100%. table 5 presents results for the extended validation dataset for the three different databases. a detailed overview of all detected genes for all three databases can be found in supplementary table s24 . within-run replicates were always fully consistent, resulting in a repeatability of 100%. reproducibility could not be assessed as no between-run replicates were available. no perfect hits were detected for any of the resistance gene databases, but several imperfect hits were found. as for the core validation dataset, performance metrics could only be calculated for the tool standard for the resfinder, card, and ndaro databases, and the resulting confusion matrices are presented in supplementary table s28 . for the ndaro database, not a single gene was detected in the extended validation dataset by both our workflow and amrfinder, meaning that precision and sensitivity could not be calculated. for the resfinder database, a perfect accuracy, precision, sensitivity, and specificity of 100% were obtained. for the card database, there were again several fp that were all due to the mtre gene being detected in several samples, resulting in an accuracy, precision, sensitivity, and specificity of 99.88, 87.50, 100, and 99.88%, respectively. results for both databases were therefore in line with results observed for the tool standard for the core validation dataset ( table 4 ). table 4 presents results for the core validation dataset, using cgmlst as a proxy for the performance of all sequence typing schemas (see the section \"materials and methods\"). withinrun replicates were always fully consistent, resulting in a repeatability of 100%. results for between-run replicates are presented in figure 2 and supplementary table s9 . the median reproducibility is 99.56% for run a versus b, 99.63% for run a versus c, and 99.75% for run b versus c, with a reproducibility of 99.65% averaged over all three comparisons. one outlier is present for the comparisons of run a versus b and run a versus c, which is in both cases due to sample z4242 in run a that only had 94.14% of its cgmlst loci identified in the first place. because results for this sample were relatively bad in run a compared to runs b and c, we checked whether contamination had occurred but could find no indication thereof (see supplementary figure s3 ). for the database standard, results are presented in figure 3 and supplementary table s10. tp were defined as loci of the cgmlst schema where our workflow identified the same allele as the database standard, with median values of 97.07, 97.13, and 97.20% of cgmlst loci a dash (\"-\") indicates a performance metric could not be calculated/determined because the data were unavailable. an asterisk ( * ) indicates that the metric could not be calculated, due to division by zero."
"are detected as perfect hits, imperfect identity hits, imperfect short hits, or multi-hits. in the fourth category, not all loci are detected. the serogroup of the highest possible category is always reported. when there are multiple possible serogroups in the highest category, the serogroup with the highest fraction of perfect hits (i.e., number of perfect hits divided by number of loci in the schema) is reported. serogroup determination fails when less than 75% of loci are detected for the best serogroup reported according to the above classification."
"a weighted approach to longitudinal social network mining is described. the approach allows large longitudinal networks, such as the gb network used to illustrate this paper, to be mined where this was not possible using more conventional approaches. three weighting mechanisms were proposed to reduce the overall computational complexity. reported experiments comparing the operation of the weighting schemes with each other and a non-weighted version of gspan demonstrated that many fewer patterns are derived. the reported experiments also indicated that ubw-gspan finds the least number of patterns while requiring the largest amount of run-time. amw-gspan provided the best compromise, a limited number of patterns found in reasonable time (especially at low support threshold values). to illustrate that the utility of the subgraphs that were discovered; further analysis was conducted to capturing changes in \"behavior\" within the network structures."
"definition 8. given an arbitrary subgraph g with its support set δ(g), the weighting function of g with respect to n g, w ng (g), is defined as:"
"minimum read length minimum read length after trimming (denoted as percentage of untrimmed read length) that minimum half of all trimmed reads must obtain (e.g., half of all trimmed reads should either be minimally 175 or 150 bases long when raw input reads lengths are 300 bases long)"
"we complement these studies by proposing a validation strategy focusing specifically on the bioinformatics analysis of the wgs workflow to exhaustively evaluate performance at this level, which is crucial because the bioinformatics component serves as the \"common denominator\" that allows to compare the different steps of the wgs workflow (e.g., library preparation, sequencing, etc.) or even different wgs workflows and/or sequencing technologies. although workflow components and quality metrics listed in figure 1 and table 1 will need to be adapted, the validation strategy proposed in our study is platform-agnostic and can be tailored toward other sequencing technologies. the underlying premise of our validation strategy consists of demonstrating that the bioinformatics workflow is \"fit-for-purpose, \" which is defined by the iso17025 standard as providing \"confirmation by examination and provision of objective evidence that the particular requirements for a specific intended use are fulfilled\" (iso/iec 17025:2005) . we addressed this by employing several classical performance metrics with definitions and formulas adapted specifically toward the nature of the different bioinformatics assays (table 3), which were evaluated on both a core and extended validation dataset. the core validation dataset was constructed by means of in-house sequencing of a selection of 67 samples from the global collection of 107 n. meningitidis strains maintained at the university of oxford [cit], because it contains strains that encompass much of the genetic diversity encountered within belgium and therefore can be considered as representative for our country. moreover, this collection has been extensively characterized and high-quality genotypic information is available for many relevant assays such as cgmlst and serogrouping, thereby providing a database standard to compare results of our bioinformatics workflow against. the extended validation dataset was composed by selecting 64 samples from publicly available ngs data, and therefore allowed to expand the validation scope to genotypes underrepresented in this reference collection and/or sequenced through different (illumina) platforms. the y and w135 serogroups are a notable example of recently endemic cases in belgium that are underrepresented in the global collection of 107 n. meningitidis samples. because no highquality genotypic information was however available for the sequence typing assay for this dataset, we employed an alternative reference based on results obtained through bioinformatics tools and resources commonly used and adopted by the scientific community [cit], such as the suite of tools for pathogen typing and characterization of the center for genomic epidemiology [cit] and pubmlst [cit], which were used as a tool standard to compare the results of our bioinformatics workflow against. the same approach was also taken for the core validation dataset to evaluate consistency between the database and tool standard, and because high-quality genotypic information was not available for all bioinformatics assays of the core validation dataset such as resistance gene characterization."
"in the context of weighted frequent subgraph mining, the weighting function associated with a subgraph pattern g can be defined in various manners. three example approaches are proposed in this paper: (i) average mutual information based weighting (amw), (ii) affinity weighting (aw), and (iii) utility based weighting (ubw). the first two approaches satisfy the anti-monotone property while the last one adopts an alternative pruning heuristic. the last two approaches employ two parameters to control the mining result while the first one uses one parameter only. each approach is discussed in further detail in the following three sub-sections."
"we report here the first exhaustive validation of a bioinformatics workflow for clinical microbiological isolate wgs data. we employed the pathogen n. meningitidis as a proof-of-concept by designing a bioinformatics workflow (figure 1) that incorporates different quality checks (table 1 ) and relevant typing schemas (table 2 ) with the aim of either extracting information that ensures backward compatibility with currently existing \"classical\" molecular biology techniques (e.g., serotyping), or alternatively taking advantage of the full potential offered by wgs by extracting information at the scale of the full genome (e.g., cgmlst). our study is relevant because recent surveys by both the efsa (garcía [cit] ) and the ecdc [cit] have indicated that, at least in europe, the data analysis and required expertise remain substantial bottlenecks impeding the implementation of ngs for routine use in microbiology. for nrcs and nrls, as well as other laboratories working under a quality system, a harmonized framework for validation of the wgs workflow presents an additional obstacle [cit] . a series of recently published studies have, however, showcased the need thereof, and presented validation approaches for certain components of the wgs workflow focusing either on a modular template for the validation of wgs processes [cit], the entire workflow \"end-to-end\" [cit], standardization [cit], external quality assessment [cit], commercial bioinformatics software [cit], outbreak clustering [cit], or specific assays such as serotyping [cit] ."
"the core validation dataset consisted out of n. meningitidis reference strains selected from the global collection of 107 n. meningitidis strains maintained at the university of oxford [cit] for which sequence data were generated in-house. reference strains from this biobank were originally used to validate cgmlst at the genotypic level, and were extensively characterized using several sequencing technologies and platforms, thereby constituting a valuable resource that represents the global diversity of n. meningitidis for which high-quality genotypic information is available. a subset of 67 samples was selected by specialists from the belgian nrc neisseria to ensure that these cover the entire spectrum of clonal complexes that are representative for belgium (see also the section \"discussion\"). the selected samples were originally collected from 26 different countries over a timespan of more than 50 years encompassing endemic, epidemic, and pandemic disease cases, as well as asymptotic carriers. at least one sample was selected for each of the disease causing serogroups (a, b, c, w135, x, y) [cit] ). an overview of these 67 samples is provided in supplementary table s4 . genomic dna was extracted using the column-based geneelute kit (sigma), using the manufacturer's instructions. sequencing libraries were prepared with an illumina nextera xt dna sample preparation kit and sequenced on an illumina miseq instrument with a 300-bp paired-end protocol (miseq v3 chemistry) according to the manufacturer's instructions. the 67 selected samples were sequenced three times in total. runs a and b were performed on the same miseq instrument using newly created libraries from the samples, whereas run c was performed on a different miseq unit but using the same libraries as prepared for run b. run a was done by a different operator than runs b and c. all wgs data generated for these samples in the three different runs have been deposited in the ncbi sequence read archive (sra) [cit] under accession number srp137803. individual accession numbers for all sequenced samples for all runs are listed in supplementary table s14 ."
"a good performance (98.17% accuracy) was achieved for renal calculi classification with a knn model based on the cole-cole parameters. however, there could still be some error sources that may decrease the method's performance, which can be categorized as measurement errors, data analysis errors, or the diversity of samples. measurement errors can be due to an air gap between the samples and aperture of the probe. data analysis errors can stem from the 420 errors that emerge due to representation of the data with few parameters. for example, cole-cole parameters that can represent a curve are not necessarily unique, and different combinations can satisfy the requirements. finally including a more diverse data set containing more than 105 renal calculi samples could improve performance of the machine learning algorithm."
"each user with valid information about a region is termed informed user for that region. users interested in getting location-specific information about a region are called information seekers of that region. a seeker, essentially a user who does not have the sought information in her buffer, first broadcasts her query to her neighbors through the wireless ad hoc interface of the device. we term this a local query."
" are positive constants, they show weight coefficient of each evaluation parameter, j is global cost function value of path network, l j is the overall length of the path network, j  is the smoothness cost of path network, the calculation method is shown in the formula (3), k a is the path network of k,  is the maximum steering angle permitted by the individual, d"
"as mentioned earlier, users move in an area partitioned into multiple regions. the state of context knowledge within a region intuitively corresponds to the disease status in an epidemic. in general, a user's knowledge state would be multi-dimensional, because a different piece of information is relevant for each region. thus, for each region we would have an associated epidemic model, with the same structure but different parameters. however, the state of knowledge about a region is unrelated to the knowledge about other regions, so different regions can be analyzed separately. we present our model for a single region, with users entering and exiting it; and we describe the states and the dynamics of our epidemic model for that single region."
"the basic idea of abc algorithm is to search every potential solution and evaluate through swarm, if the new solution is better than the old one, then using the searched new solution replace the poor old solution, until selecting the optimal solution. if a solution is still not improved after a certain number of cycles, the solution will be abandoned, the corresponding hired bee will be transformed into the investigation bee, and solving the new solution through formula (10)."
"in order to determine in parallel the z-links to which each node contributes, a two-hop neighborhood inquiry is performed (achievable with localized communication). this is the minimum number of hops that must be considered as the z-link is by construction a two-hop structure. 5 the presence of a z-link can be revealed according to the following proposition, which characterizes z-links is a manner more amenable for decentralization."
"proof: the memory requirements for an agent i is dictated by the storage of z-link induced cuts known by the agent itself. therefore, in the worst case scenario, a single leader may be able to see the entire graph in two-hops and thus such an agent would need to store all potential cuts in the graph. at this point, from theorem 5, we know that the number of z-links is o(n 2 ), thus the thesis follows."
"after a z-link is detected we must check if it is a cut over the graph g, and if the removal of the edges that belong to the z-link yields two disjoint components, one with k nodes and the other with n − k nodes. this operation is described in algorithm 4. as introduced above, a dfs is performed, taking only a single arbitrary node from the current z-link as root of the exploration tree. at the end of this recursive search the visited nodes are counted and if this number is equal to k or equal to n − k it implies that a proper z-link is found. otherwise, the z-link has to be discarded and the search must continue until either a valid bipartitioning z-link is found, or the graph is deemed infeasible for the desired k-bipartition."
"one important requirement of this technique is to place the aperture of the vna essentially consists of a signal source, a receiver, and a display. vna is used for measuring microwave network parameters; that is, scattering parameters (s-parameters). the source transmits the signal to the renal calculi through a coaxial probe, and the receiver measures the signal reflected back to 135 the probe from the sample. after obtaining s-parameters, dielectric constant and dielectric loss values of each stone have been computed over the frequency range of 500 mhz to 6 ghz with 100 mhz intervals by the commercial software."
"the mere anonymization of (especially the continuous) queries does not protect users' location privacy: the queries of a user are correlated in space and time; hence, the adversary can successfully link them to each other, by using target-tracking algorithms [cit], or can successfully identify the real names of the users [cit] . changing user pseudonyms while the users pass through pre-defined spots, called mix zones [cit], makes it difficult to track the users along their trajectories. however, users must remain silent inside the mix zones, which means that they cannot use the lbs. to mitigate this problem, the size of the mix zones is kept small, which in turn limits the unlinkability of users' queries. even if the mix zones are optimally placed, the adversary's success is relatively high [cit] ."
"each user possesses a location-aware wireless device, capable of ad hoc device-to-device communication and of connecting to the wireless infrastructure (e.g., cellular and wi-fi networks). as users move between regions, they leverage the infrastructure to submit local-search queries to an lbs, at some frequency that we term lbs access frequency. the frequency at which users query the lbs varies depending on the type of requested information, on the dynamics of information update in the lbs database, or on the geographical region."
"notice that the z-link structure is not only useful in characterizing edges that preserve rigidity, but also takes on a two-hop structure which is amenable to decentralization."
"the median dielectric properties of the renal calculi samples that were calculated from measurements collected with the open-ended coaxial probe are represented with variability bars in fig.3(a) and fig. 3(b) . to better em-290 phasize the dielectric property discrepancy between stone types, the median permittivity and conductivity values of calcium oxalate, cystine, and struvite samples are given in table 2 at six different frequency points. medians of the measured relative permittivity of calcium oxalate, cystine, and struvite between 1 ghz and 6 ghz were ranges of, respectively, 2.28 to 2.35, 2.17 to 2.63 and 295 3.16 to 3.38. the median conductivity of calcium oxalate, cystine, and struvite between 0.5 ghz and 6 ghz were ranges of, respectively, 0.45x10-2 (s m −1 ) to 1.6x10-2 (s m −1 ), 0.19x10-1 (s m −1 ) to 1.5x10-1 (s m −1 ), and 2.0x10-2 (s m −1 ) to 3.5x10-2 (s m −1 ). the permittivity discrepancy between renal calculi types tends to decrease with increasing frequency, whereas the conductivity discrep-300 ancy increases with increasing frequency. cystine had the lowest and struvite had the highest relative permittivity at microwave frequencies as seen in fig. 3(a) . although the conductivity parameter was very low for all stone types, the conductivity of struvite was relatively higher than the other two types. the cole-cole parameters were fitted to the measured relative permittivity and conductivity of each stone sample by utilizing gnr method. two examples of the cole-cole fitting to median measurements of each type are shown in fig. 4(a) and fig. 4(b) . a good agreement was achieved between the median of the measurement data and the cole-cole fitting. a comparison of the fitted cole-cole parameters is given in table 3 . the error parameter of euclidean distance was lower than the threshold value. one can classify the renal calculi just by looking into the cole-cole parameters or median measurements. however, the measurement system suffered from low measurement accuracy and low repeatability rates. therefore, machine learning algorithm was implemented to 315 compensate for the errors stemming from the measurement methodology and other systematic errors."
"the method of figure 1(c) can reduce the path points, this requires the dimension of the solution is variable, traditional abc algorithm cannot meet such requirements. we put forward artificial bee colony algorithm with modified dimensionality, and taking overall structure design to the solution, it doesn't affect the operation of the algorithm when the dimension changes, the structure of each solution is expressed as:"
"oordinated autonomous systems continue to demand strong attention from researchers, particularly within recent years. the driving force of this surge is the increasing promise of multiagent systems in reality, a product of rapid advancements in computation and communication, and the implications that collaborative systems have for impactful applications. examples range from base behaviors such as tracking and coverage [cit], formation control [cit], and state consensus [cit], synchronization and optimization [cit], to higher-level objectives such as collective transport [cit], wireless network optimization [cit], environmental monitoring [cit], and data fusion [cit] . additionally, multiagent systems may yield significant advantages over single-agent solutions in terms of heterogeneity of mobility and sensing, fault tolerance and flexibility, and scalability [cit] . in this paper, we are concerned with identifying two properties of the graph that describes a multiagent network. first, given a single team of connected agents, we wish to identify partitions in the network graph that yield two sub-teams, i.e., graph bipartitions or split maneuvers. partitioning or splitting a team emerges as an important behavior primitive in navigating uncertain or cluttered environments by endowing the team with the flexibility to change in both composition and scale. further, network partitioning is compelling in the context of task assignment (see [cit] ), as it would enable task-centric collaboration, allowing for example the decoupling of tasks across spatiotemporal scales. there is also promise in applying the team splitting primitive for collective transport [cit], multitarget entrapment or encirclement [cit], and multiteam cooperation [cit] ."
"we take the path planning problem of the group system as the research object in this paper, combining with the ability to solve the global optimization of multi variable function of abc algorithm [cit], and improving in allusion to the characteristics of the path planning problem of the group system, and we put forward artificial bee colony"
"centralized approaches introduce a third party in the system, which protects users' privacy by operating between the user and the lbs. such an intermediary proxy server could anonymize (and obfuscate) queries by removing any information that identifies the user or her device. alternatively, it could blend a user's query with those of other users, so that the lbs server always sees a group of queries [cit] . however, such approaches only shift the problem: the threat of an untrustworthy lbs server is addressed by the introduction of a new third-party server. why would the new server be any more trustworthy? additionally, new proxy servers become as attractive for attackers as centralized lbss."
"we implement mobicrowd on three different nokia mobile devices (n800, n810, and n900) by building a mobile privacy proxy in each device. the proxy does not require any modification of the supported applications and it is transparent to their operation. the prototype works with the maemo mapper lbs and mobicrowd acts as an http transparent proxy to which the client traffic is redirected. note that knowing the format of the lbs queries and the data format of the server replies is enough to adapt mobicrowd to new lbs applications. our implementation in python is 600 lines of code, including the proxy module, ad-hoc networking module, and the server interface module. memory utilization does not exceed 3 percent of the total device memory."
"end for 13: end if 14: end for 15: end procedure yielding significant speedup to the z-link search, a property enabled by the local z-link structure."
"before starting the measurement process, the slim-form dielectric probe was calibrated by implementing the standard open, short, and deionizedwater calibration procedure. after performing the calibration, complex permittivity measurements of pure methanol were collected to validate the calibration. then the probe aperture was pressed against the flat surface of the renal calculi sample, 225 ensuring that the probe tip was fully in contact with the sample. at least five measurements were obtained from different points on each sample surface whenever appropriate. the measurements were taken between 500 mhz and 6 ghz, with 100 mhz intervals."
"renal calculi samples were categorized using the knn algorithm, where the calculated cole-cole parameters were given as inputs to the classification algorithm. in the first stage, input vectors were normalized in the range of (-1, 1) 265 since normalization allows faster convergence during training. after normalization, the data were divided into training and testing sets by applying 10 fold cross-validation. in this method, the data set was split into 10 equal folds. during each round, the data in nine folds were used for training, and the remaining fold was used for testing. a fair test of validation for limited sample size can be 270 obtained with 10 fold cross-validation by using features for both training and testing."
"in section 7, we show that all the eigenvalues have a strictly negative real part for the range of system parameters we consider; hence, the equilibrium point is stable, and it persists under small perturbations of the system parameters. the stability analysis justifies using the equilibrium point to evaluate our system. if it were unstable, then either the system would not converge to it or the smallest disturbance would cause the system to leave it."
"in the case of no collaboration among users, i.e., in bufferonly mobicrowd, the users can retrieve the information either from their buffer or from the server. only the i users have the information in their buffers, whereas the r users are forced to contact the server when they become interested. the i users ask queries at a total rate of gi, and the r users at a total rate of gr. therefore, the hiding probability in this case is"
"having expressed all variables in terms of s, we need to solve the quadratic equation (5f) for s, keeping in mind that any solution s 0 has to satisfy 0 s 0 1. the value of s 0 can be found from the quadratic formula:"
"in our renal calculi classifier, k was set to 10 because it is a common practice to set the k parameter equal to the square root of the number of training samples [cit] . the distance parameter chosen was euclidean distance. by utilizing these 275 parameters, the knn model was then trained and tested. the performance of the knn model was calculated with a two-by-two confusion matrix consisting of true positive (tp), false positive (fp), false negative (fn), and true negative (tn) counts in the classification. in this study, accuracy, sensitivity, specificity, precision, recall, and f1 score performance measures were calculated from confusion 280 matrices and used to evaluate the model."
"dielectric properties of biomaterials have been widely investigated to enable the advancements in microwave diagnostic and therapeutic technologies. different techniques have been used for measurement of dielectric properties, including resonant cavity perturbation, parallel plate, free space, waveguide, 105 and the open-ended coaxial probes. choosing the proper technique depends on the frequency, material properties, and application requirements. for example, the parallel plate technique is suitable for low-frequency measurements and requires heavy machining of the sample. similarly, the cavity perturbation technique can only perform narrowband measurements, and the sample needs to be 110 heavily machined. on the other hand, the open-ended coaxial probe technique is simple to operate, does not require machining of the sample, and is able to perform broadband measurements [cit] . the technique is mostly used for dielectric property measurements of biological materials with high permittivity and loss. despite all the advantages, the technique is only utilized in laboratory 115 environments, and multiple measurements from one sample are required due to high error and low measurement repeatability rates."
"various threats associated with sharing location information have been identified in the literature. for example, users can be identified even if they share their location sporadically [cit] . knowing the social relations between users can help an adversary to better de-anonymize their location traces [cit] . finally, location sharing of a user not only diminishes her own privacy, but also the privacy of others [cit] ."
"we denote by sðtþ, s ã ðtþ, iðtþ, i ã ðtþ, rðtþ, and r ã ðtþ, respectively, the fraction of seeker insider, seeker outsider, informed insider, informed outsider, removed insider, and removed outsider users of a given region at time t. the network state yðtþ is the vector of these values. the system of equations that models the evolution of the network state is"
"in addition, the information the lbs provides is selfverifiable, i.e., users can verify the integrity and authenticity of the server responses. this can be done in different ways; in our system, the user device verifies a digital signature of the lbs on each reply by using the lbs provider's public key. as a result, a compromised access point or mobile device cannot degrade the experience of users by altering replies or disseminating expired information."
"our goal in this paper will ultimately be to partition (or split) the graph g such that each resultant component is rigid and follows the properties outlined above. thus, we define the following."
"in this paper, we proposed the conditions under which rigidity-preserving bipartitions are identified and iterative algorithms to perform such an identification. motivation was derived from the implications of rigid networks for example in formation control and localizability, and the flexibility that splitting can provide for a robotic team. our methods exploited the previously considered z-link structure for defining rigid partitions, and a supergraph search mechanism to facilitate the discovery of network z-links. decentralization was then achieved through parallelization of z-link discover, and leader election to distribute the evaluation of potential graph cuts. finally, simulation results corroborated our claims of algorithm correctness and guaranteed polynomial complexity."
"under the above network conditions, 4 we address the decentralization with an algorithm that operates in two phases. first, all agents search in parallel for the z-links and build decentralized z-link sets which guide graph exploration. then, auctions are applied to determine leaders in order to sequentially evaluate all local z-link sets in the graph, until a cut is found. for convenience, we will denote by d our decentralization of algorithm 1, with pseudocode given in algorithm 5."
"future work will focus on optimally rigid bipartitioning, multipartitioning, possible application in a 3-d workspace, integration with obstacle avoidance and detection for environment appropriate splitting, and real-world application."
"the microwave dielectric properties, namely permittivity and conductivity have been widely used to exploit the diagnostic and therapeutic potential of microwaves. this is enabled by the inherent dielectric property discrepancy between healthy and abnormal tissues. although the dielectric properties of many 90 different biological tissues and biological anomalies have been widely reported in the literature, there are very few reported studies on the microwave dielectric property behavior of renal calculi. this work explored the inherent dielectric property discrepancy between different renal calculi types and exploited this property to classify the discarded samples by utilizing a machine learning algorithm. in this section, we first emphasize the significance of the open-ended coaxial probe dielectric property measurement method and explain the features that were given as inputs to the knn algorithm. both the open-ended coaxial probe technique and the knn algorithm are explained in great length in the literature; thus, both topics are discussed only briefly in this work. 100"
"relying on hidden markov models, the bayesian inference framework quantifies the correctness with which an adversary can estimate the location of users over time. the error of the adversary in this estimation is exactly our privacy metric [cit] . we evaluate mobicrowd on a real locationtrace data set and we show that it provides a high level of privacy for users with different mobility patterns, against an adversary with varying background knowledge."
"in the no protection scenario, users submit their queries directly and immediately to the server without using any protection mechanism. this scenario reflects the risk of unprotected use of lbss. we compute privacy against the observation and against the inference adversaries."
"a popular technique that enhances privacy against local eavesdroppers is to change identifiers frequently. cellular network operators make use of network-issued pseudonyms (tmsis) to protect the location-privacy of their users [cit] . mobicrowd-ready mobile devices can also mimic this defense (as has already been proposed for wireless networks, e.g., [cit] ). they can change their identifiers (e.g., the mac addresses) as often as desired, even while in a single pointof-interest area. this would essentially root out any threat by any curious local observer. even in the case of a stalker, it would not be possible to link together the successive identifiers of a device, as multiple users' identifiers will be mixed together. the only remaining option for the stalker is to maintain visual contact with the target user, but defending against this threat is clearly orthogonal to our problem."
"we perform measurements on a 5-device testbed to estimate the delay for obtaining a peer response. three out of the five are randomly chosen to collaborate each time. mobiles access the lbs server over a cellular link (e.g., gsm) and communicate with each other via the wi-fi interface. averaged over 100 queries, the delay is 0:17 sec. we also note that cryptographic delays are (for a typical openssl distribution) low: the weakest of the three devices, the n800, can verify more than 460 rsa signatures per second (1;024 bit), or 130 signature verification per second (for 2;048 bit modulus); this implies that the digitally signed lbs responses can be easily handled by the devices to protect against malicious peers."
"although lbss are convenient, disclosing location information can be dangerous. each time an lbs query is submitted, private information is revealed. users can be linked to their locations, and multiple pieces of such information can be linked together. they can then be profiled, which leads to unsolicited targeted advertisements or price discrimination."
"we can see the additional damage caused by an inference adversary, compared to an observer, by comparing corresponding (observation) and (inference) scenarios. there is a difference of about 3â for the individuals' mobility model, and a much smaller one, 15-30 percent, for the average mobility model. this is to be expected, as the quality of the inference depends heavily on the quality of the background knowledge."
"the direct objective of mobicrowd is to hide user queries from the server. we quantify this objective, as our first evaluation metric, through the hiding probability: the probability that a user's query becomes hidden from the server due to mobicrowd protocol. under various user mobility and information spreading dynamics, we compute this metric using the results of the time-dependent epidemic model, and we compare to the results of simulations on a data set of real mobility traces. in section 7, we show that the simulation results corroborate our model-based findings about the hiding probability."
"a detailed work on the dielectric properties of three renal calculi categories, calcium oxalate, cystine, and struvite, was carried out for classification. addi-180 tionally, another study was performed by the authors on the dielectric properties of renal calculi with a limited number of samples covering the frequency range of 0.5 ghz to 18 ghz [cit] . complex permittivity data composed of dielectric constant ( ) and dielectric loss factor were obtained via the open-ended coaxial probe technique, which is widely used to measure broadband dielectric 185 properties of biomaterials in a laboratory environment. the properties of the renal calculi samples, the experimental setup, and the classification procedures used are detailed in the following sections."
"even worse, the habits, personal and private preferences, religious beliefs, and political affiliations, for example, can be inferred from a user's whereabouts. this could make her the target of blackmail or harassment. finally, real-time location disclosure leaves a person vulnerable to absence disclosure attacks: learning that someone is away from home could enable someone to break into her house or blackmail her [cit] . an stalker can also exploit the location information."
"in each of the sub-figures, the baseline (inference) and no protection (inference) scenarios reflect the risk of using location-based services without any protection. even an adversary with knowledge of the average mobility can significantly decrease users' location privacy, hence the extreme need to employ privacy enhancing protocols such as mobicrowd."
"this study proposed to employ the open-ended coaxial probe technique to collect dielectric property measurements. in comparison to the techniques previously employed in studies, the open-ended coaxial probe technique requires minimal sample preparation (e.g., machining of the sample is not required) and is able to perform broadband dielectric property measurement. in this work, dielectric properties of samples belonging to three different renal calculi types 70 of calcium oxalate, cystine, and struvite were measured between 500 mhz to 6 ghz. one pole cole-cole equations, a mathematical expression frequently used for expressing the dielectric property behavior of biological tissues over wide frequency ranges, were fitted to the measurements, and the parameters of the cole-cole equations were used as features for the k-nearest neighbors algo-75 rithm. the algorithm was then utilized for determining the class of renal calculi samples. the promising results indicated that the technique can be employed for rapid determination of discarded renal calculi types to enable the necessary measures for prevention of the disease. this paper is organized as follows: dielectric property measurement and the 80 machine learning algorithm are explained in section 2. the sample preparation and dielectric property measurement setup are given in section 3. the dielectric property measurements are provided in section 5.1, and the knn algorithm results are given in section 5.3. finally, the conclusions drawn are discussed in section 7. 85"
"the performance of our system depends on various parameters, such as the rate of contacts and the level of collaboration between users, the rate of lbs query generation, etc."
"the information that the lbs provides expires periodically, in the sense that it is no longer valid. note that information expiration is not equivalent to the user accessing the lbs: a user accesses the lbs when her information has expired and she wishes to receive the most up-to-date version of it."
"the dielectric properties of the renal calculi samples were measured using a slim-form open-ended coaxial probe with an aperture size of 2.2 mm. the s-parameters were measured with an agilent n5245a pna-x microwave network analyzer (shown in fig. 1) . the pna-x was connected to the probe with a 50 ω rf-cable. both the vna and the open-ended coaxial probe have char-210 acteristic impedance of 50 ω as well, which ensures impedance matching. the impedance of the open-ended coaxial probe was determined by the diameters of the inner and outer conductors and the relative permittivity of the dielectric material sandwiched between them. the probe utilized in this work had an outer conductor diameter of 2.2 mm and an inner conductor diameter of 0.6 215 mm, and the material between the concentric conductors was teflon. the vna was also connected via local area network (lan) to a notebook computer, where a commercially available agilent 85070e software was used for converting the scattering parameters to material dielectric properties."
"in conclusion, we designed a new pcf-lpg methane sensor considering transverse-stress compensation, and the methane sensitivity can reach up to 6.39 nm/%. the application of side-hole structure makes it easier to coat methane-sensitive film onto the inner surface of holes. especially, the large holes are conducive to the rapid diffusion and fully reaction of methane gas. after comprehensive consideration of photoelastic effect and material deformation caused by transversestress, we can achieve high sensitivity and accuracy of methane measurement acknowledgment thanks to the modern analysis and computing center of cumt for providing comsol software."
"s martphones, among other increasingly powerful mobile computing devices, offer various methods of localization. integrated gps receivers, or positioning services based on nearby communication infrastructure (wi-fi access points or base stations of cellular networks), enable users to position themselves fairly accurately, which has led to a wide offering of location-based services (lbss). such services can be queried by users to provide real-time information related to the current position and surroundings of the device, e.g., contextual data about points of interest such as petrol stations, or more dynamic information such as traffic conditions. the value of lbss is in their ability to obtain on the fly up-to-date information."
"having specified the transition and observation probabilities, we run the forward-backward algorithm (for hidden markov models) to compute the localization probabilities for each time t. we then compute their average value over all time units t to compute the location privacy of users of our privacy-preserving scheme for various system parameters."
"based on the stated design objectives, we propose a novel location-privacy preserving mechanism for lbss. to take advantage of the high effectiveness of hiding user queries from the server, which minimizes the exposed information about the users' location to the server, we propose a mechanism in which a user can hide in the mobile crowd while using the service."
"to compare the performance of the knn for three renal calculi types with other machine learning algorithms, we applied the ann method to the problem of classification of renal calculi. the ann algorithm mimics the neural structure of the brain and has been utilized in other studies for different tasks, including but not limited to autonomous driving and medical diagnostics. briefly, the ann algorithm employs concatenated artificial neurons, where weighted arti-355 ficial neuron inputs mimic the dendrites, node represents the soma, and the weighted output represents the axon in a biological neuron. the nodes sum the weighted inputs and pass them from an activation function that can be a linear, step, or sigmoid function. in this study, a tangent sigmoid function was used as an activation function. then the output was taken and passed to another 360 layer of concatenated neurons. the ann algorithm works by optimizing the weights of the inputs and outputs via gradient descent backpropagation. the number of layers and inputs depend on the neural net, and for this work, it was determined via trial and error. in this work, we employed one hidden layer with 15 neurons. the input chosen for the neural network was the cole-cole param-365 eters along with bias, and the outputs of the network were the three classes of calcium oxalate, cystine, and struvite. the inputs were normalized in the range of (-1, 1) since the normalization enabled fast convergence. finally, the chosen learning rate was between 0 to 1, adjusted along with the weights through back propagation during training. the training and testing were then performed, 370 and the results of the ann algorithm, as well as the knn results, are given in table 4 . as seen in table 4, knn outperformed the ann, especially when the f1 score is considered. the performance of the ann could be further optimized by adopting different activation functions or even by adding layers to the neural 375 network. ultimately, we expect that the performance of both algorithms would improve with a significant increase in sample size. one should also consider that the computational cost of knn is higher for more data."
"a microwave dielectric property based knn renal calculi classification method was presented in this work. dielectric properties of three different renal calculi 400 types were measured with the slim-form open-ended coaxial probe technique between 500 mhz and 6 ghz, with 100 mhz frequency steps. the medians of the dielectric properties were calculated, and an inherent dielectric property discrepancy was observed in the different renal calculi types. the cole-cole parameters were then fitted to measurement data, which aided in the represen-405 tation of dielectric property measurement data with only five parameters. the knn algorithm was then employed for classification of the renal calculi, and the cole-cole parameters were used for training and testing the algorithm. the benefits of the proposed method include rapid measurement, minimal sample preparation requirements, and an automated decision making mechanism that 410 can eliminate personnel costs, decrease diagnosis time, and decrease equipment costs. it should also be noted that the system itself is simple and requires very little output power. in addition, unlike x-rays, no special chamber is required to confine the low-power microwaves."
"there exist cryptographic approaches that redesign the lbs: the service operator does not learn much about the users' queries, though it can still reply to their queries [cit] or can obtain imprecise information about user location [cit] . the lack of incentives for lbs operators to change their business model and implement these solutions, and their high computational overhead, have made them impractical so far."
"to be able to isolate the effect of collaboration, we study the case where there is no collaboration among users and mobicrowd relies only on its buffer to protect users' privacy: a user who becomes interested checks her buffer, and if the content is not there, she immediately contacts the server. thus, there are no seeker (s and s ã ) users in the model for this case for the stationary regime analysis, we compute the equilibrium point of the system, and study its stability as before"
"the optimization results are shown in figure 2, it can be found from simulation results that the path network which obtained through traditional abc algorithm could get the initial path from the starting point to the target point, but the cost value and smoothness of the obtained path can't meet the requirement of robot motion constraint, using the random and good searching ability of mdabc algorithm and the characteristics of flexible transformation of solution, to take iterative optimization for path network."
"we want a model that describes transitions between, and keeps track of, the various states a user is in as time progresses. however, it is prohibitively complex to keep track of the state of each individual user. therefore, we make use of the mean field approximation [cit], which focuses on the fraction of users in each state; these fractions are collectively called the network state. the approximation applies when the number of users is large and each individual interaction contributes a vanishingly small change to the network state. the approximation requires a random contact pattern among users, rather than a spatially correlated pattern, and random contacts are not far from reality when users are clustered in the same region (recall that we partition the whole area into regions)."
"we implemented our scheme on nokia n800, n810 and n900 mobile devices, and we demonstrated it with the maemo mapper (a geographical mapping software for points of interest) [cit] . our approach can be used in the upcoming technologies that enable mobile devices to directly communicate to each other via (more energyefficient) wi-fi-based technologies [cit] that aim at constructing a mobile social network between mobile users."
"the collective mobility of users with respect to a region is modeled using three parameters: b, the average number of times a user makes a proximity contact with other users per time unit within a region; m, the probability of an outsider user entering a region within a time unit; and, the probability of an insider user leaving a region within a time unit. we derive these parameters from the markov mobility models of users, as follows. let parameters i and m i be the probabilities of exiting and entering region r i, respectively. they correspond to the expected number of users who exit/ enter r i normalized by the expected number of users who are inside/outside of r i,"
"next, we need to optimize the structural parameters for better sensing performance. then, the gas sensitivity of lpg with different d and t ch4 can be obtained as shown in fig. 3 . fig.4 indicates the variation of the loss spectra of the core fundamental mode in y-polarization direction with different parameters. in other words, we can manipulate the loss peak, resonant wavelength and sensitivity by adjusting such structural parameters. the side-holes should be large enough when the gas sensitivity measured by the lpg is still sufficiently high. in addition, we need to make the loss peak at the communication window of 1550 nm. therefore, we select the parameters as shown in table. 1 after all these comprehensive considerations. fig.5 (a) describes the transmission spectrum of pcf-lpg, the coupling between lp 01 mode and lp 02 mode happens at around 1308 nm. fig.5 (b) show the loss spectrum of core fundamental mode in y-polarization direction. the illustrations (a) represent the electric field mode distribution of the lp 02 mode. (b-d) show the electric field mode distributions of the core mode, the spp mode, and the resonant coupling, respectively. the coupling between y-polarized core mode and spp mode occurs at 1570nm. in summary, the resonance wavelength of transmission spectrum and loss spectrum will have a peak-shift when the ri of methane-sensitive film or transverse stress changes."
"adding dummy queries to the user's queries might help to confuse the adversary about the real user location. but generating effective dummy queries that divert the adversary is a difficult task [cit], as they need to look like actual queries over space and time. an optimum algorithm for generating dummy queries is an open problem."
", it will increase the robot's control error, if the number of navigation points and r are changed, it ensures the smooth sailing route and meet the kinematic constraint of the robot, it is shown as figure 1(c)."
"we consider n users who move in an area split into m discrete regions/locations. the mobility of each user u is a discrete-time markov chain on the set of regions: the probability that user u, currently in region r i, will next visit region r j is denoted by p u ðr j jr i þ. let p u ðr i þ be the probability that user u is in region r i ."
"we quantify the location privacy of users as the expected error of the adversary in estimating the actual location of each user at each time instant [cit] . the more queries the adversary observes, the more successful he will be in reconstructing their actual trajectories; so privacy is proportional to the distortion of the reconstructed trajectories."
"algorithm initialization parameter setting are shown in table 1. applying the method of endpoint elicitation to generate path network to robot group, avoiding all kinds of threats successfully and reaching the target point, then using mdabc to optimize path network, to obtain the path network which meets the requirements of the value and smoothness."
"our approach avoids the problems of these two extremes by having users collaborate with each other to jointly improve their privacy, without the need for a trusted third-party (ttp). in effect, the mobile crowd acts as a ttp, and the protection mechanism becomes a distributed protocol among users. mobile users concerned about their location privacy are indeed the most motivated to engage in protecting themselves. we require no change in the lbs server architecture and its normal operation, and we make no assumption on the trustworthiness of the lbs or any third-party server."
"the difference, approximately 35 percent, between location privacy in mobicrowd (observation) and no protection (observation) shows the added value of mobicrowd with respect to an observer (e.g., a curious but not adversarial lbs operator). however, these privacy values do not constitute a lower bound on user privacy, as an inference adversary can estimate the actual location of users more accurately."
"we build a mobile transparent proxy in each device that maintains a buffer with location-specific information. this buffer keeps the replies the user obtains from the server or other peers. each piece of information associated with a given region has an expiration time (which is attached to the information and protected with the digital signature), after which the information is no longer valid. invalid information is removed from the buffer."
"proof: from theorem 5, we know that the number of z-links in the graph bounded by o(n 2 ). in the decentralized algorithm each z-link can be directly identified with cost o(1). for each z-link, a traversal of the graph is performed that is upper bounded by o(n) complexity, yielding o(n 3 ) overall, our desired result."
"informed. users who have information about the region are in the informed state. if they are inside the region (called insider informed), they accept to spread the information at each contact with a seeker user with probability f. this is because the information spreading process imposes some communication cost on informed users, hence they might not always collaborate. if they are outside the region (called outsider informed), we assume they do not spread the information. the information that the informed users have, whether they are inside or outside the region, expires with rate d and the users become removed."
"the difficulty of the problem lies in protecting privacy of users who also want to earn the benefits of lbss. therefore, solutions such as not using lbss are not acceptable. for instance, a user could download a large volume of data and then search through it for specific context information as the need arises. but this would be cumbersome, if not impractical, and it would be inefficient for obtaining information that changes dynamically over time."
"since the external environment often changes randomly during the sensing process, we define a two-dimensional parameter (ch 4 (%), f (n )) to match the actual situation. three random sampling points including a (0.9%, 4.6n ), b (1.5%, 2.8n ) and c (2.7%, 1.1n ) are selected as an example. the actual values of the methane concentration change come from the selected sampling points. fig. 12 (a) and (b) show the transmission and loss spectra of the pcf under the corresponding conditions, respectively. table 2 shows the shifts of the resonance wavelength under above three conditions, and the calculation results show that they are consistent with the actual situation. obviously, the measurement accuracy is higher when the transverse-stress compensation is considered."
"the added value of mobicrowd against an inference adversary is about 50 percent, when the adversary's knowledge is individual mobility model, and a bit less than 50 percent when the knowledge is average."
"the cross-section of proposed pcf is shown in fig.1(a), and six ultra-large holes symmetrically distributed in the outermost layer. the side-hole on the right is plated with silver nano-film having a thickness t when other five holes are coated with methane-sensitive film with the thickness t ch 4 . here we select the ultraviolet curable fluoro-siloxane nano-film incorporating cryptophane a as methane-senstive film. among them, the diameter of smaller hole near the core is d 1, and the diameter of ultra-large hole is d 2 . the lattice period is, and the distance between large hole and core is d. we can prepare such pcf through multi-step \"stack and draw\" procedure [cit] . silica rods and capillaries are stacked together and drawn down by using a standard fiber drawing tower. fig.1 (c) indicats the experimental setup, and the predetermined methane gas is introduced into the gas chamber to have a free diffusion in the cladding holes. the x-polarization light can be filtered out of the light emmitted from broadband source (bbs) by polarization controller (pc), and then y-polarized light travels to pcf through smf. finally, we can obtain responding spectra in optical spectrum analyzer (osa)."
"the rest of the paper is organized as follows. we survey the related work in section 2. in section 3, we describe our assumptions for the location-based service, for mobile users and the adversary, and we state our design objectives. we present mobicrowd in section 4, and then we develop an epidemic model of its operation in section 5. we present our bayesian localization attacks in section 6. we evaluate the effectiveness of mobicrowd in section 7."
"the adversary's background knowledge on user mobility, which can be -the mobility model of each individual user (individuals' mobility model), or -the average mobility model of the whole user population (average mobility model). the adversary's method of attack, which can consist of -just observing exposed locations, i.e., not trying to guess a user's locations between two queries (observation adversary), or -perpetrating bayesian localization attacks to infer the whole location trace of each user (inference adversary). we compute privacy for multiple combinations of these factors, with and without our protection protocol. the concrete scenarios we study are baseline: inference without observations. no protection versus observation/inference. mobicrowd versus observation/inference. in the baseline scenario, we compute privacy against the inference attack, assuming that the adversary ignores his lbs observations and relies only on his background knowledge. this scenario quantifies the extent to which the adversary's knowledge is by itself sufficient to predict the users' locations over time. it is a baseline scenario, in the sense that no privacy mechanism can achieve better privacy than this."
"m is the number of nectar, it is equal to the number of employed bees. formula (6) is suitable to solve maximum problem, in order to apply the abc algorithm to solve the minimum value problem, we take the fitn ess calculation formula of nectar as the following improvement:"
"removed. users who do not have information and are not currently interested in obtaining information are in the removed state. we distinguish between insider removed and outsider removed users. an insider removed user becomes a seeker if the user becomes interested in obtaining information about the region. as lbs users usually query information about the region they are in, we assume that outsiders have to enter the region to become interested."
"all this information is collected by the lbs operators. so, they might be tempted to misuse their rich data by, e.g., selling it to advertisers or to private investigators. the mere existence of such valuable data is an invitation to attackers, who could break into the lbs servers and obtain logs of user queries, or governments that want to detect and suppress dissident behavior. the result in all cases is the same: user-sensitive data fall in the hands of untrusted parties."
"lbs servers concentrate location information from all user queries. thus, an untrusted service provider could act as a \"big brother,\" that is, it could monitor user whereabouts and activities over time. in such a setting, the adversary can be categorized as a passive global long-term observer [cit] . we assume the adversary has some background knowledge about the users' mobility patterns. this background knowledge consists of each user's mobility model, expressed as a markov chain, the users' lbs access frequency, and the information lifetime."
"we evaluate mobicrowd through both an epidemic-based differential equation model and a bayesian framework for location inference attacks. the epidemic model is a novel approach to evaluating a distributed location-privacy protocol. it helps us analyze how the parameters of our scheme, combined with a time-dependent model of the users' mobility, could cause a high or low-degree privacy. we validate the modelbased results (on the probability of hiding a user from the server) with simulations on real mobility traces. we find that our epidemic model is a very good approximation of the real protocol; it reflects the precise hiding probability of a user."
"in fig. 2, an example of henneberg construction is given. finally, we provide an important result that relates the henneberg construction to minimally rigid graphs."
"remark 2: it is important to note that we will be restricted to two-hop decentralization precisely by the nature of the rigidity problem. as pointed out in [48, lemma 3.3], \"for an agent i, the local graph containing neighboring nodes and incident edges possesses only independent edges.\" that is, at least locally all edges appear necessary for a graph to remain rigid, and thus they will be perceived as nonremovable in a cut operation. therefore, as expressed by the z-link structure, we must consider at least two-hop information when determining cuts in the network."
"g 2 ← visited 12: return true 13: end if 14: return false 15: end procedure theorem 4: consider a minimally rigid graph g in the plane. by construction, algorithm 1 and its constituent components exhibit polynomial complexity when applied to g, and therefore the rigid bipartitioning problem is solved in polynomial time."
"bee colony algorithm has good flexibility, multi functionality and robustness in solving optimization problem, but the dimension of the solution is certain, this i s difficult to meet the requirements in solving multi path or group system path network optimization problem. the number of navigation points is the dimension of the solution in the method of generating path generation for endpoint, the distance between the path points is the length of a single route segment r. if the number of navigation points (the dimension of the solution) and the r are fixed, it's hard to get a smooth path if we just change the navigation point to optimize the route, it is shown in figure 1(a), initialization path is s-p 1 -p 2 -p 3 -g, optimized path is s-p 1 -p 2 -p 3 -g, if you keep the number of the navigation points unchanged and change r, it is shown as figure 1(b), this will lead to the optimized route segment less than the minimum line movement distance of the robot's requirements, such as 3"
"user-centric approaches operate on the device. typically they aim to blur the location information by, for example, having the user's smartphone submit inaccurate, noisy gps coordinates to the lbs server. however, obfuscation approaches that protect user location-privacy can degrade the user experience if users need high privacy, e.g., lbs responses would be inaccurate or untimely. obfuscation also is not effective against absence disclosure [cit] ."
"the rationale behind our scheme is that users who already have some location-specific information (originally given by the service provider) can pass it to other users who are seeking such information. they can do so in a wireless peer-to-peer manner. simply put, information about a location can \"remain\" around the location it relates to and change hands several times before it expires. our proposed collaborative scheme enables many users to get such location-specific information from each other without contacting the server, hence minimizing the disclosure of their location information to the adversary."
"j is the shortest distance of the path segments required by the individual, the goal is to make the path network to meet the individual movement of the minimum linear movement constraints, its values could be obtained through formula (4)"
"the knn algorithm works by categorizing data via correlating inputs to similar outputs. in a sense, as the algorithm confronts unknown data, it investigates similar instances from the training set. two design parameters -the number of nearest neighbors, k, and the distance between data points -were 170 adjusted while developing the knn model. the distance could be calculated with euclidean distance, manhattan distance, or minkowski distance relations. to train the model, data in the training set were positioned on a coordinate system that is compatible with the data dimensions. during the testing stage, unknown data were placed into same coordinate system to specify the knn."
"as expected, the adversary does considerably better when using each user's own mobility model in the attack, rather than using the average mobility model for everyone. more precisely, the success probability of our bayesian inference attack, in estimating a user's location between two successive observations, significantly increases if we provide the adversary with a more precise mobility model. however, we see that here again mobicrowd helps when it is most needed, and significantly improves (up to 550 percent) the users' location privacy when the adversary is very powerful due to his accurate background knowledge."
"in recent years, with the rapid development of heuristic optimization algorithm, some bionic intelligent optimization algorithms have been widely used in the problem of path optimization, such as genetic algorithm, particle swarm optimization algorithm and ant colony algorithm, and achieving a good effect according to the different problems of the model [cit] ."
"the location traces that we use belong to 509 randomly chosen mobile users (vehicles) from the epfl/mobility data set at crawdad [cit] . we set the time unit of the simulation to 5 minutes and we consider the users' locations at integer multiples of the time unit, thus synchronizing all the traces. we group time units into three equal-size time periods: morning, afternoon, evening. we divide the bay area into 10 â 25 equal-size regions. two users in a region are considered to be neighbors of each other if they are within 100 m of each other (using wi-fi). we run our simulation for 100 times on the traces and compute the average of the results."
"remark 4: in electing leaders to inspect local z-link sets, it is apparent that by choosing appropriate bid fitness, we can control the order in which leaders are elected, and thus the order of z-link evaluation. this immediately admits control of the identified partition, particularly when there are multiple possible solutions. we suggest that future work aimed at determining bid fitness for example for optimal rigid partitions, may prove useful."
"the user is insider informed with probability i iþr . by definition, the query of an insider informed user is immediately answered by the buffer. so, her hiding probability is 1."
"remark 1: the extension of laman's conditions to higher dimensions is at present an unresolved problem in rigidity theory, and thus our contributions will be limited to the case of multiagent networks in the plane. while there exist special cases under which extensions to three dimensions have been achieved, we point out that results in the plane do not necessarily limit application. for example, in aerial platforms, rigidity may only be necessary in a 2-d subspace, where team altitude is often fixed or relatively easy to measure."
"a represents the variation of gas concentration (percentage) volume 7, 2019 or transverse-stress (n ). therefore, the units of sensitivity are nm/% and nm/n, respectively."
"when an agent becomes the leader, it must determine whether each z-link to which it contributes is a proper cut, i.e., if it leads to a k-bipartition of the graph. in our decentralized approach, we have addressed the cut search problem by modifying the classical dfs to yield significantly improved complexity (as will be demonstrated in section v). in the basic dfs algorithm, each node is marked as it is visited, in order to guarantee search correctness. however, in its standard form, dfs visitation conveys no insight into partition rigidity. we will show that with simple modifications, we are able to reduce both the size of the z-link search space by discarding infeasible z-links, and the time required for graph exploration using the implications of previously discovered cuts."
"predictive or descriptive machine learning algorithms have been applied to 140 solve many different problems, including but not limited to data mining, natural language processing, image recognition, and expert systems. machine learning algorithms are known to be particularly effective when the exact relationship between inputs and outputs of a process are not apparent. machine learning, a subfield of artificial intelligence, refers to the computer's ability to learn from 145 training data or past experiences and successfully generalize the model to a test set. in machine learning algorithms, data can be processed with supervised or unsupervised learning approaches. both inputs and their corresponding outputs are used in supervised machine learning algorithms, while unsupervised ones utilize only inputs to train the algorithm. considering this description, the type of machine learning algorithm is determined by input types, the presence of outputs, and desired outputs. for classification of renal calculi, supervised machine learning models such as support vector machine (svm), artificial neural network (ann), k-nearest neighbors (knn)and other supervised learning algorithms, can be used since both the inputs and the outputs of the training set 155 are known."
"section 5.1 reports the dielectric measurement results for the renal calculi samples, the cole-cole fitting results for the dielectric properties are given in section 5.2, and section 5.3 describes the renal calculi classification results using 285 knn."
"through environment modeling. the group system is composed of n independent individuals, task set is composed of the m tasks which are distributed in the working group. individual collections can be expressed as t . the difference is the task executor is a smart individual robot set r comparing to the single robot path planning, and task points are also extended to a task set t, we could obtain the initial position"
"the baseline privacy in figs. 3a and 3b show how much information is embedded in the background knowledge of the adversary, i.e., how accurately he can predict users' locations, relying only on their mobility models."
"overall, our goal is to design a practical and highly effective location-privacy preserving mechanism for lbss: we want to protect privacy with a minimal compromise on lbs quality of service. the nature of existing threats and the structure of stakeholder incentives, outlined earlier, is the determining factor of our design objectives."
"the need to enhance privacy for lbs users is understood and several solutions have been proposed, falling roughly into two main categories: centralized and user-centric."
"we now describe a model for mobicrowd, with the help of which we can directly evaluate the effect of various parameters on users' location privacy. observing the effect of the parameters also helps when designing a system and testing \"what-if\" scenarios. for example, we can immediately see the level of collaboration required to achieve a desired privacy level or how the privacy level will change if the users make queries more frequently or less frequently. we draw an analogy between our system and epidemic phenomena: location-context information spreads like an infection from one user to another, depending on the user state (seeking information, having valid information, etc.). for example, a seeker becomes \"infected\" when meeting an \"infected\" user, that is, a user with valid information."
"the mean-field approximation tells us that the time evolution of the fraction of users in each state can be described with increasing accuracy, as the number of users grows, by a system of ordinary differential equations (odes). by studying the system of odes, we find the steady state(s) to which the network converges. similar models have been used in human virus epidemics [cit], in worm propagation in wireless networks [cit], and in research on forwarding/ gossiping protocols [cit] ."
"in all the above-mentioned mechanisms, there is always a trade-off between users' privacy and the quality of service they experience [cit] . the tension is maximized when it comes to hiding queries from the lbs server. hiding a query from the server minimizes the revealed user information and thus maximizes her privacy with respect to that query. simply put, it is more effective than the other three privacyprotection methods, and it protects users against both presence and absence disclosure. this is what mobicrowd provides: it hides users from the server, yet allows them to receive the query responses from other peers."
"the prevalence of renal calculi and the types depend on the geographic, climatic, and dietary conditions as well as the race, sex, and age of the patient. therefore, the availability of these samples depends on the patients visiting a 335 clinic, the location of the clinics, and also the severity of the disease (e.g., the renal calculi can be disintegrated in vivo by response to a nonsurgical treatment such as oral medications and extracorporeal shockwave lithotripsy (eswl) allowing it to be passed naturally). uric acid is a relatively rarely diagnosed renal calculi type that composes 10% of all renal calculi types globally, and the prevalence varies geographically, being 2.1% in texas, usa, and 15.8% in okinawa, japan [cit] . more importantly, uric acid is known to be soluble in high ph urea. since it is known to be responsive to nonsurgical treatments, for patients with uric acid renal calculi, oral chemolysis is generally prescribed [cit] . most urolithasis patients with uric acid respond well to oral medications. in some 345 cases, this treatment is accompanied with eswl and pcnl, and pncl is preferred when the uric acid sample is larger than 2 cm [cit] . due to the high solubility of uric acid renal calculi, the uric acid samples were not available at the hospital. hence, uric acid was not included in this work."
"as our second evaluation metric, we quantify the location privacy that mobicrowd offers to users against localization attacks. specifically, we compute the expected error of an adversary who observes a user's trace and then forms a probabilistic estimate of her location. this probabilistic estimate is based on a bayesian location inference approach [cit] that enables us to incorporate both the background knowledge and observation of the adversary and to precisely quantify the location privacy of users. we link this bayesian inference to our epidemic model, by computing the observation probability of the adversary from the hiding probability of mobicrowd."
"the correctness of the algorithm follows from the preceding propositions of this section. as will be shown in section v, our proposed algorithm executes in o(n 3 ) time, in a monte carlo sense."
"we do not address the threat of local observers sniffing the wireless channel trying to infer users' private information, as such a threat could exist with or without mobicrowd, and it can be alleviated by frequently changing device identifiers (e.g., changing mac addresses for wi-fi networks [cit] similar to changing tmsi for gsm networks [cit] ). more importantly, local observers, to be effective, would need to be physically present next to any given victim user, over long periods of time and across different locations. in contrast, a centralized lbs can by default observe all the queries of a user, which is why we focus on this much greater threat in this paper."
"as seen from fig. 5, all performance measures of this classifier reached to 100% in at least one fold. among 10 folds, the minimum obtained values of ac-"
"note that this joint epidemic/bayesian evaluation is necessary and, in fact, a significant component of our approach, as mobicrowd is a distributed protocol running on multiple collaborating devices, so its performance depends on network characteristics (e.g., time-dependent mobility), not just on what an individual device does. the focus of the existing work in the literature is more on privacy-preserving functions (e.g., obfuscation functions run independently by each user [cit] . to the best of our knowledge, this is the first such evaluation, and it is significantly more realistic than our own previous work [cit] that quantified privacy with just the fraction of queries hidden from the server."
"in the initial setting of the algorithm, initializing m initial solution ) is d dimension vector which is consistent with the optimization parameters. each observation bee choose the nectar which they need to go to collec t honey with a certain probability i p according to russian roulette mechanism, i p is defined as:"
"to keep the presentation simple, we focus on one type of context information, hence we consider a single average information lifetime. no loss of generality results from this, because, to model a complete system with multiple types of information, we can merge multiple versions of this model, one for each type."
"for the sake of completeness, we conclude by analyzing the correctness, finite termination and cost properties of the d algorithm. first, we formally establish the stopping condition for the d algorithm."
"from figure 3 and figure 4, we can obtain the generation value and fitness value of the path network are optimized by the 100 generation of the algorithm, optimization is trivial during 100 to 400, because the path optimization problem is to find a feasible path which could meet the constraints of the intelligent body movement, the optimization results which obtained in the 0 to 100 generation are fully capable of meeting the needs of the problem. figure 6 represent the changing situation between path length and path smoothness of path network, the specific values of path length are shown in table 2. the sum of the path angle is the sum of each path angle, it represents the smoothness of path, the maximum rotation angle of the path network is less than the maximum angle constraint of the robot, it shows that the optimized path network can satisfy the robot's motion constraint."
"the key idea of our scheme, called mobicrowd, is that users only contact the lbs server if they cannot find the sought information among their peers, i.e., other nearby reachable user devices. hence, users can minimize their location information leakage by hiding in the crowd. clearly, mobicrowd would be most effective when there are many peers gathered at the same location. indeed, this clustering phenomenon has been observed in human mobility studies [cit] . moreover, the places where people gather are points of interest, where users are most likely to query an lbs. thus, mobicrowd would be used exactly where it is most effective."
"other centralized approaches require the lbs to change its operation by, for example, mandating that it process modified queries (submitted in forms that are different from actual user queries, possibly encrypted using pir [cit] ), or that it store data differently (e.g., encrypted or encoded, to allow private access [cit] ). centralized interventions or substantial changes to the lbs operation would be hard to adopt, simply because the lbs providers would have little incentive to fundamentally change their operation. indeed, if a revenue stream is to be lost by user data not being collected, then not many lbs providers can be expected to comply. misaligned incentives have been identified as the root of many security problems [cit] ."
"advantages of the technique make it attractive for using to measure renal calculi dielectric properties. in this study, we aimed to eliminate the heavy machining of the sample, and the disadvantages of the technique were mitigated via 120 the machine learning algorithm. therefore, this study enables the classification of renal calculi with a single dielectric property measurement. in addition, the discarded renal calculi sample will not be destroyed and can be used for further processing."
"which, as we see, is only a function of s and i. the eigenvalues of jðs; iþ evaluated at the equilibrium point can be found by solving the fifth order equation"
"from the location traces, we construct the timedependent mobility model of each individual user, in the format of transition probability matrices (one matrix per time period). we also compute the average mobility model, which reflects how the whole crowd moves. for each region and time period we compute the mobility parameters, m, and b separately (see section 5) ."
"meanwhile, the problem is that the side-hole structure is prone to unnecessary deformation when the fiber is under external force. the photo-elastic effect due to transversestress also leads to a change in effective refractive index. hence, it is necessary to take the transverse-stress compensation into account. based on this concept, we plate the silver nano-film onto the large hole on the right to form a new sensing channel because of spr effect. the deformation of the silver-plated hole has great influence on the imaginary part of the effective refractive index of core mode. then, the methane concentration and stress can be demodulated by the dual-parameter matrix method [cit] .the structural parameters are optimized for this purpose, and the working wavelengths of two sensing mechanisms are manipulated to avoid mutual interference. the results show that the device has a high sensitivity of 6.39 nm/% for methane measurement."
"where · is the standard dot product over r m . that is, the edge lengths (interagent distance) over g are preserved in time, in other words, no edge is compressed or stretched over time."
"the decentralized version of the proposed algorithm follows directly from the centralized formulation, and therefore inherits much of its structure and primary insights. to begin, consider the following assumptions which will facilitate sound agent interaction."
"it may be perceived that evaluating only z-links as candidate cuts could be very limiting in terms of identifying bipartitions in the network. for this reason, to validate our strategy, we performed a monte carlo simulation to determine the ratio of cuts that are overlooked in randomly generated graphs by only considering z-links as candidate cuts. in this regard, fig. 9 shows that the percentage of cuts that are lost is relatively low (at least statistically), that is on the order of twenty-five percent. note that, there may be degenerate cases where z-links fail to identify cuts, however, the monte carlo simulation suggests that, at least in the sense of an average case, the proposed algorithm performs well in identifying cuts in a network. in our opinion, the relatively low impact of considering only z-links represents a reasonable tradeoff for the purposes of decentralization. in order to analyze complexity, an extensive monte carlo analysis for a network with a number of nodes varying from 4 to 20 has been considered. thousand trials were run for each configuration and average values were taken. fig. 10 shows the average (blue solid line) of the number of z-links found in the network by varying the number of nodes, corroborating the conclusion in proposition 5 on the o(n 2 ) scaling of the z-link search problem. our claim of overall polynomial complexity is then verified by fig. 11 showing the average of the number of operations performed by centralized algorithm 1, as a function of the number of nodes. it is apparent that our method exhibits o(n 3 ) complexity, making it a feasible means of rigid splitting in multiagent teams."
"a total of 105 naturally occurring renal calculi samples were obtained from calculi samples are shown in fig. 2 . the sample preparation was minimally laborious compared to other methodologies. since the aperture of the probe has to be fully in contact with the material being tested, the surfaces of the renal calculi samples were sanded lightly to obtain a flat surface."
"we use the location-privacy meter tool [cit] to quantify the location privacy of users as the expected error of the adversary in guessing their correct location, including at times when they do not issue a query, i.e., between two successive lbs queries. we are interested in analyzing the privacy effect of the following factors:"
"we have proposed a novel approach to enhance the privacy of lbs users, to be used against service providers who could extract information from their lbs queries and misuse it. we have developed and evaluated mobicrowd, a scheme that enables lbs users to hide in the crowd and to reduce their exposure while they continue to receive the location context information they need. mobicrowd achieves this by relying on the collaboration between users, who have the incentive and the capability to safeguard their privacy. we have proposed a novel analytical framework to quantify sub-figures (a) and (b) differ in terms of the background knowledge of the adversary (used in the bayesian inference attack). the baseline (infr) graph shows their location privacy against the bayesian inference attack, if the adversary relies only on his background knowledge. the no protection (infr) graph shows users' location privacy against the bayesian inference attack, if they do not use any protection mechanism and submit their queries to the server. the no protection (obs) graph shows location privacy of users in terms of the fraction of times their true location is not exposed to the server, because they didn't have any query. the mobicrowd (infr) shows location privacy of mobicrowd users against the bayesian inference attack. the mobicrowd (obs) shows location privacy of mobicrowd users in terms of the fraction of times their true location is not exposed to the server, due to the protection or lack of a query."
"remark 5: notice that the search improvements described above are easily integrated into the centralized methods of section iii, in cases where decentralization is not an application concern."
"our first design objective is to not rely on architectural changes of the lbs; any such changes would be impractical and highly unlikely to be adopted. relying on centralized trusted third parties (e.g., central anonymity servers) to provide privacy enhancing mechanisms can be as hard as having trusted lbs operators. this leads to our second design objective: no reliance on any third party server to provide privacy protection. in fact, we would like to place the privacy protection exactly where there is incentive and motivation, that is, with the users themselves."
"measured dielectric properties are expressed with cole-cole equation. first, the median of measurements collected from each sample was calculated. next, the cole-cole parameters were fitted to the median dielectric property data."
"where i and r are computed from (4). we can see that if we set the collaboration probability f to zero, the hiding probability becomes equal to (14)."
"location privacy of our distributed protocol. our epidemic model captures the hiding probability for user locations, i. e., the fraction of times when, due to mobicrowd, the adversary does not observe user queries. by relying on this model, our bayesian inference attack estimates the location of users when they hide. our extensive joint epidemic/ bayesian analysis shows a significant improvement thanks to mobicrowd, across both the individual and the average mobility prior knowledge scenarios for the adversary. we have demonstrated the resource efficiency of mobicrowd by implementing it in portable devices. jean-pierre hubaux is currently a professor in the school of computer and communication sciences of epfl and has pioneered research areas such as the security of mobile ad hoc networks and of vehicular networks. he is currently working on data protection in mobile communication systems and healthcare systems. he is a fellow of the ieee and acm."
"finally, our monte carlo analysis illustrates the performance of the decentralized algorithms proposed in section iv. fig. 12 depicts that the average number of operations for the parallelized z-link search, by means of algorithm 7, is o(n 2 ). fig. 13 compares the complexity of the decentralized algorithm 5, using our proposed smart search heuristics, with a standard dfs as in the centralized algorithm 1. it is apparent that our heuristics have asymptotically improved complexity, which is demonstrated 7 to be of order o(n 2 log(n)). 7 our future work aims to provide a formal proof of this complexity. remark 6: it is worth noting that our simulation results indicate that a particular cut, if it exists, can be found in sub o(n 3 ) time, and that the set of all rigidity-preserving cuts can be found in o(n 3 ) time, by storing all encountered cuts. as a point of comparison, consider that evaluating infinitesimal rigidity of a graph requires spectral techniques which exhibit o(n 3 ) complexity in a decentralized context [cit] ."
"seeker. users who are interested in obtaining information (i.e., have requested the information but not yet received it) are in the seeker state. once they have it, they move into the informed state. as long as a seeker user stays in the region that she seeks information about, she is called an insider seeker. these users can receive information from other informed users in the region, or from the server, the ultimate source of information. a seeker who leaves the region after requesting information about that region is called an outsider seeker. an outsider seeker can only receive information from the server, as users need to be in the same region in order to be able to propagate information to each other."
"the only cases where mobicrowd's improvement is below 100 percent is when privacy is already high, in which case a further increase does not really matter, or when information expires too fast, in which case the users are forced to contact the server for most of their queries."
"therefore, we can expect that our bipartition search will possess polynomial complexity; a fact that we corroborate in the sequel. during our search, when a z-link has been identified it is necessary to verify two properties: that it is a cut over g, since there could be some z-links in the graph that are noncuts, and if the two subgraphs obtained by the cut are minimally rigid graphs with k and n − k nodes, as we could discover feasible cuts that do not meet this condition. given the result in theorem 3, i.e., a cut yields two minimally rigid subgraphs over which counting vertices is sufficient, we can simply perform an exploration of the graph by means of a dfs. starting from a node of the z-link, the graph exploration is carried out avoiding all the edges that belong to the z-link. the search ends when no more nodes can be visited. letting v be the set of visited nodes, we can obtain one of the following results."
"techniques proposed to protect location privacy in lbss can be classified based on how they distort the users' queries before the queries reach the lbs server. the queries can be anonymized (by removing users' identities), pseudonymized (by replacing users' real names with temporal identifiers called pseudonyms), or obfuscated (by generalizing or perturbing the spatiotemporal information associated to the queries). queries can also be camouflaged by adding some dummy queries, or be completely eliminated and hidden from the lbs [cit] . combinations of these methods have been employed in the existing (centralized or distributed) mechanisms. we now discuss these approaches in more detail."
"there are many collaborative schemes for mobile networks. mobile users, for example, can collectively build a map of an area [cit] . collaboration is also needed when sharing content or resources (e.g., internet access) with other mobile nodes [cit] ."
"in recent years, with the development of computer technology and communication technology, intelligent robots are playing a more and more important role in human society. intelligent robots can replace humans to perform tasks under dangerous environment and hard environment, to reduce the mission risk and to increase the probability of success, the mission planning and task coordination problems of intelligent robot group have been received more and more attention by researchers [cit] . the aim of path planning is to generate a feasible path which meets the kinematic constraints and specific task constraints of intelligent individual through self environmental awareness and search algorithm under known, partially known and unknown environments [cit] . the first step is to establish environment model of task interval of path planning. establishment methods of existing environmental model are grid method, geometric model, topological model and so on [cit] ."
"where wi and wi are the measured dielectric properties obtained with the 250 open-ended coaxial probe technique andˆ wi andˆ wi are calculated dielectric properties. in this work, these dielectric properties were calculated with the cole-cole equation to evaluate the performance of the suggested cole-cole parameters. last, n is the number of points used within the frequency range of 0.5 ghz to 6 ghz [cit] . n was set at 56 in this study, which is the num-255 ber of points at the measured frequency of interest. the calculated cole-cole parameters were saved to use as inputs for the classification algorithm. the iterative process stops when the algorithm finds the first euclidean distance below a determined threshold and picks the cole-cole parameters that result in a minimum euclidean distance. note that the threshold picked in this study 260 was 0.05."
"for x, where i 5 is the 5 â 5 unit matrix. as we have mentioned, if all the solutions have a strictly negative real part, then the equilibrium point is locally asymptotically stable. moreover, if all the solutions have a strictly negative real part, the equilibrium point persists under small perturbations of the system parameters. that is, if vðyþ is any smooth vector field on r 5, then for sufficiently small the equation"
"at a high-level, the algorithm operates as follows. the set of quads that contain z-links is generated by the swap operations of algorithm 2, and for each z-link we determine if it is a graph cut which yields rigid disjoint components, exploiting the conclusions of proposition 3. starting from a minimally rigid graph g(v, e) the first step is to find an initial z-link containing quad for the supergraph search, which is done in a"
"pos is the position number in the whole solution group, sp is the selection parameter, f it ( ) pos is the nectar of fitness value after the change of new fitness value. according to the new fitness formula, the probability formula of observation bee is as follows:"
"the adversary aims to perform inference attacks against the locations of users. in other words, he uses his background knowledge to estimate the locations from which the users issue queries, but also the locations they visit between successive queries that are not directly disclosed to the lbs."
"proof: according to theorem 5, we know that the number of z-links in the graph is o(n 2 ). in order to check if a z-link is a proper cut a dfs-visit of the graph has to be performed."
"we have tested our method on a data set collected from 35 calcium oxalate, 35 cystine, and 35 struvite renal calculi samples. training and testing data 320 were selected by applying 10 fold cross-validation to avoid over-fitting and to achieve unbiased classification results. calculated performance measures are demonstrated in fig. 5 ."
"this paper puts forward multiple cluster path planning method of the robot which combined endpoint heuristic initialization path network with mdabc. using endpoint heuristic method in route network initial stage, and generating initial solution set in feasible path solution space, then optimizing path network through mdabc algorithm, and putting forward a new abc model during optimization, to adjust the fitness according to dimension changing of the solution in solution space, the optimal solution is selected by greedy selection mechanism, and to obtain the path network point which meets the motion constraints of robot. this new method keeps the global search ability of abc algorithm, and solves the problem of local optimum and robustness which faces multi constraint and multi task of path planning problem of multiple robot cluster. simulation experiments show that the new method can plan a feasible optimal or sub optimal path for the multiple robot cluster under different task complexity and environment complexity, and analyzing the influence of the parameters on the performance through experiment."
"any of the receivers of such a local query might respond to it, by what we term a local reply, as long as it has the information its peer seeks. however, an informed device will not necessarily respond to any received query: this will happen if the device is not only informed, but also willing to collaborate. we design our system with this option for its users; the collaborative status can be set explicitly by the user or automatically recommended or set by the device. simply put, having each user collaborate a limited number of times (a fraction of the times she receives a local query from her neighbors), or during a randomly selected fraction of time, balances the cost of collaboration with the benefit of helping other peers. in practice, this is equivalent to the case where only a fraction of users collaborate."
"in the mobicrowd scenarios, we again compute privacy against the observation/inference adversaries. however, in this case, users make use of mobicrowd, hence their observed traces contain fewer locations than in the no protection scenario."
"by obtaining a local reply, the seeker is now informed while, more importantly, her query has remained hidden from the service provider. no privacy-sensitive information has been exposed to the server and the user has obtained the sought service. of course, in case there is no informed user around the seeker willing to assist her, she has no choice but to contact the server directly. in essence, a subset of users in every region has to contact the lbs to get the updated information, and the rest of the users benefit from the peer-topeer collaboration. intuitively, the higher the proportion of hidden user queries, the higher her location privacy is."
the contact rate b i between users in region r i corresponds to the expected number of contacts of a device within its communication range
"now that, we understand the structural elements we seek in order to partition the graph, let us characterize our worst-case expectations in searching for such cuts (a bound which we tighten in section iii-b)."
"in summary, the 2-algorithm converges towards the true activity and attenuation, even though true nmse and apparent ll curves are nonmonotonous in parts. figure 11 confirms that mlem-osl can replace scatter-mlga in joint estimation, at low-attenuation and at the cost of reduced convergence speed. figure 12 shows the results for the 4-algorithm in the low-attenuation case. due to the fact that each iteration consists of only 2 sub-iterations (2 updates each), the ll and nmse curves appear smoother than the same curves for the 2-algorithm."
"this paper is thus concerned with characterizing s2a reconstruction as a building block in joint estimation of activity and attenuation (joint estimation). we follow three objectives: (1) further understand fundamental properties impacting convergence and convergence speed of s2a algorithms; (2) compare s2a algorithms using simulated data, specifically, in terms of convergence speed, the impact of object size, and improved performance of one algorithm by reducing its input data; and (3) study joint estimation, which implies dropping the assumption of known radiotracer activity images in s2a reconstruction [cit] . therefore, we integrate scatter data into joint estimation by interleaving s2a reconstruction with trues-to-activity reconstruction, as proposed before [cit], as well as with trues-to-attenuation and scatter-to-activity reconstruction."
"in the previous notation of the measurement equation ( [cit], eq. 5) and its derivation ( [cit], eq. 11), the continuous measurement equation is:"
"we first compare the 4-algorithm, (scatter-mlem + trues-mlem) + (scatter-mlga + trues-mltr), to the 2-algorithm in terms of performance on the high-resolution, low-attenuation data; then, we apply only the 4-algorithm to high-resolution, high-attenuation data."
"the plots of apparent (using estimated activity and attenuation) and ideal likelihoods indicate several nonmonotonicities that are overcome by the combination of algorithms. in particular, in the 2-algorithm, increasing the apparent scatter likelihood with mlga updates of the attenuation often decreases the true-activity scatter likelihood, limiting the number of repeated mlga updates that can be concatenated. this observation is one reason for choosing only a single update of each algorithm in each iteration of the 4-algorithm (algorithm 1)."
"depending on the level of integration of pet with other modalities (standalone or multi-modality pet), information from radionuclide transmission sources [cit], x-ray computed tomography (ct) [cit], or magnetic resonance imaging (mri) [cit] can be used. however, radionuclide transmission data suffer from low signal-to-noise ratio, necessitating segmentation to prevent noise in the transmission data from impacting activity images. in pet/ct, 4-d attenuation correction of pet data acquired from a moving subject remains limited due to concerns over radiation doses induced by cine ct imaging. in pet/mri outside of the head/neck area, mri is often incapable of distinguishing bone from air in reasonable scan times [cit] ."
"furthermore, photon energy measurements suffer from uncertainties in the range of 10% fwhm in state-of-the-art pet scanners. this uncertainty leads to blurred estimation of potential scattering locations in the object. so far, it is unclear exactly which energy resolution is required to successfully use this approach in practice, although some comparisons have been made for s2a reconstruction ( [cit], fig. 11 ). in joint nalgorithms, separation of scattered and unscattered photons will be of importance in all sub-algorithms. new detector materials, such as labr 3 [cit] or cadmium zinc telluride [cit], might be needed."
"after statement of the problem, introducing required imaging models for use in s2a reconstruction and joint estimation, we summarize and propose algorithms for both and describe the evaluation data used and the experiments carried out, before presenting and discussing our results."
"we have studied reconstruction of attenuation information from scattered coincidences in pet. unlike other problems regarding reconstruction of activity or attenuation from nonscattered or scattered coincidences, the problem at hand is unique in that the reconstructed quantity appears twice in the measurement equation (2). the problem is therefore nonlinear, with the degree of nonlinearity depending on attenuation, which in mostly water-like objects implies spatial scale 6 ."
"we apply trues-osem 10 4 + scatter-mlga 10 4 to the high-resolution, low-attenuation data. to verify that mlem-osl can replace mlga, we also apply trues-osem 10 4 + mlem-osl 10 to the same data."
"applied to the same phantom at high attenuation, the 4-algorithm converges slower, but in a similarly smooth way as for low attenuation to the true activity and attenuation (figs. 9c, d and 13 ): some nonmonotonicity remains, both at the sub-iteration level (for apparent lls) and at the scale of dozens of sub-iterations (e.g., the true-activity ll early on or the true-attenuation ll, for which we note that it increases for later sub-iterations)."
"in reconstruction of attenuation information from scattered pet coincidences, maximum-likelihood gradient-ascent algorithms provide faster convergence and convergence in more diverse setups than mlem-osl, for which we have presented both analytic and experimental evidence: mlga converges across all spatial scales, while mlem-osl may only converge with smaller objects. nonetheless, mlem-osl can be a lower-complexity alternative to mlga. we have defined a numerical criterion to determine when the simpler and more efficient mlem-osl can be used and described how its performance can be improved by reducing data based on said criterion. finally, joint estimation of activity and attenuation from scattered and nonscattered coincidences has been presented using either mlga or mlem-osl, in particular, in an example where mlaa fails to converge to the correct solution."
"a multi-voxel interpretation of the high-attenuation situation is presented in the appendix: it is of importance in patients with great attenuation-length-electron-density products ρ k 4 . one conclusion from the arguments in the appendix is that it is not straightforward to downsample (or downsize) s2a experiments, as that can transform low attenuation into high attenuation (by downsampling), or vice versa (by downsizing) 5 ."
"in this section, we summarize two recent s2a algorithms and then look at fundamental differences between them. we then propose two novel joint estimation approaches using these algorithms and present our evaluation strategy."
"the amount of scatter as a function of electron density may not be sufficiently well represented by a linearized measurement equation, and (2) may require more careful treatment. to explore the limits of the linearization, we derive a geometrical interpretation as well as a numerical criterion. this criterion is used to distinguish data that can be used for algorithms based on linearization (here, mlem-osl) from data that cannot; further, it is linked to the log-likelihood gradient (11) ."
"motivated by the 1-d results around (17), we are interested in the sign of the gradient of the expected data. in component-wise notation, (12) reads:"
"by characterizing high attenuation, we have found a criterion to separate low-from high-attenuation data and improve mlem-osl convergence speed (fig. 7b ). thus, one strategy to decrease the size of system matrices and tensors, and thus computational complexity, lies in choosing the most useful sors from the full data. here, we only briefly [cit] ."
"for mlem-osl, the linearization of the measurement equation may only be appropriate whenever attenuation effects do not reverse the sign of ∂ȳ i /∂ρ j . since this may be true for some sors i, but not for others, it may be appropriate to remove the latter from the data and apply mlem-osl to the reduced data set: (46) represents an approximate inclusion criterion used later."
"which can be determined knowing only e. since i comprises the scattered photon energy e (25), and each broken lor (i, s) defines which voxels t are part of the respective lowerenergy section (fig. 15a), the effective attenuation lengths l i s,t can be computed. while this reasoning confirms that the shape of (2) does not impede taking into account energydependent attenuation, we did not do so here."
"finally, fig. 14 shows the results of the crosstalk study. while the mlaa 2-algorithm is stuck in an apparent local maximum (fig. 14c), 7 the proposed 4-algorithm is able to not only avoid, but escape from this local maximum and converges towards the true solution ( fig. 14d )."
"for (mostly) unscattered data, where the one value of e represents the photopeak energy window, an sor reduces to an lor, andã i e is replaced byũ l j for activity reconstruction; similarly, in attenuation reconstruction, a i s is replaced by u l j ."
"μ ∝ ρ the linear attenuation coefficient μ is proportional to the electron density ρ: approximately, this is true because in biological (low-z) materials at pet energies, compton scattering is the dominant interaction preventing gamma photon pairs from being detected. at fixed energy, the ratio μ/ρ can be formulated to depend on the mass attenuation coefficient (μ/ρ m ) and the quotient of mass density and electron density (ρ m /ρ). the former is fairly constant across human tissues at pet energies ( [cit], fig. 3 ), and the latter is almost perfectly constant for materials less dense than water and deviates a maximum of 10% for materials three times as dense ( [cit], fig. 1 ). note that μ/ρ may well depend on the photon energy; no assumption about the energy dependence of μ is implied (see the discussion around (35) )."
"the mlem(-osl) update (13) can be written as a scaled gradient ascent, with the gradient given by eqs. (9 and 10) and a vector-valued step size [cit] :"
"in joint estimation, in addition to computing nmses, we are interested in the evolution of several likelihood values. attenuation-and activity-reconstruction algorithms are designed to maximize likelihoods given the true value of all other quantities. for scattered data, these are:"
"this algorithm, called mlem by its authors [cit], is based on subsuming attenuation effects under the system matrix in the linearized measurement equation (3), yielding the mlem update [cit] :"
"the first part of this comparison of mlga and mlem-osl is along the lines of earlier work comparing mlga with 2bp [cit], using additional simulation data with higher numbers of voxels than before. therefore, both algorithms are applied to the (low and high resolution) data described above. due to the small number of voxels, specific features of reconstructed images are of less interest; for the agreement between reconstructed images x with their respective references x true, we therefore report normalized mean squared errors (nmse):"
"while mlga converges, we find it to be more computationally complex than mlem-osl ( fig. 8) . also, mlem-osl with reduced data is more complex than using the full data: this could be remedied by stopping to re-evaluate (20) after some iterations."
we denote attenuated system matrices by a tilde (see table 1 ;ã for components) and refer to (3) as the linearization of the scatter measurement equation.
"in this algorithmically oriented proof of concept, studies are carried out using 2-d digital phantoms and simulations restricted to single scattering without tof information. furthermore, we assume perfect energy resolution that enables ideal separation of scattered and nonscattered events and noise-free data."
"in previous work, we introduced the two-branch back-projection (2bp) algorithm [cit] which chooses between a positive and a negative update of ρ in a binary random-walk fashion. since we found this algorithm to be impractical for most applications [cit], we focus on two gradient-ascent-based algorithms here. the poisson log-likelihood (ll) of some expected data ȳ, given the data y and omitting terms that do not depend on ȳ, reads:"
fov size variations both algorithms are applied to the data simulated at all three spatial scales (human: scale 1; rabbit: scale 0.35; rat: scale 0.2).
"in this paper, we have used mlga as a s2a building block in the context of joint estimation. it might be possible to find improved algorithms: for example, one might pursue one of the many paths that lead to the mlem update equation for an algorithm which features more of the well-known properties of mlem. this may include the minorize/maximize (mm) algorithm [cit], of which regular mlem is one special case. following earlier incomplete-data formulations [cit], one might define complete data that involve not only the emission location, but also the scattering location of every coincidence; this may result in a formulation similar to that for joint estimation from nonscattered data [cit] . algorithms that use the formulation of the hessian of the log-likelihood may also be of value without requiring inversion of the full hessian during image reconstruction, as has been shown recently [cit] ."
"in addition to this mlem-like step size, two additional step sizes have been tested: the constant step size, proposed before [cit], and the scaled nonuniform step size:"
"the above expressions are equivalent to (41) and (42), respectively. their derivation is subject to the following assumptions: 1 sors are single-scatter analogs of lors, which comprise potential emission locations of nonscattered coincidences. 2 while energy bin may be a more appropriate term for list-mode data with energy information, and energy window more appropriate for list-mode data without energy information or histogrammed data, we use these terms interchangeably as we are only concerned with the maximum granularity of the energy information that can be obtained from the data. effective intersection length of photon path along the broken lor (i, s) with the transmitting (or attenuating) voxel t, taking photon energy into account"
"we use five building blocks: the aforementioned mlga with mlem-like step size for s2a reconstruction, henceforth referred to as scatter-mlga; scatter-mlem-osl as an alternative; scatter-mlem for scatter-to-activity reconstruction; trues-mlem for trues-to-activity reconstruction; and trues-mltr for trues-to-attenuation reconstruction. combinations of n individual algorithms form n-algorithms."
"another type of available data is low-energy, object-scattered pet emission data, which may contain enough additional information to address both aforementioned limitations [cit] : particularly, in a joint reconstruction scheme [cit] . similar opportunities arise in single-photon emission computed tomography [cit] . unfortunately, the model of the measured pet scatter data is neither based on the regular radon transform nor linear in μ. a maximum-likelihood gradient ascent algorithm for scatter-to-attenuation (s2a) reconstruction has therefore been proposed [cit] but, so far, not been used in joint estimation. most recently, a broyden-fletcher-goldfarb-shanno (bfgs)-based algorithm has been proposed for attenuation reconstruction from coincidences in a lower energy window [cit] ."
"thus, when taking these attenuating effects into account, e.g., in an iterative update, it would be more appropriate to speak of a volume rather than a surface of response; we still hold on to the term sor here."
"with δ sj the kronecker delta. based on the sign of this expression, we can identify distinct regions of the football-shaped sor in fig. 1. figure 15a shows half a cross-sectional plane through the football, 8 determined by d s, s, and d n . on the inside and outside of the football, the sign is determined mainly by the quantities a i j and k i s,j, which represent the scattering contributions of a voxel j along an sor i and the attenuating contributions of a voxel j along broken lors (i, s), respectively."
"regarding the impact of the results outside of pet imaging, we have achieved a definition of data being more or less compatible with a linearization of the measurement equation that may be applied in external compton scatter imaging, in which a number of ways have been tried to solve a structurally similar measurement equation [cit] -compare in particular (2) to ( [cit], eq. 3) or ( [cit], eq. 1). furthermore, full knowledge of the \"source\" distribution is given in ct and other transmission imaging modalities, where scattered radiation could be similarly exploited if discerned by energy measurements, such as in multi-energy (spectral) ct [cit] ."
"in this section, we verify the theoretical findings using nmses of reconstructed ρ-maps, of which we present some examples in fig. 5 . figure 6a shows the nmses of ρ for the human-sized phantom and system, for both low and higher resolution. all variants of mlga converge to the correct solution as all nmse curves tend to zero, while mlem-osl does not; mlga with the mlem-like step size is the fastest algorithm in both cases. figure 6b and c show the data for the same phantom and system at rabbit and rat sizes, respectively. in these cases, all algorithms converge to the correct solution; generally, mlga with the mlem-like step size is among the fastest."
"where z is the expected nonscattered data, u is the lor system matrix without attenuation, i.e., the usual system matrix applied for the usual pet reconstruction (see appendix), andũ ρ the attenuated one; u l j andũ l j represent entries of u andũ ρ, respectively, for lor l and voxel j."
"the problem of estimating attenuation from scattered pet photons shares similarities with compton scatter imaging, in which external gamma sources are used to probe an object's electron density for medical [cit] or industrial [cit] applications. while it is known from the latter that the nonlinearity of the problem favors thin, low-density objects, the impact of object size in scatter-based pet attenuation correction remains to be studied."
"this study has several limitations in the simplicity of the simulations: in particular, in neglecting detector scatter, multiple scatter, and energy-measurement uncertainties; in using the same forward model for the simulation as for the reconstruction; and in using low-dimensional objects and scanners."
"for the simpler case of activity reconstruction [cit], we consider ρ as a parameter for attenuation correction. then, changing the order of summation in (38) :"
"in reality, the use of scattered photons is complicated by the fact that the detected signal of a nonscattered 511 kev photon, when it deposits only part of its energy in the detector, resembles that of a lower-energy, object-scattered photon [cit] . one solution may be the use of an object-scatter energy window above the compton edge (at 341 kev) and below the photopeak, which is virtually free of detector-scattered photons ( [cit], fig. 1) . this highest-possible energy window also has a lower contribution of multiply scattered photons [cit] ."
"for low-attenuation data, the simple 2-algorithm may be sufficient: in this case, mlem-osl may serve as a drop-in replacement for mlga, however with decreased convergence speed (compare figs. 10 and 11 ). this result is compatible with the results for s2a reconstruction ( fig. 6b and c) . the more sophisticated 4-algorithm enables joint reconstruction with high-attenuation as well as low-attenuation data. in addition, this 4-algorithm can employ the scatter information to escape from a nonoptimal fix point of mlaa (fig. 14d) ."
"up to this point, we have focused on dedicated s2a reconstruction algorithms, assuming knowledge of the activity distribution; in this section, we drop this assumption and extend our studies to joint estimation of activity and attenuation using scattered as well as nonscattered data. the added value of combining scattered and nonscattered data is visualized in the appendix (fig. 16 )."
"for quantitative reconstruction of the activity image, attenuation correction (ac) is essential, compensating for a lack of detected photon pairs along lines of response (lors) due to the photoelectric effect and compton scattering in the patient. a complementary step, scatter correction (sc), computes an estimate of extraneous photon pairs along broken lors, which are generated through compton scattering. both corrections usually input the spatial distribution of the electron density ρ in the form of a map of linear attenuation coefficients μ or, for ac purposes, the so-called attenuation sinogram rμ. given μ, both ac and sc are state of the art using well-validated algorithms [cit], but vast research efforts had to be-and still are-directed to the determination of μ."
"pubchem annotates substance records with mesh terms for entrez searching purposes similar to compound records. however, no attempt is made to filter chemical names or mesh concepts for substance records, using chemical name information directly provided by individual data contributors for a given substance. in addition, the mesh annotations of substances are not displayed on the substance record page, to avoid confusion about the provenance of displayed information. mesh annotations to compound records made by pubchem are used to annotate assay records testing the compounds. they are also used to generate entrez indices that allow one to search the pubchem bioassay database for assays that have tested compounds with a particular mesh annotation. for example, the \"mesh term tested\" index retrieves assay records in which any tested compound is annotated with the query mesh term. the \"mesh term active\" retrieves assays in which only active compounds are annotated with the query mesh term."
"the venn diagrams shown in fig. 7 illustrate the overlap between cid-pmid associations provided by depositors and derived annotations via mesh. among the 39.2 million cid-pmid associations, only 4 % overlap, being both depositor-provided and nlmcurated, indicating that the two types of associations are nearly orthogonal and complement each other. of these 4 % overlapping cid-pmid pairs, the cids and pmids that make them up correspond to 10 and 19 % of all cids and all pmids found in all cid-pmid pairs, respectively. importantly, although depositor-provided cid-pmid links are much fewer in number than those generated via mesh, there are more cids involved in the depositor-provided links than in those derived via mesh."
"while the \"pccompound_mesh\" filter retrieves any compounds with mesh annotations, the \"pccompound_mesh_ pharm\" filter selects those with pharmacological action annotations. these filters are equivalent to the \"has_mesh\" and \"has_pharm\" filters, respectively. the \"pccompound_ pmc\" filter allows one to choose compounds that have associated full-text articles in pubmed central."
"lévy flights: for each individual i in subpopulation1, it will fly around the best one in the form of lévy flights. the resulting new solution is calculated based on equations (5)-(7)."
"the mesh annotations of pubchem records are \"indexed\" as indices for text search within ncbi's entrez search system [cit] . entrez is the text/numeric search and retrieval system that integrates pubchem's three primary databases (bioassay, compound, and substance) with approximately forty other ncbi databases, including pubmed and mesh. the entrez indices are tied to individual records in pubchem and include information on particular aspects (often referred to as fields) of the records. the available fields and their indexed terms in pubchem can be explored on the advanced search page as part of the search builder (see fig. 2 )."
"retrieve compounds annotated with the mesh terms whose description contains the query string pharmaction retrieve compounds annotated with the pharmacological action term, which are a subset of mesh terms pharmactionid retrieve compounds annotated with the pharmacological action term corresponding to the numeric identifier given as a query"
"considering the results shown in tables 8 and 9, a conclusion can be drawn that the performance of mso4 is superior to or at least quite competitive with the five other methods. to investigate the difference between the results obtained by mso4 and those by the comparison algorithm from the perspective of statistics, wilcoxon's rank sum tests with the 5% significance level were performed. the results of rank sum tests are recorded in table 9 . in table 9, \"1\" and \"−1\" indicate that mso4 is superior or inferior to the corresponding comparison algorithm, respectively, while \"0\" shows that there is no statistical difference at 5% significance level between the two comparison algorithms. the statistical result is shown in table 9 ."
"in this paper, twelve principal transfer functions are used and then twelve new discrete ms algorithms are proposed to solve sukp. these functions include four s-shaped transfer functions [cit], named s1, s2, s3, and s4, respectively; four v-shaped transfer functions [cit], named v1, v2, v3, and v4, respectively; and four other shapes transfer functions (angle modulation method [cit], nearest integer method [cit], normalization method [cit], and rectified linear unit method [cit] ), named o1, o2, o3, and o4, respectively. therefore, combining twelve transfer functions with ms algorithm, twelve discrete ms algorithms are naturally proposed, named as mss1, mss2, mss3, mss4, msv1, msv2, msv3, msv4, mso1, mso2, mso3, and mso4, respectively."
"the compound summary page of a compound provides an aggregated view of all available information on that compound collected from various data sources (fig. 4) . when a compound has literature information, this will be shown in the literature section. a link to pubmed articles associated with the compound via depositor-provided cross-references is displayed under the depositor provided pubmed citations section, and a link to those mesh tree node retrieve compounds annotated with the mesh term that match the query and those annotated with any mesh terms beneath the node corresponding to that mesh term. for example, \"penicillins[meshtreenode]\" will retrieve records annotated with mesh term \"penicillins\"and those with mesh terms \"oxacillin\", \"cloxacillin\", and so on, which correspond to child nodes beneath the \"penicillins\" node in the mesh tree"
"once pubchem chemical names are matched to mesh concepts, the mesh concept is associated with the pubchem compound record containing matching chemical names. additional consistency filtering suppresses multiple mesh concepts from being associated with the same pubchem compound record (by taking the consensus mesh concept for a given compound record). this helps to reduce errors but also helps to provide a consistent chemical representation for a given mesh chemical concept."
"overall, it is evident that mso4 has the best results (considering rpd values and average ranking values) on fifteen sukp instances. therefore, it appears that the proposed other-shapes family of transfer functions, particularly the o4 function, has many advantages combined with other algorithms to solve binary optimization problems. additionally, the o3 function and s2 function are also suitable functions that can be considered for selection. in brief, these results demonstrate that the transfer function plays a very important role in solving sukp using discrete ms algorithm. thus, by carefully selecting the appropriate transfer function, the performance of discrete ms algorithm can be improved obviously."
"some journals directly provide pubchem with literature information for chemicals. examples include journals published by the nature publishing group (including nature chemical biology, nature chemistry, and nature communications). when a new article is published in one of these journals, the publisher identifies chemicals studied in the article and submits them to pubchem with information on the source article (including the journal name, authors, title, publication date, and digital object identifier (doi)). these chemicals are stored in the substance database, and the substance record page for each chemical has a link to the source article (see fig. 1 ). similarly, on the web page of the original article the publisher provides links to the corresponding substance records in pubchem. this allows readers to readily access comprehensive information available in pubchem for these chemicals, through the compound records associated with the substance records. this demonstrates the benefit of concurrent publication of scientific articles in journals and associated data in public databases."
"to further evaluate the comprehensive performance of twelve discrete ms algorithms in solving fifteen sukp instances, the average ranking based on the best values are displayed in table 6 and figure 3, respectively. in table 6 and figure 3, the average ranking value of mso4 is 1.60 and it still ranks first. in addition, mso3 and mss2 are the second and the third best algorithms, respectively, which is very consistent with the previous analysis. the ranking of twelve discrete ms algorithms based on the best values are as follows: by looking closely at figures 2 and 3, it is not difficult to see that v3, v4, and o2 exhibit the worst performance, which is consistent in the two figures. similar to the previous analysis in figure 2, o1, o3, and o4 show satisfactory performance among 12 transfer functions. thus, it can be inferred that pr value can be used as a criterion for selecting transfer functions."
"to further verify the performance of discrete ms algorithm, we chose mso4 algorithm to compare with five other optimization algorithms. these comparison algorithms include pso [cit], de [cit], global harmony search (ghs) [cit], firefly algorithm (fa) [cit], and monarch butterfly optimization (mbo) [cit] . in de, the de/rand/1/bin scheme was adopted. pso, fa, and mbo are classical or novel swarm intelligence algorithms that simulate the social behavior of birds, firefly, and monarch butterfly, respectively. de is derived from evolutionary theory in nature and has been proved to be one of the most promising stochastic real-value optimization algorithms. ghs is an efficient variant of hs, which imitates the music improvisation process. it is also noteworthy that all five comparison algorithms adopt the discretization method introduced in this paper and combine with o4, respectively. the parameter setting for each algorithm are shown in table 7 . the best results and average results obtained by six methods over 100 independent runs as well the average time cost of each computation (unit: second, represented as \"time\") are summarized in table 8 . the frequency (t best and t mean ) and average ranking (r best and r mean ) of each algorithm with the best performance based on the best values and average values are also recorded in table 8 . the average time cost of each computation for solving fifteen sukp instances is illustrated in figure 19 . in table 8, on best, mso4 outperforms other methods on eight of fifteen instances (f01, f03, f05, f07, s01, s03, s05, and t03). mbo is the second most effective. in terms of average ranking, there is little difference between the performance of mso4 and mbo. in terms of the average time cost, it can be observed in figure 19 that de has the slowest computing speed. however, ghs has surprisingly fast solving speed. in addition, mso4 is second among the six algorithms. overall, the computing speed of pso, fa, mbo and mso4 shows little difference. to investigate the difference between the results obtained by mso4 and those by the comparison algorithm from the perspective of statistics, wilcoxon's rank sum tests with the 5% significance level were performed. the results of rank sum tests are recorded in table 9 . in table 9, \"1\" and \"−1\" indicate that mso4 is superior or inferior to the corresponding comparison algorithm, respectively, while \"0\" shows that there is no statistical difference at 5% significance level between the two comparison algorithms. the statistical result is shown in table 9 ."
"however, a caveat against depositor-provided associations is that the accuracy and usefulness of these links solely rely on individual depositors' quality control efforts, as pubchem does not (currently) provide an independent quality control mechanism. to address this, pubchem is exploring a few possibilities. for example, one option is to process pubmed abstracts (and pubmed central full-text papers) using text-mining software for chemical recognition (in text, tables, and figures). the resulting cid-pmid associations can be used to crossvalidate the existing depositor-provided associations and to better understand the context of the chemical mention. annotation of cid-pmid associations (e.g., location, context) may help prioritize the display of pmids associated to a given compound."
"some mesh annotations (such as solvents, carcinogens, inhibitors, and so on) are too general to describe a specific biological function of a compound. therefore, \"pharmacological actions\" (a subset of mesh terms) are separately included as an entrez index, as these annotations indicate specific biological roles of chemical concepts. the \"pharmacological action id\" index searches for records with the pharmacological action annotation corresponding to a numeric identifier assigned by mesh given as a query."
"the remainder of the paper is organized as follows. in section 2, we briefly introduce the sukp problem and ms algorithm. the families of transfer functions and repair optimization mechanism are presented in section 3. in section 4, the twelve discrete ms algorithms are compared to shed light on how the transfer functions affect the performance of the algorithm. after that, the best algorithm (mso4) is compared with five state-of-the-art methods on fifteen sukp instances. finally, we draw conclusions and suggest some directions for future research."
"to evaluate the performance of each algorithm, the relative percentage deviation (rpd) was defined to represent the similarity between the best value obtained by each algorithm and the best solution 5. the rpd of each sukp instance is calculated as follows."
"some journals, such as nature chemical biology, provide pubchem with information on chemicals that appear in their newly published articles. this enables pubchem to direct users to the new articles on the journal web site, even before their abstracts become available in pubmed. in turn, the publisher can provide their readers with access to comprehensive information available in pubchem about the chemicals mentioned in a given article. this exemplifies the mutual benefit of concurrent publication of scientific articles in journals and associated data in public databases."
"in table 8, mso4 outperforms pso and de on all fifteen instances. in addition, mso4 performs better than ghs and fa on most of the instances except for s05 and f01, respectively. meanwhile, mso4 is superior to mbo on eleven instances except for f07, f09, s01, and s05. statistically, there is no difference between the performance of mso4 and that of mbo for these four instances."
"as we all know, the metaheuristic algorithm is usually discretized in two ways: direct discretization and indirect discretization. direct discretization is usually achieved by modifying the evolutionary operator of the original algorithm to solve a particular discrete problem. this method depends on the algorithm used and the problem solved. obviously, the disadvantages of direct discretization are lack of versatility and complicated operation. the latter is discretized by establishing a mapping relationship between continuous space and discrete space. concretely speaking, indirect discretization is usually achieved by an appropriate transfer function to convert real-valued variables into discrete variables. many discrete versions of swarm intelligence algorithms using transfer functions have been proposed to solve various optimization problems. discrete binary particle swarm optimization [cit], discrete firefly algorithm [cit], and binary harmony search algorithm [cit] are among the most typical algorithms. through analyzing the literature, many kinds of transfer functions can be used, such as sigmoid function [cit], tanh function [cit], etc. however, most existing metaheuristics only consider one transfer function. little research concentrates on the importance of transfer functions in solving discrete problems. in addition, a few studies [cit] investigate the efficiency of multiple transfer functions."
"literature information, both provided by depositors and derived via mesh, can be accessed from the docsum page of an entrez search result, or from the compound summary, substance record, or bioassay record page. users can also retrieve pubchem records associated with scientific articles, using appropriate entrez filters. these tools allow pubchem users to more readily explore information available in literature related to pubchem records."
mesh description tested retrieve assays in which any tested substance is annotated with the mesh terms whose descriptions have a query string pharm action active retrieve assays in which only an active substance has the pharmacological action annotation given as a query
"the associations between pubchem records and pubmed records, whether provided by data contributors or computationally derived via common mesh annotations, are available to pubchem users as entrez links. in general, entrez links are crosslinks between records in different entrez databases or within the same entrez database. the entrez links provide a way to discover relevant information in entrez databases based on a user's specific interests. for example, one can readily obtain a list of fig. 2 retrieving compound records annotated with mesh terms using the advanced search builder. clicking the \"advanced\" link under the \"compound\" tab on the pubchem homepage directs users to the pubchem compound advanced search builder. selecting the \"meshterm\" from the dropdown menu and providing a mesh term in the search box will retrieve compounds annotated with that mesh term scientific articles related to a particular molecule through an entrez link from the corresponding compound record in pubchem to associated articles in pubmed. as another example, users may access all data sets (aid) in the bioassay database associated with a research article in pubmed. entrez links from pubchem to pubmed records are available to users in several ways."
"the guide to pharmacology (http://www.guidetopharmacology.org) [cit], created under the auspices of the international union of basic and clinical pharmacology (iuphar) and the british pharmacological society (bps), provides in-depth integrated views of the pharmacology, genetics, functions, and pathophysiology of important drug targets, including g-protein-coupled receptors (gpcrs), ion channels, and nuclear hormone receptors (nhrs). in addition, it provides information on the interactions between these target proteins and their ligands. pubmed citations containing the bioactivity data for these ligands are also provided to pubchem."
"literature information in pubchem can be programmatically accessed through e-utilities [cit] or pug-rest [cit] . more detailed information on programmatic access to pubchem is given elsewhere [cit] . figure 6 shows the frequencies of pmids per cid and cids per pmid for the three types of cid-pmid associations: those provided to pubchem by pubchem depositors (pccompound_pubmed), those derived by pubchem through matching between chemical names and mesh terms (pccompound_pubmed_mesh), and those provided to pubmed by journal publishers (pccompound_pubmed_publisher). note that the frequencies for the pccompound_pubmed_publisher links are much lower than the other two link types because only a small number of publishers submit cid-pmid associations to pubmed. therefore, this section focuses the other two link types."
"literature information available in pubchem for substances, compounds and assays, as well as how this information can be accessed, was described. individual data contributors provide pubchem with cross-references between chemical substances and pubmed articles that contain information on that substance. from these sid-pmid cross-references, pubchem generates crossreferences between the corresponding compound and the pubmed articles (i.e., cid-pmid cross-references). data contributors can also supply a list of pmids for scientific articles that have information relevant to a given assay record. these articles may contain various kinds of information related to the assay, including experimental protocols, assay targets, diseases associated with the targets, and known ligands that bind the targets. of particular interest, some data contributors provide bioactivity data extracted from literature through manual curation or data mining and are an important source of bioactivity information in pubchem that complement hts data from the now-concluded nih molecular libraries program and other hts projects. in addition to communityprovided literature information, pubchem generates entrez links between pubchem records and pubmed articles that share the same mesh annotation. this automated process allows pubchem users to leverage the biomedical literature and its mesh indexing for search and analysis purposes."
"here, mso4 is specially selected to analyze the solution space for f01, s01, and t01 sukp instance. the distribution of fitness at generation 0 and generation 100 is presented in figures 10-12 . the distance between each individual and the best individual is given in figures 13-15 . in figures 10-12, we can see that, at generation 0, the fitness values are more dispersed and worse than that at generation 100. in figure 13, it can be observed that the hamming distance varies from 0 to 35 at generation 0 while the range is 0 to 12 at generation 100. moreover, the hamming distance can be divided into eight levels at generation 100, which demonstrates that all individuals tend to some superior individuals. however, this phenomenon is not evident in s01 and t01. to intuitively understand the similarity of the solutions, the spatial structure of the solutions at generation 100 is illustrated in figures 16-18 . in figure 16, the first node (denoting the first individual) has the maximum degree which also shows more individuals have approached the better individual. however, the value of degree is not much different in figures 17 and 18 . this result is consistent with the previous analysis."
"it is worth noting that the cid-pmid associations via mesh tend to be limited to chemicals that are sufficiently well known to be included in mesh. these chemicals correspond to only a small fraction of all chemicals studied in scientific articles contained in pubmed. therefore, associations generated via mesh may ignore specific chemicals (favoring instead a chemical class) unless they are already included in mesh or sufficiently noteworthy to be specifically added. with that said, depositorprovided cid-pmid associations do not suffer the same limitation as mesh and provide a greatly increased number of chemicals mentioned in the biomedical literature, as suggested in fig. 7 ."
"the substance database also has entrez filters that are similar to those for compound records listed in table 4 . for example, the \"pcsubstance_pubmed\" and \"pcsubstance_pubmed_publisher\" filters can be used to find substance records with cross-references to pubmed that are provided by pubchem depositors to pubchem and by publishers to pubmed, respectively."
"the pdbbind database (http://www.pdbbind-cn.org) [cit] collects experimentally measured binding affinity data (ic 50, k d, or k i ) for biomolecular complexes in the protein data bank (pdb) [cit] . the majority of the data in pdbbind is for complexes between proteins and small molecule ligands, although it also contains other types of complexes such as protein-protein, proteinnucleic acid, and nucleic acid-ligand complexes. all binding affinity data in pdbbind are manually curated from nearly 24,000 original references. pdbbind contributes to pubchem binding affinity data for ~10,000 protein-ligand complexes that involve ~3000 unique small molecule chemical structures, with related information including the pmid for the source article, pdb id and mmdb id [cit] for the protein-ligand structures, and the protein gi and name for the protein target."
"considering the results shown in tables 8 and 9, a conclusion can be drawn that the performance of mso4 is superior to or at least quite competitive with the five other methods."
"in this section, we describe the newly proposed discrete ms for sukp. the main purpose of extending ms algorithm to solve the novel sukp is to investigate the significant role of the transfer functions in terms of improving the quality of solutions and convergence rate. the basic ms algorithm was initially proposed for continuous optimization problems, while sukp belongs to a discrete optimization problem with constraints. therefore, the sukp problem must contain three key elements, namely, discretization method, solution representation, and constraint handling. the three key elements are described in detail subsequently."
"the ms starts its evolutionary process by first randomly generating n moth individuals. each moth individual represents a candidate solution to the corresponding problem with a specific fitness function. in ms, two operators are considered including lévy flights operator and straight flight operator. correspondingly, an individual update in subpopulation1 and subpopulation2 is generated by performing lévy flights operator and straight flight operator, respectively. i."
"there are several possible directions for further study. first, we will investigate some new transfer functions on other algorithms such as krill herd algorithm (kh) [cit], fruit fly optimization algorithm (foa) [cit], earthworm optimization algorithm (ewa) [cit], and cuckoo search (cs) [cit] . second, we will study other techniques to discrete continuous optimization algorithms such as k-means framework [cit] . third, we will apply these twelve transfer functions-based discrete ms algorithms to other related and more complicated binary optimization problems including multidimensional knapsack problem (mkp) [cit] and flow shop scheduling problem (fssp) [cit] . finally, we will incorporate other strategies, namely, information feedback [cit] and chaos theory [cit], into ms to improve the performance of the algorithm."
"in addition to these resources containing manually curated sources of bioactivity, there is a growing number of aid-pmid references deposited directly by the authors of scientific publications uploading their research data into the pubchem bioassay database. this includes researchers from rnai screening and is open to all researchers in the chemical biology, medicinal chemistry, and related fields. such submissions help to satisfy table 2 summary of cross-references from literature-extracted bioassay data to pubmed articles n aid, n sid and n cid are the number of pubchem assays, substances and compounds extracted from scientific articles, respectively; and n aid-pmid, n sid-pmid and n cid-pmid are the number of cross-references from pubchem assays, substances, and compounds to pubmed articles, respectively. n pmid is the number of unique pubmed articles from which the assay data are extracted a ref. [cit] . https://www.ebi.ac.uk/chembl/ b ref. [cit] . http://www.pdbbind-cn.org c ref. [cit] . http://www.guidetopharmacology.org d ref. [cit] . https://www.bindingdb.org e ref. [cit] open access and data sharing requirements from research funding agencies. note that different pubchem bioassay data sources can provide pubmed links in different ways. for example, chembl provides the cross-reference between an assay and the article from which bioactivity information for that assay were extracted, and therefore, literature evidence on bioactivity data for substances tested in that assay can be deduced from the aid-pmid cross-reference. pdbbind provides information on the source article of bioactivity data for each substance tested in an assay (as a property of the tested substance reported in the assay data table). glida does not explicitly provide literature information for assays or tested substances in them. instead, it provides the glida id (the database identifier used in glida) for a substance. this external id is used to search for the corresponding record in glida to get information on the source article."
"straight flights: for each individual i in subpopulation2, it will fly towards that source of light in line. the resulting new solution is formulated as equation (8)."
"where rand( ) is a random number in (0, 1). in figure 1, it can be observed that s-shaped transfer functions, v-shaped transfer functions, and o3 will return a random real number between 0 and 1. therefore, the comparison of rand( ) to t(x i ) equals 1 or 0. then, the mapping procedure is shown as table 2 . the second mapping method: choose the transfer function o2."
"in table 8, mso4 outperforms pso and de on all fifteen instances. in addition, mso4 performs better than ghs and fa on most of the instances except for s05 and f01, respectively. meanwhile, mso4 is superior to mbo on eleven instances except for f07, f09, s01, and s05. statistically, there is no difference between the performance of mso4 and that of mbo for these four instances."
"there are multiple sources of links to pubmed articles within pubchem and different contexts where they can be specified within a given pubchem record type. understanding where these links can be provided and who provides them can help aid in their use. for example, they can provide pubchem users quick access to the original source article of a bioactivity result. in some cases, data contributors provide chemical information extracted from the scientific literature through manual curation or data mining. in addition, pubchem performs an automated process that annotates pubchem records with mesh terms (by means of chemical name matching), creating associations between pubchem records and pubmed articles that share the same mesh annotation. this work summarizes the various sources of pubmed links in pubchem, explains ways to access these links, and examines the scope of literature information associated with pubchem records."
"the first option \"pubmed citations\" is for entrez links derived from cross-references to pubmed articles that data contributors provided to pubchem. that is, this option will retrieve scientific articles containing information relevant to a given chemical, according to pubchem depositors. on the other hand, the third option, \"pubmed (publisher)\", is from cid-pmid associations provided by journal publishers to pubmed (not pubchem) as a part of pubmed abstract submission. both the first and third options are similar in the sense that some organization (e.g., data contributors or journal publishers) contributed the chemical-literature associations. however, the second option \"pubmed (mesh keyword)\" corresponds to cid-pmid associations derived via mesh annotations. this option returns articles annotated by medline indexers with mesh terms (chemical names). mesh annotation can be further leveraged to find which articles have a given mesh term as a major topic to obtain articles specifically about that chemical."
"the disproportionate number of cross-references and chemical substance coverage coming from a handful of data contributors (table 1) points to their diverging focus areas in data collection. for example, ibm uses data mining to collect information on patents and scientific papers that mention a very broad range of chemicals, resulting in a large number of links to scientific papers. on the other hand, drugbank primarily focuses on u.s. food and drug administration (fda) drugs (approved and experimental) and collects relevant information through manual curation. similarly, the iedb resource performs manual curation but considers chemical substances in the immunology epitope research domain. as such, pmid cross-references provided by different chemical substance data depositors can have a significantly varied focus and approach."
"three different types of entrez links also exist for substance records to pubmed articles, similar to the compound-to-pubmed links. for the bioassay-to-pubmed links, only one type of entrez link to pubmed is available through the right column of the docsum page. this returns publications associated with the bioassay records, based on aid-pmid associations provided by data contributors to pubchem. these publications may be either source articles from which assay data are collected in curation projects, or general articles that provide background information relevant to the assay."
"where n represents the number of items. the value of longitudinal axis in figure 2 is the average of pr among 100 independent runs. as shown in figure 2, the four s-shaped transfer functions have similar pr values, which are close to 0.5. however, the pr values of the four v-shaped transfer functions differ considerably. v2 has the best pr value while the pr value of v3 is less than 0.2. it seems that v3 combining with ms should show poor performance. similarly, v4 also demonstrates unsatisfactory performance, with a pr value of less than 0.25. of the four other shapes of transfer functions, o1, o3, and o4 obtain a similar pr value, that is, close to 0.5. the pr value of o2 is slightly smaller than that of o1, o3, and as shown in figure 2, the four s-shaped transfer functions have similar pr values, which are close to 0.5. however, the pr values of the four v-shaped transfer functions differ considerably. v2 [cit], 7, 17 6 of 25 the best pr value while the pr value of v3 is less than 0.2. it seems that v3 combining with ms should show poor performance. similarly, v4 also demonstrates unsatisfactory performance, with a pr value of less than 0.25. of the four other shapes of transfer functions, o1, o3, and o4 obtain a similar pr value, that is, close to 0.5. the pr value of o2 is slightly smaller than that of o1, o3, and o4. in sum, according to the preliminary analysis of pr values, it seems that v3, v4, and o2 are not suitable for combining with ms to solve binary optimization problems."
"the majority of compounds have no more than ten associated pmids (i.e., 95 % of cids with depositorprovided pmids and 70 % of cids with automated annotations via mesh). however, some compounds are associated with many pmids. for example, d-glucose has as many as 130,545 depositor-provided pmids and 132,017 pmids generated via mesh."
"pubmed, whose primary identifier is the pubmed id (pmid), provides free access to more than 25 million scientific abstracts covering the fields of medicine, nursing, dentistry, veterinary medicine, health care systems, and preclinical sciences. nearly 90 % of the pubmed contents are from medline [cit], which is the nlm's bibliographic database containing more than 22 million abstracts of journal articles in life sciences with a concentration in biomedicine. a distinctive feature of medline is that the records are \"indexed\" with medical subject headings (mesh) [cit] . mesh is the nlm's controlled vocabulary thesaurus, consisting of sets of terms naming descriptors in a hierarchical structure. indexing of scientific papers with mesh terms enables users to perform a literature search at various levels of specificity. of keen interest to pubchem is that mesh includes a large number of chemical substance concepts, chemical names associated with each concept, and specific/qualified links between these concepts and pmids. considering pubchem contains many chemical names, mesh allows pubchem records to be linked to the biomedical literature using matching chemical names."
"in this section, we present experimental studies on the proposed discrete ms algorithms for solving sukp. algorithm 2. the main procedure of discrete ms algorithm for sukp."
"bindingdb (https://www.bindingdb.org) [cit] provides measured binding affinities, focusing chiefly on the interactions of proteins considered to be drug targets with small, drug-like molecules. bindingdb contains more than one million binding interactions for seven thousand protein targets and 495 thousand small molecules."
"an entrez document summary (docsum) page, as shown in fig. 3, displays multiple records returned from an entrez search. entrez links are available via the \"find related data\" menu on the right column. if no compounds are selected, these links will be applied to the entire search result list by default. note that, when \"pubmed\" is selected from the database drop-down menu, three items appear under the \"option\" menu: \"pubmed citations\", \"pubmed (mesh keyword)\", and \"pubmed (publisher)\". these options correspond to different types of compound-to-pubmed entrez links."
"retrieve assays in which any tested substance has the pharmacological action annotation given as a query associated with the compound via common mesh annotations is shown under the nlm curated pubmed citations section. in some cases, the per compound pmid associations derived via mesh annotations are categorized according to mesh subheadings, which allow for describing particular aspects of a subject. mesh terms associated with a compound are displayed under the classification section."
"the biggest source of sid-pmid cross-references is the ibm almaden research center (http://www.research.ibm. com/labs/almaden/index.shtml). this pubchem depositor provides pmids of the scientific articles mentioning a particular chemical. while beyond the scope of this paper, they also provide links to patent documents mentioning a particular chemical. the ibm substance records provide more than 90 % of the sid-pmid crossreferences involving about half of all substance records containing cross-references to pubmed articles."
"publishers are also an indirect source of records in pubchem. while many funding agencies mandate that data from studies they support should be freely available to the public, some journals also require data submission of research data to public databases as a precondition to publishing a research paper. data sharing requirements by journals and granting agencies may be satisfied by use of the pubchem data archiving platform."
"pubchem generates cid-pmid associations via mesh using depositor-provided chemical names. this is achieved by first determining which depositor-provided chemical names in pubchem are also found in mesh. not all chemical names in pubchem are used in this step. chemical names provided to pubchem are passed through a sort-of crowd-sourcing filtering process, where consistency of chemical name-structure associations of depositor-provided synonyms is determined using differing levels of chemical structure sameness (e.g., potentially grouping different charge states or salt forms of the same chemical). this reduces the count of pubchem compound records associated with a given chemical name. the resulting filtered chemical names are considered \"filtered synonyms\" in pubchem."
"the entrez indices derived from mesh annotations are summarized in table 3 . the \"complete mesh term\" entrez index retrieves compounds annotated with the mesh term that exactly matches the query, while the \"mesh term\" index searches for those with the mesh terms that partially match the query. the \"mesh tree node\" index finds compounds annotated with the query mesh term and those with any mesh terms that correspond to the child nodes beneath the node for the query term. the \"mesh description\" index searches the descriptions of mesh terms for the query string."
"computational complexity is the main criterion for evaluating the running time of an algorithm, which can be calculated according to its structure and implementation. in algorithm 2, it can be seen that the computing time in each iteration is mainly dependent on the number of moths, problem dimension, and sorting of items as well as moth individual in each iteration. in addition, the computational complexity is mainly determined by steps 1-4. in step 1, since the quicksort algorithm is used, the average and the worst computational costs are o(mlogm) and o(m 2 ), respectively. in"
"one may want to retrieve all pubchem records associated with pubmed articles. this can be done using entrez filters, which indicate whether or not a given record in an entrez database has a particular type of annotations. the entrez filters may be used to subset other entrez searches according to this annotation type, by adding the filter to the query string. importantly, entrez filters are closely fig. 3 the document summary (docsum) page that shows the results for a search for \"warfarin\". scientific articles associated with the returned compound records can be accessed via the entrez links, which are available under the \"find related data\" menu (for multiple records) or from a link for individual compound records related to entrez links in that many entrez filters are generated by checking whether a given record in an entrez database has an entrez link to a record in the same or different entrez database. the filters for each entrez database may be listed by going to the advanced search page of each database, selecting \"filter\" from the \"all fields\" dropdown, and clicking \"show index list\" (fig. 5) . table 4 lists the entrez filters available in pubchem that can be used to get the list of records associated with pubmed articles. as mentioned in the previous section, a docsum page for a compound search result (fig. 3) shows three different entrez links between compounds and pubmed articles. these three links are used to generate three entrez filters that indicate the presence of associated pubmed articles for compound records: (1) pccompound_pubmed, (2) pccompound_pubmed_ mesh, and (3) pccompound_pccompound_publisher. the \"pccompound_pubmed\" filter allows one to retrieve compound records with cross-references provided to pubchem by data contributors. the \"pccompound_pub-med_publisher\" filter retrieves those with cross-references to pubmed articles that are provided to pubmed by publishers. the \"pccompound_pubmed_mesh\" filter retrieves those with computationally generated links to pubmed articles that have a common mesh annotation, as explained in the previous section. fig. 4 the literature section of the compound summary page (docsum) for cid 5288826 (morphine). clicking the \"literature\" section in the table of contents allows users to jump to the literature section, which consists of two subsections: depositor-provided and nlm-curated pubmed citations"
"to intuitively understand the similarity of the solutions, the spatial structure of the solutions at generation 100 is illustrated in figures 16-18 . in figure 16, the first node (denoting the first individual) has the maximum degree which also shows more individuals have approached the better individual. however, the value of degree is not much different in figures 17 and 18 . this result is consistent with the previous analysis. to intuitively understand the similarity of the solutions, the spatial structure of the solutions at generation 100 is illustrated in figures 16-18 . in figure 16, the first node (denoting the first individual) has the maximum degree which also shows more individuals have approached the better individual. however, the value of degree is not much different in figures 17 and 18 . this result is consistent with the previous analysis. to intuitively understand the similarity of the solutions, the spatial structure of the solutions at generation 100 is illustrated in figures 16-18 . in figure 16, the first node (denoting the first individual) has the maximum degree which also shows more individuals have approached the better individual. however, the value of degree is not much different in figures 17 and 18 . this result is consistent with the previous analysis."
"this perceptual evaluation revealed that approximately 40% of the sequences can be better recognized after the refinement, while 12% become less comprehensive. for the others no significant difference can be heard before and after the refinement. the most common improvement is that the acoustic sequences estimated by the initial inverse model sometimes contain click noises, but sound more smooth and natural after the refinement. it can be observed that the reproduction of the initial model is very rough and noisy. such noisy outcomes can be especially observed in case of few initial training data. after model refinement, the articulatory trajectories are smoother and more accurate. although the estimation is still not perfect, this improvement leads from an incomprehensible utterance to a clear [a:gae:] and thus a successful acoustic imitation using the refined inverse model. not all sequences are improved during the refinement. especially those sequences used for initial training can be better approximated by the specialized initial models than by the refined models, which is an expectable result. 48% of the sequences do not show a perceivable improvement at all. the problem is two-fold: firstly, in the learning process errors are weighted equally in each articulatory parameter, whereas perceptual features in fact change in a highly non-linear manner with respect to articulatory parameter changes. this can be addressed by utilizing respective error metrics, which, however, are not easy to define. secondly, the unsupervised refinement presented in this paper does not actively explore its actuation space in order to achieve better imitation results."
"we especially draw inspiration from the smarth2o project, which aims at raising awareness of water consumption and at encouraging efficient water-use behaviours [cit], by means of a gamified platform providing consumption feedback with a visualization interface. users are provided with water saving tips and learning elements (videos), encouraged in competing for the highest water savings using a points-goals based competition and in getting social recognition through virtual prizes badges and leaderboards, and enabled to share their platform activities on popular social networks [cit] ."
"upc implementers were unable to utilize mpi-2 rma as a runtime system because of a semantic mismatch between mpi and upc that could not be overcome at the runtime level [cit] . instead, an active message runtime was built on top of mpi two-sided messaging [cit] . it is hoped that mpi-3 has addressed this gap and that it will be suitable as a low-level, portable runtime system for a variety of one-sided and global address space models. the mpi-2 rma interface has existed for over a decade, and significant effort has been invested in improving its performance [2, 12, 13, 16, 17, 21, 29, 31, 32, 35 ] and in building higher-level libraries using mpi rma [cit] . mpi rma has been demonstrated to be effective in a variety of applications, including earthquake modeling simulations [cit] and cosmological simulations [cit] ."
"h) digital game extension: besides in-app gamification, encompass supports energy saving awareness also via traditional gaming, implemented with a hybrid game, called funergy, mixing real and digital play. the digital part implements an energy trivia game, which complements the game play of the funergy real card game; funergy is designed with the purpose of increasing the awareness of users, especially kids, about the importance of energy efficiency labels in domestic appliances. the digital game is implemented as a web and mobile app, which pulls from a dedicated backend server the content and play statistics necessary to populate its user interface (e.g., trivia quizzes, difficulty levels, correct responses, points, players skill level and achievements, etc.); it targets both casual players and users of the encompass gamified platform and of the funergy card game; the latter can use the digital game as a side-kick to overcome critical situations in the real card game. i) energy efficiency assessment console: it is a web and mobile app whereby building managers (bms) and utilities can analyse aggregated consumption and sensor data of all the users and set saving goals/incentives. it allows bms to create customized dashboards to get different perspectives about energy consumption in one or more buildings, such as the current status of one building or user, the cumulative consumption over a period, or the comparison of multiple users and/or buildings, etc. the console retrieves consumption and sensor data stored in the platform database and displays them using multiple visualization widgets (e.g. plots, bars, pies, maps, donuts, bubbles, polar and spider charts, etc.), allowing the bm to select the ones that most fits his/her needs. widgets can be parameterized to display specific data for an arbitrarily chosen period. j) encompass apis: they provide on-demand, cloudbased web services that enable end-user applications to access and exploit the encompass processes. the data and service apis enable developers to access both encompass functionalities and data while implementing their own services."
"in the mpich rma design, a window object contains base pointers and displacement units for all processes in the window's group. when an rma operation is issued, the origin process calculates the effective address at the target process and transmits this in the packet header. this approach reduces the work that the target must do and in some cases, avoids a window object lookup at the target. however, the use of an o(p ) structure can limit scalability. in the future, we will investigate other approaches, such as transmitting the displacement instead of the effective address, in order to improve scalability."
"ch3 provides functionality for allocating and attaching to shared-memory regions. the sharedmemory allocation function takes the size as an input parameter and returns a pointer to the allocated region along with the a handle. the handle can be serialized into a character string that can then be communicated to other processes. the serialized handle can be sent by using ch3 messages or using an out-of-band communication mechanism such as the process management interface (pmi) [cit] . when the serialized handle is received by another process, it can be used to attach to the sharedmemory region allocated by the other process."
"information theoretic, or physical layer, security uses the physical properties of the wireless channel in order to establish a higher level of security. this security only depends on the channel; so whatever transformation is applied to the signals that are received by non-legitimate receivers, the original message cannot be reproduced with high probability."
"in the following, we show that the refinement process significantly enhances the inverse as well as the forward model with respect to imitation accuracy of the acoustic stimuli. note that the learned forward model f is optional in this approach. while we assume the presence of an initial model trained on supervised data, we show that a small initial training set is sufficient. generating initial models without requiring supervised data is subject of future work."
"an rma lock may be either exclusive or shared. performing accesses in an exclusive lock epoch ensures that no other lock/unlock epochs (of either type) will appear to occur concurrently. a shared lock allows multiple origin processes to access the window at the target concurrently. if both shared and exclusive epochs are requested, mpi ensures mutual exclusion of shared and exclusive epochs. the application is responsible for ensuring that accesses in a shared lock epoch do not conflict with accesses from concurrent shared lock epochs originated by other processes."
"in this work, we have presented the first complete implementation of the mpi-3 rma specification. while our implementation is feature complete, many opportunities for performance optimization and system integration still remain."
"initially, the tail pointer contains −1, indicating that the mutex is available. when processes request the mutex, they perform an atomic swap to exchange their rank for the value in the tail pointer. if the swap operation returns −1 the process has obtained the mutex; otherwise it is the new tail of the list and it updates the next pointer of the previous tail process's list element. in order to ensure data consistency, list elements are updated by using the fetch-and-op operation, where the mpi replace operation is used to write to the shared location and the mpi no op operation is used to read it. once they have updated their ancestor's next pointer, enqueued processes wait in an mpi recv operation for the previous process to forward the mutex."
"in the past years, the industry has been constantly and successfully working on energy efficiency, by producing devices and appliances that sensibly reduce their energy consumption; yet it can be observed the so called \"rebound effect\" [cit], as exemplified by the case of lcd tvs: the consumption per square inch has been reduced in comparison with a traditional cathode-ray tube tv, but the size of the average tv has greatly increased, thus reducing the potential energy savings."
"to evaluate the forward model, we take the estimated articulatory sequence x robot as a basis and compare the outcome of the learner's internal forward model y robot to the acoustics generated by the true forward model y"
"the new mpi fetch and op and mpi compare and swap operations restrict the datatypes that can be used to ensure higher performance. for these operations, the rma communication protocol is simplified significantly. because fewer communication parameters are needed, surplus space is available in the packet header. we use this space to embed the origin data and avoid additional steps required to transmit the payload data."
"mpi-2 rma defined a conservative but extremely portable system for one-sided communication. the mpi-2 memory model, termed the \"separate\" model in mpi-3 rma, provided an efficient and portable interface on systems such as the earth simulator, then the world's fastest machine. however, the separate model did not exploit hardware that provided stronger guarantees about memory coherence (the mpi-3 \"unified\" model) or remote atomic operations; in addition, the completion models limited some of the common uses of one-sided programming. the enhancements provided by mpi-3 rma have addressed these and other limitations and have resulted in a powerful, well-defined rma model that fits within the mpi environment and can perform efficiently on current and future systems."
"mpi-3 rma is a proper extension to mpi-2 rma; as such it is backward compatible. in this section, we describe the aspects of the mpi-3 rma specification that are inherited from the mpi-2 rma specification. at its core, the mpi-2 rma interface is composed of a set of communication operations and two data access synchronization schemes: \"active target\" and \"passive target\" synchronization."
"for data generation and evaluation of the estimated articulatory sequences, the speech synthesis system vocaltractlab developed by birkholz [cit] was used to generate acoustic signals from articulatory parameter trajectories. the articulatory parameters describe the positions of important articulators and the vocal tract shape (e.g. the tongue tip position, lip distance and jaw opening angle) as a function of time. the sequences were created manually using the phone definitions and the gestural scores representation provided by vocaltractlab 1.0. 22 out of the 25 tract parameters and 4 glottis parameters were used for the articulatory representation. each utterance is 500 ms long. note that the data comprise four voiced and four voiceless consonants. three of the voiced consonants are plosives and used together with their voiceless counterparts. the other two consonants are fricatives. for each of the 64 sequences, 50 noisy samples were generated by varying the consonant and vowel durations, the articulatory effort, and by adding noise to the lung pressure parameter and the fundamental frequency."
"the put operation transmits data from the origin to the target. the get operation transmits data from the target to the origin. the accumulate operation transmits data from the origin to the target, then applies a predefined mpi reduction operator to reduce that data into the specified buffer at the target. the target and origin datatypes used in the accumulate operation may be derived datatypes, but they must be composed of only one distinct basic element type."
"for applications where synchronization should be avoided or the communication pattern is difficult to predict, mpi offers an alternative to active target synchronization, known as passive target synchronization. processes perform communication operations in access epochs demarcated by mpi win lock and mpi win unlock calls. despite the names, these routines do not provide a traditional lock or mutex. instead they serve two primary purposes: (1) to group one-sided communication operations and certain load/store accesses that target a particular process, and (2) to ensure completion (i.e. visibility) of specific accesses relative to other accesses."
"new synchronization mechanisms were included in the mpi-3 passive target synchronization mode. mpi-2 passive target mode was restricted to a simple lock/unlock interface, where an origin process could lock only one target at a time. mpi-3 permits locking multiple targets simultaneously. it also offers a new mpi win lock all routine that is equivalent to locking each target in window with a shared lock."
"the dynamic linked list construction benchmark can be expressed by using three different synchronization idioms that utilize varying degrees of mpi-3 synchronization. pseudocode for each idiom's linked-list traverse-and-append loop is shown in figure 9 . the exclusive lock/unlock implementation uses only mpi-2 communication and synchronization operations (but uses mpi-3 dynamic windows). the shared lock/unlock implementation uses mpi-3 accumulate operations to enable the use of mpi-2 shared-mode locks. the lock-all implementation adds the use of flush to complete communication and avoid repeated calls to lock. figure 10 we compare the performance of the mpi-2 and mpi-3 implementations of the linked-list construction benchmark. a key factor in the performance of this benchmark is the ability to deal with high amounts of reader-writer contention as each element is appended to the list. if we assume first-come, first-served processing of rma operations at target processes, each writer that appends an element to the list must wait for an average of p/2 readers to complete polling read operations before the write succeeds. for a list of n elements and a communication latency of l, the expected execution time is o(n · p/2 · l). under high amounts of contention, the latency of read and write operations l increases proportional to the number of processes, p . thus, the expected execution time is o(n · p 2 ). from this figure, we see that the mpi-2 exclusive lock communication mode provides the least tolerance in the presence of contention. in comparison, the mpi-3 implementations use the new accumulate operations that enable both to use a shared lock, providing greater concurrency and less overhead. at 256 processes, the mpi-3 implementation provides an more than an order of magnitude improvement in performance."
the process of refinement is depicted in fig. 2 using the example of a robot interacting with its environment. the novel acoustic utterances are mapped to the articulatory space using the current inverse model. this mapping corresponds to an estimation process of how the perceived acoustics could be imitated. the estimated articulatory sequence is then fed into the true forward model given by the articulatory synthesizer. the generated acoustic feature trajectory y robot v t l together with the estimated articulatory trajectory x robot then represents a new training pair which is used to update the internal forward and inverse model.
"mpi-3 also introduced request-generating operations, which return a request to the user that can be used to wait for completion of a specific rma operation. in the mpich implementation, we use the mpich extended generalized request framework to support these operations. we enqueue request-generating operations in the corresponding rma operations queue and return a request handle that contains a reference to the window. when the user completes the request, we perform a local flush of the window to the target process. we plan to improve this design by enabling completion of only the operation corresponding to the request."
"comparing the mpi-3 shared lock and lock-all implementations, we see the additional protocol overhead reduction that is provided by the mpi-3 lock-all mode of operation. we measure this gap directly in figure 11, which shows that lock-all provides a significant latency reduction by eliminating the communication involved in the lock operation. the corresponding protocol for each operation is shown in figure 12 . for the experiments in this section, we have disabled the lock-opunlock merging optimization to provide a fair comparison, because it is not yet implemented for get-accumulate or flush operations."
"when a window synchronization is performed, the mpi implementation must synchronize the public and private window copies. thus, mpi-2 forbids concurrent overlapping operations when any of the operations writes to the window data; the only exception is that multiple accumulate operations can perform concurrent overlapping updates when the same operation is used. because rma communication operations are nonblocking, the programmer must ensure that operations performed within the same synchronization epoch do not perform conflicting accesses. in addition, because the mpi library is unaware of which locations are updated when the window buffer is directly accessed by the hosting process, local updates cannot be performed concurrently with any other operations. any violation of these semantics is defined to be an mpi error."
"the simplest form of active target synchronization uses the concept of a \"fence.\" all processes in the window collectively call mpi win fence in order to demarcate the beginning and end of rma epochs. during these epochs the application may issue zero or more mpi communication operations or in some cases may perform direct load/store operations to that process's portion of the window. rma operations issued before the fence call began will be completed before the call returns."
in this section a set of articulatory-acoustic sequences is introduced and a cross-validation test is conducted which verifies that mappings between the articulatory and acoustic representations of these sequences can be learned with excellent generalization errors by an efficient recurrent neural network model.
"we have extended the mpich rma implementation with support for new mpi-3 rma functionality. our implementation is integrated in ch3 and can be used with a variety of networks that are supported as ch3 channels and nemesis network modules, as described in section 3. the mpi-2 rma design focused on a messaging-based design, and we have developed mpi-3 rma within this design space; in the near future, we plan to extend this design to leverage one-sided network capabilities."
"dynamically allocated windows enable a powerful new usage pattern where memory can be asynchronously attached to and detached from the window by the origin process. the address of the memory is used directly as the window displacement argument to rma operations, obviating the need for o(p ) structures. in the mpich design, we expose all local memory in every dynamic window, effectively making attach and detach operations no-ops. one could validate that operations target only attached memory; however, this check has a significant performance penalty and is not required by the mpi standard."
"in this work we studied the efficient integration of confidential services in bidirectional relay networks at the physical layer with strong secrecy. this required the analysis of the bbc with confidential messages for which we derived the strong secrecy capacity region. interestingly, it is shown that the strong secrecy capacity region coincides with the corresponding weak secrecy capacity region. thus, a requirement of strong security for confidential services in bidirectional relay networks does not lead to a loss in the transmission rates compared to weaker security requirements."
"after the initial forward and inverse models have been trained, we improve the models by conducting a number of refinement iterations given by max iterations (cf. algorithm 1). in each iteration, 64 sequences are randomly chosen (one from each sequence class) and presented to the network. 8 of these sequences are known to the network from the initial model training. the other 56 sequences are new to the learner, as the vowels and consonants appear in novel contexts."
support for the unified model is not required by the mpi-3 standard. users must query the implementation via the predefined mpi win model attribute in order to ensure that a particular window supports the unified model before taking advantage of its relaxed consistency semantics.
"in the active target mode in mpi-2, data is transmitted from the memory of one process to the memory of another, with direct participation from both processes."
"many applications generate and operate on data that mutates in size or layout during execution. to accommodate these types of algorithms, mpi-3 has added a dynamic window that allows processes to asynchronously add and remove memory to and from their window. in contrast, under mpi-2, windows are immutable and are created collectively. thus, resizing a window required the programmer to collectively create a new window, copy data, and destroy the old window. figure 13 shows the time required to double the size of an rma window using the mpi-2 approach and using mpi-3 dynamic windows. the figure shows traces for windows with an initial size ranging from 1kb to 16mb. for each window size, we vary the number of processes. creating a window requires several collective operations, including an all-gather operation, whose overhead increases with the number of processes. in contrast, attaching memory to a dynamic window is a local operation with a fixed cost, regardless of the buffer sizes and the number of processes. 6. related work a variety of low-level one-sided communication systems have been created, including shmem [cit], armci [cit], and gasnet [cit] . these systems have been used independently and as runtime systems to support higher-level global address space models, such as global arrays [cit], upc [cit], coarray fortran v1.0 [cit], coarray fortran v2.0 [cit], chapel [cit], and x10 [cit] ."
"we conducted our experiments on the eureka cluster at the argonne leadership computing facility. this cluster is configured with 100 nodes, each with two quad-core intel xeon processors and 32 gb of memory. nodes are connected by using myricom 10 gb/s cx4 myrinet network interfaces, configured with a 5-stage clos topology. mpich was configured to use the myrinet mx network module."
"following an initial idea already developed by smarth2o, to further encourage user awareness and energy saving actions, encompass does not limit to a digital gamified platform which offers virtual rewards, but it is backed up by a tangible, real-life matching energy-related card game, named funergy. the board game is offered to encompass users as a reward in return for their energy efficient behaviour or for high levels of activity with the encompass platform. it is designed to be played as a common card game in school classrooms or family contexts but users get the most by exploiting its digital extension which connects it to the activities performed on the digital encompass platform. using funergy therefore generates additional rewards, in a reinforcing virtuous circle."
"by bringing these elements together, encompass aims at influencing users to adopt long-lasting energy efficient behaviours, in order to produce higher energy consumption reduction rates with respect to related projects."
"because of these restrictions, fetch-and-op offers numerous optimization opportunities to the mpi implementation, potentially reducing software overhead latencies and permitting direct use of hardware-supported atomic operations."
"all three of these new operations are safe to use concurrently with each other and the mpi-2 accumulate operation. atomicity for accumulate and get-accumulate operations with derived datatypes or a count greater than one with predefined datatype occurs elementwise, at the granularity of basic, predefined mpi datatypes. the atomic and accumulate operations are not atomic with respect to put and get operations. instead, accumulate with the mpi replace operation may be used as an atomic \"put,\" and get-accumulate with the mpi no op operation may be used as an atomic \"get.\" 2.2.2. request-generating operations all mpi rma communication operations are nonblocking, but mpi-2 operations do not return a request handle. instead, completion is managed through synchronization operations such as fence, pscw, and lock/unlock. mpi-3 adds \"r\" versions of most of the communication operations that return request handles, such as mpi rput and mpi raccumulate. in turn, these requests can be passed to the usual mpi request completion routines, such as mpi wait, to ensure local completion of the operation. this provides an alternative mechanism for controlling operation completion with fine granularity. however, these requestgenerating operations may be used only in passive-target synchronization epochs (i.e., with lock/unlock)."
"come from the constraint (7) together with the communication constraints from the following result. it treats achievable rates for the bidirectional broadcast channel with messages m 1 and m 2 as above and another individual message intended for node 1. similarly as the confidential message in theorem 1 this messages originates from the relay node, but it does not have to be kept secret from node 2."
"each of these communication operations must occur in the context of either an active target synchronization epoch or a passive target synchronization epoch. in mpi rma, all communication operations are nonblocking and are completed at the end of the synchronization epoch."
"mpi-3 rma defines the unified memory model, which relaxes window access semantics for systems where hardware can provide the needed level of data consistency. a key difference between the unified and separate memory models is that the unified model permits rma operations concurrent with nonoverlapping load/store operations."
"in contrast with the mpi-2 algorithm, the mcs algorithm in mpi-3 uses a distributed queue, which allocates space for one integer value per process. in addition, it utilizes new atomic compareand-swap, update, and read operations that enable processes to use the shared lock access mode. figure 5 shows the average time to acquire and release the mutex in the mpi-2 and mpi-3 mutex libraries. a single mutex is created, and all processes perform 10,000 lock and unlock operations. while both mutexes use a queueing algorithm that does not poll over the network, the mpi-3 mcs algorithm uses a shared lock that enables a greater amount of concurrency and better tolerance of contention. for smaller process counts, there is roughly an order of magnitude difference in lock-unlock latency, which grows to nearly two orders of magnitude with 256 processes."
"when a shared-memory window is created, the processes perform an all-gather operation to collect the sizes of the individual window segments of each process. the total size of the window is computed, and the root process (the process with rank 0 in the communicator associated with the window) allocates a shared memory segment of that size as described in section 3.3. the root then broadcasts the serialized window handle, and the other processes attach to the shared-memory region. because each process knows the size of every other process's window segment, each process can compute the offset into the shared-memory region where any process's window segment begins. the pointer to the beginning of each process's window segment is stored in an array."
"actively engaging consumers to reduce their energy consumption, namely acting on the demand side, is therefore widely acknowledged as an urgent need for the future. for instance, recent research has shown that energy consumption feedback provided by in-home displays can lead to consumption reduction from 4%to 12% on average (with peak savings beyond 20% [cit] ); however, this type of intervention can fail if not combined with measures that make consumers' behavioral change durable. indeed, providing visual feedbacks on energy usage does not guarantee the achievement of consumption reductions by itself, as its success is largely dependent on user engagement [cit] . moreover, a careful design of visualization mechanisms should be adopted to make feedbacks readily understandable by the users and provide them with insights about their energy consumption habits and routines, possibly suggesting energy saving actions they could adopt [cit] ."
"model mpi-2 rma defined the \"separate\" memory model, which specifies the consistency semantics of accesses to data exposed in an rma window. this model was designed to be extremely portable, even to systems without a coherent memory subsystem. in this model, the programmer assumes that the mpi implementation may need to maintain two copies of the exposed buffer in order to facilitate both remote and local updates on noncoherent systems. the remotely accessible version of the window is referred to as the public copy, and the locally accessible version is referred to as the private copy."
"in this work we use devetak's approach [cit] to establish strong secrecy in bidirectional relay networks. therefore we start with a basic observation concerning the relationship of total variation distance 2 and mutual information. lemma 1: let a, b, and c be finite sets and a, b, and c be corresponding random variables. if"
"the learner tries to imitate the perceived acoustics by applying the current inverse model g and producing the acoustics corresponding to the articulatory estimations using the true forward model f v t l . then, error values are computed for evaluation purposes. finally, the forward and inverse model are updated with the new training pair: the learner's articulatory estimation and the acoustic outcome."
"gamification, in particular, has become a popular approach to behavioural change for resource-efficiency projects as a motivational tool, as it encourages users to set saving goals to achieve virtual prizes like badges and medals. examples of such approach can be observed in the projects makahiki [cit] and ley [cit] . it has also been used to provide information and training to users towards resource-efficient behaviours (projects like power house [cit] and the \"water mansion\" game [cit] ) or to promote social collaboration and competition for resource saving (wattsup [cit], social power [cit] and social electricity [cit] ). initially targeting households, gamified approaches have recently started been applied also to public buildings (greensoul [cit] and charged [cit] ) and schools as well, such as in the gaia project [cit] ."
"the adi3 interface provides the greatest flexibility for the developer; however, implementing a device using this interface also requires considerable effort because of the number of functions that must be implemented. for this reason mpich has a default device called ch3. the ch3 device implements functionality such as message matching, connection management, and handling of onesided communication operations and exposes a significantly simpler interface called the channel interface. developers can choose to implement a channel to support their platform. the figure shows two channels: nemesis and sock. although there is an additional layer between a channel and the application as compared to a device, common cases are fast-pathed through ch3 by using function pointers or function inlining, so the overhead of the additional layer is minimized and in some cases avoided entirely."
"we have created a linked list construction benchmark that can benefit from several of these new mpi-3 semantics. this benchmark uses a dynamic window; processes dynamically create new list elements, attach them to the window, and append them to the list. creation of such a dynamic, distributed data structure is not possible in mpi-2, because windows are fixed in size and must be created collectively. rather than storing the tail pointer in a fixed location, all processes traverse the list to locate the tail. thus, this benchmark captures an application behavior where processes traverse a dynamically growing linked list-for example, a work list in a producer-consumer computation."
"mpich is an implementation of the mpi standard developed at argonne national laboratory. a primary goal of the project is to provide a portable high-performance implementation that can 7 be ported and adapted as necessary by third-party developers to support various architectures and interconnects. for example, ibm and cray have ported and adapted mpich to support their supercomputers. to realize this goal, mpich is designed with various internal portability interfaces allowing third-party developers the flexibility to choose the best interface when porting mpich for their system. figure 2 shows these interfaces and the internal layers of mpich. the top layer in the figure is the application that uses the mpi interface to communicate with mpich. below this layer is the device-independent layer. this layer implements functionality such as object management and error handler management that would be common to all derivatives of mpich. this layer also implements collective communication operations, which can be overridden by lower layers if needed to provide implementations that are optimized to a particular platform. the device-independent layer exposes the adi3 interface to the device layer below it. a developer has the option to implement a device at this layer to support a particular platform. for example, the pamid device supports platforms using the pami interface, such as ibm's blue gene/q."
"these purposes are compatible with a lock/unlock implementation based on strict mutual exclusion. however, the lock routine is not required to synchronously block in order to acquire such a lock and may instead synchronize in the background or defer synchronization and communication altogether until the synchronizing unlock."
we showed the efficient learning of forward and inverse models for speech production and imitation using a recurrent neural network model. the considered data contain coarticulations of a large set of vowels and consonants. we demonstrated that initial models trained on small subsets of articulatory-acoustic data can be improved significantly by imitation-based refinement. this unsupervised process requires only acoustic data and can be developmentally interpreted as imitative learning in a tutoring situation.
"when using lock-all, finer-grained synchronization can be achieved with the request-generating operations discussed in section 2.2.2. it can also be achieved with the new \"flush\" routines: mpi win flush and mpi win flush local. these routines specify a particular target and ensure that all operations initiated to that target before the flush are remotely complete at the target (in the case of flush) or locally complete at the origin (in the case of flush-local). these routines also have \"all\" variants, mpi win flush all and mpi win flush local all, that are equivalent to flushing each target in the window in sequence."
the set of all achievable rate triples is the strong secrecy capacity region of the bbc with confidential messages and is denoted by c s bbc . theorem 1: the strong secrecy capacity region c s bbc of the bbc with confidential messages is the set of all rate triples
"the design of mpi rma synchronization gives the implementation great flexibility in deciding when to issue rma operations. the most obvious approach is an eager approach, where operations are issued as they are encountered; and for large transfers this is often an efficient choice. however, for short updates such as a single word put or accumulate, this approach generates a significant amount of network traffic as well as large latencies while waiting for operations to complete. as shown in figure 12, epoch synchronization operations can result in several additional messages."
this paper tackles the question of how to efficiently learn the forward and inverse mapping for syllable sequences from few supervised training examples and how such initial models can be refined in an unsupervised manner by trying to imitate acoustic stimuli.
"while the channel is responsible for actually putting bytes into and pulling bytes out of the network, ch3 assembles packets for sending and processes received packets. to send a packet, ch3 creates the packet header, then calls a channel send function passing a pointer to the header and a description of the data to be sent. if the data is contiguous, the description is simply a pointer to the buffer and the size. for noncontiguous data the description consists of a datatype segment. the channel uses the datatype-processing engine either to convert the description into a form usable by the underlying network (e.g., an i/o vector) or to pack the data into contiguous buffers. the send functions are nonblocking and therefore need to queue packets that cannot be immediately sent."
"we conduct a leave-one-sequence-out cross-validation test in order to assess the generalization performance of the esn for the forward and inverse model. that is, we train esns on 63 out of the 64 sequences (including the 50 variations per sequence) and compute the generalization error on the left out sequence. the errors of the forward and the inverse models are both calculated in the acoustic space using the dimension-normalized mean square error (mse)"
"we are currently investigating an extension to ch3 to better support devices that provide one-sided primitives. such support would require the addition of function-pointers for networksupported one-sided operations to ch3's per connection data structure, the virtual channel structure. in addition, a channel can provide different function pointers per connection, depending on whether shared-memory or network communication should be used to perform one-sided accesses at the target. significant challenges in this work will include the maintenance of data consistency and operation ordering when multiple communication mechanisms are used, for example, when put/get use rdma, but long-double-precision accumulate requires the use of messaging."
"speech production and the imitation of perceived sounds requires knowledge about how to control the articulators of the vocal tract in order to achieve desired acoustics. knowledge of two mappings is required in this context: the forward mapping estimates which acoustics will result from a specific articulatory movement. this corresponds to the learner's expectation of which acoustics his vocal tract will produce in response to a motor command. the inverse mapping, in contrast, estimates which vocal tract movements are required in order to reproduce an acoustic signal. knowledge of the inverse mapping enables acoustic imitation."
"the performance of the initial model depends highly on the number of initial training data: models trained with a larger amount of data produce lower errors. the refinement process reduces the error of the forward and inverse model in all cases significantly. while the error decreases quickly in the first iterations, it converges to a minimum that is comparable to the earlier generalization results presented in tab. i. the standard deviations of the errors in the last iteration are very low (on average 0.0008 for the forward model and 0.0025 for the inverse model)."
"mpi datatypes are descriptions of the layout of data in memory. a simple datatype may describe a contiguous buffer, whereas a more complicated datatype may describe a section of a multidimensional matrix. while the datatypes are defined by the application in a recursive manner, ch3 processes the datatypes in a iterative manner to improve performance. in mpi a buffer is described by a pointer, a datatype and a count specifying the number times the datatype repeats. ch3 internally defines a segment object, which consists of a buffer pointer, datatype and count tuple to identify the data to by sent or received, as well as an offset specifying a location in the stream of bytes defined by the tuple. once a segment is constructed, it can be passed to various datatype processing functions either to pack the data into or unpack the data from a contiguous buffer or to generate a different representation of the buffer (e.g., an i/o vector)."
"this paper describes the ongoing research of the encom-pass project, which aims at investigating the impact of ictbased behavioral change tools for energy savings. provided that the project team will manage to retain participants' attention and prevent high drop-out rates, the encompass real-life trial will enable a deeper understanding of what works and what does not in the design and deployment of an original mix of instruments, including contextual, personalized energy 6 saving recommendation, delivered with web and mobile user interfaces, gamification-based stimuli, such as points, achievements and rewards, and original game concepts, mixing real and digital games to increase awareness and promote education towards sustainable energy consumption."
"b) user data acquisition: it handles the acquisition of user-related psychographic variables from multiple sources: profile information gathered with a gamified mobile apps (e.g., about household composition and existing appliances), results of instant polls (e.g., quick feedback provided via app on activity or comfort conditions), along with data inferred in a privacy preserving way, through activity tracking measurements produced by sensors (e.g. user presence in a room)."
"all mpi windows are created collectively over the input communicator, and traditionally each process could associate only one contiguous region of memory with the window. this restriction posed significant challenges to applications that need to dynamically allocate and deallocate memory. mpi-3 addresses this issue by providing a new dynamic window facility, which collectively creates a window with no initially associated memory. memory can then be asynchronously attached to (or detached from) this window by individual processes. the new routines mpi win create dynamic, mpi win attach, and mpi win detach facilitate this new functionality."
"the simplicity offered by fences comes at a cost to application flexibility. to address this, mpi offers a more versatile, more complex synchronization mode known as generalized active target synchronization (gats). this facility is sometimes also known as post/start/complete/wait (pscw), referring to the primary synchronization routines involved in this mode. this mode differentiates between two different epoch types: an exposure epoch and an access epoch. processes that will be accessed as targets must invoke mpi win post calls, supplying a group argument to indicate the set of peer processes that will perform communication operations targeting the posting process. those origin processes must all correspondingly invoke mpi win start to begin an access epoch before issuing any communication operations. the start routine also takes a group argument indicating with which target processes the calling process will communicate. when the accessing processes have posted all rma operations, they must call mpi win complete to end the access epoch and force origin completion of the previously issued communication operations. target processes call mpi win wait (or mpi win test repeatedly) in order to wait until any communication operations initiated during the exposure epoch are completed at the target (this process)."
"theorem 1 shows that confidential services with strong secrecy can efficiently be integrated in bidirectional relay networks at the physical layer. but besides such confidential services, operators of current wireless systems usually offer also multicast services where a common message has to be transmitted to a whole group of receivers. the multimedia broadcast multicast service (mbms), as specified by the 3gpp organization, is only one example."
"if a derived datatype is specified for the target, an additional step is required to transmit the serialized datatype, whereas predefined datatypes are compact and can be embedded in the packet header. the derived datatype is serialized at the origin and transmitted following the packet header; the size of the derived datatype is contained within the packet header and used by the target to determine when the full datatype has arrived. once the target has processed the packet header, it may need to wait for the arrival of the datatype. to facilitate this process, it generates a request and registers the operation's request handler which the ch3 progress engine will use to continue the rma operation when additional datatype data has arrived."
"2.2.3. mpi-3 rma windows mpi-3 adds three new window types: mpi-allocated windows, dynamic windows, and shared-memory windows. mpi-allocated windows are created by calling mpi win allocate. in contrast to mpi-2 windows where the user supplied the window buffer, mpi allocates the buffer for this window, enabling the mpi implementation to utilize special memory (e.g., from a symmetric heap or a shared segment) or optimize the mapping for locality."
"mpi-3 added several refinements to passive target synchronization, including a lock-all passive target communication mode, request-generating operations, and flush operations. in addition, mpi-2 allowed only one passive target epoch at a time using lock/unlock operations; mpi-3 has lifted this restriction and allows a process to initiate one passive target epoch to every process in the window's group."
"if alloc shared noncontig info argument is specified when the window is created, the individual process's window segments are not required to be contiguously arranged, and the implementation is free to allocate them in a more optimal manner; for example, each window segment may be aligned on a page boundary. rather than allocating a separate shared-memory region for each window segment, a single shared-memory region is allocated as before, except that the size of each window segment is rounded up to the next page size. in this way only a single shared-memory region needs to be created and attached, but each window segment is aligned on a page boundary."
"the world is looking for alternatives to traditional energy sources: fossil fuels have been linked to climate change [cit], while at the same time nuclear energy sources still face issues related to safety and to the disposal of nuclear waste. renewable energies are gaining momentum, but their supply is still limited and often subject to high volatility. all of the above lead us to consider that energy is still a limited resource and that ideas, methods, and solutions for energy efficiency and saving will continue to be needed in the future."
"the receipt of user data is also facilitated by the progress engine, through the use of requests. for put and accumulate operations, once the datatype information has arrived in the data stream, the remaining data forms the data payload and it is received and processed by using an additional request handler. for get operations, the origin transmits a reference to its receive request in the packet header. this reference is included in the target's response, and is used to match the get operation with the corresponding local request that contains the origin's communication parameters."
"an alternative lazy approach to synchronization queues all operations and waits until the unlock call to process them. when this approach is used, synchronization operations can be piggybacked, merged, or in some cases eliminated, yielding significant communication latency improvements. in the case of active target synchronization, it is possible to eliminate one of the barriers within the fence operations in exchange for a single reduce-scatter operation [cit] . the lazy approach is advantageous when epochs contain few operations and perform short data transfers. however, when 19 there are either large numbers of rma operations or the operations involve large amounts of data, it is often better to issue those operations as they are encountered. thus, neither the eager nor lazy methods are always the best choice."
"the current messaging-based implementation of mpi rma provides good performance for networks that do not natively support one-sided operations. however, many modern networks provide one-sided and rdma support, which can yield considerable performance benefits."
"in spite of achieving these objectives, the mpi-2 rma interface has been found to be inadequate for many common one-sided use cases [cit] . to correct these insufficiencies, the mpi forum substantially updated and revised the mpi rma interface with the release of the mpi 3.0 [cit] . this effort involved many organizations and individuals, including the authors of this paper, over the span of several years. the update focused on addressing issues that have prevented mpi rma from providing a common, portable one-sided substrate for higher-level onesided and global address space models, as well as adding other features that have been demonstrated to significantly benefit application developers. these features include additional communication operations that more closely match traditional shared-memory programming models, relaxed synchronization rules, lighter-weight mechanisms for controlling ordering of communication operations, and new ways to allocate and associate memory with mpi one-sided windows."
"comparing figures 12a and 12b, we see that exclusive write epochs can have a lower protocol overhead than shared write epochs. the added overhead in the case of shared access is to ensure remote completion, in case of third-party communication. when exclusive access epochs are used, the target ensures that all operations from an epoch are complete before granting access to another process. in figure 10 we see the benefit from the reduced protocol on up to four processes, but the serialization of exclusive access epochs quickly overcomes the performance advantages of the simpler protocol."
"e) adaptive in-context action recommendation: this component embodies the core intelligence of the system, which computes and delivers actionable energy saving suggestions from all the different types of data available, including consumption and sensor data, user profile data, building information and user actions in the consumer app and in the building control system. the recommender cluster users in categories and exploits their activity patterns to elaborate energy saving recommendations for a specific user category and context. a rule-based engine maps consumption and activity patterns into energy saving tips, which are delivered to users as notifications in the consumer app. user's feedback on the suitability and utility of recommendations is also collected by the consumers app and fed back to the component, which exploits such information to adjust the recommendation rules. to overcome the cold-start problem, the recommender is endowed with generic rules, which apply to all classes of users and buildings, which are used to initialize the feedback loop between the recommender and the user. f) behavioural change consumer apps: these web and mobile applications are developed to help users make sense of their energy usage and engage in energy saving efforts, supported by gamification techniques. they are also responsible of delivering to the users the personalized suggestions elaborated by the recommender, in the form of notifications. their interface comprises several sections, targeting different phases of the behavioral change process and including: the visualization of consumption and comfort data over time, charted in histograms; the alternative representation of consumption data with visual metaphors that convey the impact of energy saving under different perspectives, e.g., emphasizing the monetary or ecological value of the achieved savings; the collection of generic and personalized energy saving tips, which help nudge the user from the intention of saving into concrete practice; a goal-setting area, where users can reinforce their willingness of acting by setting individual or collective energy saving goals; a profile data input area, where users can gain gamification points by inputting information on their household and building; and a gamification area, where users can inspect the complete history of their actions and achievements, the received and available rewards, and compare their results with those of their peers. the app feeds back user's achievements to the consumers' community in the form of individual or team leaderboards, which are designed with different time horizons (e.g., weekly and over the entire experimentation period) to support both early adopters with a long history of interaction with the system and newcomers."
"gats makes synchronization more flexible by making it possible to limit the number of processes with which any given process must synchronize to a minimal subset (e.g., neighboring processes in a halo exchange operation). only processes that actually communicate with each other must synchronize. furthermore, the synchronization pattern may be asymmetric in some ways because of the separation of the exposure epoch from the access epoch."
"the three monitoring periods are run consecutively and, during these periods, meters and sensors data of the involved users (both intervention and control groups) will be continuously collected. a three-wave survey (one survey for each monitoring period) will instead gather behavioural data on attitudes and perceptions. the assessment of the impact on electricity consumption data will be performed by means of the eemeasure methodology [cit], which allows for direct comparison with other energy saving projects and interventions developed throughout europe, and fits to trials both with and without a control group. assessment of the impact on behavioral data will instead be performed by comparing likert-scale answers to the same questions across the three waves of the survey, and by accounting for their variations. the target kpi values are 20%-25% reduction in average weekly electricity consumption and 1 point increase on a 5-points likert scale (+20%) in the user awareness of energy consumption, knowledge of energy saving actions and intention to save energy."
"a particularly important area for performance optimization is the management of rma operations and synchronizations. two key areas of future work are (1) piggybacking and merging synchronization messages with rma operations and (2) efficiently managing rma operations to optimize for short, latency-sensitive epochs, and long, bandwidth-bound epochs."
"the mpich rma implementation uses the message-processing capabilities to perform rma operations in the target process' address space. we define new ch3 packet types, packet handlers, and request handlers for each new mpi-3 rma operation. when performing an operation, the origin process generates a packet header with the corresponding operation type; populates the header with the communication parameters; and sends the header, any datatypes, and the data payload to the target. when the packet header is received by the target, ch3 dispatches the packet handler corresponding the packet type field in the message header, and the packet handler performs the operation at target and sends a response to the origin, if needed."
"errors of the forward and inverse model are measured in the acoustic domain as illustrated in fig. 3 . the inverse model error is calculated between the initial acoustic sequence and the signal reproduced from the estimated articulatory sequence via vocaltractlab, i.e. it is the difference between the tutor's signal and the learner's imitation."
"the mpi-2 standard defines just three communication operations: mpi put, mpi get, and mpi accumulate. the process that invokes a communication operation is designated the origin, and the process in which data is accessed is designated the target. the origin and target may be the same process, although performing the corresponding role in each case. origin communication buffers are specified by passing a pointer, count, datatype triple, whereas target communication buffers are specified by passing a displacement, count, datatype triple. this displacement value is scaled by the displacement unit value given by the target process at window creation time."
"when releasing the mutex, the last process must reset the tail pointer to −1. an atomic compareand-swap is performed on the tail pointer; if the process releasing the mutex is the tail, it replaces the old tail pointer with −1. otherwise, the process polls for its next pointer to be updated and forwards the mutex to the next process."
"in a first step, we apply an efficient recurrent neural network approach to learn the forward and inverse model of speech production for syllable sequences which cover the coarticulation of eight vowels and eight consonants. the recurrent neural network model handles syllable sequences as continuous trajectories in the acoustic as well as in the articulatory space. the network dynamics account for temporal dependencies in the sequences."
"however, most of these works use the criterion of weak secrecy which is heuristic in nature, in that no operational rates r 2 and r 1 to the relay node. in the succeeding bidirectional broadcast (bbc) phase, the relay forwards the messages m 1 and m 2 with rates r 2 and r 1 and adds a confidential message mc for node 1 with rate rc to the communication which has to be kept secret from node 2."
"the mpi-2 [cit] and culminated with the release of the mpi 2.0 [cit] . this document included the first version of the mpi rma interface, which added support for one-sided communication to mpi for the first time. the design and text of the mpi specification included contributions from several of the authors of this paper. the goals of the mpi-2 rma interface included providing a portable interface for one-sided communication; separating data movement from interprocess synchronization; and supporting cache-coherent and non-cache-coherent systems."
"put and get operations are implemented by having the origin process directly access the memory of the target process in the shared memory region. accumulate operations are required to be atomic for a given basic datatype and operation, so an interprocess mutex is created for each window to serialize accumulate and atomic operations. in the future, native atomic operations will be used for datatypes and operations supported by the processor (e.g. mpi sum on integer data). because processes are directly accessing the memory of other processes, window synchronization operations include appropriate memory barriers to ensure the proper ordering of memory accesses between the processes."
"this algorithm has two significant drawbacks. it allocates an array of size o(p ) bytes at the process that hosts the mutex. in addition, it requires the use of an exclusive access epoch in order to access the mutex structure. figure 5 . average time per lock-unlock cycle for a mutex on rank 0, requested by all processes 10,000 times."
"mpi-3 has introduced new synchronization operations for passive target communication, including lock-all and flush operations. these operations provide lighter-weight synchronization than mpi-2 lock/unlock epochs do. in addition, mpi-3 accumulate operations allow concurrent overlap of accumulate calls with get-accumulate calls that perform a no-op (i.e., atomic read). this allows applications to perform concurrent reads and writes to overlapping locations in the window, with well-defined results. the combination of these new semantics and operations allows applications to express many algorithms by using mpi shared locks, which greatly increases the concurrency with which data can be accessed."
the unified model relaxes several restrictions present in the separate model by assuming that the public and private copies of the window are logically the same memory. these restrictions and relaxations are summarized in figure 1 . this figure shows which operations are permitted to occur concurrently in the same window and whether those concurrent operations are permitted to access overlapping regions of the window.
"the sock channel uses tcp exclusively for communication, whereas the nemesis channel uses shared memory for intranode communication and a network for internode communication. nemesis exposes the netmod interface that allows the developer to implement a network module to support a particular interconnect. as with ch3, although nemesis adds an additional logical layer between the network module and the application, common cases are fast-pathed to avoid performance overhead. in fact, the nemesis tcp network module outperforms the sock channel despite the additional logical layer."
"in this paper we focus on the description of the en-compass gamified platform. after a brief overview of the related literature (section ii), section iii presents the overall encompass architecture and section iv details the main components of the gamified platform. section v outlines the design of the validation trials and the methodologies to assess its impact."
"besides data analytics, the input data streams are also used as a device to enhance awareness: measured statistics on consumption are exposed to the users via a consumer app comprising gamification elements that promote behavioural changes by means of game-like elements, such as points, achievements, and rewards, related both to measured energy savings and to the user's response to the delivered recommendations that require her to act on the building (e.g., changing the set point of the heating system). the consumer app also rewards the user for entering a few psychographic variables, e.g.,the composition of the family or the presence of appliances, which improve the accuracy of the recommender system, of the user and building modeler and of the disaggretator."
"we have redesigned the rma error detection in mpich to detect incorrect use of rma synchronization operations. an important design goal was that error checking add no more than several tens of cycles of overhead. to achieve this, we took a state machine approach to defining correct use of rma synchronization calls at each process. the corresponding state transition diagram we developed is shown in figure 3 . this diagram captures all correct uses of mpi calls; any deviation is erroneous and is reported by the mpi implementation. examples of incorrect usage include unmatched lock/unlock calls, mixing of passive and active target synchronization, use of flush or request-generating operations in active target, and mixing of passive target lock and lock-all synchronization."
"we have created a new mutex library, which uses the mcs algorithm [cit] . like the mpi-2 algorithm, this is also a fair, queueing mutex. rather than storing the queue on a single process, as was the case in the mpi-2 algorithm, the mcs algorithm creates a distributed linked queue. a shared tail pointer is created on the process that hosts the mutex and all processes allocate one list element, which holds the next pointer, an integer value that indicates the next process waiting on the mutex."
"compare-and-swap atomically compares a \"compare buffer\" at the origin with the target buffer and replaces the target buffer contents with the (separate) origin data buffer contents if the values are equal. the original value of the target buffer is always returned to the caller at the origin. this operation is limited to the integer subset of predefined datatypes, and the same datatype must be used for all buffers."
"vocaltractlab generates acoustic signals based on the articulatory trajectories. as acoustic features we chose melfrequency cepstral coefficients (mfccs), the standard features for speech recognition. the 39-dimensional feature vector contains logarithmic energy and the first 12 mfccs as well as the first and second derivatives. all articulatory and acoustic sequences have been normalized to the range [−1, 1]."
"from this perspective, the encompass project [cit] aims at inducing a durable improvement in energy consumption in public and private buildings, by adopting a theoreticallyfounded multi-faceted approach for changing the behavior of consumers; the envisioned method integrates in novel ways gamified motivational stimuli and recommendations for energy saving, tailored to the user's context, activity, comfort level and phase in the behavioral change cycle. this objective requires understanding the user's typology and habits, her comfort level and current activity, to deliver timely, relevant, and personalized energy saving hints. to engage consumers, en-compass develops an adaptive gamification platform aimed at dynamically motivating users to save energy. the platform processes smart meter and sensor data and collects a limited amount of profile data; it then estimates the users' consumption patterns and current activity context and provides them with personalized and context-aware recommendations of energy saving actions, supported by a rewarding system that is designed to foster the level of engagement (i.e., the extent to which recommendations are put into practice) and the consequent behavioural change and energy saving. in particular, the encompass platform tracks users' activity related to energy consumption (captured with smart meters), personal attitudes (identified through surveys and application activity logs) and context (estimated from comfort and sensor readings), and, by means of a recommendation engine, maps such an information into energy saving tips tailored to their own profile and current status. effectiveness of the encompass platform will be assessed in real-life field trials, involving households, schools and public buildings in three european pilot sites (germany, greece and switzerland)."
"as shown in the diagram, fence operations require additional state to track. the fence called state changes only during collective calls to fence. if fence has been called without the nosucceed assertion, it is possible to enter into a fenced active target access epoch. however, it is also valid to ignore the call to fence (i.e., it may have closed an active target phase in the program) and perform a different rma access type. it is invalid to perform a fenced active target access epoch if another synchronization mode is used on the window; however, we do not currently detect this error because of the state tracking complexity."
"the mpi-3 standard extends the mpi-2 rma functionality with improvements to the atomic operations interface, finer-grained control over operation completion and synchronization in passive target epochs, relaxed access restrictions when hardware-assisted coherence is available, and new window types that enable dynamic exposure of memory for rma and interprocess shared memory."
"passive target lock and lock-all operations utilize the same locking facility in the mpich design. lock-all is a one-sided operation that requests a shared lock, and it must be compatible with (possibly exclusive mode) lock operations issued by other processes. rather than performing a nonscalable lock operation at all targets, the lock-all implementation relies on the synchronization state tracking (see section 4.4) to indicate that all processes can be targeted by rma operations. we track the synchronization state of each target and issue a lock request on the first synchronization with the given process."
"ch3 uses packet handler functions to process incoming packets. once an entire header has been received from the network, the packet handler function is determined based on the packet type and is called with a pointer to the buffer where the packet is stored along with the number of bytes that have been received. the channel does not need to know the size of the packet, only the size of the header. if the buffer contains the entire packet, the handler will process the packet and then return the size of that packet. if the buffer does not contain the entire packet, the handler will process as much of the packet as possible and then return a request indicating where the channel should put the remainder of the packet when it is received, along with a handler function to call when the data has been received."
"mpi-3 defines several new rma window types, referred to as window flavors: mpi-allocated windows, dynamically allocated windows, and mpi-allocated shared memory windows. mpiallocated windows allow mpi to map the memory that will be targeted by rma operations, potentially enhancing performance. in ch3, mpi-allocated windows allocate memory using the device's memory allocator, which can be used to allocate memory that is associated with the device."
theorem 2: an achievable rate region for the bbc with an additional message from the relay to node 1 is given by set of all rate triples
"mpi-3 introduced several significant changes to passive target synchronization, including new flush and lock-all operations, as well as the ability to perform passive target epochs at multiple target processes concurrently. active target synchronization was unchanged in the mpi-3 specification, and the existing mpi-2 design has changed very little. in addition to adding support for per target operation queues, the mpich mpi-3 implementation separates release of a passive target lock from completion of operations in order to facilitate flush and request-generating operations."
"the goal of the encompass platform is to transform the data streams acquired from smart ambient and electricity consumption sensors into representations intelligible by the consumers and into recommendations of energy saving actions based on the current context and activity of the users. this task is accomplished by the coordinated cooperation of multiple components, embedded in the technical architecture portrayed in figure 1 . a) sensors data acquisition: this component ingests energy consumption data from smart electricity meters and smart plugs at individual appliance level, and sensor data (presence, temperature, luminance, humidity, etc.). it acquires data from multiple heterogeneous metering infrastructures, home control systems, and sensing equipment, converts them to a centralized uniform data model and serves the integrated data to all the other components of the platform."
"we implemented our messaging-based one-sided implementation in ch3 because (1) at this layer, all channels and network modules will be able to use the one-sided operations without having to reimplement them; (2) functionality that is needed by the one-sided implementation, such as processing of mpi datatypes and handling of packets, is available in ch3 but not at a higher level; and (3) an implementation of mpi-2 one-sided operations has previously been implemented in ch3, so we can reuse some of the existing functionality. next we briefly describe three aspects of the ch3 architecture that are relevant to the implementation of messaging-based one-sided operations: datatype processing, sending and receiving messages, and allocating and attaching to shared memory regions."
"in order to produce a deterministic result, each process p appends a new element only when the tail pointer points to an element at process p − 1, as shown in figure 8 . this process repeats until each process has appended n elements. initially, process 0 creates the head of the list and broadcasts the pointer to all other processes. pointers in mpi dynamic windows are represented by using the tuple rank, displacement, and we use a rank of −1 to indicate a null next element pointer. when the next element pointer of the current list item is null, processes poll on the location of the next pointer until it is updated."
"we show that the error of the forward and inverse model can be decreased significantly by sequentially retraining both models with the estimated articulation and the corresponding acoustic outcome. the results are systematically evaluated by standard error measures and by perceptual tests, i.e. rating the acoustics produced by the model."
"d) data analysis and building modeling: this component implements algorithms for estimating the comfort levels and energy behaviour of buildings, based on a reduced set of measured input parameters dependent on the class of the buildings. it allows for the estimation of the impact of consumers actions on energy consumption and on the status of the building, so to help the recommender component to prioritize energy saving recommendations based on both the user's context and activity and on the potential impact of the suggested action. the base model includes: thermal building characteristics (i.e., envelope materials, building size), user profile, local climate data, energy consumption of subsystems (i.e., hvac, plug loads etc.)."
"in encompass we adopt a similar approach, extending it in several aspects in order to overcome key limitations related to the provision of static, one-size-fits-all tips and recommendations for change [cit] . in fact, smarth2o offers its users a fixed set of tips and educational videos, which may lose effectiveness in case they are not related with the user specific action or context [cit], or the time of the action [cit] . instead, encompass incorporates a dynamic recommendation engine that considers the users' profiles and the context of their actions (by means of data on movement, temperature, humidity and luminance automatically gathered by sensors), that allow to provide users with timely and personalized recommendations. besides just collecting additional information by sensors and providing suggestions, in presence of automated home management solutions already deployed at the customers' premises, encompass is also able to automatically perform energy saving actions, when specific situations are detected. for example, if the encompass platform understands that in an empty room (no motion detected by movement sensors) a light is on (electricity consumption detected by smart meter data and energy consumption disaggregation algorithms or luminance sensors), it automatically turns the light off."
"all the buildings participating in the trial will be endowed with the encompass platform (smart meters for electricity consumption, sensors for motion and indoor and outdoor temperature and humidity, smart plugs and behavioural change app) and energy-related behaviour of its users will be monitored over time. in particular, smart meters have been on purpose installed in the swiss site, while they were already available in germany and greece. sensors, instead, will be installed on purpose for the trial: households will install them in the living room, schools in each classroom and in the foyer area, public buildings in each office, in the common areas and in visitors areas. in the same areas, for school and public buildings a tablet will be installed, in order to let groups of students/colleagues/visitors frequently interact with the encompass platform."
"operators of wireless networks are confronted with an inherent problem: due to the open nature of the wireless channel a transmitted signal is received by its intended users but can also easily be eavesdropped by non-legitimate receivers. to keep information secret, current systems usually apply cryptographic techniques which are based on the assumption of insufficient computational capabilities of non-legitimate receivers. it is clear that with increasing computational power these techniques become more and more insecure."
"both electricity consumption and smart meters data will be continuously gathered, with a frequency of fifteen minutes. besides them, the field trial will also collect user perceptions and attitude data, as proxies for behavioural predictors. such data will be gathered by using methods typically employed in user-centered design and psychological research, such as questionnaires, focus groups and interviews. impact of the encompass platform will be measured by comparing the set of data collected during the encompass intervention period, with respect to the same set of data collected before the intervention. to univocally attribute differences in consumption and behavioural predictors to the encompass platform, for households the trial also envisions a control group in each pilot site, that is a group of users that are similar to those of the intervention group, and are monitored in order to obtain data uninfluenced by the encompass platform -in fact they are not treated by the encompass tools. also for them, data will be collected before and after the intervention. therefore, comparing differences in the set of data collected from the intervention group and the control group, will allow for and assessment of the net effect of the encompass platform. conversely, no control group is envisioned for schools and public buildings, due to the difficulty of individuating buildings with similar characteristics (in building type, pupil/employee composition etc.) in the same geographical and climate context."
"mpi-3 introduced an optional capability that orders accumulate operations issued by a particular origin process, at each target. this has a direct impact on programming models and algorithms that require location consistency-that a given process observes the results of its own operations in the order in which they were issued. models such as armci [cit] and ga [cit] utilize this consistency model. this model is believed to be convenient because it resembles shared memory programming and is easier to utilize at an application level. figure 6 . pseudocode for the location consistent read-after-write benchmark. in mpi-2, separate epochs are needed to order operations; in mpi-3, ordering is provided for accumulate operations. in figure 6 we show pseudocode for a simple benchmark that performs a write operation followed by a read to the same process. location consistency can be achieved by using individual epochs (the approach needed in mpi-2) or by using ordered accumulate operations. accumulate operations are ordered by default in mpi 3.0; to improve performance, this behavior can be disabled using an info argument when the window is created. figure 7 shows the latency for location consistent read-after-write between two processes using the ordered epochs (mpi-2) and ordered accumulates (mpi-3) approaches. mpich includes an optimization that merges the lock, rma operation, and unlock operations into a single one-way communication for writes and a single round-trip communication for reads [cit] . we show the mpi-2 implementation with and without this optimization; when the optimization is disabled, the protocols shown in figures 12a and 12d are used. from this data, we see that accumulate ordering results in a significant reduction in latency, because of the reduction in rma synchronization overheads."
"the deployment of smart meters in an increasing number of households has boosted research on methodologies for inducing behavioural change based on energy consumption feedback. several energy saving applications embedded in users' everyday environment have been proposed [cit], using visualized consumption feedback and gamified social interactions to motivate people to adopt energy-efficient lifestyles. several are data-oriented (e.g. bar or pie charts of consumption [cit], some are closely connected to the real consumption contexts (e.g. floor plans [cit] ), others are metaphorical (e.g. traffic lights and gauges [cit] ), playful and ambient (beaware [cit] and smarth2o [cit] ), or connected to nature or animal habitats (eco-visualization [cit] ."
"three releases of the encompass platform are actually envisioned: the first one (r1) [cit], the second one (r2) [cit], the third one (r3) [cit], at the end of the project. release r2 will offer more advanced functionalities of the encompass platform, while release r3 will simply include bug fixes and final refinements, as identified from the trial themselves, and therefore does not need to be tested. therefore, three monitoring periods are scheduled: (i) before the intervention electricity consumption baseline data are collected [cit] ), (ii) during \"intervention 1\" (use of r1, [cit] ) and (iii) during \"intervention 2\" (use of r2, [cit] . note that slightly different periods are set for schools, in order to take account of the summer holidays."
"shared memory programming can provide an efficient means for utilizing on-node resources [cit] . to this end, many programmers combine mpi with openmp or a threading library, which adds to the complexity of managing two parallel programming systems in the same program. mpi-3 adds a new shared-memory window, created via mpi win allocate shared, which allows processes to portably allocate a shared-memory segment that is mapped into the address space of all processes participating in the window. in addition, the new mpi win sync routine that synchronizes load/store operations and incurs less overhead than a full window synchronization. by using mpi rma synchronization and atomic operations, shared-memory windows provide programmers with a complete, portable, interprocess shared-memory programming system."
"no coherence in the memory subsystem or network interface is assumed by the mpi-2 rma \"separate\" memory model, resulting in logically distinct \"public\" and \"private\" copies of the window copies described in section 2.1.2. this conservative model is a poor match for computers with coherent memory subsystems, as it does not provide access to the system's full performance and programmer productivity potential. a new \"unified\" memory model was added in mpi-3 to address this shortcoming."
"2.1.1. one-sided communication operations all mpi rma communication operations occur in the context of a window. a window is composed of a group of processes, specified at windowcreation time by a communicator, and a contiguous region of memory at each process. the memory region at each process may differ in size and address at each process. mpi window creation is a collective, potentially synchronous operation over the input communicator that occurs via a call mpi win create. in addition to a communicator, buffer pointer, and buffer size parameter, the caller must specify a \"displacement unit\" and an optional set of mpi info hints used to enable potential optimizations by the mpi implementation. only memory that has been exposed in a window can be accessed by using one-sided communication operations."
"we use our implementation of mpi-3 rma to evaluate the performance impact of new functionality and semantic changes introduced in mpi-3. our rma implementation has been integrated into mpich 3.0.1 and is publicly available. we evaluate the effectiveness of several major changes in the mpi rma specification: the new unified memory model, new atomic communication operations, new passive target synchronization operations, and new window types."
"mpich maintains rma operation queues, which are flushed when an access epoch is completed [cit] . when a given window is in active target mode, all operations are batched in a single queue, since the completion of any active target access epoch will require the completion of all operations. when a window is accessed by using passive target mode, individual operation queues are created to manage communication with each target. we distinguish active and passive target access epochs through the state tracking discussed in section 4.4. this permits more efficient synchronization with individual targets, for example, flushing or unlocking an individual target process."
"the original concept of the encompass architecture integrates several components and processes aimed at triggering individual and collective behavioural response to specific energy conservation policies. the framework collects data generated by electricity meters and sensors installed at the users' premises, as well as data derived from users actions on a gamified application. the collected data are processed using data analysis instruments. the sensor data stream is exploited by an activity tracker subsystem, which infers the current activity of the user in the building; the consumption data stream is exploited by a disaggregator, which estimates the partition of the total metered consumption into end-uses (e.g., water heating, specific appliances, etc); both the sensor and consumption data stream are fed to a recommender subsystem, which computes in-context adaptive recommendations, tailored to the user's current context and activity. the data analytics processes also support users and building modeling, which helps the administrator of an utility company to monitor, simulate, and ultimately better manage the demand."
"note that inputs u and outputs v of the esn take the role of articulatory parameters x and acoustic representation y for the forward model (cf. fig. 1 ). accordingly for the inverse model, inputs u correspond to y and outputs v to x."
"despite these limitations, our model provides a principled explanation for speakers' choices of referring expressions. in future work we hope to look at a broader range of referring expressions, such as null pronouns and definite descriptions, and to explore the extent to which our model can be applied to other linguistic phenomena that rely on discourse information."
"neither corpus has agreement information. the following section describes manual annotations that we have done for this study. due to time constraints, we annotated only a part of the childes gleason corpus, 9 out of 70 scripts."
this portion deals with the intuitive way about the experiment and data reading using wireless devices. we first elaborate the first part of the experiment i.e. person's body posture detection through csi using lcx.
"suppose that the speaker is considering using 'he' to refer to barack obama, which has been referred to n o times in the preceding discourse, and there is another singular and male entity, joe biden, in the preceding discourse that has been referred to n b times. in this situation, the model computes the probability that the speaker uses 'he' to refer to barack obama as follows:"
"the main idea of frank and goodman's model is that a rational pragmatic listener uses bayesian inference to infer the speaker's intended referent r s given the word w, their vocabulary (e.g., 'blue', 'circle'), and shared context that consists of a set of objects o (e.g., visual access to object referents) as in (1), assuming that a speaker has chosen the word informatively."
"to collect csi measurements, the client pings the ap at 20 packets per second. a modified driver is used to collect client side data [cit] . wifi link observes the static multipath depending on its location of endpoints [cit] . this results in data collection that is different at different locations. hence the two different scenarios show the variance in recordings when spread over two different rooms."
note that h i (k) is the cfr of the i th sub-carrier at time instant k. h i (k) is a complex number denoted as amplitude
salience of the referent is computed differently depending on its information status: old or new. the following illustrates the speaker's assumptions about the listener's discourse model:
"many mentions (46,246 out of 56,575 mentions in ontonotes and 10,141 out of 10,530 mentions in childes gleason) [cit], leaving 10,329 to be manually annotated from ontonotes (about 18%) and 389 from childes (about 4%). 5 the guidelines we followed for this manual agreement annotation were largely based on pronoun replacement tests. nps that referred to a single man and could be replaced with he or him were labeled 'male singular', nps that could be replaced by it, such as the comment, were labeled 'neuter singular', and so on. nps that could not be replaced with a pronoun, such as about 30 years earnings for the average peasant, who makes $145 a year, were excluded from the analysis."
"we considered only maximally spanning noun phrases as mentions, ignoring nested nps and nested coreference chains. for the sentence \"both al gore and george w. bush have different ideas on how to spend that extra money\" from ontonotes, the extracted nps are both al gore and george w. bush and different ideas about how to spend that extra money."
"in recent years, a number of formal models have been proposed to capture inferences between speakers and listeners in the context of gricean pragmatics [cit] . these models take a game theoretic approach in which speakers optimize productions to convey information for listeners, and listeners infer meaning based on speakers' likely productions. these models have been argued to account for human communication [cit], and studies report that they robustly predict various linguistic phenomena in experimental settings (goodman and stuhlmüller, 2013; [cit] . however, these models have not yet been applied to language produced outside of the laboratory, nor have they incorporated measures of discourse salience that can be computed over corpora."
here the wifi transmitter sends 20 packets per second. we receive the csi reading and formulate the data according to matrix h representation. it is a 30x 1 matrix known as the channel frequency response (cfr). each row indicates one sub-carrier frequency while the column of the cfr matrix refers to the receiving antenna (i.e. lcx).
"where the sum denotes the total discourse probability of the referents referred to by that word. the information content of an event is defined as the negative log probability of that event. in this scenario, the information conveyed by a word is the logarithm of the first term in (7), − log r p (r ). this means that in deciding which word to use, the highest cost a speaker should be willing to pay for a word should depend directly on that word's information content."
"where count u sing&masc in the denominator of the last line denotes the number of unseen singular & male entities that could be referred to by 'he'. we estimate this number for each type of pronoun we evaluate (singular-female, singular-male, singularneuter, and plural) [cit] . the term ("
"for example as seen in figure 5, change in body posture affect the shift between los and nlos as a result the signal packets received by lcx through los and nlos paths are distinctive. recording the cfr signals over time helps in deducing the postures for individual patient's in hospital room or across multiple rooms. the key advantage is only one lcx is used instead of installing a dedicated receiving antenna by each bed side."
"after filtering pronouns and proper names according to these criteria, 553 pronouns and 1,332 proper names (total 1,885 items) in the ontonotes corpus, and 165 pronouns and 149 proper names (total 314 items) in the childes gleason corpus remained for use in the analysis."
"in the next section, we propose a speaker model that formalizes the relation between discourse salience and speakers' choices of referring expressions, considering production cost and speakers' inference about listeners in a principled and explanatory way."
"while our work does not make use of this pragmatic listener, it does build on the speaker model assumed by the pragmatic listener. this speaker model (the likelihood term in the listener model) is defined using an exponentiated utility function as in (2)."
"speakers' choices of referring expressions have long been an object of study. pronominalization has been examined particularly often in both theoretical and experimental studies. discourse theories predict that speakers use pronouns when they think that a referent is salient in the discourse (givón, 1983; [cit], where salience of the referent is influenced by various factors such as grammatical position [cit], recency [cit], topicality [cit], competitors [cit], visual salience [cit] b), and so on."
"relevant formal models in computational linguistics include centering theory [cit] and referring expression generation [cit] . these models propose deterministic constraints governing when pronouns are preferred in local discourse, but it is not clear how these would account for speakers' choices of referring expressions, nor it is clear why there should be such deterministic constraints."
"there is also a rich body of research that points to the importance of production cost [cit] b; [cit] ) and listener models [cit] in language production. these studies suggest that only considering discourse salience of the referent may not precisely capture speakers' choices of referring expressions, and it is necessary to examine discourse salience in relation to these other factors."
obstacles in the wireless range cause signals to be out-ofphase at the receiver. the signal strength is reduced due to multiple reflections and diffractions [cit] . a simple description of monitoring sleep postures is shown in figure 4 . lcx of one meter in length is wired around the area and signals are watched periodically.
"in this scenario, we have used two wifi routers operating at different channels i.e. 2.41 ghz and 2.45 ghz respectively. the key idea of selecting different frequency channels is to avoid interference."
"the model requires coreference chains, agreement information, grammatical position, and part of speech. these were extracted from each corpus, either manually or automatically. the coreference chains let us easily count how many times/how recently each referent is mentioned in the discourse, which is necessary for computing discourse salience. the agreement information (gender and number of each referent) is required so that the model can identify all possible competing referents for pronouns. for instance, barack obama will be ruled out as a possible competitor for the pronoun she. the grammatical position that each proper name occupies 2 determines the form of the alternative pronoun that could be used there. for example, the difference between he and him is the grammatical position that each can appear in. the part of speech is used to identify the form of the referring expression (pronouns and proper names), which is what our model aims to predict. 3 ontonotes includes information about coreference chains, part of speech, and grammatical dependencies. gleason childes has parsed part of speech and grammatical dependencies [cit], but it does not have coreference chains."
"these maximally spanning nps were automatically extracted from the ontonotes data, but were manually annotated for the childes data using brat [cit] by two annotators. 4"
"c· to unseen referents in the denominator of (5), regardless of whether the word is a proper name or pronoun. in other words, this model does not have good estimates of unseen referents like the complete model does."
"our model predicts that the speaker's probability of choosing a word for a given referent should depend on its cost relative to its information content. to see this, we combine (4) and (5), yielding"
"the denominator in (5) represents the sum of potential referents that could be referred to by word w. we assume that a pronoun can refer to a large number of unseen referents if gender and number match, but a proper name cannot. for example, 'he' could refer to all singular and male referents, but 'barack obama' can only refer to barack obama. this assumption is reflected as a probability of unseen referents for the pronoun as illustrated in (10) below."
"while these results appear to support uid, there are several inconsistencies with previous uid accounts. information content of words has been estimated using an n-gram language model [cit], a verb's subcategorization frequency [cit], and so on, whereas here the information content is that of referents with respect to discourse salience. in addition, selecting between a pronoun and a more specified referring expression involves deciding how much information to convey, whereas previous applications of uid [cit] have been concerned with deciding between different ways of expressing the same information content. we show in the next section that we can derive predictions about referring expressions directly from a model of language production."
"the total accuracy of the model without good estimates of unseen referents was the worst among the four models, but this model did predict pronouns to some extent. because the number of proper names is larger than the number of pronouns in this dataset, the difference in total accuracies between the model without good estimates of unseen referents and the models without discourse or cost reflects this asymmetry. comparison between the complete model and the model without good estimates of unseen referents also suggests that having knowledge of unseen referents helps correctly pre-"
"experiments with the adult-directed news corpus show a close match between speakers' utterances and model predictions. on the other hand, experiments with child-directed speech show that the models were more likely to predict proper names where pronouns were used, suggesting that the estimates of discourse salience using simple measures were not sufficient to capture a conversation."
"the automatic patients' body posture detection with finegrained channel state information is essential to prevent pressure ulcer development: measurements indicated that using the leaky coaxial cable with low-cost wireless devices can classify postures for multiple patients in hospitals. the system works for two or more than two patients when a dedicated wifi router is deployed for each subject that operates at different frequency channel. the features considered in the proposed system for posture detection are computationally inexpensive, can be easily deployed and obtain high identification accuracy. the improvement in system performance is based on improving the ap and better monitoring applications. the reporting and notification has to be automated and classifiers that are robust will notify based on longer observation time. this can vary from 6-24 hours."
"the methodology has two components. first, we selected a conference room in xidian university to detect single patient body posture as shown in figure 4 . the subject data is as shown in table-1. second experiment, one conference room and an adjacent sitting room were used to detect body positions on bed for two different persons simultaneously, to emulate hospital scenario."
"note we have considered people who are an obstacle between the transmitter and receiver and causing reflection, refraction and scattering. by accommodating the change in body posture causing a shift in nos and nlos, we have the result from the signal received by the receiver as per cfr values [cit] . figure 10 shows the amplitude of cfr and time history for the subject's three postures in a sitting room. this set of measurements includes baseline values to differentiate the sleeping postures verses sitting. here the cfr amplitudes for the lateral and supine position are almost identical while for the fowler posture, this presents a major shift in amplitudes. in order to see a change in body posture, we select the cfr sequence from the ninth row of the cfr matrix (the time history of the csi data from the ninth subcarrier) [cit] ."
"resulting in the suppression of the nth harmonic. radiating field strength is higher for odd order modes. when compared to an antenna setup, wireless access systems with lcx are robust. installation of the access point unit and overloads of the handover mechanism within one cable segment is minimal [cit] ."
"to extend frank and goodman's model to a natural linguistic situation, we assume that the speaker estimates the listener's interpretation of a word (or referring expression) w based on discourse information. we extend the speaker model from (3) by assuming that a speaker s chooses w to optimize a listener's belief in speaker's intended referent r relative to the speaker's own speech cost c w . this cost is another factor in the speaker model, roughly corresponding to utterance complexity such as word length. 1"
") is the sum of probabilities of unseen referents that could be referred to by the pronoun 'he'. the unseen referents can be interpreted as a penalty for the inexplicitness of pronouns. in the case of proper names, the denominator is always the same as the numerator, under the assumption that each entity has one unique proper name."
"the paper is organized as follows. section 2 reviews relevant studies on choices of referring expressions. section 3 describes the details of our model. section 4 describes the data, preprocessing and annotation procedure. section 5 presents simulation results. section 6 summarizes this study and discusses implications and future directions."
"bedsores or pressure ulcers are caused by prolonged pressure on the skin that limits blood circulation in neighboring tissues. pressure ulcers are common clinical problems encountered in hospitals or nursing homes. limited mobility and lack of repositioning on the bed makes the patient predisposed to bedsores. chronic suffering that results in critical illness, frail elderly patients; spinal or cerebral patients are at high risk of developing bedsores. some of the reasons for such lack of susceptibility are: 1) constant pressure: when the layers of the epidermis, dermis and subcutaneous tissues are compressed due to long periods of lying on the bed. the functioning of small capillaries that circulate blood providing nutrients and oxygen are limited. with lack of oxygen and nutrients, the skin tissues are unable to thermo-regulate and perform metabolic functions. this kind of pressure is typically felt at the elbows, shoulder blades, spine, hips and spine causing deeper infections. 2) friction: the patient's skin is fragile, with movements on the bed resulting in injuries. the frictional action enhances the chances of pressure ulcers. 3) shear force: when the bed is raised at the head, an inadvertent motion results in sliding. this can cause movement of bone a shear or a tensile strain in the subcutaneous layer [cit] . subcutaneous tissue is a rich layer beneath the skin connecting the dermis and epidermis. its functional roles include acting as an insulator to conserve internal body heat and protect deeper tissue and organs. it also acts like a reserve to store energy and bodily fluids [cit] as a measure to save rapid damage to the skin and to reduce the risk of developing bedsores, it is necessary to relieve the skin pressure by changing the patient's postures every two hours [cit] . hence posture recognition on the bed is vital since timely intervention can reduce the risk of developing bedsore. for example, in persuasive hospital, a nurse can use a mobile activity monitor to provide immediate care for patients in need of assistance or in risky situations [cit] . at homes elderly care requires assistance because of frail mental and physical abilities. the length of hospital stays, as well as hospital costs, for treating bedsores has increased, whereas there has been rise in shortage of nurses [cit] . in some cases patients are often unable to make the desired body movements and repositioning that is critical for blood circulation and relieving of prolonged pressure on the body [cit] as illustrated in figure 1 . in order to prevent the aforementioned effects, continuous patient's body postures and reporting of the status is necessary."
"signal measurement with one antenna as the transmitter and another as the receiver is called a single input single output (siso) system. a single input multiple output (simo) can reduce the error rates, and increase the snr and reliability. yet the deployment of receiving antennas that are omni-directional over a large area is expensive. it also results in complex receiving and computational setups. figure 3 compares the signal acquisition with an antenna in an occluded structure. the conventional antenna with structural obstruction will have shadow zones and chances of missing data are inevitable. lcx deployment is considerably easy and hence blind zones are overcome and seamless data reception is possible."
"overall, the recency salience measure provides a better fit than the frequency salience measure with respect to accuracies, suggesting that recency better captures speakers' representations of discourse salience that influence choices of referring expressions. on the other hand, the models with frequency discourse salience have higher model log likelihood than the models with recency do. this is because of the peakiness of the recency models. model log likelihood computed over pronouns and proper names (complete model) were -1022.33 and -222.76, respectively, with recency, and -491.81 and -467 .06 with frequency. the recency model tends to return a higher probability for a proper name than the frequency model does. some pronouns receive a very low probability for this reason, and this lowers the model log likelihood."
"although the simulations in this paper employed simple measures for discourse salience (referent frequency and recency), the discourse models used by speakers are likely to be more complex. studies show that semantic information that cannot be captured with these simple measures, such as topicality [cit] and animacy [cit] ), affects speakers' choices of referring expressions. future work will test to what extent this latent discourse information could affect the model predictions."
the periodicity and adjacency in the slots make it very efficient in signal collection. the orientation of the slots results in constructive and destructive interferences of the rf feed. the radiation of the signal is characterized by the mode of the lcx.
the experimental setup is low-cost cots wifi device compatible with existing standards. netgear r8500 wifi routers used as the access point (ap) that operates around 2.4 ghz frequency. the wifi device connected to the ap is an hp desktop computer embedded with intel 5300 wifi card. the transmitter and receiver are placed at a distance of 2 meters apart.
"in a leaky coaxial cable or continuous antenna, each slot radiate energy with the propagation angles that make it highly directional. the directivity is also dependent on the relative permittivity of the material, periodicity of the slots and rf signal wavelength. the lcx cable as shown in figure 2 has a spacing and orientation that alternates. the rf signal is radiated and received from these slots located on the cable."
"note that cfr is a 30x m matrix, where m is the number of packets received. each row of cfr represents the temporal change of the csi information over one subcarrier [cit] . it shows the variability in amplitude of cfr and time history for a subject's three postures. the cfr amplitudes for the supine and lateral position are almost identical while, for the fowler position, a major change is observed."
"in wireless range, people around an obstacle cause multipath propagation phenomena. while such effects are often averaged out, when looking at a single average rss measurement, the individual subcarrier measurements are more likely to change when small movements have altered the multipath environment [cit] ."
the usage of lcx subcarriers is mainly to identify unique patterns for body postures in bed. this is verified for single and multiple patients in accordance to the 802.11 wireless range. the experiment was performed on eight different subjects at two different locations. the testing was carried out in the presence of another person and close frequency range of signals.
"computational models relevant to speakers' choices of referring expressions have been proposed, but there is a gap between questions that previous models have addressed and the questions that we have raised above. grüning [cit] examine the significance of various factors that might influence choices of referring expressions by using machine learning models such as neural networks, logistic regression and decision trees. although these models qualitatively show some significant factors, they are data-driven rather than being explanatory, and have not focused on why and how these factors result in the observed referring choices."
"in this paper, we propose a probabilistic model to explain speakers' choices of referring expressions based on discourse salience. [cit] to incorporate updates to listeners' beliefs as discourse proceeds. the model predicts that a speaker's choice of referring expressions should depend directly on the amount of information that each word carries in the discourse. simulations probe the contribution of each model component and show that the model can predict speakers' pronominalization in a corpus. these results suggest that this model formalizes underlying principles that account for speakers' choices of referring expressions."
"unlike the adult-directed news text, neither recency nor frequency discourse salience provides a good fit to the data. the low accuracies of pronouns and the high accuracies of proper names in all models indicate that the models are more likely to predict proper names than pronouns. there are several possible reasons for this. first, the childes transcripts involve long conversations in a natural settings. compared to the news, interlocutors are not focusing on a specific topic, but rather they often switch topics (e.g., a child interrupts her parents' conversation about her father's coworker to talk about her eggs). this topic switching makes it difficult for the model to estimate discourse salience using simple frequency or recency measures. second, interlocutors are a family and they share a good deal of common knowledge/background (e.g., a mother said she as the first mention of her child's friend's mother). the current model is not able to incorporate this kind of background knowledge. third, many referents are visually available. the current model is not able to use visual salience. in general, these problems arise due to our impoverished estimates of salience, and we would expect a more sophisticated discourse model that accurately measured salience to show better performance."
"to estimate the salience of a referent, p (r), our framework employs factors such as referent frequency or recency. although there are other important factors such as topicality of the referent [cit] that are not incorporated in our simulations, this model sets up a framework to test the role and interaction of various potential factors suggested in the discourse literature."
"we used the provided coreference chains for the ontonotes data, but for the childes data, it was necessary to do this manually using brat. the guidelines we followed for determining whether mentions coreferred came from the ontonotes corefer-ence guidelines [cit] . 6"
"note that the wifi router (transmitter 2) operates at a central frequency of 2.42 ghz. the reason for selecting different frequency channels for transmitter 1 and transmitter 2 is to avoid co-channel interference. the measurement changes in amplitude variations along with consistency in empirical observations give a fair estimate. the time interval during which there are no changes indicate a routine check on the patient. long duration of the patient in any aforesaid position will result in constant amplitude. thus, this indicates a plausible need for assistance."
"previous linguistic studies have focused on identifying factors that might influence choices of referring expressions. however, it is not clear from this previous work how and why these factors result in the observed patterns of referring expressions. where formal models relevant to this topic do exist, they have not been built to explain why there is a relation between discourse salience and speakers' choices of referring expressions. even uid, which relates predictability to word length, is not set up to account for the choice between words that vary in their information content."
"discourse information plays an important role in various aspects of linguistic processing, such as predictions about upcoming words [cit] ) and scalar implicature processing [cit] . the relationship between discourse information and speakers' choices of referring expression is one of the most studied problems. speakers' choices of referring expressions have long been thought to depend on the salience of entities in the discourse (givón, 1983) . for example, speakers normally do not choose a pronoun to refer to a new entity in the discourse, but are more likely to use pronouns for referents that have been referred to earlier in the discourse. a number of grammatical, semantic, and distributional factors related to salience have been found to influence choices of referring expressions [cit] . while the relationship between discourse salience and speakers' choices of referring expressions is well known, there is not yet a formal account of why this relationship exists."
"in order to see a change in body posture, we select the cfr sequence from the twelfth row of the cfr matrix (the time history of the csi data varies from the twelfth subcarrier onwards) [cit] .the choice of this particular subcarrier for analysis is that the highest difference between corresponding subcarriers is observed at this subcarrier and is also observed in the posture changes as shown in figure 7 . in this case we look at the realistic scenario with multiple patients in different rooms or spatially separated. the subject data is as shown in table-2 and the lcx is of a longer length, currently using a 3-meter long cable as shown in figure 4 (b). to correlate the change in body posture, we select the cfr sequence from the third row of cfr matrix (the time history of the csi data from the third subcarrier) [cit] . again, the choice is empirically done by selecting the particular subcarrier for the analysis, which has the highest difference between observed subcarriers. figure 9 indicates the cfr registrations of three body postures for 235 seconds. the reading is as follows: subject was lying in the fowler position for 100 seconds then changed posture to lie laterally until the 250 th second and remained in a supine position for 85 seconds until the 325th second."
"the orthogonal frequency division multiplexing (ofdm) splits the signal into 30 subcarriers. the frequency spacing of the subcarriers is orthogonal and hence reduces interference. it also improves the spectral efficiency for reconstruction and reduces hardware cost for realization. with the ofdm-phy layer, currently 802.11a/g/n/ac standards, we can extract the channel frequency response (cfr) in the format of channel state information (csi) from off-the-shelf commercial hardware [cit] .the usage of standardized 802.11 bands makes the application low cost, which and seemingly integrates into the existing infrastructure."
"the model without discourse and the model without cost consistently failed to predict pronouns (these models predicted all proper names). this happens because in the model without discourse, the information content of pronouns is extremely low due to the large number of consistent unseen referents. in the model without cost, pronouns are disfavored because they always convey less information than proper names. the log likelihoods of these models were also below that of the complete model. these results show that pronominalization depends on subtle interaction between discourse salience and speech cost. neither of them is sufficient to explain the distribution of pronouns and nouns on its own."
where the count v is the number of words that can refer to referent r. we assume that v is constant across all referents. our reasoning is as follows.
"thus, the speaker model chooses a word based on its specificity. we show in the next section that this corresponds to a speaker who is optimizing informativeness for a listener with uniform beliefs about what will be referred to in the discourse. the assumption of uniform discourse salience works well in a simple language game where there are a limited number of referents that have roughly equal salience, but we show that a model that lacks a sophisticated notion of discourse falls short in more realistic settings."
"we use the adult-and child-directed corpora to examine to what extent each model captures speakers' referring expressions. we selected pronouns and proper names in each corpus according to several criteria. first, the referring expression had to be in a coreference chain that had at least one proper name, in order to facilitate computing the cost of the proper name alternative. second, pronouns were only included if they were third person pronouns in subject or object position, and indexicals and reflexives were excluded. finally, for the childes corpus, children's utterances were excluded."
"one potential formal explanation for the relation between discourse salience and speakers' choices of referring expressions is the uniform information density hypothesis (uid) [cit] . uid states that speakers prefer to smooth the information density distribution of their utterances over time to achieve optimal communication. this theory predicts that speakers should use pronouns instead of longer forms (e.g., the president) when a referent is predictable in the context, whereas they should use longer forms for unpredictable referents that carry more information [cit] ."
"our experiments are designed to quantify the contributions of the various components of the complete model described in section 3.2 that incorporates discourse salience, cost, and unseen referents. we contrast the complete model with three impoverished models that lack precisely one of these components. the comparison model without discourse uses a uniform discourse salience distribution. the model without cost uses constant speech cost. the model without good estimates of unseen referents always assigns probability"
"discourse theories have characterized the link between referring expressions and discourse salience by stipulating constructs such as a scale of topicality (givón, 1983), accessibility hierarchy [cit], or implicational hierarchy [cit] . all of these assume fixed form-salience correspondences in that a certain referring expression encodes a certain degree of salience. however, it is not clear how this form-salience mapping holds nor why it should be."
"in our simulations, the speaker's cost function c w is estimated based on word length as in (9). we assume that longer words are costly to produce."
"thus, we propose a novel idea in terms of system deployment and analysis of the acquired data. we determine a typical patient's body postures through wireless sensing. a fine-grained phy layer csi based on the leaky coaxial cable provides the signal for the analysis. in this case the patient's body is presented as an obstacle for radio signals that in turn generate a unique signature. the channel state information is measured using new csi 802.11n wlan nics."
"protecting the cable from dust, a dielectric outer sheath is used to cover the slots. in the radiating mode, energy is radial and slots behave like an array antenna of length l. in case of a leaky electromagnetic signal, reception and transmission is all along the cable. this overcomes the disadvantage of the existence of blind zone in traditional communication [cit] ."
"our model was run on both adult-directed speech and child-directed speech. [cit] task 1 subset of ontonotes [cit], a corpus of news text, as our corpus of adult-directed speech. [cit] subset of childes [cit] was chosen as our corpus of child-directed speech."
"the water depth, velocities in x and y directions, gravity acceleration, bottom elevation, and manning coefficient are denoted by h, u, v, g, z b, and n m, respectively. using a cell-centered finite volume (ccfv) method, equation (1) is spatially discretized over a domain ω as"
"f n x + g n y i ∆l i in arrays for each cell-center; only the values of f n x + g n y i ∆l i are saved corresponding to the total number of edges, instead. the values of an edge are only valid for one adjacent cell-and such values are simply multiplied by (−1) for another cell. it is now a challenging task to design an array structure that can ease vectorization and exploit memory access alignment in both the edge-driven and cell-driven levels."
"a pancreatic tumour was derived from orthotopic injected kpc cells in a c57bl/6 mouse [cit], and cant tumours were subcutaneously implanted [cit] in 3 cba mice, as part of separate studies."
"we have shown in the previous sections that the hllc, roe, and cu schemes are quite accurate for simulating the test cases, where only non-significant differences are shown between them. in this section we analyze and compare the performance of each scheme. all schemes were written and compiled in the same code nufsaw2d on three machines-avx (intel xeon e5-2690/ sandy-bridge-e), avx2 (intel xeon e5-2697 v3/haswell), and avx-512 (intel xeon phi/knights landing)-for a linux operating system using intel fortran 19. the first computing resource \"sandstorm\" was available at our chair [cit] and the last two resources \"coolmuc-2\" and \"coolmuc-3\" were provided by the leibniz supercomputing centre (lrz) [cit] . each node of the avx, avx2, and avx-512 machines has a total of eight physical cores (16 logical cores), 14 physical cores (28 logical cores), and 64 physical cores, respectively. note that avx-512 is built on many-core architecture that incorporates cores with low-frequency and small memory. therefore, in order to achieve a notable performance, this machine relies on the vector operations on 512 bit simd registers."
a user defined period of 150 ms determined the amount of slice data acquired immediately preceding the detection of each breath that was set to be reacquired. combining the reacquisition with extension of the respiration gating control signal beyond each snatched breath results in a period of about 500 ms duration about each breath that is not available for data acquisition.
"to solve the time-dependent swes, all the aforementioned schemes must be temporally discretized either by using an implicit or an explicit time stepping method. despite its simplicity, the latter may, however, suffer from a stability computational issue particularly when simulating a very low water on a very rough bed [cit] . the former is unconditionally stable and even is very flexible to use a large time step. however, the computation is admittedly complex. another way that can be used to overcome the stability issue of the explicit method and to avoid the complexity of the implicit method-is to perform a high-order explicit method, such as the runge-kutta high-order scheme. this method is more stable than the explicit method, while the computation remains simple and acceptably cheap as that of the explicit method."
"it has previously been reported that only small variations in physiological cycles result in sufficient asynchrony such that motion insensitive data can be sampled efficiently [cit] . this is demonstrated by simulation for a normal distribution in supplementary material. in practice, the respiration rate typically drifts and scans complete according to the time that is available for respiration insensitive scanning. spreading the standard ungated scan time over the inter-breath periods that are available for respiration insensitive scanning defines an extended scan time. analysis of real respiration traces suggests that at least 97% of splicer scans will complete within a 10% overhead of this extended scan time."
"the proposed prospective gating acquisition scheme enables efficient multi-slice scanning in small animals at the optimum tr with reduced sensitivity to respiratory motion. the method has been implemented and demonstrated in multi-echo and magnetisation prepared spin-warp scans, and is compatible with a wide range of complementary methods including non-cartesian scan modes and reduced data acquisition techniques. in particular, the proposed scheme reduces the need for continual close monitoring to effect operator intervention in response to respiratory rate changes, which is both difficult to maintain and precludes high throughput."
"for quantification of mri metrics in general it is important to minimise scan time as precision is improved when longer term physiological instabilities are minimised. we have previously observed, albeit in a 3d scan mode, that measurement of t 1 in a number of organs have improved precision when acquiring data in a shorter scan time whilst minimising the effects of respiration motion [cit] . for 2d multislice scanning the splicer acquisition mode is the most efficient way to minimise the effects of respiration motion for quantitation."
"the evolutions of the simulated free surface elevation using the cu scheme are visualized in figure 5 . suddenly after 0.1 s, water started to move in all directions. at 0.4 s, the circular shock wave propagated outwards, whereas the circular rarefaction wave traveled inwards showing that this wave almost reaches the center of the domain. this phenomenon continued until the rarefaction wave has fully plunged into the center of the domain at approximately 0.8 s and this wave was suddenly reflected creating a sharp gradient of water surface elevation. at 1.6 s, the circular shock wave propagated further outwards the from domain center, whereas the reflected rarefaction wave now caused the water to fall below the initial depth of 0.5 m. this produced a secondary circular shock wave, the depth of which was slightly less than 0.5 m. the primary circular shock wave kept propagating outwards the center of the domain at 3.8 s and interestingly, the secondary circular shock wave that had recently been created traveled towards that center. at 4.7 s, it is shown that the primary circular wave almost reached the domain boundary and at this time a very sharp gradient of water surface elevation had been created near that boundary."
"a numerical investigation for studying the accuracy and efficiency of three common shallow water solvers (the hllc, roe, and cu schemes) has been presented. four cases dealing with shock waves and wet-dry phenomenon were selected. all schemes were provided in an in-house code nufsaw2d, the model of which was of second-order accurate in space wherever the regimes were smooth and robust when dealing with strong shock waves-and of fourth-order accurate in time. to give a fair comparison, all source terms of the 2d swes were treated similarly for all schemes, namely the bed-slope terms were computed separately from the convective fluxes using a riemann-solver-free scheme-and the friction terms were computed semi-implicitly within the framework of the rkfo method."
"showing that a symmetric boundary condition is applied to cell 8 in y direction. considering the fact that the total number of internal cells is significantly larger than that of boundary cells, we group the internal cells into a single loop and distinguish them from the boundary cells, see algorithm 2."
to determine exactly which data should be used in image reconstruction each acquired data trace was timestamped (bruker) or the projection counter index of each slice was passed to the parallel port of the spectrometer host computer for storage (varian).
"dam-break or tsunami flows cause not only potential dangers to human life, but also great losses of property. these phenomena can be triggered by some natural hazards, such as earthquakes or heavy rainfall. when a dam breaks, a large amount of water is released instantaneously from the dam and will propagate rapidly to the downstream area. similarly, tsunami waves flowing rapidly from the ocean bring a large volume of water to coastal areas, which endangers human life as well as damages infrastructure. since natural hazards have very complex characteristics, in terms of the spatial and temporal scales, they are quite difficult to predict precisely. therefore, it is highly important to study the evolution of such flows as a part of a disaster management, which will be useful for the related stakeholders in decision-making. such study can be done by developing a mathematical model based on the 2d shallow water equations (swes)."
"typically, there are three classes of vectorization: auto vectorization, guided vectorization, and low-level vectorization. the first type is the easiest one utilizing the ability of the compiler to automatically detect loops, which have a potential to be vectorized. this can be done at compiling time, e.g., using the optimization flag -o2 or higher. however, some typical problems, e.g., non-contiguous memory access and data-dependency, make vectorization difficult. for this, the second type may be a solution utilizing some compiler hints/pragmas and array notations. this type may successfully vectorize the loops that cannot be auto-vectorized by the compiler. however, if not used carefully, it gives no significant performance or even the results can be wrong. the last type is probably the hardest one since it requires deep-knowledge about intrinsics/assembly programming and vector classes, thus not so popular."
"we now discuss the subroutine edge-driven_level and sketch it in algorithm 3. note for the sake of brevity, only the pseudo-code for internal edges is represented in algorithm 3; for boundary edges, the pseudo-code is similar but computed without muscl_method. the first loop corresponds to the edges 1-16 and the second one to the edges 17-31. in the first loop (lines 1-7), each flux computation accesses the array with the farthest alignment of seg_y, whereas the arrays are designed in the second loop (lines [cit] to have contiguous patterns. every edge has a certain pattern for its two corresponding cells, where no data-dependency exists, thus enabling an efficient vectorization. note with this pattern, both loops can be auto-vectorized; however, we still implement a guided vectorization as it gives a better performance."
"mri was performed on a 4.7 t 310 mm horizontal bore biospec avance iii hd preclinical imaging system equipped with 114 mm bore gradient insert (bruker biospin gmbh, germany) or a 7.0 t 210 mm horizontal bore vnmrs preclinical imaging system equipped with 120 mm bore gradient insert (varian inc., ca). rf transmission and reception was performed with a 45 mm long 32 mm id quadrature birdcage coil (rapid biomedical gmbh, germany). the splicer scheme was implemented in rare and magnetisation transfer prepared gradient echo scan modes with evaluation of the respiration gating signal preceding the acquisition or dummy scanning of each slice."
", where m and n are scalar. this, however, leads to a data-dependency problem that makes vectorization difficult. to avoid these problems, we have designed a cell-edge reordering strategy, see figure 3, where the loops with similar computational procedures are collected to be vectorized. note that this strategy is only applied once at the pre-processing stage in figure 1 . the core idea of this strategy is to build contiguous array patterns between edges and cells for the edge-driven level as well as between cells and edges for the cell-driven level. we point out here that we only employ 1d array configuration in nufsaw2d, so that the memory access patterns are straightforward, thus easing unit stride and conserving cache entries. the first step is to devise the cell numbering following the z-pattern, which is intended for the cell-driven level. secondly, we design the edge numbering for the edge-driven level by classifying the edges into two types: internal and boundary edges in the most contiguous way; the former is the edges that have two neighboring cells (e.g., edges 1-31), whereas the latter is the edges with only one corresponding cell (e.g., edges 32-49). the reason for this classification is the computational complexity between the internal and boundary edges differs from each other, e.g., (1) no reconstruction process is required for the latter, thus having less cpu time than the former-and (2) due to corresponding to two neighboring cells, the former accesses more memories than does the latter; declaring all edges only in one single loop-group therefore deteriorates the memory access patterns, thus decreasing the performance. cell-edge reordering strategy [cit] and an example of memory access patterns."
"providing it coincides with an inter-breath period. each slice advances through its own projection/encoding loop whenever suitable data are acquired during an inter-breath period, as determined by the spontaneous respiration rate, and the scan only completes when all of the slices have acquired the specified number of projections/encodings."
"the assessment of temporal stability, rather than single frame examination, provides an unequivocal demonstration of the signal intensity stabilisation and increased image fidelity of the proposed method. the data sets are publically available at https://doi.org/10. 5287/bodleian:pvrrye7kk in nifti-1.1 format (http://nifti.nimh.nih. gov/) and can be viewed with imagej (https://imagej.nih.gov/ij/). it is particularly instructive to inspect the stability of data by selecting a slice and scrolling through the time course. it can readily be seen that the single slice data presented in fig. 2 are entirely representative. fig. 4 shows 12 contiguous slices from an orthotopic pancreatic tumour bearing mouse from 3 repeats of splicer rare scan acquired in 2.5 min. the data of all slices are devoid of motion artefact and exhibit good tumour delineation which can be used to plan mr guided radiotherapy treatment [cit] . in this application it is particularly crucial to minimise scan time to ensure that long term animal motions such as peristalsis, bladder filling and body droop do not lead to significant deformation of the body in between the start of imaging and the delivery of radiotherapy. it is also particularly important to minimise image artefacts since they can compromise image registration and result in misdirected treatments."
"as the high-order time stepping method is now considered, the selection of solvers included in models must be taken into careful consideration, since such solvers-which are the most expensive part in swes simulations-need to be computed several times in a single time step. for example, the runge-kutta fourth-order (rkfo) method requires the updating of a solver four times to determine the value at the subsequent time step. the more complex the algorithm of a solver is, the more cpu time one obtains."
"recent numerical models of the 2d swes rely, almost entirely, on the computations of (approximate) riemann solvers, particularly in the applications of the high-resolution godunov-type methods. the simplicity, robustness, and built-in conservation properties of the riemann solvers, such as the roe and hllc schemes, had led to many successful applications in shallow flow simulations, see [cit], among others. highly discontinuous flows, including transcritical flows, shock waves and moving wet-dry fronts were accurately simulated."
"anaesthesia was induced and maintained using isoflurane (1-4%) in room air supplemented with oxygen (80%/20% v/v) for mri. rectal temperature was monitored and maintained at 36°c with an optical system (acs-p4-n-62sc and otp-m, opsens inc., quebec, canada) that provided feedback to a twisted pair resistive heating system developed for mr compatible homeothermic maintenance [cit] . respiration was monitored and maintained at 40-60 breaths/min using a pneumatic balloon (vx010, viomedex ltd, uk) positioned against the animal's chest and coupled to a pressure transducer. the respiration signal was passed to a custom-built gating device to generate a threshold based respiration gating control signal with additional duration of about 150 ms set to last until after the completion of each breath to reduce the sensitivity to respiratory motion."
"we have also shown that the edge-driven level, especially the reconstruction technique and solver computations, were the most time-consuming part, which required 65-75% of the entire simulation time. this shows that some more \"aggressive\" optimization techniques still become a hot topic for future studies to make shallow water simulations more efficient, particularly in the edge-driven level. finally, we conclude that this study would be useful as a consideration for modelers who are interested in developing shallow water codes."
"mri scanning in the abdomen and thorax of small animals is compromised by the effects of respiration motion. an isoflurane anaesthetized (1-3% in air and/or o 2 ) normal healthy mouse takes snatched breaths of about 200 ms duration with a significantly depressed and often variable respiration rate, typically 40-80 breaths/min depending on the depth and duration of anaesthesia [cit] . prospective gating methods incorporating the automatic reacquisition of respiratory motion corrupted data have enabled highly efficient motion desensitised 3d scanning at short and constant tr in the mouse [cit] . the methods adaptively track spontaneous changes in the respiration rate to maximise acquisition during all inter-breath intervals when respiration motion is minimal, and have been shown work very well in spoiled gradient echo and balanced ssfp scan modes. these methods, however, are not able to replicate the same level of t 2 contrast offered by the multi-slice rare scan mode, which is well known to be particularly useful for tumour visualization. this paper reports a conceptually simple yet effective prospective gating acquisition scheme for efficient multi-slice scanning in free breathing small animals at any fixed tr of choice with reduced sensitivity to respiratory motion."
"we did not use the vectorization directive provided by intel, e.g., !dir$ simd, since we have experienced that this directive was not always able to vectorize the loop. instead, we implemented the directive !$omp simd simdlen(vl) aligned(var1,var2,... :nbyte) provided by the openmp 4.0. the first component (simdlen) was aimed to test the benefit of vectorization on our code compared to the theoretical speed-up based on the vector width, while the second one (aligned) was employed to know the benefit of the aligned memory accesses supported by the reordering strategy proposed. since we would like to emphasize the effect of vector width, we restricted our discussion here to single-precision arithmetic. the variable vl was the vector length, set to eight for avx/avx2 and 16 for avx-512-and nbyte was the default alignment of the architecture, set to 32 for avx/avx2 and 64 for avx-512."
"conventional multi-slice mri is characterised by having a slice index counter that cycles more quickly than the projection or phase encode index counter. by assigning each slice its own specific projection or phase encode loop index counter it is possible to scan the slices continually at the optimum tr. when a breath is registered rf pulses continue to be applied but data are not acquired, and the corresponding counters remain fixed so that the data are acquired one tr later, fig. 1 . diagrammatic representation of respiration gated scan modes for 8 example slices. a threshold is set on the analogue respiration signal (resp) to generate the r-logic control signal. a user-variable post breath delay (τ) is used to ensure that motion artefact is minimised from the trailing portion of each breath. a, b: splicer scan modes set for two example trs. the r-logic control signal is evaluated in real time. if the signal registers a breath, rf continues to be applied but data are not acquired, as indicated by the absence of a projection/phase encoding (pe) step. each breath is already in progress at the time of detection using threshold breath detection. in the diagram a single respiration corrupted pe step (marked in grey) is reacquired the next time the slice falls favourably within an inter-breath period. c, d, e: conventional modes where tr is equal to a single instantaneous breath interval (c), the sum of two successive instantaneous breath intervals (d), and the sum of n successive instantaneous breath intervals selected to allow full relaxation (e). in the conventional modes pulse sequences are suspended following acquisition of a predefined block of slices until triggered by a low to high switch of the r-logic signal."
"note that the edge-driven level is the most expensive stage among the others; one should thus pay extra attention to its computation. we also point out here that we apply the computation for the edge-driven level in an edge-based manner rather than in a cell-based one, namely we compute the edge values only once per single calculation level. therefore, one does not need to save the values of"
"alternative acquisition strategies such as radial, spiral and, in the case of the rare scan mode, propeller, are able to reduce the effect of physiological motions by oversampling the centre of k-space in a manner that essentially performs low spatial frequency signal averaging, albeit at the expense of scan time [cit] . the reduced sensitivity of propeller to respiration motion has enabled the adc of mouse liver to be determined with an approximately three-fold reduction in standard deviation when compared to both ungated and respiration triggered rare scan modes [cit] . although a good degree of motion correction is possible, the destructive interference between echoes of the rare cpmg echo train that is caused by a loss of phase coherence cannot be recovered. in general, the effect of motion on pure frequency encoding methods is to spread the artefact energy in all directions, which is considered to be less detrimental to image quality than the phase encode ghosting exhibited by conventional cartesian spin-warp methods. the prospective gating multi-slice method presented here is compatible with all of these acquisition schemes, and follows the guiding principle that the best way to remove artefacts is to avoid them altogether [cit] ."
"we present the comparison between the analytical and numerical results at 4.7 s in figure 6 showing that all schemes can simulate this highly discontinuous flow properly. to point out the difference between the three schemes more clearly, we present in figure 7 both the depth and velocity profiles near the two discontinuous areas: 20-22 m and 38-40 m, where only non-significant differences are shown."
"author contributions: b.m.g. developed the numerical code nufsaw2d, conceived the framework of this work, analyzed the results, and wrote the paper. r.-p.m. contributed to the practical guidance of this work and provided some corrections, especially regarding the theoretical concepts of the vectorization and parallel computing."
"it must be noted, however, that through slice motion during the breath provides an opportunity for inadvertent excitation of the wrong slices. it is not straightforward to formally assess the extent to which this will confound quantitation as it will depend on many factors including slice position, slice profile, slice thickness, and depth of anaesthesia. for parts of the body where significant through slice axis motion does occur, e.g. the thorax and liver, it is possible to suspend slice selective rf excitation during the breath whilst maintaining delivery of magnetisation preparation pulses as necessary [cit] . this requires scans to be operated in a near fully relaxed mode which results [cit] in long scan times, but the real-time and adaptive tracking of the respiratory interval by the proposed method ensures that the scan efficiency is always maintained. nevertheless, if possible, we would always choose to avoid the complications of through slice motion by deploying a short tr 3d method. the proposed method naturally enables global steady-state magnetisation preparation schemes that are applied in the absence of slice selection, such as mtc and cest, to be maintained through periods of bulk motion. fig. 5 shows that good image quality is produced when using gradient echo splicer in conjunction with a steady-state mtc preparation scheme which shows darkening of the muscle and cant tumours relative to images acquired without mtc preparation. the cant tumour in the third column shows mtc delineating regions where it is suspected that necrosis has occurred and fluid has filled the resulting cavities. other preparation schemes are, of course, more forgiving and do not need to be applied with such fixed regularity. for example, fat saturation only requires the fat magnetisation to be saturated immediately prior to data acquisition."
"finally, we sketch the subroutine cell-driven_level in algorithm 4. again, for the sake of brevity only the pseudo-code for internal cells is given. similar to the internal cell in the subroutine gradient, the loop is designed sequentially with a factor of seg_x-2 for the outer part. in the inner part the arrays access patterns are, however, different to those of the gradient computation, where w accesses f, g, and s b from the corresponding edges-and s f from the corresponding cell; in other words, more array accesses are required in this loop. nevertheless, the vectorization gives a significant performance improvement since the array accesses patterns are contiguous. however, there is a part that cannot be vectorized in this cell-driven level due to non-unit strided access, similar to that shown in algorithm 2. again, since the dimension of this non-vectorizable loop is considerably smaller than the others, there is no significant performance alleviation for the entire simulation."
"we focus our reordering strategy here on tackling the two common problems for vectorization: non-contiguous memory access and data-dependency. regarding the former, a contiguous array structure is required to provide contiguous memory access giving an efficient vectorization. typically, one finds this problem when dealing with an indirect array indexing, e.g., using x(y(i)) forces the compiler to decode y(i) for finding the memory reference of x. this is also a typical problem for a non-unit strided access to array, e.g., incrementing a loop by a scalar factor, where non-consecutive memory locations must be accessed in the loop. the vectorization is sometimes still possible for this problem type. however, the performance gain is often not significant. the second problem relates to usage of arrays identical to the previous iteration of the loop, which often destroy any possibility for vectorization, otherwise a special directive should be used."
"each breath is already in progress at the time of detection using threshold breath detection. the approach is refined to reacquire the slice data acquired during a user-defined time period before each breath is detected as this has been shown to dramatically improve image stability [cit] . for convenience the acquisition scheme is referred to as splicer (slice projection loop index counter enabled reacquisition) [cit], and only the data with reduced sensitivity to respiratory motion are spliced together for image reconstruction. a diagrammatic representation of the splicer scan mode is shown for two different trs in fig. 1a and b and the scan modes that have previously been used to prospectively synchronise data acquisition with inter-breath periods are shown in fig. 1c, d and e."
"the scanner architecture of both systems necessitates that the exact amount of data to be acquired is specified at the start of a scan. traditionally, for a full 2d multi-slice data set, this corresponds to the acquisition of np complex data points for each projection multiplied by ns slices multiplied by nproj projections. increasing the projection loop size by a factor of four easily ensures that scans complete naturally with a full set of acceptable data and do not terminate prematurely."
"here we explain in detail how the data structures of our code are designed to advance the solutions of equation (6) . note this is a typical data structure used in many shallow water codes (with implementations of modern finite volume schemes). as shown in figure 1, a domain is discretized into several sub-domains (rectangular cells). we call this step the pre-processing stage. each cell now consists of the values of z b and n m located at its center. initially, the values of h, u, and v are given by users at each cell-center. as our model employs a reconstruction process to spatially achieve second-order accuracy with the muscl method, it requires the gradient values at cell-center. therefore, these gradient values must firstly be computed. this step is called the gradient level. hereafter, one requires to calculate the values at each edge using the values of its two corresponding cell-centers. this stage is then called the edge-driven level. in this level, a solver, e.g., hllc, roe, or cu scheme, is required to compute the non-linear values of f and g at edges. prior to performing such a solver, the aforementioned reconstruction process with the muscl method was employed. note the values of s b are also computed at the edge-driven level."
"nowadays, performing swe simulations is becoming more and more common on modern hardware/cpus towards high-performance computing (hpc) using advanced features such as avx, avx2, and avx-512, which support the algorithm vectorization for executing some operations in a single instruction-known as single instruction multiple data (simd)-so that a significant computation speed-up can be achieved. vectorization on such modern hardware employs vector instructions, which can dramatically outperform scalar instructions, thus being quite important for having more efficient computations. among the other compilers' optimizations, vectorization can even be regarded as the common ways for utilizing vector-level parallelism, see [cit] . such a speed-up, however, can only exist if the algorithm formulation is suitable for vectorization instructions either automatically (by compilers) or manually (by users) [cit] ."
"after the values of all edges are known, the solution can be advanced for the subsequent time level by also computing the values of s f . for example, the solutions of w at the subsequent time level for a cell-center are updated using the f, g, and s b values from its four corresponding edges-and using s f values located at the cell-center itself. we call this stage the cell-driven level."
"g. patient scenario objectives. two patient scenarios were developed in cooperation with domain experts (a physician and nurse). the purpose of the patient scenarios was (1) to provide a realistic clinical situation, (2) to employ the physicians' professional experience and practices, (3) to reduce the scope of the dialog in the consultation, (4) to reduce variations in the outcome and (5) to make sure the physicians had at least one interaction with the pda or paper chart during the visit, triggered by the patients' complaints and concerns. one of the scenarios is provided below:"
"on the other hand, many physicians commented that handwriting made it easier to make mistakes, harder to correct them, and more difficult to interpret the information. they also commented on the greater risk of medication errors and interactions, or that they had to take heavy reference books with them. as one physician said; \"( ... ) its more that you don't get the same quality of information. it is hard to write medications, they have complex names. it could be nice to have some support\". in addition, the information registered in the paper chart cannot be reused by other systems, so it has to be registered twice. as one physician said; \"quick and easy to use, but more work afterwards.\" they also had to remember things such as medication name and dosage, rather than selecting from a list."
"for this experiment we utilized a usability laboratory where we recreated the hospital environment and evaluated the interaction techniques in a \"hospital ward simulator\" with real physicians who played out realistic ward round scenarios. the general setup was a simulated ward round where a physician made changes in the medication of patient impersonators using four different information systems; three pda-based medication systems and one paper based medical chart."
"the usage of the stylus interaction technique mostly revealed ergonomic usability problems. a few physicians had problems locating the stylus in the pda. others had problems taking it out of the pda and some physicians fiddled with the stylus during the patient conversation. we also observed a few incidents where the stylus \"got lost\" in the lab coat pocket because the user placed the handheld device and the stylus in the pocket separately. b) interview: some physicians experienced the stylus interface as faster and more accurate than finger and button. one physician pointed out that he was \"not as distracted from the patient with this one as for the other two [finger and button]\". however, many physicians considered the stylus as an extra thing to be dependent on, and to get distracted of. they were also worried about the stylus being lost or misplaced. as one of them said: \"physicians are very good at misplacing things and put things in their pockets, so the stylus will be lost the first day. your finger will probably not.\" he also pointed out that they \"will need a big box of styluses\". some also commented that it was awkward to get the stylus in and out of the pda every time they used it. c) card ranking: despite the problems, stylus was ranked as most preferred by the physicians. a total of 6 ranked it highest. none ranked it as the least preferred. however, there were only significant differences with paper."
"in this comparative evaluation study we have used simulated ward rounds with physicians using a paper version and three pda versions of a medication prescription system using stylus, finger and button interaction."
"the future work is to achieve the knowledge produced during the former multi-satellite cooperative task planning and to transfer them to the following task planning. currently, our tl method just focuses on the action transference to accelerate the learning speed for emerging task using historical task planning results. however, the transfer learning is not only to simulate the action, but also to discover the knowledge and transfer them for the incoming cooperative planning."
"while established guidelines and principles exist for the specification of guis [cit], few such guidelines exist when it comes to choosing and specifying the interaction techniques used by the system. such decisions are mainly based on quantitative measures [cit] . however, in healthcare there are other factors, social and physical ones, which are equally important for the overall usability of a mobile information system [cit] . take a ward round as an example. here, factors such as the ability of the doctor to attend the patient rather than the user interface [cit], the ability of the patient to be aware of the doctor's actions [cit], and the ability of the doctor to communicate non-verbally with the device [cit], are important to allow \"sensitive or full patient disclosure\" [cit] . whether one interaction technique is 10% faster than another is of less importance. therefore, we have in this study mainly focused on qualitative measures, such as the doctors' and patients' preferences and concerns about the interaction techniques."
"while most usability evaluation studies have covered a single system, several examples of comparative usability evaluations of multiple design alternatives are found in literature. traditionally, when performing comparative studies of interaction techniques, time completion rate, error rate and other quantitative measurements are used to decide which design to move forward with. one of the earliest attempts in the hci-field was conducted during the development of the star user interface [cit] 's, where several design concepts were developed using comparative evaluations. a standard for something as trivial as selecting, copying and pasting text using a mouse was not yet established, so the star team designed several different solutions for text selection and exposed them to users. the team measured the completion times of each solution, refined the \"winner\", and used it in new design iterations."
"some of the emerged issues with the different input techniques could be attributed to lacking experience with pdas and the medication system and could be expected to vanish with increasing usage experience. but we think it is of major importance to realize the significant role of input techniques when designing any mobile information system for point-of-care usage and to include the selection of input techniques as an essential part of the interaction design process. given the large variation in users' preferences it often seems advisable to provide the system with multiple input techniques and leave the final choice to the user to accommodate their individual needs. however, this requires the user interface to be tailored to accommodate all the interaction techniques, which may require suboptimal and costly solutions."
"however, since traditional ne approaches [cit] have determined the structure of neural network in advance, including number of hidden nodes and the connections, the weight of neural network is just opti-mized through ne algorithm. and what is more, the topology of neural network has great influence on the optimality of the result."
"qualitative survey on usefulness. to evaluate the usefulness of counterfactual (action-oriented) explanations, we conducted a survey with amazon mechanical turk (amt) master workers (www. mturk.com/help#what_are_masters). in this survey, we showed 500 workers three recommendation items (\"series camelot\", \"pregnancy guide book\", \"nike backpack\") and two different explanations for each. one explanation was limited to only the user's own actions method explanation for \"baby stroller\" with category table 4 : anecdotal examples of explanations by prince and the counterfactual baselines."
"in the preliminary briefing all the test subjects were encouraged to communicate and interact with the patient impersonators during the test, as they would have done with actual patients in real clinical situations."
"for that, according to the idea of neuroevolution of augmenting topologies (neat), the algorithm of maconeat is proposed, combining the multi-satellite cooperative task planning problem. in maco-neat, the topological structure and the weight of policy individual of neural network evolve at the same time in the populations of each satellite agent. avoiding the task observation redundancy of multiple satellites, the joint fitness is calculated among agents, which improves the global planning effectiveness."
"similar studies have been performed within the health care domain. one study made a direct comparison of tablet pcs and pdas for point-of-care documentation using focus groups and usability walk-troughs with end-users [cit] . they found that the amount and complexity of the data to be entered were important factors for the physicians' preference. tablet computers were better suited for complex high volume data entry, while pdas were preferred for simple low volume data entry."
"the tests were conducted in a usability laboratory for testing medical systems at a research center connected to the regional hospital area. the laboratory is a full-scale model of a section of a hospital ward with two patient rooms, one office for health workers and one hallway connecting the rooms. only one of the patient rooms was used in the experiment. adjacent to the lab area is a control room with recording and editing equipment."
"because they already had access to medication names and dosages, the chances of making mistakes were reduced. in addition, mistakes were easier to correct. one of the physicians said; \"from a quality perspective i would prefer the pda, because i tend to misspell things. that makes me look up things, but [the pda] does this automatically.\""
"the patients also perceived physicians as being more confident and comfortable when using the paper chart, something that made them trust the physicians more. this was visible in the observation data through more eye contact and a livelier conversation while writing in the chart. on the other hand, the patient experienced the pda as less invasive because it took less space between the patient and physician."
"(2) cooperative environment information is partially visible to the agent. each agent only has limited resources and capability, and the agent could only make the decision in accordance with its own state and the partial environment information."
"a typical data point looks like a row in table 6, that shows representative examples (goodreads shown only for completeness). we divided the samples into ten hits (human intelligence tasks, a unit of job on amt) with 20 data points in each hit. for each data point, we showed a recommendation item and its explanation, and asked users about the usefulness of the explanation on a scale of 1 − 3 (\"not useful at all\", \"partially useful\", and \"completely useful\"). for this, workers had to imagine that they were a user of an e-commerce platform who received the recommendations as result of doing some actions on the platform. only amt master workers were allowed to provide assessments."
"for the stochastic incoming observation requests, tl takes full advantages of historical cooperative planning information, which not only guarantees the cooperative planning quality but also accelerates the learning speed. examples verify that the proposed algorithm in this paper is more suitable for the dynamic randomly occurring observation requests."
"in maconeat, mutation consists of two mutation operators, which are node connection weight mutation operator and network structure mutation operator. the mutation operator of connection weight is the same as other ne algorithms, which is to adjust the weight of each connection gene with the probability of m c . structural mutation operators achieve the changes of structure of chromosome group by two ways of connection mutation and node mutation. the connection mutation operators establish connection by stochastic weight between unconnected nodes in the existent neural network (see fig. 3(a) ); node mutation is operated by adding the node n new to the current connection, cutting off existent connection and generating two new connections, and setting n new as the connection weight of input node to 1 while n new as the output node consistent with original connection (see fig. 3(b) )."
"however, such methods come with critical privacy concerns arising from nodes in paths that disclose other users' actions or interests to user u, like the purchase of user v above. even if user v's id was anonymized, user u would know whom she is following and could often guess who user v actually is, that bought item j, assuming that u has a relatively small set of followees [cit] . if entire paths containing other users are suppressed instead, then such explanations would no longer be faithful to the true cause. another family of path-based methods [cit] presents plausible connections between users and items as justifications. however, this is merely post-hoc rationalization, and not actual causality."
"the pda's screen content was wirelessly mirrored and mixed real-time together with video streams from four cameras and audio streams from three wireless microphones. three of the cameras were mounted in the ceiling and were remotely controllable from the control room. the fourth camera, a non-intrusive mini camera, was mounted on the bedside, giving a video stream from the patient's perspective. all together the recorded video provided details about the overall care situation and the physician's interaction with the gui, with the physical device, and with the patient. fig. 2 shows the resulting video stream."
"based on ghi, the similarity of topological structure of individuals is calculated and the speciation is achieved, which is to avoid complex topological calculation between different individuals. thus, species is established. the compatibility distance between indi-viduals is defined by"
"the paper based interaction technique used in the simulations employed the paper chart as in use today (fig. 3, right) . domain experts, a senior physician and a nurse, were involved to create a realistic paper chart for the fictive patients used in the scenarios. however, it contained only the same information that was available on the pda."
"proof. assuming that each node has at least one outgoing edge, the ppr can be expressed as the sum over the probabilities of walks of length l starting at a node u [cit] :"
"where tsk rdn means the task set of repeated visits generated by the current satellite and other satellite, r j is the number of repeated observation of tsk j, r max is the maximal number of repeated observation, u max and u min respectively mean the maximal utility and minimal utility of tasks, c s is the coefficient of stimulation."
"next, we just need to prove that the individuals in the policy neural network subpopulations could converge to the optimal. lemma 1 [cit] corresponding to general genetic algorithm with the finite homogeneous markov chains, if the population keeps the best solution before selection operation in each generation, the genetic algorithm converges to the optimal with probability 1."
"the high diversity of categories in the amazon data, ranging from household equipment to food and toys, allows scope to examine the interplay of cross-category information within explanations. the key reason for additionally choosing goodreads is to include the effect of social connections (absent in the amazon data). the datasets were converted to graphs with \"users\", \"items\", \"categories\", and \"reviews\" as nodes, and \"rated\" (user-item), \"reviewed\" (useritem), \"has-review\" (item-review), \"belongs-to\" (item-category) and \"follows\" (user-user) as edges. in goodreads, there is an additional node type \"author\" and an edge type \"has-author\" (item-author). all the edges, except the ones with type \"follows\", are bidirectional. only ratings with value higher than three were considered, as lowrated items should not influence further recommendations."
the patient is a 44-year-old woman hospitalized with an acute episode of crohn's disease since two days. the treatment was started right after admission of the patient.
"the comparison of the different input techniques reveals that physicians do have strong opinions about the input techniques they use and that these opinions vary widely between individuals. this is very clearly demonstrated by the fact that even the least preferred input technique is still most preferred by more than one fifth of the participants. given the very simple usage pattern in the tests, it is even imaginable, that user preference ranking might be reversed for different usage patterns, e.g. scrolling long lists or multi-selection. moreover, new technologies, such as touch screens with better precision and tactile feedback [cit], together with increased popularity, will make the views on mobile devices and appropriate interaction techniques evolve quickly."
"several liked the physical attributes of the handheld device; it was easy to carry and it fit in their pockets. on the other hand, the small screen gave a limited view of information compared to paper. this made it hard to get a good overview, especially for heavily medicated patients with a complex medical history."
"where α denotes the teleportation probability (probability of jumping back to u) and δ is the kronecker delta function. the only required modification, with regard to recw alk [cit], is the transformation of the transition probability matrix from w to w β . for simplicity, we will refer to the adjusted probability matrix as w ."
"we asked the workers three questions: (i) which method do you find more useful?, where 70% chose the action-oriented method; (ii) how do you feel about being exposed through explanations to others?, where ≃ 75% expressed a privacy concern either through complete disapproval or through a demand for anonymization; (iii) personally, which type of explanation matters to you more: \"action-oriented\" or \"connection-oriented\"?, where 61.2% of the workers chose the actionoriented explanations. we described action-oriented explanations as those allowing users to control their recommendation, while connection-oriented ones reveal connections between the user and item via other users and items."
we now describe experiments performed with graph-based recommenders built from real datasets to evaluate prince. amazon 2k 54k 58k 43 114k goodreads 1k 17k 20k 16 45k table 1 : properties of the amazon and goodreads samples.
"through k,h, the history learning policy is transferred to the initial target learning policy. all target learning policies inherit the topological structure and weigh information to accelerate the learning speed of new observation for multiple satellites. the specific process for tl algorithm of sat k policy is given in therefore, the detailed process of multiple satellites dynamic hybrid learning algorithm is shown in fig. 7 ."
"from the concept of nash optimal, it is seen that each decision maker in a distributed system can only decide its own decision variables. a rational decision maker should choose the policy without affecting other decision makers. because the decision results would affect each other, each decision maker could not change its decisions to get better performance of the system, then the system achieves equilibrium. for the msdtp, there is an implication in definition 1 that: each satellite optimizes his local policy under the assumption of the optimal of other satellites in the process of local optimization have been given. only in this way could the system get the nash optimal. it is known that the fitness of policy neural network individual in each satellite is calculated with the best individuals of other satellite subpopulations. therefore, as long as each satellite converges to its local optimal policy, the distributed system could converge to the nash optimal."
"as part of the preliminary briefing we explained that the motivation for the experiment was to \"test a user interface for a medication module for an electronic patient record system\". each participant was also given a guided presentation of the laboratory and the associated control room. they were also presented with the general tasks to carry out during the tests. since the purpose was not to find usability errors of the system as such, they were able to familiarize with the prototypes prior to the experiment."
"the patient focus group revealed that the patient actors did not notice any differences between the various pda based medication systems. however, they claimed to experience \"more embarrassing silence\" during the simulated ward rounds when the physicians used the pda. this silence prevented them from asking questions because they did not want to disturb the physician. they found it easier to ask questions when using the paper chart."
"three versions of the prototype were developed, each adapted for interaction with stylus, finger and device buttons. the differences between them were minor; the finger prototype had larger gui-buttons than the stylus prototype, and the button prototype had an indicator moving between gui-buttons as the user navigated through them with the hardware buttons (the touch screen was disabled)."
"the general concept of a paper-based medical-chart has been developed over centuries of medical practice. in the large regional university hospital in norway where this study was conducted, \"the chart\" (as it is widely named by its users) is a collection of important medical documents about the patient, gathered in a binder. it is used as documentation when the health worker is visiting the patient [cit] . the main document is an a4-sized form containing the most important information about the patient, such as blood pressure, body temperature, prescribed medication, etc. in addition the binder contains other documents such as recent test results or reports."
"the second part of the interview focused more on general and open questions about the physician's opinion on potential distraction by the information devices, suitability for real life usage and the patients' anticipated perception of pdas at the point-of-care."
"during the interview the physicians were asked to rank the interaction techniques devices in a card sort exercise, where they ordered the cards representing the interaction techniques by preference (ties between cards were not allowed). they were asked to state the reason for their final rank order."
"some younger physicians commented that the patient chart was large and good to hide behind and less disturbing than pda. in addition they liked that there was no chance of crashes or software malfunctions; \"the paper chart has its advantages; it works as long as you can write\". c) card ranking: paper was ranked significantly lower than the pda based interaction techniques. only one ranked it as most preferred while 9 ranked it as the least preferred."
"the user interface was designed using minimal attention user interface principles [cit], and the symbols and icons used were already well known from the paper chart or other applications (screenshots are found in fig. 3, left and middle). in the simulations, we used a fujitsu siemens power loox. this pda offers input with stylus, finger, and hardware · ."
"[goodreads] prince action 1: you rated highly \"blackest night\" with categories \"comics, fantasy, mystery, thriller\" action 2: you rated highly \"green lantern\" with categories \"comics, fantasy, children\" replacement item: \"true patriot: heroes of the great white north\" with categories \"comics, fiction, crime, fiction\""
"before genetic encoding, the first thing is to describe the task planning problem of each satellite in msdtp in the way of neural network. for the agent sat k in the msdtp, each neural network represents the task selection policy of sat k under the current task distribution. each input node in the neural network represents the segment of learning policy at time t; each output represents the task selection action of sat k . in order to make the subsequent operation of crossover and mutation, a neural network is firstly transferred into a chromosome. considering the characteristics of the policy neural network which describes the problem of msdtp, chromosome is generated by adopting the length-variable genetic encoding for policy neural network of agent. as illustrated in fig. 2, every single chromosome contains some connection genes, where each connection gene contains interconnected input and output node identity, connection weight, current state and global historical identity (the details are in section 4.3). the length of each chromosome is determined by input nodes, output nodes, hidden nodes it should be noted that all the neural network individuals of initial policy population in each satellite agent only consist of connection gene constructed by input and output nodes."
"earth observing satellites (eoss) receive the remote sensing information from the surface of earth sent by satellite borne sensor in space, which have advantages of no constraint on any country, long observation, wide coverage, etc. they are widely applied in environment monitoring, military reconnaissance and so forth. in the actual satellite observation process, a mass of observation requests occur randomly. how to support multiple satellites to fulfill complex and randomly occurring tasks cooperatively in the dynamic environment through the effective planning strategy is the current problem of multi-satellite task planning re-*corresponding author. tel.: +86-731-84574439. search to be solved urgently . [cit] to guarantee the calculation speed, the traditional satellite task planning algorithms [cit] for the dynamic environment, mostly adjust the existent plan based on heuristic rules. however, the heuristic strategy has greater dependence on the concrete tasks, so that the optimality of the results could not be guaranteed as it is restricted by the task. moreover, the distributed cooperative task planning for multiple satellites needs to respond to the dynamic environment quickly, but it does not pursuit one-sidedly the solving speed for the algorithm. balancing the calculation speed, optimization and adaption to the dynamic environment is the key to solve multiple satellites cooperative task planning."
"this work explored a new paradigm of action-based explanations in graph recommenders, with the goal of identifying minimum sets of user actions with the counterfactual property that their absence would change the top-ranked recommendation to a different item. in contrast to prior works on (largely path-based) recommender explanations, this approach offers two advantages: (i) explanations are concise, scrutable, and actionable, as they are minimal sets derived using a counterfactual setup over a user's own purchases, ratings and reviews; and (ii) explanations do not expose any information about other users, thus avoiding privacy breaches by design."
"the main argument in favor of the finger interaction technique was that the physicians were \"not dependent on a separate tool [the stylus] because you always have your finger with you\". many pointed out that the interaction with the pda is not a problem as long as the screen and buttons are large enough. when comparing finger and button many users liked that they could directly select their target without clicking through several steps to reach it. however, for many users finger felt inaccurate and some were worried about how much pressure to apply to the touch screen. in addition some users felt that their fingertip was too large for some user interface elements. they felt that the buttons was hidden under the finger and that they had to do an extra check before they pressed them. c) card ranking: finger was ranked as the second most preferred interaction technique. four physicians ranked this method highest and one ranked it lowest."
"another validity issue is that the medication system used in the simulated ward rounds did not compare to reality. the functionality was too sparse, and the user interface too simplistic. interestingly, only a very few physicians commented on this -the simple system offered exactly what they needed. the reason was that the scenarios and patient cases were carefully aligned to the functionality offered in the prototypes."
"our main research goal with this study has been to find out which interaction technique a pda-based point-of-care medication system should employ. to find the answer, we have evaluated commonly used basic interaction techniques and compared them with their current paper based counterpart in a series of ward round simulations with physicians and patient actors."
"the usability design lifecycle [cit] represents a similar process: explore the design space by developing several different design solutions, evaluate them, and use the best designs or design elements as a basis for a new or improved solution."
"(ii) shortest paths (sp): sp computes the shortest path from u to rec and deletes the first edge (u, n i ) on this path. this step is repeated on the modified graph, until the top-ranked rec changes to a different item."
"after debriefing the participants, they were asked to fill in a questionnaire on personal data, job details, and their previous experience with computer and pda usage. afterwards the physicians were given the opportunity for an informal concluding chat."
"proof the framework of maconeat shows that once the length of chromosome and number of subpopulations is determined and finite, both the individual space formed by all individuals and subpopulation space formed by subpopulations in each generation are finite. because the parameters used in the algorithm do not change over time, and the state transition is not dependent on generation but on the selection, crossover, and mutation operators, maco-neat is the one of the genetic algorithms with the finite homogeneous markov chains. so it is known that maconeat converges to optimal with the probability 1. the original proposition is proved."
"physicians and nurses have for several years used small handheld devices, such as pdas, as a replacement for pen and-paper and medical reference books [cit] . such devices have in recent years received wide adoption in health care and are increasingly used for ubiquitous clinical information access [cit] . however, to further enhance user acceptance [cit], and to reduce medical errors in the process of entering and retrieving information [cit], hardware and software design must be improved. while much work has been done to improve and optimize clinical software designs for handhelds, few studies have reported which interaction technique is optimal for use in a point-of-care setting."
(1) satellite agents have differences in capability. the number of resources consumed by different satellite may not be equal when they perform the same task.
"limitations of state-of-the-art. at the core of most recommender systems is some variant of matrix or tensor decomposition (e.g., [cit] ) or spectral graph analysis (e.g., [cit] ), with various forms of regularization and often involving gradient-descent methods for parameter learning. one of the recent and popular paradigms is based on heterogeneous information networks (hin) [43, [cit] ], a powerful model that represents relevant entities and actions as a directed and weighted graph with multiple node and edge types. prior efforts towards explanations for hin-based recommendations have mostly focused on paths that connect the user with the recommended item [1, 19, 44, 47, [cit] . an application of path-based explanations, for an online shop, would be of the form:"
"where k,source means a individual in policy neural network population while sat k turns towards the historical task set, k,target the individual in the target initial policy neural network population of sat k for the new observa- satellite agent learning policy is described by neural network in maconeat, thus k,h is used to establish the mapping function between individuals of historical policy neural network and policy target neural network."
the stylus is a small and thin pen-like device used to point at and interact with gui elements displayed on the pda touch screen. it is designed to fit into a slot in the handheld device for storage when not in use.
"the explanation is complete as it goes into full details of how to use the product, which is in alignment of my review and useful to me."
"before the arrival of the physician, the patient impersonators were given detailed instructions how to behave according to the patient scenarios and asked to memorize this. they were not given any explanation of the research questions being studied."
"one validity constriction of the study is that the patients were not real patients -they were carefully instructed actors and were not sufferers of the medical problem they simulated. however, when asking the physicians about the realism of the simulations they all gave a positive response. in addition, judging from the observations, most of them seemed to be \"on duty\" and asking questions and acting like physicians do. on the other hand the patient actors reported that they might have acted differently if they were real patients."
"another study found that some users used their finger or the hardware buttons to operate handheld prototypes when comparing interaction techniques for a pda-based health information system, even when the system under test was designed for stylus usage [cit] . this demonstrates that the users do not necessarily consider the stylus as the optimal solution for point-of-care usage."
"when performing formative evaluations of mobile systems it can be beneficial to do so in realistic environments. if doing otherwise, one might find usability problems in the graphical user interface, but will fail to find critical usability problems concerning the physical and social aspects of usability [cit] . in ward round situations field studies are often considered the optimal research method, but due to issues such as patient safety, time cost, and the lack of control they are not always the method of choice [cit] ."
"counterfactual explanations. a user u interacts with items via different types of actions a, such as clicks, purchases, ratings or reviews, which are captured as interaction edges in the graph g."
"sampling. for our experiments, we sampled 500 seed users who had between 10 and 100 actions, from both amazon and goodreads datasets. the filters served to prune out under-active and power users (potentially bots). activity graphs were constructed for the sampled users by taking their four-hop neighborhood from the sampled data (table 1) . four is a reasonably small radius to keep the items relevant and personalized to the seed users. on average, this resulted in having about 29k items and 16k items for each user in their hin, for amazon and goodreads, respectively."
"the colored marker was not enough feedback for some users; \"even if the color changes, you are not sure where you are\". another problem was that the physicians had to move the marker towards what they wanted. with the other interaction techniques they could go there directly. c) card ranking: button was the least preferred of the pda-based interaction methods. yet, it was ranked better than paper. three ranked it as their most preferred method and equally many ranked it as their least preferred."
"the survey revealed that the participants' level of experience with computers was very good. they all used computers daily, both in their job and privately. one participant reported using computers privately weekly only."
"motivation. providing user-comprehensible explanations for machine learning models has gained prominence in multiple communities [cit] . several studies have shown that explanations increase users' trust in systems that generate personalized recommendations or other rankings (in news, entertainment, etc.) [cit] . recommenders have become very sophisticated, exploiting signals from a complex interplay of factors like users' activities, interests and social links [cit] . hence the pressing need for explanations."
"the ward round simulations offered a realistic view on how the physicians used the paper and pda at the point-of care. together with data from the survey, card ranking exercise, patient focus group, and physician interviews, we were able to get a good impression of the qualities of the different interaction techniques. first we present general results from the survey, preference ranking and patient focus group. next, detailed findings from the observation and physician interviews are presented."
"in the multi-satellite cooperative task planning system composed of multiple eoss with autonomous planning capability, each satellite is considered as an agent for the randomly occurring observation request. realizing the dynamic distribution for the random tasks by coordination, each agent expects that tasks are assigned quickly and reasonably to every satellite based on autonomous computing, so that global observation obtains maximum benefit under the condition of meeting the constraints of each satellite. multi-satellite dynamic task planning (msdtp) has characteristics as follows:"
"to test the influence of different policies of tl, evolutionary generations, using three tl policies to perform all examples accumulatively and converge to the same threshold, are compared with those of maconeat. half, quarters and final mean 50 000 generations, 750 000 generations and 100 000 generations are respectively taken as history learning information to transfer."
"the great variance between the different interaction techniques makes it hard to select which technique a pda based point-of-care medication system should employ. since technology is evolving and user preferences are bound to shift, the important contribution in this study is the factors that affects physicians' (and patients') acceptance of a mobile point-of-care system."
"it also has to be noted, that due to the recruitment process the participants' willingness to use new information systems probably was above average of the whole professional group."
"the proposed prince method implements these principles using random walks for personalized pagerank scores as a recommender model. we presented an efficient computation and correctness proof for computing counterfactual explanations, despite the potentially exponential search space of user-action subsets. extensive experiments on large real-life data from amazon and goodreads showed that simpler heuristics fail to find the best explanations, whereas prince can guarantee optimality. studies with amt masters showed the superiority of prince over baselines in terms of explanation usefulness."
"a few physicians found button unsuitable for longer list, because it caused a lot of button presses. several considered this interaction technique as more distractive. as one said it; \"i noticed that i had to change my focus, gaze or thoughts from a screen to a keyboard\", because they had to constantly move their focus from the screen to the device buttons and back again."
"the ppr computation could simply re-run a power-iteration algorithm for the entire graph, or compute the principal eigenvector for the underlying matrix. this could be cubic in the graph size (e.g., if we use full-fledged svd), but it keeps us in the regime of polynomial runtimes. in our experiments, we use the much more efficient reverse local push algorithm [cit] for ppr calculations."
"the card ranking provoked the users reflection by reminding them what they had just used. instead of having to remember the differences, this knowledge was provided to them through the cards. we also observed that very few referred to the interaction techniques by name -they rather referred to it by pointing at the card or the device, which was available during the card sort. when sorting the cards in the preferred order, the physicians reasoned and argued for their decision. this revealed what the important usability factors were for them. these factors, and the way the interaction technique accommodated them, decided the position of the card in the stack. the aspects mentioned during the card sort are reported together with the interview results in section d."
"for tsk t t, the set of randomly occurring observation requests at t t, agent needs to integrate it with the historical observation set based on the idea of tl [cit], we design an incremental planning strategy transfer learning algorithm (ipstl). satellite agent learning policy mapping function is given by"
"the different prototypes were discussed in the order they were used in the test. to facilitate discussion and to avoid misunderstandings, cards with symbolic pictures of the different prototypes were provided for reference. they also had the paper chart and pda available."
"through analyzing the model, it is known that the process of solving msdtp is a problem of decentralized markov decision process (dec-mdp). bernstein pointed out that the optimal strategy of solving dec-mdp has the complexity of nexp-complete [cit] ."
"in general, the physicians found the mobile device input faster and its content easier to read compared to handwriting. some found the devices almost as simple as paper. one of them said; \"this was quite simple to use. if i tried it for weeks or months it would be as simple as the one i'm used to [the chart].\" however, most of them were worried about software malfunctions, usability problems or other situations forcing them to spend a lot of time fiddling with the device."
"physicians, who are the primary users of such systems in hospitals, often have a strong standing in their institution, and have a large degree of freedom in their choice of work methods and organization [cit] . they often choose what tools they want to use, and they also have the power not to use tools that do not accommodate their needs. they can leave the handheld device in the desktop drawer, use old fashioned pen-and-paper, or rely on their excellent memory if they want."
"extensive experiments with amazon and goodreads datasets show that prince's minimal explanations, achieving the desired item-replacement effect, cannot be easily obtained by heuristic methods based on contribution scores and shortest paths. a crowdsourced user study on amazon mechanical turk (amt) provides additional evidence that prince's explanations are more useful than ones based on paths [cit] . our code is public at https://github.com/ azinmatin/prince/."
"in order to further analyze the improvement of algorithm performance enhanced by the transfer learning strategy of the algorithm, the evolutionary curve for the group of case ph127 and ph125 based on two algorithms is given in section 6, as shown in fig. 9 . from the comparison between the evolutionary curves of two algorithms, the hybrid learning converts the learning policy of case ph127 to the follow-up groups, which not only improves the initial solution, but also takes the advantage that history learning information accelerates the learning rate of algorithm. thus, the convergence speeds of hybrid learning are all faster than maconeat in table 2 ."
"the results show that pda-based systems have some qualities that make physicians willing to replace their paper based medication systems. paper had a number of benefits compared to pda; the users were confident using it, it provided better information overview, and it supported the patient-physician dialogue better. however, no undo mechanism and poor error prevention on paper made the physicians prefer the pda-based solutions, despite generally low experience with handhelds and sparse training on the information system. this study can inform the design and choice of interaction techniques on new handheld point-of-care systems."
"in the msdtp, an exponential relationship between state and action space and current cumulative task scale will lead to \"curse of dimensionality\" if the method of temporal difference (td) and so forth are used for iterative estimation based on vfi. different from the algorithm of td solved by vfi iteration, policy search [cit] in the reinforcement learning searches policy space by direct utilization of optimization algorithm to find the optimal policy. and neuro evolution (ne) algorithm [cit] iteratively optimizes neural network populations by using search capability of ne, which could tackle effectively reinforcement learning problem in high dimensional state space."
"(ii) to determine if some a * replaces the top node rec with a different node rec * which is not an out-neighbor of u, we need to compute only the first of the two components in (i)."
"at the beginning of each test, the participant impersonator was lying in the bed in the patient room. in the hallway the physician was reminded of the case description of the next patient case and the user interface she would be using. this recapitulation was repeated for each new case, while the next patient impersonator was assuming his position in the patient bed. after completing all tests, the physician was interviewed about his or her experiences during the test and about their opinions regarding mobile computing in hospitals in general. the interview was performed in a semi-structured form with some predefined questions, but the physician was encouraged to raise other issues of personal concern in the context of the test."
the interviews were transcribed and analyzed to find the physicians' opinions and concerns about each technique. the interviews data were analyzed in software for qualitative studies (qsr nvivo 8) using an approach inspired by grounded theory [cit] .
"the card ranking data was used to calculate a quantitative measure of the physicians' interaction technique preferences. the preference ranking of each user was coded from 1 (least preferred) to 4 (most preferred). a friedman test was used to reveal if there were any significant rank differences in the data set, and a mann-whitney test was applied to each pair of interaction technique to reveal any significant rank differences between them."
"when all 14 physicians had tested the four interaction techniques, we arranged a focus group where the patient actors participated. during this session we asked them about their observations and experiences as patients."
"(3) planning environment changes due to the randomly occurring observation request, which increases the complexity of solving the problems. in the process of cooperative planning, not only coordination among multiple satellites is considered, but also new tasks are required to be integrated into historical planning results."
"evaluation metric. the metric for assessing the quality of an explanation is its size, that is, the number of actions in a * for prince, and the number of edges deleted in hc and sp."
"it comprises (1) prednisolon (prednisolone), 20 mg, tablet, twice a day, and (2) salazopyrin (suljasalazine), 500 mg, tablet, three times a day. during the ward round the patient discloses to the physician that she has developed an itching rash over her whole body. she remembers that she had a similar reaction to some antibiotic, but does not remember its name."
"the physicians showed a variety of holding techniques when standing and writing in the paper chart. usually writing while standing did not seem to bother them too much. however, in some cases the users experienced problems when writing in the chart; they got tired in their arm and were forced to change their holding position several times, or they were distracted by their id card, which was placed around the neck in a line and obstructed their writing. another source of minor distractions was the pen. sometimes the physicians had to search for it, other times they had to click it to get it working. b) interview: the paper chart was the interaction technique most users were confident with. they commented that it provided better overview because more information was available on the page, such as blood pressure, heart rate, and allergies. flexible input with the \"analog\" pen, freedom to make notes, and the ability to easily share the paper with colleagues was appreciated."
"before performing the crossover operation, both of connection genes in parent chromosome are firstly matched according to their ghis: if their ghis are equal, the genes are matched; otherwise, the matched connection genes do not exist in the two parent chromosomes. there are two cases classified as \"disjoint\" and \"excess\" based on the position of unmatched connected gene, as shown in fig. 4 . when the crossover operation is performed, firstly, two parent individuals are sorted according to the ghi order. the matched genes in parent individuals are stochastically selected as offspring gene. when \"disjoint\" and \"excess\" happen, the connection genes are inherited from the more suitable parent. in this case, equal fitness is assumed, so the disjoint and excess genes are also inherited randomly. in this way, ghis allow neat to perform crossover using linear genomes without the need for expensive topological analysis."
"after the crossover and mutation operation, the initial fitness of offspring individual of policy neural network with new topological structure may be low. to ensure that offspring individuals with topological innovation are able to store in the subsequent evolution to maintain the diversity of the policy population, the individual with similar topological structure is composed of sub-population, and is optimized iteratively within the niche."
"where r j,vst r k,vst means the time window resource taken by tsk j, and s j,k and e j,k are start time and end time of the task. eq. (3) means the observation process could not violate the energy constraint of satellite; eq. (4) prevents the observation from exceeding the memory capacity; eq. (5) means the time window of the total selected tasks could not exceed the available time window; eq. (6) is to ensure that the time window resources do not collide caused by different tasks of the same satellite."
"to devise a more efficient and practically viable algorithm, we express the ppr scores as follows [cit], with ppr(u, rec) denoting the ppr of rec personalized for u (i.e., jumping back to u):"
"each record in both datasets consists of a user, an item, its categories, a review, and a rating value (on a 1 − 5 scale). in addition, a goodreads data record has the book author(s) and the book description. we augmented the goodreads collection with social links (users following users) that we crawled from the goodreads website."
"credpaths you follow \"some user\" who has rated highly \"the multiversity\" with categories \"comics, historical-fiction, biography, mystery\" table 6 : explanations from prince vis-à-vis credpaths [cit] ."
"2) finger a) observations: in general the physicians managed to use the finger interaction technique without larger problems. however, we observed a few occasions of pressure and accuracy problems during the ward rounds with finger. in a few cases the physician were able to operate the device with only one hand. we did not see this for the other interaction techniques."
"3) button a) observations: the button interaction technique caused most problems. common mistakes were that they navigated the marker too far, or that they accidentally double-tapped the button. another common mistake was that they used the finger to press the on-screen buttons even if they were instructed to only use the hardware buttons. in general the users spent longer time to finish their tasks with this interaction technique. when using the other interaction techniques, the physicians often used their finger, stylus or analog pen as a reference point in the device or chart to keep track of their reading position in the information. with the button technique this was harder because their fingers were occupied with the hardware buttons. b) interviews: some users commented that user interface \"was like a mobile phone\", a device they felt comfortable with. with this interaction technique they could use their finger, and even one hand. they liked the immediate physical and audible (click) feedback from the buttons, in addition to the marker indicating the current selection. some experienced button as more secure, because they were forced to confirm their selection with a click on the center-key. with the other techniques they feared selections by accident."
"baselines. since prince is an optimal algorithm with correctness guarantees, it always finds minimal sets of actions that replace rec (if they exist). we wanted to investigate, to what extent other, more heuristic, methods approximate the same effects. to this end, we compared prince against two natural baselines: (i) highest contributions (hc): this is analogous to counterfactual evidence in feature-based classifiers for structured data [cit] . it defines the contribution score of a user action (u, n i ) to the recommendation score ppr(u, rec) as ppr(n i, rec) (eq. 4), and iteratively deletes edges with highest contributions until the highest-ranked rec changes to a different item."
"it could be seen from fig. 10, quarters could get faster convergence speed than that of the other two transfer learning policies by taking quarters as the transfer learning policy. it suggests that learning history planning by using transfer policy is faced with the problems of insufficient learning and excessive learning. thus, the selection for suitable transfer policy should be considered with specific problems. quarters is selected as transfer learning policy for msdtp in this paper."
"where e and d are respectively the number of connection gene \"excess\" and \"disjoint\" by two chromosomes, n means the number of the connection genes in one of the two chromosomes larger in scale, w is the average weight difference of match genes, c 1, c 2, and c 3 are respectively the weight coefficients of e, d and w . based on eq. (10), for a new chromosome g is calculated with chromosome representatives of each species. if the compatibility is less than the threshold value t, g will be added to the species. otherwise, a new species is created with g as its representative."
"the sharing function sh( ) is set to 0 when the distance (i, j) is above the threshold t, otherwise, sh( (i, j)) is set to 1. thus explicit fitness sharing lowers the individual fitness with larger scale in sub-population. every species is assigned a potentially different number of offspring in proportion to the sum of adjusted finesses f (i) of its member avoiding that any species controls the entire population. thus, the diversity of population is guaranteed and the premature is avoided."
"the algorithm simulates with 5 satellites. the time window of satellites for object observed is calculated by stk [cit], and the details for the data set are given in table 1, in which, sn means the serial number of test data, or the number of objects, tr the number of time window generated according to each satellite, and ds the distribution scheme of the objects. for comparison, all satellites in the planning are presumed to have the same spatial resolution."
"pdas are better for low volume and simple data entry [cit] . therefore, the prototypes were deliberately designed with only four functions; prescribe a new medication, change, pause, or cease treatment of a prescribed medication. the first function required more effort (select medication, medication dose and medication administration) than the three other (select prescribed medication and change it). in the medication selection process we simulated an idealized form of context dependency to reduce the length of the medication list. the list only presented medications that were reasonable to be prescribed in the particular scenario. each task required only 3 or 4 buttons to be pressed in the user interface."
"a within-subject testing approach was chosen to limit the number of tests and test subjects. thus each participant tested all user interfaces. the order of the tests was rotated between the tests to control and reduce possible order and learning effects. to reduce the number of patient impersonators, the two patient cases were alternated so that patient scenario a and patient impersonator a was used with the first and third interaction method while patient scenario 8 and patient impersonator 8 was used with the second and fourth. pilot testing with physicians not part of the study revealed that they were able to play the scenario realistically despite repeating the scenarios."
"n. analysis j) video data from usability evaluation given that usability problems of mobile devices often go beyond the graphical user interface [cit], we made detailed observations of (1) usability problems related to the user interfaces, (2) usability issues regarding the different interaction techniques, (3) issues regarding the physical and bodily aspects of usability, and (4) issues regarding the social aspects of usability. the video data from the experiments were transcribed and analyzed by two observers."
"the study provides both insight into the general attitude of physicians towards pda-based clinical information systems, and their opinions and preferences concerning different input techniques. while the particular findings about each specific interaction technique can be of interest to usability practitioners when selecting the primary form of interaction with the system, we have acquired an table iii. looking at the general assessment of physicians' attitude towards information devices in clinical practice, the study shows, that while some reservations and fear for potential problems exists, today's physicians are open for future implementations of pda-based systems. while physician hold strong expectations about the advantages of such systems, e.g. to facilitate information lookup, it is important to scrutinize and consider the raised factors, e.g. patient experience and software malfunction, when designing a system for routine usage."
"considering the characteristic that task planning requests occur randomly in multi-satellite dynamic task planning problem, a transfer learning strategy is introduced to multi-agent reinforcement learning, and a multi-agent hybrid learning algorithm is proposed. the algorithm describes the learning policy by using neural network which is encoded in a chromosome. by augmenting network topologies, the learning policy is optimized iteratively. and the fitness of the individual in the population is calculated in a fitness sharing way, which avoids the observation redundancy and improves the efficiency of resource utilization of satellites."
"in both methods seems unrelated to the recommendation. in the goodreads example, both hc and sp yield the same replacement item, which is different from that of prince. approximating prince is difficult. explanations generated by prince are more concise and hence more user-comprehensible than those by the baselines. this advantage is quite pronounced; for example, in amazon, all the baselines yield at least one more action in the explanation set on average. note that this translates into unnecessary effort for users who want to act upon the explanations."
"to detect spammers, we planted one honeypot in each of the 10 hits, that was a completely impertinent explanation. subsequently, all annotations of detected spammers (workers who rated such irrelevant explanations as \"completely useful\") were removed ( 25% of all annotations). table 5 shows the results of our user study. it gives average scores and standard deviations, and it indicates statistical significance of pairwise comparisons with an asterisk. prince clearly obtains higher usefulness ratings from the amt judges, on average. krippendorff's alpha [cit] for prince and credpaths were found to be ≃ 0.5 and ≃ 0.3 respectively, showing moderate to fair interannotator agreement. the superiority of prince also holds for slices of samples where prince generated explanations of size 1, 2 and 3. we also asked turkers to provide succinct justifications for their scores on each data point. credpaths you rated highly \"men's hair paste\" with category \"beauty\" that was rated by \"some user\" who also rated highly \"baby stroller\" with category \"baby\""
"figure 2: toy example. (a) a weighted and directed graph where the ppr scores are personalized for node n 1 . node n 4 has higher ppr than n 5 . (b) scores in a graph configuration where outgoing edges (n 1, n 2 ), and (n 1, n 3 ) are removed (marked in red)."
"for a fixed choice of rec *, the summands in expression 10 do not depend on a *, and so they are constants for all possible choices of a * . therefore, by sorting the summands in descending order, we can greedily expand a * from a single action to many actions until some rec * outranks rec. this approach is then guaranteed to arrive at a minimum subset. □"
in this study we have compared the three most common techniques for interacting with a small handheld computer. the techniques are supported by most current pda models. as a baseline we present the current paper-based practice (fig. 1.) .
the main rationale for making the handheld medication prescription system basic and simple with few functions and few screens was that the number of navigation and usability problems often increases with number of functions and navigational complexity.
"the planning results of the three algorithms without considering the conditions of history information of tl are compared under the condition of different scales and distributions of tasks. where, nonstationary converging policies (nscp) is a cooperative algorithm based on temporal difference reinforcement learning. ecnp is an extended algorithm of task distribution base on contract net protocol [cit] . as shown in fig. 8, whatever the condition of scale and distribution of tasks is, the results of maconeat are all superior to the other two algorithms. under the conditions of the large-scale task, example ph126- ph131, the advantages of maconeat are more obvious. due to a higher degree of multi-satellite potential collision for objects visited under the largescale task, maconeat adopts the method of solving the fitness for joint individuals of multi-population to reduce the collisions re-visited by multisatellite tasks, taking full advantage of relatively limited observation resources to gain the better planning results."
"to test the improvement of algorithm by tl, two examples with different scales and distributions are divided into a group. the hybrid learning of ipstl and maconeat is adopted and the results are recorded, as shown in table 2 . notes: first is the first example number, second the second example number; avg and max are separately the average value and the maximum value of results; cpu is the time of calculation (unit: s); task the quantity of average completion, imp the increased ratio for comparison between msdhla and maconeat, time the increased ratio for the time to reach the same threshold value of capability, and eva the increased ratio of results. table 2 it is seen that, compared with maconeat, hybrid learning accelerates the convergence speed, especially, which is reflected more obviously when the scale of previous is larger than the following example."
"with the evolution of maconeat, the neural network individual containing in each agent policy population changes the topological structure of neural network through a variety of operations. the first thing is to match the connection gene of parent chromosome if crossover operation is to be made. for this reason, a global historical identity (ghi) is employed to record the additional connection together for all individuals in populations, and the same connection gene has the same ghi."
"however, due to the low number of patients and the fact that they were not real patients, these results should be used as nothing else than an indication of patient experience and a direction for further work on how patients perceive handhelds compared to paper charts."
"while some approaches aim at scalable routing using different approaches, they lack a thorough evaluation of the impact of different mobility models. in fact, regarding this aspect, most routing solutions disregard the dynamics of different mobility models, focusing only on one mobility pattern. nevertheless, in order to appropriately evaluate the efficiency of an ad-hoc network and the performance of routing protocols, these aspects have to be taken into account. moreover, other works that study the impact of mobility fail to provide an extensive evaluation with existing mobility models [cit] ."
"inspired on a previous study on a dynamic addressing paradigm, the \"dynamic address routing for scalable ad-hoc and mesh networks\" dart [cit], is proposed as a proactive hierarchical approach that efficiently manages the organization of nodes into zones for large scale networks. address allocation and lookup are the main drawbacks of this proposal. however, the published study presents schemes to tackle these problems, showing how addresses can be allocated taking into account node positioning, building a tree with l levels -where l is the number of bits used in the routing address. a clear distinction is made between routing address and the identity of a node (a unique identification tag) as the routing address is dynamic and changes with node movement, contrasting with the node identifier which is always the same."
"regarding this last aspect, even though many mobility models have been proposed in previous works, each one of them has unique characteristics, not replacing other existing models. therefore, in this evaluation, several mobility patterns will be taken into consideration. in order to do so, the bonnmotion tool [cit] has been used to generate different node trajectories later employed in conjunction with the opnet modeler wireless simulator [cit] . these trajectories were created assuming a plausible speed for a person walking [cit], between 0.5 and 1.5 m/s and a pause time of 60 s, when applicable. the mobility generation disregarded the first 3600 s, solely using the next 900 s of path randomization, avoiding the initial warmup from the random number generations thus achieving a more stable scenario. moreover, the area of motion was of 1500 by 1500 m, for a total number of 541 nodes. higher speeds were not considered as the sense of clusters would be faded away and the realm of vehicular ad-hoc networks would be entered. also, even though new mobility models already present similarities with human mobility, the used http://jwcn.eurasipjournals.com/content/2013/1/86 mobility patterns were chosen for the sake of comparison with existing works on this subject. moreover, being the cgp protocol designed to explore spatial locality, it would benefit from non-random mobility models, rendering this comparison unfair."
"quite a few hybrid routing protocols for ad-hoc networks can be found in the literature, still, despite the fact that many rely on clusters or well defined zones, not many implement a hierarchical routing scheme. the following protocols propose a hybrid routing scheme capable of retrieving inter-cluster information in a reactive approach, avoiding the necessity of restraining routing information in cluster-heads to reduce the overall overhead. however, on a downside, inter-cluster communication may be subject to route retrieval delay if no previous path has been maintained in cache."
"based on the above remarks, we have modified the split-ring metasurface by adding a horizontally misplaced array of metallic wires parallel to the x-axis on one side of the srr array, as in fig. 3b, which allows breaking both horizontal and vertical symmetries at the same time."
"the cid information for each node is included in the header of the sent routing messages. the cgp protocol is an entirely proactive routing protocol, periodically exchanging its routing messages with the cid of the cluster to which the nodes belong. moreover, since routing messages are not forwarded across different clusters, a list of ip addresses and their corresponding cids is also included in some routing messages. the details about the routing procedures are explained in further detail in the 3.2 section."
"the proposed hierarchy allows the necessary amount of routing messages to be reduced, such that only location managers are required to update their information and only then perform the location finding process. however, this aspect is also negative on the overall performance of the protocol, as routing is strongly related with the hierarchy of the network, making the routing process complex and vulnerable to disruptions when location managers change. the cgp protocol is more efficient in this aspect as it is completely decentralized, using interchangeable gateway nodes for packet forwarding between clusters."
", which in the spectral domain has a transfer function ( ) k, as required in the first-derivative operation, it is necessary to break both vertical and horizontal symmetry."
"in zhls, each node contains an intrazone and interzone routing table to manage routing between nodes from a same zone and from different zones respectively. the update of these tables is performed by sending two types of lsp: node lsp and zone lsp for intrazone and interzone, in that order."
"one key advantage of using the cgp is that, by keeping its optimized network hierarchy, it is able to limit not only the effects of intra-cluster mobility but also the effects of inter-cluster mobility. moreover, it does not require additional routing messages for inter-cluster routing, being adaptable to any available link-state routing protocol. a preliminary version of this study, entitled deferred routing [cit], has been proposed with a similar hierarchy but which does not take into account node mobility across different clusters. not only does the cgp protocol handle mobility, but it also makes use of an optimized gateway metric selection which is able to load-balance the existing traffic among the existing gateway nodes, avoiding bottlenecks. moreover, the thorough evaluation provided with this study also shows that the store-and-forward technique of cgp, in conjunction with self-healing routes are also relevant contributions for efficient traffic delivery in largescale multi-hop networks."
"even though the approach presented by these authors uses the olsr protocol for intra-cluster routing, the use of the aforementioned c-hello and c-tc extensions to support a clustered network, may have a negative impact, as the propagation of these new messages across clusters is required. moreover, the introduced mechanisms may suffer from mobility phenomena, requiring an additional overhead of updating the entire network structure. regarding this aspect, the cgp protocol does not exchange additional messages keeping routing more scalable while handling mobility more efficiently by using hierarchically aggregated views of the network."
"another proactive hierarchical routing protocol is the \"cluster-head gateway switch routing\" protocol, cgsr [cit], where nodes are also grouped into clusters. this protocol relies on a cluster-head node to keep routing information about its cluster, and all other nodes only need to know the routing path until their own cluster-head. additionally, all the inter-cluster routing is also processed by the cluster-head which connects to remaining clusters' cluster-head nodes."
"in addition to the already mentioned challenges and overheads related to the maintenance of clusters and their cluster-heads, it is clear that, even though routing overheads can be reduced, the cluster-head will always have to be part of any routing path, leading to non-optimal paths, and additional interferences in the vicinities of clusterheads. the cgp approach is focused on choosing the most stable routing paths, not being restrained by any cluster-heads."
"similar approaches such as the \"fisheye\" and \"hazy sighted link state\" routing protocols [cit], improve their scalability by using mechanisms that allow imprecise or slightly out-of-date routing information regarding distant nodes, on a node basis. even though these schemes reduce the amount of routing information, they do not support clustered networks nor do they do avoid disruption from strong mobility."
"for illustration purposes, after being imported to the simulator, the resulting trajectories were then converted to image files and are depicted in figure 9 representing the gauss-markov (figure 9a ), manhattan (figure 9b (figure 9f ) mobility models. these different mobility models are entirely random but each one has its own specificities. by using them the intent is to demonstrate that the cluster gateway paradigm is suitable in the most diverse scenarios."
"despite being able to scale, as each node only maintains a partial topology graph of the network, the star may suffer from large memory and processing overheads in scenarios where constant mobility may report different source trees, and routing paths are too long due to the network size. to handle this aspect of increased memory and processing overhead, the cgp makes use of virtual clusters which aggregate real clusters, reducing the amount of information required for routing."
"since interactions between members of the same cluster are likely to take place, the cluster gateway protocol (cgp) is defined taking this aspect into account. the protocol resorts to aggregated views of the network, establishing a hierarchy between existing clusters and virtual ones, camouflaging the negative impacts of node mobility between them. as a result, each node will solely keep detailed information about its own cluster and will maintain aggregated information about the network according to a pre-defined cluster hierarchy."
"an example of hierarchical proactive routing protocol presented in \"source-tree routing in wireless networks protocols\", star [cit], is a link-state protocol which has on average less overhead than on-demand routing protocols. its bandwidth efficiency is accomplished by restraining the dissemination of link-state information only to the routers in the data path towards the desired destinations. star also creates paths that may not be optimal while avoiding loops, such that the total available bandwidth is increased. moreover star has specific mechanisms to know when update messages must be transmitted to detect new or unreachable destinations, and loops."
"one important assumption, and a possible limitation from this protocol is that each node knows its own position (for instance by using gps) and consequently its zone http://jwcn.eurasipjournals.com/content/2013/1/86 id which is directly mapped to the node position. with this approach packets are forwarded by specifying in their header the zone id and node id of their destination."
"even though the ddr algorithm does not require any sort of cluster-head for cluster maintenance, the possibility of some nodes being chosen as preferred neighbors by other nodes may lead to the creation of bottlenecks, as they would be required to transmit an increased amount of both routing and data packets. it is important that the choice of preferred neighbors is balanced in order not to compromise the overall performance of the protocol. moreover maintaining the entire logical structure of the network could be heavy, depending on how dynamic nodes may be. the cgp handles the dynamism of nodes by using aggregated view of the network and avoids bottlenecks with the used link quality metric for gateway selection."
"regarding this aspect, the olsr protocol is clearly less scalable than the c-olsr and cgp protocols which register a significantly smaller number of topology changes, as shown in figure 13 . in particular, the olsr protocol has a worse performance for a static scenario. such behavior is a result of the wireless medium interactions of the nodes which are strongly connected in this scenario. in fact, in the mobile scenarios, where connectivity is more often scarcer, there is a clear reduction of the number of topology changes, suggesting once more that the olsr protocol does not scale appropriately."
"while having a wireless network organized according to the expected node interactions will allow routing protocols to perform more efficiently, less common interactions between different clusters must also be handled. for instance, referring back to the network hierarchy presented in figure 2, the worst case scenario would occur with packets being sent from a source node s within cluster 7 to a destination node d in cluster 28. even though this social interaction is not expected to be common, it might occur and the packet forwarding by cgp will be http://jwcn.eurasipjournals.com/content/2013/1/86 described next. the presented gateway and path choices are merely illustrative and, despite aiming at minimizing the number of cluster hops, different paths may exist and will be chosen according to the current load in each link."
"in the following, we propose a new approach to impart linear mathematical operations on the impinging wavefront, based on nonlocal metasurfaces in transmission mode, providing large resolution and efficiency. by introducing a slow periodic spatial modulation over the metasurface profile, we significantly boost its nonlocal response, and through appropriate selection of the modulation parameters we can realize different types of mathematical operations."
"in addition to the already mentioned aspects of the cgp hierarchy, the most relevant characteristic is how it is able to cope with mobility phenomena and with changes in the clustered network. by using different network perspectives to each cluster, the addition or deletion of clusters, as well as the changes in nodes' cluster association, will only have an impact to hierarchical nearby clusters in which the changes occur."
"taking into account the defined objectives of this evaluation and their statistical validity, all the presented results have a 95% confidence interval obtained from the central limit theorem which states that, regardless of a random variable's actual distribution, as the number of samples (i.e., runs) grows large, the random variable has a distribution that approaches that of a normal random variable of mean m, corresponding to the same mean as the random variable itself."
"the traffic delivery percentage obtained by a routing protocol is a good indicator of its performance when defining routing paths. figure 10 illustrates the percentage of losses registered by the routing protocols in all the defined scenarios. in these scenarios, the cgp stands out by dint of having almost less than half of the losses than the remaining protocols. conversely, the c-olsr protocol registers the worst performance, having always more lost packets than the olsr and aodv protocols. this is mainly due to the usage of c-mprs which are not efficient in scenarios with mobility."
"regarding the static scenario, the olsr and c-olsr protocols unexpectedly show worse delivery performance than in the mobile scenarios. this is a consequence of their inability to scale, as in the static scenario more paths exist, whereas in the manhattan scenario, for example nodes are separated by the arrangement of the streets. however, the cgp scheme is oblivious to the nodes' placement and has a similar performance in all the scenarios. this is extremely important as the social interactions between users may several times lead to static scenarios, for instance in concerts and sport events."
"having presented the cgp concept and its main features towards scalable routing, it is important to evaluate its performance against other routing protocols. moreover, being a protocol which aims at reducing the impact of mobility, the analysis of different mobility patterns becomes inevitable while reasoning about routing specific parameters."
"since the nodes move freely across the entire scenario, their cluster association has to be changed. these cluster changes are handled by the cgp protocol which considers a total of 9 clusters, divided across the scenario, updating the nodes cid when they move to a different cluster. as a result of nodes not being constrained to specific clusters, different node densities per cluster exist throughout the simulation time, while nodes follow their trajectories. these different densities impact negatively the cgp protocol since clusters are expected to be equally balanced throughout the simulation, however the cgp will still present a good performance."
"in the used hierarchy, inter-cluster communication is guaranteed by border nodes, gateways, which are responsible for forwarding packets. however, since several gateway nodes may exist in one cluster, it is important to select the most suitable one. this is achieved by using a new gateway selection metric which estimates the link quality of a gateway using kernel based regression methods and the interval time between received routing messages."
"after processing this information, border nodes, or gateways, learn their own network perspective as well as the reliability of themselves as gateways, and announce it to every node within their clusters. this aspect results from the link quality of the gateway node, determined by using kernel estimators, presented later in this study. an additional feature of the used gateway nodes, is the storeand-forward capability. this allows gateways to temporary store data packets when for some reason a broken link is detected, or when a cluster changes. by using this mechanism, less packets are lost and the healing process of previous routes is automatically triggered."
"in addition to the limitation of requiring some positioning system, the zhls protocol requires that all nodes exchange inter-zone flooding information when only gateway nodes need this routing information for calculating the shortest path between different zones. moreover, the zhls is susceptible to a route retrieval delay when establishing inter-zone paths, as reactive routing is used for this purpose."
"hierarchical routing is expected to improve resilience to mobility [cit] . however, to the extent of our knowledge, there is only one hierarchical routing protocol that aggregates cluster information with different granularity, named deferred aggregated routing for scalable adhoc networks, dash [cit], which implements a deferred routing approach [cit] . even thought this routing concept is more effective in supporting node mobility, it does not avoid bottlenecks in the choice of the used gateways for inter-cluster communication, leading to a higher overhead in the network and losses. moreover, it lacks a proper evaluation that considers demanding node mobility, with nodes constantly changing between different clusters. the least disruptive approach regarding communication overhead is provided by hybrid hierarchical protocols which use reactive routing for inter-cluster paths, however they suffer from a typical delay in on-demand solutions when retrieving paths. the cgp is an purely proactive routing protocol that aims at tackling the identified issues in scalable multi-hop routing, being resilient to node mobility."
"gw_connectivity_create(cluster id ) 14: for each cluster entry in message.cluster connectivity do 15: overhear information by foreign clusters 16: for each gw entry in cluster entry .gateways do 17: gw_connectivity_create(src addr, cluster entry, gw entry ) 18: end for 19: end for 20: messages from different clusters must not be further processed 21: return consequently, end the procedure 22: 23:"
"the aggregation is performed according to the hierarchical relationship between clusters, such that hierarchically closer clusters are less aggregated and further away clusters are progressively more aggregated. a similar aggregation level is obtained for clusters 9 and 10, as illustrated by figure 3b . by using broader parameters that bring clusters together, these different granularity perspectives allow the desired organization for the forwarding of packets through selected gateways. moreover, as presented later in this study, the gateway selection will take into account the reliability of each gateway link, avoiding congestion on existing links. in this example, the given hierarchy reveals that clusters 9 and 10 are more likely to interact with clusters 7 and 8 (aggregated into cid 3) rather than with any other node in the remaining clusters."
"in another context, metasurfaces have been explored to efficiently manipulate the optical wavefront in the spatial domain over deeply subwavelength thicknesses [cit], with the goal of imprinting a profile of choice to the impinging optical wave. metasurfaces for beam steering, anomalous reflection and refraction, holography, light trapping and focusing have been recently developed, opening a successful route towards replacing bulky and complex optical components with ultrathin devices. in the following, we explore the use of metasurfaces to perform optical analog computing, manipulating the impinging optical wavefront in momentum space, rather than in the spatial domain."
"regarding the traffic delivery and routing performance of the evaluated protocols, the cgp achieved an overall http://jwcn.eurasipjournals. better performance than the remaining protocols. its selfrepairing behavior and load-balancing properties for an efficient gateway choice revealed that instead of dropping packets in changing routes, storing them temporarily and re-forwarding these packets to new appropriate routes enables a much more robust routing protocol. moreover, the cgp also revealed that its cluster-based hierarchy and network perspective is able to maintain higher routing stability, being less resource demanding and consequently more energy efficient."
"considering the proactive approach taken by the aodv protocol, shorter paths are provided in half of the scenarios. this reveals that proactive protocols are not as efficient, even in static scenarios. however, the cgp protocol is always capable to deliver more packets than the aodv protocol."
"since this metric analyzes the link quality between two nodes in real-time, an efficient gateway selection can be achieved. whenever a gateway node is under a significant amount of traffic load, its link quality will decrease and thus an alternative gateway, if existing, will be selected. another external influence that can be predicted by this metric are link disconnections due to mobility since, a departing link will progressively lose its quality, leading to the selection of another gateway node. by using this metric with a well defined cluster hierarchy and low routing overhead, the cgp is be able to efficiently forward its packets throughout the network until the final destination is reached."
"the presented results revealed that both the olsr and c-olsr protocols are highly influenced by the used mobility patterns. in the performed simulations, the gauss-markov and random waypoint models, despite being different, present a good distribution of nodes where some areas are more dense than others. however, between these two mobility models, even though no significant changes are registered for the cgp, the performance of the olsr and c-olsr protocols changes considerably regarding routing overhead and path length. moreover, the provided results also considered a reactive routing protocol, the aodv protocol, which revealed that it suffers from an increased end-to-end delay when compared to proactive versions and also that it registers more losses than the cgp protocol."
"even though the total number of hops achieved by cgp may not be the smallest possible, as packets travel through clusters, their proximity to the destination cluster unveils a more precise network view and thus shortest paths are more likely to be established. in addition to this, mobility phenomena which might render previously calculated paths impractical, are transparent throughout the packet forwarding amongst different clusters. this straightforward approach allows the cgp to automatically repair routing paths such that an outdated routing decision does not result in a packet drop. hence, with this self-healing characteristic, whenever a packet is incorrectly forwarded, the following nodes will certainly be able to re-forward it into the correct path."
"in order to evaluate the performance of the presented routing paradigm (cgp in the presented figures), six scenarios incorporating different mobility models and an additional one with static nodes have been used. all these scenarios have the same area and number of nodes, using the trajectories defined by the bonnmotion tool, as previously defined."
"in order to define a proper routing hierarchy to be used by cgp, groups of nodes are defined, similarly to autonomous systems in bgp but applied to wireless networks [cit], which represent their clusters as presented in figure 1 . when considering mobile ad-hoc networks, this organization can be achieved by using a clustering algorithm such as the generalized max-min clustering algorithm [cit], being the management of the clusters and their identification managed by the cgp routing scheme. in order to do so, each routing message used by the cgp protocol includes a cluster identification (cid) kept by each node. due to mobility, whenever a node changes its cluster, the cgp routing scheme will update the cid of that node and perform the required adjustments regarding the existing routing tables."
"motivated by the lack of a routing scheme where large clusters of wireless nodes may exist, this article presents a new routing approach that takes into account the increased interaction between users within a same context, regardless of the used mobility pattern, using a well defined network hierarchy of real and virtual clusters. previous studies show that content is exchanged between millions of individuals resorting to phone interactions or on-line services. in this sense, clusters of users can be identified in friendship circles, or on common interest groups where clusters within clusters exist [cit] . therefore, and due to the registered growth of wireless capable portable devices, this study aims at defining a scalable routing scheme, resilient to mobility phenomena capable of taking advantage of existing clusters."
"the r statistical language [cit] was used together with the \"locpol\" package [cit] in order to perform the required bandwidth computations and regression fitting, using as input hello message's traces obtained from the opnet modeler wireless simulator [cit], between two nodes placed at different distances and by collecting the link quality information."
"taking these different aspects into consideration, this performance assessment must involve the evaluation of a large scale network, measuring the stability and overhead of this concept, as well as its overall traffic delivery performance. moreover, in order to allow a more exhaustive evaluation it is important to determine the protocol's ability to handle mobility phenomena, introducing dynamic scenarios with different mobility models."
"in another existing study, entitled \"multimedia support in mobile wireless networks\" mmwn [cit], the authors propose an architecture consisting of two main elements, corresponding to different node types, which can either be switches or endpoints. both of these can be mobile, however only switches can route packets and only endpoints can be sources of or destinations for packets. this protocol also keeps a cluster hierarchy as a location management scheme, capable of obtaining the address of an endpoint. this information is kept as a dynamic distributed database, such that in each node there is a location manager node."
"as a result of the randomly chosen destinations and of the wireless medium interactions, the confidence interval registered for the path length is higher than for other parameters. nevertheless, this interval is consistent and similar for all the analyzed routing protocols, validating the outcome of the parameter."
"in contrast with typical flat routing protocols, hierarchical protocols usually exchange their routing information in different ways, according to a cluster or node hierarchy level. the usage of hierarchies in conjunction with proactive routing approaches is found in the form of a hierarchy of clusters, as an organized tree of addresses, or even as trees of paths forming a topology."
"the forwarding process begins with identification of the destination's node (d), and the cluster to which it belongs. the source node (s), is responsible for this task and verifies that the most suitable gateway is g 1 which has a distance of 2 cluster hops. the routing from s to g 1 is performed within the cluster by checking the intra-cluster routing tables. at this point, all the nodes handling packets from this flow know only that node d has a cid of 2 (according to their hierarchical perspective). this is illustrated by figure 5 ."
"each flow was defined with constant bit rate of 8 packets of 4 kbit per second (using udp), being this type of traffic flows representative of typical interactive gaming, simple file transfers or information exchange [cit], which are all well suited applications for mobile ad-hoc networks. the start time of each flow is randomly determined following a uniform distribution between 50 and 250 s of simulation time, being concluded by the end of the simulation."
"the previously mentioned steps remain the same since no change is detected by the nodes in other clusters. as far as it is perceived by nodes not in clusters 27 or 28, the destination node d has always been either in cluster 2, cluster 6, or cluster 13, depending on the hierarchical position of the observing clusters."
"the cgp approach consists in efficiently handling routing in clustered networks by defining a network hierarchy, without the use of cluster-heads. in addition to this, the cgp protocol does not exchange any additional routing messages, including only the required cluster and routing maintenance information in already existing messages."
"while ddr creates and maintains a dynamic logical structure of the wireless network, the \"hybrid ad hoc routing protocol\", harp [cit] finds and maintains routing paths. the harp protocol aims at discovering the most suitable end-to-end path from a source to a destination by using a proactive intra-zone routing approach and a reactive inter-zone scheme, by performing an on demand path discovery and by maintaining it while necessary."
"the \"zone-based hierarchical link-state\" routing protocol, zhls [cit], is characterized by dividing the network into non-overlapping zones where two different routing paradigms are used: proactive routing within the zones and reactive between different zones. this proposal alleviates single points of failure and bottlenecks by not being dependent on cluster-head nodes and, at the same time, by maintaining a scalable hierarchy based topology."
"another hierarchical hybrid routing protocol, the \"distributed dynamic routing\" algorithm for mobile ad-hoc networks ddr [cit], is a tree based routing protocol which consists of six different stages. in these stages an election of the preferred neighbor is made, followed by the forest construction, which creates a suitable structure for the wireless network, allowing an improved resource utilization. afterwards intra and inter tree clustering is performed, followed by zone naming and partitioning. zones are responsible for maintaining the protocol scalable and reducing the delay."
"the cgp approach does not require additional routing messages and limits its overhead by inserting gateway information in the messages of the oslr routing protocol. moreover, the presented routing approach maintains a mapping of each node's cluster association, propagating this information in existing routing messages only when changes occur."
"the network organization used by cgp resembles to the cartographic division of continents, countries and cities, assigning identifiers with different granularities to each region. as an example, when traveling through different countries or when sending a letter, people only consider their destination in a broader view, setting their goal to it. for instance, when someone writes a letter, they specify the name and address of their correspondent, however the postman only takes into account a broader view of the destination, such as the destination country. then, upon reaching the desired goal (e.g., a country), will then the destination perception be updated into a city, municipality, street and so on until ultimately the building or person in question is discovered. in cgp the same principle is found: routes are established according to their reliability and available gateways, using different granularity levels, rather than on the total hop count from source to destination, discovering the path as packets get forward through different clusters."
"in the c-olsr protocol the definition of cluster hello and topology control messages (c-hello and c-tc), allows the maintenance of paths among the existing clusters while reducing the required amount of routing information, as only mpr clusters generate c-tc messages."
"as previously mentioned, the estimators used in this study are from the class of kernel-type regression which allows the estimation of a least-squared weighted regression functionm(x; p, h), that \"locally\" fits a pth degree polynomial, for a given data set (x, y) [cit], where h is the smoothing or bandwidth parameter. kernel methods and, in particular, kernel regression methods are also called memory-based methods because they require keeping or storing the entire training set to estimate or compute future data points. in fact, these methods fit a model separately at each data point x i . only data points close to x i are used to fit the above mentioned model. this fitting process is such that the resulting estimated function is smooth in ."
"the increasing number and portability of wireless capable devices led to the creation of new applications and has brought new challenges for networking in wireless ad-hoc networks. in this study a new routing scheme, motivated by the existing issues in ad-hoc network and inspired by the relationships between adjacent clusters, is presented. the cgp can be defined as an \"imprecise\" routing scheme with self-healing routes that handles aggregated views of a clustered ad-hoc network, organized in an hierarchy. it does not exchange inter-cluster routing messages, relying on the routing information overheard by gateway nodes, forwarding data packets between clusters. the protocol also makes use of a kernel-based link quality estimator which allows the choice of the most suitable gateways, providing load-balancing and disconnection prediction in each cluster. these properties allow it to be a scalable routing protocol with constant communication complexity, being implementable with any linkstate routing protocol."
"return cid new 32: end procedure 33: 34: procedure get_level(cid) 35: level ← log 2 (cid + 1) 36: return level 37: end procedure interference, allowing load-balancing between the existing resources."
"other regression functions related with kernel regression are the k-nearest neighbor (knn) classification, state vector machines (svm), neuro-fuzzy models and radial basis functions (rbf), which may not be so robust. for instance, on the classification rss based fingerprints, [cit] do not consider the knn approach as it presents a poor performance when training vectors that are nonconvex and multimodal. also, previously used svms and rbfs have shown no resilience in scenarios with highly dynamic wireless settings, where manets should be included."
"when defining a routing path, a source node's main concern is to identify where the destination node can be found, the node's perspective to what concerns their own cid is presented in procedure 2. assuming that the destination node is within the same cluster as the source node, the shortest path is already known according to the routing table defined by the link-state protocol. however, when a destination is found in a different cluster, the next task of the source node is to find the most suitable gateway node. by analyzing the provided information by each gateway node in each cluster, the source node will choose the path with less cluster-hops, forwarding packets to it."
"the usage of hierarchical reactive protocols is modest when compared with proactive or hybrid routing approaches. this is most likely due to the fact that most well defined hierarchies require constant updates in order to be efficiently kept. however, this goes against the concept behind reactive routing, which only exchanges routing information when required. nevertheless, some hierarchical reactive protocols do exist and, as an example, the \"cluster based routing protocol\", cbrp [cit], proposes a variation of the \"min-id\" [cit] for cluster formation, restraining the typical flooding required by proactive protocols within each cluster. by relying on flooding between cluster-heads in different clusters, adjacent clusters can be known and therefore reducing routing overhead."
"when comparing the standard proactive and reactive protocols, olsr and aodv, the on-demand approach has slightly less losses in a majority of the presented scenarios. however, there is still a large number of losses since the aodv protocol is more suitable for sparse networks and does scale appropriately."
"another important aspect that motivates and influences wireless multi-hop networks is the establishment of data flows between nodes. in the defined scenarios, 24 traffic flows with different destinations were generated in each run. from these flows, 50% were randomly chosen throughout the network, while the remaining traffic destinations were set to nodes within the cluster of the source node. by using this approach, both social interactions within clusters and outside will be assessed, providing a complete evaluation of the protocol's performance."
"computing metasurfaces may combine the small size of digital systems with the high speed and near-zero power consumption of conventional optical components. a few proposals of engineered compact optical structures realizing fourier optical processing have been recently put 3 forward [cit] . a relevant example employed multilayered metamaterials [cit], showing the possibility of performing linear mathematical operations over a footprint comparable to the wavelength, but at the cost of complex requirements on materials and fabrication. another approach [cit] was based on the excitation of surface plasmon polaritons at the interface between a dielectric prism and a plasmonic material. interesting for its simplicity, this approach is however limited to differentiation in reflection mode, and it works for obliquely incident waves of narrow spatial spectrum and with limited efficiency, as it operates around the reflection zero associated with the plasmon resonance. ref. [cit] expanded this idea, proposing analog signal processing in transmission mode for normally incident signals based on suitably engineered photonic crystals, however, limited to the second derivative operation, and with similar limitations for efficiency and angular aperture, implying limitations on the overall achievable spatial resolution. all these approaches still require volumetric interaction with light, which limits the size of the metamaterial to the wavelength of operation."
"another noteworthy example of this aggregation scheme is presented in figure 4a, which represents the view of clusters 27 and 28. these two clusters have an additional hierarchical level, which may have resulted from a more detailed cluster division due to an increasing number of nodes or separation of interests, resulting in an additional cluster in their view. moreover, in this particular example, even though cluster 14 is the actual cluster, if one or more divisions were to occur, no changes would be noticed by clusters 27 and 28. the remaining views for clusters 11 and 12, as well as for cluster 14 are presented in figures 4b,c, respectively ."
"the division of the network into a number of zones depends on factors such as node mobility, network density, transmission power and propagation characteristics. the geographic awareness is much more important in this partitioning process, as it facilitates it when compared to radio propagation partitioning."
"the hierarchy defined by the cgp also establishes a relation between the virtual clusters and the real clusters which represent the most detailed level of knowledge about an existing cluster (represented by the leaf clusters). while other hierarchies simply take into account existing clusters, the virtual aggregation of clusters allows the cgp scheme to be more resilient to mobility phenomena, reducing the undesirable effects of micro and macro mobility, thus maintaining routing more scalable. in particular, since people movement is not expected to be entirely random, this organization will reduce the number of registered cluster changes."
"previous studies addressing the topic of scalable multihop routing have relied on the usage of straightforward http://jwcn.eurasipjournals.com/content/2013/1/86 clustered network organizations. by using a clustering protocol, these approaches are able to restrain the propagation of routing messages throughout the entire network and reduce the impact of node mobility within clusters. however, a major drawback of these solutions is related with inter-cluster routing overhead and poor support of node mobility between different clusters. moreover, some existing solutions also rely on super-nodes or clusterheads, to guarantee the dissemination of routing information, creating bottlenecks and putting these nodes under additional stress."
"consider a metasurface consisting of a periodic array of resonant particles in the xyplane, as in fig. 1a, here formed by split-ring resonators (srrs) parallel to the xz-plane (magnetic dipole moment parallel to the y-axis) [cit] . other forms of resonant particles, such as this strong nonlocal response can be used to realize different mathematical operations."
"even though most wireless networks are in fact intermittently connected due to interferences in the wireless medium, the mobility of nodes has also an important role in this aspect. typical dtn solutions such as prophet [cit] are capable of operating with delay tolerant traffic when wireless connections are not reliable, but fails to perform well with completely unknown node mobility. other approaches focus on more stable parameters such as social interactions between nodes. for instance, the friendshipbased routing (fbr) protocol [cit] or the social aware networking (sane) scheme [cit] take into account social interactions, both physical and virtual, in order to take a packet forward decision. nonetheless, these schemes fail to determine a possible path to deliver their packets in real time being therefore not comparable with the presented routing solution. moreover, scalability issues are again not taken into consideration, rendering these approaches useless in highly populated scenarios where a large amount of data traffic may depend on one node alone."
"this impact taken by mobility is less noticeable when analyzing the cgp which has a more stable performance than the other protocols. its organization, gateway selection metric with load-balancing and store-and-forward techniques allow an optimized routing performance, in both static and mobile scenarios, scaling appropriately, contrary to other routing protocols."
"even though the total percentage of losses is significant to all of the protocols, they are all real-time protocols and some routing paths may never exist between source and destination nodes. this is a characteristic of the used networks which do not guarantee traffic delivery at anytime, in particular since udp is used. however, when analyzing the obtained results, the cgp always performs better than its competitors."
"in cgp the appropriate choice of a gateway node is crucial for correct forwarding of packets throughout the network. therefore, an important part of this study is the definition of a link quality metric, achieved by using local polynomial kernel estimators. this metric will allow the selection of most suitable existing gateways, being able to re-route in real-time packets to more reliable gateways. this estimation tool is obtained from analyzing the time interval between the reception of periodical routing messages, while deriving an accurate model for that purpose, as suggested in a previous study [cit] which evaluated the performance of this technique. by extending this model in order to take into account routing decisions, load-balancing properties will also be given to the protocol, avoiding links under congestion."
"in order to provide a thorough evaluation of the cgp behavior in large scale networks, an assessment of its performance against different routing approaches must be considered such as proactive and reactive routing schemes. moreover the following metrics will be considered in the provided evaluation:"
"moreover, when comparing with the on-demand aodv protocol, the cgp protocol has a significantly better performance registering up to four times less delay. the delay from the aodv protocol is accounted in part from the route discovery process but results mostly from a poor choice of paths."
"when considering the scalability of a routing protocol, the stability of its routing tables is a key aspect on how it performs. even though reactive protocols do not periodically update a routing table, the update of a proactive protocol's routing table may be a costly procedure in terms of both processing power and required energy, possibly leading to the creation and dissemination of additional routing messages, depleting the batteries of mobile devices faster than desirable."
"a different perspective on wireless multi-hop routing has been provided with the definition of delay-tolerant networks (dtn). in these networks routing protocols are designed to deliver traffic that is not delay sensitive, despite the sparse intermittently connected properties of such network. conventional routing in wireless multi-hop networks is not suitable for highly dynamic scenarios as it needs to establish an end-to-end path before starting the routing of data packets, which may not be possible at a given moment."
"for comparison purposes, all the presented scenarios have been used to evaluate the cgp, olsr, c-olsr, and aodv protocols. these protocols comprise respectively the different approaches available for proactive protocols, employing hierarchical clustered routing, flat un-clustered routing and flat clustered routing, taking also into account the different approaches taken by the reactive aodv protocol. by analyzing the four approaches it is easier to understand which one is more suitable for large scale networks, highlighting the main advantages and disadvantages. moreover, by using the well known olsr protocol, which is a standard proactive protocol from the ietf manet working group, the cgp protocol can be validated as a functional protocol for manets."
"despite having a robust hierarchy, a key aspect of the cgp is enabling the routing between the virtual and real clusters, ensuring scalable routing between source and destination nodes. similarly to other routing protocols, the path establishment within clusters is performed by a link state routing protocol, such as the olsr protocol. however, additional procedures have to be guaranteed so that packets are correctly forwarded between different clusters, as no additional protocol is used."
"else 23: even ← 1 24: end if 25: end while 26: return cid own 27: end procedure 28: 29: procedure join_view(cid, n level ) 30 :"
"in addition to the sent routing traffic, the amount of received control traffic should also be considered since some of the routing information may be received by several nodes at the same time. the results presented by figure 15 confirm the tendency previously observed, where olsr is more resource demanding than the other protocols and where the cgp scheme is the lightest. in fact, for the nomadic scenario, where the c-olsr protocol sends more routing information, it is possible to see that this information is only received by a reduced number of nodes when comparing with the olsr protocol, which has a higher amount of received traffic."
"in order to achieve scalable routing many different protocols have been proposed using distinct techniques. one typical solution for scalable routing is known as clustering, where nodes are grouped into clusters, limiting the amount of shared information amongst them. routing in these clustered networks is typically characterized by the definition of specific hierarchies by routing protocols, aiming at keeping themselves more scalable."
"in the described packet forwarding scheme, the gateway nodes always have to identify the current destination's (d) cluster. this is required since a gateway node may be a gateway to several clusters at the same time and it needs to choose the appropriate one. another important aspect of packet forwarding is the choice of the most suitable gateways. this parameter results directly from the reliability of the link between the nodes in neighbor clusters and the gateway, as presented in the following section."
"the dart proposal reveals to be an efficient solution for routing in large scale ad-hoc networks. however, for small networks the dynamic address heuristic has a strong overhead impact and in general it is difficult to implement, as the distributed lookup table is hard to manage. since the cgp scheme can be implemented on top of any link-state routing protocol, using it for intracluster routing, small networks are no challenge and its implementation is straightforward."
"regarding the store-and-forward properties of the cgp gateways, a 4 s limit was the maximum store time defined, while re-establishing a path, before discarding packets. even though a higher store time could be used, delay tolerant networks are out of the scope and in this study a higher delay time will mean that no route exists at a given moment from source to destination. in addition to these parameters, a gaussian kernel was used for the link quality metric, estimating the quality of gateways by analyzing the interval between received hello messages."
"in this section, the cgp paradigm is presented, defining a method for scalable proactive routing in clustered manets and the necessary procedures to be added to an existing link-state routing protocol such as olsr, allowing it to support this scheme."
"even though the proposed cluster hierarchy may reduce the amount of flooding for dissemination of routing information, as only the cluster-heads are responsible http://jwcn.eurasipjournals.com/content/2013/1/86 for this task, the process of maintaining these clusters involves additional overheads, in particular the election of an appropriate cluster-head node. moreover, this special node will always represent a bottleneck on each cluster, overloading it and possibly leading to a faster energy depletion, and consequent cluster-head re-election. the cgp approach has also a reduced amount of required routing information but does not required a centralized entity, being more resilient to node mobility."
"regarding the overhead of the aodv protocol, it strongly depends on the number of initiated flows which trigger a flooding process for route retrieval. even though on-demand protocols are expected to generate less overhead than proactive approaches, the cgp sends less routing information than the aodv protocol."
"here, to prove the principle, we use a basic sinusoidal modulation, adequate to realize basic operations, such as differentiation or integration. more sophisticated modulation profiles provide more degrees of freedom to engineer the nonlocal response, and therefore may allow the realization of complex mathematical operation with high accuracy. we demonstrate this concept for first-and second derivative and integration via a metasurface based on split-ring resonators, 4 but similar responses can be obtained with other metasurfaces, such as based on dielectric resonators, which may be suitable for optical frequencies. our results pave the way towards the realization of efficient analog signal processing devices for edge detection and image processing operations [cit] ."
"regarding the self-restoring property of the cgp, it may occur in demanding situations, where due to the mobility phenomena, instead of dropping packets while routing tables change. thus, as previously concluded, a higher total delay average is expectable. moreover, when bottlenecks are avoided due to load-balancing, the re-routing process may also introduce a slight delay. however, as the cgp scheme is able to reach more challenging destinations than its competitors, the additional delay overhead is justifiable, being still suitable for many different applications."
"regarding the c-oslr protocol which is a clustered version of the olsr protocol, it achieves a greater stability when compared with the standard olsr. the number of topology changes registered by this protocol is only slightly higher than the ones obtained from the cgp. however, the overall performance of the c-olsr protocol regarding traffic delivery suggests that its ability to timely register important topology changes is not appropriate, resulting in wrong or outdated routing paths. on the other hand, the cgp awareness of the network is entirely different, detecting only the required amount of topology changes thus being more stable, leading to an increased traffic delivery performance, lower routing overhead and better energy efficiency."
"the three most important functionalities in dart are: first, the address allocation responsible for maintaining one routing address per network interface according to the movement and current position of a node; second, the routing which determines how to deliver packets from source to destination and, third, the node lookup which consists of a distributed lookup table in charge of mapping identifiers to network addresses."
"the paramount importance of scalable routing in wireless multi-hop networks has been stressed out by many recent works in the area of mobile ad-hoc networks (manets). in fact, in a near future, users are expected to be surrounded by thousands of wireless capable devices [cit], connecting people to their everyday objects, jobs, and hobbies."
"the mobility of nodes occurs not only within but also between clusters, issuing both micro and macro mobility phenomena that must be handled by the routing protocol. the presented results reflect these routing challenges and identify the advantages of deferred routing."
"in the manhattan model, nodes are geometrically separated, avoiding a large concentration of nodes and leading to less routing path alternatives. this aspect allows the olsr protocol to perform better than in other scenarios, but still with a large amount of losses. due to the group mobility defined by the nomadic mobility reference, the c-olsr and olsr protocols are able to generate less control overhead but the number of losses increases. the impact of mobility is also noticed in the random direction model where nodes only change direction when they reach the scenario boundaries, and in the random street map model which, similarly to the manhattan model, is based on the organization of the streets in a city."
"one key aspect of the presented hierarchy is related with the different perception that nodes have of the entire network and all the existing clusters. in fact, the nodes' membership to each cluster provides them a different network perspective according to their hierarchical position. figure 3a depicts the network organization as it is perceived by clusters 7 and 8, following the hierarchy previously presented. as sibling clusters, 7 and 8 recognize each other but acknowledge only two other clusters: 4 and 2. as previously explained, clusters 4 and 2 are the result of an aggregated view of the network, being themselves virtual clusters. in a real scenario, clusters 7 and 8 could be for instance two groups of people within a building, whereas cid 4 would correspond to a next door edification, being cluster 2 another infrastructur nearby."
"as previously mentioned, the effect of mobility on the performance of a routing approach is an important aspect to take into consideration. therefore, a thorough evaluation of the proposed scheme is provided, using six distinct mobility models and a static scenario, allowing a deeper understanding of how mobility is handled by cgp and how the cluster changes of nodes are processed, comparing the impact different patterns in the routing performance."
"its efficiency is demonstrated against two different routing approaches by performing an exhaustive simulation assessment in several scenarios with different mobility patterns. the obtained results reveal that the cgp routing scheme is successful in improving traffic performance delivery while reducing the required amount of routing traffic, regardless of the mobility pattern. in particular, regarding different mobility patterns, it has been shown that while the cgp is consistently efficient, the clustered version of the olsr protocol suffers from disruptions in some scenarios, having a higher overhead than the un-clustered version."
"as a 2-level hierarchy, this protocol can be scalable to a certain extent, however, the typical cluster formation and cluster-head election computational cost still exists. even though node mobility does not necessarily lead to inaccurate routing table calculations, as it would happen with a proactive approach, the inherent route retrieval propagation delay may lead to temporary loops. in a highly dynamic network where several flows may exist, the cgp protocol is more efficient as it is able to immediately initiate the packet-forwarding process, not requiring an expensive flooding for each flow."
"after having received an incoming flow from cluster 7, node n 1 processes it similarly to node s, verifying that g 2 is able to reach the desired destination with a single cluster hop. once the packets are received by n 2, the hierarchical perspective is changed and the forwarding path is narrowed. as illustrated by figure 6, node n 2 has a more precise knowledge about the destination's cluster and forwards the traffic to g 6 . as soon as packets arrive to cluster 27, node n 6 already knows the exact cluster to which the packets must be forwarded as it is a sibling cluster. the final forwarding steps are depicted in figure 7 ."
"the \"cluster-based olsr extensions to reduce control overhead in mobile ad hoc networks\", c-olsr protocol [cit], proposes an extension to olsr by introducing a cluster organized network. while this study does not define a clear hierarchy between nodes and clusters, the authors propose a scheme where the existing clusters are considered as nodes themselves, using the multipoint relays (mpr) concept, which was introduced by the olsr protocol, and apply it to clusters. this scheme results in a flat clustered routing approach even though it bears resemblance to hierarchical routing because of the existing mpr clusters."
"the previously described scenarios were simulated with a total of 30 runs per scenario, always using different seed values and the linear-congruential random number generator algorithm, for a total simulated time of 15 min (900 s). the considered wireless nodes follow the ieee 802.11g standard [cit] at 2.4 ghz, and have a maximum range of 100 meters (transmit power of 3.7e −4 w ) which corresponds to the maximum obtainable range of common wireless cards [cit] . however, due to the accurate radio model implemented by default in the opnet simulator, asymmetric links or even unidirectional links may occur, as well as channel errors and multi-path interferences, respectively. moreover, the ccir propagation model was used, configured to represent small to medium city with a building coverage of 15.8 percent, as it is considered one of the best propagation models [cit] . all other simulation parameters not mentioned here use their values set by default in the opnet modeler wireless suite simulator, version 16.0.a pl1."
"in figure 12, the average end-to-end delay is presented for all the analyzed mobility models. being the static scenario the only exception, in the remaining scenarios the cgp protocol presents a higher delay when compared against olsr and c-olsr. this aspect may not be desirable for certain types of traffic such as voice which are not well suited for ad-hoc networks. the explanation for the higher delay registered by the cgp is a consequence of the additional traffic delivery achieved, as an increased load of traffic is forwarded instead of being dropped."
"regarding this aspect, the aodv protocol provides the most efficient approach as it only requires route requests to be forward through the network until the destination node is reached. nonetheless, the expanded ring search problem, resulting from the flooding mechanisms used by aodv, has been known for generating a high overhead in challenging scenarios [cit], while the cgp maintains a stable behavior."
"having defined the required theoretical aspects of the proposed link quality estimator model, it is necessary to calculate and integrate the link quality estimation as a metric in the cgp."
"the routing traffic overhead introduced in the network is demonstrative of a protocols' own scalability, indicating how much information it needs to exchange in order to maintain its functionality. in figure 14, the number of sent routing control data corroborates once again the scalable properties of the cgp approach. in contrast, the olsr protocol tends to generate a higher routing load, being only better than the c-olsr protocol for the random street and nomadic mobility models. concerning the latter, the olsr protocol registers a significant decrease of sent routing traffic. however, it achieves the highest number of losses when compared to the remaining mobility models, indicating its failure in establishing routing paths. this shows that even though the number of nodes in a network directly impacts the scalability and overall performance of a protocol, the mobility model must not be disregarded as it also plays an important role in these aspects."
another important mathematical operation that in contrast to the second derivative has been less explored in the literature is the first derivative d dx α
"in fact, while the end-to-end delay is typically a result of a higher path length, as shown before this is not the case. specifically, when analyzing the manhattan scenario, where the highest hop count of the all mobile scenarios is registered for cgp, it has at the same time the lowest delay of the all mobile scenarios. this confirms that the approach taken by cgp, which registers less losses by sometimes using longer but more stable paths, is efficient and does not introduce delay by itself. the higher delay times are not registered in the manhattan model has the nodes follow well defined trajectories, being the additional delay overhead in the other mobile scenarios due only to repairing of broken paths, allowing the increased performance in traffic delivery registered by cgp."
"the creation of gateway information occurs only when a node in the vicinities of a neighbor cluster receives routing messages from other clusters. while routing messages from foreign clusters are typically discarded, the cgp uses these foreign messages to overhear the network topology information as perceived by other clusters. a http://jwcn.eurasipjournals.com/content/2013/1/86 foreign message is received when a routing message with different cid is received. this process, which extracts foreign information for use inside the gw's own cluster, is described in procedure 1."
"in the previous example, the destination node d is expected to remain static in cluster 28. however, if this node moved itself to a nearby cluster, only small parts of the forwarding procedure would have to be changed. this transparent way of dealing with mobility results from the usage of virtual clusters which enables a progressive or deferred routing discovery. for instance, assuming that the destination node moves to cluster 27, all the previous steps would be kept unchanged until node n 6 is reached, using the default intra-cluster routing tables to route the packets to node d. moreover, if any packets are sent to the destination node during the update of routing tables, no problem will be raised as nodes will automatically re-direct the packets back to the new destination's cluster."
"minimizing the path length is one typical target of routing protocols, with the purpose of reducing the network load and optimizing packet delivery. however, due to network dynamics, which is strongly influenced by node mobility, such routing approach may reduce the protocols' traffic delivery. regarding the number of hops in a perfect scenario, the maximum path length should be . in most of the scenarios, the cgp scheme is able to achieve a better path length than the remaining proactive protocols while maintaining lower losses, as depicted by figure 11 . nevertheless, for the manhattan, random waypoint and static mobility models, the cgp has a slightly higher path length. this is a consequence of the scenarios' specificities and of increased traffic performance of the cgp as it reaches more demanding destination nodes. thus, the trade-off between path length and traffic efficiency, in order to achieve an increased traffic performance, should be regarded as an important feature from cgp."
"as large-scale image processing becomes important in many technological areas, the demand for integrated, faster and more efficient devices that can manipulate optical signals and images becomes more and more relevant. currently, image processing is predominantly performed in the digital domain via integrated circuits, or in an analog fashion through bulky optical components [cit] . the first approach offers great versatility in the operations that can be performed, but it suffers from microelectronic limitations regarding operational speed and power consumption, which grow rapidly with the size of the images to be processed. the second approach can overcome these limitations, however at the expense of a significantly larger size, which is unattractive in modern optical systems."
"for example, if the operation frequency is aligned with the transmission zero of the fano resonance at normal incidence, due to the strong dispersion of the leaky mode associated with this resonance, the metasurface will exhibit a transmission coefficient that changes rapidly as we increase the incidence angle. this property can be seen in fig. 2a, in which the transmission changes from zero at normal incidence to unity at an incidence angle of 39 deg. this metasurface can be readily used to implement the second derivative we stress that the response described above is the optimal response achievable with passive metasurfaces, for which transmission is restricted to values smaller than unity. although ideally we would like α to be as large as possible, ensuring maximum edge enhancement, the fact that ( ) 1 directions (the flat segments resulting from the parabolic ones are hard to see, because they are obscured by the delta functions). this is exactly the response expected from a second derivative operation, which is also presented in fig. 2b, for the same cutoff angle as the metasurface. fig. 2c shows the effect of the metasurface on a sinusoidal signal. the output signal has the same form as the input (i.e., a sine/cosine function is transformed to a sine/cosine function), as expected from an ideal second derivative operation, also shown in the same figure."
"the time interval between a packet being sent and received depends not only on propagation characteristics, but also on the number of required packets sent until one is properly received, as depicted in figure 8 . figure 8a represents a link quality of 100% for t 1, t 2 and t 3, while in figure 8b, t 1 has a link quality of 100% and in t 2 the link quality is only of 50%. these errors are more prone to occur when a poor link quality is registered. therefore, by measuring the interval between consecutive hello messages, an estimation of the link quality can be retrieved using kernel regression estimation."
"the cgp stands out for exploring locality within a cluster while still being able to deliver packets in distant areas of the network. the provided routing scheme has some resemblances with both conventional and dtn routing as it establishes an end-to-end path when routing inside a cluster but it also uses a store-and-forward approach when routing between different clusters, without previously determining the entire path. in addition to this, since clustered networks are used in the existing hierarchy, the used gateway selection scheme, in conjunction with a kernel-based link quality estimator, will also allow load balancing of traffic in the network."
"in section 2, an overview of existing techniques used for scalable routing in wireless multi-hop networks is presented, followed by the description and specification of the cgp approach, presenting the overall concept in section 3. as the name indicates, cgp relies on gateway nodes for an efficient forwarding of packets between clusters, which in their turn rely on an efficient link quality metric defined in section 4. regarding the evaluation of the presented routing scheme, an assessment http://jwcn.eurasipjournals.com/content/2013/1/86 methodology is defined in section 5, presenting six distinct mobility models applied to 541 wireless nodes in a total area of 2.25 km 2 . the obtained results are presented in section 6, comparing them with a clustered and unclustered version of the olsr protocol. finally, in section 7, the final thoughts on this study are presented."
"obstacles. augmented reality is effective displaying technology which can provide dual information in a single display, overlapping computer-generated graphical information with real camera image. therefore the technology has the advantages on displaying navigational information directly on the screen and providing clear images when weather is not clear. homography matrix represents relationship between realworld and camera coordinate and it is commonly defined by a 4x4 matrix for 6 degree of freedom and this is needed four-points of a camera image at least. however we are hard to find detectable objects to define a specific plane in sea environment. therefore we use camera image as well as ais (automation identification system) information."
the clustering algorithm returns candidate horizon line and it have to be refined. we do second pixel profiling analysis in the randomly sampled images along the candidate line and extract the changing point from 0 to 1 or 1 to 0. fig. 4 shows a concept of random sampling and its image processing result.
"because of advantage of ar technology, various industries have been researching in their products, for example detecting lane vehicles and avoiding collision between aircraft. [cit] in case of marine application, many studies are suggested to find floating objects on sea surface, such as pixel profile analysis technique for horizon detection, lucas-kanade optical flow for extraction robust feature points [cit], automatic identification by using arpa radar for tracking vessels [cit] and so on. in ar, navigational object are overlaid on camera and are positioned in their real geographic location. other projects employing ar are galileo augmented maneuvering, galileo augmented rescue and galileo augmented logistics. they all exploit high accuracy and reliability of galileo, the europe's navigation satellite system."
"this paper introduces the process of finding ship's homography pose matrix using camera image and ais information. basically ship is always floating upon sea surface and nearby horizontal line. therefore if we could find horizon line, then we can define respective roll/pitch angle. and ais can provide azimuth and other ships' location. fig. 1 describes the process flow from cctv to final ar-navigation image."
"although various electronic marine navigational devices are introduced such as ecdis (electronic chart display and information system), enc (electronic navigational char) and gps (global positioning system), marine accident has been being reported that it is mainly caused by human errors such as missing obstacles and wrong making decisions. for example, a study of 6,091 accident claims over $100,000 concerning all cases of commercial ships which are conducted by the uk p&i club over a period of 15 years, reported that about 62% of the claims were known that is contributes to human errors [cit] and moreover atsb (australian transportation safety bureau) [cit], mais (marine accident investigation board) [cit] and tsb (transportation safety board of canada) [cit] are significantly including humans errors into accident-causes. most of navigator always looks down and analyzes the information of ecdis/radar and compares outside view information with the devices. however during this time, they are placed in blindness circumstance for outside information and sometimes it is caused to happen near accident like as collision or missing 1 please note that the lncs editorial assumes that all authors have used the western naming convention, with given names preceding surnames. this determines the structure of the names in the running heads and the author index."
"this paper describes the pose estimation process with camera images and ais information. and we use k-means algorithm and pixel profiling method to find horizon line in a navigating image as well as canny edge detector and hough transform. moreover we use ais data to estimate target ship position and use saliency map to find traffic ship in a camera image. and finally, we built an augmented reality based navigation scene image with those information. we conclude that the proposed algorithms are appropriated for shore environment and we can get proper horizon line and pose information. in this paper, all the algorithms examined in a still image however the algorithms can be used also in video stream, which is the subject of future research. and also, the proposed system is based on cctv and computer image processing therefore the performance is influenced by sea condition for example, the low light condition such as foggy, dark-night, heavy rainy days. therefore we intend to apply a thermal camera to acquire the consistent images in poor weather condition in the future research."
"a navigation scene is almost divided into two side of area, the first one is sky area and the other one is sea surface. hence almost of researches are starting their work with finding a horizon line. [cit] introduced an algorithm that uses regional covariance in a gray scale images. [cit] compared five algorithms for automatic detection of the horizon line in marine images. these five algorithms includes a method based edge detection and hough transform (h-edht), a covariance based method (h-cov), a new histogram based method (h-his) and two combinations of h-edht with the latter algorithms: h-edht-cov and h-edht-his. wang was use to pixel profile method to detect sea-coast line in an image, calculating differentiation between vertical pixels' intensity and oliver was used the external hardware (inertial power station) to define candidate area to analyze images. however if there are some obstacles upon horizon line such as a bridge, then it gives inaccurate result. fig. 2 describes the pixel profiling result and above a bridge and mast are interrupting pixel profiling, because the vertical pixels intensity has high gradient like as near area of horizon line."
"where a c-s is a fixed propagation delay between client c and server s; q(t) is the queue length of a single congested router, which is the home gateway in our use case; and ς is the transmission capacity of the router. indeed, q(t)/ς models the queuing processing delay. to comply with equation (6), we used the normal distribution with a mean value a c-s and a standard deviation equal to 0.07.a c-s . the standard deviation emulates the queuing processing delay q(t)/ς. this emulation is accomplished by using the \"netem delay\" parameter of the traffic controller tool in the gateway machine interfaces."
"the time needed to execute the first and the last phase is independent of the role. if a user with the public role accesses a judgment, the anonymization phase is an order of magnitude longer than the rest of the phases. the duration of all phases is in the same order of magnitude if a judgment is accessed by a user with the participant role. the anonymization phase for the role public is longer than the same phase for the role participant because it requires execution of anonymization and redaction methods (conversion to initials, hashing, replacing with ellipsis, and redaction) which take considerable time."
"in this paper, we have conducted a comparative evaluation between two gateway-based shaping methods, htbm and rwtm, employed to improve the has user's experience (qoe). the use case is defined as has clients being located in the same home network and competing for bandwidth. we define the same bandwidth manager in the gateway, and we present the testbed and its parameters to permit an accurate comparison. by defining objective qoe criteria, the comparative evaluation shows that rwtm is more beneficial; it is 37.3% more stable, 5.41 times more faithful to optimal quality level, and converges 3.4 times faster to the optimal quality level than htbm. the main explanation of this result is directly related to the additional queuing delay induced by htbm to shape has traffic, while rwtm just reduces the advertised window of has clients and thus does not add significant queueing delay."
in step (a) we have removed comparators and in step (b) comparators (note that in the full sorter is compared to all preceding elements but of them correspond to even-indexed elements whose corresponding comparators have already been removed in step (a)). hence we have comparators.
"modern communication standards sometimes allow the usage of very long block-lengths. the error-rate performance of polar codes under conventional sc decoding is significantly improved if the block-length is increased. however, a long block-length implies long decoding latency and large decoders. thus, an interesting question is whether it is better to use a long polar code with sc decoding or a shorter one with scl decoding, for a given target block-error probability. in order to answer this question, we first need to find some pairs of short and long polar codes which have approximately the same block-error probability under scl and sc decoding, respectively to carry out a fair comparison."
"theorem 1: for each path and each level let the path-metric be defined as: (10) where is the log-likelihood ratio of bit given the channel output and the past trajectory of the path . if all the information bits are uniformly distributed in, for any pair of paths, if and only if"
"two types of users have been identified according to the functions they perform and permissions they have for viewing judgments: participant (judge, clerk, accused and claimant) and public. all users can search archived documents and view search results according to their access control rights. the other roles in the archive system (e.g. archiver and security administrator) are not described in this paper since they are not relevant for anonymization and redaction use case."
"each judgment document consists of the header section, the body section and the conclusion section. the body section contains an introduction (the summary of the case), decision (the decision of the judge) and motivation (the argumentation of the judges). fragments of the document, which are interesting from the perspective of anonymization and redaction, are described in the rest of this section."
"luckily, we shall see that the decoding paths can still be ordered according to their likelihoods using all of the past decision llrs, and the trajectory of each path as summarized in the following theorem."
"we implemented a hardware architecture for an llr-based scl decoder and we presented synthesis results for various list sizes. our synthesis results clearly show that our llr-based scl decoder has a significantly higher throughput and lower area than all existing decoders in the literature, leading to a substantial increase in hardware efficiency of up to . finally, we showed that adding the crc unit to the decoder and using ca-scld is an easy way of increasing the hardware efficiency of our scl decoder at a given block-error probability as the list size can be decreased. specifically, our ca-scld at list size has somewhat lower block-error probability and more than five times better hardware efficiency than our standard scld at list size ."
"rtt c-s estimation is only possible when the has client sends a http get request message, as illustrated in fig. 1 . in fact, the has client will receive the http response message after one rtt c-s and will immediately send an ack packet. the time difference between the http get request message and the first ack message in the home gateway is close to the rtt c-s value because it satisfies equation (5). fig. 1 . rttc-s estimation in the home gateway accordingly, the home gateway will be able to estimate rtt c-s at the beginning of each chunk request. then, it applies equations (2) and (4) to update the rwnd c-s value."
"where is a 'hardware-friendly' function as it involves only the easy-to-implement operation (compared to which involves exponentiations and logarithms). for a hardware implementation of the sc decoder the update rule is replaced by . given, such an approximation is called the \"min-sum approximation\" of the decoder."
"the diagram in figure 2 shows main classes and their relations. the user class models users in the system, the role is represented by the role class, and the permission class defines permissions. the childroles-parentrole relation connects roles into the hierarchy, while the associations between user, role and permission establish appropriate assignments. the operation class models the operation for which permission is defined. anonymizereadoperation performs reading with enforcing anonymization and redaction rules. when parts of document should be anonymized or redacted it is necessary to define anonymization/redaction rules. the anonymizerule class is used to define those rules. it defines how replacement of the data (in the xml document) matched by the pattern attribute will be performed. specializations of the anonymizer class implement those anonymization rules. anonymizesearchoperation actually represents extension of anonymizereadoperation to prevent a user from searching non-anonymized and unredacted data. if a user searches for data, she/he may get a certain number of hits which include searching non-anonymized and unredacted data. those situations could compromise enforcement of anonymization and redaction rules. the purpose of anonymizesearchoperation is to prevent those cases."
"the gateway may have to compute the rtt c-s value exhaustively. since this may be a heavy processing task, our idea consists of only estimating rtt c-s from packets sent by the has client: this is called passive estimation of rtt [cit] . as a result, we define two parameters, rtt g-s and rtt c-g, which are the round trip time between the home gateway and the has server, and the round trip time between the has client and the gateway, respectively. since all packets circulating between the has client and has server pass through the home gateway, we can provide the following equation:"
"all llrs are quantized using a -bit signed uniform quantizer with step size . the path metrics are unsigned numbers which are quantized using bits. since the path metrics are initialized to 0 and, in the worst case, they are incremented by for each bit index, the maximum possible value of a path metric is . hence, at most bits are sufficient to ensure that there will be no overflows in the path metric. in practice, any path that gets continuously harshly penalized will most likely be discarded. therefore, as we will see in section vi, much fewer bits are sufficient in practice for the quantization of the path metrics."
"however, rwtm estimates the round trip time between the client and the server, rtt c-s, once for every chunk. this leads us to inquire about the robustness of the shaping method against rtt c-s instability in wireless home networks. this is a very valuable aspect of our research that should be investigated in future work."
"let the total number of cycles required for metric sorting at all information indices be denoted by . as we will see in section v-c, the sorting latency depends on the number of information bits and may depend on the pattern of frozen and information bits as well (both of these parameters can be deduced given ). then, our scl decoder requires (16) cycles to decode each codeword."
"the performance of the system was tested on the same corpus. we analyzed the time needed for executing loading a judgment from the database, anonymizing a judgment, and presenting a judgment to the user. the tests presented in this section were executed on a computer with i7 processor and 8gb of ram, running linux os, java 7 and apache tomcat 7 as a runtime environment. in the experimental setup, the server and the client are deployed on the same computer. on average, a judgment has 624 nodes, of which 194 nodes need to be anonymized or redacted (52 nodes converted to initials, 77 nodes hashed, 12 nodes replaced with ellipsis, and 53 nodes redacted)."
"since legal systems must be essentially public, that does not imply that all and any piece of information used in the judicial process must be published. at times, judges must have access to highly sensitive information that is not, and should not be made public. some of this information may concern the parties; some may relate to non-parties such as witnesses, jurors, and victims; and some may relate to third persons not involved in any way in the legal proceeding. in addition to sensitive information concerning private individuals and businesses, a broad category of sensitive information involves the operation of government [cit] . judgments are published in law reports or on the internet in order for lawyers to get acquainted with the case law and to provide data for legal research."
"in view of theorem 1, one can implement the scl decoder using parallel low-complexity and stable llr-based sc decoders as the underlying building blocks and, in addition, keep track of path-metrics. the metrics can be updated successively as the decoder proceeds by setting"
"in this work, we introduced an llr-based path metric for scl decoding of polar codes, which enables the implementation of a numerically stable llr-based scl decoder. moreover, we showed that we can simplify the sorting task of the scl decoder by using a pruned radix-sorter which exploits the properties of the llr-based path metric. the llr-based path metric is not specific to scl decoding and can be applied to any other tree-search based decoder (e.g., stack sc decoding [cit] )."
"this section describes the prototype of the judicial archival system with the focus on the anonymization and redaction of case law. this system is a part of a larger information system which is used to retrieve and browse legal norms and legislation [cit] . this system can be used by lawyers to access judgments (case law) that are made by applying retrieved legal norms (contained in a piece of legislation being browsed). therefore, this system provides retrieval of judgments of interest. the global architecture of the prototype system for archiving judicial documents is presented in figure 3 . this system is designed as a typical multi-tier application. the whole server-side of the system is implemented using the java open source technologies. users can search archive and read search results through the web interface. the archived documents are stored in the native xml database (exist), the documents\" metadata are archived in the rdf store (joseki), while anonymization and redaction policies are kept in the relational database (mysql). the archive uses the xslt processor (xalan) for converting xml documents to html, the xml db api library for accessing the native xml database, the java persistence api (jpa) for accessing the relational database (hibernate) and the rdf store (joseki). the anonymization and redaction is performed by the xxacf implementation. apache tomcat is used as the application server."
"and rwtm description in this section, we describe the bandwidth manager that defines the shaping rate for has clients in accordance with the optimal quality level definition used for qoe metrics in section i. we next provide a more accurate description of the rwtm method. for this purpose, we define in table i the parameters that are used in this section."
"in the modeled home network, the clients are connected to the gateway. the bandwidth is limited to 8 mbps. we selected this value because it is lower than twice the video encoding bitrate of the highest quality level. as a result, two clients in the home network cannot easily select the highest quality level at the same time. in this case, client 1 should select quality level 4 and client 2 should select quality level 3 as the optimal qualities defined by the bandwidth manager. we do not employ a use case in which two clients can select the same quality level, because this is a specific case in reality, and dissimilarity between optimal quality levels represents a more general case."
"after convergence, with htbm we have a high congestion rate: as presented in fig. 3, the majority of congestions is caused by retransmission timeout. in contrast, with rwtm, congestion events are negligeable: only two visible congestions occur in fig. 4 . as a consequence, ssthresh becomes lower when using htbm than when using rwtm: 75 mss in fig. 3 vs. 110 mss in fig. 4 . on the other hand, rwtm has frequent off* periods, with one off* period every 4.14 chunks, on average, unlike htbm, which shows pratically no idle periods in fig. 3 . in fig. 4, cwnd is divided by 8 after each off* period. based on algorithm 1, the off* period's duration is included in the interval [3 rto, 4 rto] . this observation leads us to compute the rtt variation between client 1 and server, rtt 1-s, for each test. rtt 1-s jumps from 100 ms to around 200 ms when using htbm, as presented in black in fig. 5 . in contrast, when using rwtm, rtt 1-s increases slightly and reaches approximately 120 ms, as presented in green in fig. 5 . consequently, approximately 100 ms of queuing delay is generated by htbm, in contrast to about 20 ms generated by rwtm. as result, on one hand, a large number of packets is buffered in the gateway queue when using htbm, and that increases the congestion rate. on the other hand, the high queuing delay generated by htbm has the advantage of reducing the frequency of off* periods by increasing rto duration: the ratio (off*)/rto is reduced. even if an off* period occurs, based on algorithm 1, the cwnd value will be divided, but will still be twice as high as when using rwtm."
"if the user has permissions to access whole (non-anonymized and unredacted) judgment, it will be displayed in the non-anonymized form (figure 4) . the elements of the judgment that must be anonymized and redacted are shown in bold."
we define three scenarios that describe the concurrence between has clients in the home network. we used only two has clients because this setup is easy to analyze and is sufficient to show the behavior of the competition for avail_bw:
"the channel llrs are fixed during the decoding process of a given codeword, meaning that an scl decoder requires only one copy of the channel llrs. these are stored in a memory which is words deep and bits wide. on the other hand, the internal llrs of the intermediate stages of the sc decoding (metric computation) process are different for each path . hence we require physical llr memory banks with memory positions per bank. all llr memories have two reads ports, so that all pes can read their two -bit input llrs simultaneously. here, register based storage cells are used to implement all the memories."
"(1) provides a more efficient work and labor saving; judgments anonymization and redaction has typically been a manual procedure. however, manual anonymization is labor-intensive and error-prone. publishers of anonymized and redacted judgments would benefit from the proposed solution for providing automated anonymization and redaction process."
"the objective of the present study is to improve the user's experience (qoe) when has clients compete for available home bandwidth. our methodology involves comparing between proposed methods, mainly gateway-based methods, by using three objective metrics of qoe in accordance with the description provided above. the remainder of this paper is organized as follows. in section ii, we describe and critique recent works addressing related methods. in section iii, we define a well-optimized bandwidth manager and describe the gateway-based shaping method receive window tuning method (rwtm) [cit] . section iv presents the experimental implementation that was used. section v provides a detailed evaluation of results. in section vi, we conclude the paper and suggest future directions to extend this work."
"first, we see that the scl decoders occupy an approximately larger area than their sc decoder counterparts. this may seem surprising, as it can be verified that an sc decoder for a code of length requires more memory (llr and partial sum) than the memory (llr, partial sum, and path) required by an scl decoder with list size for a code of length, and we know that the memory occupies the largest fraction of both decoders. this discrepancy is due to the fact that the copying mechanism for the partial sum memory and the path memory still uses crossbars, which occupy significant area. it is an interesting open problem to develop an architecture that eliminates the need for these crossbars."
"in fig. 5, we compare the fer of the scl decoder with that of the ca-scld for list sizes of, and, using the above-mentioned crcs."
"on the other hand, a sequential implementation of the computations enables a more hardware-friendly solution [cit], where each path has its own virtual internal llr memory, the contents of which are physically spread across all of the llr memory banks. the translation from virtual memory to physical memory is done using a small pointer memory. when a path needs to be duplicated, as with the partial sum memory, the contents of row of the pointer memory are copied to some row corresponding to a discarded path through the use of crossbars."
"xxacf supports the context-sensitive access control that may depend on multiple context factors. it can support context-sensitive access control through the condition class. if the condition assigned to the permission is satisfied, the permission will be applied, otherwise, it will not be the case. the cobacowlcondition class is used for representing context condition using appropriate ontology based on the ontology defined for the cobac model (context-sensitive access control model for business processes) [cit] . those conditions are used to specify different anonymization and redaction policies for users with the same user\"s role depending if they are participants of a particular judicial proceedings. for example, a user with the judge role can see her/his judgments in non-anonymized and unredacted form, but judgments of all other judges can see only in anonymized and redacted form. documents are anonymized and redacted when a user accesses them. the anonymization and redaction enforcement comprises of the following major steps:"
"we propose a testbed architecture, presented in fig. 2, that emulates our use case described in section i. the choice of only two clients is sufficient to demonstrate the behavior of the concurrence between many has flows in the same home network. in this paragraph, we describe the configurations of each component presented in fig. 2 : fig. 2 . architecture used in testbed  has server the has server is modeled by an http/1.1 apache server installed on a linux machine operating on debian version 3.2. it employs cubic [cit] as its tcp congestion control algorithm. all tests use five video quality levels denoted by 0, 1, 2, 3 and 4. their encoding rates are constant and equal to 248 kbps, 456 kbps, 928 kbps, 1,632 kbps, and 4,256 kbps, respectively. the playback duration of a chunk is 2 seconds."
"(where ). having the channel output, the receiver has all the required information to decode the input of the synthetic channel as, as, in particular, is a part of the known sub-vector . since this synthetic channel is assumed to be almost-noiseless by construction, with high probability. subsequently, the decoder can proceed to index as the information required for decoding the input of is now available. once again, this estimation is with high probability error-free. as detailed in algorithm 1, this process is continued until all the information bits have been estimated."
"http adaptive streaming (has) is a streaming video technique based on downloading video segments of short playback duration, called chunks, from a has server to a has client. each chunk is encoded at multiple encoding bitrates, also called video quality levels. after requesting a chunk using an http get request message [cit], the player on the client side stores the chunk into a playback buffer. the player operates in one of two states: buffering state and steady state. during the buffering-state, the player requests a set of chunks consecutively until the playback buffer has been filled. however, during the steady state, the player requests the chunks periodically to maintain a constant playback buffer size. this periodicity creates periods of activity, called on periods [cit], followed by periods of inactivity, called off periods [cit], without impacting the continuity of the video playing. the player selects the quality level for each chunk by estimating the available bandwidth during the previous on period."
"the xxacf extension for anonymization and redaction of xml documents is presented in this section. also, the overview of the core xxacf\"s entities is given in order to understand how anonymization and redaction can be implemented using xxacf."
each decoder core reads its input llrs from one of the physical llr memory banks based on an address translation performed by the pointer memory (described in more detail in section iv-d).
we used two linux machines as has clients. we developed a simple player in each client that reproduces the behavior of the has player without decoding and displaying a video stream.
"in the role-based access control (rbac) model, access to resources of a system is based on a role of a user in the system [cit] . rbac organizes individual users into roles according to their competency, authority, and responsibility within the enterprise and assigns permissions to roles according to their access rights. the core rbac model consists of: users, roles, permissions and sessions, where permissions are composed of operations applied to objects (resources). in rbac, roles are assigned to users, while permissions are assigned only to roles. a user\"s interaction with the system, where a user activates a subset of the roles which she/he is assigned to, is called a session [cit] . the main benefit of rbac is the ease of administration of security policies and its scalability."
"1-video quality level stability [cit] : a frequent change of video quality level bothers the user. therefore, quality level fluctuation should be avoided to improve the qoe."
"one may expect the pruned radixsorter to always outperform the radixsorter. however, the decoder equipped with the pruned radixsorter needs to stall slightly more often to perform the additional sorting steps after groups of frozen bits. in particular, a ( ) polar code contains groups of frozen bits. therefore, the total sorting latency for the pruned radixsorter is (see (19) ). thus, we have, which is an increase of approximately compared to the decoder equipped with a full radixsorter. therefore, if using the pruned radixdoes not lead to a more than higher clock frequency, the decoding throughput will actually be reduced."
"extensible xml role-based access control framework (xxacf) [31, 32, 33 ] enables access control definition and enforcement for xml documents. xxacf access control model is based on the rbac model, extended to support different granularity levels, and they may be content dependent, thus facilitating efficient management of access control. it supports access control for following operations executed on xml documents: reading, searching, creating, updating and deleting a document/document fragment. the most notable improvements over the reviewed xml access control methods include: context-sensitive access control based on the hierarchical rbac model, document-dependent definition of access control policies on different priority and granularity levels, and support for separate access control enforcement for different operations on documents and different ways of implementing the same operation."
"in order to explain the cause of these results, we used the tcp_probe module in the has server. this module shows the evolution of the congestion window, cwnd, and slow start threshold, ssthresh, during each test. we selected two tests that have the nearest qoe measurements of the first column of table iii related to htbm and rwtm, respectively. we present the cwnd and ssthresh variation of the two tests in fig. 3 and 4 . in the two figures, we indicate by a vertical bold dotted line the moment of convergence. we observe that this moment coincides with a considerably higher stability of ssthresh and higher stability of cwnd in the congestion avoidance phase. in order to be accurate in our analyses, based on the cubic [cit] congestion control algorithm used in the server and its source code tcp_cubic.c, we present some important value updates of cwnd and ssthresh for different events:"
the round-trip time between client c and server s rwndc-s the rwnd value defined by rwtm in the tcp ack packet sent from client c to server s
we define the infidelity metric if c (k) of client c for a test duration of k seconds. it measures the portion of time during which the has client c requests optimal quality.
"the pruned radix-2l sorter presented in this section reduces the complexity of the sorting logic of the radixsorter and, thus, the maximum path delay, by eliminating some pairwise comparisons whose results are either already known or irrelevant."
"the crc unit contains -bit crc memories, where is the number of crc bits. a bit-serial implementation of a crc computation unit is very efficient in terms of area and path delay, but it requires a large number of clock cycles to produce the checksum. however, this computation delay is masked by the bit-serial nature of the scl decoder itself and, thus, has no impact on the number of clock cycles required to decode each codeword. before decoding each codeword, all crc memories are initialized to -bit all-zero vectors. for each, the crc unit is activated to update the crc values. when decoding finishes, the crc unit declares which paths pass the crc. when a path is duplicated the corresponding crc memory is copied by means of crossbars (like the partial sums and the path memory)."
"akoma ntoso is the set of principles for electronic parliamentary services in a panafrican context [cit] . it has several goals: to define common data exchange standard between parliaments, to specify a basic document model that can be used to build information system and to define simple citation mechanism. the document model aims to provide a long-term storage of and access to parliamentary, legislative and judicial documents that allows search, interpretation and visualization of documents [cit] . recently, the oasis\"s legaldocml technical committee has started work on structured standards in the legal field that are based on akoma ntoso [cit] . akoma ntoso separates content of the document, its presentation and its metadata and it uses xml design patterns such as hierarchy, container, block element, inline element, marker, etc."
"however, the successful anonymization and redaction of documents is dependent of their proper markup. a potential drawback of the presented prototype is that it can anonymize and redact judgments incorrectly if data that needs to be anonymized and redacted is not marked up according to the xml format in use."
"many research studies have been conducted to improve the qoe when several has clients are located in the same home network. the methodology most often employed involved avoiding false estimations of the available bandwidth during on periods in order to more effectively select the video quality level. three types of solutions were proposed to improve has user qoe: client-based, server-based and gateway-based solutions. these differ with respect to the device in which the solution is implemented. below, we cite relevant methods for each type of solution:"
"an interesting question, which is, to the best of our knowledge, still unaddressed in the literature, is whether it is better to use sc decoding with long polar codes or scl decoding with short polar codes. in section viii we study two examples of long polar codes that have the same block-error probability under sc decoding as our ( ) modified polar codes under crc-aided scl decoding. by comparing the synthesis results of the corresponding decoders, we observe that, while the scl decoders have a lower throughput due to the sorting step, they also have a significantly lower decoding latency than the sc decoders."
"as discussed in section ii-c, the performance of the scl decoder can be significantly improved if it is assisted for its final choice by means of a crc which rejects some incorrect codewords from the final set of candidates. however, there is a trade-off between the length of the crc and the performance gain. a longer crc, rejects more incorrect codewords but, at the same time, degrades the performance of the inner polar code by increasing its rate. hence, the crc improves the overall performance if the performance degradation of the inner polar code is compensated by rejecting the incorrect codewords in the final list."
"however, from table vii we see that the scl decoders have a significantly lower per-codeword latency. more specifically, the scl decoder with and has a lower per-codeword latency than the sc decoder with, and the scl decoder with and has a lower per-codeword latency than the sc decoder with . thus, for a fixed fer, our llr-based scl decoders provide a solution of reducing the per-codeword latency at a small cost in terms of area, rendering them more suitable for low-latency applications than their corresponding sc decoders."
"having shown (13), theorem 1 will follow as an immediate corollary to lemma 1 (since the channel output is fixed for all decoding paths). since the path index is fixed on both sides of (10) we will drop it in the sequel. let (the last equality follows since ), and observe that showing (13) is equivalent to proving (14) since (15) repeated application of (15) (for ) yields"
"in order to construct a polar code of rate and block length for a channel, the indices of the least noisy synthetic channels are selected as the information indices denoted by . the sub-vector will be set to the data bits to be sent to the receiver and, where, is fixed to some frozen vector which is known to the receiver. the vector is then encoded to the codeword through (1) using binary additions (cf. [1, section vii]) and transmitted via independent uses of the channel ."
"the introduction section summarizes the case (listing 5). the element party is of particular importance since it contains the name of the party that took part in the proceedings (the defendant in this particular case), and a link to the concept in the ontology that specifies it. the content of this element as well as the content of its refersto attribute needs to be anonymized."
"feeding this list to the pruned radix-sorter will result in an output list of the form zeros where is the ordered permutation of . proof: it is clear that the assumptions (17a) and (17b) hold for . the proof of proposition 1 shows if the last element of the list is additionally known to be the largest element, the pruned radix-sorter sorts the entire list."
"since the values of those attributes usually correspond to personal name, it is necessary to anonymize them. it is also necessary to preserve the referential integrity after anonymization. therefore, the values of those attributes are replaced with their hmac (hash-based message authentication code) value [cit] . the similar case is applied to elements person and location. the content of the date and the inline (which contain personal identifier) elements has to be replaced with the ellipsis."
"in cases where anonymization and redaction rules are not context-dependant, the system\"s performance can be optimized by creating anonymized documents offline and providing those documents to users."
"the sc decoder, in contrast, finds a sub-optimal solution by maximizing the likelihood via a greedy one-time-pass through the tree: starting from the root, at each level, the decoder extends the existing path by picking the child that maximizes the partial likelihood . 3) decoding complexity: the computational task of the sc decoder is to calculate the pairs of likelihoods needed for the decisions in line 5 of algorithm 1. since the decisions are binary, it is sufficient to compute the decision log-likelihood ratios (llrs),"
"for the third scenario, we observe that the instability measurements of the three methods are similar. this result is expected because the client operates alone for 150 seconds, and therefore is easier to stabilize. moreover, htbm has the worst instability and convergence speed measurement. additionally, rwtm maintains practically the same qoe measurements of scenario 1."
"finally, we show that a crc-aided scl decoder can be implemented by incorporating a crc unit into our decoder, with almost no additional hardware cost, in order to achieve significantly lower block-error probabilities. as we will see, for a fixed information rate, the choice of crc length is critical in the design of the modified polar code to be decoded by a crc-aided scl decoder. in section vi-e we provide simulation results showing that for small list sizes a short crc will improve the performance of scl decoder while larger crcs will even degrade its performance compared to a standard polar code. as the list size gets larger, one can increase the length of the crc in order to achieve considerably lower block-error probabilities."
"moreover, we observe that both sc decoders can achieve a slightly higher operating frequency than their corresponding scl decoders, although the difference is less than . however, the per-bit latency of the sc decoders is about smaller than that of the scl decoders, due to the sorting step involved in scl decoding. the smaller per-bit latency of the sc decoders combined with their slightly higher operating frequency, make the sc decoders have an almost higher throughput than their corresponding scl decoders."
"finally, for we observe that crc-of (21c) is the best candidate among the three different crcs in the sense that the performance of the ca-scld which uses this crc is significantly better than that of the decoders using crc-or crcfor db, while all three decoders have almost the same fer at lower snrs (and they all perform better than a standard scl decoder)."
"in fig. 4, we present the fer of floating-point and fixed-point implementations of an ll-based and an llr-based scl decoder for a ( ) polar code as a function of snr. 7 for the floating-point simulations we have used the exact implementation of the decoder, i.e., for computing the llrs the update rule of (8a) is used and the path metric is iteratively updated according to (11) . in contrast, for the fixed-point simulations we have used the min-sum approximation of the decoder (i.e., replaced with as in (9)) and the approximated path metric update rule of (12) ."
"the href attribute of the tlcperson and tlclocation elements refers to the proper information about person/location in the metadata database. therefore, it is necessary to remove that reference in order to completely anonymize document. the permissions in table 3 deny access to those attributes of the public role."
"we assume that the sorting procedure is carried out in a single clock cycle. a decoder based on the full radixsorter, only needs to sort the path metrics for the information indices, hence, the total sorting latency of such an implementation is cycles (18) using the pruned radix-sorter, additional sorting steps are required at the end of each contiguous set of frozen indices. let denote the number of clusters of frozen bits for a given information set . 6 the metric sorting latency using the pruned radix-sorter is then cycles (19) vi. implementation results"
"if the crc unit is present, the codeword selection unit selects the most likely path (i.e., the path with the lowest metric) out of the paths that pass the crc. otherwise, the codeword selection unit simply chooses the most likely path."
"in fig. 6 (a) we see that a ( ) polar code has almost the same block-error probability under sc decoding as a ( ) modified polar code under ca-scld with list size and crc-of (21a). similarly, in fig. 6(b) we see that a ( ) polar code has almost the same block-error probability under sc decoding as an ( ) modified polar code decoded under ca-scld with list size and crc-of (21b)."
"besides the comparators, the pruned radix-sorter requires -to-multiplexers (see fig. 3(b) ). the pruned radix-sorter is derived based on the assumption that the existing path metrics are already sorted. this assumption is violated when the decoder reaches the first frozen bit after the first cluster of information bits; at each frozen index, some of the path-metrics are unchanged and some are increased by an amount equal to the absolute value of the llr. in order for the assumption to hold when the decoder reaches the next cluster of information bits, the existing path metrics have to be sorted before the decoding of this cluster starts. the existing pruned radix-sorter can be used for sorting arbitrary positive numbers as follows."
"2) path memory: the path memory consists of -bit registers, denoted by . when a path needs to be duplicated, the contents of are copied to, where corresponds to an inactive path (cf. line 25 of algorithm 3). the decoder is stalled for one clock cycle in order to perform the required copy operations by means of crossbars which connect each with all other . the copy mechanism is presented in detail in fig. 2(b), where we show how each memory bit-cell is controlled based on the results of the metric sorter. after path has been duplicated, one copy is extended with the bit value, while the other is updated with (cf. lines 26 and 27 of algorithm 3)."
"the metadata are organized into several groups: identification, publication, analysis and references. the metadata belonging to the identification group identifies documents at different frbr levels (listing 1). the frbrthis element contains the uri of the specific document\"s component, frbruri contains the uri of the whole document, frbrdate contains a relevant date of the document and the frbrauthor element contains a relevant author of the document at the particular frbr level."
"proposition 1: it is sufficient to use a pruned radix-sorter that involves only comparators to find the smallest elements of . this sorter is obtained by (a) removing the comparisons between every even-indexed element of and all following elements, and (b) removing the comparisons between and all other elements of . proof: properties (17a) and (17b) imply for . hence, the outputs of these comparators are known. furthermore, as we only need the first elements of the list sorted and is never among the smallest elements of, we can always replace by (pretending the result of the comparisons involving is known) without affecting the output of the sorter."
"2) throughput reduction: adding bits of crc increases the number of information bits by, while reducing the number of groups of frozen channels by at most . as a result, the sorting latency is generally increased, resulting in a decrease in the throughput of the decoder. in table vi we have computed this decrease in the throughput for different decoders and we see that the crc-aided scl decoders have slightly (at most ) reduced throughput. for this table, we have picked the best decoder at each list size in terms of hardware efficiency from table iii. 3) effectiveness of crc: the area of the crc unit for all synthesized decoders is in less than m for the employed tsmc 90 nm technology. moreover, the crc unit does not lie on the critical path of the decoder. therefore, it does not affect the maximum achievable operating frequency. thus the incorporation of a crc unit is a highly effective method of improving the performance of an scl decoder. for example, it is interesting to note that the ca-scld with has a somewhat lower fer than the standard scl decoder with (in both floating-point and fixed-point versions) in the regime of db. therefore, if a fer in the range of to is required by the application, using a ca-scld with list size is preferable to a standard scl decoder with list size as the former has more than five times higher hardware efficiency."
"arıkan shows that as, these synthetic channels polarize to 'easy-to-use' b-dmcs [1, theorem 1] . that is, all except a vanishing fraction of them will be either almost-noiseless channels (whose output is almost a deterministic function of the input) or useless channels (whose output is almost statistically independent of the input). furthermore, the fraction of almost-noiseless channels is equal to the symmetric capacity of the underlying channel-the highest rate at which reliable communication is possible through when the input letters are used with equal frequency [cit] ."
"in an extended version of their work [cit], tal and vardy observe that when the scl decoder fails, in most of the cases, the correct path (corresponding to ) is among the paths the decoder has ended up with. the decoding error happens since there exists another more likely path which is selected in line 19 of algorithm 2 (note that in such situations the ml decoder would also fail). they, hence, conclude that the performance of polar codes would be significantly improved if the decoder were assisted for its final choice."
"this paper proposes a method for anonymization and redaction of judgments represented in the akoma ntoso format. this method is based on the anonymization and redaction extension of the xxacf framework. the system\"s prototype has been evaluated in magistrate court in novi sad, the republic of serbia. the proposed method represents a successful application of the context-dependent role-based xml access control for anonymization and redaction of judicial documents. when judicial proceedings are completed, final judgments are made and archived. as far as the described solution is concerned, both archived and non-archived judgments can be anonymized and redacted in the same manner. however, according to the rules of the court proceedings, only the final (and subsequently archived) judgments can be published. judgments that are being drafted are not publicly accessible."
"we also note that is the direction that the llr (given the past trajectory ) suggests. this is the same decision that a sc decoder would have taken if it was to estimate the value of at step given the past set of decisions (cf. line 5 in algorithm 1). equation (12) shows that if at step the th path does not follow the direction suggested by it will be penalized by an amount . having such an interpretation, one might immediately conclude that the path that sc decoder would follow will always have the lowest penalty hence is always declared as the output of the scl decoder. so why should the scl decoder exhibit a better performance compared to the sc decoder? the answer is that such a reasoning is correct only if all the elements of are information bits. as soon as the decoder encounters a frozen bit, the path metric is updated based on the likelihood of that frozen bit, given the past trajectory of the path and the a-priori known value of that bit (cf. line 6 in algorithm 3). this can penalize the sc path by a considerable amount, if the value of that frozen bit does not agree with the llr given the past trajectory (which is an indication of a preceding erroneous decision), while keeping some other paths unpenalized. we devote the rest of this section to the proof of theorem 1."
"the participant role can perform read and search of the whole document (without being anonymized) only if she/he participated in a judicial process that resulted in the particular judgment document ( table 1 ). the condition in the table verifies whether the current user was a judge who made the judgment or one of the parties. the function currentuser(), used in the condition, returns the identifier of the logged user, and the xpath() function returns values selected by the given xpath expression in the current document. the xpath expression //judge/@refersto selects the referesto attribute of all judge elements, while the xpath expression //party/@refersto selects the referesto attribute of the all party elements in the document. the attribute referesto of the party element contains the identifier of the corresponding user of the organization (see section 2). thus, the subcondition currentuser() in xpath(//judge/@ refersto) checks if the current user identifier corresponds to the identifier of at least one judge. the second subcondition, currentuser() in xpath(//party/@ refersto), checks if the current user identifier corresponds to the identifier of at least one party."
"even though the block-error probability of polar codes under sc decoding decays roughly like as a function of the block-length [cit], they do not perform well at low-to-moderate block-lengths. this is to a certain extent due to the sub-optimality of the sc decoding algorithm. to partially compensate for this sub-optimality, tal and vardy proposed the successive cancellation list (scl) decoder whose computational complexity is shown to scale identically to the sc decoder with respect to the block-length [cit] ."
"1. both clients start to play simultaneously and continue for 3 minutes. this scenario shows how clients compete. 2. client 1 starts to play, and after 30 seconds, the second client starts; both continue together for 150 seconds. this scenario shows how a transition from one client to two clients occurs. 3. both clients start to play simultaneously, and after 30 seconds, we stop client 2; client 1 continues alone for 150 seconds. this scenario shows how a transition from two clients to one takes place. the choice of test duration of 3 minutes has an objective to offer sufficient delay for players to stabilize."
"in this section, we discuss the qoe measurements of the two shaping methods during the three scenarios, and we provide a more precise explanation of the observed results by showing the congestion window variation."
"as can be observed in table iii, this is exactly the case for, where the llr-based scl decoder with the pruned radix-sorter has a lower throughput than the llr-based scl decoder with the full radix-sorter. however, for the metric sorter starts to lie on the critical path of the decoder, therefore, using the pruned radixsorter results in a significant increase in throughput of up to for . to provide more insight into the effect of the metric sorter on our scl decoder, in table iv we present the metric sorter delay and the critical path start-and endpoints of each decoder of table iii . the critical paths for and are also annotated in fig. 2(a) with green dashed lines and red dotted lines, respectively. we denote the register of the controller which stores the internal llr memory read address by . moreover, let and denote a register of the partial sum memory and the metric memory, respectively. from table iv, we observe that, for, the radixsorter does not lie on the critical path of the decoder, which explains why using the pruned radixsorter does not improve the operating frequency of the decoder. for the metric sorter does lie on the critical path of the decoder and using the pruned radix-sorter results in a significant increase in the operating frequency of up to . it is interesting to note that using the pruned radix-sorter eliminates the metric sorter completely from the critical path of the decoder for . for, even the pruned radixsorter lies on the critical path of the decoder, but the delay through the sorter is reduced by ."
"the average time of loading a judgment from the database, anonymizing a judgment, and presenting a judgment to the user phases executed by public and participant roles is presented in figure 6 . same data, along with standard deviation, is shown in table 4 ."
"2) general case: variable rtt when rtt c-s becomes variable over time, rwtm should update the rwnd c-s value to maintain the same shaping rate as described in equation (4):"
"so, on one hand, htbm delays packets considerably, which causes a high congestion rate and, by consequence, lower convergence speed, but it practically eliminates off* periods. on the other hand, rwtm does not generate congestions, but concerning off* periods, rwtm reduces their frequency but does not eliminate them. the cause is the non-exhaustive estimation of rtt c-s, i.e. limited to only one estimation per chunk, which may induce a low conformity of the shaping rate computation to the shr c-s value defined by the bandwidth manager."
"the metadata belonging to the references group (see listing 4) contains the list of locations (tlclocation), organizations (tlcorganization), persons (tlcperson), roles (tlcrole), etc. referenced from the document and relevant to understanding its content."
"(3) customizable for different anonymization and redaction rules; since anonymization and redaction rules are expressed declaratively, there is no need to change the implementation of the proposed system to support new anonymization and redaction rules."
"to this end, we note that the real numbers that have to be sorted in line 13 of algorithm 3 are not arbitrary; half of them are the previously existing path-metrics (which can be assumed to be already sorted as a result of decoding the preceding information bit) and the rest are obtained by adding positive real values (the absolute value of the corresponding llrs) to the existing path metrics. moreover, we do not need to sort all these potential path metrics; a sorted list of the smallest path metrics is sufficient. hence, the sorting task of the scl decoder can be formalized as follows: given a sorted list of numbers a list of size, is created by setting and where, for . the problem is to find a sorted list of smallest elements of when the elements of have the following two properties: for,"
"for the second scenario, we observe an improvement of qoe measurements of rwtm but a degradation of htbm. this finding suggests that htbm is highly disturbed by the increase of has clients from one to two clients."
"the decision section (listing 6) gives a detailed overview of the judicial decision. this section is the most important one with regards to anonymization, because most of the personal data is contained in it. apart from the content of the party element, contents of the elements person, date, location and inline could also be used to identify a defendant. therefore, those elements and their attributes need to be anonymized as well."
"we decided to use akoma ntoso because it entered the oasis standardization process [cit] which aims to provide interoperability between different legal information systems and reusability of software based on it. another important reason is that akoma ntoso separates content, presentation and metadata and uses xml design patterns. akoma ntoso has been used as is, without modifications, since it supports our national judgments drafting guidelines and anonymization rules."
"the bandwidth manager is sufficiently sophisticated to be able to update the number of active connected has clients in the home gateway, n, by sniffing the syn and fin flags in the headers of tcp packets. we also assume that it is capable of estimating the avail_bw and updating its value over time. accordingly, the manager updates the shaping rate when any change occurs."
"note that while the same comparator network of a pruned radix-sorter is used for sorting numbers, separate -to-1 multiplexers are required to output the sorted list."
"although the standard rbac model identifies granting policies only, the literature (see section 1) identifies the need for denying policies to achieve more efficient xml access control. the permission type (granting or denying) of a policy is specified with by the type attribute of the permission class. to avoid explicit permission definition for each entity, propagation of permissions is enabled, starting from the point specified by resource down or up the hierarchy. the permission\"s propagation direction is identified by the propagationdirection attribute. the maximum number of xml hierarchy levels where the propagation is performed (propagation level), is defined by the propagationlevel attribute."
"(2) public access to judgments; using limited resources available to courts, it is possible to publish a larger number of judgments with automated anonymization and redaction process than by manual anonymization and redaction methods."
"since existing format was used, the representation of a judgment was straightforward. firstly, document was identified at different frbr (functional requirements for bibliographic records) [cit] levels as an url according to akoma ntoso guidelines. then, the important metadata were identified and serialized into the metadata section of the document. at the end, the document structure was marked up using standardized set of elements."
"since the permissions in table 2 grant access and the permissions in table 3 deny it, the attribute href of tlcperson/tlclocation will be assigned to both granting and denying permissions. according to the conflict resolution principle \"more specific object takes precedence\" (see section 3) the final permission for this attribute will deny access. the denying permissions in table 3 are more specific than the granting permissions in table 2, because they are defined for more specific entity (the permissions in table 2 are defined for the whole document, while the permissions in table 3 are defined for the specific attribute), and therefore they are selected as the final permissions."
"2) sc decoding as a greedy tree search algorithm: let (5) denote the set of possible length-vectors that the transmitter can send. the elements of are in one-to-one correspondence with leaves of a binary tree of height : the leaves are constrained to be reached from the root by following the direction at all levels . therefore, any decoding procedure is essentially equivalent to picking a path from the root to one of these leaves on the binary tree."
"2-fidelity to optimal quality level selection: the user prefers to watch the best quality level of video available. accordingly, the has player should select the optimal quality level that has the highest feasible quality level permitted by the available bandwidth. 3-convergence speed [cit] : the user prefers to achieve watching the optimal quality level as soon as possible. therefore, the has player should efficiently select this level. the player's delay to attain the optimal quality level is called the convergence speed [cit] ."
"in the context of has, an idle period coincides with an off period between two consecutive chunks. we denote by off* the off period whose duration exceeds rto."
"having transformed identical copies of a 'moderate' b-dmc into 'extremal' b-dmcs, arıkan constructs capacity-achieving polar codes by exploiting the almost-noiseless channels to communicate information bits."
"the rest of this paper is structured as follows. the next section describes the akoma ntoso document format and how akoma ntoso can be used in our national legal system. the third section presents the extension of the extensible xml role-based access control framework (xxacf) for anonymization and redaction. the application of xxacf for akoma ntoso documents is given in section 4. the fifth section presents the judicial archive with the focus on the anonymization and redaction of case law. in the conclusion, strengths and weaknesses of this approach are elaborated on, and directions of further research are given."
this metric describes the duration of time that the player of has client c requires to reach a stable optimal quality level for at least t seconds for a test duration of k seconds.
"the resource class is used to represent resources for which permissions are defined. the permission can be defined for a document schema (the class documentschema) or a document instance (the class documentinstance) identified by its unique identifier (the id attribute of the resource class). if permissions are defined for a document schema they are applied to all document instances of that schema. on the other hand, permissions are applied only to an instance if they are defined at the instance level. the xpath expressions are used to define permission for fragments of documents or schemas. it is also possible to define content dependent permissions by xpath expressions which contain condition. the documentinstance's or documentschema's attribute fragment denotes the fragment of a document instance or schema for which the permission is defined. by using document fragment it is possible to define anonymization and redaction rules for specific parts of documents."
"1. loading of user properties and roles -when the system accepts the request, it loads user\"s properties from the database and assigns roles to the user. 2. selection of the applicable permissions -in this step the system finds anonymization and redaction policies which will be applied. 3. marking document nodes -this is a process of applying permissions determined in the previous step to the nodes of dom (document object model) representation of xml document selected by those permissions. 4. conflict resolution -it might be necessary to resolve conflicts since both policy types can be applied to the same nodes. as a result of this phase, only policies of one type (granting or denying) will be applied on the particular node. 5. execution of the requested operation -the operation execution depends on the operation\"s type. the read operation retrieves only parts of the document that are allowed to be read by the user. if an anonymization and redaction rule is defined for some segment of the document, it will be removed or replaced. the search operation is performed in the same way as the read operation and it also performs anonymization and redaction rules when counting and displaying search results."
"in the position paper [cit] we have proposed an idea that judgments, represented in the xml format, can be automatically redacted and anonymized using access control principles. this paper elaborates on the given idea and describes in detail how an xml access control solution can be adopted for judgments redaction and anonymization. since majority of archival systems have access control features, we believe that anonymization and redaction can be implemented more easily and more efficiently by extending existing access control modules than as a separate module."
"the above workflow process gives a clear path from exploration to dissemination [cit], and bridges the gap between visualization tools and traditional presentation tools such as ms powerpoint."
"in the example in figure 3 (a), the user is studying a financial dataset of the hong kong stock exchange (hngkngi) [cit] . he first creates a child strip for the black monday crash of october 19, 1987 by dragging on the main strip (see figure 3(b) ). [cit] (figure 3(c) ). a continued description of this usage scenario can be found in section 5."
"the z-stack structure supports all basic tree operations such as finding the depth of individual nodes, the depth of the whole tree, the number of children, as well as all standard tree traversals."
"the traxplorer exploration interface (figure 2 ) consists of the main visualization window, the data box for getting details and statistics about the dataset, and the layer control box for managing the visual display of the time series data. traxplorer was designed for loading several tracks of time-series data (defined as comma-separated files, or similar) into the same visual space. the main visualization window is a visual space supporting stack zooming. it contains a set of visualizations of time-series data on a common time axis and potentially different value axes. the type of visualization is independent of the layout management-our implementation currently supports basic line graphs, filled line graphs, and horizon graphs [cit] . for line graphs, multiple time series will be drawn as tracks overlaid on the same temporal axis, whereas for filled line graphs and horizon graphs, the charts will be juxtaposed."
"in our work, we take the history concept a step further by utilizing the history as a starting point for creating a presentation of the exploration. the analyst can prune, move, or hide individual nodes (i.e. child strips) in the presentation tree to refine the presentation, while at the same time being able to extend the presentation by continuing exploration at any time, even during the actual presentation. this feature is important to allow transitioning between exploration and presentation. also, by selectively hiding and showing parts of the zoom stack using the presentation tree interface, analysts can better structure their narrative while presenting their results."
"to further support visual exploration, we also provide a data box that gives local statistics about the currently selected strip in the zoom stack. this provides detail-on-demand for computing measures such as minimum, maximum, average, median, and standard deviation metrics for a particular track. the data box also doubles as a presentation tool; this is discussed more closely in the next section. in addition, users can always hover their pointer over a time series point to retrieve exact data about the point in a tooltip."
"consider a user trying to use line graph visualizations (figure 4 ) to analyze a large-scale temporal dataset. because such datasets often contain more data points than can be fit on the screen, the user will need to zoom and pan in the line graph to see details necessary to solve a particular task. however, using just a sequence of zoom and pan actions to support higher-level temporal analysis tasks such as comparison and correlation over time [cit], both between different time series and within the same time series, will quickly become tedious and ineffective [cit] . with the stack zooming technique, the available display space is split evenly among the currently visible line charts, or strips, that show subsets of the time-series dataset. when the user begins to analyze the dataset, the whole display is taken up by the full time series drawn as a line visualization on a single strip. using a mouse drag on the surface of this strip, the user can create a child strip of the main strip that displays the selected subset of the time data. additional zoom operations on the main dataset strip will create additional children in the zoom stack, all of them allocated an equal amount of the available display space for that particular level (space allocations can be changed by dragging the borders of a strip). each child strip serves as a focus, and is guaranteed to always be visible."
"mere images may not be enough for an effective presentation, so the data box that supports details-on-demand in the exploration phase also doubles as an annotation mechanism. checkboxes in the data box for each computed and user-supplied metric (i.e. annotation) allows the user to toggle display of these metrics on the actual visualization strip. in other words, this functionality can add local statistics-such as a key, the extrema, or the average of a particular data track-to the visual display of the track."
"we designed the stack zooming technique to be an alternative to existing multi-focus interaction techniques such as rubbersheet displays [cit], accordion drawing [cit], and space-folding [cit] . the tradeoff with stack zooming in comparison to these is that the competing techniques generally provide integrated focus and context, whereas stack zooming lays out focus points as separate viewports on the visual space. it may even be argued that stack zooming is not a true multi-focus interaction technique because of this. however, our intention was to provide a distortion-free display that retains the familiarity of the original visual representations drawn on the display, and that is what distinguishes the technique from its competitors. we are naturally interested in comparing stack zooming to these techniques through empirical user studies."
"to show how traxplorer could be utilized to analyze a particular dataset, we return to our stock market analysis example. our analyst, joe, is part of a team of financial analysts studying the history of stock market crashes to predict future behavior."
"once done with his exploration of the two datasets, joe decides to share his findings with his colleagues. to add more force to his points, he uses the presentation interface of the traxplorer system to explain his exploration process. in doing so, joe can use the system itself to prune away unused exploration branches and streamline his presentation. in addition, as he is communicating his findings, he is able to return to the exploration phase at any time as the discussion among the team members progresses. during the discussion, the team observes that markets remain unstable for next few months following any major crash. after their discussion, the team collectively decides on a linear presentation sequence for an upcoming board meeting where joe will use traxplorer to share"
"one of the basic requirements of effective temporal exploration is to allow the user to navigate the focus points through the dataset. we have already described the zoom functionality, which for stack zooming-unlike traditional visualization applications-is not a navigation operation in itself, but used to create new focus regions. therefore, the zoom operation is intrinsic to stack zooming. instead, the main navigation operation is panning-or movinga focus point. in our technique, moving a focus point can either be done by dragging the zoom area selections in a parent strip, or by panning a child strip directly, such as using the arrows keys, or by a dragging mouse gesture at the left and right border of the strip."
"all of these projects concentrate mainly on interactive exploration of time-series datasets. however, beyond standard low-level visualization tasks [cit], temporal visual exploration often require attention to the special properties of the time dimension [cit] . in particular, additional important temporal analysis tasks include [cit] support for (a) dynamic temporal hierarchies, (b) across-concept relationships, and (c) large-scale overviews. few of the above time visualization tools, with the exception of liverac [cit] and continuum [cit], explicitly support all of these requirements."
"the reason for this restriction is obviously that stack zooming uses one of the dimensions of the visual space for layout. it may be argued that we could support stack zooming in an inherently two-dimensional visual space, such as a treemap [cit], if we only extended our technique to three dimensions, and thus used the z (depth) axis for layout. another method may be to use dynamic layout of focus regions drawn as insets on top of the main visualization. however, this is outside the scope of our current work."
"the basic structure in stack zooming is a hierarchical zoom stack (or z-stack) 1 . the zoom stack describes the layout of the hierarchy of focus regions, orthogonal to the underlying visual representation. in other words, the zoom stack can manage any one-dimensional visual space. just like any other tree, a z-stack is defined by a single root node r containing all of the nodes of the tree."
"to illustrate another use of stack zooming, figure 12 shows an example of a multi-scale representation of a document visualized using stack zooming. this approach might be useful for comparing several passages of a long document (e.g., when studying an advanced software manual), or for showing several occurrences of a specific word in a document as well as their surrounding context."
"in this section, we give the background of temporal visualization, starting with the visual representations of time-series data, and then focus+context techniques [cit] that few existing temporal analysis tools fully support. we also discuss collaboration and communication aspects for visualization that integrate with these ideas. stack zooming in a stock market dataset using traxplorer. the horizon graph segments are arranged in a stack hierarchy that was built during visual exploration. the exploration and presentation interface of the tool is also visible on the right side of the image."
"the traxplorer system was implemented in java using the piccolo 2 structured 2d graphics toolkit [cit] . it accepts basic commaseparated (.csv) data files as input, and enables the user to select any two dimensions in the dataset as the time (x) and value (y) axes for a particular time series. this way, the user can easily load several data series into the tool to allow for comparison between different datasets and not just within the same dataset."
"instead, the analyst (or the team of analysts) can use the exploration history contained in the presentation tree to linearize the combined exploration sessions of the data, i.e., to create a linear sequence of views similar to a slideshow presentation suitable for presentation to the audience. because stakeholders may not be experts in data analysis, traxplorer allows for hiding the exploration interface entirely, and instead just show the visualizations (standard line graphs) in fullscreen mode. the analyst can still revert back to the whole branching exploration history, even during a presentation."
"a number of general techniques have evolved to support multifocus interaction, e.g., split-screen [cit], fisheye views [cit], and space-folding [cit], etc. however, none of these techniques was originally developed for the exploration of time-series data. the continuum faceted timeline browser [cit] implements the above extended temporal tasks, but provides only one level of overview of the timeline, meaning that detail and context awareness is limited."
"beyond directly supporting visual exploration, the stack hierarchy also serves as a tangible graphical history [cit] of the visual exploration session. recent advances in visual communication have suggested that to ameliorate the process of sensemaking [cit], visualization should also support collaboration and communication [cit] . our traxplorer prototype takes this a step further by introducing a presentation tree that is automatically built during exploration, and which can then double as a management interface for annotations, computations, and, ultimately, presentation of the results of the exploration. to complete our stock market example, this functionality would enable our analyst to present predictions of future market trends to a manager using the tool itself, progressively expanding selected parts of the exploration history and displaying relevant statistics integrated with the visual display."
"color-coded frames for the child strips and correspondingly color-coded selection areas in the parent strips show the correlation between parents and children, as well as provide intervening context and distance awareness between the focus points. dragging a selection area in a parent strip will pan the connected child strip, and children can be panned directly by using the arrow keys. in this way, users can quickly explore the temporal dataset with a sequence of simple zoom and pans while retaining multi-focus support."
"we have presented a theoretical background and a practical implementation of a multi-focus interaction technique called stack zooming that integrates multiple focus points in time-series dataset visualizations with their respective context and relationships. we have also shown how the zoom hierarchy built during visual exploration can serve as an input to presenting the insights gained during a particular session. our prototype is built using java and showcases the basic functionality of the stack zooming technique for temporal data. to further showcase our contribution, we have exemplified how to use the tool for exploring stock market crashes in a financial dataset consisting of several market indices for a 10-year period."
"the traxplorer system is a time-series visualization tool supporting multi-focus interaction using the stack zooming technique introduced in this paper. time series are represented as multiple tracks, hence the name of the tool. the system was designed to support a communication-minded [cit] iterative workflow process depicted in figure 7 where there are three phases involved: individual exploration, collaboration within the analysis team, and dissemination to external stakeholders. for the traxplorer tool, this is realized in the following ways (explained in subsequent sections):"
"our future work will entail studying the empirical performance of the tool in comparison to similar tools, such as liverac [cit], continuum [cit], and the mélange [cit] technique. we would also like to improve the tool to better support collaborative visual exploration settings involving teams of analysts working together, and study how the tool can help analysts fill different roles in the analysis process. some temporal data consists of discrete events rather than quantitative values, and we would like traxplorer to support this as well. finally, we are also interested in exploring similar applications of communication-minded visualization [cit] ."
"to help manage multiple tracks, the layer control box can be used to move, to delete, and to toggle the visibility of individual tracks, as well as to change color mapping, transparency, and track title. furthermore, using the layer control, two or several tracks can be linked to use the same scale for the value (y) axis, thereby supporting direct comparison of values across tracks. if tracks are not linked, they will be scaled to use the whole vertical space of each visualization strip, independent of other tracks (useful for comparing trends rather than absolute values). layer control is also used to determine which track should be used for the value axis labels."
"the traxplorer objective is to support an analyst communicating insights gained during an individual exploration session to the whole team, while allowing for continuing the exploration in collaborative mode, or iterating back to individual exploration (by the same analyst or another) after conferring with other team members. in other words, for this setting, the analyst will need support from the visualization tool for a semi-structured presentation, while at the same time being able to continue visual exploration at any time."
"we argue that the temporal analysis tasks discussed above can be generalized to the concept of multi-focus interaction [cit] . multifocus interaction is a conceptual framework that integrates multiple focus+context [cit] views with guaranteed visibility to support both focus, context, and distance awareness. this is the approach taken by the liverac system, implemented using the accordion drawing framework [cit] for guaranteed focus visibility. in this paper, we take a different approach to multi-focus interaction compared to liverac: instead of using a single integrated view with visual distortion, we take advantage of the one-dimensional nature of the data to present hierarchies of undistorted visualization strips."
"next, joe decides to seek more insight into market behavior to predict future market crashes. looking at the two focus points, he creates two additional time windows in the black monday focus point. he centers one of them at the crash date and navigates the other time window to approximately a year before the crash time. joe also resizes the child strips to give more visual space to the new focus points and enables local statistics in them (figure 10) . by looking at these views, he notes that within only a year before the crash, both ftse100 and hngkngi saw a steep increase of approximately 55% and 100%, respectively, and once the crash occurred there was a 35% drop in the ftse100 index and a 50% drop in the hngkngi index."
"the key components in our implementation include the time strip class (implemented as subclasses of piccolo's pnode basic scene graph node class), the actual visualization (which is orthogonal to the strips and the layout), and the layout manager responsible for calculating and animating the layouts for a hierarchy of strip nodes. although a recursive layout algorithm may seem to fit the zoom stack hierarchy, our implementation does not use recursion because the layout on each level is performed globally for the whole level."
"in this paper, we present a general technique to supporting multifocus interaction for one-dimensional visual spaces-which includes the temporal datasets motivating this work-that we call stack zooming. the technique is based on vertically stacking strips of 1d data into a stack hierarchy and showing their correlations ( figure 1 ). to exemplify the new method, we also present a prototype implementation, called traxplorer (figure 2 ), that supports multi-focus interaction in temporal data through stack zooming, as well as additional functionality such as local statistics computation and annotation support. our prototype would allow our analyst to create a large number of focus regions within the timeline to compare between and across different stocks and over time."
"our implementation of the stack zooming technique for the main visualization window allows the user to explore time series data by building zoom stacks. more specifically, the user can drag the mouse on a visualization strip to define a time interval. this will create a child strip for that interval in the zoom stack. colorcoded correlation graphics show the relationship between parent and child. dragging the time window graphic in a parent will pan the corresponding child a corresponding amount. each strip has additional decorations to allow for maximizing the size of a specific strip on the layout space, hiding it (see below), as well as deleting it. deleting a strip will delete all of its children. furthermore, dragging the border of a strip enables directly resizing its space allocation."
the contributions of this paper are the following: (i) the general stack zooming technique and its implications to visualization of time-series data; (ii) the implementation of our prototype system for temporal visualization based on stack zooming and with support for communication based on the exploration history; and (iii) a usage scenario conducted with the traxplorer system showing how it can be used for visual exploration of a large financial dataset.
"to preserve the tree structure of the zoom stack, one design alternative is to not divide space equally across siblings of each level of the zoom stack, but rather to assign space to whole subtrees. this would mean that each child would have to stay within the extents of its parent. this would give an explicit indication of the parent-child relationships between strips in adjacent levels, and thus decrease the need for explicit correlation graphics (discussed next). however, we found that because visual exploration using stack zooming often results in unbalanced zoom trees, this design would result in suboptimal use of the available screen space. therefore, global space allocation across each level is a better solution."
"finally, there comes a point when the results collected by an analyst team must be communicated to the external, and often non-expert, stakeholders [cit] . our aim with traxplorer was to bridge the gap between the visualization tool and the presentation tool, eliminating the need to, for example, cut and paste visualization images into traditional presentation applications like microsoft powerpoint."
"nodes in a zoom stack are laid out on the visual 2d substrate using a space-filling layout ( figure 5 ) that splits the vertical space by the depth of the tree (assigning an equal amount to each tree level), and the horizontal space by the number of siblings for each level of the tree (assigning an equal amount to each sibling). at any point in time, the user is able to change the layout allocations by dragging the borders of individual strips. as discussed above, the layout may also be reversed, using horizontal space for stacking layers in the tree and vertical space for siblings for each level. the ordering of child strips for each level may be significant for the purpose of conveying the relative positions of the displayed intervals of a time series to the user. therefore, the layout manager will always order child strips for each level in the zoom stack to be the same as the order of their intervals on the parent strip. overlapping intervals, such as when moving focus points, is a special case-see section 3.4."
"the zoom stack already captures the exploration history, and we reinforce this by providing a presentation tree (top right in figure 2 ) that maintains a hierarchical representation of the zoom stack. this approach is similar to the branching exploration histories of gras-parc [cit] as well as of shrinivasan and van wijk [cit] ."
"if we are to retain focus, context, and distance awareness for a visual space supporting stack zooming, we need to make explicit the relationship between parent strips and child strips in adjacent levels of the zoom stack. however, as argued above, we cannot directly show ancestor relationships in the layout, or we may waste valuable screen space. therefore, we introduce correlation graphics that visually indicate the relationships between parents and children."
"almost all data found in the real world have a temporal component, or can be analyzed with respect to time. temporal data is prevalent in virtually all scientific domains, and permeates societal phenomena as well-examples include computer and network logs, chronological history, political poll data, stock market information, and climate measurements. one of the common properties of this wide range of data is that the datasets tend to be large, not only in the captured time period but also in the number of observed attributes [cit] . visualization has been suggested as a way to handle this problem (e.g., [cit] ), but effectively exploring large-scale temporal datasets of this kind often requires multi-focus interaction [cit] -the capability to simultaneously view several parts of a dataset at high detail while retaining context and temporal distance awareness."
"screen allocations (alloc) are specified as scale-independent ratios of the full allocation of the whole zoom stack. this measure, along with the node order in the list of children for the parent, governs the actual screen location of the node when it is visualized. note that the technique does not stipulate the layout of the zoom stacks on the screen; in our examples so far, layers are stacked on top of each other (so the space allocation is a ratio of the horizontal space), but we will show a horizontal layout in section 6 (where space allocations accordingly are ratios of the vertical space)."
"joe has been tasked to study two crashes in particular: [cit] . because these events are spaced ten years apart in a large temporal dataset (daily values for all major stock market indices), joe knows he will need multiple focus points, and thus loads the datasets into traxplorer. since the crashes affected european and asian markets differently, he decides to study one representative index for each continent, and loads the uk (ftse100) and hong kong (hngkngi) [cit] . looking at the initial view of these two tracks (figure 8 ), joe notes that both indices were clearly affected by the two crashes. pairwise comparison does not make sense for this task, because the two indices have different scales, so he does not link the tracks, but he uses the exploration interface to change transparency and toggle their visibility to better see the individual values."
"nodes in a zoom stack are called zoom nodes (z-nodes). a single z-node n captures a single strip (a focus region in the 1d visual space) in the stack zooming technique. thus, the node n consists of a range [e 0, e 1 ] describing the extents of the focus region in data dimension space, a alloc factor describing the layout allocation for this particular node on the screen (width or height, depending on the orientation of the space), a parent node p, and an ordered list of child nodes c. in our particular application, the data dimension is time and the visual representations are temporal visualizations, but stack zooming can be applied to any one-dimensional visual space."
"interested in understanding the buildup and aftermath of a crash, joe creates two focus points, both spanning over an interval of two years around the events (figure 9 ). this allows him to study the behavior of the market before and after the crashes. [cit] . from this data, joe hypothesizes that there was a \"bubble\" [cit] one affected both."
"a wo uses its state diagram to compute a path from a current state to a known state [cit] . according to the frequency of the paths used, a wo can adapt its behavior. for instance, if a path is often used between non-adjacent states, the wo can build a shortcut transition between the initial and destination states and then build the corresponding method within its subclass instance (application objet). this consequently modifies the state diagram of this instance."
to build its state diagram -its capability knowledge -the wo executes the methods of its sub-class (i.e. an application class) to know the effect on the attributes of this sub-class. each set of attribute values produces a state in the diagram and a method invocation produces a transition. the main constraint in this step is that the method invocation must have no effect on other objects of the application when the wo is dreaming. this is solved thanks to the system architecture described in section 4.2.
modeling and simulation of biological systems gained tremendous interest thanks to the increasing predictive ability of the modeled systems in healthcare and the biotechnology industry [cit] . microbial and human systems are most amenable to modeling given the development of high-throughput techniques that enable the spatiotemporal characterization of biological systems [cit] .
"problem 2 is entirely parallelizable through distributing the 2n lps among the available workers. the strategy used so far in the existing implementations was to divide 2n equally among the workers. nevertheless, the solution time can vary widely between lps because ill-conditioned lps can induce numerical instabilities requiring longer solution times. consequently, dividing equally the lps among the workers does not ensure an equal load on each worker."
"using a dynamic load balancing with a chunk size of 50 resulted in similar results to the guided schedule. the final run time equaled 197s, while ffva took 581s. an optimal chunk size has to be small enough to ensure a frequent update on the workers' load, and big enough to take advantage of the solution basis reuse in every worker. at a chunk size of one, i.e., each worker is assigned one iteration at a time, the final solution time equaled 272s. in fact, for a small chunk size, the worker is updated often with new pieces of iterations, looses the stored solution basis of the previous problem, and has to solve the lp from scratch which slows the overall process."
"a system based on new technologies should be able at runtime to: (1) know by itself on itself, i.e. to learn how it behaves, to consequently reduce the understanding effort needed by end-users (even experimented ones); (2) know by itself on its usage to adapt to users according to the way and to the context it is used in. in addition like any service-based system (3) such system should be capable of improving the quality of services it is offering. wof aims at producing such systems while meeting those end-user related requirements:"
"on average, one instance needs 2 gb. in a parallel setting, the memory requirements are at a minimum 2 j gb, which can limit the execution of highly parallel jobs. in the julia-based distributedfba, the overall memory requirement exceeded 15 gb at 32 cores. vffva requires only the memory necessary to load j instances of the input model, which corresponds to the mpi processes as the openmp threads save additional memory through sharing one instance of the model. the differences between the ffva and vffva get more pronounced as the number of threads increases ( figure 5) i.e., 13.5 fold at eight threads, 14.2 fold at 16 threads, and 14.7 fold at 32 threads."
"in systems that do not share memory, message passing interface (mpi) was used to create instances of problem 2. every process then calls the shared memory execution through openmp."
we introduce the fundamental concepts of wo and wos from a runtime perspective. we adapt to this end the ibm mape-k known cycle for autonomic computing [cit] .
"the call to vffva is done from bash as follows: where schedule can be static, dynamic or guided, and chunk is the minimal number of iterations processed per worker at a time. the source code is available online [cit] ."
"to illustrate our purposes, all along the paper we use a simple example in home automation domain. let us consider a system composed of a roller shutter (actuator) and a control key composed of two buttons (sensors). in the very general case and in a manual mode, with a one-button control key, a person uses the button to: bring the shutter either to a higher or to a lower position. with a second button, the user can tune inclination of the shutter blades to get more or less light from the outside. as the two buttons cannot be activated at the same time, the user must proceed in two times: first, obtain the desired height (e.g. 70%) then the desired inclination (e.g. 45%). for such systems, three roles are generally defined: system developer, system configurator and end-user. assume an end-user is at his office and that according to time and weather, his/her requirements for the shutter change (height and inclination). this would solicit the end-user all along the day and even more when there are several shutters with different exposure to the sun. from a developer's point of view, very few support is dedicated to easily construct adaptive systems: when provided, such support is limited to an application domain and cannot be reused with a minimum of constraints. generally, adaptation mechanisms and intelligence are merged with the application objects which make them difficult and costly to reuse in another application or another domain."
"the lp problem modeling the metabolism of a given organism has n reactions that are bounded by lower bound lb (n,1) and upper bound ub (n,1) vectors. the matrix s (m,n) represents the stoichiometric coefficients of each of the m metabolites involved in the n reactions."
"vffva had at least 20 fold speedup ( table 2 ). the main contributing factor was the use of c over matlab in all steps of the analysis. in particular, the loading time of matlab java machine and the assignment of workers through parpool was much greater than the 8 analysis time itself."
"in the paper, we focus mainly on the architecture of the wo and wos, their global structure and behavior. in section 2, we discuss the challenges and requirements for nowadays self-adaptive systems. then we present design principles and fundamental concepts underlying wof in section 3. in section 4 we detail the structure and behavior of a wo and a wo system (wos). to illustrate how to use the wof, we give an example in the home automation domain in section 5. finally, in section 6 we discuss our approach and conclude with ongoing work and some perspectives."
"designing intelligent adaptive distributed systems is an open research issue addressing nowadays technologies such as communicating objects (cot) and the internet of things (iot) that increasingly contribute to our daily life (mobile phones, computers, home automation, etc.). complexity and sophistication of those systems make them hard to understand and to master by human users, in particular endusers and developers. those are very often involved in learning processes that capture all their attention while being of little interest for them."
"many approaches are proposed to design and develop the kinds of systems we target: multiagent systems [cit], intelligent systems [cit], adaptive systems [cit] ), self-x systems [cit] . in all those approaches, a system entity (or agent) is able to learn on its environment (including other entities) through its interactions. our intention is to go a step forward by enhancing a system entity with the capability of learning by its own on the way it has been designed to behave. we see at least two benefits to this: (a) a decentralized control: as each entity evolves independently from the others, it can control actions to perform at its level according to the current situation; (b) each entity can improve its performance and then the performance of the whole system."
"the input does not rely on matlab anymore as the lp problem is read in the industry standard .mps file, that can also be obtained from the classical .mat files through a provided converter. the improvements in the implementation allowed to speed up the analysis by a factor of three and reduced memory requirements 14 fold in comparison to ffva and the julia-based distributedfba implementation [cit], in a similar parallel setting."
"new usages, amounts of information, multiplicity of users, heterogeneity, decentralization, dynamic execution environments result in new system design requirements: new technologies should adapt to users more than users should do to technologies. if we take home automation systems for example, both endusers and system developers face problems:"
"they define that when the event \"on\" occurs on the switch, the action -method -\"up\" must be executed on the rolling shutter and that when the event \"off\" occurs on the switch, the action \"down\" must be executed on the rolling shutter. for the experiment and feasibility study, the action on the switchinstance -\"on()\" and \"off()\" invocations -are simulated using the wo simulator we are developing. the actions \"on\" and \"off\" occur according to a poisson distribution and depend on the elevation of the rolling shutter. the likelihood of action \"off\" occurrence is rollingshutterinstance.elevation/100, the likelihood of action \"on\" occurrence is inversely proportional. when an action occurs, \"on\" or \"off\", it can occur x times successively without delay, where x is bounded by the number of occurrences to reach the bound of shutter elevation, respectively 100% and 0%. presently, a wo acquires its knowledge about its capabilities using a graph representation. the knowledge about its usage is the logs of all its actions/events and can be presented by a markov graph. the logging presented in log 1 shows the events occurred on each wo of the system. this information is collected from each wo. with this information each wo can determine its current behavior and a manager can determine the system behavior. this is discussed in section 6. log 2 gives markov graph logging representation. let us note that the markov graph representation hides time-related information as it is based on frequency of occurrences. log 2 shows that the wise part of the switch instance detects the 2 states and the 6 transitions. it also shows a 2x2 adjacency matrix followed by a description of the 6 transitions including their usage-related statistics."
"organism size ecoli_core [cit] escherischia coli (72,95) p_putida [cit] pseudomonas putida (911,1060) ecolik12 [cit] escherischia coli (1668,2382) recon2 [cit] homo sapiens (4036,7324) e_matrix [cit] escherischia coli (11991,13694) e c _matrix [cit] escherischia coli (13047,13726) harvey [cit] homo tributedfba was run on julia v0.5. ilog cplex was called with the following parameters:"
"in shared memory systems, open multi-processing (openmp) library allows balancing the load among the threads dynamically such that every instruction runs for an equal amount of time. the load is adjusted dynamically, depending on the chunks of the problem processed by every thread. at the beginning of the process, the scheduler will divide the original problem in chunks and will assign the workers a chunk of iterations to process. each worker that completes the assigned chunk will receive a new one until all the lps are processed."
"from a system development perspective, our design decisions are mainly guided by the following characteristics: software support should be nonintrusive, reusable and generic enough to be maintainable and used in different application domains with different strategies. developers should be able to use the framework with the minimum of constraints and intrusion in the source code of the application. we consequently separated in the wof the \"wisdom\" and intelligence logic (we name abilities) of the objects from application services (we name capabilities) they are intended to render."
"regarding the rollingshutter instance, the logging after the 2nd iteration (log 3) and the last log 4 are given. log 3 shows that the wise part of the rollingshutter instance detects 6 states and 10 transitions (green values of adjacency matrix). consequently, it has not detected all the possible transitions yet. this incomplete knowledge is not a problem, during the next dream state or if it uses those transitions during the awake state, the wo part of the application object will update its knowledge. the last log 4 shows that all states and transitions are detected (learnt)."
"particularly, constraint-based reconstruction and analysis (cobra) methods enable the reconstruction of the metabolism of biological systems in silico as linear programs [cit] . subsequently, an objective function of the system is formulated and optimized for, e.g., biomass yield, metabolite production. although the objective is uniquely determined, the set of corresponding solutions forms the space of alternate optimal solutions (aos) that describe the possible conditions in which the optimal objective is achievable. the aos space is quantified using flux variability analysis (fva) [cit], which provides a range of minimum and maximum values for each variable of the system. biologically, these values overlap with the fitness of a given system to achieve optimality and allow to validate the metabolic phenotype through matching the empirical ranges with the fva bounds. fva was applied to quantify the fitness of macrophages after the infection of mycobacteirum tuberculosis [cit], resolve thermodynamically infeasible loops [cit], and compute the essentiality of reactions [cit] ."
"a first one is the scalability. it is easier to add wos, managers, loggers... on this kind of architecture than to modify a hierarchical architecture. moreover, this architecture is obviously distributed and enables distribution/decentralizion of wos in the environment."
"to provide application (e.g. home automation system) developers with relevant support, we designed wof with the minimum intrusion in the application source code. the adaptiveness, intelligence and distribution related mechanisms defined in wof are inherited by application objects. we realized a java implementation of wof and validate it on a home automation example."
"log 2 shows for instance that from the state 0 with the position attribute at false, the switchinstance may execute method \"on()\" or \"switch()\" and go to state 1 or execute method \"off()\" and remain in the same state 0. usage-related statistics show that method \"switch()\" is never used from the state 0 all along the 1000 iterations."
"the system uses an event/action mechanism for wos' interactions. on an event, a state change occurs in wo, an action may be triggered on another wo. these peers \"event/action\" are defined by event, condition, action (eca) rules that are managed by a manager. when this latter catches events (statechangeevent), it checks the rules and conditions and posts a request for action (actionevent) on the bus. from the wo point of view, if its state changes during its awake state, it posts a statechangeevent on the bus. when a wo receives an actionevent, two cases may occur: either the wo is in awake state or in dream state. if the wo is in awake state, it goes to the end of its current action and starts the action corresponding to the received request. if the wo is in dream state, it stops dreaming and enters into the awake state to start the action corresponding to the received request."
"since it is challenging to estimate a priori the run time of an lp, the load has to be dynamically balanced during the execution of the program."
"in the end, the final program is comprised of a hybrid mpi/openmp implementation of parallelism which allows great flexibility of usage, particularly in high-performance computing (hpc) setting."
"we designed wo in a way its behavior splits into two states we named dream and awake. the former is dedicated to introspection, learning, knowledge analysis and management when the wo is not busy with service execution. the latter is the state the wo is in when it is delivering a service to an end-user or answering an action request from the environment. the wo then monitors such execution and usage done with application services it is responsible for. we use the word dream as a metaphor for a state where services invoked by the wo do not have any impact on the real system: this functions as if the wo is disconnected from the application device/component it is related to."
"taken together, as metabolic models are steadily growing in number and complexity, their analysis requires the design of efficient tools. vffva allows making the most of modern machines specifications to run a more considerable amount of simulation in less time thereby enabling biological discovery."
"finally, vffva outran ffva and distributedfba both on execution time and memory requirements ( table 3 ). the advantage becomes important with larger models and a higher number of threads, which makes vffva particularly suited for analyzing the exponentiallygrowing-in-size metabolic models in hpc setting."
"load management describes the different approaches to assign iterations to the workers. it can be static, where an even number of iterations is assigned to each worker. guided schedule refers to dividing the iterations in chunks of size n/workers initially and remaining_iterations/workers afterward. the difference with static lies in the dynamic assignment of chunks, in a way that fast workers can process more iteration blocks. finally, the dynamic schedule is very similar to guided except that chunk size is given by the user, which allows greater flexibility. in the following section, the load balancing strategies of e c _matrix and harvey models were compared."
"the speedup gained on computing large models (recon2 and e_matrix) reached three folds with vffva ( figure 2 ) at 32 threads with recon 2 (35.17s vs 10.3s) and e_matrix (44s vs 14.7s). in fact, with dynamic load balancing, vffva allowed to update the assigned chunks of iterations to every worker dynamically, which guarantees an equal distribution of the load. in this case, the workers that get fast-solving lps, will get a larger number of iterations assigned. conversly, the workers that get ill-conditioned lps and require more time to solve them, will get fewer lps in total. finally, all the workers synchronize at the same time to reduce the results. particularly, the speedup achieved with vffva increased with the size of the models and the number of threads (figure 2-e_matrix) . finally, the different load balancing strategies (static, guided and dynamic) were compared further with two of the largest models (e c _matrix and harvey)."
"the generation of 30,000 warmup points were compared using the cobra toolbox function createwarmup matlab and a dynamically load-balanced c implementation createwarmup vf on a set of models (table 4 ). since the cobra toolbox implementation does not support parallelism, it was run on a single core and divided the run time by the number of cores to obtain an optimistic approximation of the parallel run times. the speedup achieved varied between four up to a factor of 100 in the different models (table 4 ). similarly to ffva [cit], the main factor for the speedup was the c implementation that allowed the reuse of the lp object in every iteration and save presolve time. equally, the dynamic load balancing between workers ensured a fast convergence time."
"in a second step, in order to delineate the aos space, the objective function is set to its optimal value followed by an iteration over the n dimensions of the problem. consequently, each of the reactions is set as a new objective function to maximize (minimize) the lp and obtain the maximal (minimal) value of the reaction range. the total number of lps is then 4 equal to 2n in the second step which is described as the following:"
"as explained in section 3, wos are not aware of the existence of other wos. they are distributed and communicate data/information towards their environment. wos may be on different locations and one or many managers carry out communication and coordination among them. in this paper, we propose a concrete architecture based on a bus system, where any wo communicates with other objects through the bus. this architecture has many advantages."
"in this paper, we focused on adaptation, monitoring and communication mechanisms and showed how wise object behave according to their experience. there are still many research issues we are investigating: a first issue to handle is how to repre-sent the knowledge in a wise object. state diagram and markov graphs are used in our first approach, but other approaches are envisaged. knowledge, specific to each wise object, represents an amount of information that can be big but not necessarily relevant. a second issue relates then to knowledge aggregation by wise object so that they can extract relevant information to the whole system. this issue may involve techniques from information fusion approaches, multi-criterion scales and fuzzy modeling. knowledge aggregation allows us to represent emotion of a wise object, namely the distance from its current behavior to its usual behavior (surprise, stress, etc.). a last issue is related to the use of aggregated knowledge within the system during its execution. this is typically a problem of information fusion. the goal is to generate a (sub-)system knowledge/emotion from the knowledge/emotion translated by the wise objects."
"the concept of wo has many scopes of application. it can be used to adapt an application to its environment, to monitor an application from inside, to manage an application according to its uses... in this section, we highlight the wo behavior within a home automation application. this choice is justified by the fact that:"
"fastfva (ffva) [cit], a recent implementation of fva gained tremendously in speed over the fluxvariability cobra toolbox matlab function [cit] . two main improvements were the driving factor of the gained efficiency: first, the c implementation of fva allowed higher flexibility in comparison to matlab [cit] through the use of the cplex c api. the second was the use of the same lp object, which avoided solving the problem from scratch in every iteration, thereby saving presolve time. ffva is compiled as matlab executable (mex) file, that can be called from matlab directly. nevertheless, given the exponentially growing size of metabolic models, ffva is run in parallel in most cases. parallelism simply relies on allocating the cores through matlab parpool function [cit] and running the iterations through parfor loop. the load is statically balanced over the workers such as they process an equal amount of iterations. nevertheless, the solution time varies greatly between lps which does not guarantee an equal processing time among the workers in static load balancing. often, the workers that were assigned a set of fast-solving lps process their chunk of iterations and stay idle, waiting to synchronize with the remaining slower workers, which can result in less efficient global run times. here i present veryfastfva (vffva), which is a pure c implementation of fva, that has a lower level management of parallelism over ffva. the program is provided as a standalone, and does not rely on matlab thereby offering an open source alternative for constraint-based biological analysis. the significant contribution lies in the management of parallelism through a hybrid openmp/mpi, for shared memory and non-shared memory systems respectively, which offers excellent flexibility and speed up over the existing implementations. while keeping the up-mentioned advantages of ffva, load balancing in vffva was scheduled dynamically [cit] in a way to guarantee equal run times between the workers."
"to alleviate human interaction with such systems and help developers to produce them, we propose wof, an object oriented framework founded on the concept of wise object (wo) [cit] . a wo is a software-based entity that is able to learn on itself and also on the others (e.g. its environment). wisdom refers to the experience (on its own behavior and on the usage done of it) such object acquires by its own during its life. according to this approach, \"wise\" buttons and shutters would gradually construct their experience (e.g. by recording effect of service invocation on their state, statistics on invoked services, etc.) and adapt their behavior according to the context (e.g. physical characteristics of a room, an abstract state defined by a set of data related to the weather, the number of persons in the office, etc.). from the development perspective, we separate in the wof the \"wisdom\" and intelligence logic (we name abilities) of the objects from application services (we name capabilities) they are intended to render."
"as introduced in 3, an emotion of wo is a distance between its current usage and its common us-age (i.e. unusual usage). wo can be stressed if one of its methods is more frequently used or conversely, a wo can be bored. wo can be surprised if one of its method is used and this was never happened before. emotions of wo are a projection of its current usage with regard to its common usage. when a wo expresses an emotion, this information is catched by the wos that may consequently lead to behavior adaptation. at the object level, two instances of the same class that are used differently -different frequencies, different methods... -may have different emotions, thus, different behavior and interaction in the wo system."
"we define a wise object (wo) as a software object able to learn by itself on itself and on its environment (other wos, external knowledge), to deliver expected services according to the current state and using its own experience. wisdom refers to the experience such object acquires by its own during its life. we intentionally use terms dedicated to humans as a metaphor. when human better succeed in observing the others, a wise object would have more facilities to observe itself by introspection. a wise object is intended to \"connect\" to either a physical entity/device (e.g. a vacuum cleaner) or a logical entity (e.g. software component) (see figure 1 ). in the case of a vacuum cleaner, the wo could learn how to clean a room depending on its shape and dimensions. in the course of time, it would in addition improve its performance (less time, less energy consumption, etc.). a wo is thus characterized by:"
"the presented software can be implemented in fortran since the library openmp is supported as well. additionally, python's multiprocessing library allows to balance the load dynamically between non-shared memory processes, but the parallelism inside one process is often limited to one thread by the global interpreter lock (gil). this limitation could be circumvented through using openmp and cython [cit] . the advantage of vffva lies in the implementation of two levels of parallelism following a hierarchical model where mpi processes are at a top-level and openmp threads at a lower level. the mpi processes manage the coarse-grained parallelism, and openmp threads manage the finer-grained tasks that share memory and avoid copying the original problem, which increases performance and saves consequent memory. this architecture adapts very well with modern distributed hardware in hpc setting."
"in our java implementation of wof, object classes produced by a developer inherit the behavior of wise object (wo) class. an instantiated system is defined as a wise system composed of wise objects that interact through a (or a set of distributed) manager(s) implemented by an event bus according to publish-subscribe design pattern."
"taken together, the dynamic load balancing strategy allows the efficient parallel solving of metabolic models through accelerating the computation of fva and the fast preprocessing of sampling points thereby enabling the modeller to tackle large-scale metabolic models."
"as one design principle behind wof is to minimize intrusion within the application source code, we have succeeded to limit them to the number of two \"warts\". the examples highlight those 2 intrusions in the code. they are concretized by two methods implemented in the wo class -methodinvocate() and methodinvocated() -and must be called at the beginning and the end of any method of the wo subclass (application class). those methods monitor the execution of a method on a wo instance. we will discuss about these \"warts\" in the last section."
"to ensure adaptiveness, each wo incorporates mechanisms that allow it to perform a kind of mape-k loops [cit] . dream and awake mape- when dreaming, a wo introspects itself to discover services it is responsible for, analyzes impact of their execution on its own state and then plans revision actions on its knowledge (experience). wo constructs its experience gradually, along the dream states. this means that wo knowledge is not necessarily complete and is subject to revisions. revision actions may relate to adaptation, for instance recording a new behavior, or to optimization like creating a shortening of a list of actions to reach more quickly a desired state. when awake, a wo observes and analyzes what and how services are invoked and in what context. according to its experience and to analysis results, the wo is able to communicate an emotion if necessary. we define a wo emotion as a distance between a current usage of its services and its common usage (usual one). with this metaphor, a wo can be stressed if one of its services is more frequently used or conversely, a wo can be bored. it can be surprised if one of its services is used while it has never been used before. wo emotions are intended to be used as a new information by the system (other wos) and/or the end-users. this is crucial to adaptation at a wos level (e.g. managing a new behavior) and to attract attention on potential problems (e.g. alerts when temperature is unusually too high). with respect to its emotional state, a wo plans relevant actions (e.g. raising an alert, opening the windows and doors, cutting off the electricity, etc.)."
"the third main advantage is the ability for a wo to disconnect/reconnect from/to the bus when needed. this makes it possible the implementation of the dream state section 3. let us recall that in the dream state, a wo can invoke its own methods to build its state diagram, but these invocations must not have any effect on the subject system. thus, when a wo enters the dream state, it disconnects itself from the bus and can invoke its methods without impact on the real world system. more precisely, the wo disconnects its \"sending system\" from the bus, but it continues receiving data/information via the bus. therefore, if a wo in dream state receives a request for another ob- ject, it reconnects to the bus, goes out from the dream state to enter into awake state and serves the request. figure 5 shows the class diagram of a wo system based on the bus. this model is simplified and highlights the main classes that compose a wo bus system."
"in the analysis of large models, where matlab loading time becomes less significant, dynamic load balancing becomes the main driving factor of the gained speedup."
it is worth noticing that this example is intention-ally simple as our goal is to highlight the kind of knowledge a wo can currently acquire. state diagrams and usage logging are the knowledge base for wos. we discus about management and use of this knowledge in section 6.
"with guided schedule (figure 3-a), the highest speedup (2.9) was achieved with 16 threads (figure 3-b ). the run time per worker was quite comparable, and the iterations processed varied between 719 and 2581. the final run time was 281s."
"the remaining explanatory variables reported in table 4 sales. commercial success should follow greater investment into business and marketing 8 a referee suggests that firms in business for only a few years may have a higher percentage of new products simply because they themselves are new. this conjecture seems to be warranted, but the impact on innovation is unclear. omitting firms less than 15 years old reduces the sample by about 100 firms and the coefficient on age become insignificant in the first regression. however, the coefficient on age becomes stronger (significant at the .05 level) in the project-level innovation regression when these younger firms are omitted. age seems to have an impact but deserves more study."
"in this section, we present a novel hilbert-curve based data aggregation scheme that supports both privacy preservation and integrity protection for wireless sensor networks. in order to support data privacy, we first provide a data privacy preserving algorithm by using sensor nodes' seeds and hilbertcurve values. a seed exchange algorithm is applied to reduce the number of messages during data aggregation. in order to support data integrity, we provide a private information retrieval (pir) [cit] based integrity checking algorithm that communicates between a child node and its parent node by exchanging a pir message and its response message."
"empirical tests require data, data require measurement, and this paper faces considerable measurement challenges. the first is to measure innovativeness, recognizing that any measure will be indirect."
"in the end, survival depends on both innovativeness and proficiency. innovators always make the correct decision and thus outperform agents whose decisions do not match the current environment. but innovators build proficiency more slowly because they frequently switch strategies. conservative agents more quickly acquire proficiency in their chosen philosophy because they always play the same strategy, but they sometimes make incorrect decisions. the interplay of these survival advantages, innovation and proficiency, drives the dynamics in this model and two parameters tune their relative importance. in each period a state of the world emerges randomly. in a perfectly symmetric world the probability of each state is identical and there is no systematic pressure for the population to make one type or another type of decision."
"in order to solve the problems of growth rules determination in conventional regional growth algorithm and speed up the support vector machine segmentation algorithm [cit], we proposed a parallel image segmentation method combining support vector machine with regional growth in past work [cit] . that method worked better and faster than conventional algorithm, but there are some drawbacks: the seed point's detection was not always exact in some case, and the algorithm complexity should be reduced to meet the need of some real-time computing. therefore we amend the method, it includes four steps: firstly, from target and non-target area in an image whose segmentation result has been obtained, a certain numbers of sample points were selected for the support vector machine classification training. secondly, select pixel points by granularity n, which means a pixel point every other n lines and n rows, and then use the trained support vector machines for discriminating whether these points and their 4 neighboring points belong to the target area. if so, the point recorded as a seed point. thirdly, carry out the regional growth starting from those seed points, stopping with some gray values above or below the threshold. finally, essential post-processing is done for the dealing with edge and noise points."
"(2) seed point selection: pixels of every nth column of nth row and their 4 neighboring points will be checked by the trained svm whether they belong to the target area, where n is a certain granularity. then the pixels in the target marked as the seed points."
"in this paper, our goal is to design a reliable and efficient data aggregation scheme in terms of energy consumption, propagation delay, and accuracy of the aggregated result by following these design considerations."
"innovative agents overtake the network in only two periods. starting with a regimented network such as a tree, line, or grid, rewiring increases the exposure of agents to alternative decisionmaking philosophies. greater exposure leads to a more rapid dispersion of strategies, and as we observed in figure 2, speed favors innovators. thus, these more complex networks are more likely to evolve an innovative culture."
"as far as innate character is concerned, the medical image is fuzzy [cit], for there are many fuzzy factors in medical image, such as grey-scale, texture and region boundary and so on. moreover, for the mass medical image data, we must consider the effectiveness and speed of segmentation simultaneously. because of these medical image particularities, a universal suitable theory and method, which can be used as a perfect solution on the medical image segmentation question, is not exist."
"in the first column of table 2 the probability that an agent is rewired equals zero, and so these results replicate those of the original networks displayed in the fourth row of table 1 ."
"on the other hand, there exist multiple potential attacks against a data aggregation scheme. some attacks aim at disrupting the normal operation of the sensor network, such as routing attacks and denial of service (dos) attacks. a number of previous efforts have addressed these behaviorbased attacks. in this paper, our major concern is the types of attacks that try to break the privacy and/or integrity of aggregate results, rather than worrying about those attacks. we assume that a small portion of sensor nodes can be compromised and focus on the defense of the following categories of attacks in wireless sensor networks."
"neighbor, and the pattern of connections influences this spread. as some randomly dispersed links are introduced into a regimented network, there is a greater mixing of strategies: in one structure different decision philosophies might be separated by several steps, but in another structure those philosophies might be physically close."
"(4) region growing: region growing starts from these seed points, using the gray level threshold value to decide which points should be consider as the target area;"
the next step of the work lies in the evaluation of the approach in a realistic environment. the torque control approach should lead to a better terrainability and smaller energy consumption in a real outdoor environment. hence the model should be improved to handle the wheel sinkage as well as more complex trajectories for the rover.
"this dynamic suggests another testable hypothesis: that younger firms might be more innovative and more successful with their innovative activities than older firms, ceteris paribus."
"the main performance criterion in this analysis is slip. this choice is motivated by the fact that several aspects of a rover mission demand for as little slip as possible. navigation is more accurate if the rover does not slip; since slipping wheels do not contribute to the rover's movement, slip is a loss of energy; potential slip increases the risk of an operation failure due to loss of control of the vehicle."
"the seed is used for hiding the original data from an adversary. the principle underlying our seed exchange method is as follows. the original data can be changed by extracting some part of a seed value, which is sent to other nodes. some part of the seed value is also added from another node. as a consequence, the sensed data can be hidden among seed exchange group members. the following equation shows the final sending value from each node for data aggregation, where is the number of seeds received from other nodes. figure 3 shows a sensed data encryption result on each sensor node after exchanging a seed:"
"the controller was implemented on a rough-terrain rover called crab. the hardware has to provide all the necessary information which describes the rover's state. the sensors necessary, including the tactile wheel, are presented in this paper. the tactile wheel is based on a flexible wheel whose deformation is measured to provide the wheel-ground contact angle. the controller is evaluated and compared to a standard velocity controller. it shows very good results as the torque based controller has reduced slip and torque values for all tested friction coefficients."
"winning causes the successful decision-making algorithm to spread to the losing agent, in this example, agent i switches from being type c 0 to being of type c 1 . this spread of a decisionmaking philosophy can be thought of as the loser seeing the light and becoming a disciple of the victorious agent. over time, the decision-making philosophy of the most frequently successful agents spreads, the distribution of agents following each decision-making algorithm adjusts, and we can track the success of a particular approach by observing its spread or contraction."
"each network configuration, or organizational structure, was analyzed with different population sizes, the largest containing 900 agents and the smallest containing only 9. then, each network/population combination was subjected to one hundred independent experiments; that is, the data in each cell of table 1 reflect 100 different initial scatterings of the three agent types."
"the agent-based model in the previous section suggests that smaller firms, firms with complex, small-world configurations, and younger firms are more likely to evolve an open and innovative atmosphere. these results were robust to changes in memory length and the probability weights on the state of the world (excluding trivial cases such as no memory). [cit] and funded by the national science foundation that covers more than 400 firms located in fifteen different countries. the central objective of that study was to investigate the new-product development process, and so a team of researchers would enter an establishment and interview project managers of new-product development efforts. the interviewers attempted to explore at least one successful and one failing new-product development project in each firm. in addition to the interviews, these managers also answered a battery of survey questions that covered a range of information pertaining to the firm and to newproduct development. these surveys contain the information used here."
"our privacy preserving algorithm chooses a tree-based topology to perform intermediate aggregations. note that we do not use a clustering-based topology because it is affected by the communication range between cluster heads and it suffers from a large amount of messages for constructing network. first, a sink node triggers a query by sending a hello message generated from a message flooding scheme [cit], as shown in figure 1 (a). upon receiving the hello message, a sensor node determines whether the hello message is from the sink node or not. if a sink node is located within its communication range, the sensor node receives the hello message from the sink node and sets the sink node as a parent node. otherwise, the sensor node waits for a certain period of time to receive the hello message from its sibling nodes and then selects one of the sibling nodes as a parent node by broadcasting a join message. the sink node forwards the hello message to its sibling nodes with its corresponding level (figure 1(b) )."
"in the three of the following regressions the dependent variables measuring innovation are categorical and rank-ordered. for example, one dependent variable measures the perceived commercial success of an innovation using responses in one of five categories ranked from \"far below expectations\" to \"far above expectations.\" to account for the discrete nature of the dependent variables and yet still take advantage of the information provided by the ranking of responses, we use an ordered probit model to estimate the impacts of the independent variables on innovation."
"typically, the main indicators of performance evaluation of parallel algorithm are speedup and efficiency. speedup is defined for each number of nodes n as the ratio of the elapsed time when executing on a single node and efficiency is a concept closely related to speedup ratio, defined as the quotient of speedup and the numbers of nodes. we used a different number of nodes, repeat 10 times the implementation of the program, and finally we get the average value shown in figure 3 from the results we can analyze that, as the number of node increases, the execution time is leveled off, the speedup is improved smoothly, and the efficiency always maintains in a higher level. more intuitively, the segmentation results of ct images by this algorithm are satisfactory."
"based on the nature of questions, region growing algorithms vary depending on the criteria used to decide whether a pixel should be included in the region or not, the type connectivity used to determine neighbors, and the strategy used to visit neighboring pixels. furthermore, some kind of restrictive criterions were defined to finish the region growing automatic. grays and texture information of sub -regions in different space position are similar in contents and changes generally, so we can select the appropriate features to describe the differences in these regions. before region growing, a curvature flow filter is used to reduce the noise and get a result with sharp and smoothing boundaries."
"for there is not relativity between images but between pixels and their neighborhoods in every images, we can segment different images in different nodes at the same time. we realize the parallel segmentation algorithm by master-slave model under the torque environment. here is the main execution sequence: first, according to the numbers of images and the load of nodes, node 0 will send the image which should be segmented to the others then gets ready to receive the result. meanwhile, after receiving the images, other nodes will segment images following the segmentation algorithm this paper proposed then send them back. the process above repeated until all the images were segmented. figure 2 shows the flow chart."
"it is thus necessary to design a new data aggregation scheme that supports both data privacy and data integrity. the new scheme should be reliable and efficient in terms of energy consumption, propagation delay, and the accuracy of the aggregated result."
"this paper introduces an image segmentation method based on svm and region growing. this method selects the feature vectors corresponding sample point to train the svm. secondly, feed point is computing features and classified. the concrete processes are as follows:"
"the rover is tested on the described track with the two control types, simple velocity control and the presented torque control. the tests are done with static friction coefficients µ 0 of 0.35, 0.55 and 0.75 and the speed of the rover is set to 0.1 m · s −1 . the length of the test terrain is approximately 2.4 m. one test run consists of moving once across the track from one side to the other. several runs (4 to 7) are done for each controller and the averaged measurements are then compared."
"data were collected at two levels of aggregation: the individual firm is the initial unit of analysis, and then we disaggregate to focus on specific innovation projects within firms. at each level of aggregation, different measures of innovativeness are reported and different information is provided on the firms' organizational structure. this provides a variety of perspectives of the impact of structure on innovation."
"7 at the firm level of analysis we use two innovation metrics. the first reflects the percentage of the firm's products that are \"new.\" interviewees were shown a taxonomy of the product life cycle in which a product goes through four stages: introduction, growth, maturity, and decline. they were then asked, \"what percentages of your current products are in each stage?\" summing the percentage of products in the introduction and growth stages yields the first measure of innovativeness. thus more innovative firms are defined as firms with a higher proportion of \"new\" products in their product mix."
"the region growing algorithms, the basic idea of which based on region growing method integrates similar pixel points into a region, have proven to be an effective approach for image segmentation. typically, region growing is to start from a seed region which is considered to be inside the object to be segmented [cit] . the pixels neighboring this region are evaluated to determine whether they should also be considered part of the object. if so, they are added to the region and the process continues as long as new pixels are added to the region."
"1 ), one of the two mars exploration rovers (mer) of nasa, opportunity, got almost stuck in a sand dune in meridiani planum. on the 12th [cit] the other mer, spirit, got trapped at his turn and was retasked as a \"stationary research platform\". these examples show how fatal wheel slip can be for autonomous rovers and how important it is to control an exploration rover as well as possible to lower the potential risks. minimizing slip, or maximizing the rover traction is also related to the rover capability to negotiate the terrain irregularities named terrainability [cit] . this rover property is influenced mostly by two aspects one can act upon; the kinematics of its suspension system and how well its actuators are controlled. the topic of optimal control for a rough-terrain rover lies in the main focus of the work presented here."
"our integrity checking algorithm is performed through three phases: a pir message construction phase, a pir response phase, and an integrity checking phase. in the pir message construction phase, upon receiving the encrypted data from a child node, the parent node constructs a pir message and sends the message to the child node to check data integrity. in the pir response phase, a child node responds with a result message by calculating row values based on the pir message received from its parent node. finally, in the data integrity checking phase, the parent node checks whether the data from its child node are valid by comparing two values, that is, the first received value and the second value."
"third, notice how the number of firms adopting an innovative decision-making culture increases as we move horizontally to the right in table 1 . this increase in innovation stems from a more subtle organizational characteristic: it involves the pattern of the connections in these networks. the left side of table 1 contains relatively formal, regimented organizational structures: the line, tree, and grid. such organizations have extreme order, lines of communication that reflect a well-defined chain of command or a formal, almost mechanistic, set of procedures and processes through which decisions are channeled. toward the right side of table 1 lie networks with a less systematic structure, a more informal organization. by definition, random networks lack any systematic organization whatsoever. the complete network has a systematic structure because everyone is connected to everyone else, but it is not a very \"organized\" organization. thus, table 1 suggests that organizations with a formal architecture that involves well-defined chains of communication will exhibit greater rigidity in their decision-making practices, while more free-flowing, open-access or organic organizations may be more conducive to a flexible decision-making culture."
"in the context of a pre-study for the european space agency (esa) project exomars [cit], the development of a new robotic platform called crab [cit], depicted in fig. 1, offered a new opportunity to pursue the idea of implementing and testing this controller. this paper describes the required development of the crab rover, as well as the torque control impact. the following section gives an overview of the torque control. section iii presents briefly the crab and its model. the next section focuses on the sensors providing the state of the rover, and especially the crab's tactile wheels. section v presents the results and a conclusion ends the paper. its effect is depicted by the blue curve in the bottom graphic of fig. 8 . note also that the position of the wheel with respect to the obstacle in both cases can be observed in fig. 9 . to summarize, the tactile wheel provides at 10 hz a wheel ground contact angle, which is required for the torque control."
(1) data privacy: privacy concern is one of the major obstacles to civilian applications for wireless sensor networks. curious individuals may attempt to gather more detailed information by eavesdropping on the communications of their neighbors. it is increasingly important to develop data aggregation schemes to ensure data privacy against eavesdropping.
"these dummy variables are used in the analyses reported in table 4, but, to further explore the relevance of organizational structure, a matching set of regressions were studied in which the organizational dummy variables concentrate on the more severe instances of concern."
"the impact of size is less definitive. the firm-level data did not find size to be significantly related to either the product mix or the commercial success of the firm's innovations. the project-level data suggests that smaller firms do indeed offer more radically different products than larger firms, as expected, but there was no effect on their commercial success. while the computational model in section ii suggests smaller firms are more likely to evolve a culture of innovation, ceteris paribus, there are other size effects that may work in the opposite fashion. [cit] offer several reasons why larger firms may have an innovative edge: cost spreading, greater access to finance, a greater ability to internalize spillovers, and their ability to absorb the risks associated with r & d. thus, size may alter a firm's innovative culture, but that organizational effect on innovation may be offset by other attributes of firm size. [cit], that size increases the level of innovation, has received mixed empirical support through the years. this study may offer an explanation as to why: size may give a firm greater access to financial capital and spread risk, but it may also foster a culture that does not promote innovation."
"with the proliferation of advanced technologies of mobile devices and wireless communication, wireless sensor networks (wsns) are increasingly attracting interest from both industry and research institutes [cit] . because sensor nodes have limited resources (i.e., battery and memory capacity), data aggregation techniques have been proposed for wsns [cit] . however, the wireless communication can be overheard, and consequently data privacy in sensor networks is a crucial issue. although data aggregation schemes that preserve data privacy have been proposed, they have the following limitations. first, the communication cost for network construction and data aggregation is considerably expensive. second, the existing schemes do not support data integrity due to communication loss. since the existing privacypreserving schemes do not support privacy preservation and integrity protection simultaneously, it is necessary to carefully design an effective data aggregation scheme for recent applications of wsns, such as military and environmental monitoring, where both privacy and integrity of the sensed data should be provided [cit] ."
"there are shortcomings to these data. for example, while certain firm characteristics are measured directly (size, industry group, etc.), much of these data represent responses to an interviewer's question. those data capture an individual's perception of things as opposed to capturing actual things. furthermore, the information concerning an organization's structure lacks the specificity needed to plot a company's organizational network with precision."
"to concentrate on organizational differences, we fix the parameters b and m and study a variety of different network structures, starting with the simple networks shown in figure 1 ."
"a test environment was set up to develop, test, and compare the torque control algorithm. this set up as well as the results are discussed below."
"the cells in table 1 report the percentages of the 100 experiments that converge on various strategies. each experiment ended with the universal adoption of a single decision-making strategy, sometimes a flexible strategy and sometimes a rigid one. naturally larger networks take longer to converge than smaller networks, and networks with a greater diameter (measured by the longest geodesic path in the network) tend to converge more slowly. however, most networks converge quickly, in less than 500 rounds of play."
"that excessive structure stifles creativity, while a randomly rewired, less formal network is fertile ground for creativity is intuitively appealing, but there is more than intuition at work"
"in the integrity checking phase, a parent node analyzes the pir response message and determines whether the received data from its child node is valid. the parent node checks the qr and qnr of the received data by using the selected two prime numbers (in the second phase) and jacobi symbol. if the received data are valid, there exist − 1 qrs and one qnr. otherwise, the received data are not valid. algorithm 6 shows our integrity checking algorithm for the received data. first, a parent node finds the qr and qnr for all columns (lines 1-2). second, if qnr is set to the column of the modified value, the parent node assures that the processed data from its child node are valid. finally, the parent node also checks the validity for qr (lines 3-8)."
"the medical image segmentation that divides the image into meaningful sub regions is a key technology in the medical image processing and analyzing [cit] . the development of image segmentation technology not only affects the progress in the medicine image processing, such as visualization, 3d reconstruction, different pattern medicine image registration and fusion, but also plays an extremely important role in the biomedical image analysis."
"when firms report that a particular project was organized with a more participatory style of management, they also tended to report greater commercial success with their new products (as expected). however, greater participation did not significantly affect the degree of innovation."
"recently, as advanced technologies of mobile devices and wireless communication proliferate, wireless sensor networks (wsns) have increasingly attracted interest from various applications including military and environmental monitoring. moreover, since sensor nodes have limited resources, such as battery and memory capacity, many data aggregation techniques have been proposed for wsns. however, the wireless communication can be easily overheard, and thus the provision of a data aggregation scheme to support data privacy is a challenging issue in wsns. although several data aggregation schemes have been proposed to preserve data privacy, they have the following limitations. first, the communication cost for network construction and data aggregation is considerably expensive. second, only a part of the existing methods supports data privacy. in addition, it is necessary to assure that the aggregated data are not polluted by an unauthorized third party. for this, we propose a new data aggregation scheme for enforcing both data privacy and data integrity in wsns. our scheme makes use of a seed exchanging algorithm to reduce the communication cost for preserving data privacy. it also utilizes an integrity checking algorithm based on a private information retrieval (pir) technique. from our performance analysis, we show that our hda scheme achieves 100%-300% longer network lifetime and about a 10% better attendance rate for the aggregated data than the existing privacy preserving schemes. in addition, our ihda scheme achieves 40%-160% better performance in terms of network lifetime and about a 16% better participation rate for the aggregated data. as future work, we plan to verify that our scheme is efficient in wsns by applying it to a real environment."
"algorithm 4 shows our pir message construction algorithm. first, a parent node randomly selects a subtracted value and calculates a modified value (line 1). second, the node converts into two-dimensional data (line 2). third, the parent node selects large prime numbers and in order to obtain the set of qr and qnr. a cell whose hilbert id is the same as the modified value is set to be qnr and the others are set to be qr (lines [cit] . finally, the node sends and the group of the qr and qnr values (line 10)."
"the remaining sections of this paper are structured as follows: section 2 shows the main relevant works. in section 3, we introduce the algorithm and its realization in detail. we carry out the experiments in section 4. finally, some concluding remarks are given in section 5."
"(2) data integrity: since data aggregate results may be used to make critical decisions, a base station needs to guarantee the integrity of the aggregated result before accepting it. therefore, it is crucial that data aggregation schemes can protect the aggregated results from being polluted by attackers."
"(1) the simple method using support vector image segmentation requires classification of all points in image, but the proposed method only needs part of points, which is faster than the former."
"while the particular measures of network structure used here are indirect, they capture enough of the flavor of the computational model presented above to mount a test of the premise that organizational structure affects innovation. starting with the firm-level analysis in the first two columns of table 4, managers who express concern about the bureaucratic bottlenecks in their organizational structures tend to belong to firms that have fewer products in the early stages of the product life-cycle. these firms have not kept pace with their less rigid counterparts when it comes to introducing new products into their product mix. this result is bolstered by the similarly depressing effect of a rigid structure on the commercial success of the firm's new products. as hypothesized, a rigid decision-making culture seems to suppress the firm's ability to innovate and dampen the energy needed to make innovation successful. in addition, firms that stress communication across functional groups had significantly more new products in their product mix, but communication did not affect the firm's commercial success with innovation."
"support vector machine (svm) algorithm is a machine learning technique based on statistics theory [cit] . compared with traditional machine learning techniques, this technique can overcome the dimensional disaster into a high dimensional features space as well as stronger generalization ability."
"subsequent columns report the results of increased rewiring and show that rewiring affects different networks differently. for example, because the connections in a random network are random, a random rewiring has little effect. severing a few links in the complete network also appears unimportant. however, in all three of the highly structured networks the increasing \"messiness\" of its rewired links leads to organizations that are much more likely to evolve innovative cultures. something about the unruliness of these rewired networks seems compatible with innovation. in table 1, the least innovative organizations were the line, tree, and grid. in table 2, we see that rewiring has significant effects on all of these previously rigid organizations."
"within each of the 400 firms, one or more new-product development projects were singled out and studied in detail, providing data on more than 900 innovation projects. data were collected on attributes of the firm and on specific projects within each firm. the survey also gathered rudimentary information on organizational structure, and these parallel observations on structure and innovation allow us to test the hypothesis that structure affects innovation."
"(3) efficiency: data aggregation achieves bandwidth efficiency through in-network processing. in integrityprotecting private data aggregation schemes, additional communication overhead is unavoidable to achieve additional features. however, the additional overhead must be kept as small as possible."
"the computational experiments also suggest that smaller firms should be more innovative even though previous studies claim that larger firms will be more innovative because they have more resources to fund innovation. this empirical question is tested by including firm size, measured as the number of employees. while there are other potential measures of firm size (output or sales, for example), the number of employees is appropriate for this study because our computational model suggests that it is the size of the organization's network of individuals that influences its culture. thus more workers should lead to a more rigid, hierarchical system that inhibits innovation."
"there are two situations where slip occurs: the wheels are fighting each other due to uneven terrain or different commanded wheel velocities; the applied torque is too high and the ground cannot sustain the created traction. torque control tries to avoid the latter by assigning bigger torques on wheels where the load is bigger because more traction can be generated. for the experiments, the slip per wheel is determined by subtracting the measured traveled distance from the encoder value of the respective motor. the traveled distance is determined by measuring the distance between start-and endpoint of the wheels. this approach to determine the slip only takes the total traveled distance into account. local slippage, such as sliding when the wheel is moving up and down the obstacles compensates itself in the final calculation. this approach describes consistently the overall behavior and is therefore a valid measure to evaluate and compare the performance of both controllers."
"this is a critical decision because it constrains the degrees of freedom in the model and the universe of potential results. in short, this restriction allows us to perform controlled experiments. by fixing the topology of the network at the beginning of each experiment, we can test specific network attributes and hold all other attributes constant. the experiment can then be repeated in a different (but also fixed) network. if networks are allowed to evolve, it would be difficult to isolate the impact of specific network characteristics, and designing empirical hypotheses would be too arbitrary . in addition, there is a practical side to this restriction. even though most networks evolve over time, their evolution may occur more slowly than the pace of the economic decisions being made in the network. in such cases a fixed network may be a better representation of reality than an evolving network. table 1 displays results of the many virtual experiments performed within this structure."
"to formalize this organizational structure, we view the organization as a network. each node of the network is occupied by an agent and the edges that connect nodes define which agents interact. altering the architecture of the network alters the organization's structure. the question is, do these structural changes affect the evolution of the decision-making culture in some systematic fashion? to explore this possibility, we do not restrict the range of organizational structures by mapping the decision-making machinery of specific firms; instead we explore the evolution of decision making in abstract organizations with exaggerated characteristics. among these organizations are linear, well-defined organizations, rigid hierarchical structures, organic or free-flowing organizations, and random networks. this study initially reports experimental results of six vastly different networks (these would be idyllic organizational structures) and later a series of more complex networks are created by randomly severing and reattaching edges in these base structures. by repeatedly playing the conservative/innovative decision-making contest in these hypothetical networks we can observe how network structure (organizational characteristics) can affect the evolution of corporate culture. to help visualize the organizational characteristics, a small sketch of each initial network is given in figure 1 ."
"the dynamics of the virtual experiments (figure 2) suggests innovativeness is more pervasive early in an organization's existence. younger organizations are more likely to tolerate and/or encourage innovation. but with the passage of time, conservative decision makers gain an edge in proficiency, which can allow them to eventually dominate the organization's culture."
"schemes. we compare our hilbert-curve based data aggregation scheme (hda) with cpda, smart, twin-key, and gp 2 s, in terms of the number of transmission messages and the average lifetime of the sensor nodes. here, the number of sensor nodes ranges from 10 to 100. figure 8 shows the communication overhead with respect to a varying number of sensor nodes. the number of transmission messages in all schemes is increased as the number of sensor nodes increases. this is because when the number of sensor nodes is large, every sensor node in the wsn is capable of sensing data and hence a large number of messages should be transmitted. however, our scheme outperforms the existing schemes by about 10%-20%. the reason for this is that our scheme does not need to generate unnecessary messages during data aggregation since each sensor node can transform only its own data whereas the existing schemes require an additional message for privacy preservation. figure 9 shows the number of transmission messages with respect to different distributions of sensor nodes. figure 10 shows the number of transmission messages with a varying communication boundary when the number of sensor nodes is 100. in both figures, our scheme outperforms the existing schemes because it does not require unnecessary messages in all the cases. in particular, our scheme, smart, and gp 2 s show consistent performance regardless of the type of distributions and the communication boundary. this is because they are less affected by the placement of sensor nodes owing to the use of a tree topology. meanwhile, cpda and twin-key are strongly influenced by both the type of distributions and the communication boundary, because they make use of a clustering method. figure 11 shows the average lifetime of the sensor network with varying number of sensor nodes in the wsn. in this analysis, we measure the time until the number of sensor nodes, whose energy is completely consumed, is greater than 50% of all sensor nodes. the lifetime of all the schemes decreases as the number of sensor nodes increases. this is because the number of messages generated in the network is proportional to the number of messages required for data aggregation. however, the lifetime of our scheme becomes 100%∼125% longer than those of all the existing schemes, because our scheme can reduce unnecessary messages during data aggregation."
"but their flexibility exacts a price. agent-based models have so many degrees of freedom that a particular simulation can be designed to fit almost any data array. if we can always construct a computational version of some model that fits our data, is the model truly falsifiable? this malleability is especially troublesome when ab models verify their simulations by comparing a model's results to some vague stylized facts-facts that may have been considered during the model's design. we can do better. in this manuscript we suggest that ab models be subjected to the same scrutiny commonly applied to neoclassical theory: their predictions should be tested empirically. ultimately, it will be the empirical relevance of agent-based models that will lead to the broader acceptance of computational modeling as a standard theoretical tool in economics. [cit] review the issue of empirical validation in agentbased models and provide a critical guide to the alternative validation approaches being explored in the ab modeling community. this study enters the empirical validation fray, but in a more traditional fashion. in this manuscript an agent-based model extends an established, neoclassical theory, and that extended model generates empirical hypotheses which are then tested using standard econometric procedures. [cit] who use an ab model to examine the geographic distribution of cropsharing contracts in illinois. our objective is similar; to show by example, how a computational model can lead theory into areas it previously did not tread, and once that extension is complete, how we can proceed down the conventional hypotheses-testing path."
"this paper proposed a new method of image parallel segmentation combining support vector machine with regional growth, this method selects sample from the known segmentation result image, trained support vector machines is used for searching seed point, avoiding the manual seed selection. compared with solo support vector machines segmentation, this method reduces the distinguishing times, speed up the segmentation. parallelization of the algorithm in torque environment further speeds up the segmentation. the method this paper proposed has a well application prospect in image segmentation domain."
"similarly, we tune the impact of proficiency, p, by restricting the length of each agent's memory. an agent acquires proficiency with experience, i.e., the more often an agent chooses an action, the better he becomes at executing that action. however, proficiency fades because the value of practice decays over time and eventually vanishes. proficiency is assumed to deteriorate linearly, the rate of decay being set by memory length. specifically, labeling the maximum memory length as m, then agent i's proficiency is"
"to illustrate, consider a straight-line, k2 network in which half of the agents are innovative decision makers and the other half are traditionalists, and suppose a string of states of the world arise such that innovators win in every round. as shown in figure 3, it takes a minimum of four periods for innovative agents to dominate this small organization. compare that outcome with the same network after only one rewiring, as shown in the bottom of figure 3 ."
"the more disaggregated, project-level data concentrate on specific innovation projects within each firm and contain additional information on the firm's internal structure. again, two dependent variables were used to capture different aspects of innovativeness. the first measure of innovativeness rates the \"degree of innovation\" of this new product. respondents were asked to classify this product as being:"
"regional growth is a technique for image segmentation which starts at some known pixel points and extends to all neighboring pixels that are similar in gray level, color texture, or other properties in order to form a complete region. easy to implement and run fast is the advantage of the algorithm, while requiring manual seed point selection is the shortcomings of the algorithm. svm are a set of related supervised learning methods that analyze data and recognize patterns, used for classification and regression analysis. a solo support vector machine is effective in segmentation, but the speed is slow."
the implementation and the results of an enhanced controller based on an optimal torque distribution are presented in this paper. a rover model provides the basis to compute the optimal torques which minimize slip on the rover's wheels. the controller itself uses the optimally distributed torques and adds a correction term based on the rover's velocity.
"to resolve these problems, we propose a new energy efficient and privacy preserving data aggregation scheme in wsns. to reduce the communication cost for preserving data privacy, we propose a seed exchanging algorithm for data aggregation. the seed generated by this algorithm is used not only to conceal the sensed data but also to preserve data privacy without additional message exchanges during the data aggregation step. for data privacy preservation, we also utilize a hilbert-curve based technique, where it is difficult to obtain the actual sensed data, even if attackers try to overhear it, because the data being sent can be changed by using a unique hilbert value. for providing data integrity, we propose an integrity checking algorithm based on a private information retrieval (pir) technique. upon receiving aggregated data from child nodes, a parent node starts an integrity checking algorithm in which the parent node generates a message based on the pir technique by multiplying two large prime numbers. by sending a pir 2 international journal of distributed sensor networks message to child nodes, the parent node can verify the aggregated data. our integrity checking algorithm is more efficient than the existing schemes since it checks the data integrity between child and parent nodes, instead of checking all data during the communication. therefore, our scheme requires low communication cost and yields an accurate aggregate result even in reasonably dense networks. this paper is organized as follows. in section 2, we present related work on privacy preserving aggregation schemes in wsns. in section 3, we provide both considerations and attack models for designing an efficient privacy preserving aggregation scheme. in section 4, we propose a new privacy preserving data aggregation scheme including a seed exchange algorithm in wsns. in section 5, we present a performance analysis of our scheme. finally, we draw conclusions and suggest future work in section 6."
"the second quantification challenge is to find measures of the internal network structure of a firm. these data do not permit a direct mapping of a network, but at each level of aggregation there are some questions that reflect on that structure. in the more aggregated, firmlevel data, managers were asked about their concerns with \"shepherding ideas through the bureaucracy\", rating their concerns from one (no concern) to five (a great deal of concern). in general, a higher score reflects a more rigid, stifling organization and a lower value reflects a more open, less hierarchical, or less bureaucratic structure. managers were also asked how much they emphasize \"communication between groups and functions.\" those placing an emphasis on"
"(2) data pollution: in a data pollution attack, an attacker tampers with the intermediate aggregate result at an aggregation node. the purpose of the attack is to make the base station receive a wrong aggregate result with large deviation from the original result, which leads to improper or wrong decisions. in this paper, we do not consider the attack where a node reports a false reading value, and we assume that the impact of such an attack is usually limited. by using privacy preservation measures, individual sensory data are hidden. however, not only the sensory data but also the aggregated value of a small group of sensors must be in a reasonable range. this implies that if a malicious user pollutes the individual sensory data (at a lower level in the aggregation tree), it can be easily detected since this introduces a large deviation from the original data. therefore, a more serious concern is the case where an aggregator close to the root of the aggregation tree is malicious or compromised."
"for wireless sensor networks, we provide a novel privacy preserving algorithm by using a hilbert-curve technique [cit] and seed exchanges among sensor nodes. our privacy preserving algorithm is performed through three phases: a network construction phase, a data encryption phase, and a data transmission phase. in the network construction phase, each node determines its sibling nodes, parent node, and child nodes by sending broadcast messages. each node exchanges a seed to other nodes among its sibling nodes. in the data encryption phase, each node changes the sensed data into a value by using its generated seed and the received seeds. the changed value is encrypted by the hilbert-curve algorithm. finally, in the data transmission phase, each sensor node sends the aggregated data to a parent node where all the data from child nodes are merged with its encrypted data. a sink node aggregates all data of sensor nodes in the network. we explain each step in detail in the following."
"as the next section demonstrates, organizational structure seems to be a fundamental component of organizational decision making, that is, the same agents making the same decisions and facing the same states of the world behave differently in one organizational structure than another. the pattern of connections in an organization, or the topology of a firm's network, affects the evolution of corporate strategy and the mores of the eventual culture."
", where with minor changes, the above describes the flexible/ [cit], but at this point we make two significant departures. first, harrington analyzed an infinitely large population in which losing agents die and are removed while surviving agents advance to the next level of play: even after many rounds and many deaths, many agents remain. in this study the population is finite, sometimes quite small and, as we shall see, size matters. second, at every level of play in the harrington model agents are matched with another agent chosen randomly from the entire population. in this model agents are embedded in an organization and their interactions do not occur with randomly-selected agents from the entire population. within organizations, individuals tend to interact with a few specific others-their colleagues and co-workers or their immediate subordinates and superiors-and they tend to interact with this smaller subset on a frequent basis."
"the authors gratefully acknowledge e. carrasco for his work related to the design and realization of the tactile wheel, as well as f. seitz for his contribution related to the can network implementation."
"in terms of slippage the torque control approach performs better than velocity control in slippery conditions. for a high friction coefficient the difference between the controllers diminishes, as depicted in fig. 11 . in other words, the more traction is provided by the ground, the less important is the optimal choice of wheel torques."
"(4) accuracy: an accurate aggregate result of sensed data is generally desired. therefore, we should take accuracy as a criterion to evaluate the performance of integrity-protecting private data aggregation schemes. when accurate aggregate results are needed, schemes based on randomization techniques are not applicable."
"the second project-level measure of innovation mirrors the commercial success data collected at the firm level. each respondent was asked to rate this new product's commercial success as being far below expectations to far above expectations on a five-point scale. as before, we expect more irregularly structured firms to be more attuned to the challenges of innovation and thus to have greater success with their innovative endeavors."
"in this procedure, we set the maximum number of child nodes so as to avoid network imbalance. if the network has imbalance, the sensor node of the imbalanced area may consume more energy than the other areas. therefore, we define the maximum number of child nodes as given below."
"algorithm 5 shows our pir response message construction phase. first, a child node extracts x from its processed value (line 1). second, the child node finds the hilbert id of the result (lines 2-3). third, the child node generates 2 data based on definition 3 (line 4). it then constructs the pir response message by using (6) (lines 5-12). finally, the child node sends the pir response message to its parent node (lines [cit] ."
"delegation and decentralization of authority. there is free flow of people and information across different jobs. there is wide latitude as to the means used to achieve objectives. responses were coded categorically on a one (mechanistic) to five (organic) scale. this mechanistic/organic scale and the accompanying description of each is reminiscent of the networks used in the rewiring experiments of section ii. prior to rewiring, the line, the tree and the grid are rigid and hierarchical (mechanistic); after rewiring, they are more decentralized, with a freer flow of information across distant parts of the network (organic). because the rewiring experiments suggest that organic organizations are more conducive to an innovative culture, we hypothesize that \"organic\" organizations will be more innovative and more successful with their innovation."
"alternatively, mechanistic, rigid organizations are more likely to confine their innovation to existing product line extensions and variations on products they already offer. organic organizations also report significantly greater commercial success with their new products than do mechanistic organizations. as hypothesized, because an organic organization is more conducive to an innovative culture, it achieves greater commercial success from its innovation."
"and furthermore, those cultural differences seem to impact performance. not only are mechanistic, hierarchical firms less likely to innovate, they have less commercial success with the innovations they possess."
"the support vector machine theory operates on a principle, called structural risk minimization, which aims to minimize the bound on the expected generalization error, so it has stronger generalization ability."
"consequently our network measures are indirect and descriptive. nonetheless, having firm-level and project-level data on organizational structure and innovation is uncommon, and it allows us to explore the hypothesized relationship between structure and a firm's decision-making culture."
"in this section, we present requirements for a data aggregation scheme to support both data privacy and data integrity. the desired data aggregation scheme should satisfy the following criteria."
three types of agents make these decisions. two of these types are conservative decision makers who adopt a particular philosophy and adhere to it regardless of the current environment.
"our computational experiments suggest firms with less formal, more organic structures are more likely to be open to innovation, while rigid, hierarchical organizations are more likely to become conservative decision makers and less tolerant of change. this generates our first set of empirical hypotheses, and four different network measures are used to test whether the structure of the organization influences innovation. the virtual experiments suggest the size of the firm may matter as well. the question of firm size and innovativeness has received considerable scrutiny, but empirical studies have not yet reached a consensus on this issue. [cit] argue that \"cost spreading\" gives an additional advantage to large firms, but empirical work has not provided consistent support for this relationship [cit] suggests that size may be a detriment to innovation, but he does not propose a formal model to support that contention. this paper provides such a theoretical link, but faces the same empirical challenges of the past studies, that is, size may have different and opposing impacts on innovation. all else equal, the computational model suggests that smaller firms are more open to flexible decision making and thus we expect them to be more innovative."
"(2) contrasted with the classics region growth algorithm, the interaction of manual seed point selection is avoided. in this proposed method, the seed selection automatically searches by using support vector machines."
"consider the line network at the top of figure 1 : each agent interacts with four other \"neighbors,\" two on each side, thus agent i interacts only with agents g, h, j, and k. or he loses and is converted. manipulating the pattern of connections between nodes alters the internal structure of the organization: it changes the agents' neighbors. we explore whether these alternative topologies systematically affect the evolution of a firm's innovative culture."
"the computational experiments also suggested that younger firms are more likely to embrace innovative cultures, and that property is present in the empirical data. older firms have significantly fewer new products in their product line and younger firms are more likely to introduce radical innovations than are older firms."
"with that foundation, we can probe more deeply into the impact of different organizational structures by introducing different networks. as we shall see, altering an organization's structure can affect the evolutionary path of its culture."
(3) denoise filter: a curvature driven image de-noising filter is used to reduce the noise and it also contributes to a sharp and smoothing segmentation result.
"the last two columns of table 4 report the project-level analysis. consider the effect of an organic versus a mechanistic structure on the degree of innovativeness. organic organizations do not seem to exhibit a greater degree of innovation, but this result changes when the more severe measure of organic/mechanistic organization is used. firms that were rated most strongly as having an organic structure (5 on the organic/mechanistic scale) were significantly more likely to introduce new products that are a radical departure from their current product line."
at this point a critical constraint is imposed: we assume an organization's network architecture does not change during the experiments. culture evolves but networks do not.
the method is divided into four steps: (1) training classifier with selected sample: select the certain amount support vector machines sorter's training sample from target area and non-target area of the known segmentation result's image and trains support vector machines;
"we compare our data integrity validation hda scheme (ihda) with icpda and ipda in terms of the number of transmission messages per query round, the average lifetime of sensor nodes, and the attendance ratio of sensor nodes. figure 12 shows the communication overhead with respect to varying number of sensor nodes in a wsn. the number of transmission messages for ipda, icpda, and our scheme is increased as the number of sensor nodes increases. our scheme outperforms the ipda and icpda schemes because the existing schemes generate unnecessary messages during data aggregation in the network. that is, each sensor node generates only two additional messages for privacy preservation and integrity checking in our scheme whereas the ipda and icpda schemes generate six and four messages, respectively. due to numerous messages exchanges among sensor nodes, there is a high rate of data collisions in the existing schemes. therefore, the ipda and icpda schemes are very expensive in terms of communication overhead because the number of messages generated in the network is very large for successful data transmission. figure 13 shows the average lifetime with respect to varying number of sensor nodes in the wsn. the dissipated energy for all three schemes is increased as the number of sensor nodes increases. this is because every message generated in the network requires energy to reach the sink node. however, in terms of lifetime, our scheme shows 35∼130% better performance than ipda and icpda schemes. the reason is that the ipda and icpda schemes generate too many unnecessary messages for data aggregation to enforce both integrity protection and privacy preservation. in the existing schemes, every sensor node becomes active to send its messages for a longer time. figure 14 shows the attendance ratio of sensor nodes for data aggregation. during data aggregation, a sensor node sends the sensed data (or aggregated data) to its parent node. the attendance ratio of sensor nodes in our scheme is about 100% whereas both ipda and icpda have some sensor nodes that do not take part in data aggregation. because a given sensor node in the ipda and icpda schemes has to communicate with at least six and two neighboring nodes, respectively, some sensor nodes cannot participate in data aggregation. therefore, our scheme shows the best performance among the three schemes."
"three critical steps help validate this model. first, we take an existing analytical model and extend it computationally to create a model of innovation and corporate structure. the first test was to see if the expanded computational model would replicate the results of the analytical model given similar initial conditions. it did. second, we fix the architecture of the underlying network throughout each experiment. this greatly constrains the set of potential results and allows our experiments to isolate the impact of different network shapes on the evolution of culture. to agent-based modelers this may be the most egregious step because it limits one of the powerful features of computational modeling-that all things can vary. in many agent-based models such constraints may unnecessary; in this study it allows us to isolate the effects of organizational structure and to run controlled experiments. third, the virtual experiments lead to empirically testable hypotheses that are tested using data from a sample of international firms."
"we use this ordered probit model to ask, \"does the organizational structure of a firm affect the level of innovation occurring in that firm?\" following others [cit], the level of innovative success is expected to be related to firm size, the resources dedicated to r & d, the resources devoted to business and marketing plans for the product, the industry to which the firm belongs, and the country in which it resides. to those traditional measures we add variables that reflect some of the organizational attributes of firms so as to test whether those attributes affect innovation."
the network model suggests that firms with a structure that encourages innovative decision making are more likely to introduce radically new products and to stretch themselves into new markets. more soberly structured firms are expected to lean towards product-line extensions and improvements to existing products.
"[ thus, even though a firm's culture may not be fully formed, we expect younger firms to be more innovative. and, we might expect to see changes where young dynamic firms grow pedantic and conservative over time."
"(1) eavesdropping: in an eavesdropping attack, an attacker attempts to obtain private information by overhearing transmissions over its neighboring wireless links or colluding with other nodes to uncover the private information of a certain node. eavesdropping threatens the privacy of data held by individual nodes."
"where the positive parameters c are chosen in accordance with specific issues and the slack variables ξ 0 ) denotes the support vector. it can directly be the decision-making function from function (3) using of kernel function (, )"
"the achilles heel of agent-based modeling has been empirical verification. agent-based computational models can produce captivating economic simulations, but, absent testable, refutable hypotheses, such models are sometimes dismissed as little more than stories. science is cautious, conservative. new tools and methodological innovations are scrutinized before they are embraced by the broader community. this paper suggests that agent-based models can withstand that scrutiny."
"finally, we expect younger firms to be more innovative. this hypothesis was not a parameter we planned on exploring when the computational model was initially constructed. [cit] demonstrate that the level of top management support for a project as well as the marketing effort put forth for the innovation have been shown to have significant effects on that innovation's success. we have measures for each. finally we introduce a host of dummy variables for the industry and country in which the firm resides."
"a history of study on innovation [cit] identifies size, research and development efforts, new product marketing efforts, top management support, as well as the industry to which the firm belongs, and the country in which it resides as determinants of a firm's innovativeness. this study adds the potential effect of an organization's structure. our contention is that the organization of a firm affects the evolution of that firm's decision-making processes, which affects the firm's corporate culture. such a view is consistent with prior research examining organizational learning [cit], the effects of organizational design on individual decision making (e.g., [cit] ), and the effects of organizational types on corporate culture (e.g., [cit] ) . [cit] argues that organizations store information in their forms (i.e., organizational networks), which, in turn, are used to socialize individuals in the organizations. [cit] provide evidence that organizational types affect corporate culture through, among other things, recruitment and socialization. thus, some organizational structures may be conducive to the proliferation of conservative decision makers, while others might foster innovative approaches to problem solving. over time the former entity is likely to evolve a stogy, tradition-bound culture and the latter a dynamic, innovative one."
industry effects are long associated with different levels of innovation [cit] document a significant cultural influence on innovative activity. table 3 provides summary statistics of the data for these measures.
"the guidance in iso 9241-11 (1998) can be used in procurement, design, development, evaluation, and communication of information about usability. iso 9241-11 (1998) includes guidance on how the usability of a product can be specified and evaluated. it applies both to products intended for general application and products being acquired for or being developed within a specific organization (international standards organization iso 9242-11, 1998) . iso 9241-11 (1998) also explains how measures of user performance and satisfaction can be used to measure how any component of a work system affects the whole work system in use. the guidance includes procedures for measuring usability but does not detail all the activities to be undertaken. specification of detailed user-based methods of measurement is beyond the scope of iso 9241-11 (1998) (international standards organization iso 9242-11, 1998) . according to the benefits and importance of iso 9241-11 (1998), this paper proposed a testing model for assessing usability of smp. as reflected in the definition, three central criteria for usability are the effectiveness, efficiency and satisfaction with which users can achieve specified goals (international standards organization iso 9242-11, 1998 ) as shown in figure 1 (note 1)."
"smp-based kms communities contain the software engineers, software developers, workers' knowledge or (system's users) and maintenance engineers in order to facilitate knowledge sharing among cop. smp involve many activities in which different people intervene. each person has partial information that is necessary to other members of the group. if the knowledge only exists in the software engineers and there is no system in charge of transferring the tacit knowledge (contained in the employees) to explicit knowledge (stored on paper, in files, etc) when an employee abandons the organization part of the intellectual capital goes with him/her. when this occurs in an organization involved in sm the end effect is a loss in intellectual capital and increased maintenance effort and costs. unfortunately, this is often the case. another well-known issue that complicates the maintenance process is the scarce documentation that exists related to a specific software system, or even if detailed documentation was produced when the original system was developed, it is seldom updated as the system evolves. for example, legacy software from other units often has not documentation which describes the features of the software [cit] ."
"the inputs to our model are word embeddings [cit] corresponding to words in the asr 1-best hypothesis. these embeddings are denoted as (x 1, ...x t−1, x t, x t+1 ..., x n ), where x t represents the current word and n is the length of the asr hypothesis. each word embedding is input to the forward and backward lstm-rnns, represented in figure 1 by a chain of blocks of f"
"as shown in figure 3 (note 1) the process of sharing knowledge start when a user willing to share its knowledge (file) of smp application. the user login to his/her email and if the file available with the user then the encryption and decryption agent will encrypt/decrypt the file, and send and receive email will activate directly after that the file will be shared (sent) to the requester user, or if the file exist into smp warehouse then the file transferring agent will activate and the encryption and decryption agent will encrypt/decrypt the file and send and receive email will activate then the file will be shared (sent) to the requester user."
"this evaluation model considers both quantifying elements of performance (experience and experiment) as well as subjective empirical. if the answer is wrong, or he/she not familiar with this question then skip to the second question until all the question will be solved. we will, however, record whether respondents are able to complete tasks successfully. the criteria for successful task completion are:"
"groupware software has been used which is lotus notes [cit] . this software provides various types of services for a cop. the most important service offers by this software or product is particularly in term of smp warehouse. this service will serve the cop to share their knowledge and any things that are stored in smp warehouse. another common service is agent technology, which it allows people to share the knowledge or notification regarding the latest knowledge of sm in the smp warehouse at any time and any place. the set of the agents are communicated to show how the system running as shown in section 4 (system development). usability testing model cafeterias (effectiveness, efficiency & satisfaction) are supporting this idea based to the (appendix b). (note 4)"
"analyzing the structural and representational aspects of large application software, explanation and/or predictions can be made about its maintainability. in that study, maintenance improved significantly when the language used supported (rombach, h. [cit] ) ."
a series of questions designed to collect demographic information about the respondents to assess their level of his information about the system is shown in table 1 (appendix a). (note 4)
"apart from the euronews test set, we evaluate our models on a real test set consisting of 20 news programs that appeared on the french tv channel tv5 [cit] . this test set is referred as the tv5 test set. on average a program is 12 minutes long and consists of 3 to 10 segments."
"normally, programmers will start creating the knowledge that being proposed for a certain project in the organization. when he or she has finished depositing knowledge into the knowledge repositories, the system will trigger the event and pass it to any member specified through e-mail system. this notification will be done based on previous record in order to make sure alerts could be done to those who are interested in the particular knowledge in order to make a decision. otherwise, this knowledge will be un-meaningful for the other member (ginsawat, r., abdullah, r. & nor, m. [cit] ) . when the system analysts or supervisors also want to make a decision, they should open their mailboxes and look on the subject matter. if they are willing to know about the detail of the knowledge created, they are asked to enter the username and password for security purposes. at the same time another agent will work by updating the status of accessing document as users who are interested with the subject matter (ginsawat, r., abdullah, r. & nor, m. [cit] ) cop is groups of people who share a concern, set of problems, or a passion about a topic, and who deepen their knowledge and expertise in this area by interacting on an ongoing basis [cit] ."
"from the above results, the integration of bim and sensor technology has been widely researched. in this section, by referring to the result of the review, some insights and research gap have been drawn as following:"
"the review has clarified and evaluated the current state-of-theart in the integration of bim and sensors based on the identified fifty-seven publications. these publications are mainly focused on the aspects of integration methods of bim and sensor technology, such as sustainability, site management, structural health monitoring, operation and maintenance, plan and design, and positioning and tracing. it has been discovered that integration methods have been widely researched as a fundamental aspect. moreover, sustainability, site management, and operation and maintenance are popular research directions. several noteworthy implications have been uncovered based on the current research gap and status of development. ultimately, more applications in structural design could be expanded based on the monitoring data of existing buildings."
"approaches based on sequence models [cit] and discriminatively trained models [cit] require a topic segmented corpus, as well as (pseudo) topic labels for these segments, during training. these previous works have tackled this issue by automatically generating topic labels for segments of text in the training corpus. to obtain topic labels, unsupervised clustering is performed on the text segments and each resulting cluster is marked with a different topic. a similar technique can be employed for training our model with bi-directional rnns. however, in this work we present an alternative approach which does not rely on a segmented corpus and topic labels."
"this theme is concerned with two parts: energy consumption and environment protection. building energy consumption research focuses on energy monitoring and the establishment of a method to improve energy performance or save energy, while environmental protection research concerns about saving resources and carbon emission. using sensors to monitor and record the use of energy/resources in buildings is the basis of the methods."
"in the beginning, the respondents will receive a short, scripted verbal orientation explaining the purpose of the usability testing. then they will be asked to complete a short background questionnaire to collect their demographic characteristics. the respondents will be asked to perform a set of information about how to share knowledge using the usability test as a kind of multi-agent technology for smp. the tasks were written on a sheet of paper that included a space where respondents will be asked to indicate their answers. once the tasks are completed, respondents will be asked to complete a short participant satisfaction questionnaire to collect and test their own perceptions towards sm."
"several types of data were collected to assess user's performance and user's perceptions of negotiating the system. in addition, we examined selected features of the normal lotus notes system to determine their effectiveness. tasks were deemed to be either completed or not completed."
"in recent years, multi-agent system (mas) has been an active research topic. due to the difficulties in solving process planning and production scheduling problems using traditional centralized problem solving methodology, mas approach -a distributed problem-solving paradigm is used as another attempt to solve the planning and scheduling problems. as a distributed problem-solving paradigm, mas breaks complex problems into small and manageable sub-problems to be solved by individual agents co-operatively (vermeulen, s. bohte, d. somefun &poutré j. [cit] ). agent paradigm lets users think in term of agents rather than objects / functions. the agent exhibits presents high dependencies compared with an object-oriented approach. such a software application needs an appropriate software development method. an analysis and design methodology is intended to assist first in gaining understanding of a particular system, and secondly in designing it [cit] . there are few choices of agent-oriented methodologies to help software engineers to specify, design and build agents to achieve the system's goals."
"system design includes user interface design, designing database, designing form and reports and designing dialogues. in software engineering context, design focuses on four major areas of concern: data, architecture, interface, and components."
the result was conducted according to the methodology described in the previous section. it is starts with an overview of data collected by analyzing trends. the pre-survey questionnaire for the respondents shown in (appendix a). the post-survey questionnaire for both quantitative and qualitative mas questions. satisfaction is a multi-dimensional construct. this study applies mas technology to support knowledge sharing of smp. (note 4)
"basically, there are two types of questionnaire that we prepared as part of usability testing for the respondents for the level of the questions is shown in (appendix b). (note 4)"
"sm is one of the most important stages of the software life cycle. this process takes a lot of time and effort. besides, it generates a huge amount of different kinds of knowledge that must be suitably managed. mas in charge of managing this knowledge might improve the maintenance process since agents would help developers find information and solutions to problems and to make decisions, thus increasing organization's competitiveness. kms is a good place where people could share their knowledge between the cop. in this case, agent's technology is a tool that could be used in order to act on behalf of cop of sm to do something repetitively and time based system especially in maintaining various types of maintenance such as adaptive, perfective, corrective and preventive. the agent techniques describes send and receive agent use to enable the user to share their knowledge among their emails and file transferring scheduled agent use to extract the file from the smp warehouse and encryption/decryption agent use as the security agent to protect the file. usability testing model use the three main criteria effectiveness was measured by the number of tasks successfully completed, efficiency was measured by amount of time taken to complete the tasks, and satisfaction was measured by a rating scale for several satisfaction elements."
"after the test subjects complete all the scenarios, he/she will answer thirty six points, eighteen related to kms and also eighteen related to mas questionnaires to record user satisfactory."
"as we can notice all respondents were able to complete all the tasks. effectiveness was measured by the number of tasks successfully completed. for other tasks respondents were able to complete in success percent ranging between (50% & 100%). the successful task completion for the individual tasks is summarized in figure 8 and figure 9 below for the respondents. as we can see the average of successfully completion task are high, according to this results the successfully completion task that presented the effectiveness, achieved correctly. moreover these results of the tasks successfully completed are high. figure 8 (note 3) shows that q2, q6, q8 and q9 are completed answered successfully (100%) and the others questions got more than 70%. figure 9 (note 3) shows that q1, q5 and q9 are completed answered successfully (100%) and the others questions got more than 60%."
"previous works on topic segmentation have adopted sequence modelling approaches based on hidden markov models (hmm) [cit], and also discriminatively trained models including conditional random fields (crf) [cit], deep feed forward neural network with hmm [cit] and support vector machines with sliding windows [cit] . however, rnns are better than fixed size windows and hmms at exploiting contextual information [cit] . moreover, these works required a segmented training corpus and used (pseudo) topic labels of these segments for training their models (although pseudo labels were obtained in an unsupervised manner). as opposed to this, our proposed model is trained using news articles obtained from the internet."
"the main contributions of this paper are (a) bi-directional rnns for topic segmentation in asr transcripts, which eliminate the need for an initial segmentation based on heuristics like fixed size windows or silence, (b) methods for discriminative training of bi-directional rnns for topic segmentation without the need of a segmented training corpus and/or (pseudo) topic labels. we consider the application of segmentation of asr transcripts of french broadcast news. the proposed model is evaluated on concatenated broadcast news videos as well as on real television news programs, and is compared to classical methods. the rest of the paper is organised as follows. section 2 presents the idea behind our model and its architecture. it further includes a discussion on how our model is trained. section 3 presents our experiment setup including the corpora, model configurations and evaluation methodology. this is followed by the segmentation results and a discussion in section 3.4 and the conclusion in section 4."
"as shown in figure 2 (note 1) the methodology discusses about the usability testing model used as the methodology to describe that the mas applied in collaborative environment will help the users according to their needs to support communities in the organization and performance aspects, as well as any other aspects that suggested by the respondents during the survey. also in these following sections, each criteria of the usability testing model with domino designer, system configuration and others software used in development of the system will be further elaborated in order to achieve its objectives."
"where t are positions of topic change in the input text. during test the output value at each t is regraded as the probability of change point at the corresponding word in the input text. this output function, unlike the softmin function, is (a) independent of the length (n ) of the input text and (b) allows training using input samples with more than one change point (for e.g. by concatenating more than two articles during training). it should be noted that a softmax function and a standard sigmoid function can be used in place of the softmin and flipped sigmoid functions. however, our choice is motivated by the fact that dot product values in (5) represent similarity values and should be minimum at the topic change point. results from our initial experiments, using different output functions, also supported this argument."
"we proposed a novel approach for topic segmentation which measures lexical and topic cohesion using bidirectional recurrent neural networks. the bi-directional rnns captured context in the past and the following set of words, and performed topic change detection by comparing the past and following contexts. these models were trained discriminatively by concatenating news articles from the internet. evaluation on asr transcripts of french tv news programs showed that our rnn models can perform better than the c99-lsa and topictiling baseline methods. our models achieved best f 1 scores throughout, due to higher precision rates, and performed well in mis-matched conditions where the baselines started lacking. with the use of standard word embeddings our model can be readily adapted to topic segmentation in different domains."
"the problem of topic segmentation, to automatically breakdown a text document into topically coherent segments, has been studied for a long time. with the increase in multimedia content on the internet, there has been an interest to extend topic segmentation to audio-video documents. multimedia documents like broadcast news programs, meeting recordings, telephone conversations and lectures commonly consist of information on more than one topic. for example, broadcast news present events related to politics, economy, sports, weather and so on. automatic segmentation of such documents, into coherent segments, is required by several down stream tasks such as topic detection and tracking [cit], summarisation, named entity extraction and for multimedia indexing and organisation [cit] ."
"a transition from off state to on state increases the floating gate charge from 0 to qfg. when charges are injected from source to floating gate, power consumption during program, wprog is calculated using:"
thank you very much for agreeing to participate in this experiment. all of your personal data that we collect will be entirely confidential. i would like to gather a bit of background information about u.
"for the c99-lsa and topictitling baseline methods, 200 dimensional lsa and lda models were trained on the same training corpus that was used to train our rnn models. stopwords were removed while training these baseline models, as well as the proposed models. additionally lemmatisation was applied in case of baseline models. c99-lsa operates on sentence level representations. since asr transcripts do not have punctuations we used fixed blocks of words instead. similarly, topictitling was used with fixed blocks of words as basic units. the block size was tuned using the validation set."
"(elst,l. v., dignum v., & abecker a., 2004) asserted a three-dimension overview on agent-mediated knowledge management which includes (i) understanding the stage in a system's development process where agents are used (analysis, conceptual design, or implementation) (ii) analyzing the architecture / topology of the agent system, and (iii) identifying km functionality / application focused on."
"the above explained issues motivated us to design a kms for capturing, managing, and disseminating knowledge in a sm organization, thus increasing the workers' expertise, the organization's knowledge and its competitiveness while decreasing the costs of the smp. to have a shared vision of the maintenance process it is advisable to define a conceptualization of the domain. an explicit specification of such conceptualization is ontology (hoffer, j. george, j. [cit] ) ."
"there is also a post-survey questionnaire that specifically examines mas techniques. after completing a task, the respondents will ask to rank satisfaction and to write down comments."
"participant is able to give a definite answer to the question. where respondents indicated they are unsure about the answer or would seek clarification, the task will record as not successfully completed."
"kms also help employees build a shared vision, since the same codification is used and misunderstanding in staff communications may be avoided. several studies have shown that a shared vision may hold together a loosely coupled system and promote the integration of an entire organization (international standards organization iso 9241-11, 1998) ."
"more consideration of the cost of sensors. by reviewing those literature, little research into the cost of sensors could be found. more cost-effective sensors are needed. to achieve this goal, several research directions can be pursued: the first is to upgrade sensor performance, which means to develop cheaper and more powerful sensors; the second is to develop efficient and useful sensor networks by optimizing distribution of sensors or data processing efficiency. further, cost assessment research is also needed."
"after the test subject completed each scenario, he/she should answer a specific questions related to the tasks. to indicate whether the tasks was clear and completed successfully."
"this theme includes many aspects, such as operation of site equipment, monitoring site environment, site security management and construction quality management. various types of sensor are used in site management because it involves many aspects."
"topic segmentation results on the tv5 test set are presented in table 2 . only asr transcription results are shown (due to lack of reliable reference transcriptions). note that this test set is from a different time period than our training set (see section 3.1) and hence there is a mis-matched train-test conditions in terms of contents and topics. results in table 2 show that our proposed models perform better than the baseline models, which seem to be affected by the mis-matched train-test conditions. on contrary to results on the euronews test set, rnn-smin gives the best results and rnn-fsig has a performance similar to that of rnn-smin. to further demonstrate the effectiveness of our proposed approach and to highlight the learning in our models we present"
"carriers in drifting to the drain end. the control gate and source are highly ndoped terminals. an oxide-nitride-oxide (ono) stack (6nm sio2/3.5 nm sin/6 nm sio2) lies in between the control gate and the floating gate, and an 8nm tunneling oxide separates the control gate from the source. extinction ratios of ~25 db and ~20 db were achieved during on and off states."
"the mos waveguide effective index change is affected by two parts; firstly, carrierinduced and secondly, wavelength shift (dispersion)-induced. these are described in eq. (n2-4) . considering eq. (n2-1), and the wavelength dispersion effect, we can get eq. (n2-5). by replacing ng in eq. (n2-5), the relationship of resonant wavelength and effective index change by carrier's dispersion can be expressed in eq. (n2-6)."
"topic segmentation can also be seen as a problem of topic change detection. a typical approach to mark a change point in a sequence is to take a window (of points) on either sides of a supposed change point and compare the adjacent windows using suitable features and representations. the comparison of adjacent windows gives a measure of whether the two windows belong to same the class or not. following such a computation for all the points in the sequence, a similarity graph is obtained in which most of the peaks/valleys correspond to actual change points in the sequence. texttiling [cit] based algorithms for topic segmentation closely resemble this methodology."
where w seg and b seg are weight and bias parameters of the feed forward layer. then the outputs corresponding to forward and backward lstm-rnn at each t are compared as:
"alizadehsalehia and yitmen [cit] conducted a survey of construction companies and found that in automated construction project progress monitoring (accpm), in terms of popularity, global positioning system (gps) and wsn are important for the accpm. moreover, siddiqui [cit] introduced site distribute scheme and management strategy of sensors."
"participant is able to give a correct answer based on his own information about the system. any guessed or assumed answers, whether correct or not, are not record as successfully completed tasks."
"the send and receive agent communicates with the other two agents. when it receives information, it processes it in order to determinate to what agents should the information be sent to. since specific information may influence the knowledge managed by different agents, the send and receive agent must know the relationship that exists between all the agents."
"the review described in this paper was divided into five stages, as specified in table 1, such as searching for literature, clarifying research directions, data evaluation, pointing out research gap and provide suggestions, and conclusion. technology), sustainability, operation and maintenance, site management, structural health monitoring, positioning and tracing, and planning and design. the methods of integrating bim and sensor technology is a fundamental research area, which forms the basis for subsequent applied research. in total, fifty (50) academic publications are related to integration method. in addition, sustainable building, site management and operation & maintenance are popular research themes; academic publications related to these aspects were sixteen, fourteen and thirteen, respectively. further, there are seven out of 57 research related to structural health monitoring, seven out of 57 to positioning and tracing, and five out of 57 to planning and design."
"the changeable character of the smp requires that the information generated be controlled, stored, and shared. we proposed in order to manage the knowledge generated during maintenance a mas formed of three agents are under the client agents implementation. one agent, called the send and receive mail agent, is in charge of organizing the information sent and received from the group. the second agent is scheduler agent and the third agent for the security. the rest of the agents are also communicated, thus enabling them to interchange information. the main goal of this paper is to design and applying mas techniques-based kms in a collaborative environment of lotus notes to facilitate knowledge sharing of smp among the users of the community of practice. this techniques was inspired by the \"there is lack of model of mas used in sm in order to product the sharing of knowledge in smp\" and also\" there is inconsistency of mas using in test of its functionality\" an experiment is setup setting based on the proposed usability testing model is discussed. the testing is specially designed to verify the significant of the send and receive mail agent, decryption and decryption file agent and file transferring schedule agent and the algorithms used, and to get user satisfaction on the overall system. the success of system is evaluated through user satisfaction survey which covers (i) file sent (ii) file encrypted/ decrypted (iii) file extracted from smp data warehouse, and (iv) smp data warehouse technique."
"mas developed for job shop scheduling problems in which standard operating procedures are combined with a look-ahead coordination mechanism that should prevent 'decision myopia' on part of the agents. using their approach, system performance is said to improve in tightly-coupled, real-time job-shop scheduling environments. however, their coordination mechanism is not appropriate for competitive, self-interested agents, which makes it an undesirable choice for coordination in a de-icing setting (liu and k. p. [cit] ) ."
"this work is funded by the contnomina project supported by the french national research agency (anr) under contract anr-12-bs02-0009. experiments presented in this paper were carried out using the grid'5000 testbed, supported by a scientific interest group hosted by inria, cnrs, renater and other universities and organisations (https://www.grid5000.fr)."
"building information modelling (bim) has received much attention in academia and architecture/engineering construction sector [cit] . bim is defined by the us national bim standard as \"a digital representation of physical and functional characteristics 1 permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. csae '18, october 22-24, 2018 of a facility and a shared knowledge resource for information about a facility forming a reliable basis for decisions during its life-cycle; defined as existing from earliest conception to demolition\". in broader terms, \"bim refers to a combination or a set of technologies and organizational solutions that are expected to increase inter-organizational and disciplinary collaboration in the construction industry and to improve the productivity and quality of the design, construction, and maintenance of buildings\" [cit] . from the report about construction industry informatization development of china, currently bim involves many kinds of technology such as 3d scanning, internet of things (iot), geographic information system (gis), 3d-printing etc. and is applicable in lots of aspects in building management. according to isikdag [cit], the first evolution of bim was from being a shared warehouse of information to an information management strategy. now the bim is evolving from being an information management strategy to being a construction management method; sensor networks and iot are technologies needed in this evolution. the information provided by the sensors integrating with the building information, becomes valuable in transforming the building information into meaningful and full state information that is more accurate and up-to-date [cit] . therefore, this paper provides a brief review to evaluate and clarify the state-of-art in the integration of bim and sensor technology. a systematic approach was adopted in reviewing related publications. methods of integrating the two technologies were reviewed. a brief summary is given to highlight research gaps, and recommend future research."
"with a kms the staff may also be informed about the location of information. it is critical for maintenance engineers to have access to the knowledge the organization has carried out a study which found that the number one barrier to knowledge sharing was \"ignorance\": the sub-units are ignorant of the knowledge that exists in the organizations, or the sub-units possessing the knowledge are ignorant of the fact that another sub-unit needs such knowledge. sometimes the organization itself is not aware of the location of the pockets of knowledge or expertise (orton, j.d., & weick, k. [cit] ) .this fact has been summarized by management practitioners as \"the left hand not only does not know what the right hand is doing, but it may not even know there is a right hand\" [cit] ."
"user interface design is the design of computers, software application, and websites with the focus of the user's experience and interaction. it's also creates and effective communication medium between a human and computer. an effective user interface design must able to understand how users think and needs. any user interface contains the name, mail, phone number, year of experience and user expertise as shown in figure 5 (note 1). the author followed the steps above in order to design the all user interfaces design that exist in the system. logical and physical database design is the efficient guidelines for design database and file. file and database design occurs in two steps, we begin by developing the logical database model, which describes data using notation that corresponds to a data organization used by data management system. this is the system responsible for storing, retrieving, and protecting data as shown in figure 6 (note 1)."
"more commercial applications. from the review, optimizing user experience is an important aspect of commercial applications, but more commercial potentials could be realized. in future works, users' behavior data may be collected by sensors, if possible. data mining can uncover building occupant preferences, which is potentially useful for targeting business activities (albeit with ethical concerns)."
"the contributions of the paper are three-fold. first, this research identifies components for agent-techniques-based knowledge sharing system of smp. second, finding suitable approach for mas techniques to be used in knowledge sharing system of smp. third, provide reasonable background for applying existing usability testing of mas techniques-based kms to facilitate knowledge sharing of smp and describe whether mas techniques is significant in knowledge sharing of smp through user satisfaction experiment."
"higher accuracy of positioning and tracing. according to the review, the accuracy of current positioning system is over 1.5m. this accuracy is relatively adequate for indoor navigation and most site works. but the accuracy still needs to be improved to meet higher navigation and facility management requirements, which means that the higher performance sensors and more scientific algorithm need to be developed in future works. more applications in structural design. the integration of bim and sensors has been used in layout plan and facility design, but applications in structural design are relatively lack of investigation. from the review, several methods of monitoring structural mechanical conditions have been developed, so a knowledge based containing structural mechanical monitoring data could be established to assist further restructure, expansion, and similar structures' design work."
"an all-pass mrr was described in the main manuscript. using the mrr, an for carriers injected into mos waveguide, the oscillation wavelength is shifted as described in eq. (n2-3). this is because mos waveguide effective index has changed."
"most topic segmentation approaches were originally tried on textual resources or manual transcriptions of spoken resources. unlike on text documents, topic segmentation on transcriptions obtained from an automatic speech recognition (asr) system cannot readily exploit sentence boundaries. in this regard, some works have used a fixed block of words as an unit [cit] while others have relied on pauses in speech [cit] . interestingly, prosodic cues and automatic sentence segmentation techniques have been leveraged in other works [cit] . performance of topic segmentation on asr transcripts is also affected by word errors from the asr. to reduce the effect of word errors, the use of asr confidence measures and lattices have been proposed [cit] . discriminative features from the speech signal, speaker patterns and news structure have shown to improve performance on spoken documents [cit] ."
"first agent, called the send and receive mail agent, is in charge of organizing the information sent and received from the group. the second agent is encryption and decryption agent, use to protect the sm files that existed into smp warehouse and the third agent is file transferring scheduled agent, use to extract the file from smp warehouse and apply it to the user that requested to share it with other users."
"in environment protection subtheme, [cit] concerned about the rational use and conservation about natural resources. they used sensor network and bim to monitor the usage of water resources and established an intelligent management system to manage water resource smartly. similarly, [cit] concerned about carbon emission from buildings. they established a quantitative relationship between carbon and energy consumption data and natural gas consumption data collected by sensors, and founded a carbon emission model via bim, which can assist carbon emission management and related decision making."
"by using the groupware of lotus notes [cit], the best agent technology capability that could be developed is used java script programming that comes along with this package. the examples of the ide and scripting development of interface are shown in figure 7"
"the topic segmentation models will be evaluated on the reference and asr transcriptions of the euronews test set, and on the asr transcriptions of the tv5 test set. the asr transcriptions are obtained from our french asr system based on dnn-hmm acoustic models. our asr gives a word error rate of 16.4% on the euronews test set."
"completion time is the one factor used for measuring efficiency in this paper. efficiency was measured by the amount of time taken to complete all tasks. an average of 44 minutes and 3.5 seconds per respondent was taken to complete the all tasks. however, there was much variation among the respondents, for example, the fastest respondent took only 18 minutes and the slowest took 37 minutes and 9 seconds which are about three times longer. pearson's product-moment correlation analysis was conducted to see if the respondents' completion time is related. the results showed that total completion time is independent, (see figure 10 and figure 11 ) (note 3)."
"knowledge about agent concept alone is not sufficient to build a good agent system. there are some fundamental issues needed to drive the design of an agent (bigus, j. p., bigus, j., 2001 ). the first is to view the agents as adding value to a single standalone application, or as a freestanding community of agents that interact with each other and other applications. the first type views the agent from the perspective of application-centric, where the agents are helpers to the application, while the second is more agent-centric, where the agents monitor and drive the application."
"the model with the softmin function in the output layer, as discussed in section 2.3.1, will be denoted as 'rnn-smin'. it is trained using samples obtained by concatenating 2 articles at a time from the l'express corpus. model with the flipped sigmoid function in the output layer, as discussed in section 2.3.2, is trained with samples obtained by concatenating 2 to 4 articles at a time from the l'express corpus. this model configuration will be denoted as 'rnn-fsig'."
"for program operation, the pn junction is forward conduction state. we assume it is equivalent to the conductor. from the conservation of charge in a capacitor:"
"as previously unarticulated expertise and experience resident in individual workers. km issues include developing, implementing and maintaining the appropriate technical and organizational infrastructure to enable knowledge sharing [cit] ."
"this direction is to develop a method to locate or trace facilities or people inside a building by using sensors. positioning and tracing can be applied in many occasions, such as emergency management, site security management, user experience optimization and facility management."
"for training our model we concatenate randomly chosen articles from the l'express dataset. our validation set consists of 3000 samples formed by concatenating 2 articles at a time from euronews text articles. our first test set, referred as the euronews test set, consists of 3000 samples with each sample obtained by concatenating 2 to 7 videos from euronews. (news videos from euronews are 2 to 5 min in duration.) for training and test, the punctuation marks are removed and all words are converted to lower case, as it would be with asr transcripts."
this theme is concerned about the monitoring of mechanics situation of structures and the discovery of structure defects. structural defect can be divided into two types: structural partial
"for this reason, different solutions are often used in order to solve the same problem. using a kms which acquires workers' knowledge and transmits it, the above mentioned situation would decrease since all workers could benefit from other employees' experience and the organization would increase its expertise and coherence of information."
"our proposed topic segmentation model is also based on a similar idea. the input is a sequence of words hypothesised by an asr system and topic segmentation is to be performed on this 1-best asr hypothesis. words in the asr hypothesis are represented using word embeddings. instead of using fixed windows of words, rnns are used to model the long term topic context and to perform change detection. more specifically we choose a bi-directional rnn [cit] with lstm cells. the bi-directional rnn allows to model context from the past and the following set of words, and it could be trained to measure topic cohesion between these contexts."
"architecture design is the design process for applying mas in a collaborative environment and the framework for mas control and communication. usually it involves applying major system components and communication. figure 4 (note 1), describes the communication between the agent and the whole system among the users mails and also demonstrate the agents into the system. smp requires that the information generated be controlled, stored, and shared. we propose in order to manage the knowledge generated during maintenance a mas formed of three agents are under the client agents implementation. this part contains the description for the applied mas and describes the name, job, and the language used. first agent, called the send and receive mail agent, is in charge of organizing the information sent and received from the group. the second agent is encryption and decryption agent and the third agent is file transferring scheduled agent. the send and receive agent communicates with the other two agents. when it receives information, it processes it in order to determinate to what agents should the information be sent to. since specific information may influence the knowledge managed by different agents, the send and receive agent must know the relationship that exists between all the agents. they also mediate communication among people, and this is of prime importance. in this case, agents will act as a communicator for the user that is based on the direction given and produces the result when it is required to do so. agents also could be categories in terms of its roles in knowledge searching, knowledge monitoring, and others."
"the changeable character of the smp requires that the information generated be controlled, stored, and shared. we propose in order to manage the knowledge generated during maintenance a mas formed of three agents are under the client agents implementation."
"approaches to topic segmentation are based on the idea of lexical cohesion [cit] . some of these methods analyse the lexical distribution across the document and mark segment boundaries in areas of low cohesion. this includes the original texttiling algorithm [cit] and its extensions using lexical chains [cit], semantic/topic space representations [cit] and laplacian eigenmaps [cit] . another set of segmentation methods try to cluster together neighbouring areas instead of directly looking for topic boundaries. this includes the prominent c99 algorithm [cit] and its extensions using semantic/topic representations [cit] . alternative methods based on generative probabilistic models have also been proposed for topic segmentation. these include extensions of classical probabilistic topic models to incorporate topic changes and boundaries [cit], and approaches which directly model words in each topic segment as draws from a corresponding multinomial language model [cit] ."
"ten individuals participated in this study. after collecting the background questionnaire we calculate the demographic characteristic of the respondents. table a .1 summarized the basic characteristics of the initial sample as well as those of the respondent. as can be noted from the table above, the sample is rather skewed towards males in the age group between 23 and 40 years old and of higher education. most participants were experienced and familiar with lotus notes software: forty percent (40%) used the system on a daily basis and were familiar with the lotus notes system and its applications; moreover sixty percent (60%) have shown and described the system for them."
"where, f () represent the operations inside an lstm cell with forget gates (refer [cit] for details). hidden layer activations of the forward and backward lstmrnns are transformed using a fully connected feed forward layer as follows:"
"systems design is the process or art of defining the architecture, components, modules, interfaces, and data for a system to satisfy specified requirements [cit] ."
"our model consists of a layer of bi-directional rnn with lstm cells. to understand the functioning of our model, a bi-directional lstm-rnn is depicted in figure 1 ."
"as discussed in section 2.2, the training objective is to mark the boundary between the concatenated training set samples. recall that in the last layer of our model, in (5), the similarity value should be minimum at the boundary. accordingly, we present two possibilities for the choice of the output function g(), which can achieve this training objective."
"handle the aspects that come into play in km, such as the existence of a strategy, the processes of quality control of data, the content that is being managed, and the functioning of communities of practice."
"in this paper, we focus on lexical cohesion based topic segmentation on the asr 1-best hypothesis. in contrast to the previous works, we propose a novel approach based on recurrent neural networks (rnn). recently rnns have been shown to effectively model sequences like text, speech, music and videos, and have given state-of-the-art results in several sequence and temporal classification tasks [cit] . their ability to model long term context and the potential to train them discriminatively motivates us to try rnns for the task of topic segmentation. more specifically, we aim to capture lexical cohesion using a bi-directional rnn with long short-term memory (lstm) cells [cit] . the bi-directional model would capture context from the past and the following set of words. these past and following contexts can then be compared to perform topic change detection."
"the respondents including system analyst, system developer, software engineer, and user and will be chosen to fill the questionnaire of this study. the respondents should be applying the system before solving the questionnaire to be situated."
"this phase involves evaluation of existing software maintenance processes and activities and literature reviews on academic journals, software maintenance books, and information search on groupware and our cop tools, mass applications, usability testing model and other tools supporting software maintenance activities ( described in section 2 above)."
"where qfg is the charge at the floating gate terminal, qcg is the charge at the control gate terminal, qd is the charge at the drain and qs is the charge at the source. qcg, qs"
"as a result, efficiency was measured by evaluating completion time used in this survey by each respondent. respondents who they were familiar with the systems in general tended to use less time to complete their tasks. when the respondent knows how to get the answer, it takes them fewer time while when they don't know how to use the system, they take more time."
"in this phase the actual coding or development of the system is done. the system designs are translated into code. based on the system design, the work is divided in modules/ units. with respect to the type of application, the right programming language is chosen. in this stage, resources such as programming language, database and compiler will work together to produce a system that can function effectively."
"(dignum, v., 2006) proposed operation per organizations (opera), a model for agent's organization, society and interaction model. the organizational model implements the desired organizational structure of an agent society, the description of an agent population that will enact the roles described in the structure is detailed in the social model, and the specification of agent interactions to achieve the desired society global objectives is described in the interaction model. however, this model needs other agent oriented methodology to help designing the system. (park, s., sugumaran, v., (2005) introduced a framework of multi-agent system (mas) development that considers both functional (services to solve complex problems in distributed environments) and non-functional service (capability to reuse, easy to extend, adapt and process uncertain data) of the system. they also suggested that, in order to develop mas in a systematic way, system should be analyzed in terms of its ultimate goals and the system should be designed both in the abstract as well as concrete by mapping the goals and the sub-goals to software agents."
"our training set consists of news articles crawled from a news website, as detailed in section 3.1. each article contains text around a particular news event. to train our model, two or more news articles are randomly chosen and concatenated. then the training objective is to mark the boundary between the concatenated articles as a topic change point. the assumption is that the training set has a significantly large number of news articles and from a wide time period (ranging over months). with such a training set, it is very less likely for a generated training sample to contain two or more news segments which are on the same news event and adjacent to each other. even otherwise an unsupervised topic model, like latent dirichlet allocation (lda) [cit], can be built on the training set and topic distributions of adjacent segments can be compared to avoid concatenation of articles on same news events. however, a detailed analysis on selection of training set samples is not in the scope of this paper and we form our training set using news articles spread over a period of 6 months."
"where r is the rotation matrix, t is the translation vector, p i and q i is the corresponding point pair between the p and q point clouds."
"in this study, a global registration method is developed to overcome one of the most challenging problems remaining in current 3d scanning systems for accurate image registration, even with no reference coordinate existing between the neighboring scanned surface patches. the key innovation in this work is the strategy in determining a robust registration transformation between two neighboring scans. the experimental results demonstrate the validity and applicability of the proposed approach."
"comparing to the signal model in (2), the components x(t) and y (t) can be considered as the first imf f 1 (t) and the residual r 2 (t), respectively. the reason that we set a common frequency of each x [i] (t) and y [i] (t) is to facilitate the discussion. since the memd, as the same as the vemd, has the mode alignment property (see [cit] ), different frequencies involved in the components x"
"in this paper, we explain how to realize the vector-valued emd (vemd) with an optimization based back projection. to simplify the discussion, a typical signal decomposition model in 4-d space (3-d location and time) will be studied. in the following, section ii recalls the recently developed multivariate emd (memd) with approximately uniform direction sampling, section iii introduces the proposed vemd, section iv presents the numerical studies, and section v concludes the paper."
"point cloud, is determined in step 2. in the next step, the generation of the descriptors of the point cloud is performed. then, the matching process between the descriptors is implemented and achieves the initial transformation that can be used for further refined registration using icp."
"if p and q represent the same free-form shape, the least squares method can be used to calculate the initial rotation and translation by minimizing the sum of the squares of alignment errors. let e be the sum of the squares of alignment errors. accordingly,"
"in the vemd, the order of the derivative operator in (6), the projection direction number and the prim base in (5) are free parameters, each of which may effect the behavior of the method. to determine each one, we study the decomposition problem of the following vvs"
"in section ii-b we have shown that, given a fixed prime base for projection sampling, the local mean approximation operator m[·] might be considered as an ensemble approach. in practice, we have to determine a finite projection number such that the approximation would be good enough or at least approximately convergent. let's consider again the vvs f (t) in (7). if we apply the operator on f (t) with a fixed projection number m, m m [f ](t) shall be the approximated local mean, e.g. m fig.4 (a) describes the approximation performance of the operators"
"comparing to the memd method in section ii-b, the vemd employs the back projection to obtain the vves from the envelopes interpolated in the 1-d projected space w.r.t a selected projection direction."
"rotate q around the three axes of the coordinate system defined by the center of gravity of q as the origin and the three unit vectors considered as the three directions of the obb of point cloud p with every increment ∆θ on each axis, generating the obb regional area-based descriptors of q 4 match the descriptors between p and every q having an increment ∆θ of rotation performed in step 3."
"the recent memd method has been shown its strength for vvs decomposition, especially for mode alignment problem and noise-assisted applications. the white noise decomposition test illustrates its remarkable dyadic filter bank property comparing to classic emd and ensemble emd [cit] . the whole algorithm can be summarized in algorithm 1."
translate the obb of p to q using the translation matrix defined by the centers of gravity of p with its estimated missing points and q with its estimated missing points
"t in the original 4-d space ( fig.2 (a)-(c) ). fig.2 (e) illustrates that the projected vves (p[u m ](t), p[v m ](t)) are nothing but the envelopes (u p (t), v p (t)) in the projected space. in other words, fig.2 graphically explains why the memd interpolates the vves directly in the original space but not in the projected space. now, based on the interpolated envelopes together with the lels in the projected space, we can obtain the 4-d vves using the proposed back projection method, i.e. solving the optimization problems (p1) and (p2) described in sec.iii-a. considering that, normally, the first order derivative won't be an optimum choice for smoothness measurement [cit], we set the 2nd or 3rd order derivative in (6) alternatively for simulation. fig.3 presents the corresponding solved vves based on the data shown in fig.2 (d) ."
"and its interpolated vves um(t) and vm(t) w.r.t an unit projection direction p; (d) the projected signal pp[f ](t) and its interpolated envelopes up(t) and vp(t); (e) the absolute difference between the projected vves and the envelopes in the projected space, e.g."
"2 ], we can easily obtain the projected vvs p p [f ](t) together with its lels t k + /t k − (fig.2 (d) ). based on these"
"in the first step of the matching process, the obb's parameters of point cloud p are compared with the obb's parameters of point cloud qi. if the obb matching satisfies the given condition, the regional area-based descriptor of point cloud p is then compared with the regional area-based descriptor of qi. these two obbs should be satisfied with the following equation:"
"in this paper, we concentrate on the latter, more difficult, case. we define the vector-valued signal as follows. definition 1.1: given a finite number of 1-d signals"
", where e denotes the expectation over the direction p m . in real application, the projection number m needs not to be a large number compromising both computational complexity and approximation performance."
"in addition, the weight loss at the missing parts of two the scan series may result in a significant bias between the translation matrix being estimated (by the difference between the centroid of p and the centroid of q) and the real translation matrix. the shifting of two gravity centers (the red and green points shown in figure 4 ) being created using a virtual camera from a model having the centroid (white point) in figure 4a illustrates the influence of weight loss from the missing points to the centroid point position. assume that the two point sets are separated as shown in figure 4b . the accurate translation matrix to merge set q (green point cloud) with set p (red point cloud) is defined through the difference between the current centroid of p and the original centroid of the green point cloud (green points). therefore, it is necessary to estimate the original centroid of p on the object for translation."
"to find the transformation between two series of scans that are acquired from multiple single scans, the obb regional area-based descriptors of source point cloud are matched with the descriptors of the target point cloud. figure 2 illustrates the concept of the proposed registration method based on the obb regional area-based descriptor to register two series of scans which are represented by the red and green point cloud for the target and source 3d point cloud that are scanned and obtained from the optical probe. each of the proposed descriptors includes two critical components. the first component contains the information about the obb of the point cloud. each obb is represented by a datum corner, c(x c, y c, z c ), and three vectors, cc 1 (x max, y max, z max ), cc 2 (x mid, y mid, z mid ), cc 3 (x min, y min, z min ), corresponding with the maximum, middle, and minimum dimensions of the obb, respectively. the second component represents the spatial distribution of the surface area of the object in the obb. the similarity between the regional area-based descriptor representing the source point cloud and the regional area-based descriptor representing the target one is then determined by using the normalized cross-correlation."
"rotate q around the three axes of the coordinate system defined by the center of gravity of q as the origin and the three unit vectors considered as the three directions of the obb of point cloud p with every increment δθ on each axis, generating the obb regional area-based descriptors of q 4 match the descriptors between p and every q having an increment δθ of rotation performed in step 3. 3 5 determine the best machine and obtain the initial transformation matrix calculate the obb regional area-based descriptor of p"
"in this section we provide a discussion on the influence of parameters on the performance of the proposed method. in addition, a comparison against several already published local feature descriptor methods is also introduced. the considered approaches are: spin images (si), point signatures (ps), point feature histograms (pfh), and the signature of histograms of orientations (shot). all methods were implemented in c++ using the open source point cloud library (pcl). to estimate the computation cost of the registration process, the experiments are processed on a computer with an intel core i7 processor with 3.40 ghz and 8 gb ram."
"in this section, a different emd extension, namely the vemd, will be explained, in which the vves of the given vvs are generated by back-projecting the 1-d envelopes interpolated in the projected space to the original 4-d space. since the naive back-projection might result in infinite solutions, a novel optimization scheme will be designed to guarantee an unique vve which should be also as smooth as possible."
"to find the transformation between two series of scans that are acquired from multiple single scans, the obb regional area-based descriptors of source point cloud are matched with the descriptors of the target point cloud. figure 2 illustrates the concept of the proposed registration method based on the obb regional area-based descriptor to register two series of scans which are represented by the red and green point cloud for the target and source 3d point cloud that are scanned and obtained from the optical probe. each of the proposed descriptors includes two critical components. the first component contains the information about the obb of the point cloud. each obb is represented by a datum corner, c(xc, yc, zc), and three vectors, cc1(xmax, ymax, zmax), cc2(xmid, ymid, zmid), cc3(xmin, ymin, zmin), corresponding with the maximum, middle, and minimum dimensions of the obb, respectively. the second component represents the spatial distribution of the surface area of the object in the obb. the similarity between the regional area-based descriptor representing the source point cloud and the regional area-based descriptor representing the target one is then determined by using the normalized cross-correlation. to make the proposed method clear in its operation procedure, the following flow chart diagram, shown in figure 3, as well as algorithm 1, is used to describe the proposed method in its main five steps. in step 1, the boundary points located near the scanning point missing zone can be detected through the geometric relationship with the support plane, using a set of connected edges that satisfy two conditions, in which all the boundary edges in the point missing zone have a similar orientation and each edge lacks an assigned triangle, either on the left or on the right side [cit] . then, the oriented bounding box (obb), which is a rectangular bounding box that covers all the object point cloud, is determined in step 2. in the next step, the generation of the descriptors of the point cloud is performed. then, the matching process between the descriptors is implemented and achieves the initial transformation that can be used for further refined registration using icp."
"the first term in equation (9) does not depend on t 1 . therefore, the total error is minimized if t 1 \" 0, and then:"
"in this section we provide a discussion on the influence of parameters on the performance of the proposed method. in addition, a comparison against several already published local feature descriptor methods is also introduced. the considered approaches are: spin images (si), point signatures (ps), point feature histograms (pfh), and the signature of histograms of orientations (shot). all methods were implemented in c++ using the open source point cloud library (pcl). to estimate the computation cost of the registration process, the experiments are processed on a computer with an intel core i7 processor with 3.40 ghz and 8 gb ram."
and the corresponding hammersley points set is [cit] it should be noted that a more satisfactory sampling scheme in multi-d space can be generated by introducing different prime bases which connects to the hammersley points based on halton sequences [cit] .
"to obtain equidistributed direction on a unit sphere, in this paper, we employ a low-discrepancy sampling scheme based on transformed hammersley points, which can produce more uniform samples on the hypersphere than other methods, like the polar coordinate lattices or rotation method [cit] ."
where n is the number of points in the object point cloud. the covariance matrix of the input point cloud cov is then computed as follows:
"the oriented bounding box is a rectangular bounding box that covers whole object point cloud. each obb is represented by a corner, c(x c, y c, z c ), and three vectors, cc 1 (x max, y max, z max ), cc 2 (x mid, y mid, z mid ), cc 3 (x min, y min, z min ), corresponding with the maximum, middle, and minimum dimensions of the obb, respectively (as shown in figure 5 ). [cit] . the algorithm is started by obtaining the centroid p px, y, zq of the object point cloud:"
"where d (n) is the nth order derivative operator as its matrix form for discrete problem is well-known . now, the back-projected vves can be uniquely obtained by solving the following optimization problems"
"in this experiment, the mean deviation value is 0.0347 mm and the standard deviation value is 0.0235 mm. figure 10 illustrates the other examples being achieved by the developed method. figure 10 . the triangle meshes of three objects being scanned by the developed method."
"to keep the terminology from previous papers, in this section, the memd concerns the vvs decomposition problem. for a concrete 4-d decomposition problem, the signal model (1) can be generalized as"
"if the projection number approaches infinity, m[f j ](t) should be the expectation of the local mean approximation based on the projection. such operations belong to a typical ensemble approach. this implies that the uniform direction sampling scheme should be an optimal choice [cit] . moreover, decomposition tests with white noise show that such emd-like decomposition of vvs perfectly inherit the dyadic filter bank property of the classic emd [cit] ."
"in fact, (p1) and (p2) are both quadratic optimization problems with equality constraints. they can be written as quadratic optimization problems without constraints by solving the linear constraint system and then implementing the variable reduction."
"keeping the notation system in algorithm 1 but ignoring the trivial subindexes, in this subsection, h(t), p m denote the considering vvs and the projection direction, u pm (t)"
translate the obb of p to q using the translation matrix defined by the centers of gravity of p with its estimated missing points and q with its estimated missing points
"to verify the feasibility of the developed methodology, some experiments were performed and evaluated. in the experiments, the point cloud of a hammer head were acquired from the ntu-developed optical 3d measuring probe being developed by using a random speckle pattern projection and triangulation measurement principle [cit] . the dimensions of the hammer head are approximately 110ˆ35.5ˆ21 mm 3 . firstly, the hammer head was placed on a fixed table, allowing for viewing the object from several positions around the object. however, the whole object surface could not be detected due to optical occlusion. therefore, after the first series of automated scans to obtain point cloud p, the object was reoriented manually and the second series of automated scans was continuously carried out to acquire point cloud q. figure 8 illustrates the coarse registration process between the two series of scans. the width of the normalized cross-correlation peak depends on the number of iterations and ∆θ, as shown in figure 9 . meanwhile, the height of the normalized cross-correlation depends on the number of obb segments k 1ˆk2ˆk3 and ∆θ."
"in this experiment, the mean deviation value is 0.0347 mm and the standard deviation value is 0.0235 mm. figure 10 illustrates the other examples being achieved by the developed method."
"error function for computing the costs. an error function is used to ensure the visual quality of simplification. the value of the error function, usually defined as the cost, indicates the amount of visual changes after an edge is collapsed. based on the description in [mel98], we collapse the minimal-cost edge after computing the costs of all edges using equation 1."
"the quality of a model is highly dependent on the underlying data. the data for the aforementioned expectable climatic influences (water, snow, ice, foliage, dirt) has been investigated theoretically and partially verified by measurements. in the following the datasets of each medium are presented."
"we generate two 360-degree camera turning movements for each model (see figure 8) . we run 300 frames for each of four camera setups, and use them to test the performance."
"performance factors of gpu defragmentation. the defragmentation re-organizes the data on gpu. the more data is used for rendering a frame, the more time is required to defragment them. based on our experimental results from (a) and (b), we notice that the time of gpu defragmentation is scaled with respect to the number of visible triangles and visible vertices determined by our lod selection method. in figure 10 -(iii), we plot the relationship between defragmentation time and the number of visible data; and each dot represents a frame. it shows that defragmentation time changes linearly over different numbers of triangles and vertices."
"similar to snow, foliage can also be considered as a mix of different components. the components in this case correspond to high permittivity water, low to moderate permittivity organic material, and air. [cit] provides a model for the dielectric permittivity. this was designed for foliage of different plant species and provides information about its figure 5 . one-way absorption rates in db mm −1 of water for different temperatures, with focus on 5.8, 24, and 77 ghz and the corresponding slope for thick water layers (right value in brackets) impact on electromagnetic waves. from this, the dielectric permittivity of foliage is given as"
"figure 7: an example of defragmenting triangles on gpu. ta, t b, tc, t d and te will be substituted with a source triangle from existing or additional block of triangles in parallel."
"mesh simplification algorithms have been an active research for decades in computer graphics. given an input 3d model, a less complicated but visually faithful representation can be approximated as an alternative for rendering. current algorithms of mesh simplification have been designed according to a series of operations on geometric primitives, such as vertex decimation [szl92], edge collapse transformations [hop96, gh97], region-merging measurement [rrr96] and selective refinement [hop97] . a well-known approach of mesh simplification is progressive meshes [hop96] simplifying meshes with edge-collapsing criteria, then different levels of detail of meshes can be recovered by applying a prefix of splitting sequence to the base mesh."
"1. cpu-gpu data streaming. in this step, we first need to collect the vertices and triangles not existing on the gpu but required to render the next frame, and store them in a block of continuous cpu memory. then, we transfer this data block to gpu memory with a single memory transfer call. 2. gpu data defragmentation. for the reason of efficient processes in triangle reformation and rendering, the geometry data of all the meshes are concatenated into a single continuous memory block. however, the frame-toframe coherence approach does not preserve the continuity and the order of geometry data in the gpu memory."
"we concentrate on the intensity values close to the contour, i.e. we bin the intensities observed in distances from −25 to 25 in 1 u increments, while remaining bins are not included."
wavelet analysis allows one to simultaneously examine the frequency and resolution components of a signal. this is accomplished by iteratively decomposing the signal with high and low bandpass filters. the low-frequency filter serves the purpose of downscaling
financial support. this work was supported by the german research foundation (dfg) and the technical university of munich (tum) in the framework of the open access publishing program.
where h is the bandwidth parameter and controls the smoothness of the estimate. this window allows us to provide a smoother and more robust mi calculation when dealing with sparse data.
"as a matter of fact, the detection of an object can only be successful if a signal is received. for this purpose, radomes are generally employed to protect the system. these radomes shall not affect the antenna characteristics nor the transceived signal in any way. depending on the system, generally a signal is transmitted and the reflection of objects within the propagation path of the electromagnetic wave is processed. the received signals must be sufficiently strong to be employable. the radar equation,"
"according to fig. 1, a safety-critical radar sensor system is taken into consideration in the following. the surveillance system monitors a hazardous area, which is unproblematic under standard weather conditions. climatic influences affect the radar sensor performance due to absorption of energy. the solution is to minimise the impact of absorbing or reflecting layers within a predefined range, all that goes beyond these limitations results in blindness of the sensor. there are several possible ways to overcome the effects of absorbing material, the most naive solution is to increase the transmitted power in such a way that the absorption is compensated. however, as the attenuation of water can easily reach high values, such an approach can only counteract insignificantly small covering layers. especially for low-cost and cost-critical systems with little dynamic range, simply increasing the transmit power is a less favourable methodology. a more effective approach is to optimise the whole system. this means that not only the radome is optimised for maximum performance, but also the snow, dust, or water layer that is expectable and within the absolute maximum range is taken into account."
"algorithm 2 defragmenting triangles on gpu procedure triangledefragmentation( in array tc f +1, array tc f, arraytc f, existing triangles, additional triangles; out active triangles)"
"in addition to providing characteristic priors on the shape through hierarchical pca, we also model the filtered intensity values around the contour of the embryos. we create a set of histograms of the intensity values by binning observed intensity values by their respective signed distance."
"our simplification method is based on the idea of collapsing edges of a mesh, where the edge-collapsing operations are applied to an original mesh iteratively according to a pre-defined order. therefore, dependencies have to be introduced between the iterations of collapsing. at an edgecollapsing iteration, an edge is removed based on a specified cost, and the topological structure of the mesh is modified. the next iteration has to rely on the resulted mesh of the previous iteration. such dependencies make the design of a parallel algorithm very difficult. in this section, we will introduce an approach that can remove the dependencies effectively to support the parallel process of mesh simplification."
"limitations. our approach assumes the high temporal coherence between frames. if the camera is changed dramatically from one frame to the next, the amount of transferred data based on the frame difference could be increased significantly. as a result, it may lead to a noticeable performance lost. another limitation of our system is that we require the entire 3d model can fit into cpu main memory."
"the berkeley drosophila in situ database consists of 78 621 images of 3724 genes expressed in drosophila embryos across six time windows (covering the developmental stages 1-3, 4-6, 7-8, 9-10, 11-12, 13-15 ). [cit] . annotations are based on an ontology describing embryonic expression patterns, consisting of 314 terms, and were obtained from the latest release of the database [cit] . the annotation set was curated by the bdgp group by manually inspecting the in situ images and providing ontology terms for each gene at every time stage. additionally, information on the orientation of the embryos (i.e. the viewpoint) was manually curated for the stage window 4-6."
"to summarize, our method differs from leventon's method by three main distinctions. first, the effect of the intensity distribution is limited to the area around the zero crossing of the evolving shape. this is necessary to limit the contribution of internal staining and multiple/impeding embryos to the score. it also allows for a narrow band approach to be used with minimal effect on the overall score which increases the runtime performance. second, the variability in size of the objects is normalized prior to creating the shape model. this is required to provide accurate representations of the actual differences in the shape of the drosophila embryo which are not simply related to size. the size of the embryo then becomes an additional parameter in our optimization step. lastly, we provide an additional term that is based on the direct values of the filtered image. this term is not decomposed into individual components, but rather describes the distribution of the filtered image in relation to the signed distance location. this allows the registration to accurately detect and align the shape model to the correct position of the image."
"[cgg * 04] used a geometry-based multi-resolution structure for out-of-core data management. the approach constructed a hierarchy of tetrahedra by recursively partitioning the input dataset. each tetrahedral node contained a simplified mesh representation (or a patch) precomputed in a fine-tocoarse manner. during the run-time, the hierarchy was topdown traversed to fetch the appropriate patches from disk to cpu. [ysgm04] represented a 3d model as multiple progressive meshes in a clustered hierarchy (chpm). at runtime, based on a list of fetched clusters, the desired model was generated by performing the refinement operations and cluster merging. however, both approaches of [cgg * 04]"
"note that, tv a is the set of triangles containing vertex va, and tv a v b is the set of triangles containing both vertices va and v b . in equation 1, the cost of an edge is affected by both edge length and curvature."
"an alternative way of memory copy is to manipulate each element of the block in parallel. on the gpu, it has been shown that it is much more efficient than the direct memory copy when the data size is large. we design a parallel process of data defragmentation that each gpu thread only copies the data for one triangle into its required position. our algorithm, as illustrated in algorithm 2, defragments the triangle data of all the meshes with one kernel call to the gpu, instead of one call for a mesh, in order to avoid the high cost of multiple gpu calls."
"as images are increasingly becoming a source of high-throughput genomics data, theoretically sound approaches to evaluate them become necessary. whatever the specific context, appropriate quantitative methods along the lines proposed in this study will allow us to move image expression data from qualitative descriptions to the quantification of gene expression, and to its use as phenotype for which we can assess the significance of changes under perturbations of the genotype or the environment."
"we have implemented our approach on an intel core i7 2.67ghz pc with 12 gb of ram, and a nvidia quadro 5000 graphics card with 2.5 gb of gddr5 device memory. it is developed using nvidia cuda toolkit v4.0, and runs on a 64-bit windows system. our approach has been applied to two complex 3d models. one is a boeing 777 airplane model composed of about 332 million triangles and 223 million vertices. the other one is a coal fired power plant model composed of about 12 million triangles and 6 million vertices."
"in the preprocess stage, we generate a new representation of the original model by re-arranging the vertices and triangle information based on edge-collapsing operations. we also compute other data, such as bounding boxes for the meshes, to be used in the run-time. our run-time approach contains a series of parallel processing steps. we first determine the complexity of the model in the step of levelof-detail(lod) selection. second, the data is streamed from cpu main memory to gpu memory using our gpu outof-core approach. we employ a frame-to-frame coherence scheme to minimize the size of the streamed data. third, the meshes are simplified in parallel on gpu at the step of triangle reformation. finally, the simplified model is rendered using opengl."
"to evaluate our coherence-based gpu out-of-core method, we compare our implementation, streaming with coher-ence (sc), with two other approaches: streaming without coherence (snc) and no streaming (ns), which are common brute-force strategies. streaming without coherence first collects all of them to a continuous cpu memory block, then streams the entire block to gpu with one call. no streaming approach sequentially copies all selected vertices and triangles from cpu memory space to gpu one-by-one. neither snc nor ns approach needs the step of defragmentation. and ns approach even has no cost of preparing data on cpu. we show the performance comparisons of these three approaches in figure 10 -(ii). our coherence-based streaming transfers only new-added vertices and triangles, and has a better overall performance than the other two approaches."
"where n is the pre-defined maximal vertex count, which is wisely chosen based on a desired rendering frame rate or visual quality; a i denotes the projected area of the aabb on the image plane; α is a parameter to control the perceptive contribution of the mesh to the overall model, introduced in [fs93] . to provide an efficient computation of a i, we approximate it by using the area of the bounding rectangle of the projected region on the image plane. to have a fast execution, we use cuda cudpp [hso07] to implement equation 2 on gpu."
"summarizing the result of the given example. the traditional way of optimising the radome allows to increase the dynamic range by about 1.2 db, but only without a cover layer. the presented optimisation which considers also performance degrading sediment layers, allows to increase the system dynamic range by almost 3 db with presence of a worst-case cover layer, but loses 1.2 db without cover layer. however, this effect is even useful for the overall system design. the realistic air-radome-water-air example would have a total maximum attenuation of 19 db if optimised with the traditional approach. the radome optimisation including the cover layer decreases the maximum attenuation to almost 16 db. bringing that increase of the performance in context to the two-way attenuation of the overall system, then the dynamic range is increased by about 6 db. an increase of 6 db allows the radar system to detect an object that is a quarter the size (virtual size represented by the rcs), as compared with the traditional radome optimisation."
"rendering large-scale massive models has become a commonly requested task for scientific simulation, visualization and computer graphics. many research areas generate extremely complex 3d models, such as industrial cad models (e.g. airplanes, ships and architectures), composed of more than hundreds of millions of geometric primitives. however, these complex datasets cannot be rendered efficiently using brute-force methods on a desktop workstation. thus, the challenge is how to increase the rendering performance, so that people can visualize massive models interactively. to solve this problem, mesh simplification techniques have been commonly used to reduce the complexity of 3d models without losing visual fidelity."
". the probability of a swap is calculated for all permutations (including the identity permutation) and the most probable swap is chosen. by iteratively resampling from this distribution, surrogate datasets are created that account for both the marginal as well as the spatial dependency structure."
"the significance of a scoring metric can be computed based on a series of surrogate datasets used for hypothesis testing. we create appropriate surrogate data by drawing constrained realizations [cit] which account for both the marginal distributions of the intensity values as well as the joint spatial dependencies between neighboring variables. this strategy is based on surrogate data whose spatial complexity is representative of the underlying spatial processes; genes with high spatial dependencies (e.g. smooth gradients) will result in a surrogate dataset with similar gradient patterns, whereas those having low spatial dependencies (e.g. hard gradients) will have dissimilar patterns. this approach requires the use of a sampler capable of drawing values from both the marginal as well as the spatial dependencies."
"future works. there are some future works that can strengthen our approach. first, lod selection metric is an important factor for managing the amount of selected data and preserving visual fidelities. we would like to explore other metrics applicable for massive model rendering. second, during the phase of defragmentation, the data used for rendering the previous frame is stored at its own memory allocation. however, it is not the best method to optimize memory usage. in the future, we would like to explore some in-place algorithms to assemble gpu data."
"using the computed complexity levels of the meshes, we first select the amounts of vertices (vc i ) and triangles (tc i ) from the original meshes, which will be the active data for generating the simplified version of the input model. since the vertices and triangles of original meshes have already been re-arranged in the preprocessing step by following the edge-collapsing order, we simply select the first vc i vertices and first tc i triangles of mesh m i . then, we need to reform each of active triangles of m i to reflect the changes of its three vertex indices during edge-collapsing by looking up the corresponding ecol of m i, as mentioned in section 4.1."
"dirt can also be considered as a four-component dielectric mixture, which is composed of soil solids, air, bulk water (free, unbound earth water) and bound water [cit] has created a model which takes into account a more precise composition of soil solids and subdivides these into clay, silt and sand, with the dielectric permittivity of the soil dependent on the percentage of organic matter. however, the permittivity strongly depends on the free volume water. if a small volume of water is present in the dirt and it is increased, the dielectric permittivity is only slightly elevated. this is the result of the fact that, in addition to the mixture, added unbound water is to some extent transformed into bound water. this means that the added water molecules are not as freely repolarisable as the free volume of water. however, if continued to increase the percentage volumetric water content, it reaches a saturation point, from which much more water is taken up from the ground in the form of bound, but the proportion of free volume water is increased. as a result, the dielectric permittivity increases faster. soil compositions with a larger organic matter content can absorb more water, resulting in a later saturation point and thus reach a higher level of bound water. these insights are based on the texts given in the bibliography [cit] . in summary, dry soils have a real and imaginary part of the complex dielectric permittivity that is close to unity. these increase only with a volumetrically increasing water content. from this it can be concluded that some gravel, soil and similar earth compositions occurring on the concreted, paved, and otherwise pretreated areas do not significantly disturb the radar systems."
1. parallel mesh simplification. we present a parallel approach of mesh simplification to generate simplified representations of an input model dynamically. 2. gpu out-of-core. we propose a novel out-of-core approach on gpus that minimizes the overhead of data streaming from cpu to gpu by exploiting the frame-toframe coherence.
"by using the shape parameters as well as the empirical histogram values, we assess how well the image is registered with the following metric: the resulting contours are converted into signed distance maps and processed using a hierarchical pca. four of the principal shapes of the embryo are shown. these images depict 2 standard deviations of the principal component from the mean of the signed distance map."
"in recent years, graphics hardware, as a massively parallel architecture and commoditized computing platform, has been praised due to the significant improvements of performance and the capability for general-purpose computation. since most simplification algorithms are not naturally data parallel, they do not have trivial gpu implementations. in addition, comparing to the computational power of gpu, gpu memory is insufficient to store massive datasets. for example, boeing 777 model shown in figure 1 requires approximately 6 gbyte memory to hold its vertex and triangle data, which is not applicable for most modern gpus. although primitives can be directly streamed for rendering, the cost of cpu-gpu communication could decrease performance significantly, if a large number of primitives need to be transferred constantly from cpu to gpu."
"the task of image registration is to find the optimal set of parameters, θ, such that the images are accurately aligned to a common frame of reference. the parameters we optimize over can be separated into two categories: rigid transformation parameters of the image θ r and the principle shape components of our shape prior θ s . the principle component parameters provide a non-rigid component to the registration, allowing the underlying shape model to take on a variety of possible shapes which are defined by the signed distance maps. the rigid transformation parameters are used to rotate, translate, scale and flip (horizontally and vertically) each image so that it maps onto the evolving shape model. these two sets of parameters are simultaneously optimized using an in-house implementation of a particle swarm optimizer [cit] ."
"the rest of the paper are organized as follows. we review some related works in section 2. section 3 provides a brief overview of data pre-processing and run-time algorithms. in section 4, we talk about the parallel approach of mesh simplification. in section 5, we present the gpu out-of-core approach. section 6 describes our implementation and shows the experimental results. finally, section 7 concludes our work and discusses some future works."
"at each edge-collapsing iteration, an edge (v 1, v 2 ) is removed. there are two steps involved during the removing process: (1) collapsing the edge by merging the two vertices, v 1 and v 2, to a target vertex, v; (2) removing v 1, v 2 and all triangles having both vertices."
"to meet these requirements, we sampled as follows: for each column vector, we initialize every element by drawing a sample from the marginal distribution. this initialization provides us with column vectors that have no spatial dependency. to account for the spatial dependencies, an iterative swap sampler is used on this random initialization. let p 1 a i,a j, p 2 a i,a k, p 3 a i,a l be the probability of observing an intensity a i while its first, second and third neighbor elements are a j,a k and a l, respectively, where"
"between rendering frames, the desired complexity level for a mesh sometimes decreases. in this case, we do not need to transfer any data of the mesh to the gpu. instead, we need to remove the unnecessary data for this mesh from the gpu, so that we can use the space for other meshes with additional storage requirement. such operation will make the continuous gpu memory block fragmented. for example, many small and unusable \"holes\" will be created in the block. in addition, since our parallel triangle reformation approach requires that the geometry data for each mesh has to be stored"
"in general, when a signal is transmitted towards an obstacle, reflection, absorption, and interference attenuate the power that penetrates the obstacle itself ( fig. 3a) . the electromagnetic power of a wave is partially reflected at the obstacle boundaries (fig. 3b ), partially absorbed ( fig. 3c ), and affected by multi-path propagation and interference ( fig. 3d ). these effects are also taken into consideration within the layer based model as well as the equivalent transmission line (tl) model."
"given the gpu parallelization and storage scheme, our parallel triangle reformation approach is described in detail in algorithm 1. since we store all the selected (or active) triangles in a single array, when reforming a triangle t k of this array, we first need to find which mesh it belongs to, so that we can reform it by using the ecol of the mesh. to do this, we perform a prefix-sum on the array tc. the prefixsummed tc indicates the offsets (or ranges) of triangles of the meshes. we then use the triangle index, k, to conduct a binary search in tc to find the index of mesh that t k belongs to. for example, if k falls into a range (tc i,tc i+1 ], t k belongs to mesh m i . this is the process of line 3 in algorithm 1."
"we access the original data stored on cpu main memory, and fetch only those active portions to the gpu memory. since the overhead for transferring data from cpu to gpu is a significant factor impacting the overall rendering performance, we propose a gpu out-of-core approach that transfers much smaller amount of data by exploiting frame-to-frame coherence. as such, we can re-use most of existing data on the gpu, which has been devoted to the last rendered frame, so that the overhead of transferring data is minimized."
"based on empirical investigations, the expectable climatic and external influences are: liquid water, as closed or continuous water layers with and without interruptions, individual drops, and condensation; dry or wet snow; fully-frozen ice layers, crystallised local spots; dry or wet foliage layers or individual leaves; moist or wet dirt, dust, and sedimentation layers."
"four metrics were evaluated to determine the similarity between expression patterns: mse, mi, hws and smi. mse and mi are two frequently used measures for comparing vectors of data. however, both assume that the samples within each vector are independent, which is not the case for spatial and time series data. interaction terms between individual elements of a sample are capable of describing higher order structures, such as the formation of gradients, or the alternating striped pattern of odd-and even skipped. these dependencies are relevant not only in terms of kinetics and molecular diffusion, but also regarding interactions between genes. it is for this reason that similarity metrics that account for spatial dependency can be expected to provide more biologically relevant measures for comparing images. we implement two such measures, hws and smi, which are the spatial counterparts of mse and mi, respectively. for all scoring metrics, we compute similarities independently on row and column vectors, and then combine them in a sum weighted by the vector size (i.e. in our case, with 1/3 and 2/3, respectively)."
"using the extracted expression patterns, we assess the importance of using spatial metrics [haar wavelets (hws) and spatial mutual information (smi)] by comparing their performance with two previously used non-spatial metrics [mean squared error (mse) and mutual information (mi)]. we determined how the similarity values computed by each metric corresponded to manually annotated expression terms in the in situ database. for this purpose, we focused on the subset of 27 157 images covering 3127 genes acquired during the time window of developmental stages 4-6. images in this stage window have been annotated with information on the view from which the images were taken (lateral or dorsal/ventral), and this crucial information is not yet provided for later stages. furthermore, the images at this stage balance the frequency of spatially diverse expression patterns with the resolution and coverage in the database, and are not subject to the additional complexity that expression in later stages is conditional on earlier expression, which is also reflected in the annotation terms. genes in the selected window were annotated with 38 unique terms describing the spatial expression patterns. we removed all genes that had annotation terms which indicated the lack of spatial variability (e.g. ubiquitous, maternal) and non-lateral views, leaving us with 209 genes, 1231 images and 29 annotation terms. for each scoring metric, we calculated an enrichment significance for each annotation term describing how often genes annotated with a particular ontology term show the strongest similarity to genes annotated with the same term. using a p-value cutoff of 0.05, smi performed the best, with 22 of the 29 annotation terms being significantly enriched, while mi led to the second best result with 21. both mse and hws led to 19 and 18 enriched terms, respectively (table 1) ."
"while consistent differences between metrics are observed, and the significance estimates produce more stable results across metrics, the overall performance is not dramatically different. the advantage of using appropriate similarity metrics and significance estimates becomes more apparent in noisier scenarios, or when fewer features are used. many, but not all, terms exhibit distinct patterns along the ap axis; however, when using projections onto the ap axis only, mse results in only 15 terms compared with the 18 terms for both axis. in comparison, mi, smi and the rss scores remain largely consistent regardless of whether the dv axis is incorporated into the score. the full results analogous to table 1 are given in the supplementary material."
"we presented a novel gpu approach to visualize massive 3d models at interactive rates. first, we design a parallel algorithm of mesh simplification that supports real-time generation of lod model. second, we propose a gpu out-of-core approach by employing frame-to-frame coherence. a parallel defragmentation algorithm is developed to maintain the data continuity in gpu memory."
figure 6: an example of data preparation on cpu. the purple blocks replicated from the re-arranged meshes stand for the new data required by gpu. the blue blocks are equivalent to those data already existing on gpu.
"in order to eliminate the dependencies between the iterations, we process the mesh and record the collapsing information into an array structure, called ecol. similar to the data structure described in [swa99], each element of ecol corresponds to a source vertex, and its value is the target vertex that it merges to. we define that the value of the ith element of ecol can be recovered from the function ecol(i). meanwhile, we also record the vertex count and triangle count remaining after each collapsing iteration. since an edge-collapsing operation removes only one vertex but varied number of triangles, we employ a structure, named as map, to record the relationship between the vertex count and the triangle count. if i is the remaining vertex count after a collapsing operation, the value, recovered from the function map(i), is the triangle count of currently simplified mesh."
"structure of gpu data storage. the natural way of organizing meshes on gpu is storing them separately into different memory blocks, then they can be rendered one-by-one after reformation. because a 3d model can potentially have many meshes (718,727 meshes in our boeing 777 model), the overhead of multiple rendering calls to all the meshes is very high. therefore, we concatenate the array data for all the meshes together into a single array as an opengl buffer object, as illustrated in figure 4 -(i), and render the entire model with one rendering call."
"gpu parallelization. obviously, we can parallel the reformation process at mesh-level, e.g. one gpu thread for a mesh. as we know, the design of modern graphics chips allows tens of thousands threads to be executed concurrently. if we choose the mesh-level parallelization, gpu resources will be underutilized when the number of visible meshes is less than the number of concurrent threads. in addition, using the mesh-level parallelization can lead to the load-balancing problem, since different meshes contain different number of triangles after lod selection. to avoid these issues, we employ a triangle-level parallel approach, e,g. one triangle per thread, so that a sufficient number of gpu threads are created simultaneously, and the workload is also balanced."
"during the last decade radar systems have experienced a renaissance with a broad field of new applications. [cit] s, nowadays all middle class and above vehicles are equipped with at least one radar system. for these radar systems, mostly two frequencies are utilised: 24 ghz for short range radar (srr) and 77 ghz for long range radar (lrr). another frequency is already making its way into the automotive sector: 5.8 ghz. especially safety and monitoring applications are adapting the emerging c-band systems. the most promising advantages of 5.8 ghz are material penetration capabilities, reduced environmental impairment due to water, and -most important -low-cost availability. the main differences between the three frequencies (5.8, 24, and 77 ghz) are not only the associated regulations, but also their fundamentally different behaviour in varying environments. especially the signal attenuation due to absorption, reflection, and interference is highly frequency-dependent. what matters the most in the automotive and safety sector is the resiliency of a system, so that reliable operation can be guaranteed at any time. for arising radar applications in the automotive and safety field, protective sensors for people and machinery are often subject to external installation and, hence, also to pollution, icing, and wetness. especially critical safety systems must not be operationally affected in any way but secure safety at all times. hence, adverse environmental influences should be assessed beforehand, to ensure a sufficiently designed radar system."
"performance factors of cpu-gpu streaming. based on our experiments, the time spent on cpu-gpu streaming depends on cpu side, because to-be-transferred data is prepared sequentially on cpu, which is the major cost of the streaming. two factors influences the time of data preparing on cpu: the size of to-be-transferred data and the number of meshes with increased complexities between frames."
"i are neighborhood association sets. to account for edge effects, neighbor relationships are considered on a torus. let p a i,. be the cumulative probability of observing a i . for each iteration, four random locations are chosen:"
"in average, our approach achieves about 1.66x speedup comparing to snc approach, and achieves about 51.96x speedup comparing to ns approach. table 3 shows the averaged timing results and the averaged data amounts of our comparison experiments. note that \"avg. visible meshes\" means the average number of meshes with non-zero complexity. \"avg. streamed meshes\" means the average number of meshes with the increased complexity, so that some of their vertices and triangles will be cpu-gpu streamed. our sc approach requires much less amounts of \"streamed meshes\" and \"streamed triangles/vertices\" than the other two approaches, so that much less cost of memory transferring is required by our approach. for example, in camera (a), only 0.68% of total 12.884 million visible triangles and 0.69% of total 10.721million visible vertices are transferred."
"to formally analyze registration accuracy, we uniformly sampled 200 images from 200 uniformly sampled genes from the dataset for quantitative assessment. each image was manually segmented, registered and compared with the automated segmentation and registration. we found that the most common inconsistencies in registration resulted from incorrectly orienting the axis, and were observed in 20 cases (10%) for the anterior/posterior axis, and in 6 cases (3%) for the dorsal/ventral axis. while our anterior/posterior axis alignment of 90% is consistent with an 85% accuracy reported earlier [cit], our approach differs from gargesha's approach as we encapsulate the alignment problem into the registration process by allowing the axis-corrected shape model to more properly fit to the embryos in the image. the total registration accuracy was assessed using a test point error (tpe) measure [cit] ) ."
"the following measurements include an obstacle, the alteration in the reflected power can then be attributed accordingly to the obstacle. for the correct assignment of the reflected power to the metallic target, it is essential to minimise the effects of standing waves between transceiver, obstacle, and ceiling. hence, the modulation parameters of the radar system have been optimised and an application specific postprocessing has been applied. figure 11c shows the measured two-way attenuation of water for different water layer thick- nesses. several measurements have been carried out, the result area is plotted with its maximum and minimum values. the cubes represent the upper bound of the executed measurements, the crosses the lower bound. the solid blue line shows the aforementioned theoretical results including interference, reflection, and absorption effects. the measured results and the theoretical results deviate due to the complex handling and modelling of water layers, especially the thickness. since the employed radar hardware has a spatially extended patch antenna with several channels, the uniform water layer must be equally distributed over the whole area. the deviation originates from the surface tension of water, manufacturing accuracies, and the overall sum of smaller effects. the discrepancy at zero water layer thickness results from the experimental setup for introducing the water layer within the reactive near-field. however, the results are reproducible in terms of measurement and computational accuracy."
"snow can be classified as dry or wet, where both consist of three main components: air, ice, and water. dry snow has very little impact upon electromagnetic waves, since the density is low and absorption is almost not present. the opposite is the case for wet snow. wet snow has a higher density and consists of dry snow on the one hand, and a water volume fraction on the other hand. the water volume fraction with its freely polarisable molecules is mainly causing the attenuation of snow."
"in order to minimize the overhead of cpu-gpu communication, we only transfer the additional data that is required in the next frame compared against the currently rendered frame. let us denote the arrays of vertex counts and triangle counts for the current frame f as vc f and tc f, respectively, and for the next frame as vc f +1 and tc f +1, respectively. the number of the additional vertices required between frame f and frame f + 1 is defined as in order to avoid multiple cpu-gpu memory copies, which impose a significant performance cost, we prepare the additional vertices and triangles on the cpu by assembling the data from each mesh into a block of continuous cpu memory, and only copy the block to gpu memory once per frame. to do this, we first perform a prefix sum on the count arrays of the additional vertices and triangles, vc f andtc f, respectively, so that we can obtain the position offset for each mesh in the continuous memory block. we then copy the addition data from each mesh into this block at its corresponding position offset. according to the data rearrangement scheme used in the preprocessing step, the addition vertex and triangle data from each mesh is also stored together in a continuous memory space, as illustrated in figure 6. as such, preparing the additional data on cpu can be efficiently implemented, because the data copy for each mesh will only require a single call of memory copy."
", where i represents a specific spatial location. the high pass filter is responsible for creating the coefficients of the wavelet which will be ultimately used to represent the similarity between patterns. the high pass filter is written as:"
"according to the order of edge-collapsing operations, we re-arrange the vertex and triangle data of the original mesh. the order of storing re-arranged data reflects the sequential order of edge-collapsing operations. in our implementation, the vertex and triangle data of a mesh are stored as arrays; the first removed vertex by the collapsing operations is stored at the last position in the vertex array; and the last removed vertex is stored at the first position. the triangle array is also re-arranged in the same manner. with such a representation, a level of detail of the mesh can be simply determined by using a certain number of vertices and triangles starting from the beginning of the vertex and triangle array, respectively. the smaller amount of data is selected, the lower level of detail the mesh is represented in."
"author contributions. us and mge conceived the presented concept. ed, mm, and mg ehrnsperger implemented the model and performed the computations. us, mm, and mge verified the analytical methods. mm and mge carried out the experiments. mge wrote the manuscript in consultation with us and tfe. tfe supervised the findings of this work. all authors discussed the results and contributed to the final manuscript."
"the majority of image analysis work in the context of development of model organisms has been carried out for drosophila, and can be broadly grouped into two categories: quantitative high-resolution analysis of a relatively small selected subset of genes [cit], and higher throughput pattern analysis of thousands of genes [cit] . fine-grained high-resolution analysis often extracts quantitative expression values from image data, e.g. for numerical simulations of specific regulatory pathways. this study follows registration flowchart: during initialization, gradient and filtered images are calculated, and the parameters of the registration are randomly initialized. the parameters are then iteratively evaluated and optimized for a fixed number of iterations, using a numerical optimizer. the final parameters are used to transform the image and create the shape model as shown in (c). the optimized shape models for the embryos in the example images are shown in red, superimposed on the normalized input images. previous high-throughput approaches which uses large datasets to quantitatively address expression in the context of spatial patterns: prediction of annotation terms, or clustering/classification of genes. we use a dataset of drosophila embryonic expression patterns [cit] to illustrate how we address the three questions outlined above."
"in an ordered and continuous fashion, we have to reshuffle the existing data on the gpu and copy the additional data into the right position in the gpu block. the goal of this data defragmentation process is to make sure that, (1) the active data selected for the frame is still continuously stored; (2) the vertex and triangle data for each mesh is stored in the same order as it is re-arranged in the preprocessing step; (3) the appearance of each mesh in the block is also stored in the same order as indicated in the arrays vc f +1 and tc f +1 ."
"we have developed a fully automated registration method based on statistical shape models and improved numerical optimizers. this approach estimates both the average shape of an embryo, as well as the main components of variation of embryo shape, including orientation, from labeled data. it then uses this model to segment new images into foreground (a single complete embryo) and background ( fig. 1 ). we applied this registration method to the complete set of 78 621 images in the latest release of the berkeley drosophila in situ database. overall, the model-based registration addresses problems that limited the successful application of previous methods on the whole database, highlighting its ability of extracting embryos under a large variety of imaging conditions: multiple embryos or impeding boundaries, changes in lighting/microscope settings, and out of focus boundary regions."
"and [ysgm04] relied on heavy pre-processing stages to build all levels of mesh details or densely clustered pms, which enlarged the data size dramatically. although cachecoherent layouts have been used for their out-of-core techniques, it is still a major performance overhead to fetch and access data in a slow bulk memory (e.g. hard drive). in our approach, we will not construct spatially complex structures, so that cpu main memory can hold original meshes sufficiently; and the simplified versions of meshes will be generated dynamically by taking advantage of gpu parallel com- putational architectures along with an efficient coherencebased cpu-gpu streaming."
"the goal of our system is to simplify complex 3d models for real-time rendering. an input model consists of multiple triangulated meshes. our approach includes a preprocess stage and a run-time stage, which are illustrated in figure 2 ."
"while our method for the automated registration of images is more robust and complete than many previous approaches, it cannot be an off-the-shelf solution for all image registration problems. differences in sample preparation and imaging technology, as well as differences in the morphology of specimens which are imaged, pose restrictions on the possible choices of analysis methods. for instance, our previous work on arabidopsis root images [cit] deals with confocal images of gfp reporter constructs. in addition to the different morphology, the fly dataset poses additional challenges of dealing with multiple objects, and a high variability in imaging conditions; conversely, we needed to apply non-rigid registration for the plant roots. further, while our method for significance testing and surrogate dataset generation are well suited to the syncytium formation of the drosophila embryo, this is generally not applicable across all organisms. for example, in c.elegans, gene expression is heavily influenced by cell lineage, and a more appropriate representation for neighboring dependencies would be between neighboring lineage elements, and not direct spatial location. additionally, different sampling methods may need to be applied to deal with differences in the structure of the organism. while general platforms for cell-based image analysis have been published [cit], a universal framework for complete multicellular organs or organisms has to be placed at a higher level and poses a challenge for the automated analysis of image expression datasets pertaining to developmental biology."
"our work distinguishes itself from previous work on this drosophila data by three main contributions. first, robust and fully automated image analysis techniques are used to process and register the raw images. through the use of statistical shape models and partial mapping methods, these techniques are capable of handling sources of phenotypic and imaging variability that limited previous approaches. second, we comprehensively compare different similarity metrics, implement similarity measures that incorporate spatial dependencies to distinguish complex spatial patterns, and validate the different measures against visual annotation terms provided by experts. third, we develop a new significance testing framework for spatial similarity scores through constrained realization monte carlo simulations and demonstrate how it can generalize similarity measures to achieve similar performance to spatial metrics. finally, we illustrate this method of significance testing on known biological examples, emphasizing the importance for fully automated image registration/comparison models in the context of regulatory interactions. while we use a fly embryo dataset as an example, the general registration and comparison approach is adaptable and thus of interest to the study of spatial expression patterns in a wide range of model organisms."
"prior to any quantitative analysis of expression image data, it is necessary to normalize and register the images to a common frame of reference. by first mapping different specimens such as fly embryos to a common reference, we can subsequently apply a large variety of methods for univariate and multivariate data analysis for cross-subject comparisons (between replicates, genes, time stages or different strains or species)."
"after normalization and registration of the images, expression patterns or features useful for classification can be extracted. here, we are interested in comparing the global 2d expression pattern, and a choice of representation of the patterns has to be made: instead of working with the complete pixel-based 2d patterns, it is common to map them to a smaller set of features representing expression in small subregions of the specimen. as the registration maintains morphological variabilities (size and stretch), it prevents a one-toone mapping on the pixel level. to be able to evaluate the effect of different metrics in a simple scenario, we therefore chose to project the staining intensities along the anterior/posterior x-axis and dorsal/ventral y-axis as described in section 3.3. projections have been used used in a variety of recent applications in drosophila [cit] -as well as mouse brain images [cit] -and are particularly suited for the analysis of early embryonic expression as discussed below."
"to address questions that arise with the analysis of spatial gene expression patterns, we have implemented a complete pipeline for 2d drosophila rna in situ staining images that is fully automated and highly robust to variable conditions in the input data. in addition to accurate methods for the registration of images to extract expression patterns, we implemented similarity metrics and significance scores that are appropriate for spatial patterns. such methods are critical for a proper interpretation of spatial expression similarity with dependencies among neighboring areas. we have also demonstrated how our significance tests can be used to generalize metrics that do not include spatial structure directly, we implied imaging in situ hybridizations under bright field microscopy often introduces undesired variability and artifacts. this creates problems for quantitative comparisons: depending on probe affinity, staining protocol, the overall position, lighting conditions and focal plane of the image, genes with near identical spatial expression patterns may exhibit low similarity values. many of these quantification issues can be addressed by prospective experimental planning, including standardization of microscopy settings, number of replicates, time stages/conditions and staining protocols. unfortunately, this type of planning is not available in a retrospective study as this; as a result, quantification will be affected by these issues until this information is provided and methods are developed to adequately model these sources of noise. another critical limitation is the frequent lack of biological replicates, which reduces the ability to filter input noise and model the actual variance of expression patterns."
"since boeing model requires approximately 6 gb memory space, the data is streamed for rendering based on the gpu approach explained in section 5. but for power plant model, it can fit into gpu memory, so that the cost of cpu-gpu communication is completely eliminated."
"a particularly interesting and fundamental problem that arises with the availability of image data, and which we address in this study, is to compare two samples on the level of their expression profiles: for instance, the same gene under different conditions or across different species, or different genes with the goal to cluster them akin to approaches developed for microarray data. several general problems arise when comparing image expression data: (i) we need to develop methods to process the raw input images, to eliminate noise under a typical large range of imaging conditions (e.g. different viewpoints, different locations, multiple specimens per image) and to perform normalizations to decouple the variability in morphology from the variability in expression; (ii) we need to represent the expression patterns and to specify appropriate similarity metrics capable of assessing spatial/temporal similarity; and (iii) we need to assess the significance of observed similarities."
"the algorithms of mesh simplification replace tessellated objects with coarser representations containing less amount of primitives, such as levels of detail (lod). hoppe [hop96] introduced a well-known lod-based al- gorithm, progressive meshes, to simplify meshes using a sequence of modifications (e.g. edge-collapsing). however, given a massive 3d model, constructing its simplified representations can be a very expensive process, which makes online simplification impossible on a desktop workstation."
"to verify the correctness and the performance of the tl model, the calculated results are compared to computer systems technology (cst) microwave studio (mws). hereby, a general purpose time domain solver, the transient solver which is based on the finite integration technique (fit), of cst mws has been utilised. in the simulation the air-waterair layer constellation has been excited with a linearly polarised plane wave. what can be observed from fig. 8 is, that both results from the tl model and from cst mws agree to an acceptable extent. since the test configuration is a single homogeneous dielectric layer that is illuminated by a plane wave, the comparison not only shows the correctness of the analytical vs. the numerical solution, but also their correct implementation. comparing the required calculation power, taken into consideration that the tl model does not provide the extensive potential of cst mws, the tl model calculates the same result significantly faster (equal workstation hp-z840 employed for calculations)."
"to obtain relative results, the experimental setups of the fig. 11a and b have been employed. here, the radar system is positioned on the ground, looking upwards. as reference object a metallic target, such as a corner reflector is utilised. the corner reflector is positioned on a stand at a predefined distance. the first measurement is carried out and the echoed power of the corner reflector is evaluated."
"boundary edge constraint. in many 3d models, disconnected faces separated by borders and holes are important visual features. to preserve them, we restrict that boundary edges are not collapsible. a boundary edge is the edge only existing in one triangle, and the two vertices of the edge are boundary vertices. note that any edge containing boundary vertices cannot be collapsed by moving a boundary vertex to the other. using this kind of constraint, the lowest level of detail of the mesh is represented by the mesh with only the boundary vertices, rather than a single triangle."
"radar sensors are advantageous for the recognition and detection of moving or static objects -most notably living objects -within a predefined vicinity, area, or space. usually radar systems are assembled in a locally protected environment to avoid signal debasing sediments. however, externally installed safety-critical radar-based sensors in disadvantageous assembly positions are becoming more and more popular. for living object protection (lop) and living object detection (lod), the sensors face two major challenges: first, safety-critical sensor systems have -by definition -to be operational at all times, to ensure maximum safety, and second, since an external installation has to be taken into consideration, the resulting environmental influences must be evaluated and operationability must be self-tested. by way of illustrating such scenarios, fig.1a and b show an operative radar sensor under good weather conditions (fig. 1a ), which may fail under different weather conditions such as rain (fig. 1b) ."
"this paper was focused on the signal degradation of radarbased safety-critical sensors due to sediments and external influences. new applications may expose sensor systems to such climatic and environmental phenomena. to assure operationality, these signal affecting effects have to be taken into consideration. the proposed tl-based model helps to evaluate worst-case scenarios. measurement and simulation results show that the model is applicable and performant. the underlying material data helps remarkably for the evaluation of relevant covering layers. the practical example demonstrates that the proposed optimisation by employing the tlmodel allows to increase the sensor performance by up to 6 db under worst-case conditions. data availability. the underlying research data can be requested from the authors."
"as described in algorithm 2, each gpu thread copies the data for one triangle, t k, of the active triangles from either the existing triangles or the additional triangles, as illustrated in figure 7 . first, we identify the mesh index, i, that t k belongs to, since the source triangle for t k has to come from the mesh m i . to find the mesh index quickly, we perform a binary search on the array tc f +1, in the same way as we do in algorithm 1. second, we identify if the source triangle for t k should be an existing one or an additional one of m i . to do this, we convert the index of t k in the active triangles to a local triangle index in m i . we denote this local index as tidx (see line 4 of algorithm 2). if tidx is smaller than the number of existing triangles, we copy the triangle from the existing ones (see line 6-8); otherwise, we copy from the additional triangles (see line 9-11). at the end, the block of existing triangles is replaced with the completed active triangles, so that we can use it to defragment the following frame."
"as application of the registration and expression comparison pipeline, we use smi and significance tests to validate known biological interactions, suggesting their usefulness for inference on biological data. gene regulation and spatial patterning are a tightly coupled process: transcription factors acting as activators for a gene are often co-expressed in similar spatial regions, while repressors are often expressed inversely to the targeted gene. [cit] . this network consists of a set of genes with many direct regulatory interactions and shared functional roles; in many cases, similarities in function/interaction are reflected in noticeable similarities of spatial expression patterns. since the subset we are using does not contain the irregularities/noise of the full dataset, we here calculated the unweighted significance scores of the anterior/posterior projections described in section 3.5.1."
"various out-of-core techniques have been proposed to solve the problem of huge amounts of static lods, multiresolution geometries or multi-level vertex hierarchies constructed for interactively rendering massive models, such as [ used a binary tree for mesh partitioning, and allowed the construction of multi-resolution and per-node simplification."
"advances in high-throughput microscopy have led to a rapid increase of digital image data in biology. new methods to image biological specimens at high resolution, and to visualize expression of genes of interest, have lead to a high interest in imaging in developmental and * to whom correspondence should be addressed. molecular biology, including the creation of virtual embryos to map expression profiles of important regulatory genes [cit] . in the past, these images were analyzed in a manual fashion, e.g. comparing two expression patterns by qualitative visual inspection. in order to deal with these datasets more appropriately, it is necessary to develop automated methods for extracting and analyzing images. methods to quantitatively describe spatial/temporal expression patterns is a relatively new area of research that has begun to be explored in model organisms [cit] . in sea urchins, a comprehensive analysis of normalization methods was used for determining and quantifying spatial expression data [cit] . recently, large databases of images for caenorhabditis elegans have become available [cit], accompanied by methods to analyze the data [cit] . for arabidopsis root images, image registration techniques have been used to quantify tissue-specific expression from green fluorescent protein (gfp) reporter lines [cit] ."
"while smi and hws are capable of incorporating the spatial structure of expression pattern, they do not fully incorporate the significance of the spatial pattern: genes with complex spatial patterning (e.g. even-skipped) will be scored similarly to genes with simpler spatial patterning (e.g. bicoid). to better account for this, and to provide actual significance values between expression patterns, we have developed methods that account for the complexity of the spatial patterning. the significance values are computed by comparing the observed similarity value with a null distribution that preserves spatial dependencies between the two gene expression patterns, as described in section 3.5.1 and shown in figure 2 . to accommodate for low signal/strong lighting effects, the significance values were converted into reweighted significance scores (rss) as described in section 3.6. the enrichment significance calculation was repeated for the rss values and are shown in table 1 . by incorporating the spatial structure using rss for each score, the scoring metrics performed similarly, with all the metrics resulting in 19-22 enriched significance scores."
"at this step, on gpu memory, we have a block of existing data from the current frame, f, and a block of additional data required by the next frame, f + 1. to assemble them into the block reserved by active data, a straightforward method is using the system calls of gpu memory copy. for mesh m i, we copy its vertices and triangles from both existing data block and additional data block to the active data block at the position with the offsets vc f +1 i and tc f +1 i, respectively. however, there will be a large number of system calls of gpu memory copies, and they have to be initiated by the cpu and executed sequentially, which would be a significant cost on performance."
"quality management system adequacy: is ability of this system to meet applicable requirements, specified by the organization or standards. for example, the requirements may be about the iso 9001, contractual, organizational or regulatory demands. simply to say: adequacy means being equal to the requirements, no more, no less."
"the respondents were also asked to describe what approach their organizations apply for quality management system performance assessment: if this system is assessed through individual features as adequacy, suitability, effectiveness and efficiency (it is marked as yes) or as a whole. fig. 6 shows results."
quality management system performance: extent to which quality management system fulfils its functions and goals. by the way: author this term as well as possibilities of this performance measurement has already described [cit] .
"i have mentioned some serious facts which can influence practical implementation or assessment of the quality management systems performance against the iso 9001:2015 in the introduction of this paper. a confusing attribute of the iso 9001:2015 is hidden at clauses of this standard which require assessment of the quality management system adequacy, suitability, effectiveness and efficiency in spite of these terms are not defined at all or are defined incorrectly. therefore this article brings the own explanation of these terms and on a basis of the empirical field research findings proposes fundamental steps of all features of quality management system´s performance measurement and assessment, including set of relevant indicators."
quality management system effectiveness: relationship between the results achieved by the quality management system and the resources used. we will consider effective quality management system as system which brings undoubtful economic or social effects.
"see also [cit] b) . let us have a look to these definitions. from the core economic point of view, basic indicator of effectiveness is relation between benefits and costs -see [cit] and many others. and practically: all technical sciences associate the term \"efficiency\" with evaluation how a certain capacity delivered to input of the technical system is successfully converted to desirable outputs [cit] or [cit] . as to performance definition: what is measurable result of the quality management system -that is a question! a number of certificates seem to be doubtful result, i am sure. these notes make us to define these terms more preciously:"
"we are able to summarize main lessons learned from this empirical study: a) the organizations have not mostly a problem to understand terms \"effectiveness\" or \"efficiency\" inversely against the iso 9000:2015 definitions, b) but 50 % of organizations or so have problems related to the terms \"adequacy\" or \"suitability\", c) the most of organizations are not aware of fact that effectiveness, efficiency, adequacy or suitability represents only a partial features of overall quality management systems performance, d) the approach to the quality management systems performance assessment as a whole seems to be logic and rationale. on the contrary: such information that 23 % of organizations do not generally perform this type of assessment is strongly correlative of number of organizations without quality management system certification, e) quality professionals are mostly confused when assign relevant indicators to such features of the quality management systems performance as effectiveness, efficiency, adequacy and suitability really are."
when giving all mentioned definitions thought we are able to come to the logic conclusion: strong relationships must exist among all these quality management features! we can depict this fact by fig. 1 .
"before anything else, we must identify set of indicators corresponding with particular features of the quality management system performance. a proposal of such set of indicators is presented by tab. 2 -tab. 5."
"what can we read from this figure? the quality management system can be suitable and efficient, but this system need not be effective as a large amount of various resources was wasted for example. all arrows illustrated in fig. 1 can be seen also as hypotheses which wait for future confirmation. unfortunately, we are not able to confirm these hypotheses at present as it asks for huge amount of relevant data -and these data are simply unobtainable at czech organizations now."
"the organizations which answered \"yes\" in this case were additionally asked to list specific indicators used for quality management system adequacy, suitability, effectiveness and efficiency evaluation. analysis of these lists allowed us to recognize that organizations take use wide range of indicators but most of them are not relevant for quality management system features evaluation. for example: we have occurred that indicators related to customer satisfaction or internal auditing are used for evaluation and assessment of all features (such indicators are about efficiency for one organization, while another organizations the same indicators apply in the field of adequacy, and so on). it is evidence that understanding of discussed terms is little bit confusing in practice, in spite of fact, that the same respondents declared sooner that these terms are fully understandable for them!"
"the list of indicators presented by these tables must not be considered as comprehensive set of course -it is only about examples! various organizations could be able to define some others indicators describing such features of their quality management systems, as adequacy, suitability, effectiveness or efficiency are. if we realize that all features describe also quality management system overall performance, we look upon each of these indicators also as relevant performance characteristic! even though, correct assignment of indicators is important, but not crucial part of the quality management systems performance assessment. therefore, let me introduce all general steps which seem to be necessary to implement rationale performance assessment within establishing, maintenance and improvement of the quality management system regardless the type or size of the organizations:"
"3) the top management must make a decision if the organization´s quality management system performance will be assessed and monitored as a whole, or within its particular features as adequacy, suitability, efficiency and effectiveness. any approach can be applicable; a choice depends solely on the organizational environment."
"therefore we see as challenge all answers obtained to the last question within the survey, focused on exploring if the organizations are interesting in special methodology for quality management system´s adequacy, suitability, effectiveness, efficiency and performance assessment: 89 % of all respondents declared this concern without any hesitation as they perceive low level of knowledge in this field on one hand and as important hindrance to objective and fair quality management system assessment and review on the other hand."
"4) anyway, it is necessary to establish relevant set of indicators for each quality management system performance feature. tables 2 -5 should serve as possible inspiration. management representative (or another function) of the organization as well as process owners should approve these indicators before releasing. 5) each of approved indicators must be described by mathematic formula. all necessary responsibilities and authorities related to data gathering and processing must be assigned to individual people. such set of information should be maintained through relevant documented information."
"additionally, another requirement related to the quality management system performance is repeatedly stressed at different clauses of this standard. when consulting text of this standard in more detail, we are able to discover some serious facts which can influence practical implementation or assessment of the quality management systems against the iso 9001:2015:"
"a) the iso 9001:2015 takes use the terms \"adequacy\", \"suitability\" \"effectiveness\" or \"performance\" somewhat arbitrarily without explanation of these terms with relation to the quality management system, b) the iso 9001:2015 standard is not concerned with mutual relationships among these terms at all, although these relationships really exist and play important role in practice, c) the iso 9001:2015 standard ignores term \"efficiency\" related to the quality management system although this quality management system´s feature should be vital, d) these three shortcomings can lead to different interpretation or misunderstandings of all these terms from the point of quality professionals and managers view, e) but also internal and external auditors will be able to explain these terms differently what can influence objectivity of all types of audits, including third party audits performed by the certification bodies."
"2) all managers of the organization must understand each area of quality management system performance. it means that they have to see all aspects of quality management system adequacy, suitability, efficiency and effectiveness as useful and rationale and they must support corresponding measurement and monitoring."
"quality management system efficiency: extent to which planned activities within the quality management system are realized and planned results are achieved. briefly, an efficient quality management system must be in rational operation."
"1) the top management must define and communicate purpose, goals and functions of the organization´s quality management system. establishing of quality policy, which is required by cl. 5.1 of the iso 9001:2015 [cit] ), is not sufficient."
"quality management system suitability: is capability or fitness of this system to meet defined purpose. the organizations can identify various kinds of quality management system´s purpose. to guarantee a maximum level of customer´s satisfaction and loyalty, to support improvement culture at the organization or to be a catalyst in the area of organization´s excellence should serve as example of the quality management system´s purpose."
"assuming that there is only one maneuvering target moving within the underwater three-dimensional space [cit], the dynamic system can be represented by the state space model, and we adopt the uniform velocity turning model [cit] . the target moving state of the tracking system is given by formula (1) ."
"one popular modeling assumption for network data is to assume dyadic independence of the edge measurements when conditioned on a set of latent variables [cit] . the number of latent parameters in such models generally increases with the size of the graph, however, meaning that computationally intensive fitting algorithms may be required and standard consistency results may not always hold."
"we take as our definition of nominal value the quantityφ (z) computed under the baseline model, which we denote by φ (z) . table 3 lists normalized divergence terms"
"proof: according to the definition of mutual information entropy and the nature of mutual information entropy, the mutual information between the measured value of the jth sensor node and the target state at time k is expressed as formula (11) ."
"from the experimental data, we find that the time spent in the delegation generation phase of our dvpbs scheme is the shortest, while the time cost in the delegation verification phase is longer than other schemes. both the time spent in the dvpbsg phase and the time spent in the dvpbsv phase are longer than huang's shdvps [cit] and islam's id-sdvps [cit], while shorter than shim's shdvps [cit] and hu's wdvps and stdvps [cit] . we sum the time spent in delegation generation phase, delegation verification phase, dvpbsg phase, and dvpbsv phase of different signature schemes and obtain the results, which is illustrated in figure 5 . the results show that the total time of our dvpbs scheme is longer than islam's id-sdvps and shorter than huang's shdvps, shim's shdvps, and hu's wdvps and stdvps. hence, we think that our pvdbs is efficient."
"(v k,x, v k,y, v k,z ) are the corresponding velocities in x, y and z coordinates, and a k,x, a k,y, a k,z are the corresponding acceleration in x, y and z coordinates. also, the process noise w k−1 is always assumed to be gaussian with zero mean. the state transition matrix f and the process noise variance matrix q are given as follows"
"a dvpbs scheme is existentially unforgeable under an adaptive chosen message attack if the success probability of any polynomially bounded adversary in the above game is negligible. that is,"
"in this section, we compare the efficiency of our dvpbs scheme with some other related digital signature schemes. we mainly compare them from two aspects. one is the time spent in the process of signature computation and the other is the length of signature."
"in the target tracking process, only the sensor nodes and cluster heads in the small area where the target is located can be awakened at each moment, and the remaining sensor nodes are in the dormant state. the measured data from the sensor nodes participating in the observation task at time k are analyzed and verified by the grubbs criterion for eliminating the abnormal data. the activated cluster head at the current time receives the processed observation data from the corresponding sensor nodes and runs the improved sir particle filter. once the target leaves the current small area, the cluster head node passes the target state information at the last sampling instant to the next activated cluster head."
(iii) we analyze the security of our dvpbs scheme based on the random oracle model. the result indicates that our dvpbs scheme is existentially unforgeable under an adaptive chosen message attack.
"lgcs which is close to the uav to temporarily command uav. here leasing company, lgcs, user, and uav are original signer (os), proxy signer (ps), blind signature provider (bsp), and designated verifier (dv) for our dvpbs scheme, respectively. the brief process of our dvpbs lists as follows and is illustrated in figure 3 ."
"transcript simulation. it takes the system parameter, warrant, private key of designated verifier, public key for proxy signature, and message as inputs and computes a simulated signature which is indistinguishable from the original designated verifier proxy blind signature."
"the study of this paper mainly focuses on the node selection module and prediction module in the process of the target tracking. first, due to the limitations of centralized estimation method, such as large amount of calculation, constraints of network structure and poor robustness, this paper adopts distributed computing mode and uses the exchange and coordination of local measurement information between sensor nodes to complete the state estimation of the target. to further improve the target tracking accuracy and reduce the energy consumption of the network, when the target enters an area, the sensor nodes and cluster heads in this area are activated and the remaining nodes are in the dormant state. second, the target information obtained by the sensor nodes is analyzed and verified through the grubbs criterion, by which the interference information and the error information are eliminated. then, the mutual information entropy between the measured data obtained by the sensor nodes and the target state are used as the weighting factor of the importance weight of the particle filter algorithm. eventually, the particle filter algorithm is used to track and predict the target state."
"the tracking response time, including the computation time of the algorithm and data transmission time, indicates the time required for underwater wsns to obtain the target state information [cit] . it is assumed that the data transmission time of each algorithm is the same at each moment. the tracking response time at time k is determined by the computation time of the algorithm."
"- karp, t. and fliege, n. j. (1999) . modified dft filter banks with perfect reconstruction. circuits and systems ii: analog and digital signal processing, ieee transactions on, 46(11), 1404 46(11), -1414 46(11), . -lim, y. (1986 . frequency-response masking approach for the synthesis of sharp linear phase digital filters. circuits and systems, ieee transactions on, 33(4), 357-364. -li, n. and nowrouzian, b. (2006, may -fuller, a. t., nowrouzian, b., and ashrafzadeh, f. (1998, august). optimization of fir digital filters over the canonical signed-digit coefficient space using genetic algorithms. [cit] . proceedings. 1998 midwest symposium on (pp. 456-459) . ieee."
sample by x i k ∼ p x k−1 x i k 12: end for 13: for j ← 1 to m sa do 14: update the fusion weight vector β k by (10) estimate the statex k and covariance p k :
"when an autonomous uav performs a task, it will receive the command which contains relevant parameters about task, including time, target location, and security and communication networks 3 actions to be performed. it can fly autonomously to the target location in the specified time without requiring operator manual navigation. it can even cruise in the air and wait for commands, which reduces response time and accomplishes tasks more efficiently. therefore, it is important for uav to confirm whether the command is issued by command center. it typically uses digital signature to confirm the source of command. the overall process is roughly as follows."
"substituting (14) and (18) into (11), the mutual information entropy between the measured value of the sensor nodes and the target state can be expressed as follows."
"to test the real-time performance of the three algorithms, the sensor density is set as 0.00008 per m 3 and the observed noise variance is set as 0.36. the simulation runs 100 times for each experiment on these algorithms. under the same initial conditions, the average tracking response time of the different algorithms for the target tracking algorithm is shown in table 2 ."
"unmanned aerial vehicle (uav) is the aircraft without human pilot aboard. it is an emerging technology which can be applied for military applications and civil applications. its military applications mainly include border surveillance, reconnaissance, and strike."
"the designated verifier, uav, is also able to compute the signature which is indistinguishable from the signature generated by lgcs. the uav computes as the following steps."
"when requests a signature on a message under a warrant, c will run the dvpbs generation algorithm to generate a signature and return it to a. dvpbs verify queries. when requests a signature verification on a message and a signature, c will responds with true if the signature is correct, or ⊥ otherwise."
"however, it is inevitable for uav to perform remote tasks. when the uav is far from command center to perform tasks, it will experience longer propagation delay to receive command and corresponding signature and even cannot receive them due to weather or terrain. mobile edge computing (mec) technology can be introduced to solve this problem. mec is proposed by european telecommunication standard institute (etsi). it can bring the function of commanding uav to the edge of network, which is closer to uav, to reduce . meanwhile, it needs to use proxy signature scheme. the command center may temporarily authorize a command station near to uav to command it, which can reduce the propagation delay. for example, if a ground control station is much closer to uav than the command center, the command center may temporarily authorize ground control station to command uav. the ground control station and uav use proxy signature in this case. the command center delegates ground control station. the ground control station computes a private key for proxy signature, which allows it to sign on behalf of command center. hence, the ground control station is able to command uav."
"delegation verification. given the system parameter, public key of original signer, and warrant, this algorithm can determine whether the delegation is successful. if it is successful, it will compute the private key for proxy signature."
"-fliege, n. j. (1994, april) . modified dft polyphase sbc filter banks with almost perfect reconstruction. [cit] . icassp-94., 1994 ieee international conference on (vol. 3, pp. iii-149) . ieee."
"ab ) under the approximate ml partition determined via algorithm 1 was then tabulated for each of these 1300 fits, and compared to its 95% confidence upper bound given by eq. (3). the empirical divergences are reported in the histogram of fig. 1 as a fraction of the upper bound."
"ab ), bonferroni-corrected 95% confidence bounds, and measures of alignment between the corresponding partitions z and the explanatory variables. the alignment with the covariates are small, as measured by the jacaard similarity coefficient and ratio of within-class to total variance 2, signifying the residual quality of the partitions, while the relatively large divergence terms signify that the bonferroni-corrected confidence set bounds for each school have been met or exceeded."
"theorem 7. the designated verifier can distinguish the proxy signature generated by proxy signer from the signature generated by original signer. namely, our dvpbs scheme has distinguishability."
"typically, the nodal indices themselves impart no inferential information and the latent variable formulation is exchangeable; in fact, in lieu of independence one may start with a much weaker assumption that the graph itself is generated from an exchangeable distribution [cit]"
"(iii) the uav will verify the signature when it receives the command and signature. if the signature is valid, the uav will believe that the command is issued by command center and execute this command. otherwise, uav will consider that the command is counterfeit and does not execute it."
"-storn, r., and price, k. (1997) . differential evolutiona simple and efficient heuristic for global optimization over continuous spaces. journal of global optimization, 11(4), 341-359. -karaboga, d., and basturk, b. (2008) . on the performance of artificial bee colony (abc) algorithm. applied soft computing, 8(1), 687-697."
security and communication networks (2) the entity which issues and sends command should be as close as possible to the uav. it helps to reduce the propagation delay . the command center uses proxy signature scheme to authorize a command station which is close to uav to command it on behalf of command center.
"secondly, we theoretically analyze the signature length of different signature schemes and list them in table 1 . the shorter the signature length is, the less the time and energy are taken to send and receive signature. the result indicates that the signature length of our dvpbs scheme is longer, but it is still short compared with the transmission capacity of uav."
"*, * ) where * has never been queried during the dvpbs sign queries and * is a valid designated verifier proxy blind signature of message * under warrant * . the success probability of an adversary to win the game is defined as −,"
"underwater target tracking is a process of estimating the state of the moving target by using some sensors. acoustic sensors are usually used to obtain the observed data of the target, and these related target observation data contain uncertain interference. the features of the target generally include the number, the size, the shape, the coordinates, the speed, and the acceleration of the target. the process of the target tracking generally includes five modules: detection module, node selection module, routing module, prediction module, and positioning module [cit] . the process of target tracking is shown as fig. 1 ."
"when the lgcs receives the message sent by leasing company, it will verify the delegation. if the delegation is invalid, the lgcs will request delegation again. if the delegation is valid, the lgcs will compute the private key for proxy signature. it performs the following steps to verify the delegation and compute private key."
it is assumed that 1 is a cyclic additive group and its order is a prime . the following problems defined over an elliptic curve are assumed to be difficult to solve within polynomial time.
where i j k represents the mutual information entropy between the j-th sensor node and the target state at time k. β j k indicates the corresponding weight of the j-th sensor node participating in the measurement task at time k.
"the experimental results indicate that our dvpbs scheme is efficient. through theoretical analysis, the signature length of our dvpbs scheme is longer than those of other schemes, but it is still short compared with the transmission capacity of uav. we will research how to further reduce the time spent in signature processing and shorten the length of signature in the future."
"it can be seen from table 2 that the average response time of the three algorithms have the similar order of magnitude. due to the small amount of computation of the rekf algorithm, the average tracking response time of rekf is the shortest. the proposed gmiew algorithm first needs to analyze the measurement data from the sensor nodes though grubbs criterion, and needs to calculate the mutual information entropy between the measured value and the target state of each sensor node, thus the average reflection time of gmiew is slightly higher than that of the other two algorithms. the average reflection time of ahpw algorithm is shorter than gmiew and longer than aw. however, considering the tracking accuracy, robustness of the proposed gmiew, and the increment of the average reflection time is not so large, the proposed gmiew is still more suitable to be used in underwater target tracking."
"key generation. given the security parameter, c runs the algorithm to obtain public-private key pairs of original signer, proxy signer, provider of blind signature, and designated verifier."
"this analysis also provides an indication of how inflated the confidence set sizes are expected to be in practice; while conservative in nature, they seem usable for practical situations."
"for autonomous uav, the command center or command station commands it by sending instruction and corresponding digital signature. the uav verifies the signature and decides whether to execute corresponding command based on the verification result. if the signature is valid, the uav will execute the command. otherwise, it will refuse to execute the command."
"definition 2: at time k, the importance degree of the measured value of the j-th sensor node to determine the target state, that is, the weighting factor corresponding to the sensor node, is defined as formula (32)."
"as a result, it can often be difficult to assess statistical significance or quantify the uncertainty associated with parameter estimates. this issue is evident in literatures focused on community detection, where common practice is to examine whether algorithmically identified communities agree with prior knowledge or intuition [cit] . this practice is less useful if additional confirmatory information is unavailable, or if detailed uncertainty quantification is desired."
definition 1: the mutual information entropy between the measured value of the j-th sensor node and the target state at time k can be approximately expressed as follows.
"we now report on the assessment of residual block structure in the adolescent health friendship network data. recalling that the confidence sets obtained with eq. (3) hold uniformly for all partitions of equal size, independently of how they are chosen, we therefore may freely modify the fitting procedure of algorithm 1 to obtain partitions that exhibit the greatest degree of structure."
"in this paper, we proposed an algorithm based on distributed particle filter with grubbs criterion and mutual information entropy. firstly, the grubbs criterion method is inducted to test and analyze the data measured by the sensor nodes, thus the abnormal data can be eliminated. secondly, a dynamic weighting factor based on mutual information entropy is inducted in the process of calculating the importance weight. the detailed process of the algorithm is given as follows."
we provide a formal definition of existential unforgeability of our dvpbs scheme under an adaptive chosen message attack (euf-cma). it is defined using the following game between a challenger and an adversary a.
where p 0 is the power level needed by the input of the receiver. p τ is a constant parameter depending on the receiver devices. d is the communication distance. the energy attenuation part a (d) is defined as formula (38).
"it is assumed that a user rents uav from leasing company to perform some tasks, such as taking pictures and surveying. the leasing company authorizes a"
"where z j k denotes the signal power measured by the j-th sensor at time k. (x k, y k, z k ) are the position of the target at time k, and x j, y j, z j are the position of the j-th sensor at the time k. s k denotes the target's sound pressure of sourcelevel. n j k ∼ n 0, r j k is the measurement noise. in this paper, we assume that the observation processes of the sensor nodes are independent each other, and the cluster heads know the position coordinates of each sensor node in the cluster."
"in this paper, for the problems of the existing anomalous data, the weighted fusion, and the limited energy in underwater wsns, a distributed particle filter algorithm based on grubbs criterion and mutual information entropy was proposed. firstly, during the tracking process, the obtained measurement data will have more correctness by eliminating some interference and error information measured from the sensor nodes. secondly, we conduct the dynamic weighted factors to weigh the contribution of the measurement data of each sensor node to the target state prediction, which effectively improves the accuracy of the measurement data to the target tracking. therefore, the tracking accuracy of the target tracking system is improved. the simulation results show that the proposed algorithm has higher tracking accuracy and better robustness than rekf, aw and ahpw algorithms. increasing the sensor density appropriately can improve the target tracking accuracy. however, considering the amount of calculation and the limited energy of the network, the density of the sensors should not be too large. furthermore, setting the suitable partition number of small regions of the network according to specific application requirements can result in applicable tradeoff between tracking accuracy and tracking energy consumption."
"a uav network consists of uav, command center, and some command stations. it takes uav application as its core. according to the characteristics of uav, uav network is sensitive to latency. moreover, it is important to protect the integrity and authentication of commands which are sent by command center and some command stations to uav. in this paper, we proposed a uav network architecture based on mec with low latency. we also proposed a dvpbs scheme for the scenario of uav rental. in this scenario, a user rents uav from a leasing company. we proved the security of our dvpbs scheme in the random oracle model. it is existentially unforgeable under an adaptive chosen message attack. moreover, we think that our dvpbs scheme has distinguishability and blindness. we compared the efficiency of our dvpbs with some other signature schemes by simulation experiments and theoretical analysis."
"it can be seen from fig. 11 to fig. 13 that, as the number of small regions increases from 4 to 16, the average position rmse of the proposed algorithm is always lower than the other three algorithms. in table 3, it can be seen intuitively that, as the number of small areas increases, the average position rmses of the algorithms will get larger and larger, that is, the target tracking accuracy becomes lower and lower. fig.14 shows that, as the number of small regions increases, the energy consumption of the network decreases. obviously, as the number of small regions increases, the target tracking accuracy decreases and the network energy consumption becomes lower. the network energy consumption and the target tracking accuracy are contradictory. therefore, it is crucial to make a tradeoff between the tracking accuracy and the energy consumption when we use the partition tracking strategies."
"entropy is a basic concept in information theory and it represents the amount of information in a random variable. mutual information entropy is used to represent the relationship between information, and it is a measure of the statistical correlation between two random variables. in the application of underwater wsns, the mutual information entropy can be used to measure the amount of information that the sensor nodes detect to the target state, and it also means the amount of target information which the sensor nodes can provide. the definition of the mutual information entropy between the measured value from the sensor nodes and the target state is given follows."
"it takes the system parameter, warrant, private key for proxy signature, proxy blind signature, public key of designated verifier, and conversion of message as inputs and outputs the signature."
"despite concerns regarding estimator consistency in this and other latent variable models, we were able to show that the notion of confidence sets may instead be used to provide a (conservative) measure of residual block structure. we note that many open questions remain, and are hopeful that this analysis may help to shed light on some important current issues facing practitioners and theorists alike in statistical network analysis."
"(iii) device layer. it mainly contains uav in the device layer. a uav receives commands from command center in cloud computing layer or command stations in edge computing layer, performs tasks, and sends response."
"thus, we can substitute (25) and (26) into (24), the recursive formula for the weight importance of the particles can be expressed as formula (27) ."
"however, before each resampling, the effective particle number at current moment needs to be judged to decide whether to perform resample. the effective number of the particle is used to measure the degree of degradation of particle weights, which is presented as formula (34)."
"proof. in our dvpbs scheme, the blind signature provider chooses a random number and uses it to blind the original message. according to the process of blinding message, the proxy signer cannot recover the original message from the message blinded. moreover, it also does not receive or infer the original message from the following communication content. hence, the proxy signer does not know the original message. namely, our dvpbs scheme has blindness."
"analogous to the deterministic setting, exact maximization of the corresponding log-likelihood l(a; z, θ, α, β, x) is computationally demanding even for moderately large k and n, owing to the total number of possible nodal partitions induced by z. however, when z is fixed the corresponding conditional likelihood maximization task is convex, and so we may adopt a stochastic expectationmaximization (em) approach to inference in which z is treated as missing data [cit] ."
"with the rapid development and maturity of the technologies of chip design and embedded systems, sensors are gradually developed towards miniaturization and integration with perceptual computing and networked communication capabilities. [cit] s, the us proposed the wireless sensor networks (wsns). wsn consists of a large number of intelligent sensor nodes with communication and computing capabilities, which are densely deployed in the monitoring area, and can perform specified tasks autonomously according to the environment characteristic. the underwater wsn is an extension of the terrestrial sensor network with more communication technology and information processing challenges [cit] . it has the inherent advantages of self-organization, wide coverage, high fault tolerance, highprecision measurement, low cost of networking, and flexible structure. therefore, it has been widely used in monitoring marine ecological environment, marine resources detection, and marine search and rescue as well [cit] ."
"wheren eff represents the approximate value of the number of effective particles at current time, and n th is an appropriate threshold (n th generally takes 2n 3). the judgment needs to be performed in each iteration. ifn eff is less than the threshold n th, the resampling is performed. if not, it is not performed."
"in section 3.1 below, we describe the dataset. then, in section 3.2, we verify empirically that 1 69 220 21 377 1531 67 456 926 2 105 349 22 614 2450 70 76 344 3 32 91 26 551 2066 71 74 358 5 157 730 29 569 2534 72 352 1398 6 108 378 38 521 1925 76 43 185 8 204 809 55 336 803 77 25 106 9 248 1004 56 446 1394 78 432 1334 10 678 2795 63 98 283 80 594 1745 18 284 1189 66 644 2865 table 2 : network characteristics (number of students and reported friendship edges) corresponding to each of the 26 schools used for analysis."
"this section mainly analyzes the effect of the different sensor densities on the three tracking algorithms. in the simulation experiment, the observed noise variance is set as 0.36 and the sensor density is taken as 0.00003 to 0.00012. the other parameters are set as the same as table 1. fig.10 shows the average position rmse of the different algorithms at different sensor densities. fig. 10 shows the average position rmse comparison of the different algorithms under different sensor nodes densities. the average position rmses of these algorithms decrease as the node density increases. as can be seen from fig.10, the average position rmse of the three algorithms is gradually reduced with the sensor node's density increasing. when the density increases from 0.00003 to 0.00008, the tracking accuracy is significantly improved. however, when the density increases from 0.00008 to 0.00012, the improvement of tracking accuracy is slower. this indicates that there is a limitation value of ρ, and if is exceeds that ρ, the benefits of using more sensors in tracking performance are very small. furthermore, as the ρ increases, the amount of computation and communication overhead of the network will increase accordingly. therefore, when designing an underwater tracking wsn, it is necessary to select a balanced value of sensor density of the network on the premise of compromising consumption and performance."
"since we will be unable to verify such blocks by checking against explanatory variables, we rely on the confidence sets developed above to assess significance of the discovered block structure."
"tracking accuracy mainly reflects the tracking effect of the target. in this paper, we adopt the root mean square error (rmse) to reflect the tracking accuracy of the target [cit] . the rmse of the target position is defined as formula (35)."
"from the above analysis, we find that it is necessary to shorten the distance between sender of command and uav and select efficient digital signature algorithm in order to reduce total delay time ."
"(proof completed) the mutual information entropy between the measurement data of the sensor nodes and the target state can effectively measure the correlation between the two variables, which is the effective information of the target that the sensor nodes can provide. the greater the mutual information entropy is, the more target information the sensor nodes can provide."
"confidence sets are a standard statistical tool for uncertainty quantification, but they are not yet well developed for network data [cit] . in this paper, we propose a family of confidence sets for network structure that apply under the assumption of a bernoulli product likelihood. the form of these sets stems from a stochastic blockmodel formulation which reflects the notion of latent nodal classes, and they provide a new tool for the analysis of estimated or algorithmically determined network structure. we demonstrate usage of the confidence sets by analyzing a sample of 26 adolescent friendship networks from the national longitudinal survey of adolescent health (available at http://www.cpc.unc.edu/addhealth), using a baseline model that only includes explanatory covariates and heterogeneity in the nodal degrees. we employ these confidence sets to validate departures from this baseline model taking the form of residual community structure. though the confidence sets we employ are known to be conservative [cit], we show that they are effective in identifying putative residual structure in these friendship network data."
(ii) process delay is mainly the delay time in which the receiver (uav) verifies the digital signature. it should select efficient digital signature algorithm to lower .
"(4) the signature computed by command station can only be verified by the designated verifier, namely, the uav. it uses designated verifier signature scheme to meet this requirement."
"key generation. it takes the security parameter as input and outputs some public-private key pairs (pk, sk) for the designated verifier, original signer, proxy signer, and blind signature provider."
"to test the performance of the underwater tracking sensor network, the observed noise variance is set as 0.36 and the sensor density is set as 0.00008 per m 3 . the number of small regions is taken as 4, 8, and 16, respectively. 100 times simulation experiments are performed for the three algorithms. to study the influence of the number of small regions on target tracking, we analyzed the tracking accuracy and network energy consumption of target tracking algorithms under different partition of small regions. the average position rmse of the three algorithms under different number of small regions is shown in fig. 11-13, and the network energy consumption is shown in fig.14. table 3 shows the average position rmse of the three algorithms with different numbers of small region."
"in the above importance sampling process, the particle will meet the degradation problem, and this phenomenon is usually unavoidable. therefore, the resampling method needs to be inducted. the basic principle of resampling is to eliminate the particles with small weights and duplicate the particles with large weights for keeping the number of particles unchanged. then the new particles will be assigned the same weight as 1 n ."
"in this paper, we informally classify uavs into three categories according to their autonomy, namely, non-autonomous uav, semi-autonomous uav, and autonomous uav. a nonautonomous uav mainly depends on the control signal from operator's handheld controller. an operator sends control signal through the handheld controller to direct uav to complete task. non-autonomous uav usually has small body size, low flight speed, short flight distance, and limited computing and communication capabilities. it typically performs task within the operator's line of sight. in contrast, an autonomous uav has larger body size, higher flight speed, longer flight distance, and relatively sufficient computing and communication capabilities. it is able to fly farther and perform more complicated task without requiring the operator to command in real time. after it receives the command from operator, the uav will check this command and decide whether to execute it. a semi-autonomous uav is a kind of uav between nonautonomous and autonomous uavs. in this paper, we focus on the application of autonomous uav. generally, uav network refers to the network with uav applications as its core. it has some characteristics such as high mobility, dynamic topology, intermittent link, power constraint, and changing link quality [cit] . a uav network may be comprised of uav, command center, ground control stations, satellites, mobile command cars, or ships, which is illustrated in figure 1 . in a uav network, uav belongs to a command center. it accepts the commands issued by command center and performs related tasks. ground control stations, satellites and mobile command cars, or ships may be all called command stations and mainly responsible for maintaining the communication between command center and uav. the command center can communicate with uav with the help of command station which forwards the commands received from command center to uav. in addition, the command station also commands temporarily uav in some cases."
"in this paper, it is supposed that a user rents uav from a leasing company to perform some tasks. the leasing company authorizes a lgcs to command the uav. the user requires service of uav through lgcs, and lgcs does not know what service the user requires. the command sent by lgcs can only be verified by the uav. we make the following contributions for this scenario."
"to study the stability of the target tracking algorithms under different conditions, this paper analyzes the performance of the algorithms by changing the observed noise. firstly, we set the initial conditions to an ideal state with the small observed noise, and then gradually increase the observation noise to compare the performance of the three different algorithms. in order to compare the performance of the algorithm, we adopt the average position rmse described in eq.(35) to test. under the simulation conditions described in section a, the observed noise variance is taken as 0.36, 2, and 5 respectively. to compare the simulation results more intuitively, the tracking trajectories obtained by the three algorithms are shown as fig. 4, fig. 6 and fig. 8 . the average position rmse is shown as fig. 5, fig. 7 and fig. 9 . the influence of observation noise on target tracking accuracy is not negligible. it can be clearly seen from fig. 4-9 that, as the variance of the observation increases, the tracking accuracy of the different algorithms decreases to a certain extent, and the target tracking error increases to a certain extent. it can be seen from fig. 5, 7, and 9 that the tracking error of the three algorithms is small at the initial stage of target tracking. as time goes on, the tracking error of the target reaches the maximum at the target turning. the algorithm is closer to the true trajectory of the target regardless of the value variance of the observed noise, and the average position rmse of the proposed algorithm is always smaller than the other three algorithms. it indicates that the proposed algorithm can effectively perform target tracking and has better robustness, which greatly improves the target tracking accuracy of the system."
(3) a user may not want command station to know what tasks it requires uav to perform to protect the privacy. it uses proxy blind signature scheme to meet this requirement.
"(1) leasing company delegates a lgcs to sign on behalf of the company. (2) lgcs verifies the delegation and computes a private key for proxy signature, . (3) user blinds message to obtain . (4) user sends to lgcs. (5) lgcs signs the blinded message,"
"lingjun gao received the b.s. [cit] . she is currently pursuing the m.s. degree in electronics and communication engineering with shanghai maritime university, china. her research interests include target tracking in wireless sensor networks and the internet of things."
"paper. the rest of this paper is organized as follows. in the next section, we review mec and some digital signature schemes which include blind signature scheme, proxy signature scheme, proxy blind signature scheme, designated verifier signature scheme, and designated verifier proxy signature scheme. in section 3, we provide some necessary preliminaries. we propose a uav network architecture based on mec and a designated verifier proxy blind signature scheme for the uav network in section 4. we analyze the security and efficiency of our dvpbs scheme in section 5. finally, this paper is concluded in section 6."
"definition 3: the mutual information entropy weighting factor is introduced in (31) to obtain an improved weighting formula for sir particle filtering, and the importance weight of the i − th particle at time k is calculated as formula (33)."
"(2) appears to give rise to a loss of estimator consistency, as shown in table 1 where the empirical bias of each component of β is reported. this suggests, as we alluded to above, that inferential conclusions based on parameter estimates from latent variable models should be interpreted with caution."
"the rest of the article is organized as follows. in section ii, the network model, the target motion model and the observation model are presented. section iii gives the detailed description of the proposed algorithm. the performance index of the target tracking system is introduced in section iv. in section v, simulation results are presented to verify the effectiveness of the proposed method. the conclusion is drawn in section vi."
"intuitively,φ (z) measures assortativity by z; whenever the sociomatrix a is unstructured, elements ofφ (z) should be nearly uniform for any choice of partition z. when strong community structure is present in a, however, these elements should instead be well separated for corresponding values of z. thus, it is of interest to examine a confidence set that relatesφ (z) ab to its expected valuē φ (z) for a range of partitions z. to this end, we may define such a set by considering a weighted sum a confidence set is then obtainable via direct application of the following theorem."
"assume that there are a large number of sensor nodes randomly anchored in the underwater monitoring area s. each sensor node is equipped with an acoustic sensor and evenly distributed in the monitoring area with the density of volume 7, 2019 ρ per m 3 . the maneuvering target moves within the monitoring area s. according to the characteristics of the monitoring area and the target, the monitoring area is divided into several small areas. all sensor nodes in each small area constitute a cluster. in this paper, the cluster heads (ch) is deployed in the center of each small area, and one-hop communication can cover all the distances between cluster head and sensor nodes within the cluster. the base station is deployed on the water surface. the horizontal signal transceiver is mainly used for communication between the sensor nodes and the cluster heads, and the vertical signal transceiver is mainly used for communication between the cluster heads and the water surface base station."
"phase. when a leasing company rents a uav to a user, it will delegate a lgcs which is close to the uav to command uav. the leasing company performs the following steps to implement the delegation for lgcs."
"it computes the public key for proxy signature. afterwards, it takes the system parameter, public key for proxy signature, private key of designated verifier, warrant, message, and the signature as inputs and determines whether the signature is valid. it will return true if the signature is valid; otherwise, it will return ⊥ which means termination."
passx a k and p a k to the next new cluster head; 24: end for 25: the new cluster head node does the following steps: 26: for i ← 1 to n do 27:
", to obtain the signature (, ) on behalf of leasing company. (6) lgcs sends (, ) to user. (7) user recovers signature of message from (, ) to obtain (, ) and transform to ( ), where the transformation is one way."
"however, there are still some problems in the current target tracking that are not well solved by the above methods. since underwater sensors need to work in extreme underwater environments, there will be redundancy and anomalous data during the measurement process to affect the target tracking accuracy. sensors for underwater wsn are battery powered and it is not feasible to replace the battery when the battery is exhausted. this means that battery life affects the life of the entire network. during the target tracking process, the distances and directions of the individual sensor nodes from the target are different, and the measured values of each sensor node contribute differently to the target state estimation. therefore, in this paper, we comprehensively discuss the above problems, and propose a distributed particle filter algorithm based on grubbs criterion and mutual information entropy weighted fusion."
"in addition, designated verifier signature is necessary. an adversary may eavesdrop on the messages exchanged among the lgcs, user, and uav and verify the signature. it can infer the tasks that uav will perform next and take precautions in advance. therefore, it is necessary to ensure that the signature can be verified only by the designated verifier. it uses designated verifier signature scheme, which meets the requirement. for example, the user or lgcs sends command and corresponding signature to the uav, while the signature can only be verified by the specified uav. even if an adversary eavesdropped on the command and corresponding signature, it could not verify the signature and confirm the task which will be performed next."
"(5) it should use efficient cryptographic algorithm, which helps to reduce the processing delay . therefore, we propose a uav network architecture based on mec and a designated verifier proxy blind signature (dvpbs) scheme for the uav network to meet the requirements above analyzed."
"-mercier, p., mohan-kilambi, s., nowrouzian, b., 2007, optimization of frm fir digital filters over csd and dbns multiplier coefficient spaces employing a novel genetic algorithm, computers, (7), 20-31."
"we treat the friendship network data from these 26 schools as undirected graphs, with an edge present between two students whenever either of them indicated any type of friendship with the other. other approaches are also possible and can be expected to influence inferential results [cit], though we verified through exploratory data analysis that estimates were qualitatively similar even if sparser versions of these networks were constructed by instead requiring mutual indication of friendship within student pairs, resulting in a roughly 3-fold decrease in the observed number of edges in our data (cf. table 2 ). as in section 2.4, for each student pair the corresponding vector x(i, j) of explanatory variables comprised an intercept term, binary indicator variables indicating shared gender and shared race, and an ordinal value equal to the absolute difference in the grades of the students; model (2) was then fitted to these data, with results described below."
"the grubbs criterion is a judgment method for abnormal data obeying a normal distributed sample or closely obeying a normal distributed sample in case of an unknown population standard deviation [cit] . since the grubbs criterion not only considers the times of measurement, but also considers the different confidence levels, it has strong ability of inspection and identification. first, it is used to analyze the measured data to find and eliminate the abnormal data in the raw data, relatively reduces the data which are far away from their real values, and basically ensures that the remaining data approximately obeys the normal distribution. then, the mutual information entropy between the measured value and the target state is used to determine the weight of the nodes status, and this can have the weight have more objectivity instead of being given randomly. here we assume that the network sensor nodes are the same kind of nodes with same kinds of sensors to detect the target at different positions."
"the above equation is generally called sir (sampling importance resampling, sir) calculation formula. however, in the process of calculating the importance weight, it is not considered that the contribution of each sensor node to the estimation result of target state is different, and this affects the tracking accuracy of the entire tracking system. therefore, a dynamic weighting factor β i k is introduced in this paper. when the contribution of the measured value from sensor node to the target state estimation is larger, the corresponding weighting factor is larger, and when the contribution of the measured value to the target state estimation is smaller, the corresponding weighting factor will be smaller as well. mutual information entropy can effectively measure the correlation between two variables, that is, it can measure the effective information about the target provided by sensor nodes. the larger the mutual information entropy is, the larger the amount of information about the target provided by sensor node will be, and the corresponding weight coefficient of sensor node is larger. therefore, the weight coefficient corresponding to the sensor node is equivalent to the information volume about the target provided by the sensor node. in this paper, the mutual information entropy between the measured value of sensor nodes and the target state is used as the dynamic weighting factor for calculating the importance weight of particle filter, which can improve the target tracking accuracy effectively."
"it may be seen from fig. 1 that the largest divergence observed was less than 41% of its corresponding bound, with 95% of all divergences less than 22% of their corresponding bound. this analysis verifies that, despite the issues associated with estimator consistency in the context of latent variable models, our confidence sets can be used as a tool to assess putative network structure."
"the task of the detection module is to determine whether the target appears its monitoring area. since the energy carried by the sensors is limited, the sensor nodes periodically detect whether the target appears within the detection range to save the energy consumption of the network. the task of the node selection module is to select the appropriate sensor nodes for observing the status information of the target. the task of the positioning module is to estimate the location of the target by using the perceptual information obtained by the selected most appropriate sensor nodes. the task of the prediction module is to collect the current and previous target state information through the sensor node and then use the prediction algorithm to estimate the target state information at next moment. the task of the routing module is to choose a highly efficient and feasible routing algorithm to pass the obtained target state information to the corresponding cluster head or base station."
"particle filtering can be used as an estimation method based on monte carlo and recursive bayesian estimation. the essence is to approximate the posterior probability distribution using a set of randomly sampled particles with corresponding weights [cit] . it is not affected by linear errors and gaussian noise, so it has better performance than the kalman filter and the extended kalman filter for the nonlinear and non-gaussian state space models. the particle filter algorithm needs to be recursively calculated by two steps of predicting and updating."
"target tracking algorithm of underwater wsns requires to consider multiple performance metrics, such as tracking accuracy, tracking response time, and energy consumption. an ideal target tracking system has high tracking accuracy, less tracking response time and energy loss. the tracking response time mainly reflects the real-time performance of the target tracking system, which is related to the calculation time of the prediction algorithm and the routing method of the tracking system. the tracking accuracy and energy consumption should be compromised. for example, if the tracking accuracy of the system is improved, more measurement data of the sensor nodes are needed, which leads to more energy consumption of transmitting more information."
"however, common digital signature scheme is not suitable for uav network because of its characteristics and security requirements. it needs to propose digital signature scheme for uav network."
"in some cases, it also needs blind signature to protect user's privacy. it is supposed that there is a leasing company which rents uavs to users. namely, the company is lessor and user is lessee. the user rents a uav to perform some tasks, such as taking pictures, surveying, and investigation. if the user does not want leasing company to know what commands the uav executed to protect user's privacy, it will use blind signature scheme. moreover, if the user is far from the command center of leasing company, the company will use proxy signature scheme to authorize a ground control station which is closer to the uav to command it. namely, it uses proxy blind signature scheme, which meets the requirements. we call this ground control station as local ground control station (lgcs). after the lgcs obtains authorization from command center of leasing company, the user will blind the command and send it to lgcs. the lgcs computes the signature of command blinded and sends it to the user. afterwards, the user recovers the signature of original command, which is proxy blind signature of original command. it sends the proxy blind signature to uav."
(iv) we implement simulation experiments of our dvpbs scheme and other signature schemes and theoretically analyze their communication cost. the experimental data indicates that our dvpbs is efficient in computation efficiency. the signature length is short compared with the transmission capacity of uav.
"according to the definition of the conditional entropy, the conditional entropy between the measured value of the j-th sensor node and the target state at time k is calculated as formula (15) ."
(ii) we propose a designated verifier proxy blind signature scheme for uav network based on mec. this signature scheme is based on elliptic curve cryptography (ecc) which provides efficient computation.
"the architecture of uav network based on mec is illustrated in figure 2 . this architecture is divided into three layers, cloud computing layer, edge computing layer, and device layer. (ii) edge computing layer. the cloud layer connects with edge computing layer through internet or private network. this layer contains some command stations which may be authorized by command center to command the uav, including satellites, ground control stations, and mobile command cars. these command stations can provide computation and storage services. they perform as virtual servers at the edge of network and are called edge servers. they are proxy signers in our dvpbs scheme and closer to uav than command center, which is helpful to reduce latency."
"the proposed methodology involves the acquisition of data from two heterogeneous sensors, with different concepts of operation: thermographies are acquired with a thermographic camera, which absorbs radiation emitted by all bodies at a temperature over 0k in the infrared range of the spectrum, between 7 and 14 µm. according to the standards for use of thermography in building inspection (iso 6781, 1983), the indoors thermographic survey should be performed under conditions that maximize indoor-to-outdoor temperature difference, and therefore optimize the detection of defects, such as heat loss and humidity. as a consequence, the perfect times are either at midday, when outdoor temperature reaches its maximum and indoor temperature is usually colder allowing the detection of incoming heat, or at the beginning of the day in winter, with the heaters turned on, allowing the detection of incoming cold air. from a metric point of view, thermographies must be acquired from a position forming an approximate angle of 25º with the normal of the inspected wall in order to avoid angle effects and to minimize reflections due to radiation of nearby objects [cit] . the metric survey is performed with a laser scanner, from which the 3d representation of the scene, rather than images, is obtained."
"in this work, curvature analysis is performed by calculating the normal vector of each point in the point cloud as shown in figure 1 . this is done by principal component analysis (pca) using the covariance method (jolliffe i.t., 2002), taking advantage of the fact that the eigenvector associated with the smallest eigenvalue is the normal vector of the point. this is done by analysing each point and its 50 closest neighbouring points [cit] . then, normal vector smoothing is performed by averaging normal vectors of points in the same neighbourhood, as in (thürmer, 2001 ), but using a 50 closest points neighbourhood. once the smoothed normal vectors of all points are calculated, points are classified based on the direction of their normal vectors, into three main classes: (a) parallel to the main wall, commonly the longest, or the windowed one; (b) perpendicular to the main wall, consequently, parallel to the side walls; (c) horizontal, comprising of floors, ceilings, and other horizontal surfaces in the room such as tables. once points belonging to parallel planes are separated, planes with the same orientation are detected by examining distances between points. in parallel to the above steps, the computed eigenvalues from curvature analysis are also used to detect edges on discontinuities in the point cloud. in indoor scenes, there is usually a great deal of occlusions creating discontinuities in the point cloud which should be detected as lines in order to improve and facilitate image registration with thermographies. the points belonging to discontinuity edges are classified via pca analysis, exploiting the fact that their largest eigenvalue is 1.5 or 2 times larger than their second largest eigenvalue."
"once extracted, 3d lines from the point cloud and 2d lines from images are used to calculate the orientation of each image with respect to the point cloud, through two iterative processes, one inside the other as explained in figure 11 : the main process is performed with the ransac iterative method, using a minimum of three lines for the resolution of the collinearity equations and the subsequent rotation matrix and translation vector. these equations are solved iteratively until the residuals of the image orientation parameters are below 0.001. then, the orientation result is applied to the image so that the fitting error is computed and stored together with the result. then, following with the ransac method, a new set of line correspondences is chosen and the whole process is repeated until the fitting error reaches a minimum; this last result is considered as the best. however, the second and third best results are also kept in memory as backup, since the result with the highest number of coincident points is not always the optimal. this last decision is made through the visualization of the textured mesh. the mesh is generated independent of the registration process via triangulating the point cloud. figure 11 . iterative processes used for the resolution of the collinearity equations and the image registration with the point cloud."
"the proposed methodology has been applied to the study of the interior of a room, with a regular cubic shape. thermographies are acquired with a thermographic camera nec th9260, whose sensor matrix is an uncooled focal plane array size 640x480 pixels. its thermal resolution is 0.06ºc at 30ºc, and its field of vision is of 21.7º in the horizontal plane and 16.4º in the vertical plane. therefore, for example, in order to cover one wall, 10 thermographies are required, with an overlap of 30%; an example of 3 consecutive images is shown in figure 4 . the thermographic survey was performed at midday on a sunny day in order to maximize indoor-to-outdoor temperature difference and therefore optimize the detection of defects such as heat loss and humidity. indoor temperature was 27ºc, while outdoor temperature was several degrees below, at 19ºc. the blue rectangles show the overlapping areas between consecutive images, vertically and horizontally."
"once the planes are extracted, the points lying on their intersections form lines. points following the boundaries of the point cloud and its holes also manifest themselves as lines."
"this paper presents a method to automatically register thermographic images with a point cloud for an indoor environment. there are two main challenges in solving this problem: first, the limited field of view of thermographic cameras requires a large number of thermographies to be collected to represent a single wall; second, the repetitiveness of indoor scenes hinders the detection of corresponding features and the calculation of the image pose with respect to the point cloud. another fact that must be taken into account is the presence of occlusions in the point cloud mainly due to the furniture in the room, which impedes the measurement of wall points and provokes the detection of points belonging to discontinuity edges and consequently forming boundary lines, in non-physical boundary locations. however, boundary lines are not discarded from the process because these lines are also detected in some thermographic images, so they can be used in the calculation of the orientation parameters of each image."
"the 3d model resulting from the proposed methodology is shown in figure 12 . the registered thermographies are applied to their corresponding rectangles, whereas for those rectangles with more than one corresponding image, i.e. overlapping regions, the image applied is the one acquired from the most orthogonal position with respect to the triangle. this leads to some irregularities in the colour of the problematic areas, as highlighted with a white arrow in figure 12, in the wardrobe, where light red triangles appear among dark red areas. in addition, a registration error can been detected in the right part of the window, highlighted with a blue arrow in figure 12, since the thermographic image shows the window slightly turned downwards. however, these artefacts in registration do not decrease the usefulness of the thermographic 3d model for thermographic inspection and energy analysis, since: (a) the entire wall can be thermographically visualized at the same time, allowing for an overall impression and the visual detection of defects that might have gone unnoticed had they appeared divided in different thermographies; (b) defects can be spatially located, as for example heat loss can be detected in the left portion of the room, where the curtain appears at a higher temperature than in the rest of the room, implying that the joint between the wall and the window is not hermetic in that area; (c) regarding energy consumption, the non-insulated joints between panels are detected over the window, and both temperature and length can be measured for each of them as the model contains temperature information from thermographies and geometric information from the point cloud; as a consequence, these thermal bridges are characterized for use in energy simulation software, and identified for repair in energy building rehabilitation."
"the point cloud is converted into a mesh of triangles, which are then textured with the corresponding thermographic images using the orientation results obtained through the collinearity equations in the iterative algorithm."
count the number of common points between transformed 2d lines from thermographies to 3d lines from the point cloud. the entire process is repeated until the set of matches from which the rotation matrix is calculated results in the highest number of coincident points.
"combining thermographic and geometrical data is a key issue for energy building inspection. this is because thermographies provide information about the actual condition of the building, whereas geometric data allows quantification for project management, and together they stand as a valuable product for further simulation and energy evaluation. an application example for this data combination is the characterization of the thermal bridges in a building: while thermographies provide information about temperature, geometric data provides information about the surface or the length of the thermal bridges; temperature and length, together with the thermal resistance of the bridge, are the parameters needed for the characterization of a thermal bridge in an energy analysis tool, such as trnsys©. furthermore, the registration of thermographies with the geometric data captured, for example by a laser scanner, leads to the spatial localization of thermal defects, making possible the placement of the detected thermal bridges and thermal anomalies in their exact position within the walls of the building. the wide range of possible applications has led to the increasing research in 3d modeling with thermographic images, both by applying photogrammetry techniques to the images alone [cit] and by fusing them with a laser scanning point cloud (lagüela [cit] ) . in the thermographic image -point cloud fusion approach, the main obstacle to be solved is the large number of thermographic images needed for a complete representation of a building. this is due to the limited field of view of current thermographic cameras, which makes image registration with the corresponding point cloud time consuming and inaccurate if the manual selection of control points is used. this has motivated the need for the automatic registration of thermographic images, as is done with images from a photographic camera (gonzález [cit] . image registration with a point cloud can be directly performed if the thermographic camera and the laser scanner are connected via a rigid body and their pose remains fixed as data is acquired, as is usually the case in aerial lidar. however, this approach introduces many limitations, as the flexibility of the image acquisition is completely lost [cit] both from the geometric and temporal points of view; consequently it cannot be used in the thermographic case, where images have to be acquired almost perpendicular to the wall for the camera to capture radiation so as to minimize radiation from surrounding objects; on the other hand, the laser scanner only needs to see the 3d point from its position in order to measure its coordinates. furthermore, image acquisition flexibility is appreciated in the building inspection application, as it is useful for building inspections to have a building model available to which thermographies from different inspections could be registered at different times of the year in order to keep the model updated or to monitor the energy consumption of the building within a year or across the years. typically, two different approaches can be used to solve the registration of images with a point cloud. some studies propose the calculation of the orientation parameters by solving a 2d -2d problem, which requires the projection of the point cloud onto a plane (gonzález [cit], and the extraction of features in both the visible image and the intensity or range image produced from the projection; the second possibility is to solve the 2d -3d problem, either by finding point correspondences between the point cloud and the image [cit], or by extracting linear features [cit] ). this paper presents a methodology for the registration of thermographic images with a point cloud using the 2d -3d approach; this choice is supported by the fact that a great deal of valuable information about the shape and the geometric layout of objects is lost when a 2d image is formed from the corresponding 3d world [cit] . this paper is organized as follows: section 2 explains the different steps involved in the 2d -3d registration of thermographic images with the point cloud, from data acquisition to thermographic 3d model generation. section 3 presents the results obtained with the application of the proposed methodologies to the thermographic 3d modelling of a wall in a room, and section 4 includes conclusions."
"the complete methodology applied to the registration of thermographic images with the point cloud involves (i) curvature computation of the point cloud for the point clustering in planes with different orientations, (ii) the intersection of the detected planes to identify points belonging to intersection lines, and (iii) the identification of boundary points belonging to discontinuity edges and their clustering in boundary lines. thermographic images are subjected to a line segment detector algorithm that extracts lines by studying the changes in the grey level. three-dimensional and twodimensional lines are then used for the calculation of the orientation parameters of each image with respect to the point cloud. this process improves the manual registration process by not requiring a dedicated human operator. the iterative method applied to the calculation of the orientation parameters provides an optimal final result. furthermore, from the thermographic point of view, this registration does not affect the temperature-colour relation; therefore thermal inspection can be performed as is done in the images directly, although some attention must be paid to the presence of holes in the object not due to the presence of low temperatures but due to the lack of triangles to texture."
"our approach, which is predicated on developing recognition grammars for each relation, entails an effort significant enough to drive us to focus only on the most frequent relations. however, the difficulty in automatic relation detection per se hampers our ability to automatically determine the most frequently occurring relations in a question set; in order to count them, one has to be able to detect them. as an approximation, we analyzed 20,000 randomly selected jeopardy! questions with their correct answers against a few data sources we judged to match the jeopardy! domain well. for each question, we looked for known relations in any of the chosen sources between a named entity in the question and the answer. the frequency of each relation in the sources linking a named entity in a question to an answer was aggregated over all 20,000 questions. based on this approximation, figure 2 shows the relative histogram of most frequent freebase [cit] relations in these questions."
"the selection bias was the second bias which we tried to address in this study. for avoiding the selection bias, the title and abstract of each identified study were assessed by the first reviewer (kz) blinded to authors, affiliations and the publishing journal. then, the second reviewer independently assessed the title and abstracts of the studies."
"in summary, the system serves as smart sampling of the underlying flow providing insight in flow features within necessary context information. the results from the automatic adaptation of the particle density are not achievable by simple adjustment of the seeding strategy with classic particle systems. observing moving particles supports an intuitive way of understanding flow phenomena in unknown data or known data for more detailed inspection. as such, it is a powerful and easy to control method for explorative flow visualization or as complement for other visualization techniques."
"the remainder of this paper is organized as follows: the experimental study and battery modeling are described in section 2; in section 3, the drawback of taylor method is discussed and the improved ga-based sop estimation approach is proposed; the verification and evaluation of the algorithm are carried out in section 4; the influence of the soc error on the sop estimation under varying conditions are investigated in section 5 while the conclusions are summarized in section 6."
"based on the united nations' country classification [cit], the findings indicated that 46 (82%) articles were related to developed countries, as shown in table 5, while seven (12.5%) were related to developing countries [cit] . three demand based articles (5%) belong to russia as a country in transition [cit] ."
"the internet and social media platforms are considered as channels for the distribution and determination of health information in infodemiology. although social media are potentially powerful tools for engaging and enabling users searching for relevant health information but the trustworthiness of the user generated content produced in them is questionable [cit], the main barrier preventing consumers from using social media as an information channel is trustworthiness."
"with respect to the validation, 33% (nine articles) of the demand based studies made comparisons against comparison data sets to validate their output [cit] ."
"through few intuitive parameters, our approach is easily controllable and flexible to target different usage scenarios, as described in req.3. the setting of the few control parameters and the used split criterion provide a simple and meaningful way to steer the system. the resulting effect on the visualization when changing individual parameters can be observed in the accompanying video. the presented method is purely gpu-based. it is interactive also for large sets of particles. the performance evaluation in section 5 proves that the proposed method does not perform significantly slower than a traditional particle system and therefore fulfills req.4. since the concept of autonomous particles is parallel in nature, it can also serve as a basis for distributed and cluster-based methods targeting the ever-growing size of flow data sets. additionally, our method does not rely on pre-computation or field analysis, which is highly beneficial in the context of streaming and time-dependent data."
the first strength point of the study is its scoping review methodology because it is the first attempt to systematically map the published literature on infodemiology studies. the other strength of this scoping review includes a broad search of the literature using multiple databases. each article was reviewed by two independent reviewers who met at regular intervals to resolve conflicts.
"title-in-clue passage search using indri watson's tic passage search component uses characteristics of title-oriented documents to focus search on a subset of potentially more relevant documents. its implementation makes use of indri's support for dynamic subcorpus specification and a dictionary that maps canonical wikipedia document titles 1 (e.g., naomi) to all wikipedia documents with that string as their title [e.g., naomi (band) and naomi (bible), among others]. the dictionary is used to identify document titles in the question, and a subcorpus of documents is dynamically constructed by aggregating all target documents in the matched dictionary entries. although the same query is used as in indri passage search, we found that by constraining the corpus to a much smaller and potentially more relevant subset, this search strategy can succeed when the general passage search strategy fails."
"our results show that document search and passage search have very high coverage, returning candidates for all questions for passage search and all but 1% of the questions for document search. 3 although they both yield high candidate binary recall, that is, 74.43% and 79.40%, respectively, they also generate a fairly large number of candidates per question. contrast that with answer lookup and prismatic search, which are active on a much smaller set of questions and correspondingly have much lower overall binary recall. however, for these search strategies, only a small number of candidate answers are added to the pool. note that answer lookup yielded a candidate binary recall of 3.53% on blind data, which is close to the 4% estimate in an earlier discussion. the row labeled bpercentage unique[ in table 1 examines the unique contribution of each search and candidate generation approach, i.e., the loss in candidate binary recall if that strategy is ablated from the system. our results show that although the degree of overlap is quite high among the different methods, each approach does make unique contributions to achieve an 87.17% combined candidate binary recall. we analyzed the 429 questions with candidate recall failures and found that roughly three-fourths of them are due to search failures. these questions typically fail because the question contains extraneous information that is relevant but not necessary for identifying the answer. for example, in bstar chef mario batali lays on the lardo, which comes from the back of this animal's neck[, the essential part of the question is the segment in bold. however, the prominent entity bmario batali[ steered search in the wrong direction and dominated our search results. for the other one-fourth, search returned a relevant passage, but our candidate generation strategies failed to extract the answer as a plausible candidate. in most of these examples, the correct answers are common nouns or verbs, which generally have lower coverage for both our anchor text and wikipedia title candidate generation strategies."
"the search terms used were infodemiology, infoveillance, and e-epidemiology. no proximity operators or stop words were used. boolean operators and free-text searching were used. the references of some articles were checked to identify relevant studies. the inclusion criteria for the research are as follows: (1) they should be in english language, (2) [cit], and (3) the infodemiology studies should be in the field of health care. the exclusion criteria were if they were in other languages or intended to present a model in infodemiology (table 2) ."
"with the advent of the web 2.0 paradigm, the internet is being used as a means for the distribution of personal health information rather than simply as an information source. also, with the advent of the web 2.0 technologies, huge amounts of content are produced daily by users in the form of web pages, blogs and social networks [cit] ."
"in this stage, mns communicates their local information with their neighbors and make a common decision about presence or absence of pu. they share information via common control channel (ccc). the ccc is responsible for transferring control information and crus coordinate with each other using this medium. the ccc not only facilitates cooperation among crus but also provides several other network operations that include neighbor discovery, topology change, channel access negotiation, and routing information updates. in this algorithm overlay, ccc was used because crus temporarily allocated the spectrum not used by pu. further, in this algorithm, we assumed that the dedicated ccc is predetermined and usually unaffected by pu activity."
"r a new particle-based method, which adapts its density automatically to the complexity of the underlying flow highlighting important features while still providing context information."
"although the contribution of developing countries to infodemiology studies has been negligible, this methodology can be a potential for these countries; web technologies have provided new challenges and opportunities for health information specialists for analysing ugc by data mining methods in real time, providing evidence based, reliable and appropriate heath information for health information consumers and health literacy promotion."
"(1) implementing prediction rule given in (5) above given factor affect affect convergence rate, so the aim through their execution is to fasten convergence rate. implementation of these constraints is explained below. this whole process of cdcss and rwmm is also shown via flowchart in figure 7 ."
"a more proactive role might be conducting infodemiology studies by health information specialists themselves; therefore, opportunities must be available for health librarians to structure individual training efforts to develop new knowledge, skills and expertise in taking on infodemiology studies. it is important also for health information specialists to set up personal goals and a continuing professional development plan to gain new knowledge, skills and abilities to take on infodemiology studies [cit] . therefore, they should consider a baseline of competency areas such as data mining when drawing up personal professional development plans. health information specialists in emerging roles need to keep their skills up to date to remain competitive and reply to the changing requirements of their profession. it is worth mentioning that using infodemiology approaches for mining peer generated contents on social media, being aware of peers' information needs in real time and replying to their information needs with evidence based, reliable and appropriate information will cause peers' professional empowerment and profession empowerment."
"for estimating the relevance of a passage for a given search query, given that the passage is extracted from a document relevant to the query, we found that a simple measure of keyword and phrase matching outperforms more sophisticated alternatives. this score is then multiplied by the search score of the document from which the passage is extracted."
"for the demonstration of the effectiveness of the proposed method data sets related to technical and medical flow simulation have been chosen. for the first group the well-known, as benchmark data set, and the delta wing were used. [cit] [brb*15]."
"where n s is the number of cells connected in series in each module, n p is the number of modules connected in parallel, i dis max and i chg max are the maximum allowed discharge and charge current solved by:"
"the term 'infodemiology' was first used for analysing the demand side of whatever is published in the web, and then, it was used in the supply side for analysing the people's needs and monitoring their health information seeking behaviour."
"memory consumption: since our algorithm is running entirely on the gpu, we focus on the gpu memory only. we consider the three main parts, where gpu memory is consumed. first, data set storage. second, the set of autonomous particles. third, rendering, as well as post-processing (e.g. deferred shading and ssao)."
"the list of developing countries that participated and the number of their papers is as follows: china (3) hong kong (1), south korea (1), thailand (1) and taiwan (1) (chart 2)."
"the present study also has some limitations. firstly, although we tried to do a comprehensive search in the databases, it is possible that there were some studies that we could not retrieve and include in the review; limiting our search to english language may have excluded some of the studies in other languages."
"during our experiments we found that the system reaches a steady state frequently, meaning that the particle count in total stays almost constant and deletion as well as split and insertion operations level each other out with respect to particle count. this is a desirable behaviour and is intended, meaning that a smart sampling of the data set (i.e. high density in areas of interest, low density but context information in other areas) is found in a consistent manner."
"the web tools used for conducting infodemiology studies have some limitations. the first limitation of these web tools, such as google trends or twitter, is that they track only the segment of population that uses and surfs the web and monitors their health information behaviour [cit] b; [cit] . however, as mentioned in other studies, the major limitation of these web tools, especially google trends, is the lack of detailed information on the method used for the search and analysis of new big data streams [cit] b; [cit] ) ."
"the top 20 relations in the jeopardy! domain (see figure 2 ) are predominately entertainment and geographical relations, which together appear in approximately 11% of our questions. the relation detection recall is approximately 60%, and its precision is approximately 80%. the edm step has a recall of approximately 80%, and the data sources cover approximately 80% of the relations we need. multiplying these probabilities together, we expect answer lookup to return the correct answer in approximately 4% of the questions. this estimate is validated by evaluation on a blind test set discussed later in this paper. the answer lookup score is discriminating only in a few cases, such as when multiple relation instances are detected and the intersection of the query results contains only one element, or when the relation instance detected has only one answervsuch as when the question seeks the author of a book."
"sensing. spectrum sensing techniques are classified into noncooperative and cooperative spectrum sensing. the three most common schemes for noncooperative transmitter detection are energy detection, matched filter detection, and cyclostationary detection [cit] . due to random changing in wireless environment, it is difficult for a single cru to detect pu signal accurately. to cope with factors such as noise uncertainty, shading, and fading, cooperative spectrum sensing is introduced by researchers. in cooperative spectrum sensing, crus cooperate and share their information about pu detection. these methods give more accurate results as uncertainty can be minimized [cit] ."
"glyph-based techniques are widely used for scientific visualization to encode information. they are employed, for example, in molecular dynamics [tcm06], medical applications [kwl*04], astrophysics [wkm05] and flow simulation [bah13] . the shape of a glyph may be specifically designed for certain domains (e.g. glyphs on a map or for individual data points, arrow glyphs representing blood flow) or of simple shape like points. for visualizations it is common to animate glyphs over consecutive frames. the resulting motion and development of structure lead to a better understanding of the underlying data."
"after selection, the selected individuals will undergo crossover and mutation operations to generate new individuals. the crossover operator can combine two individuals to form a new individual. the mutation operator can exert a small change in the individuals, which provides the genetic density. to improve the convergence speed, a heuristic crossover operator is used here to guide the offspring to the direction of the good parents. heuristic ratio is an important parameter that determines the degree of the parents' guidance to the offspring. the heuristic ratio is generally between 0 and 2. to choose a reasonable value, we perform 20 discharge sop estimation tests on cell #1 under different heuristic ratios, as shown in fig. 5 . as shown in fig. 5 (a), the algorithm can achieve a stable convergence around the reference value when the heuristic ratio is between 1.1 and 1.7. fig. 5 (b) shows that the number of iterations is relatively small and stable when the heuristic ratio is 1.1. thus, the heuristic ratio is set as 1.1 here. the fitness value of the best individual can be obtained at each iteration step, and the objective function value of the best individual can be deduced from eq. (24) . then, the battery sop can be calculated by the following expressions:"
"in this paper, a consensus-based distributed cooperative spectrum sensing scheme (cdcss) in cr-manet is proposed which is inspired by novel biological mechanisms. these mechanisms have become an important phenomenon in handling intricate communication networks. cdcss works on mns in distributive network without using a centralized center to improve the sensing performance in cr-manets. the main contributions of this paper are as follows."
"the other limitation was the classification of the studies based on their aim and analysis type; in this case, both authors tried to resolve their disagreements by consensus. the fourth limitation is that although more demand and supply based studies in the review were carried out using google trends and twitter, there were other data sources such as online forums, wikipedia trends, youtube, search and engine queries, which were used in the studies with a lower frequency rate. we reported these web tools in table 4, but it seems that reporting and analysing the benefits and limitations for each of these data sources may cause the scattering of the results. this exceeds the scope of this discussion."
"the deepqa architecture is designed to be a large-scale hypothesis generation, evidence-gathering, and scoring architecture [cit] . figure 1 illustrates this architecture, focusing on how watson's search and candidate generation components fit into the overall qa pipeline. the hypothesis generation phase takes as input results from question analysis, summarized in the previous section. the first four primary search components in the diagram show watson's document and passage search strategies, which target unstructured knowledge resources such as encyclopedia documents and newswire articles [cit] . on the other hand, the last two search components, namely, answer lookup and prismatic search, use different types of structured resources. one or more candidate generation techniques are applied to each search result to produce plausible candidate answers. in our effort, we explored how to exploit the relationship between the title and content of title-oriented documents (such as encyclopedia articles; see below for more detail) and how to use metadata present in linked data (such as web documents) to help with effective candidate generation."
"a total of 1165 potential studies were identified for inclusion. at first, to avoid selection bias, the title and abstract of each identified study were assessed by the first reviewer (kz) blinded to authors, affiliations and the publishing journal. then, the titles and abstracts of the retrieved studies were independently reviewed by the second reviewer (ma). finally, 95 studies met the inclusion criteria for the full-text review. the 39 studies that did not meet our inclusion criteria were excluded."
"search queries for a question are generated from the results of question analysis. for all questions, a full query is generated on the basis of content terms or phrases extracted from the question, as well as any lat detected in the category. arguments of relations recognized over the focus are considered more salient query terms and are weighted higher. for instance, in example (1) above, the following full query is shown, where the arguments of the actorin relations are given empirically determined higher weights: for those questions where the lat has modifiers, as in the current example, a lat-only query is generated in addition to the full query, based on the lat and its modifiers. in this example, the full noun phrase that contains the lat is bthis depression-era grifter flick[; therefore, the lat-only query contains the terms depression era grifter flick"
the parameter identification results of the two lib cells are given in fig. 2 while the test valuations of the model prediction performances are shown in fig. 3 . the test results indicate that the established battery model can fit precisely with the test data where the maximum relative error of the model is less than 1.0% and the mean relative error is less than 0.12%.
"the rest of the paper is as follows. section 2 explains work carried out in fields of spectrum sensing and mobility models. section 3 involves the methodology adopted to develop a scheme for cooperative spectrum sensing among mns. section 4 explains implementation process, section 5 involves analysis, and section 6 contains conclusion of our work."
"r particle initialization r particle advection/evaluation/split and delete r memory defragmentation r particle insertion of context particles memory defragmentation is an integral part of our algorithm but performed only if necessary or explicitly requested. similarly, context particles are only inserted if explicitly requested."
"although the three sets of search results have a fair degree of overlap, we have empirically shown that each strategy brings a unique contribution to the aggregated search results and thus helps improve the overall recall of our search process [cit] . we discuss how we formulate search queries and how the three search strategies are performed in watson in the remainder of this section."
"implementation: an important feature of our particle system is its interactivity, which requires an efficient implementation. in each subsection we address implementation details within the last paragraph. figure 2 gives an overview of the algorithm's pipeline which each particle undergoes every frame. our current implementation uses cuda for computation and opengl for rendering. furthermore, the implementation facilitates direct interoperability between the cuda and opengl contexts. the computation is performed by separate cuda kernels for different algorithm steps. specifically, four kernels are used for:"
"in this section, we will first discuss the spectrum sensing model of cru and then illustrate the local sensing model and information sharing with neighboring crus using cdcss. network topology and consensus notions for cdcss are also presented along with the movement pattern of cru/mn using rwmm."
"the data set represents a simulation of a 3d flow around an obstacle, in our case a squared cylinder. furthermore, the left boundary plane represents the inflow and the right boundary plane the outflow. we use this data set to illustrate the variability of the system in terms of splitting parameters and criteria. we use the data set in two different configurations the original form and with subtracted mean flow. all results have been achieved by randomly inserting parent particles within the domain. furthermore, the system has been configured to constantly insert parent particles, providing context information."
"the 56 infodemiology studies that met our inclusion criteria and avoided the exclusion criteria were included in this review. to conduct these studies, the web (1.0 & 2.0) tools were used. as our main aim was doing a scoping review of infodemiology studies during the 14 years, studies with no substantial use of the web (1.0 & 2.0) tools were also included. such studies usually relate to the initial days of inception of infodemiology when their authors called them infodemiology studies. they were based on the evaluation of hospital websites, identifying health information seeking behaviour and designing the information mining system. at first, we examined all of the 56 articles included in this study and categorised them into two main groups based on the web (1.0 & 2.0) tools that were used to conduct them. these two main groups were demand based and supply based infodemiology studies. the 'demand based studies' are those that were done using web (1.0) tools, such as google trends and search engines queries. the 'supply based studies' were done using web (2.0) tools such as twitter, blogs, wikis, and online forums. the third group comprised of studies conducted using both web (2.0) and web (1.0) tools simultaneously; we called these studies 'demand + supply studies'. the last group is 'other studies' in which no web tools were used (table 3) ."
"we demonstrate our approach on four different data sets obtained by fluid simulation from technical and medical applications. the remainder of this paper is organized as follows. after related work in section 2, we give an overview of our method in section 3 followed by a detailed description of the individual building blocks. afterwards, we present several use cases and results, achieved with our method, on the different data sets. subsequently, we analyse timing and memory consumption in section 5 and finally conclude our work with a discussion in section 6, also naming possible future developments."
"from the earliest days of artificial intelligence and nlp, the prevalent vision was that machines would answer questions by first btranslating[ human language into a machine representation and then matching that representation against background knowledge using a formal reasoning process [cit] . to date, however, no one has successfully produced such formal logical representations from unseen natural-language questions in a reliable way."
"to achieve a common goal, manet nodes cooperate with each other because of the lack of centralized control in manets. in self-organization of nodes, the major responsibilities are topology organization, neighbor discovery, and topology reorganization [cit] . movement patterns of mobile nodes (mns) are defined using mobility models. it is important to keep track of node movement because nodes in manets are free to move. this also includes information about their speed, location, velocity, and acceleration change over time."
"the results of the present study showed that twitter was the most used data source in the supply based studies. most of the supply based studies used twitter and were performed in developed countries; at the top of them was usa. [cit], google trends and twitter were the most data sources used, and this is in accordance with our findings. in confirmation with our findings, some data sources like facebook or news aggregates were used at least [cit] ."
"watson's text corpora contain both title-oriented documents, such as encyclopedia documents, and non-title-oriented sources, such as newswire articles [cit] . for title-oriented sources, the document title is typically an entity or a concept, and the content of the document provides salient information about that entity or concept. this relationship between document title and content inspired us to devise special search strategies to take advantage of the relationship for more effective search. we analyzed jeopardy! question/ answer pairs and the title-oriented documents that provide answers to the questions. we observed three possible relationships between the question/answer pair and those relevant documents. in the first case, the correct answer is the title of the document that answers the question. for example, consider the jeopardy! [cit] was pardoned by ronald reagan.[ the wikipedia** article for merle haggard, the correct answer, mentions him as a country singer, his imprisonment for robbery, and his pardon by reagan and is therefore an excellent match for the question. jeopardy! questions, which often contain multiple facts about their answers, are frequently well-matched by these encyclopedia documents that cover most of the facts in the question. to exploit the strong association between the title and content for title-oriented documents, watson adopts a document-oriented search strategy to find documents that, as a whole, best match the question."
"in the current setting the method targets static data sets and cannot directly deal with unsteady flows. principally the concept is transferable to time-dependent data sets, however it would require a reformulation of the split and delete criteria as well as seeding and some of the rendering strategies to be able to handle path, streak and time line and surface like structures. given this, we consider the handling of time-dependent data sets as future work."
"one of the semantic relations prismatic records is the occurrence of isa relations. 2 this information is particularly useful for search and candidate generation because it can identify the most popular instances of a particular lat. for example, consider the question bunlike most sea animals, in the sea horse this pair of sense organs can move independently of one another[. watson's search strategies described above failed to retrieve the correct answer as a candidate since the question mentions a relatively obscure fact. prismatic search, on the other hand, focuses on the lat and its modifiers, in this example sense organ, and identifies up to 20 most popular instances of the lat with modifiers from the prismatic knowledge base. the correct answer, eyes, is the third most popular instance of sense organ and is returned by the prismatic search component. prismatic search rank and score features are associated with each result for final scoring purposes, in conjunction with additional features generated by other downstream answer scorers."
"the main findings of the paper were abstracted studies (10 articles), 67% of the supply based studies (12 papers), 17% of demand + supply studies (1 article), and 40% of the 'other studies' (2 papers) were published in the journal of medical internet research. the median of the number of citations for each article was 33.6. of course, the supply based studies with 61.7 (59%) citations per article had the maximum rate of citations of all the other groups. the demand based studies with 24 (34%) citations per article had the second closest rate of citations. the topic that was covered most in the supply and demand based studies was influenza. generally, 12 (21%) of the articles were related to 'flu' [cit] ."
"where j * d and j * c are the objective function values of the best individual from ga. to verify the effectiveness of the improvements, 200 discharge sop estimation tests with different improvements are carried out, and the statistical results of the tests are shown in fig. 6 . the probability density is based on kernel density estimation. it is seen in fig. 6 (a) that the improvements promote the rise and right shift of the wave peak, and the calculation results tend to be concentrated. this indicates that two improvements improve the convergence quality of ga, among which the elitist strategy is the main contribution. from fig. 6 (b), the introduction of heuristic crossover significantly stabilized the number of iterations at a lower level."
r an interactive framework for analysis and exploration of vector fields in real-time. few intuitive parameters provide direct control over the behaviour of the particle set during run-time.
float code is employed and the current constraints are set as the bounds of the optimization variable. the population size can be considered as an n-vector p n :
"a crucial step in achieving high qa performance is to cast a wide enough net in the hypothesis generation phase to include the correct answer in the candidate pool. in this paper, we described watson's multipronged strategies for identifying relevant content and producing candidate answers that balance high candidate recall and processing time for candidate scoring."
"in its hypothesis generation phase, watson uses the results of question analysis to formulate effective queries to identify relevant content from both structured and unstructured resources. for search against textual resources, we extended the common passage search paradigm adopted by most existing qa systems in two ways. first, to take advantage of the relationship between the title and content of title-oriented documents, we adopted a three-pronged search strategy: document search, tic passage search, and passage search. second, to increase diversity of search results, we employed two search engines in our passage search component. we have empirically found that both extensions lead to higher candidate recall. for search against structured resources, watson employs two strategies: answer lookup and prismatic search. answer lookup relies on recognition of high-frequency semantic relations involving the focus and retrieves possible instantiations of the focus from existing structured knowledge sources such as dbpedia. prismatic search focuses on isa relations mined from large corpora to produce salient instances of the lat plus modifiers."
"infodemiology studies are observational in nature and do not involve individual research participants, so the conventional tools fit for assessing their bias cannot be used [cit] . in this review, 3 kinds of biases were considered. the first bias of this review which was unavoidable was the language bias as the inclusion and exclusion criteria considered only the studies in english language in this review."
the distance to the obstacle has been used in the second row. due to the proximity of the dominant vortex in the flow also this configuration emphasizes the first vortex. these images display the impact of changing the particle displacement θ 4 during split events. decreasing this value can be used to generate sharper line like structure.
"google trends is a particularly useful tool for the monitoring of internet related activities concerning a particular topic, especially epidemics of infectious diseases over time. google trends offers several valuable insights into the people's health information seeking behaviour [cit] ."
"a key component in the ibm watson* system is hypothesis generation, which is the process of producing possible answers to a given question. these candidate answers are scored by the evidence gathering and hypothesis scoring components [cit] and are ranked by the final merging and ranking component [cit] to produce the final ranked list of answers. since the outcome of hypothesis generation represents all possible candidates that the system will consider, it is crucial that a wide net be cast at this stage. it is also important, however, that the system includes the correct answer among its candidates without overwhelming the downstream answer scorers with noise. too many wrong candidates reduce system efficiency and can potentially hamper the system's ability to identify the correct answer from the overly large pool of candidates."
"the examples of the comparison/real world datasets include canada's national statistical agency website, who -regional office for south-east asia website, ironman official website, centers for diseases control and prevention website, izsam g.caporale teramo website and the epicentro website of the higher institute of health (iss)."
"the metadata is aggregated on a per-document basis, and they become plausible candidates for search results from that document. in other words, for each search result from a document, terms or phrases in the plausible candidate set for that document that appear in the search result are generated as candidate answers. in the above sample passage, the italicized terms satisfy one of the four criteria above and represent the candidate answers that would be generated from that passage. in watson, anchor text candidate generation is applied to wikipedia document titles from document search as well as all passages retrieved from wikipedia (in lieu of wikipedia title candidate generation, which has nearly identical candidate recall but significantly lower precision [cit] )."
"observing moving particles is a powerful way to experience a flow [bkkw08] . to facilitate this experience for a large class of different flows has been the main motivation for our work. this means building a system capable of handling large sets of particles, reducing visual clutter while highlighting the flow in regions of interest. a system that automatically handles the trade-off between sparse and dense seeding. thereby the notion of an 'interesting region' is treated as a flexible concept. these goals have been implemented by the introduction of autonomous particles, which explore the flow by themselves, only communicating with the local field properties at their current position. the results given in section 4 clearly show that the set of autonomous particles fulfill these goals expressed in the targeted requirements req.1 and req.2 described in section 3."
"we conducted an experiment to evaluate the coverage of wikipedia articles on jeopardy! questions and found that the vast majority of jeopardy! answers are titles of wikipedia documents [cit] . of the roughly 5% of jeopardy! answers that are not wikipedia titles, some included multiple entities, each of which is a wikipedia title, such as red, white, and blue, whereas others were sentences or verb phrases, such as make a scarecrow or fold an american flag. the high coverage of wikipedia titles over jeopardy! answers suggests that they can serve as an excellent resource for candidate generation."
"effective answer lookup requires components that match entity names in questions to those in structured sources. more crucially, it directly depends on the quality and quantity of semantic relations that can be identified in questions, and relation detection in text, while a problem studied for some time, remains an open issue. broadly speaking, work in that area aims to develop recognition capabilities either manually or by statistical means."
"model. the basic idea behind this movement emerged from unexpected movements of particles. it was developed to follow such erratic movement patterns, and hence it became the most widely used mobility model. mn randomly chooses its new speed and direction and then moves from its current location to a new location to which it is to travel. predefined ranges are used for selecting new speed and direction. for speed, the range can be from minimum speed to maximum speed, and directions can range from 0 to 2 . either a constant time interval or a constant distance is selected for each movement in this model. at the end of each movement, a new direction and speed are selected in a similar manner for the next movement. in this model, when mn reaches simulation boundary, it bounces back from the simulation boundary and comes back at some angle. this angle is determined by incoming direction. the mn then moves along a newly selected path. rwmm is a memoryless model as no knowledge of previous speed and direction is retained, and also these parameters are not used for future decisions. the current direction and speed of mn is independent of both past and future speed and direction."
"in this study, a ga-based method of sop estimation is proposed for libs in electric vehicles concerning the influence of imprecise soc information. the presented approach can deal with the problem of big remainder error in taylor expansion under large estimation time-scale. simulation results indicate the ga-based method can improve the estimation accuracy of the sop by up to 7.2% under certain situations. a sensitivity coefficient of the sop estimation to the soc error is introduced. the impact of battery soh and estimation time-scale on the sensitivity coefficient has been analyzed. ming lv received the b.s. [cit] . he is currently pursuing the m.s. degree in automobile engineering with northeastern university, shenyang, china."
"indri supports passage retrieval through the use of the prefix #passage½x : y for a query. this prefix specifies that passages be evaluated within an x -word window, shifting y words at a time, using a scoring algorithm analogous to that for document retrieval. in watson's implementation, x is set to 20 and y to 6, to balance the quality of results and speed. watson's passage search component enhances indri's native passage search results for a qa application in two ways. first, we extend each 20-word passage at both ends to sentence boundaries so that the results can be analyzed by our natural-language processing (nlp) components [cit] . second, we augment indri's native scoring algorithm to account for coverage of query terms, i.e., rewards are given to passages that cover a large portion of query terms over those that match a small fraction at high frequency. our experiments showed that the rescoring process led to passages that are more likely to contain the answer to the question."
"watson's document search component uses the open-source indri search engine [cit] and targets title-oriented documents that, as a whole, best match the question. two separate indri indices are used, one consisting of long documents such as encyclopedia articles and the other of short documents such as dictionary entries. the two separate indices are necessary because the significant size differences between the documents caused highly relevant short documents to be drowned out by longer documents when combined in one index. the full query, constructed as previously described, is used to retrieve the top 50 most relevant long documents and the top 5 most relevant short documents. a document search rank and a search score are associated with each result, which are used as features for scoring candidate answers, in conjunction with additional features generated by other downstream answer scorers."
"to facilitate the process of answer typing [cit] in downstream processing, candidate provenance information is recorded for each candidate, if possible, in order to disambiguate the candidate string. in particular, some scorers for answer typing use structured resources, such as dbpedia, whose entries can be disambiguated via wikipedia uniform resource identifiers (uris). for example, the candidate naomi with provenance naomi (bible) suggests that the candidate is a person, whereas the same candidate with provenance naomi (band) will be typed as a musical group. for candidates generated from wikipedia documents, the distinction between multiple meanings is available in the candidate generation phase as it corresponds to the document actually retrieved in the search process. this information is recorded in metadata associated with the candidate."
"cooperative spectrum sensing is an efficient technique to improve spectrum exploitation. in this paper, consensusbased distributive cooperative spectrum scheme is proposed, which was applied to mobile nodes. the local sensing, in this scheme, was based on energy detection. this is the most widely used local sensing technique as it does not require prior knowledge of network topology and is easy to implement. for cooperative sensing, the proposed algorithm was based on a set of certain rules which includes prediction rule and average value calculation for energy values of unlicensed users. details of these rules are provided in sections 3 and 4. this technique was applied on mobile nodes because mobility of nodes results in reducing the fading effect, thus making sensing more efficient. the goal of the proposed scheme was to provide proficient cooperative spectrum sensing technique. the results showed that our technique has the potential to contribute effectively for efficient spectrum utilization. a future direction of study is to consider sensing overheads and to provide optimal spectrum heterogeneous cr-manet design."
"in the second case, the title of a document that answers the question is in the question itself. in the third case, the answer-justifying document is a third-party document whose title is neither the answer nor in the question. we expect traditional passage search strategies adopted in existing qa systems [cit] to be effective on those questions."
"the increasing complexity and size of data in computational fluid dynamics make suitable analysis tools for domain experts inevitable. thereby, particle-based techniques generally provide easyto-understand methods for flow visualization complementing static images as, for example, streamlines. however, the number of used particles and their seeding positions are crucial for the effectiveness and efficiency of the visualization. there is a general tradeoff between sparse seeding reducing clutter with a high chance of missing features and dense seeding, which increases clutter but is less likely to miss features. however, dense seeding also does not guarantee that no features are missed since particles tend to cluster in regions of low velocity or convergent behaviour, while regions with high velocity or divergent behaviour are under-sampled. by introducing a system of autonomous particles we target both aspects, decreasing the particle density in regions of less interest and increasing the sampling rate in regions of interest automatically during run-time. the resulting set of particles can be considered as a smart sampling of the flow. compared to classic particle systems, it highlights features without explicitly controlling particle density, which would corrupt the highly parallel nature of the underlying algorithms. instead, the density is controlled by particle-events like birth, death and spawning new particles based on local field information, for example, divergence or rotation and few particle properties like its energy level. the approach supports interactive data exploration without requiring pre-computation or complex field analysis. it is a compromise between efficient interactive direct visualization and a feature-based exploration. our main contributions are as follows:"
"the objective of this scoping review was to provide a descriptive overview of infodemiology studies without critically appraising individual studies, or synthesising evidence from different studies, then systematically map, and compare them based on developed, developing and in transition countries where they were performed. we identified the relevant studies by searching scopus, science direct, web of knowledge, wiley, springer, pubmed and google scholar, using a comprehensive search strategy. [cit] onwards, as this is when the term infodemiology was first coined."
"sentence offsetvsentences that appear closer to the beginning of the document are more likely to be relevant; therefore, sentence offset is used as a scoring feature. sentence lengthvlonger sentences are more likely to be relevant than shorter sentences; thus, sentence length is adopted as a feature. number of named entitiesvsentences containing more named entities are more likely to be relevant, since most jeopardy! answers are named entities. we approximate the recognition of named entities in documents through occurrences of anchor texts and document titles in each sentence."
"finally, we examined the contribution of each search and candidate generation strategy on end-to-end qa performance. the last row in the table shows the qa accuracy when the candidates generated by each approach are scored by watson's full suite of answer scorers and are ranked. our results show that document search and passage search achieve very comparable performance, about 9% the 1% of questions in the inactive subset for the document search pipeline are questions for which a numeric answer is sought. for these questions, a specialized number candidate generation component is employed."
the impact of the imprecise soc on the sop estimation is analyzed quantitatively by bringing in a sensitivity coefficient. the correlations between sensitivity coefficient and its influences are disclosed.
"as numerical integration method we use the fourth-order rungekutta scheme with a fixed step size, as this is a good trade-off between performance and accuracy. the vector field resides on the gpu as 3d-texture and is interpolated linearly."
"the query-dependent similarity score is combined with the query-independent scores described above to determine the overall search score for each passage. in order to increase recall, each top-scoring single-sentence passage is expanded to include the sentence that precedes it in the document. this expansion is helpful because co-reference targets in the current sentence can often be found in the preceding sentence."
"where s c1, s c2 and s c3 are defined as a logical value of each constraint for charging cases. the employed ga is described as follows:"
the objective of this study is to propose an effective online sop estimation approach of lithium batteries used in evs power management system and analyze the influence of imprecise soc information. three original contributions are made:
"our method empowers the user to explore an unknown data set and gain understanding without any pre-computation or data set analysis. with this we derive the following requirements. the algorithm does not perform significantly slower then a fixed-size particle system. ideally, the gpu is leveraged for computation as well as rendering due to the algorithms parallel nature."
"index update: after sorting the memory an update step to the index i c and i p is performed. instead of using a linear search pattern (o(n)) a kernel executions with n b threads (parallel complexity of o(1)) and an atomic max operation is used to update i c and i p, respectively."
"the shown results demonstrate the flexibility of our method, which performed excellent on all data sets. important flow features were highlighted easily and context information still preserved."
"for enhancing the transparency of such infodemiology studies, researchers can get a screenshot from the raw data. the corporations responsible for web tools such as google inc. can provide some precise instructions and guidelines regarding the capabilities of their tools, their changes over time and the standard methodologies for conducting infodemiology studies. some cooperation between the researchers at the universities and web tools corporations for the benchmarking of the best practices could be another useful step in this area."
"where ( ) is the received signal, ( ) is the additive white gaussian noise (awgn), ( ) is the pu signal, and ℎ is the amplitude gain of pu signal. 1 indicates that a pu is present while 0 represents absence of pu. received signal is passed through a band-pass filter of bandwidth and center frequency . signal received from filter is further fed to a squaring device and then to an integrator over a time period . output from the integrator is"
"this user generated content (ugc) or consumer created content (ucc) includes personal experiences, health information and knowledge [cit] . exploration, mining and analysis of user generated content (ugc) provide an image of the information seeking behaviour of people and tracking them over time can lead to the identification of the changes in their behaviour [cit] ."
"the third bias of this review was the publication bias as the numbers of studies that reported positive findings were more than those that reported neutral or negative findings. hence, the present study had a positive publication bias; it was in accordance with the other studies [cit] . almost 37% of the demand based surveillance studies validated the google trends output against the real world (comparison) data sets. in both the demand based and supply based studies, different search terms and search dates were used, but no rational for these selections was provided to further understand the search method and increase the face validity of the review; [cit] ."
"as part of a team of researchers, rehls worked alongside the infodemiology research team from the inception of the research process by providing not only tailored, intensive information services to infodemiology research teams within which they are integrated but also supported evidence based practice and knowledge syntheses, such as systematic reviews and scoping reviews on infodemiology studies; conducting scoping reviews on infodemiology will result to identifying potential research gaps and conducting systematic reviews will result to summing up the best available research on infodemiology methodologies. therefore, methodological standards or guidelines for the use of web tools in the infodemiology studies will be produced."
"in decentralized spectrum sensing, there is no requirement of infrastructure and fusion center. here, the crus exchange their information with each other to cooperate. the most well-known decentralized spectrum sensing technique is gossiping algorithm because it performs sensing with a significant low overhead. other decentralized techniques include clustering schemes which are already known for sensor network architectures. in these schemes, crus form clusters, which coordinate among themselves [cit] ."
"for parsing, we employ esg (english slot grammar), a comprehensive deep slot grammar parser [cit] . each node in its dependency parse tree contains 1) a headword and its associated morpho-lexical, syntactic, and semantic features and 2) a list of bchildren[ that are generally modifiers of the node, along with the slot that each modifier fills. the focus detection component identifies the question focus, which is the part of the question that refers to the answer and is often the head of the noun phrase with demonstrative determiners bthis[ or bthese[. the relation recognizer annotates certain types of unary and binary relations in the question; of particular interest here are semantic relations recognized over the focus whose predicates can be mapped to codified relationships in watson's structured knowledge resources, such as actorin and authorof. finally, the lat detection module identifies the lat of the correct answer to the question. the lat is typically a term in the focus but may also include other nouns that are co-referential with the focus in the question and/or category."
"in this scoping review of infodemiology studies, it was found that the web (1.0 & 2.0) tools are utilised widely by researchers in different topics. [cit] demonstrated that the use of search queries for disease detection could be applied to different diseases. [cit] findings, the most disease commonly evaluated using infodemiology approaches was flu. [cit], the demand based studies were generated using web 1.0 tools, whereas web 2.0 tools were used for the supply based studies."
"a sensitivity coefficient δ of soc accuracy is introduced here to reflect the impact of the soc error on the battery sop estimation, expressed by:"
"the list of developed countries that participated and the number of their papers are as follows: the united states (21), italy (13), uk (5), canada (4), austria (1), japan (1) and portugal (1). the most supply based studies belong to usa with 12 papers (44%). the most demand based studies belong to italy with nine papers (33%)."
"to avoid the risk of over-charging and over-discharging, the introduction of a voltage constraint for the sop estimation is necessary. the voltage constraint can be expressed as:"
"recent studies suggest that the information obtained from social media platforms, such as twitter and facebook, can be considered as supplements for epidemiological studies and traditional surveillance [cit] . the information generated by social media regarding health care can be used for content analysis tracking in real time, knowledge translation as well as for the awareness of health policymakers [cit] ."
"the estimation of sop has attracted a lot of attention of researchers. the existing publications for sop estimation are mainly divided into two categories, one is the characteristic map (cm)-based methods and the other is the equivalent circuit model (ecm)-based methods."
"algorithm timings: the first observation one can make is that particle advection, including split and delete operations, are the least and the defragmentation the most time-consuming part of our algorithm. timings to a non-adaptive version running under the same conditions. a direct comparison is difficult, since our proposed method provides a smarter sampling of the domain and therefore uses less particles than a traditional system."
"prismatic search prismatic is a large-scale lexicalized relation resource automatically extracted from massive amounts of text [cit] . prismatic provides shallow semantic information derived from aggregating over syntactic or semantic relation usage in a large corpus, with generalization over entity types. for example, it gathers the aggregate statistics of relations extracted from syntactic parses, such as the frequency of the string tom cruise appearing as the subject of the verb star modified by a prepositional phrase bin hmoviei[. the aggregate statistics can be used to infer selectional restrictions and other semantics."
"a gradient based distributed cooperative spectrum sensing method was proposed for crahns [cit] . the gradient field changes with the energy sensed by cru, and the gradient is calculated based on the components, which include energy sensed by crus and received from neighbors. the proposed scheme was evaluated on the basis of reliable sensing, convergence time, and energy consumption. this scheme consumes less energy compared to existing consensus-based approach."
"most existing open-domain retrieval-based qa systems adopt a pipeline of passage search against a reference corpus and generation of candidates of the expected answer type from the search results. from the search perspective, some systems have explored the use of web data for the purposes of both generating new candidate answers [cit] and validating existing candidate answers [cit] . furthermore, online encyclopedias such as grolier** and wikipedia have been used as corpora for several qa systems [cit] and in the clef (cross language evaluation forum) evaluation effort [cit] . however, to our knowledge, these systems treated the new corpus as an extension of the newswire corpus used in earlier organized qa evaluation efforts and did not exploit its inherent characteristics to improve qa performance. in contrast, we analyzed the association between title-oriented encyclopedic documents and question/answer pairs to motivate two additional search strategies for qa: document search and tic passage search. these search techniques are effective for title-oriented documents and have been shown to improve upon the results of traditional passage search alone."
"different derivates of this model are also developed by researchers. these include 1d, 2d, 3d and d-d walks [cit] . here, we here are concerned with 2d walk, which is also shown in figure 5 . in this example, mn is allowed to choose a speed between 0 and 10 m/s and a direction between 0 and 2 . in this case, fixed time interval is used; that is, mn can move in a direction for 1 sec before changing its direction and choosing a new destination. as time is made constant, the distance to be travelled is not fixed or specified. mn can move in the specified simulation boundary."
"infodemiology is a new and emerging branch of science that deals with the occurrence, distribution and analysis of electronic health information to raise awareness in people on disease patterns. one of the main characteristics of infodemiology is the collection and analysis of data in real time [cit] ."
"we tried to explain the variables for the demand based, supply based, demand + supply based studies. in the cases where the studies did not have the variables or they were not reported, such as in 'other studies', we used 'n/a', which means 'not available'. we also tried to do a subgroup analysis in addition to performing data extraction for the main groups of studies. efforts were made to capture the variables based on the aim of the study, and the analysis type was examined for every study included in this review."
where ξ is a number between z k and z k+1 . it is assumed that the change of soc is relatively small during t :
"the publication rate of the demand based studies (27 papers) in comparison with the supply based studies (18 papers) was 3-2. [cit], during which the publication rate remained stable. [cit] . the results also showed that the articles were related to 13 countries in total."
"even in the demand + supply studies, the most used data sources were google trends and twitter. the analysis types most used in the studies were the time series analysis with a frequency rate of 55% and cross-sectional analysis with 20% frequency rate. both analysis types were simultaneously used in 25% of the studies (chart 4)."
"in the ibm watson system, the hypothesis generation phase consists of two components: search and candidate generation. in its search component, watson adopts a multipronged approach to retrieve relevant content from its diverse knowledge resources. the search results may be documents or passages from unstructured sources or arguments that satisfy partially instantiated predicates from structured sources. watson's search strategies extend the passage search approach adopted by most existing question-answering (qa) systems in two ways. first, watson employs specific search strategies to exploit the relationship between titles and content in title-oriented documents (e.g., encyclopedia articles) to improve search recall. second, watson uses structured resources through queries based on syntactic and semantic relations extracted from the question."
"the vector fields are uploaded once as a four-channel texture (see section 3.2) and therefore consume four 32-bit floating-point values for each element. if an additional pre-computed scalar field is used, we store an additional one-channel 32-bit texture."
"according to the inclusion and exclusion criteria of the 56 articles included in the review, the selection process of the studies and the reasons for withdrawal of articles are shown in the prisma flow chart (fig. 1) . data extracted from the articles are shown in table 5 ."
"note that although the discussion below demonstrates how watson uses wikipedia metadata for candidate generation and the evaluation demonstrates how these strategies affect watson's performance on jeopardy!, we have previously demonstrated that the same techniques also effectively perform on questions from the trec qa track [cit] . furthermore, the techniques we developed can be easily applied to leverage metadata from other title-oriented documents (for title of document and wikipedia title candidate generation) and collections of documents with entity-oriented hyperlinks (for anchor text candidate generation)."
"to evaluate the impact of the search and candidate generation strategies described in this paper, we randomly selected 66 previously unseen jeopardy! games. we excluded from these games special questions that require a tailored process for candidate generation, such as puzzle questions [cit] and common bond questions [cit], resulting in a test set of 3,344 questions. we evaluate the coverage of each search strategy paired with its corresponding candidate answer generation components and measure performance using the candidate binary recall metric for each strategy separately as well as all strategies combined. candidate binary recall is computed as the percentage of questions for which the correct answer is generated as a candidate answer. we adopt this evaluation metric to reflect our goal of maximizing candidate recall at the hypothesis generation phase of the deepqa pipeline."
"the first qa system to use structured data sources effectively for candidate generation was the start system [cit] . similar to our answer lookup approach, a retrieval-based qa system was augmented with a capability to recognize relations in questions, and structured sources were queried with the detected relation and question focus. as reported here, the gating factor in exploiting this for qa is the ability to detect the relations in the question."
"this ranked list of relations sets priorities for the implementation of relation recognizers in questions. in general, we are looking for english expressions of these known relations between a named entity and the question focus, such as actorinðrobert redford; flick : focusþ and actorinðpaul newman; flick : focusþ. the answer lookup component first tries to find the known named entity in the database from the relation argument string (e.g., brobert redford[) using a step called entity disambiguation and matching (edm) [cit] . if a matching entity is found, a query is generated to find movies starring the given entity. as with this example, if multiple relations are found in the question, each separate relation is individually processed and its results pooled. an identical answer lookup score is assigned to each search result, except for those results reinforced by multiple search queries whose scores are boosted."
"new opportunities and challenges are emerging for health information specialists as informationists for providing evidence based and reliable health information in real time in infodemiology studies too. [cit], the delivery of high quality and appropriately targeted consumer health information is central to any achievement of health literacy. the health literacy of the population will promote understanding, decision-making and the application of knowledge and health advocacy by health information specialists [cit] )."
"currently, the astonishing growth of fossil fuel-dominated vehicles has put a serious impact on the environment and energy issues [cit] . in this situation, electric vehicles (evs), including battery electric vehicles (bevs), fuel cell electric vehicles (fcevs), hybrid electric vehicles (hevs), and plug-in hybrid electric vehicles (phevs), have gradually been recognized as the highly promising developments due to their low emission and high energy efficiencies [cit] . as the onboard power source of evs, traction battery packs play an important role in improving vehicular performance, in which the libs are most common used battery type for the high energy density, long lifespan, low self-discharge and no memory effect [cit] ."
"in the halo and aura systems [cit], closed-domain questions (on, e.g., chemistry) are answered using structured sources containing many axioms about processes and facts in these domains. aura addresses classes of questions that are out of scope for watson, having mainly to do with problem solving or procedural questions, such as bhow many ml of oxygen is required to produce 50 ml of water?[ however, the natural-language capability of these systems is constrained by question templates that the system can map to underlying structured queries."
"first of all, the battery test is conducted to investigate the battery characteristic. in this study, we focus on the battery sop estimation for the electric vehicle power management application, so therefore, the charging and discharging characteristic test is under the urban dynamometer driving schedule (udds). the first-order rc equivalent circuit model is established, as shown in fig. 1, in which the resistance block includes a charging ohmic resistance r chg and a discharging ohmic resistance r dis . the equivalent rc block is used to model the polarization effect, which contains a polarization resistance r p and a capacitor c p . the electrical behavior of the battery is described as:"
"as increases, this will cause high missed detection of pu, which will result in disturbance to pu transmission. this means that a good scheme is the one in which both of these factors are minimized. comparison of cdcss with existing egc rule is shown in figure 11 . this comparison comprises and . it is clear from figure 11 that cdcss performs better compared to existing egc rule. new scheme has lower compared to egc rule. this will result in less interference to pu, thus improving pu's transmission and causing more efficient and reliable spectrum sensing."
"the data were extracted from the studies using the instrument presented in table 3 . both authors performed data extractions, and disagreements between them were resolved by consensus. however, our primary aim for reporting these variables was not to state the reproducibility of the studies; it was to further analyse their content and provide the reader with an overview of how researchers are using web tools to conduct the studies, if possible."
"(1) employing a variable instead of constant step size: rather than using a predefined constant step size, its value is predicted by following rule [cit] :"
"the results of our experiments are shown in table 1 . for each search and candidate generation method, we computed the number of questions for which at least one result was returned, henceforth referred to as the active subset. for each approach, we also computed the average number of candidates generated per question in the active subset and the candidate binary recall computed over all questions."
"according to the t-method, battery is considered to provide the maximum power when the maximum allowed current is output. thus, the sop can be estimated by:"
"the deferred shading pipeline uses post processing for the calculation of screen space ambient occlusion (ssao) and physically based shading (pbs). whereas, the forward rendering mode is configured to disable depth testing and used for density projection. in general more rendering styles, like transparent particles, can be supported by our prototype. for this a depth-based sorting on fragment level for correct blending calculation is necessary. however, a discussion on particle rendering techniques is out of the scope of this paper."
"till now, most of sop estimation researches, in which the long time-scale of the estimation has not been well emphasized, are not specially targeted to the vehicle power management applications. in addition, the sop estimation algorithms normally rely on the precise soc information, however, precisely estimating the battery soc in evs applications is still quite difficult. some publications focus on the joint estimating of soc and sop [cit], but there is a lack of a quantitative study on the impact of soc estimating error on the sop estimation."
"where z is the battery soc, and 5 and e 1 ∼ e 5 are coefficients that need to be identified. the battery soc is updated by the ampere counting approach based on the current data and sampling time interval"
"delta wing: this simulation data set describes the flow around a simplified aircraft geometry often referred to as 'delta wing'. for this type of aircraft geometry the uplift is generated by vortices forming at both sides of the wing. for this data set particle seeding has been constrained to a binary importance volume. again, our system has been configured to constantly insert particles within this volume. these few parent particles act as a source for continuous splitting near the wings tip. primary and secondary vortex structures are found and automatically highlighted by our method. with the help of little user interaction our method revealed all vortices clearly. besides that the inner structures of the two big vortices were observable (figure 7c ) by utilizing density projection."
"in this section, an improved ga is employed for predicting the sop of the battery. the sop estimation is treated as a optimization problem to find the maximum of battery power within a certain time-scale under several constraints. the maximum battery power within a time-scale t after time k is expressed as:"
"all studies included in this review are original articles (100%). the results of this research indicate that the first studies, which were called infodemiology, were about the quality assessment of hospitals' websites [cit] . the other contexts that stand in the form of infodemiology studies include describing and assessing the internet resources of patients in a specific field, designing the information mining system on specific topics [cit] and studying the information seeking behaviour."
"although [cit], respectively. the demand based studies were the most published; however, the supply based studies were cited more often in comparison with the other three groups. a subanalysis of the studies based on some defined variables showed that the median citation rate (34 per article) is more than the average for all the scientific articles (7.64 per article) [cit] and other infodemiology studies [cit] ."
(1) the taylor method for sop estimation is discussed and the impact of its remainder error is given under the varying estimation time-scale. (2) the improved ga is employed for online estimating the battery sop under long estimation time-scale in vehicle power management application.
"lower than full system performance. we ran an additional experiment in which only the full search query is issued to indri passage search to retrieve the ten most relevant passages. this is the search configuration closest to the search strategies adopted in many existing qa systems. when these candidates are scored and ranked, the system achieves an accuracy of 54.9%. these experimental results demonstrate the effectiveness of our search and candidate generation strategies, which, in aggregation, achieve an accuracy of 71.32%."
"advancement in wireless communication requires efficient utilization of limited spectrum resources. recent research shows that this limitation is because of spectrum management policies [cit] . to overcome this limitation and for better utilization of spectrum, one requires a new networking standard, which is known as cognitive radio network (crn) [cit] . software defined radio (sdr) is used to build cognitive radio (cr). cr is a smart system that senses the environment called spectrum sensing and adapts to variations in operating parameters. the two prime objectives of spectrum sensing are the efficient detection of spectrum holes and interference avoidance with primary system/licensed system [cit] . current research divides spectrum sensing into two branches that are local sensing and cooperative spectrum sensing. in cooperative spectrum sensing, each cognitive radio user (cru) shares its local observation with the rest of crus in the network, which results in improved spectrum sensing."
"for candidate provenance, we take advantage of the linked nature of these candidate answers to accurately identify the senses for candidates that can potentially be ambiguous. for candidate answers extracted from anchor text-based resources (1 and 2 in the list above), the target document is used to identify the sense of the candidate. for those extracted from document title or redirects [ (3) and (4) in the list above], the current document is used to disambiguate among multiple senses."
"the present study is the first attempt to systematically map the published literature on infodemiology studies. therefore, a scoping review seemed the most appropriate research design [cit] . [cit] onwards, as no such study has yet been carried out. categorisation of infodemiology studies, based on developed, developing and in transition countries, is another concern of this review."
"on the other hand, professional library associations representing health librarians need to hold up data mining courses for those interested graduates and develop education policies that include specific competencies for conducting infodemiology studies. these associations should consider collaborating internationally to formulate education policies and standards tailored specifically to conduct infodemiology studies by health information specialists [cit] ."
"our method is also related to adaptive refinement methods which have been used in many contexts. thereby, refinement is mostly applied based on some local error measure related to the current sampling. this is especially useful, when high accuracy is required as, for example, for the computation of ftle ridges [bt13] . in contrast, our method is not targeted to ensure accuracy but to use a given contingent of particles in an efficient way without overhead."
"simulation result in figure 9 shows that now less iterations are taken to achieve consensus compared to previous results. the difference between energy values is less than 1 db after 10 iterations and is less than 0.1 db after 15 iterations. here also, the converged value is equal to the average of initial energy values of mns, that is, 19.6 db."
"from the candidate generation perspective, the vast majority of existing qa systems adopt a semantic-type-based approach to produce candidates that match the expected answer type on the basis of a static predefined type ontology [cit] . in contrast, watson does not rely on such an ontology but utilizes document metadata, such as document title, anchor texts, and titles of hyperlink target documents, to associate salient concepts with respect to each document and thus to create a pool of plausible candidate answers. although this approach generates a substantially larger set of candidate answers, we have found it to significantly outperform type-based methods in a broad-domain task such as jeopardy! [cit] ."
"(2) applying further cdcss scheme on mns which are travelling considering rwmm: issues faced in case of fixed graph do not occur here as nodes are mobile and motion of nodes causes small-scale fading [cit] . as nodes are mobile, nodes with a distance greater than a certain limit are not considered as neighbors, which changes the degree of node (δ) and makes step size ( ) a variable factor instead of constant value."
"conducting infodemiology studies using the other web tools, trying to run more studies using the web 2.0 tools in developing countries, conducting practical workshops on data mining, text mining and infodemiology are suggestions for the future."
"to avoid exponential increase of particles in regions where the split criterion is fulfilled, the split sensitivity also depends on the particle generation. therefore, a flow feature has to be more pronounced to cause splitting of higher generation particles. furthermore, no splitting operation is performed if the budget n b is exhausted or the maximum allowed generation is reached (determined by θ 3, see section 3.7)."
"sphere impostors: to ensure low memory usage and high rendering performance each particle is represented as a position in 3d space and a 4d property vector (see section 3.2). during rendering a screen-aligned quad is positioned at each particles location. this is realized by using instanced drawing instead of a relatively slow geometry shader. each quad is coloured by a user-selected attribute, where the user can choose from."
"the t-method is widely used and has been well evaluated as effective in battery state estimation, but it has some drawbacks. the taylor expansion produces a remainder error; for long estimation time-scale, the remainder error may become large and cannot be neglected. to illustrate this issue, the sop estimation is conducted based on t-method under udds driving cycles, which last about 2.5 hours to allow the battery charge depletes from 100% to 10%. fig. 4 shows the calculated remainder error in taylor expansion of the sop estimation for lib with different time-scales. obviously, at each time point, the taylor remainder error becomes larger with the increase of estimation time-scale in both charging and discharging situation. it can also be noticed that the situation gets worse at terminal phase of the driving cycle. this is due to the high nonlinearity of ocv at low soc region. normally, there are two solutions to reduce the remainder error, one is using high order taylor expansion, second is setting short time-scale. however, too short time-scale is not suitable for online update in real-time control. [cit] 2 volume 6, 2018 certain period of time is usually crucial for the controller to determine the power allocation in the future."
where represents number of nodes and is the neighbor with which shares information. this process continues until a converged value is achieved by all mns. this converged value is represented by * . this converged value is then compared against a predefined threshold and then cooperative decision is made by all mns about the presence or absence of pu. figure 3 shows the flow diagram of the proposed consensus algorithm applied on mns:
"the resulting configuration after sort and index update steps is illustrated in figure 4 set of autonomous particles is configurable to never sort, adaptive sort and always sort. furthermore, the before-mentioned insertion of context particles (see section 3.3) requires a configuration of always sort."
"the higher fitness value has the higher probability to remain in the next generation. this can be achieved by the roulette strategy, in which the area of each segment is proportional to the fitness value of the individual. then, the algorithm selects one of the sections randomly. however, this process probably destroys the individual genes with the highest fitness, and influences the convergence quality of the algorithm. to address this problem, the elitist strategy is used to exclude some of the great individuals (elites) from the roulette game and copy them directly to the next generation to ensure that every elite in each generation can survive in priority."
"the motivation for the lat-only query is that in some cases, the lat and its modifiers uniquely identify the answer or narrow the candidate answer space to a small number of possibilities. this example falls into the latter case. some lat-only queries that identify a unique entity include capital ontario and first 20th century us president. whereas some lat-only queries, such as french composer and 20th century playwright, are too imprecise to be useful, we have found that, overall, these lat-only queries are helpful in increasing system performance."
"where m is the total sample number of the experimental data,θ is the function coefficient vector representing the estimated parameters, andû t,k is the terminal voltage estimation."
"to retrieve relevant passages from its unstructured knowledge resources, watson extends common existing approaches [cit] along two dimensions. first, we adopt the tic passage search strategy to target search against a small subset of highly relevant documents, i.e., those whose titles appear in the question. second, we leverage multiple search engines in our implementation of traditional passage search to increase diversity and hence recall of the results. to this end, we adopt indri's existing passage search capability and extend lucene [cit] to support passage retrieval. indri and lucene passage retrieval differ in two major aspects. the first concerns the retrieval model used, where indri uses a combination of language modeling and inference network for ranking [cit], and lucene leverages tf (term frequency) and idf (inverse document frequency) for scoring relevance [cit] . the second key difference is in the implementation of passage retrieval, which we discuss in the section on lucene passage search below."
information and communication technologies have affected every aspect of the modern society enabling people to share and exchange knowledge. interaction through social media and online communications helps people to make intelligent decisions [cit] .
"twitter also has another limitation: it is almost us centric. over 67 million twitter users are in the united states [cit] . hence, it poses limitations on the mining of health information from other countries. the ban of the use of twitter in some developing countries could be another limitation. this issue deprived the affected developing countries from the benefits provided by infodemiology studies."
"regardless of the strategy or search engine used, watson's passage search components return one to two sentence passages scored to be most relevant to a given question. watson retrieves ten passages each from tic passage search and passage search. for the latter, five passages come from indri and five from lucene. our empirical results show that an aggregation of five passages from each search engine achieves higher recall than retrieving ten passages using either search engine alone. the passage rank is used as a feature for scoring candidate answers extracted from that passage. the passage score feature is not used because the search scores returned by the two search engines are not comparable."
"the parameters in the model vary with soc and temperature. in this study, the impact of temperature is neglected, thus, the parameters are treated as functions of soc, expressed by"
"the implementation of sop estimation relies on the battery soc information; however, precisely estimating soc error remains a very difficult task at current [cit] . in this section, the influence of imprecise soc information on sop estimation is discussed. we will temporarily not consider the soc constraint but will instead focus on the influence of imprecise soc information on the presented sop estimation algorithm with voltage and current constraints."
"the journal of medical internet research was the most common journal publishing infodemiology studies, in total 25 (45%) of the 56 studies. meanwhile, 37% of the demand based cross-sectional analysis using ugc for comparisons across different locations at a single time period or both 8"
"the high citation rate of a few studies included in the review could be the reason [cit] . almost half the studies were demand based, and the data source most used was google trends. most of the demand based studies were conducted in developed countries using google trends; the highest numbers were conducted in italy."
"in cooperative spectrum sensing, the crus either forward their sensing information to the central entity or they can exchange sensing information with each other for cooperative decisions. in infrastructure mode shown in figure 1 (a), there is central entity to fuse the sensing results; however, there is no central entity in ad hoc mode also known as cognitive radio ad hoc network (crahn) as shown in figure 1 (b). distinguishing features of crahn's are lack of central entity, distributed multihop architecture, dynamic network topology, and time/location-varying spectrum availability [cit] . advancement introduced to the ad hoc network involves mobility and the network is known as mobile ad-hoc network (manet). in manets, devices can move randomly in all directions, and links with other devices are also updated repeatedly. without utilizing a fixed infrastructure, a dynamic network is established by wireless nodes."
"where u tmin and u tmax represent the lower bound and upper bound of the voltage operational design limits, respectively, and u p,k+t can be obtained by:"
"therefore, the first-order residual can be generally omitted. the changes of r dis and r chg during t are ignored in t-method. from the eq. (9) and eq.(11), the maximum current can be expressed by:"
1. anchor texts in the document. 2. document titles of hyperlink targets (which are often synonyms of terms in 1). 3. title of the current document. 4. titles of redirect pages to the current document (which are often synonyms of 3).
"in which the timescale t is the time span that the maximum power could last, and i l,k+t is the battery load current from time k to k + t . the charging and discharging power need to be promptly restricted when the soc approaches the operational design limits. assuming that the battery current is constant in the next time interval, the calculation should satisfy the following constraints:"
"early qa systems took the approach of translating natural language into formal machine language, such as first-order logic or bayesian logic. they then either looked up the answer from a structured knowledge base or performed reasoning on known facts to derive the answer. most of watson's qa capability does not depend on this approach, because the problem of translating natural language into a machine-understandable form has proven too difficult to do reliably. there are cases, however, where parts of a question can be translated into a machine-understandable form: for instance, when the question asks for a relation between the answer and a named entity in the question, such as the actorin relations shown in example (1). watson turns that part of the question into a query against structured sources such as dbpedia [cit] and the internet movie database (imdb) [cit] in an attempt to instantiate the variable in each query (bflick[ in the example). we call this capability answer lookup."
"our visualization method is based on a system of autonomous particles adapting to the flow addressing both previously mentioned challenges, that is, the reduction of visual clutter and increase of sampling rate in regions of interest. the main idea is that instead of a fixed number of particles a flexible budget of autonomous particles is used, which is divided into two categories context particles 1(a) and feature particles 1(c). furthermore, particles have the ability to appear, vanish or split by following simple rules for birth, death and multiplication, this results in an intelligent sampling of the flow and an efficient use of the budget. with this, flow features are highlighted but context information is still preserved. the adaptation requires only local field evaluations and few particle properties keeping track of their history. thereby, each particle explores the flow independently without communicating to any other particle. in the following these rules will be explained in more detail."
our extension to lucene for passage search consists of evaluating each single-sentence passage from top-scoring documents separately using a set of query-independent features and a relevance measure between the sentence and the query. we identified three query-independent features that affect the a priori probability of a sentence's relevance to a jeopardy! question as follows.
"recall that document search identifies title-oriented documents that, as a whole, best match the facts presented in the question. for these search results, the entity that constitutes the title of a matched document fits the description of the question and is thus proposed as a candidate answer."
"although some contemporary studies address both the demand based and supply based approaches [cit] . [cit] . the findings of the study indicates that out of the 56 papers included in the review, 27 articles (48.2%) were demand based studies, 18 (32.1%) were supply based studies, 6 (10.8%) were demand + supply studies, and the remaining 5 (8.9%) papers belonged to 'other studies' (chart 1)."
where η k is the coulomb efficiency at the (k + 1)th sampling interval. the optimal parameters are obtained by the identification algorithm in minimizing the objective function:
"internet researchers and developers have emphasised on the development of health 2.0 or medicine 2.0 (using web 2.0 technologies for health or medicine). in this era of health 2.0, patients share their health care experiences with other patients through web technologies [cit] ."
"although our aim was not to investigate the reproducibility of the studies, but it seems that the search methodologies were not documented completely by the researchers. having no methodological standards or guidelines for the use of web tools in infodemiology studies and for a proper reporting of their use may be the reason for the incomplete documentation. it seems that research-embedded health librarians (rehls) can best express the role of health information professionals in this regard [cit] ."
"where n is the total of the artificial certain soc errors (including zero error), such as the above eight imprecise soc values and one true soc value. these values are then sequenced according to the magnitude of the error value, p j represents the j-th wrong sop value caused by the j-th imprecise soc value z j ."
"the battery management system (bms) of libs is in charge of providing estimations of the battery states, mainly including the soc, soh and sop [cit] . however, precisely estimating the battery states remains a technical challenge so far because of the complex nonlinear characteristics of libs and their complicated application circumstance in vehicles [cit] . sop is the common used indicator of the maximum charging and discharging power capabilities of lib, which is crucial for both the battery management and vehicle supervisory control system."
"the result shows that the δ decline significantly when battery is aging. from the comparison of the mean value, the average δ decreased by more than 50% due to the battery aging. this is mainly because the battery internal resistance increases and the battery power capability declines during the aging process. when the battery gets aging, the tendency of δ to change become more subdued, and its standard deviation (sd) decreases to a very low level. this indicates that the δ will become more insensitive to the soc level after aging. therefore, the influence of the soc error on the sop estimation is quite minor for the aging battery in comparison of that for healthy battery."
"the rapid advancement in wireless applications has resulted in increased usage of unlicensed band, causing a problem of nonuniform spectrum usage [cit] . the cr cycle is divided into four broad fields of research to cope with spectrum utilization challenges: (1) the spectrum sensing that determines which portion of the spectrum is available, (2) the spectrum decision that picks the best vacant channel, (3) the spectrum sharing that allows user's coordinated access to channel, and (4) the spectrum mobility that allows vacating the channel when a pu is detected."
"the movement patterns of mn are defined using mobility models. it is important to keep track of nodes movement as nodes in manets are free to move. this also includes information about their speed, location, velocity, and acceleration change over time. bai and helmy [cit] provided a detailed classification of mobility models according to the mobility characteristics, which is also shown in figure 2 . some models keep track of their history movements and are referred to as models with temporal dependency, while the others are restricted by geographic bounds. there is also a class of mobility models, known as models with spatial dependency, in which nodes move in a correlated manner. most of these mobility models are random models."
"in determining the sizes of the search hit lists, we attempted to strike a balance between increased candidate recall, processing time for all candidates, and the effect of the incorrect candidates on watson's ability to rank the correct answer in top position. we empirically determined the hit list sizes reported below on the basis of experiments that measure hit list sizes against candidate recall and end-to-end system performance."
"according to international trends of health science librarianship in some developed, developing and countries in transition [cit], formal education courses in health information science do not focus on data mining; therefore, fundamental revision of the curricula seems to be required to include the baseline competencies for conducting infodemiology studies."
"in addition to searching over unstructured resources, watson also attempts to identify relevant content from structured syntactic and semantic resources. watson adopts two primary approaches toward searching against two different types of structured resources, as shown in figure 1 . the first approach, answer lookup, targets existing knowledge sources encoding semantic relations, such as dbpedia [cit] . the second approach, prismatic search, uses a custom-built prismatic knowledge base [cit], which encodes syntactic and shallow semantic relations derived from a large collection of text documents."
"implementation: seeding the initial amount of context particles is realized with a kernel launch. it is also responsible for determining the initial position. typically the amount of these particles is very low and in the order of less than 1% of the particle budget, for all experimented data sets. however, our method does not restrict the number of initial parent particles in any sense."
"in the search strategies described below, the full query is applied to both document search and passage search. the lat-only queries are applied to passage search only because these queries tend to be short and are less effective for matching against full documents. the different queries and search strategies produce results that are aggregated for further processing."
"results of streamline selection and placement methods can, depending on the criteria, deliver similar results but are in general targeting different goals. our method can certainly not replace such approaches since the quality of a particle path is never evaluated as a whole; it is also not possible to consider view dependence with autonomous particles. on the other hand, our method avoids the expensive pre-computation of streamlines which will never be rendered, it can directly be applied to an unknown data set using a fixed budget of particles in an efficient way to render the data at hand. methods, which rely on comparison of streamlines and clustering directly counteract with the idea of autonomous particles that do not know anything of each other."
"the candidate generation component identifies potential answers to a question from the retrieved unstructured content. most existing qa systems adopt a type-based approach to candidate generation with respect to a predefined type ontology. however, because of the broad range of lexical answer types (lats) [cit] observed in the jeopardy!** domain, watson relies on a type-independent approach to its primary candidate generation strategiesvthose of producing candidate answers from unstructured content. it uses the knowledge inherent in human-generated text and associated metadata, as well as syntactic and lexical cues in the search results, to identify salient concepts from text and hypothesize them as candidate answers."
the process is schematically visualized for two particles in figure 3 . these are advected with the vector field (figure 3b ). in figure 3 (c) one of the particles (represented in gray) dies due to a low energy level. in figure 3 (d) the splitting criteria for the second particle is fulfilled and the particle spawns n split child particles. for each child a random vector is added to the parent particles location. the parent particle itself dies (figure 3e ). the newly spawned particles are added to the particle system and are treated according to the same rules as the parent particles.
"flow in an aneurysm: cfd challenge case 1 and 2: [cit] . in both data sets, complex flow structures are formed within the aneurysm sack."
"once relevant content is identified from watson's knowledge sources, candidate answers are extracted from the content. for content retrieved from structured resources, the retrieved results (which are the uninstantiated arguments in the query) are the candidate answers. for unstructured results from document search and passage search, additional processing is required to identify plausible candidate answers from those search results. as a baseline, we adopted a named entity recognizer from our trec (text retrieval conference) qa system [cit] and produced as candidate answers all entities it recognizes as an instance of any of the more than 200 types in its type system. this candidate set is a superset of what would be produced as candidates using a type-based candidate generation approach [cit] using the same type system. even with the superset, we found that the type-based approach does not produce high enough candidate recall on jeopardy! questions [cit] . this section describes three general-purpose candidate generation techniques applied to unstructured search results, which improve upon our baseline: title of document candidate generation, applied to document search results only; wikipedia title candidate generation, relevant for passage search results only; and anchor text candidate generation, which is appropriate for both types of search results. these three candidate generation strategies are not answer type dependent, apply to the vast majority of questions, and generate most of watson's candidate answers. the other candidate generation strategies, which we do not discuss in this paper, apply to a small number of questions or produce a small number of candidates only. some of these strategies may be dependent on question type (e.g., questions seeking verb phrase answers or numeric answers), and others may rely on typographic cues."
"the wikipedia title candidate generation strategy extracts from a retrieved passage all noun phrases that are wikipedia document titles and are not subsumed by other titles. these indicate topics in the passage that are sufficiently salient to warrant their own wikipedia page and, we hypothesize, are worth considering as candidate answers. to identify wikipedia titles in passages, we use the same wikipedia title dictionary used to identify document titles in questions for tic passage search. the target document titles [e.g., naomi (bible)] are used as provenance information for each candidate to help with disambiguation in downstream scoring. we observed that wikipedia contains several types of metadata that, in aggregation, make up a set of plausible candidates that represent salient concepts for each document [cit] . they include the following:"
"iel and its domains e-learning initially appealed to corporate training managers primarily for its ability to reduce the costs associated with travel and instructor-led training and improve the scalability of training programmes. today's interactive, collaborative and social web shifts the practice of delivering or pushing training to learners -often in the form of traditional self-based e-learning. instead, learners pull the information that they need any time they need it. over the years, a variety of different terms have been used as the label of the field, and a variety of different definitions of the id field have been put forth. many articles or books in the idt field focus on the skills required by instructional designers and technologists (i̇pek, i̇zciler & [cit] ) . however, we believe that professionals in the field should be able to do more than just perform the skills associated with it. they should also be able to clearly describe the nature of the field, be familiar with the field's history and its current status, and be able to describe the trends and issues that have affected it and those that are likely to affect it in the future [cit] ."
"other digital technology, including the internet, have led rapidly to an increasing interest in the use of these media for instructional purposes, particularly for training in business and industry (i̇pek, i̇zciler & [cit] ) . [cit] s [cit] s, computer-based instruction began to be used and many of the questions raised then have a familiar ring to those raised today, such as learner control, questioning techniques and screen design. 24% [cit] was presented via technology, as compared to less than 10% [cit] . during the same period, the percentage of education hours delivered by instructors in classrooms decreased from 80% [cit] to 68% [cit] ). e-learning has become to be widely used in a relatively short period of time. [cit], e-learning is: \"...a wide set of applications and processes, such as web-based learning, virtual classrooms, and digital collaboration. it includes the delivery of content via [the] internet, intranet/extranet (lan/wan), audio and video type, satellite broadcast, interactive tv, cd-rom and more\" (p. 7)."
"where w ji is the weight between the neuron i in the input layer and the neuron j in the hidden layer. meanwhile, the weight β between the neurons in the hidden layer and the neurons in the output layer can be expressed as:"
"the cc values of hnn models in the study of balas, koç and tür [cit] are slightly higher than the cc values of the m1 and m2 models in the current paper, while the training data is pre-processed by using the principal component analysis (pca), and the original data sets are 554 sets of experimental data. the pca could remove the noisy data from the training data and extract the required information [cit], so the use of pca enhances the prediction ability of the machine learning models. it could also be expected that a pca-elm model will get a better prediction performance."
"where is the weight between the neuron i in the input layer and the neuron j in the hidden layer. meanwhile, the weight between the neurons in the hidden layer and the neurons in the output layer can be expressed as: the weight w between the neurons in the input layer and the neurons in the hidden layer can be expressed as:"
"as a result, teaching e-learning remains a novel activity for most teachers, although thousands have mastered the art over the past decade. e-learning course design should involve idt strategies as integrated id models for designing iel courseware. iel should be more accessible, more flexible, more productive and more attractive for lifelong learners. the second point is that iel environments have to be considered from a wider, organizational, systemic perspective. finally, it is necessary to adopt a more balanced idt approach towards iel on several dimensions at the same time. iel design strategies provide certain sustainable contributions with lifelong learners and teachers. eventually, the next generation of e-learning and the teaching employing it will require extra performance and practice in terms of training, support and assessment, as well as integrated e-learning in schools. today, learning and development organizations are increasingly integrating the use of mobile, video and virtual classrooms."
"more details about the elm theory can be found in the studies of huang, zhu and siew [cit], huang, huang, song and you [cit] ."
"the term 'complex learning' indicates a view that another type of skill, sometimes referred to as 'new skills for the new age', is needed for the future (van merriénboer, [cit] ). the main concepts behind complex skills are based on the coordination of constituent skills, the integration of skills, knowledge and attitudes of professional competences, and different types of learning competencies. coordination focuses on the ability to solve problems for new situations. integration indicates that teaching should not be directed at discrete skills, knowledge elements or attitudes. there are certain goals and competencies involved in integrated learning. some competencies are insufficient, according to the differences in learning situations; therefore, problem-solving, critical thinking and metacognitive skills such as learning to learn, self-regulation and the evaluation of the procedures in instruction, are becoming more important for teaching higher education. new instructional approaches and materials are developed and discussed to support complex learning processes. there are several id models and strategies. these strategies use problem-based learning, case-based learning, project-based learning, and so forth. these methods present the use of rich, meaningful, realistic learning tasks for learning [cit] . 'flexible learning' is a term that points out the need for flexibility of time and place for students. there is no fixed time or place for the learner or the learning environment. this approach is also defined as 'just-in time learning'. both complex learning and flexible learning meet in the field of dual learning."
"today, there is a need for new approaches for solving individual problems in the field of ict and a clear definition and organization of iel systems. in the instructional process, e-learning tasks such as complex cognitive skills, flexible concept learning skills and dual learning skills, have a relationship with iel. these relations are important in providing changes in workplaces. the links among them provide valuable instructional strategies with future designers and students, such as using learning management systems in instructional environments and organizations."
"complex learning and flexible learning meet in the field of dual learning. on the other hand, dual learning stresses the importance of realistic learning tasks. learning provides the coordination of skills, knowledge and attitudes, as well as the development of social and career skills. at present, the gap between instruction and practice can be closed by high level flexibility. iel is another important process for higher education. the term 'e-learning' is relatively new. it indicates technology-enhanced learning via the web and internet-based learning through the intranet, cds, dvds and so forth."
where β jm is the weight between the neuron j in the hidden layer and the neuron m in the output layer. the bias b in the hidden layer is:
"many models of isd processes have been developed. although there are many isd models, a generic model can be extracted from their common features. the choice of model for an instruction changes according to the learning environment, the subject matter and the strategies to be applied, etc. isd models can be used with different learning and teaching approaches and theories if there is no strict rejection of them. for example, a 4c/id model can be used while supported by a constructivist theory for teaching complex technical skills. whatever model a designer chooses, most of them will follow the processes of analysis, design, development, implementation and evaluation. these steps are also used for designing e-learning courses. the e-courseware covers more than one course lesson in its capacity. a design model for an e-learning course is characterized as the \"vpoddda\" e-learning planning cycle, which initiates vision, profile, objective, design, development, delivery and assessment phases for the e-learning planning process (i̇pek, i̇zciler & [cit] . the process can be integrated with the idt models' strategies for designing e-courseware or its modules (see in figure 3 )."
"the basic e-learning design concepts are pedagogy, technology and organization. three terms, namely pedagogy, andragogy and e-learning pedagogy, are often used in the discussion of learning. their definitions are the art of or professional teaching, helping adult learning and pedagogical strategies for an e-learning environment, respectively [cit] . iel indicates pedagogical, organizational and technical levels for the meaningful implementation of elearning in combination with traditional methods. iel tries to combine elements in the instructional process from face-to-face teaching, distance learning, web-based learning and internet-based learning. from this perspective, iel is understood as a media mix. this is because, at the same time, different e-learning tools can be used in the instructional process. the situation requires a systematic design procedure and an id model approach to provide the sustainability, security and quality of the user interfaces, as well as to bridge the gap between disciplines (i̇pek, i̇zciler & [cit] ) . for the iel setting, a single media is not enough to reach the aspired to goals, and new technologies are necessary to give flexibility and richness to the instruction. every instructional designer and developer should use different types of technologies in their design processes, including both old and new technologies, materials and tools. the components of elearning's implementation from competency models as an input to benefit or profit as an output may be followed. the procedures are shown in figure 1 [cit] ."
a comparison of these results reveals that the elm model with the maximum cc and i a was the model built with 45 hidden neurons and the sin function. the parameters for the model training and application were determined for these conditions.
"id is a systematic process that is employed to develop education and training programmes in a consistent and reliable fashion. it is a complex process that is creative, active and iterative [cit] . in addition, it is a process for solving instructional problems by the systematic analysis of the conditions of learning. to do this, one makes decisions relating to each step in the instructional systems design (isd) process [cit] . however, there are many definitions of the field -recently, instructional design and instructional technology have been labelled idt [cit] . this categorization contains instructional technology as instructional media and human performance with instructional design practices. isd is based on the premise that learning should not occur in a haphazard manner and should instead be developed in accordance with orderly processes and have outcomes that can be measured. basically, isd determines what is to be learned, plans an intervention that will allow learning to occur, measures learning to determine if the objectives are met, and repeats the intervention until the objectives are met. isd procedures and their application have evolved through practice as well as the research into and expansion of the theory."
"step 4. developing and selecting delivery systems delivery systems for instructional materials are a means for carrying information from source to a receiver, or vice versa, for the purpose of instruction. the term can be used to describe older technologies, such as traditional media, or newer technologies. selection should be based on the isd model developed for delivery system selection. the learners' characteristics, objectives, resources and constraints must be identified before the delivery system is selected."
"the goal of the learning process is to find the relation between input training data sets and output training labels. considering an elm neural network with n neurons in the input layer, l neurons in the hidden layer, and m neurons in the output layer, the general structure of elm is shown in figure 1 : the weight between the neurons in the input layer and the neurons in the hidden layer can be expressed as:"
"the extreme learning machine is a robust machine learning algorithm based on the single-hidden layer feedforward network (slfn) [cit], which was very simple in t neural network architecture. previous studies have shown that the elm could be used in wide areas, such as classification [cit] and regression [cit], and that it showed a good generalization performance at fast learning speeds [cit] . the main advantage of elm is that the user-defined parameters for training an assessment elm model only include the kind of activation function and the number of hidden neurons, which makes the model establishment very convenient; besides, the elm model can obtain a high prediction accuracy based on a small size of training data sets. based on this, the elm method was proposed to develop a novel and simple stability assessment model of rubble mound breakwaters. this is the first study on the application of elm in the stability assessment of rubble mound breakwaters. therefore, the findings in this study make a contribution to the stability assessment and to the design of the rubble mound breakwaters. this paper is organized as follows. the fundamentals of the elm approach and the model establishment are introduced in section 2. in section 3, the application of the elm approach for the stability assessment of rubble-mound breakwaters is discussed, and a comparison between the elm approach and other approaches is given. the main findings of this paper are summarized in section 4."
"where h s is the significant wave height, d n is the nominal median diameter of the stones used in the breakwater, and ∆ is the relative mass density, which can be expressed as follows:"
"the goal of the learning process is to find the relation between input training data sets and output training labels. considering an elm neural network with n neurons in the input layer, l neurons in the hidden layer, and m neurons in the output layer, the general structure of elm is shown in figure 1"
"where x i are the measured values, and their average is x; y i are the predicted values, and their average is y; and n is the number of observations. a search of the literatures shows that the cc parameter was the most widely used index to evaluate the model performances, meanwhile, when the cc value of the model reaches the highest value, the ia value may not, so in the training process, the value of cc was selected as the only evaluation index of the models. after the training process, the weights of the model with the highest cc value are recorded, which can be used for the stability assessment. the data used for establishing the models is provided in the supplementary materials . the weights used in the m1 model and the m2 model are listed in appendix a. the code used in this paper is provided in appendix b."
"step 5. assessment and evaluation assessment is a process for gathering information and data and returning results rather than a process providing opportunities for learning. cognitive tests and performance tests can be used to clarify learners' performances during the courses. in order to develop assessment plans that include testing instruments, we need to understand the concepts of criterion-referenced testing, reliability and validity. there is also performance assessment, which is considered to be weak in e-learning systems. e-learning designers have relied on tools that are directed towards and which address the construction of test items. authors use these newly available resources to produce fast and cheap elearning products. when these products are facilitated into courses, facilitators can insert themselves to fill the content voids, correct inaccurate information or address functionality issues [cit] . successful evaluation turns out to be difficult to develop. during the design process for any e-course, the developer should create a guide and do the following:"
"the web and other interactive technologies have necessitated a rethinking of how solutions are designed. much discussion has been generated as to whether objectivism or constructivism is the best approach to designing e-learning. the constructivist approach is a discovery learning method that john dewey first wrote about. he described how all learning must be approached from the standpoint of trial and error. in other words, learning by doing -and, yes -making mistakes. it was probably the result of you doing something and making mistake. there is no best approach -it depends upon the situation, the content and the learner. we also learn from our mistakes while using cbi and e-learning technologies in education."
"e-learning seems to cover almost everything but conventional face-to-face lecture style instruction and other teaching styles. e-learning is most commonly associated with computerbased instruction, and especially internet-based instruction. e-learning has been widely used in corporate and government training. however, nowadays it is used effectively in k-12 and in higher education [cit] . e-learning has the potential to improve teaching and learning. brief descriptions of e-learning's potential follow. elearning facilitates student-centred learning. learning is possible at any time and at any place, which provides student interaction with the content. it makes collaboration, communication, and administration easier. e-learning can reduce the delivery cost of instruction because of distance learning strategies and tools. it also adds worldwide dimensions to courses [cit] ). e-learning can also be considered to be an instructional tool or mind tool and a part of distance education. distance learning technologies include all the new technologies indicated above as well as printed materials and electronic materials. this is because distance learning effectively uses all e-learning technologies and computer tools used for delivering instruction and developing ecourseware with id models. at this time, several technologies have been integrated to develop courseware, which indicates using an iel approach."
"iel and e-courseware design iel tries to combine elements such as face-to-face teaching, distance education and training on the job. thus, it is media mix -that is to say, a mix of methods -each has its own characteristics in terms of costs, availability, effectiveness and efficiency (i̇pek, i̇zciler & [cit] ) . the specific combination of methods is a result of a systematic design procedure. iel requires new pedagogical aspects and organizational and technical levels for the successful implementation of elearning in combination with more conventional systems. iel is becoming more and more important for lifelong learning as well as e-learning. the field of id is discussed in consideration of id as an engineering discipline [cit] . information technology is getting richer, with new statements and situations in technological development. new technologies are being developed and these are increasing its popularity in work settings, in daily life and in education. depending upon the type of multimedia project, it may be necessary to add to the design specifications that are suggested, such as with schedules, project teams, media, lesson structures and configuration control and review cycles [cit] . for example, a distance learning project might require a more detailed scheduling component."
"the core of this next generation of e-learning is centred on the interest of the learner and is often generated by the learner. in short, e-learning is no longer a traditional self-paced alternative to classroom learning. it resembles employees' experiences in accessing the web, as well as their online and offline social networks. today's workers need availability in a learning environment, in which they can find information, collaborate and build their own learning plans. e-learning and iel design procedures should focus on idt models' strategies or isd approaches. to get high-level value for teaching with e-learning, idt strategies present very important ways to make the design clear and the steps easier for learners and developers in the e-learning process. generally, the idt process begins with a needs assessment and a definition of the problem or its objectives. these processes for designing e-learning have been discussed and redefined to explain the next generation of e-learning lessons (i̇pek, i̇zciler & [cit] ) . these steps are as follows. step 1. as vision phase-problem analysis as a needs assessment the needs assessment (problem analysis) [cit] and the id model, consists of a needs analysis, a performance analysis, a written problem statement, a context analysis, design notes and a problem analysis for e-learning. data gathering continues during the problem analysis process and the project itself. each planning point has a clear beginning and end, with an output that feeds into the next stage. an iel course should have these strategies for learners and their learning environments for lifelong learning."
"step 2. problem analysis and tasks with instructional analysis-profile after the problem analysis, there follows a task analysis, an instructional analysis and an identification of the goals/objectives explaining how to design the iel modules for the course. instructional analysis is a process by which task areas are classified according to the types of learning required to acquire a given task or knowledge. at this stage, the various types of learning, tasks and content analysis will be completed. an idt model -basically, the taxonomy developed by gagne and briggs -or else an integrated idt model will be used to reach this objective. instructional analysis provides the entry requirements for different learning conditions, the type of learning -such as verbal, cognitive, case-or project-based and audio-visual -sequencing instruction, and the construction of hierarchies for iel modules in technology training with different social-economic groups [cit] ."
"is it possible to blend the best of modern and traditional approaches to create more powerful e-learning products? for this reason, the e-learning design process could be created based on idt models. the process would be one that requires more discipline than is often involved in modern course development. [cit], to encourage quality in e-learning in the modern environment, five critical concepts must be recognized and accepted. these concepts are: 1) instructional design principles can positively affect the quality of e-learning products; 2) not all subject matter experts are instructional designers, and so they should be partnered with someone who has expertise in designing e-learning; 3) saving money on development through the use of free content elements may ultimately cost even more money if the learners become disengaged or if the desired learning outcomes are not achieved; 4) a systematic approach to design and development is needed to ensure that an accurate and thorough learning environment is created, such that a balance of quality, efficiency and economy is achievable through the implementation of a cost analysis [cit] . at the end of the design process, iel courseware will be developed that is effectively based on integrated idt systems approaches. the evaluation and implementation are the final stages of the project management of instructional problem-solving based on id models. the project management ends with a diffusion process to explain all the procedures."
"based on the previous results, the m1 assessment model and the m2 assessment model were built using the elm algorithm with 45 hidden neurons in the hidden layer and the sin function as the activation function. the m1 model was used to assess the stability numbers of the breakwater figure 5 provides the assessment performance for the breakwater sections that were at a low damage level using the vm formula, eb formula, gpm1 formula and m1 model. as shown in figure 5, more than half of the predicted stability numbers of the breakwater sections were smaller than the measured values using the vm formula, the eb formula and the gpm1 formula. the assessment performance of the m1 model was more balanced: about half of the predicted stability numbers were smaller than the measured values, while the other half were larger than the measured values. several statistical indices were introduced to assess the prediction performance of these approaches, such as the bias, cc, si, and ia. lower values of bias and si represent a better assessment performance, and higher values of cc and ia indicate a better prediction agreement. when the values of cc and ia are close to 1, this indicates a perfect agreement between the predicted and several statistical indices were introduced to assess the prediction performance of these approaches, such as the bias, cc, si, and i a . lower values of bias and si represent a better assessment performance, and higher values of cc and i a indicate a better prediction agreement. when the values of cc and i a are close to 1, this indicates a perfect agreement between the predicted and measured stability numbers. table 3 lists the statistical index values of the three approaches. as shown in the table, the cc and i a values of the vm formula are the smallest among these three assessment approaches, while the bias and the si values are the largest, which indicates that the performance of the vm formula has the lowest quality agreement. the cc and i a values of the eb formula, the gpm1 formula and the m1 model are nearly the same, while on the other hand the si values of the two methods are also nearly the same. the evaluation indices show that the eb formula, the gpm1 formula and the m1 model have similar abilities for predicting the stability number of breakwaters with a low damage level, but that the m1 model was built based on a smaller size of training data, which indicates that the m1 model has a good generalization ability."
"in the next stage, instructional strategies for iel environments are planned [cit] . a designer must respond to the following questions: (a) what must happen for learning to occur? (b) how will the course content be presented? (c) what instructional strategies are preferred for the type(s) of e-learning involved? (d) how much practice and feedback are needed? (e) how should the content be presented for iel?"
"in general, evaluation is a basic component of iel design and implementation. iel design begins with a profile that indicates a problem analysis and needs assessment, including the analysis, design, development, implementation and evaluation of the educational systems. its design includes pedagogical, technological and organizational levels. [cit] are important steps for creating the next generation of e-learning lessons. these may be used for developing future e-learning courseware."
"the extreme learning machine is widely used in regression and classification [cit] . however, up to now, few studies involving applications of elm in coastal engineering have been published. a search of the literature revealed that elm was once used to predict the sea level, tide, and wave heights [cit] . in the following subsection, a brief introduction about the fundamentals of extreme learning machine models is given to clarify the process details of the elm model establishment. more information about elm models can be found in huang, zhu and siew [cit], huang, huang, song and you [cit] ."
