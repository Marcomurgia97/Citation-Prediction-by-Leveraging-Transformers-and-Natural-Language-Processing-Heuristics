text
"in this study, we investigate the applicability of electrocardiogram (ecg) and galvanic skin response (gsr) data for classification of expertise in the proposed adaptive simulation. this paper describes two trauma medicine simulations developed for the purpose of distinguishing between novice and expert trauma responders. ecg and gsr signals were collected using wearable sensors and used for expertise level analysis. in the following sections, we describe the materials and methods used in this study, followed by a detailed exploration of the ecg and gsr data for applications in classification of expertise. t-distributed stochastic neighbor embedding (t-sne) is used to visually observe the separability of the feature space between experts and novices. least absolute shrinkage and selection operator (lasso) is then utilized to evaluate feature importance for use in expertise classification. finally, classifiers are developed to differentiate expert and novice trauma responders using ecg and gsr features, both together and separately. these classifiers included support vector machine (svm), decision tree (dt), random forest (rf), and k-nearest neighbour (knn) models."
"in our training dataset, 480000 pairs of the original images and the corresponding classified ordered output images of chromosomes are used. 90000 samples used for validation. 90000 samples are set as test set. all images are straightened and cropped into the size as 64x32. the preprocessed images and original images are sent to the classification network in parallel. as a multi-label classification task, we measure the performance of our classification by accuracy and f1 score. table 3 gives the result of each single model of us and the combined model. it also shows the accuracy of some popular cnn models and the results of predecessors. the mcnn_go model has better performance than any single-input models. we analyze that the multi-input cnn can extract more valuable features and reduce the error of classification because the combined features have higher robustness. for comparison, we implemented the method [cit] . it is then trained on our own dataset. the result shows that, after straightening, the accuracy is slightly higher than using alexnet only. but, it highly relies on the results of preprocessing. once the preprocessing result is not ideal, the accuracy of classification may decrease significantly. in our system, we feed both of the original images and preprocessed images into the classification network so as to minimize the uncertainty caused by bad preprocessing results. from the comparison, we can observe that our accuracy is 5% higher than others including alexnet, vgg-16, resnet-50 and densenet. table 2 gives the classification performance on each class. we can see that our mcnn_go model has high robustness and good performance on most types. our model performs best on class 1, with the f1 score of 0.9901, and performs worst on y chromosomes, but the f1 score is still greater than 0.92."
"as discussed in section 3.2.4, the absolute values of the regression coefficients were calculated for each feature of the ecg, gsr, and multimodal feature sets. using the values of the coefficients, the cutoff value for which features should be used within the classifiers was experimented with. it was found that using features with a regression coefficient above 0.01 achieved the best classifier results. the regression coefficients of those features are shown in figure 6, where 17 features were found to have regression coefficients greater than 0.01. the most important features were found to be rr mean and ulf with regression coefficients of 0.9044 and 0.8979, respectively. sdsd, lf, and hf were found to be the least important features (non-discriminative), with regression coefficients of 0. generally the regression coefficients of the extracted ecg features tended to be high, indicating that ecg features are likely suitable for the classification of level of expertise. from the regression coefficient of gsr features, shown in figure 6, 9 gsr features were found to have regression coefficients greater than 0.01. the time domain gsr features were found to be more important than those from the frequency domain with the most important features being minimum hrt, standard deviation of rt, mean hrt, and minimum rt with regression coefficients of 0.1922, 0.1864, 0.1599, and 0.1387 respectively. none of the frequency domain features, or the features from scr amplitude, or scr area, were deemed discriminative as their regression coefficients were less than 0.01. there are fewer gsr features with regression coefficients greater than 0.01 when compared with the ecg feature set. additionally the features that were found to be discriminative have lower values for their regression coefficients."
"possible effects of landmark pictogram display on route recognition may have been inhibited by the experimental design. the availability of landmark pictograms in both the study maps and the recognition stimuli was completely randomized. therefore, participants may have learned after a few trials that they cannot rely on learning the route based on its relative position to landmark pictograms, as these may not be available in the recognition phase. in order to assess the effect of landmark pictograms on route recognition more accurately, availability of landmark pictograms should be a between-subject factor, or at least consistent across a single trial including study phase and recognition phase. if the experiment would be adjusted accordingly, we would expect to find significant results comparable to previous studies [cit] ."
"shimmer3 ecg and gsr wearable devices [cit] were used in this study, as seen in figure 2, to collect data during the simulations. the sensors were small and lightweight, with the ecg sensor weighing 31 grams, while the gsr sensor weighs 28 grams. the shimmer3 ecg sensor allows for ecg signals to be measured from four bipolar limb leads in addition to one chest lead. the signals obtained from the differential ecg channel between left arm (la) and right arm (ra) limb leads were used in this study. the shimmer3 gsr sensor collects one channel of gsr data by monitoring the conductivity between two reusable electrodes that can be attached to two fingers using velcro straps. the gsr sensor itself was clipped to a strap on the participant's wrist, while the ecg sensor was clipped to a strap placed around the waist of the participant. the signals from both sensors were obtained over bluetooth connection allowing for full mobility of the participants during the simulation. both signals were collected at a sampling rate of 500 hz."
"in order to adapt a simulation to a user's level of expertise, their expertise must first be detected and quantified. to the best of our knowledge, no research has been done on detecting learners' level of expertise through biological signals. however, there has been found to be an inverse correlation between level of expertise and cognitive load [cit] . biometric measures, particularly heart rate variability (hrv) and galvanic skin response (gsr), have been shown to be indicative of cognitive load [cit] . additionally, electroencephalogram (eeg) and electrooculogram (eog) have been found to have a strong correlation with cognitive load [cit] . these biological signals could also be indicative of level of expertise [cit] . machine learning classifiers using these bio-signals, could therefore, be used to facilitate dynamic classification of expertise for adaptive simulation."
"performance in the recognition task was assessed according to the signal detection theory [cit] in the form of hits, misses, correct rejections, and false alarms. if the route shape in a recognition stimulus matched the route shape in the study map (old stimuli), participants could either correctly state a match (hit) or wrongly state a mismatch (miss). if the two route shapes did not match (new stimuli) participants could either correctly state a mismatch (correct rejection) or wrongly state a match (false alarm). because of the redundancy in these measures, only the hits and correct rejections were investigated in the statistical analyses. the misses and false alarms were merely used to calculate d', an additional recognition performance measure based on all four response types. the benefit of d' is that it puts correct signal detection (hits and correct rejections) and noise responses (misses and false alarms) in proportion [cit] . the d' value increases if the ratio of hits and correct rejection increase. it decreases if the ratio of misses and false alarms increase. this allows to make statements about the sensitivity of how well participants discriminate old from new stimuli. for information about d' calculation see macmillan & creelman [cit] ."
"given that d' values put correct and incorrect responses into proportion, calculating d' requires aggregation of hits, misses, correct rejections, and false alarms across participants and specific conditions. this undermines the benefit of the gee model to handle correlations of multiple responses from the same subjects at the level of single items. the same is true for the visual attention measures, which generated only one fixation count and average fixation duration value per participant and study map. in addition, the fixation data did not follow a gaussian distribution. therefore, the nonparametric mann-whitney u test was used to compare d' and eye fixations data between the two map conditions (reduced/standard map). for the examination of the within-subject effects of landmarks (in study or recognition stimuli), d' values and fixation data were analyzed with wilcoxon signed-rank tests. additionally, the wilcoxon signed-rank test was used to compare fixation counts and average fixation durations between the route aoi and the aoi offside the route separately for each map condition (reduced/standard map)."
"in the simulation, the participant played the role of a trauma team leader, directing the trauma team on how to provide care for the patient. the trauma team consisted of 1 registered nurse and 2 residents, all of which were hired actors. the participant was given a brief description of the trauma scenario prior to entering the simulation room. the goal of the participant during the penetrating trauma simulation was early initiation of massive transfusion protocol for resuscitation and disposition for emergency surgery. the goal of the participant during the blunt force trauma simulation was early identification and intervention of left sided tension pneumothorax (collapsed lung), followed by appropriate consultation and disposition with neurosurgery for further neuro-imaging, as well as thoracic surgery for consideration of operative intervention of left sided chest wall injuries, flail chest (broken rib cage), and pneumothorax. participants completed both simulations successively, in random order. distractors were introduced verbally by the registered nurse to increase the cognitive load of the participants. these distractors included the introduction of an ecg reading with sinus tachycardia (a form of elevated heart rate), a high white blood cell count, and emergency medical services (ems) calling with a patch alerting the participant that a 60 year-old male with witnessed cardiac arrest is 5 min away from the emergency room. all of the distractors were used for each participant. after each of these distractors was introduced, the participant was asked by the trauma team what they would like to do with the information. this was done in order to distinguish differences between how novices and experts deal with the new information."
"the pan-tompkins (pt) algorithm was utilized to process the ecg signals and detect the qrs complexes [cit] . in this algorithm, to reduce the influence of electromyogram (emg) noise, powerline noise, baseline wander, and t-wave interference, a butterworth bandpass filter was applied with a passband frequency of 5-15 hz. the signal was then differentiated using a 5-point derivative transfer function to provide the qrs slope information. the absolute value of the signal was taken and a moving average filter was used to obtain the wave form features in addition to the r-peaks. examples of the raw and filtered ecg signals are shown in figure 3 . a moving 150 ms window was employed to obtain features from the filtered signal by first detecting the r-peaks. to detect the r-peaks, two threshold values were selected to distinguish between the peaks and noise. if no peaks were detected in a time window of two seconds, a search-back technique was initiated to find the missed r-peaks. the threshold values were set iteratively based on the most recent detected signal and noise peaks. all features were obtained from the intervals between two r-peaks (rr interval). an example of an rr interval is shown in figure 4 ."
"as our method generated multiple measurements per participant and item, and as visual inspections revealed that the response variables were skewed, we chose a generalized estimating equation (gee) model for our first statistical analysis. the gee model is an extended version of the generalized linear models. it can handle correlations of clustered data (repeated measures) and non-normally distributed response data [cit] and can be seen as a robust alternative for multifactorial anova models. recognition performance (hits and correct rejections) main effects were calculated for the between-subject factor (reduced/standard map) and the two within-subject factors (landmarks/no landmarks in the study maps and the recognition stimuli). additionally, interaction effects between the three factors were assessed."
"in both cases-navigation and storytelling-it can be advantageous to focus on the essential information. many maps, especially topographic maps, are task-independent. such maps are created isprs int. j. geo-inf. 2018, 7, 469 2 of 13 to represent the real environment in a most complete way. thus, they display all information that complies with the categories provided in the legend or an ontology. as an example, one expects a city map to contain all streets in the depicted area. such information might, however, be irrelevant to the user when performing a certain task. leaving out unneeded information can have several consequences. one might assume that reduced maps which do not display all information provide fewer distractions when navigating. also, the user of a reduced map might get an impression that the map is, in fact, incomplete. as a consequence, the user develops an open-world assumption. assuming gaps or errors in the map opens the possibility of more flexible use and might aid the map user when telling a story or being confronted with inaccurate map information. despite of the assumed usefulness of reduced maps, potential positive or negative consequences have only been examined in part so far [cit] ."
"unexpectedly, neither the display of landmark pictograms in the study map, nor removing or adding landmark pictograms in the recognition stimuli affected route recognition performance (as it regards the learned old items). displaying landmark pictograms along the route did not improve route recognition performance, which stands in contrast to the findings of tom & denis [cit], who showed that the display of landmark pictograms in maps improves route recognition. but displaying landmarks during memory encoding reduced the number of correct rejections. hence, participants made more false alarms, i.e., falsely recognized a new route as being learned if landmarks are displayed during study. while the performance on old items (hits) was not affected by the availability of landmarks during study or recognition, the significant effect for new routes reveals that landmarks are part of the mental representation. learning on a map with landmarks leads to higher false alarm rates. thus, these data are in line with findings by franke & schweikart [cit], who demonstrate a positive effect of landmark display on the formation of cognitive maps. they also partially confirm our third hypothesis that adding or removing landmark pictograms after a route has been learned negatively affects recognition performance."
"unfortunately, not all participant data could be used for the fixation analysis. for six participants, the eye-tracker could not be calibrated successfully. seven other participants exceeded the predefined threshold of 25% gaze data loss. this reduced the sample size of the eye-tracking analyses from 69 to 56 participants."
"additional levels of transparency (alpha levels) should be investigated. when lower transparency levels of areas offside the route are selected, visual attention should approach the pattern of standard maps as the map complexity increases. if map areas offside the road are on the other hand completely removed, a radical change of visual attention and recognition performance may occur. in this case, visual attention would be expected to only focus on the route and potentially displayed landmarks, as no other objects would be available offside the route. the question remains whether route recognition performance would still remain stable with such radical map reduction, as the amount of visual anchors inside the map would be massively reduced to the close neighborhood of the to-be-learned route."
"pix i en where i m means the line vector of image i and pix i en means effective ith pixel in the mth line. experimental results on 90000 real images show that our method has high robustness and generality. as shown in table 3, this method can improve the classification accuracy by two percentage points."
"both digital maps and navigation systems enclose a tradeoff based on their design. as mentioned before, maps are usually task-independent and strive for completeness. additionally, they allow users to obtain survey knowledge of their surroundings [cit] . however, they also contain a lot of information that is irrelevant for specific navigation tasks. studies have shown that the degree of visual complexity in a map affects performance in map-based memory tasks [cit] . while [cit] showed that topographic detail improved recognition performance of object locations in maps, [cit] found that improvements of memory performance based on the presentation of additional map elements become less noticeable at exceedingly high levels of map complexity. given that visual complexity of stimuli can increase the cognitive load of the perceiver [cit], existence of a tipping point can be presumed where the amount of displayed information is no longer helpful for map-based memory tasks and distracts from relevant visual elements. navigation apps on the other hand are highly task-oriented and, as usual for location-based services (lbs), the displayed content depends on the context (current position). they support efficient wayfinding in unfamiliar environments, but they usually visualize only a narrow area around the position of the user. this can impair orientation and route memory, as distant global landmarks are not displayed [cit] . additionally, the lack of active interaction with the environment prevents the acquisition of spatial knowledge about the environment [cit] . an ideal navigation aid would therefore combine the strengths of digital maps and navigation systems-fast and efficient wayfinding, limited cognitive load, focus on relevant map elements, and a survey view of the environment that supports the formation of survey knowledge [cit] ."
"based on this assumption, we investigate the possibility to limit the complexity of maps and the consequential effects on cognitive load. the common cartographic approach for reducing map complexity is generalization. generalization describes the process of simplifying boundaries of map elements and removing seemingly less relevant elements [cit] . however, map users may not recognize task-oriented map generalization instantly, certainly not what elements have been removed."
"once the cnns are optimized, the global-level, local-level and straightened features can be extracted and sent to the classifier. to make full use of the features, we build a concatenation layer with 4096 nodes to merge these features into a big feature map. then a mlp classifier composed by two fc layers with 1024 nodes and one softmax layer will learn the correspondence between the features and the possibility of each classes."
"several classifiers were utilized to differentiate levels of expertise. the classifiers were trained separately with the important features from ecg, gsr, and multimodal feature sets. four different supervised models were used to develop separate classifiers in order to compare the performance. support vector machine is a machine learning classification technique in which the features, or input vectors, are mapped onto a high dimensional feature space. the feature space is then divided using a hyperplane to separate the features of different classes such that the margin between features of different classes is maximized [cit] . the mapping of the features onto a higher dimension feature space is determined by a kernel function selected for the svm. multiple kernels, namely linear, quadratic, and radial basis function (rbf), were experimented with, to compare the performance. the best performance was obtained using a second degree polynomial kernel."
"the experiment consisted of a practice trial and six experimental trials. at the beginning of each trial, a study map was shown for 30 s. during this time, participants had to memorize the route displayed in the map. participants were presented only maps that belonged to the experimental condition a participant was assigned to (reduced maps or standard maps). three of these six study maps shown in the experimental trials were randomly selected to display landmarks while the other 3 maps did not contain landmarks (i.e., within-subject factor 'study map landmark' yes or no). the figure 2 . recognition stimulus variants. after each study map, four recognition stimuli were shown to the participants. at least one of these stimuli contained the same route as the study map. the rest contained slightly changed route shapes. whether landmark pictograms were displayed in a recognition stimulus was determined at random."
"karyotyping of chromosomes can be regarded as the task including two steps, respectively, segmentation and classification. to mitigate the human effort in karyotyping, numerous methods for segmentation and classification have been proposed."
"to optimize this model, we first train the global-net, local-net and straighten-net individually until convergence and fine-tuning the models. then, we combine them together. fix the parameters of resnet blocks and just train the upper layers. then unlock all parameters and fine-tune the whole model under a very small learning rate (0.0005)."
"in our experiment, we examine the use of reduced maps adapted to specific use cases in order to overcome the tradeoffs of maps and navigation systems in wayfinding tasks. when people want to communicate a route without external aids, they often use sketch maps, hand-drawn maps that show the whole route at once, but leave out most peripheral elements shown in a \"classical\" map. they are usually incomplete [cit], i.e., they only contain roads and road sections alongside the route, and landmarks at decision points [cit] . such sketch maps are a graphical representation of the task-oriented cognitive map of their creators [cit] . these sketch maps seem to be perfectly reduced to tell the story of how to follow the route to aid route learning and navigation. therefore, reducing maps based on sketch map pattern may improve route memory performance."
"1) medial axis extraction. first, as shown in figure 3 (i)-(v), we binarize the chromosome sub-image and extract its contour. then, sample the contour every Î± pixels and generate delaunay triangles based on these sample points. after removing triangles outside the contour, we can get the medial axis s by connecting the center of triangles. the result shows that our approach is more clear and has fewer branches than the medial axis locating algorithm used in scikit-image [cit] . 2) cutting-point finding. javan-roshtkhari and setarehdan [cit] found that, when rotating the original image of chromosome, the position of the bending point can be obtained from the change of histogram. however, when the width of the chromosome is relatively average or the total pixel amount is small, the histogram tends to be unchangeable during rotation and the bending point cannot be found. in our approach, we connect two ends of the medial axis s and get a line segment l (figure 3(vi) ). because the two ends of the medial axis are actually the ends of the whole chromosome, the point farthest from l must be the bending point. 3) rotation. after finding the cutting point p cut, we can cut the image into two parts, and get the minimum bounding box bbox l and bbox h :"
the presented study should be regarded as a first step towards the development of more user-friendly and task-oriented maps. additional research in this area could give clear instructions about how maps used for navigation and route learning tasks should be designed.
"encouraged by the success of the residual learning [cit], we apply resnet-50 as our backbone. the original image and straighten image will pass a maxpooling layer with a 2x2 kernel and a convolution layer with 3x3 kernel. the cropped image will be feed into resnet directly. by doing this, we can maximum the extracted features."
"the results of applying t-sne to the features extracted from the ecg signals, both before and after baseline correction are shown in figure 5a,d, respectively . the embedded feature space shows that the ecg features without baseline correction are not readily separable based on experts and novices. however, with baseline correction, the feature space appears to be separable based on participants level of expertise."
"in order to prevent response biases, no information about the study purpose was given to the participants before or during study participation [cit] . they were told that information concerning the study purpose would be provided after the experiment. before the experiment started, the procedure was explained and the participants gave informed consent. hereafter, they took a seat in front of a tobii tx-300 (300 hz, 23 inches) eye-tracker monitor that was used to visualize the stimuli. the distance between the eyes and the monitor was 65 cm."
"then, many cnn-based methods on chromosome classification have been developed. [cit] first used cnn to classify chromosome. in this method, the preprocessing procedure was also proposed to straighten those curved chromosomes. their model was trained under 1600 individual images and tested on 200 images. the accuracy on preprocessed images is 86.7%. because of the limitation of labeled images, [cit] represented a method based on siamense network, which has good performance under scarce training data. in detail, chromosomes were straightened by two different approaches in parallel before sending to siamense network. the accuracy on 209 images achieved 84.6% after being trained on 1296 images. [cit] then proposed a multiple distribution generative advertising network (md-gan) to generate labeled data and used a pre-trained cnn to classify chromosomes with samples generated by md-gan. however, their average precision only achieved 63.5%, which is further below the requirement for clinic application. one main challenge is the lack of labeled data. this may lead to overfitting, low accuracy and low robustness. another challenge is that, the performance of learning model highly relies on the results of chromosome straightening. our framework is built on the multi-input cnn [cit] . it can extract features from not only preprocessed images, but also original images. figure 1 illustrates the whole workflow of our system. in the first stage, we input the original giemsa stained images into the segmentation network. the mask of each individual chromosome is calculated as its output. then, system extracts the chromosomes to sub-images separately. in the second stage, each sub-image will be geometrically optimized by cropping, rotation and straightening. in the last stage, the original and optimized sub-images are fed into our classification network. the entire workflow is fully automatic and has no need for human intervention. due to the high complexity and uncertainty in clinic conditions, the accuracy of our system can vary widely in some most extreme case. instead of replacing doctors during the karyotype analysis completely, we aim to optimizing the processing by minimizing doctors' load."
"-each piece of the captured image contains massive chromosomes. they need to be observed by laboratory doctors one by one. especially, the cases with impurities, overlapping area and contacting will significantly increase the difficulty of the operation; -after segmentation, the classification of chromosomes is also exhaustive and labor-intensive. in clinic, the chromosomes are often curved or bent heavily, which makes it harder to verify the classes of chromosomes. to minimize the human effort consumption in karyotype analysis, [cit] provided a chromosome segmentation method based on crowdsourcing. this is the first who used cnn on chromosome classification. they also proposed several preprocessing methods such as the straightening and length normalization. however, the results could not meet the requirements of automatic karyotype analyzing. the accuracy of classification is relatively low and unsatisfactory. to improve the accuracy of chromosome segmentation and volume 7, 2019 this work is licensed under a creative commons attribution 4.0 license. for more information, see http://creativecommons.org/licenses/by/4.0/ classification, we make significant improvements to the existing approaches and increase the accuracy and availability of automatic karyotype analyzing. in this paper, we provide an approach for performing karyotype analysis using deep learning. the metaphase chromosome images are fed into a segmentation network for instance segmentation, and then each single chromosome will be extracted from the whole picture. after that, we investigate a multi-input cnn to classify the extracted chromosomes into 24 classes for karyotype analysis. for certain cases where the extracted chromosomes are curved, we develop a geometric algorithm to straighten each chromosome before classification to ensure the consistency of the training data. the straighteng method is based on the medial axis of chromosome. this preprocessing is proved to improve the accuracy of classification. the main contributions of this paper are as follows:"
"ethics approval was secured from the queen's university research ethics board (qreb). participants were recruited from two categories: expert and novice. the expert participants had completed their specialty training in emergency medicine and were practicing independent emergency medicine physicians. moreover, they have had experience managing traumas as attending physicians. the novice participants were queen's university medical students at the end of the 4th year of their medical studies. they had been rotated through multiple medical specialties (i.e., internal medicine, surgery, emergency medicine, etc.). the reason the novice group was selected from students with some background in trauma medicine, was so that they would have the knowledge necessary for being able to treat the patient successfully. a total of 10 participants were recruited, 5 experts (3 male, 2 female) and 5 novices (2 male, 3 female). the ages of the expert participants ranged from 31 to 44, while the novices ranged from 25 to 34. in addition to data collection during the simulation, baseline data were collected for 2 min prior to the start of the simulation in a quiet room, while participants were in a relaxed seated position."
"a. chromosome segmentation segmenting out chromosomes from the meta-phase chromosome image is primarily crucial, because it will directly affect the accuracy of following classification process."
"to determine which of the features from the ecg, gsr, and multimodal feature sets are important for the classification of expertise, lasso [cit] was used. lasso is a regression analysis method where the absolute size of the regression coefficients are penalized by reducing them. after the process, the features with non-zero coefficients are suitable for use in models [cit] . the larger the magnitude of the coefficients, the greater the importance of that feature for differentiation of the two classes. in our study, lasso was used to calculate the regression coefficients for each feature."
"the present study was targeted at assessing the effects of map reduction and landmark display on route recognition and visual attention. we were able to demonstrate that reducing a map by displaying map areas offside a route transparent does not affect route recognition performance. however, reducing the map shifted proportionally more fixations towards a displayed route. presenting incongruent information by removing or adding landmark pictograms after a route had been memorized only affected recognition performance of new stimuli (correct rejections and false alarms), but not of old stimuli (hits and misses), which we argued to be affected by our experiment design. overall, our findings indicate that task-oriented reduction of map complexity is a feasible approach to reduce the cognitive load of the user without compromising route recognition. besides navigation apps, other map-based lbs as point of interest locators may benefit from our results. however, further research concerning map reduction levels, completeness, landmark display, and their effects on orientation and navigation performance is required for gaining a deeper understanding of how to design task-oriented maps."
"if all elements in a map offside a displayed route are invariably displayed transparent, it needs to be considered that this may also deteriorate positive aspects of a survey map. especially landmarks are highly relevant for orientation, navigation and the formation of cognitive maps [cit] and are expected to be important elements of navigation stories. therefore, the display format of landmark pictograms can affect navigation and route recognition performance [cit] . landmark pictograms in openstreetmap and google maps are displayed based on the selected scale of the map. when a small scale is selected, only few of the deposited landmark pictograms are displayed. at the largest scale, all deposited landmarks are displayed. removing or adding such map elements based on map properties as scale would force the user to rely on other map elements for route recognition, which may in turn impair recognition performance. in order to assess whether the task-specific reduction of maps and the display of landmark pictograms affect route perception and recognition, we test the following hypotheses in our experiment."
"our analysis on the lasso regression coefficient of the multimodal feature set showed that 22 features had regression coefficients above 0.01. the ecg features had larger regression coefficients when compared to the gsr feature set. this is in line with our finding from using lasso on the unimodal feature sets. the highest regression coefficient was from an ecg feature, namely vlf, with a value of 0.6245. in comparison, the highest regression coefficient for a gsr features was 0.0832, from mean scr amplitude. more of the ecg features were found to be discriminative than gsr features. our findings suggest that the ecg features are more important for differentiating between expert and novice classes for the purposes of future adaptive simulations. table 3 shows the results of using the classifiers to differentiate the level of expertise using loso validation scheme. the classifiers using the multimodal feature set performed better than the classifiers using either feature set separately with an accuracy and f1 score of 0.8296 and 0.7996 using the knn classifier. these results differ from our findings based on lasso as our earlier analysis showed that the ecg feature set was likely the most discriminative for level of expertise. it should be noted that while the results from lasso can demonstrate which individual features are the most important for distinguishing between levels of expertise, the results do not indicate how well the features can be used together in a feature set for classification. additionally, while lasso used all of the participant data, loso validation was used during classification to set aside a subset of data for testing while the classifier was trained on the rest of the set. these differences could account for why the gsr and multimodal feature sets outperformed the ecg feature contrary to what the lasso results suggested. the high accuracy and f1 score from the loso classification results show the ability for the combined feature set to be used to differentiate between expert and novice classes, supporting our observations with t-sne. these results demonstrate the ability of the classifiers to determine the level of expertise for new participants for which the system has not been trained. the accuracies achieved using loso show the feasibility of utilizing the proposed framework for dynamically adapting the simulation based on expertise. the multimodal feature set using both ecg and gsr will be used in further studies to develop more accurate and robust classifiers as it provided the best results when using loso validation."
"in practical, chromosomes lay on the giemsa staining image in disorder as illustrated in figure 1, which makes classification process much harder and unstable. the straightening operation on chromosomes is often performed before classification. the results demonstrated that it can improve the accuracy of classification considerably [cit] . however, through the experiment on our dataset, we observe that these existing methods are only feasible on the chromosomes under fairly flat gesture. the performance on the heavily curved ones is limited."
"the usefulness of the extracted ecg and gsr features for expertise classification was examined by reducing the high dimensionality of the respective feature spaces to 2 dimensions with t-sne [cit] . this allowed for the visualization of the feature space in order to evaluate the separability of the two classes. t-sne was performed using 10,000 iterations with a perplexity of 30, and learning rate of 10. each 10 s time window of ecg data was treated as a separate sample, while every 30-s time window of gsr data was treated as a sample. the samples were assigned a class according to the expertise of the participant."
"the experiment consisted of a practice trial and six experimental trials. at the beginning of each trial, a study map was shown for 30 s. during this time, participants had to memorize the route displayed in the map. participants were presented only maps that belonged to the experimental condition a participant was assigned to (reduced maps or standard maps). three of these six study maps shown in the experimental trials were randomly selected to display landmarks while the other 3 maps did not contain landmarks (i.e., within-subject factor 'study map landmark' yes or no). the presentation order of the six selected study maps was randomized. after every 30 s study phase, the four recognition stimuli belonging to the previously shown study map were presented successively, each for eight seconds. the presentation order and the variant selection of each recognition stimulus (with or without landmarks) were randomized. the matching of study maps and recognition stimuli with and without landmarks allowed to compare recognition performance between conditions in which landmarks were shown only in the study phase, only in the recognition phase, in both phases or in none of them. after every recognition stimulus presentation, participants had to answer whether the route displayed in the previous recognition stimulus had exactly the same shape as the route displayed in the last study map. the answers were given by pressing one of two keyboard keys labeled with \"yes\" and \"no\"."
"to ensure that the learning outcomes for simulations are better achieved, we propose an adaptive simulation paradigm in which the level of expertise of the participant is autonomously classified using their biometric signals with machine learning. this classification can then be used to adapt the simulation to the cognitive load of participants by altering the simulation difficulty. this classification can then be used to adapt the simulation to the cognitive load of participants by altering the simulation difficulty. the architecture for the proposed pipeline is shown in figure 1 . to support the proposed framework, the capability of wearable sensors to act as a meaningful input for machine learning classification of expertise and cognitive load must first be investigated. we chose to tackle this problem in the context of trauma medicine as a proof of concept. we specifically picked trauma medicine for investigating our proposed framework because it is possible to objectively distinguish between novice and expert trauma responders. additionally, it has been shown that learner's cognitive load, which itself is known to be directly impacted by the learner's level of expertise, can significantly impact medical performance [cit] ."
"in the task of chromosome segmentation, we need to segment out every single chromosome from the complex meta-phase giemsa staining image, where contains not only chromosomes, but also impurities. moreover, chromosomes often overlap or contact on others. our model needs to be powerful enough to complete following tasks: 1) object detection, which means it should be able to detect the chromosomes from a messy background; 2) pixel-level segmentation."
"regarding the general research question, two main results were obtained: learning a route on a complete map display or with a reduced map that displays areas offside the route transparent did not affect route recognition memory performance and, in agreement with our first hypothesis, no behavioral differences were revealed between participants in the complete study map condition or the reduced study map condition. in other words, reducing the map display to only the significant 'story' of a map (in the present study the route) did not improve nor deteriorate performance in a route learning paradigm. thus, we found some evidence that route learning does not depend on the distant or uninformative map regions but indeed on the detail (and also likely on the landmarks, see below) in the close neighborhood of the to-be-learned route. at first glance, these results seem surprising, as there is no evidence of an advantage in information processing when participants are forced to focus on the essential map information [cit] . besides the possibility of methodological factors contributing to a null effect (like a small number of items in a condition) and the logical difficulties when deriving inverse assumptions, the results still indicate that it is possible to reduce the map display without further costs when the task is to recognize a route. this is in agreement with the findings of earlier studies [cit] showed that reduction of map complexity did not affect orientation and even improved wayfinding performance in larger areas. o'neill [cit] also found that high map complexity may negatively affect wayfinding performance. these studies together with the present results demonstrate that simplified or reduced maps may support route-based tasks just as well as a standard map."
"because the chromosomes can be intensive or intertwined, our model needs to distinguish and segment each chromosome precisely. in this part, we introduce mask r-cnn to the task of chromosomes segmentation. mask r-cnn is a general framework for object instance segmentation [cit] . the backbone of mask r-cnn is a standard convolutional neural network (resnet101 in our task) combined with a feature pyramid network (fpn) as the component of feature extraction. region proposal network (rpn) then scans the backbone feature map through sliding window so as to target areas called anchors, where contain objects. using the rpn predictions, we can find the top anchors that are likely to contain objects and refine their configuration such as location and size. finally, the regions of interests (rois) are calculated using rpn including roi class, the optimal bounding box and the corresponding mask."
where p low and p high means the end points of l. then we can find the slope of bbox l and bbox h and rotate them to vertical to get bbox l and bbox h . 4) pixel filling. because we want to maximize the horizontal banding features in the processed image. we apply a new interpolation method in our algorithm. for one pixel pix i m :
"when designing simulations, it is essential for the learner's initial level of expertise to be considered. if there is a discrepancy between their level of expertise, and that which the simulation is designed for, the learning outcomes will not be achieved, and there can even be negative implications on the learning objectives [cit] . such simulations can be designed to dynamically adapt to the participant's expertise with respect to the task, enhancing the interactivity of the simulation and tailoring the learning experience to specific learners. this approach can allow for more complex scenarios to be introduced to novice learners gradually and based on learning progress, where eventually they can be exposed to simulations designed for challenging and cognitively demanding situations."
"as the advent of deep learning, cnn have achieved excellent performance on many computer vision tasks. in medical image, scholars also tended to apply cnn for image segmentation. [cit] first used fully convolutional networks (fcn) to do image segmentation. bentaieb and hamarneh [cit] optimized the structure of fcn and successfully used it on medical image processing. medical image segmentation network u-net offers high accuracy while training on a small amount of annotated data, and have superior performance on hela cell segmentation and neuron segmentation tasks [cit] . however, most of the existing models are designed for semantic segmentation, while karyotyping needs to segment out each instance of chromosomes. in this paper, we first use deep learning for chromosome segmentation, and we compared three widely used models: mask r-cnn [cit], fcis [cit] and yolov3 [cit] . yolov3 can be trained and run fast while keeping high accuracy, especially on detecting tiny objects. however, yolov3 can only predict labels on bounding box level, while our task needs pixel-level prediction because chromosomes can be intensive in a small area. fcis can generate masks at pixel-level, but the accuracy of it is lower than mask r-cnn in most conditions. thus, we choose mask r-cnn as our segmentation network and the results show that it works well on the chromosome segmentation task."
"the f1 score is a metric that combines precision and recall. precision is the percentage of correct classifications with respect to the total number of classified participants, as shown in equation (2)."
"to evaluate the performance of the model, both accuracy (acc.) and f1 score were calculated using leave-one-subject-out (loso) validation scheme. true positive (tp) and true negative (tn) measures were the number of correctly classified expert and novice participants, while false positive (fp) and false negative (fn) were defined as the number of incorrect classifications. accuracy is defined as the percentage of the total number of correctly classified participants to the total number of participants, expressed in equation (1)."
"besides navigation, maps are often used for telling stories. television, films, social media, travelogues, newspapers, and audio books are ubiquitous examples of media used for conveying stories, demonstrating their high social relevance. as stories often have a spatial component-things exist and happen in space-maps can be used for this purpose. today, maps can easily be extended with other valuable media, such as texts, audio, and video [cit] . this helps to widen the number of map genres and to adapt the needs of a spatial story [cit] ."
"in this article, we examine in which way the absence of information in a map used for a navigation task influences our cognition. a reduced map provides less information that distracts the user, but also less information that provides context to the relevant parts of the map. we focus on the following two research questions."
"2) the straightening algorithm based on the medial axis of chromosome is proposed, which considerably increase the classification accuracy. it improves the usability of our system on the clinic samples. 3) using a multi-input cnn to classify the chromosomes, the accuracy has been significantly improved comparing to the classic baselines and predecessors."
"the feature space projection of the gsr features with and without baseline correction is shown in figure 5b,e. similar to the ecg t-sne results, it can be seen that the features without baseline correction are not separable, while the features with baseline correction show better separability. however, there are still some overlaps between expert and novice groups. in our study only two levels of expertise were used: expert and novice. the presence of more granular representation of expertise between novice and expert (instead of a binary one) could account for this overlap and close clusters between some participants as some novices could be closer to expert level, while some expert participants may be closer to novice level. the overlap between the two groups could also stem from the differences in the participant's basic ability to perform under different levels of cognitive load. an individual could innately have a greater or reduced physical response to cognitive stimulation when compared to other participants of their same class, novice or expert. this could allow for participants to be classified as having a different level of expertise. t-sne was performed on the multimodal feature set of ecg and gsr features with and without baseline correction, as shown in figures 5c,f . similarly with the individual feature sets, the features without baseline correction showed more overlap between classes, while the feature set using baseline corrected features shows clear separability. the results of combining the two feature sets show less overlap between classes than the t-sne on ecg or gsr features individually. this finding suggests that a multimodal feature set using ecg and gsr signals is capable of differentiating expertise, supporting the development of the adaptive simulations based on the learner's level of expertise classified from multimodal wearable data."
"as discussed in section 1, the goal of this work is to utilize biological signals acquired through wearables to determine the users' level of expertise. to this end, two separate trauma simulations were developed for the collection of biometric data from novice and expert trauma responders. a simman patient simulator (mannequin) [cit] was used as the patient and was outfitted with artificial injuries. in one simulation, referred to as the penetrating trauma simulation, the simulated patient had suffered a gunshot wound to the abdomen. in the second simulation, referred to as the blunt force trauma simulation, the simulated patient had been involved in an automobile roll-over resulting in blunt force trauma. both simulations were designed to last 10 min. the vital signs of the simulated patient were controlled by a simulation technician throughout the simulation. first person videos of the simulations were recorded with a microsoft hololens [cit] worn by the participants. shimmer3 wearable sensors were used to collect ecg and gsr data during the simulations [cit] . the complete simulation environment, and instrumentation worn by the participants can be seen in figure 2 ."
"decision tree classifier uses several decision functions successively to classify an unknown sample. the decision functions take the sample from a root node, through interior nodes to a terminal node that represents its classification [cit] . random forest classifier is an ensemble of decision tree classifiers that each generate a classification decision. the most popular class from the decision trees is returned as the overall classification outcome [cit] . the number of trees in the forest were changed iteratively to obtain the highest accuracy. it was found that a random forest of 100 trees achieved the best results."
"in today's world, human life is accompanied by high mobility. traveling to unfamiliar regions has become simple and cheap, increasing the need for navigation in unfamiliar environments. geographic information in the form of maps or navigation systems is thus of increasing importance. modern web mapping services such as openstreetmap, an example of volunteered geographic information (vgi) [cit], and google maps provide fairly accurate geographic information at no cost [cit] . in the era of smartphones and mobile internet, these map distributers can be used virtually everywhere. additionally, navigation apps can support wayfinding in unfamiliar environments."
"the results are shown in table 1 . after training on 343 real images, our model has achieved 52.059 ap points and 90.590 ap 50 points on 100 real test images. alternatively, by combining 20% real data and 80% synthetic data in the training dataset, the ap points and ap 50 points has been improved by 8 points and 5 points respectively. the experimental results on various test datasets show that the our synthetic data have high availability and can improve the segmentation performance. when iou is set to 0.5, the accuracy of segmentation tested on 100 real images is 95.644%. in the case on 200 combined images, the accuracy is 91.673%. figure 4 shows the results on real meta-phase chromosome images. we can observe that our model performs well, even when the chromosomes are overlapped and bowed."
"we propose a automatic approach for karyotype analysis using deep learning and geometry optimization, and first use deep learning method to perform chromosome segmentation. we propose an simple but efficient approach to generate annotated data automatically for chromosome segmentation, which can improve the accuracy of deep models and build the foundation for deep learning applications in this field. we try to use mask r-cnn to achieve the automation of chromosome segmentation and get a relatively good performance, especially on the overlapping and contacting chromosomes. for the highly curved chromosomes, we represent an optimization algorithm which has better generality than existing methods. based on this optimization algorithm, we build a multi-input cnn to classify the chromosomes and the performance is better than other existing approaches. we are one step closer to the fully automated karyotype analysis."
the study was conducted in accordance with the declaration of helsinki. the experimental design has been controlled by the ethics committee of the faculty of geosciences at the ruhr-university bochum and was classified as ethically acceptable (13 [cit] ).
"the experiment presented above provides insights into how map reading is affected by reducing the amount of information displayed in a map. in the following, we discuss implications for the design of maps used in navigation tasks, among others, in respect to landmarks displayed on the map."
"the lack of significant results in the recognition task implies that it might have been too easy. reducing the presentation time may lead to more distinct recognition performance differences between the experimental conditions. furthermore, requesting participants to respond during stimulus presentation instead of afterwards would allow to use time on task as a measure for task efficiency."
"for answering these questions, participants were asked to memorize a route in a reduced map. thereafter, it was tested how well the participants performed at recognizing the shape of the route. these results were set into context by a comparison to recognition performance when using a conventional nonreduced map."
"medical image processing has become an important assistant tool of diagnosis and therapy. nowadays, human karyotype analysis is of great significance for the clinical diagnosis of genetic diseases. karyotype analysis is one of the main techniques of cytogenetics, including analyzing, comparing, sequencing and numbering of metaphase chromosomes through the captured chromosomes digital images. before karyotyping, cells need to be cultured. as shown in figure 1, when chromosomes are at the metaphase, they will be separated from the nucleus and stained onto glass slides for observation and photography under the optical microscope. the division and classification process of chromosomes will be done by professional doctors with the help of auxiliary equipment. this set of operation is time-consuming and laborious, which is mainly due to the following difficulties:"
"simulations have been shown to be an important and effective tool for training that allow for experience to be gained through mimicking real world experiences in a risk-free interactive setting [cit] . for instance, simulations are particularly useful for aiding trauma responders in becoming experts, or increasing their level of expertise, without any risk to patient safety [cit] . by doing so, responders are able to learn how to make time-constrained life-and-death decisions, by applying knowledge learned in simulations to real world scenarios."
"eye fixation measures have been reported to be related to mental processing of visual stimuli [cit] . therefore, average fixation duration and fixation count inside predefined areas-of-interest (aois) were used as measures for visual attention. two aois were placed in each study map of both experimental conditions (reduced and complete maps). the first aoi covered the displayed route and the area that was not displayed transparent in the first experimental condition (reduced maps). the second aoi covered all areas that were not covered in the first aoi (areas offside the route). this enabled us to compare the visual attention towards the displayed route (aoi 1) and other map areas (aoi 2) between the two experimental conditions. in eye-tracker studies, completeness of gaze data is an important quality criterion. droopy eyelids and positioning the head outside of the tracking area of the eye-tracker may lead to gaze data loss [cit] . such data loss may cover up important information about visual attention towards specific areas of a stimulus. therefore, eye-tracker recordings with massive gaze data loss should be removed from analysis. according to bojko [cit], a gaze data loss threshold between 10% and 30% may be selected. based on this suggestion, we defined a threshold of 25%."
"there are additional aspects beyond route recognition performance and visual attention that could be affected by map reduction. as shown by dutta-bergman [cit], trust in information is affected by its completeness. this raises the question whether map reduction can affect the credibility of the map, because users expect relevant information to be missing. obviously missing map elements could on the other hand also, as mentioned in the introduction, lead to an open-world assumption. being aware of the incompleteness of the map could lead to a higher flexibility when representations of real-world objects in the map are inaccurate or unexpectedly missing [cit] . therefore, deliberately activating an open-world assumption via directly recognizable map reduction may be advantageous if the accuracy and completeness of map information cannot be guaranteed. effects of map reduction and the activation of an open-world assumption on trust in the displayed content and the use of inaccurate and incomplete information could be investigated by confronting people with deliberately incomplete and inaccurate maps of familiar environments."
"for the future work, we may extend it to a hashing-based methods [cit] for this classification task, due to their high efficiency in storage and retrieval."
"for future work, the resolution of levels of expertise will be increased to include levels between expert and novice. this can be achieved through having an external reviewer grade the performance of each subject thereby allowing for a more accurate representation of the participant's level of expertise. with an increase in the levels of expertise, simulations can be better fit to the learner and increase the efficacy of the learning objectives. additionally, the differences between the signals recorded from experts and novices when dealing with the effects of adding visual complexity to the simulation through ar elements should be investigated. future visual enhancements will be used to modulate the symptomology of the patient, to alter the difficulty of the simulation. the efficacy of these ar objects on successfully increasing cognitive load of the participant is crucial for the development of a dynamic simulation that can successfully increase the level of expertise of trauma responders. the optimal level of cognitive load that the participant should be put under to reach the desired learning objectives will also be investigated to successfully implement our proposed adaptive simulation framework."
"in order to prevent response biases, no information about the study purpose was given to the participants before or during study participation [cit] . they were told that information concerning the study purpose would be provided after the experiment. before the experiment started, the procedure was explained and the participants gave informed consent. hereafter, they took a seat in front of a tobii tx-300 (300 hz, 23 inches) eye-tracker monitor that was used to visualize the stimuli. the distance between the eyes and the monitor was 65 cm."
"the cost of software development is always higher for more reliable systems. consequently, the desired reliability should be determined depending on the criticalness of failure-free operation of the system. for example, the failure rate of a life-threatening system such as heart-monitor should be very low while a company website may have a higher failure rate."
"the estimated parameters for srgms are shown in table iii . in this case, the results show that the dssm model was able to provide the best results in terms of mmre while both the expm and the powm models have better vaf values as shown in table iv ."
"dirty paper coding is a technique to mitigate the effect of interference at the receiver [cit] . lattice coding methods can be used to achieve the well-known result \"known interference does not reduce capacity\" on the awgn channel [cit], and a simplified summary is given here."
"the system simulation diagram is shown in figure 4 with a 2-bus 500 kv power system. the Â±100mv ar statcom is implemented with a 48-pulse vsc and is connected to a 500 kv bus as shown in figure 3 . a general fault generator is implemented at bus 2, which results in a voltage dip at the statcom bus. attention is focused on single line-ground faults and statcom performance with the proposed \"emergency pwm' concept in this section. results given in per unit values, with 1.0 p.u as 500 kv. during steady state operation vsc voltage is in phase with system voltage. if the voltage generated by the vsc is higher (or lower) than the system voltage, then statcom generates (or absorbs) reactive power. the amount of reactive power depends on the vsc voltage magnitude and on the transformer leakage reactance. varying dc bus voltage controls the fundamental component of vsc voltage. in order to vary dc voltage and therefore the reactive power, the vsc voltages angle (alpha), which is normally kept at close to zero, is now phase shifted. this vsc voltage may lag or lead and produces a temporary flow of active power, which results in, increase or decrease of dc capacitor voltages. with help of emergency pwm the output voltage distortion and capacitor ripple current can be reduced to any desired degree. thus static var generator, employing a perfect voltage sourced converter, would produce sinusoidal output voltages, would draw sinusoidal reactive current from ac system under critical conditions. it is varying from inductive to capacitive within 0.2 to 0.3sec, which shows statcom supplying adequate reactive power under fault condition. fig.9 shows the statcom controller voltages and currents, which are in permissible limits under fault conditions. fig.11 shows dynamic response of statcom under lll-g faults, as these faults are sever, at this condition the inverters currents are very high than rated, but still statcom continuous to be in online without tripping and after a particular interval of time system comes to steady state. this is verified through fig 11. the moment fault occurred, bus voltages start to fall but statcom responds abruptly and commence within minimum interval of time and starts supplying reactive power, these can be seen in fig 13. from fig.14"
the power model is also known as the non-homogeneous poisson process (nhpp) [cit] . the equations that govern Âµ and are given in equations 3.
"an n-dimensional lattice Î» is an infinite additive group, and it is a subgroup of r n . a lattice is defined by a generator matrix g, and is the set of all points:"
"this model is known as yamada delayed s-shaped model [cit] . the model is a finite failure model. [cit] provided this model for error detection, in which the observed growth curve of the cumulative errors has an sshape. the system equations for Âµ(t; b) and (t; b) are given in equation 4 ."
"our methodology was employed to estimate the parameters of three adopted srgms: the exponential model, power model, and s-shaped model. then the proposed models were applied to three real measured test/debug datasets. the results show that the proposed methodology is able to successfully estimate the parameters of srgms. for verification, a convergence behavior analysis was conducted. the results verify the effectiveness of the gwo algorithm to solve the problem with highly accepted performance. for future work, we plan to explore other techniques for modeling the software reliability growth based on other search algorithms in an effort to improve performance."
"the proposed solution is based on \"emergency pwm\" mode, where the vscs will individually detect and self implement pwm switching to control their phase (vsc pole and device) currents within predetermined limits. each vsc will ensure that its over-current limit is not reached during and after a system fault, and under any bus voltage condition (including negative sequence and harmonics). this control strategy enables the statcom to remain online and recovering from a system fault, when its v ar support is required the most. fig.6 and fig.7 shows the vsc phase voltages and currents under normal and faulted conditions with \"emergency pwm\". the phase current rapidly increases at the onset of the fault and is typically higher than the over-current limit of the vsc devices. this \"emergency pwm\" concept is illustrated in such a way that the vsc phase voltage is modulated to control the phase (vsc pole and device) current during the fault. it is seen that the vsc phase current is controlled such that the statcom still delivers required reactive power (or current) during the fault. the extra switching's in the vsc will result in higher losses during this period. however, the priority is to keep the statcom online to support the bus voltage during and recovering from system faults."
"wom coding can be viewed from the perspective of latticebased communication in the presence of known-interference, also called dirty-paper coding. the similarity is that the state of the memory plays the role of known interference as in dirtypaper coding."
"in sec. ii, background on lattice codes and fundamental regions is given. in sec. iii, a simplified description of dirty paper coding, lattice wom codes, and the comparisons, are given. in sec. iv, the proposed lattice wom code construction is given. in sec. v, numerical results are given. the paper concludes with discussion in sec. vi."
"in this way, this lattice wom code can be seen as a kind of dirty-paper code. the current state of the memory s plays the role of the known interference. in dirty-paper coding, the shaping region is a voronoi region, to satisfy the average power constraint requirement. in the case of the wom code, the shaping region is cubic with only non-negative values, to satisfy the restriction that cell values can only be increased."
it is assumed that each candidate solution with dimension n is represented by the vectorx such that the grey wolf position vector is given as:x
"in the literature, many methods are introduced to estimate and predict software reliability [cit] . the proposed methods can be classified into two main categories [cit] . the first category is software reliability prediction models and the second category is software reliability growth models (srgm). software reliability prediction models consider predicting the reliability early in the development life cycle. in the requirements, design or implementation phases, the model uses historical data and some quantitative measurements like lines of code (loc) and depth of nesting loops to estimate the failure rate. examples of software reliability prediction models include the orthogonal defect classification model [cit] and the constructive quality model [cit] . some reliability models may be based on software architecture and others on modified adaptive testing [cit] . the second category, srgm, represents how the system reliability changes over time during the testing phase and based on test data. srgms collect defect data and statistically correlate this data with known mathematical functions to predict software reliability [cit] ."
"during the hunting process, the grey wolves surround the prey (i.e. solution of the problem). this surrounding behavior in gwo can be represented mathematically as follows:"
"our experiments explore the use of the gwo method to estimate the parameters of three software projects using three srgms. in each case, we estimate the model parameters for expm, powm and dssm models, generate the convergence curves using the gwo method and show the scattered plot."
"the gwo algorithm is based on the wild behavior of the grey wolves during hunting. according to the dominant hierarchy leadership order, the gwo divides the animals' population into four categories: alpha (âµ), beta ( ), delta ( ), and omega (!). consequently, the optimization process, the same as the hunting, is guided by the highest rank leaders: âµ, and respectively which represent the best three solutions in the search space. the ! wolves, the lowest in the hierarchical rank, represent the rest of the solutions that must adjust their positions to follow the other dominant wolves."
v. experiment results to develop our new technique for solving the problem of estimating the parameters of srgm we used gwo matlab toolbox. we started by setting the number of search agents (grey wolves) and the maximum number of iterations for the experiment. from our experience we found that 30 agents and 50 iterations led to highly accepted results. the objective function gets the variables as a vector ([x 1 x 2 ...x n ]) and returns the objective value.
"however, wom codes and wireless communication systems have different power constraints. wireless systems have an average power constraint on x, but x may take on positive and negative values and there is no constraint on x + s. in lattice wom codes, there is a peak power constraint on x + s, and x must have positive values. the peak power constraint corresponds to the maximum value v that the flash cell can store. the positive-value constraint corresponds to only allowing increases in the cell values. accordingly, in this paper, a lattice wom code is designed using dirty-paper coding lattice strategies, but the fundamental region is tuned for the restrictions of the problem. the proposed lattice wom code is shaped using a cubic (or rectangular) fundamental region with only positive values. the criterion for evaluation is average number of writes. in order to improve the average number of writes, \"coset select\" bits are introduced. this results in a one-to-many mapping from information bits to codewords. then, codeword which maximizes the average number of writes is selected."
"assume we have n measurements which represent the cumulative number of failures found at time t i where t i is the accumulated execution time. then Âµ(t i, b) can be defined as the projected number of failure at time t i by a model."
"in this paper, we proposed a gwo-based methodology to estimate the parameters of software reliability growth models (srgms). the estimated model parameters are used to predict the accumulated failures in a software system during the testing process. the problem is formulated for the gwo algorithm with the objective of minimizing the difference between the actual failures and the estimated accumulated failures."
which does satisfy the power constraint. the received signal is the above expression plus s. the receiver correctly finds the intended codeword x by computing the received signal mod v Î» s :
"to overcome this problem, the message b is formed of information bits u, and non-information \"coset select\" bits c. this reduces the amount of information that can be encoded, while increasing the number of cosets. the encoder will select the coset which has the codeword that maximizes the number of future writes. a mapping function Ï will be introduced that distributes these points in such a way that this problem is reduced, and the average number of writes is increased."
"where t represents the number of iterations andÃ£ 1,Ã£ 2 and a 3 are random vectors that vary to allow the wolves to attack towards the prey. finally, the hunting process ends when the grey wolves attack the prey after it stops moving. in the next sections we show how to utilize the gwo to estimate the parameters for number of srgms."
the parameter b 0 is the expected total number of failures recovered at the end of the testing process (i.e. v 0 ). b 1 represents the rate at which the defect rate decreases (see equation 2 ).
"this paper takes the perspective that lattice wom coding is a type of dirty-paper coding problem. the current state of the memory s, which is known to the encoder, plays the role of the known interference. encoding is performed modulo a shaping lattice with respect to a fundamental region to obtain a codeword x. in wireless systems, the interference is implicitly added by the channel, but in flash memory systems, the state of the memory is explicitly changed to x + s."
"wherex âµ,x andx are the positions of the âµ, and, respectively,x is the position of the current solution and c 1,c 2 andc 3 are random vectors. then, the final position of the current solution can be calculated as in equation 10 ."
"multi-level flash memories allow storing one of q multiple values in each of n cells. these values can easily be increased, but must be decreased using an expensive erase operation. furthermore, erase operations cause the flash memory to physically deteriorate and eventually fail. write-once memory codes, or wom codes [cit], are now being investigated by the coding theory community in order to mitigate this problem."
"the inability to meet software requirements and/or deviation from the goal for which the software was developed is defined as software failure. software reliability depends mainly on the way we handle failure. for example, detecting failure during execution and repairing it increases the reliability of the software as a function of time. this is what happens during the software testing process and before release of software to the market. software reliability growth models (srgms) are the models concerned with the explanation and the description of software failures."
"using the lattice-strategy approach for dirty-paper coding [cit], a specific code construction was proposed. but the power constraint particular to each problem imposes restrictions on the design of the code. the code is designed using a lattice fundamental region as a shaping region for the finite code. for dirty-paper codes, the average power constraint is satisfied by the voronoi fundamental region. but for lattice wom codes, the positive-only codeword requirement is satisfied by a cubic shaping region. if we take low-rate cases where m v, then the result \"known interference does not reduce capacity\" may be applied to wom codes, because it allows fairly ignoring the maximum writing level v, but this parameter choice is not practical. a specific lattice wom code was constructed. coset select bits were included in order to improve the average number of writes. the numerical results show a promising trend."
", is defined as the failure intensity (t). in the following subsections, we briefly describe three well-known srgm models that we use in our study."
"in this paper, we utilize the grey wolf optimization (gwo) algorithm to predict faults during the software testing process using software faults historical data. the rest of this paper is organized as follows: in section ii, we briefly introduce some srgm models that we use in our study. section iii provides an overview of the gwo algorithm. section iv shows the evaluation criterion adopted in this study. the experimental results developed for parameter estimation of software reliability are given in section v. finally, we provide the conclusions and future work in section vi."
"a seemingly different problem, communication in the presence of known interference, has also received attention because of the result that interference known to the encoder does not reduce capacity [cit] . dirty-paper coding provides a method to reduce the effects of interference, for example, in wireless communications settings. erez, shamai and zamir showed that this research was supported in part by the ministry of education, science, sports and culture; grant-in-aid for scientific research (c) number 23560439."
"flash memories use error-correcting codes. when a binary error-correcting code is applied to multi-level flash memory, typically using gray coding, the codewords form a sphere packing [cit] . such sphere packings are not linear. on the other hand, lattices are linear sphere packings. clearly, linear structures have a certain advantages over non-linear ones, and the error-correcting properties of lattices have been extensively studied [3, ch. 3] . recently, studies of the wom properties of lattice-based codes have begun [cit] . a lattice-based wom code inherits the error-correcting properties of a lattice, and thus is an appealing structure that both corrects errors and allows rewriting."
"this paper describes dynamic performance of statcom when it is subjected to l-g and lll-g faults. the operating characteristic of compensator during steady state, capacitive and inductive modes of operation has been reasonably acceptable and competitive for design of an economical dynamic static compensator and by implementing \"emergency pwm\" strategy statcom gains capability to prevent overcurrents and trips in the vsc based statcom. simulation results are presented for a 48-pulse vsc based Â±100 mv ar st atcom connected to a 2-bus power system. bus voltages, and primary injected currents of statcom, under normal and faulted conditions shown in detail. in addition to this a nonlinear load is connected and operated at no fault conditions, and harmonics are eliminated in the source current. this enables online operation of the statcom and supplies required reactive power when it is most required. thus the performance of statcom has improved with the new control strategy."
"the flexible ac transmission systems (facts) controllers are emerging as an effective and promising alternative to enhance the power transfer capability and stability of the network by redistributing the line flow and regulating the bus voltages. static var compensator (svc) and thyristor controlled series compensator (tcsc) are some of the commonly used facts controllers, the developments in the field of power electronics, particularly gate turn-off (gto) based devices, have introduced a new family of versatile facts controllers, namely static synchronous compensator (statcom), the statcom is one of the custom power devices that received much attention for improving system stability, with the development of power electronics technology, custom power devices play important role in bringing unprecedented efficiency improvement and cost effectiveness in modern electrical power system [cit] . the custom power is relatively new concept aimed at achieving high power quality, operational flexibility and controllability of electrical power systems [cit] . the possibility of generating or absorbing controllable reactive power with various power electronic switching converters has long been recognized [cit] . the statcom based on voltage source converter (vsc) is used for voltage regulation in transmission and distribution systems [cit] . the statcom can rapidly supply dynamic var's during system faults for voltage support. in this paper, we propose and develop an \"emergency pwm\" strategy to prevent over-currents (and trips) in the vsc during line to ground faults, all though pwm technique results in higher switching losses but it recompense total system loss. this limitation of implementing vsc with pwm functionality, results in avoiding over-currents and trips of the statcom supplies required reactive power. with \"emergency pwm\" strategy statcom gains capability to prevent over-currents and trips in the vsc based statcom. simulation results are presented for a 48-pulse vsc based Â±100 mvar statcom connected to a 2-bus power system. the operating characteristic of compensator during steady state, capacitive and inductive modes validate \"emergency pwm\" strategy [cit] to prevent vsc over-currents and to supply required reactive power under line to ground system faults [cit] . fig. 1 shows the 48-pulse voltage source converter topology for st atcom application. the vsc consists of four (lnv 1 -inv4) 3-level neutral point clamped (npc) converters which are connected in series by four (tl-t4) transformer coupling. the primary side of the transformer is connected in series as shown in fig. 1 . due to the strict loss outlay for statcom application, each vsc is operated at fundamental frequency switching or in square-wave mode. the gating of vscs is phase-shifted so as to yield 48-pulse output voltage waveform with series transformer coupling on the primary side. the performance of the statcom under system faults (such as single line-ground faults) results in converter over currents and statcom trips. fig. 2 shows the phase b bus voltage dips for 4 cycles due to line-ground fault in the system. it has been noticed that primary statcom currents are large during system faults and results in st atcom tripping. examining further, it is seen that the vscs \"stop gating\" during the fault due to over current strategy and enable the statcom to remain online, but cannot prevent the st atcom trip recovering from the fault. it is realized that the var support functionality of the st atcom is required the most during and after a system fault. this problem is the motivation for this work. shows the 2-bus 500 kv power system simulation model with 48 pulses implemented vsc based Â±100 mvar statcom. fig.4 shows the implemented angle controlled (Î±) statcom controller. an inner feedback loop is used to regulate the statcom instantaneous reactive power current iq shunt, reminding that this control is achieved only by controlling Î±, of the inverter output voltage relative towards the transmission line voltage, this technique makes it possible to maintain a constant maximum ratio between the inverter output voltage and the vsc dc-capacitor. the reference value for the reactive current control loop is generated by an outer loop responsible for the system voltage control (vbus_ref). this outer control loop is similar to that used in conformist static var compensators, and includes an adjustable slope/droop setting that defines the voltage error at full statcon reactive output. there is an unavoidable delay in the feedback of the voltage-regulating loop because of the time taken to compute the positive sequence fundamental bus voltage (vbus). as a result an extremely fast response (typically 1â4 cycle) can be achieved for the reactive current controller (iq. shunt), the response time of the voltage regulator is typically about half cycle of the line voltage."
"in the second case study, a real-time application [cit] of a software system containing 200 modules of fortran language was used to test our proposed methodology. the data consists of 111 measurements [cit] . we ran the gwo to tune the parameters of expm, powm and dssm. figure 2 (a), we show the actual and estimated accumulated failures curves for the expm, powm and dssm models and the convergence curves of the gwo process for the three developed models. a scattered plot of the three developed models is shown in figure 2 (b) . table ii shows the estimated parameters for the srgm models together with the model equations. the computed evaluation criterion are included in table iv . based on the developed experiments for this case, the results show that the dssm model provided the best performance using the gwo tuned parameters as it has the minimum mmre and the maximum vaf compared to other proposed models."
"where x is the intended codeword. because noise has been ignored, this explanation also ignores lattice inflation and random dither needed to achieve capacity on awgn channels. however, it captures the essential idea of dirty paper coding using lattices."
important to measure the model accuracy in terms of some meaningful measurements. in our case we adopt the goodnessof-fit criteria. these criteria are applied to measure the quality of the solution provided and determine the proximity of the estimated failures to the measured failures.
"infinite lattices Î» are distinguished from finite lattice codes c. labeling is assigning information labels, integers or bits, to the members of c. when Ï is the voronoi fundamental region v a labeling scheme exists [cit] . however, it requires computationally demanding quantization."
"the tutorial will interest both researchers (primarily related to tracks identified below) as well as technologists. the talk covers relatively new frontiers in research demonstrated via examples and also covers research efforts that have matured to large-scale deployments. although the content analysis portion of the presentation will be discussed in-depth, attendees are not expected to have any specialized background and/or knowledge (use of examples will make it easier to gain technical insights). those with a background in more traditional text analysis will be able to understand the novel technical challenges dealing with short and informal text as well as other aspects of social data.."
"from subject/area perspective, this tutorial will interest participants from multiple tracks: â  social systems and graph mining -e.g., issues of information diffusion â  bridging structured and unstructured data -e.g., extensive and synergistic use of both structured metadata, web of data, background knowledge and unstructured data â  content analysis -e.g., spatio-temporal-thematic-sentimentintension analysis with specific challenges in social data â  semantic web -e.g., how use of background knowledge improves machine learning and statistical nlp techniques resulting in improved ner on informal text â  web of emerging regions -coverage of not only twitter and web/mobile web but also sms extensively used in developing nations, including applications related to development and crisis management which is of special relevance to emerging regions"
"in this annual special issue, totally we have received 25 submissions, coming from different countries all around the globe in response to call for paper. [cit] iet international conference on frontiers of internet of things (fit), which was held in hsinchu, taiwan, on december 4-6, 2014. each accepted article has been reviewed by at least three reviewers. in the end, nine articles are revised and selected for publishing in this annual special issue. we believe that the accepted papers present the most up-to-date progress in algorithms and theory for robust wireless sensor networks with respect to different networking problems."
"this tutorial weaves together three themes and respective topics: 1. opportunity to exploit the massive amounts of social data that have resulted from the participation of millions of users through a wide variety of on-line interaction mechanisms and types of social data. some examples are sms through simple mobile phones, tweets with time and location coordinates, as well as multi-sensory metadata from smartphones. 2. people-content-network view of analyzing social media use, including technical challenges and recent research efforts in extracting metadata from casual/informal user-generated content on social media platforms. 3. experiences in building robust and scalable platforms and applications to serve the needs of (a) users at the top of the pyramid (1.5 billion users with smart phones and modern network access) as well as (b) users in the middle of the pyramid (3 billion users with mobile phones without access to internet or modern networks)."
"the special issue \"algorithm and theory for robust wireless sensor networks\" [cit] 's special issue, we are honored to be invited again to make this special issue (a new issue) [cit] 's annual special issue, which means that it will become the first issue in a series of special issues which will be published each year in this journal. we hope that such a series can have a longterm impact and in time gather a wireless sensor network community around it."
"the tutorial will provide breadth and depth of research on some topics matched by real-world applications as well as a combination of perspectives from academic, industry and nongovernmental organizations."
"(a) ushahidi (ushahi.org), emoksha (emoksha.org), kiirti (kiirti.org), sahana (sahanafoundation.org), and other real-world platforms for development-centric applications and crisis management in developing countries. we will borrow from case studies at harvard, columbia and institutions worldwide on the use of emoksha/kiirti projects involving social media and technologies for development. [cit] (c) twitris -application that supports spatio-temporal-thematic analysis and extraction of social signals from microblogs, also complemented by news, web 2.0, and image/video sources (http://twitris.knoesis.org) [cit] (d) twarql -annotations and management of streaming tweets, encoding information from microblog posts as linked open data for collectively analyzing microblog data for sensemaking (http://wiki.knoesis.org/index.php/twarql) [cit] part 3: metadata extraction from user generated data and analysis techniques a variety of techniques used to exploit metadata, support analysis and achieve insights will also be discussed with a related literature review. this will include the unique role of semantic web standards, technologies and techniques for building advanced capabilities. [cit] intended audience and coverage"
"for secured web applications that want to cache resources provided by secured content delivery networks (cdns). the recent standard [cit] relaxes this restriction: when a manifest's scheme is https, a web browser can cache any https urls but no http urls. an exception is safari because it does not use the recent standard changes at the time of writing this paper."
"3) the browser re-downloads each of the resources while firing a progress event for each resource. if the browser cannot cache at least one of the resources or if the manifest changes during re-downloading, it terminates the update procedure and fires an error event. 4) the browser stores the re-downloaded resources in its local storage and fires an updateready event."
"in this paper we present a methodology that, after unsupervised clustering, automatically assigns clusters to cell type based on a systematic, unbiased, voting algorithm. our method does not rely on a human expert empirically selecting a set of markers to interpret the results, but uses all the information available in a large markers database to predict cell types. while cell type identification by manual interpretation can provide good results, the proposed methodology assures that all the available information is taken into account in an unbiased way, and it allows for the identification of many datasets in parallel. from an algorithmic point of view, voting algorithms are among the simplest and most successful approaches to implement fault tolerance and obtain reliable data from multiple unreliable channels [cit] . the idea can be traced back to von neumann [cit], and since then it has been practically used in many error correction computational architectures. the voting algorithm employed here belongs to the class of approval voting algorithms. for a given cluster, each participant (a cell marker) votes for a subset of candidates (cell types) that meet the participant criteria (significant rna expression) for the position rather than picking just one candidate. the approval vote tally determines the score that we use to assign the cluster to a cell type."
"within two subsequent steps, we commit alternative realizations for edges, being directed and undirected. in step 7, a corresponding xor-group is introduced to the feature model. in the domain model, a realization for directed edges is added: two associations starts at and ends at with multiplicity 1 at the vertex end. the realization for undirected edges is committed in step 8 under the ambition undirected: an unspecific association connects with multiplicity 2 at the vertex end. since the features directed and undirected are mutually exclusive, it is ensured that no version containing both realizations may be derived."
"we analyze differences in appcache implementations of different web browsers. due to the differences, some web browsers are more vulnerable to our attack than others, and some other web browsers are robust against our attack explained in section iv."
"inspecting the repository. during the described check-out/modify/commit iterations, the management of visibilities has been completely automated by the mechanisms described in section 4.4. as a consequence, a supermod user never has to inspect or modify visibilities manually, as it had been necessary in the initial example. nevertheless, it is interesting to inspect the superimposition and the visibilities defined in the repository for a comparison with the manually defined mapping. figure 11 depicts the internal state of the repository after step 9. visibilities of the multi-version feature model only contain revision options, since the feature model is only versioned by the revision graph and not by itself. visibilities of the mvdm are hybrid: they contain a revision part (which corresponds to the version history shown in table 1 ) and a feature part (which is equivalent to the mapping shown in figure 5 )."
"we present some countermeasures that partially prevent our attacks or that prevent our attacks but lead to other problems. first, we can revise appcache to ask user permissions to allow web applications to cache resources as firefox does. this countermeasure prevents our attacks only if a user correctly judges whether a web application is malicious."
"fractions of cells of various cell types. the latter provide a snapshot of the cellular composition of the 8 bone marrow samples, see fig. 7 ."
"we state the types of urls that appcache does not cache and returns errors. using such information allows us to identify the status of a target url, which will be explained in section iv. appcache does not cache urls that satisfy any one of the following three conditions."
"the current version of supermod allows to answer research questions referring to the added value of transferring the vc editing model to sple. however, there are some questions that remain to be answered by future work:"
"in this section, we compare terms and notions of vc and sple that have been used in the previous two sections. this comparison motivates the prototype supermod, which is described in section 4. table 2 summarizes the discussion below. equivalent concepts. both vc and sple provide an abstraction for the entirety of product versions. in vc, this is a repository, whereas in sple, this corresponds to the platform. for single product versions, the terms workspace and product (configuration) are used, respectively. as mentioned in the introduction, in both disciplines, there exist two distinct representations. on the one hand, it is possible to store all variants as a superimposition, which corresponds to symmetric deltas in vcs and to negative variability in sple. on the other hand, only a minimal core may be defined, which is then extended. this is realized by directed deltas in vc and by positive variability in sple. in both cases, it is necessary to assign visibilities either to program fragments or to transformations. these correspond to version identifiers (sets or ranges of revisions), and to presence conditions, respectively. similar concepts. in both disciplines, there is an abstraction for the set of available versions. in vc, revision graphs describe the commit history. in sple, feature models organize mandatory and/or optional features of a product line within a tree. the specification of a single version is done by selection of a revision, or by a feature configuration, which in turn describes a product variant. in both disciplines, a filter operation is realized. in vcs, it populates the workspace after a revision has been selected for check-out. in sple, filtering is applied as product derivation during application engineering."
"we launched our attack on the recent versions of five major web browsers at the time of writing this paper, and confirmed that all web browsers which strictly followed the appcache standard were vulnerable to our attack (table i) . one exception was safari because it did not properly follow the up-to-date standard (section iii-d1). we reported our findings to mozilla and google, and they agreed that our attack could breach user privacy."
"the paper is structured as follows: after introducing a motivating example, a comparison of vc and sple concepts is performed in section 3. next, in section 4, the implementation and user interface of supermod are sketched and the operations check-out and commit are formalized. subsequently, the example is reconsidered. section 6 outlines related work, before the paper is concluded."
"in this paper, we have presented supermod, a modeldriven tool that combines the management of variability in time and variability in space, i.e., version control (vc) and software product line engineering (sple). typical sple processes distinguish between domain engineering, where a platform and a variability model are defined, and application engineering, where variability is resolved to automatically derive specific products. in contrast, in vc, software is developed iteratively. supermod bridges this gap by transferring vc metaphors to sple. for the selection of versions during check-out and commit, feature configurations are specified in addition to a selection among the revision graph. the mapping between the platform and the variability model is managed automatically."
"version rules. the set of available choices and ambitions is constrained by a set of version rules, logical expressions over the option set. version rules are used, e.g., in order to implement constraints such as mutual exclusion within feature models, or to designate subsequent revisions."
"we have presented a methodology that, after unsupervised clustering of scrna-seq data, automatically assigns clusters to cell types based on a voting algorithm without manual interpretation by an expert curator. the method provides the classification of individual cells into predefined classes based on a comprehensive database of known molecular signatures, i.e. cell surface (extracellular) and intracellular markers [cit] . the proposed methodology assures that extensive marker/cell type information is taken into account in a systematic way when assigning clusters to cell types. moreover, the method allows for a high throughput processing of multiple scrna-seq datasets since it does not involve an expert curator."
"an example where a correct cell type assignment requires the analysis of multiple markers is shown in fig. 1, where we analyzed single cell data from the bone marrow of the first donor from the hca (human cell atlas) preview dataset [cit] using t-distributed stochastic neighbor embedding (t-sne) layouts. after clustering ( fig. 1 (a) ), the pattern of cd4 expression ( fig. 1 (b) ) suggests that cluster 1 (red) and cluster 2 (light green) are both highly enriched for cd4+, potentially indicating t helper cells. in these cells, the expression of cd4 is crucial for sending signals to other types of cells and they are often just called cd4 cells. however, a more careful analysis of cluster 2 shows a significant expression of cd14, cd33 and cd52 ( fig. 1 (c-e)) that indicates that this cluster consists more likely of macrophages/monocyte cells."
"this section sketches the model-driven implementation of supermod. first, we explain theoretical foundations developed in advance. thereafter, the architecture is described at a coarse-grained level, before we detail the specification by means of a metamodel. next, the operations check-out, modify and commit are specified. last, we discuss current limitations and address future tool improvements. the tool is available for evaluation purposes as an eclipse plug-in (see installation instructions at the end of this paper)."
"bottom line. vc and sple share an unexpectedly large amount of similarities, particularly with respect to underlying data structures. most differences are due to the underlying editing models. as we will explain within the subsequent section, supermod eliminates these differences by transferring the filtered vc editing model to sple. in the workspace, the distinction between variability in time and variability in space is blurred, offering the user new ways of versioning, such as committing a change against a dedicated feature. in particular, the desirable properties of unconstrained variability, intensional versioning, automatic visibility management, and immutability of (temporal) version membership are transferred."
"differences. both disciplines deal with different kinds of variability. version control manages variability in time, i.e., the fact that a software project is subject to evolution. sple, in contrast, deals with variability in space, using variability models to describe commonalities and differences among related variants explicitly. in sple, it is intended that several configurations of a software project co-exist. this kind of variability has to be planned in advance by suitable variation points in the platform. most sple tools require the platform to be free of context-free or context-sensitive conflicts, e.g., a syntactically correct program that is accepted by the respective compiler, or a valid instance of the metamodel in the case of mdple. in vc, there are no restrictions concerning product variability: neither a superimposition nor directed deltas need to be syntactically meaningful; constraints are merely imposed to single-version products. vc and sple also differ in terms of version specification. typically, a vcs fixes the set of versions available for selection (extensional versioning, see [cit] ). in sple, versions may be described by a combination of features, allowing to create versions that have not been committed earlier (intensional versioning). in vc, filtered editing is applied. after a check-out, the developer sees and may modify only elements belonging to the selected revision. as soon as a commit is issued, changes are detected in the local workspace, and written back to the repository, while visibilities are updated automatically. in contrast, sple typically requires the user to edit a multi-version view (unfiltered editing) and to manage visibilities (i.e., presence conditions) manually. vcs guarantee the immutability of version membership of an element: once committed, it is not possible to remove an element from a revision. in contrast, it is allowed to modify the visibility of an element arbitrarily in sple."
"the mdple tool feature mapper [cit], which is based on negative variability, offers the possibility of change recording during domain engineering. having selected one or several features and invoked the record operation, all changes performed are associated with a feature expression derived from the provided feature selection. however, only insertions are supported, and change recording is restricted to gmf-based editors."
"check-out. for check-out, the ui offers two distinct commands: switch prompts the user for a choice in both the revision graph and the feature model, whereas update automatically generates a new choice whose temporal component is updated to the latest available successor of the current revision. in general, the operation check-out has been realized as follows:"
"although the main purpose of the explained tags is to include content with specific types, attackers can abuse the tags to obtain side-channel information by including arbitrary content and checking an error status while measuring latency. web browsers cannot determine the content type of a url until they receive an actual resource, so they send a normal get request to a web application to fetch the resource. when the content type of the received resource differs from the tag type, the web browsers abort the content inclusion and fire an error event. however, attackers can infer the status of a url from its fetch latency because the latency varies for various reasons, such as whether the browsers have previously visited the url, whether the browsers are logged in, and whether the url exists. attackers can thereby guess sensitive information by using information implied by the varied fetch latency."
"in this section, we introduce two appcache attacks that manipulate dns information: appcache poisoning [cit] and appcache-based dns rebinding [cit] . appcache poisoning attempts to store fake login pages in appcache to steal login credentials. when a victim web browser visits some web pages via an attacker's network (e.g., a rogue ap), the attacker injects hidden iframe tags that point to target login pages in responses. the victim web browser then sends requests to the target login pages. the attacker intercepts the requests and responds with fake login pages that look the same as the original login pages while declaring an appcache manifest and including backdoors. later, even when the victim web browser visits the target login pages via a secured network, it will load the fake login pages from appcache. to mitigate this attack, we need to use private browsing modes [cit] in an insecure network, and use http strict transport security (hsts) [cit] or https everywhere [cit] to secure login pages."
we first describe the appcache download procedure for a newly-visited web page and the corresponding events that are fired during the procedure. a web browser initiates the following download procedure when it visits a web page that declares an appcache manifest for caching specific resources.
"realization of optional parts. in steps 5 and 6, the optional features labeled and weighted are introduced. in addition, their realization is provided by performing the following changes in the local workspace: the feature labeled is realized as an attribute label, and the feature weighted as weight, both located in the class edge. the change performed in step 6 would also have been applicable in a feature configuration where labeled is selected, since the features weighted and labeled are mutually independent."
"1) the browser attempts to fetch and interpret the manifest originating from the remote server while firing a checking event. first, if the content of the manifest does not change, the browser terminates the update procedure and fires a noupdate event. next, if the manifest either has errors or is unreachable due to network failures, the browser terminates the update procedure and fires an error event. lastly, if the manifest no longer exists in the remote server, the browser terminates the update procedure, deletes the cached resources, and fires an obsolete event."
"we depict a timing attack to identify internal web servers located in the local network of a victim web browser [cit] . the basic idea of this attack is using html tags (e.g., the img and script tags) to include arbitrary urls of internal web servers while waiting for onerror events. attackers can guess the servers' status from the elapsed time."
"although appcache restricts standard url redirections that use 3xx status codes, it ignores non-standard redirection methods (e.g., the meta refresh tag and the javascript object window.location). when appcache encounters a web page that uses such a non-standard redirection method, appcache does not follow a redirection, but caches the web page \"as is\"."
"1) specific content inclusion: html provides tags (e.g., img, script, and link) to embed specific types of sameor cross-origin content in a web page, such as images, scripts, and csss. the tags successfully include a url that indicates a valid resource with a matched content type. but, the tags fail to include a url when the url is invalid (e.g., connection failure, non-existent resource, and unauthorized access) or the url indicates a resource with an unmatched content type. finally, web browsers call either the onload or onerror event handlers according to successful or unsuccessful content inclusion via the tags."
"options. an option is a temporal or logical property of a software system, which may or may not be included in a specific version. in supermod, two kinds of options exist: revision options and feature options (see below)."
"platform. the superimposition is defined in the form of a multi-variant domain model (mvdm) [cit] . we realize the platform in multiple vc iterations, after each of which a commit is carried out. in table 1, the performed modifications are listed. when referring to the feature model in figure 3, one feature has been realized at a time. figure 4 shows the resulting mvdm."
"realization of common parts. in step 2, both the feature model and the domain model are evolved. we add the root feature graph and its mandatory subfeatures vertices and edges to the feature model. within the domain model, a class graph is created. since this class realizes the identically named feature, we specify as ambition a partial feature configuration where graph is selected. transparently, the visibility of the added features is set to rev2, whereas the visibility of the class graph is set to rev2 and graph. similarly, realizations of the mandatory features vertices and edges are provided in steps 3 and 4, where the feature model remains unmodified. the performed commit operations result in the visibilities rev3 and vertices for the class vertex and the association has vertices, and rev4 and edges for edge and has edges."
"outlook. the presented example has only scratched the surface of supermod. performed modifications were restricted to element insertions, and the specified ambitions always include one selected feature. these simplifications have been applied for the reason of comprehensibility. we informally sketch a couple of extensions to the example, where supermod is used in a more advanced way:"
"feature mapping. after having defined the variability model and the platform, they need to be connected. in an mdple approach based on negative variability, this requires assigning feature expressions to mvdm elements. a mapping for the example"
"we further describe advanced attacks that leverage the url status identification attack. first, we can determine the login status of a victim web browser. many web applications have web pages that (1) redirect a browser to a login page if the browser has no login information or (2) return an error code to a browser if the browser has erroneous login information [cit] . by using such web pages, an attacker can identify which web sites a victim frequently visits and which web pages a victim is authorized to access. when an attacker can determine whether a victim is allowed to access web sites or web pages for specific companies, universities, regions, or groups, the attacker can de-anonymize the victim [cit] and perform context-aware phishing [cit] ."
"added value. when compared to the first version of the example, the cognitive complexity of feature mapping has been significantly reduced. the domain model and the feature model have been developed step by step, while realizations for each feature have been specified directly in the workspace. this is enabled by the uniform version mechanism for temporal and logical variability in supermod, which removes the necessity of repeated annotations. for realizing changes, the developer has used an arbitrary singleversion editor. this is in contrast to many sple tools, which require custom multi-variant editors or additional composition languages, which both disrupt the developers' workflow. the additional advantage of unconstrained variability has been demonstrated above. in sum, supermod removes the drawbacks and limitations that have been identified for an \"off-theshelf\" approach to combined vc/sple in section 2."
"the classification of the hematopoietic and immune system is predominantly based on a group of cell surface molecular markers named clusters of differentiation (cd), which are widely used in clinical research for diagnosis and for monitoring disease [cit] . these cd markers can play a central role in the mediation of signals between the cells and their environment. the presence of different cd markers may therefore be associated with different biological functions and with different cell types. more recently, these cd markers have been integrated in comprehensive databases that also include intra-cellular markers. an example is provided by cellmarker [cit], which will be used here. this comprehensive database was created by a curated search through pubmed and numerous companies' marker handbooks including r&d systems, biolegend (cell markers), bd biosciences (cd marker handbook), abcam (guide to human cd antigens), invitrogen thermofisher scientific (immune cell guide), and ebioscience thermofisher scientific (cytokine atlas). however, using these markers on each single cell rnaseq data for a one-by-one identification would not work for most of the cells. this is fundamentally due to two reasons: (1) the presence of a marker on the cell surface is only loosely associated to the mrna expression of the associated gene, and (2) single cell rnasequencing is particularly prone to dropout errors (i.e. genes are not detected even if they are actually expressed)."
"in this section, we illustrate an appcache-based url status identification attack that does not rely on timing. this attack is possible due to a standard behavior of appcache: to avoid content inconsistency and security problems, appcache should fail when any url listed in a manifest is non-cacheable. by using this attack, an attacker can correctly determine the status of target urls because this attack does not rely on unreliable timing information. we demonstrate both scriptbased and scriptless attacks."
"fourth, we can attach a no-store directive to http responses from web applications. this countermeasure prevents all our attacks, but makes appcache meaningless because web browsers no longer cache resources."
"initialization. we create an empty eclipse project and invoke the share command, which puts it under supermod version control. next, we create an empty uml class diagram and add it to version control. initially, the feature model is empty. the project is committed to the repository as revision 1. since there is no variability defined in the feature model yet, the user is not prompted for a feature configuration -the change is universal."
"our p-dcs consists of two main modules: (a) clustering and (b) cell type assignment, which are both based on an unsupervised approach. we demonstrate our methodology using public bone marrow scrnaseq data from eight donors [cit], that will be referred to as bm1-bm8. in this section, we will illustrate the methodology using the first dataset bm1. the remaining bone marrow data along with a large scrna-seq pbmc dataset, obtained from a different study [cit], are analyzed in sec. results and discussion. in sec. results and discussion we also show how the proposed methodology can be used recursively, so that for each major cell type one can find the corresponding subtypes. fig. 2 shows the workflow of the methodology. the two main modules are identified by the \"clustering\" and \"cell type assignment\" labels. the clustering module is preceded by data pre-processing, and a set of visualization tools is included in the software."
"our attack has three distinguishable features. first of all, our attack can obtain sensitive information without using client-side scripts nor plug-ins. rather, it only uses an html document that declares an appcache manifest which specifies permission to freely reproduce all or part of this paper for noncommercial purposes is granted provided that copies bear this notice and the full citation on the first page. reproduction for commercial purposes is strictly prohibited without the prior written consent of the internet society, the first-named author (for reproduction of an entire paper only), and the author's employer if the paper was prepared within the scope of employment. ndss '15, 8-11 [cit], but all of them rely on cascading style sheets (css) unlike our attack."
"choices and ambitions. as mentioned above, feature configurations are used to specify choices and ambitions in addition to revision graphs as known from state-of-the-art vcs. a feature configuration is always specified on the current revision of the feature model. when specified as an ambition, the feature configuration may be partial and typically binds only few features. the effective choice/ambition is formed during check-out/commit as conjunction of the temporal and logical component, e.g., rev4 and labeled."
"knowing internal web servers is an important privacy breach because it can reveal what kinds of routers, network printers, and nas a victim uses. an attacker can use such information to fingerprint a victim web browser. furthermore, this knowledge becomes the basis of other security attacks, such as dns rebinding [cit] and router reconfiguration [cit] . usually, a firewall protects internal hosts from outsiders such that attackers attempt to make a victim web browser execute scripts to investigate servers in the internal network of the victim web browser."
"in a running example, where a product line of graph domain models has been developed, we have demonstrated many advantages of the vc/sple integration. due to the filtered editing model, the versioning overhead is notably small when compared to existing sple approaches. for workspace modifications, the developer is not restricted by single-version constraints. furthermore, a familiar development environment can be used. intensional version specification allows for the definition of feature configurations as version descriptions. these advantages are boosted by using models as higher-level descriptions of the versioned software system. future work will address the development of a multi-user component, which will advance supermod to a full-fledged distributed vcs. the evolution of the feature model will be subject to research. furthermore, a detailed evaluation against sple tools will be conducted, using a real-world example. the obtained results will be important to understand the impact of the filtered spl editing model on the underlying development processes and tool chains."
"in this section, we explain the html5 appcache in detail. we especially focus on when appcache fails and how appcache handles failures, because they are the most important basis of our attacks presented in a later section."
"the model of the cross-origin web privacy attack resembles that of csrf attacks [cit] . in the cross-origin web privacy attack, an attacker aims to obtain sensitive information of a victim web browser relevant to a target web application by convincing the victim web browser to visit an attacker's web site, which serves slightly malicious web pages. the malicious web pages contain no exploit codes to take control of the victim web browser or to inject malicious scripts into the target web application. instead, the web pages contain legitimate html codes and scripts to include cross-origin content while measuring fetch latency to obtain side-channel information, such as the browsing history and login status of the victim web browser. therefore, it is difficult to determine the maliciousness of the web site."
"no equivalent operation exists in sple tools, which would, e.g., allow to propagate product specific modifications back to the platform. conversely, in sple, it is possible to directly modify the mapping between the variability model and the platform, i.e., the visibilities. to the best of our knowledge, there exists no vcs that would allow to retrospectively modify version identifiers (which would, indeed, destroy the property of immutable version membership)."
"next, through the manifest file, the web application specifies urls that web browsers should cache (listing 2). a manifest file starts with cache manifest and has three sections: cache, network, and fallback. (1) the cache section declares urls that need to be stored in local storage. each scheme of the declared urls should be the same as the main html document's scheme. for example, when the main html document's scheme is http, appcache ignores https urls listed in the cache section. when the scheme is https, appcache ignores http urls listed in the cache section. (2) the network section declares whitelisted urls that web browsers can download from outside. web browsers treat urls listed in neither cache nor network sections as unreachable. we can use an asterisk to allow arbitrary urls. (3) the fallback section declares alternative urls to use when original urls are inaccessible. the first url is the original resource, and the second url is the fallback to substitute for the first one. the fallback section only allows relative urls because replacing a url with another url that belongs to a different origin can violate sop."
"building the matrix m km represents the first step of the voting algorithm. this is equivalent to defining \"ballots\" in which each qualified voter, i.e. the n max (or fewer) markers chosen, has a list of candidate cell types they can approve. for each cluster c, the voting algorithm is then implemented as follows:"
"third, we can revise appcache to check the manifest even when some resources are non-cacheable. this countermeasure prevents a scriptless url status identification attack only when an attacker does not refresh an attack page."
"in addition to determining major cell types, we have shown how this methodology can be applied recursively to obtain cell sub-types. we have performed a congruence analysis of cluster identification obtained by our method with those obtained by expert curators on the same dataset, showing that the automatic assignment is consistent with expert assignment both of major cell types and cell sub-types. while we have focused on the identification of hematological cell types, the software is designed to allow the user to substitute the marker table to apply the methodology to different tissues. sub-clustering of cells from clusters 1, 4 and 6 of the pbmc dataset reveals that the p-dcs automatic sub-type identification is in good agreement with manual annotation. not applicable table 1 comparison of p-dcs and dropclust on pbmc scrna-seq â¼68.6k cells dataset."
"the current paper presents supermod, a modeldriven tool that realizes the conceptual framework in order to integrate temporal and logical versioning. the tool allows to develop a software product line in a single-version workspace step by step using the familiar version control metaphors update and commit. the product line may contain arbitrary model and non-model artifacts. the feature model plays a dual role, being subject to evolution and providing an additional (logical) variability model for the product line. our integrated solution significantly reduces the versioning overhead, since a manual mapping of product line artifacts becomes unnecessary. the tool integrates well with existing eclipse editors."
"conventional cross-origin web privacy attacks have some limitations. first, their accuracy is relatively low due to unreliable page fetch latency affected by a number of error sources, such as network condition, web server loads, and client loads. attackers can reduce the noise by averaging data from a number of timing samples, but this process requires an unreasonable amount of time to collect a sufficient number of samples. furthermore, sampling becomes meaningless when a victim web browser visits web pages via wireless networks or tor [cit] due to their high and unstable network latency."
this paper introduced a new web privacy attack that indirectly identified the status of cross-origin urls by using html5 appcache without client-side scripts nor plug-ins. we confirmed that all major web browsers which supported appcache were vulnerable to our attacks. we also suggested an effective countermeasure: a cache-origin request-header field. the countermeasure successfully mitigated our attacks.
"we explain a timing attack that uses variance in fetch latency to infer the login status of a victim web browser [cit] . an attacker can reveal the real identity of a victim web browser's user according to which web sites the user is frequently logged in. when a web browser accesses the front pages of web applications, many of them provide different web pages to the browser according to the login status. they usually redirect a logged-in browser to a personalized web page, thereby introducing additional network delay. malicious web pages leverage this delay to infer login status by manipulating a web browser to visit the front page of a target web application while measuring the latency. high latency implies that the web browser is logged in to the target web application."
"application engineering. to automatically derive specific products from the product line, feature configurations are specified which bind each feature to a selection (either selected or deselected). for instance, the feature configuration from figure 6 produces the product shown in figure 7, a directed and weighted graph."
"in this section, we briefly explain conventional cross-origin web privacy attacks. we mainly focus on attacks that rely on timing channels that are unreliable but inevitable. we introduce attack examples to infer browsing history, login status, and internal web servers."
"visibilities are represented in a memoryoptimized way using a global data structure, the visibility forest. it contains each visibility occurring on any model element at most once. furthermore, visibilities may reference each other to form several tree-like structures (hence, a forest). the visibility forest is updated during a commit transparently."
"the remainder of this paper is organized as follows. section ii explains conventional cross-origin web privacy attacks. section iii introduces the html5 appcache. section iv describes a url status identification attack based on appcache. section v demonstrates advanced attacks to determine a login status and probe internal web servers by using the url status identification attack. section vi discusses countermeasures against our attacks. section vii presents related work. lastly, section viii concludes this work. in addition, we describe an appcache-based url timing attack in appendix."
"occasionally, an appcache procedure finishes after a web page has been loaded because a web browser performs the procedure in the background. therefore, the web browser needs to refresh the web page to reflect the most recent version."
"second, we can revise appcache to not check the changes in a manifest during download or update procedures as safari does. this countermeasure, however, results in an appcache inconsistency problem. further, it cannot prevent a url status identification attack if an attacker refreshes an attack page to re-confirm an appcache procedure."
"3) referrer information: chrome, opera, and safari send no referrer information during an appcache process, so that a stealthy attack is possible. the appcache standard [cit] does not specify whether a web browser should send referrer information during an appcache process. accordingly, browser vendors choose different policies: firefox and internet explorer record the url of an html document that declares an appcache manifest in a referrer request-header field whereas chrome, opera, and safari specify no referrer information in an http request. the lack of referrer information implies that target web applications cannot recognize who forces a victim web browser to investigate themselves."
"unlike other web browsers, firefox was secure against the internal web server probing due to its sequential appcache handling. for example, it took 7648 s and 2100 s when we performed url identification attacks on 100 unreachable urls by using firefox in os x and windows, respectively. since most victim users will not spend such a long time in an attack web page, we conclude that firefox is secure against our attack when its platform is os x or windows."
"a countermeasure to this attack is to make web applications spend constant time to process http requests [cit] . but, guaranteeing constant processing time is not only difficult but also incurs much overhead."
"in addition to the single-version resources, metadata are managed transparently to the user. they augment workspace resources with vc details, such as the versioning state (versioned, non-versioned, added, removed, etc.) . furthermore, the current choice is persisted."
"3) error handling: to avoid partial resource replacement to preserve content consistency, appcache reverts completely to its previous status when it encounters errors during the download or update procedures. appcache discards all new resources that were successfully downloaded during the failed download or update procedures."
"supermod has been developed in a model-driven way using the eclipse modeling framework [cit] ). furthermore, the tool has been implemented modularly; it is is not restricted to the threelayer architecture shown in figure 8, but flexible with respect to the underlying version space space model."
"we suggest a new http request-header field that contains the origin of an appcache manifest; this field, cache-origin, resembles the origin header field of cors. the cache-origin header field only asks web applications whether they permit caching of their resources, unlike the origin header field which requests access permissions to their resources. a web browser must attach the cache-origin header field to its http requests during appcache procedures."
note that the simultaneous identification of major cell types and their relative sub-types is problematic. the best approach consists in first identifying major cell types and then separately analyzing each of them as shown in this section. we have tried to include major cell types and their subtypes in the matrices m km and have attempted their identification with a larger number of clusters. such an approach leads often to incorrect results with relative cell frequencies that are incompatible with normal physiological ranges.
"the css-filter-based attack [cit] can identify login status by exploiting the difference in filtering latency between logged-in and non-logged-in web pages. however, two shortcomings make this attack less practical than others. first, it takes much time to measure the latency of css filtering. second, target web applications should allow the iframe tag, but recent and security-aware web applications usually disallow such a tag (section ii-b2)."
no equivalence. vcs and sple tools both offer operations that are not realized by the opposite. table 2 lists two of each. the vcs operation commit detects differences in the workspace in order to write changes back to the repository automatically.
"1) the browser attempts to fetch and parse the manifest while firing a checking event to an appcache object. if the manifest either has errors or is nonexistent, the browser terminates the download procedure and fires an error event."
"however, the preceding tags are unsuitable for performing web privacy attacks due to two shortcomings. first, the fetch latency is unpredictable because the tags try to receive all resources (e.g., images, scripts, and csss) that compose a web page before rendering the resources. this procedure adds a high amount of noise to the time measurement [cit] . second, to avoid security problems (e.g., clickjacking [cit] on login pages), many modern web applications do not allow web browsers to load their web pages in such tags. the web applications use an http response-header field x-frame-options or a frame busting code [cit] to prevent such content inclusion. therefore, the html tags for arbitrary content inclusion are unsuitable for performing web privacy attacks."
"we illustrate the two procedures of appcache: download and update procedures. the first time a web browser visits a web page that declares an appcache manifest, the browser performs the download procedure. otherwise, it performs the update procedure."
"2) no-store directive: chrome, opera, and safari ignore the no-store directive of a http resource, so that we can attack no-store http resources when a victim uses one of the web browsers. the appcache standard [cit] specifies that a web browser should not cache any resources with a no-store directive. but, we observe that chrome, opera, and safari ignore a no-store directive when they cache http resources via appcache."
"3) the browser downloads each of the resources while firing a progress event for each resource. if the browser cannot cache at least one of the resources (section iii-c) or recognizes the changes in the manifest while downloading the resources, the browser terminates the download procedure and fires an error event. 4) the browser stores the downloaded resources in its local storage and fires a cached event."
"an appcache manifest written in php (example in listing 3) can be used to perform a url status identification attack. the example only specifies a single target url (https://target.net) that attackers want to identify. server probing mainly depended on whether target urls were unreachable. table ii shows measured appcache timeout values of a single unreachable url that consisted of a literal ip address, instead of a domain name, belonging to our campus, with various platforms. os x had the greatest timeout value and ubuntu had the smallest timeout value. chrome, firefox, and opera had almost the same timeout values in the same platforms, but internet explorer had a different timeout value. fig. 6 shows the execution time of concurrent internal web server probing using chrome. the number of targets urls was 50, consisting of 0 to 50 unreachable urls and 50 to 0 reachable urls. all urls belonged to our campus. the timeout value of os x was greater than those of ubuntu and windows, so that the execution time of internal web server probing was longest when a victim web browser's platform was os x. we also identified that the number of unreachable urls did not affect the overall execution time because chrome concurrently opened multiple sockets for appcache."
"the bm samples were analyzed individually and their cluster plots were combined to demonstrate the similarity between the 8 datasets of bone marrow, see fig. 6 . we restricted the candidate cell types to the ones that have more than three markers expressed in each dataset after pre-processing. the color coding is uniform for the cell types across the 8 datasets, i.e. all stromal cells are colored orange, b cells -dark blue, etc. as some of the clusters overlap on the t-sne plot [cit], it is useful to calculate the relative the barplot on the right shows relative (%) and absolute (cell count) cluster sizes. cell clusters that have 3 or less supporting markers are marked with \"*\", see figure 2 for supporting markers."
"in this paper, we demonstrate a new web privacy attack that exploits security flaws of an html5 functionality, application cache (appcache) [cit] . appcache allows web applications to cache resources in the local storage of a web browser to enable offline access to them. however, we discover security problems, side channels, of appcache due to its cross-origin resource caching. by exploiting the security problems, a web attacker [cit], who serves a malicious web application, can exploit a victim web browser to correctly identify the status of a target url, such as whether the url exists, whether the url redirects the browser to another web page, or whether the url returns an error code to the browser, without using error-prone timing information [cit] . we name the attack a url status identification attack."
"re-combination of independent features. in the original version of our example, we have derived an example product by specifying the feature configuration shown in figure 6 in the application engineering phase. since the operations check-out and product derivation are similar (see table 2 ), this step may be \"simulated\" by checking out a choice that consists of the latest revision and the same feature configuration. as a consequence, the workspace is populated with the domain model version shown in figure 7 . this version may be refined with product specific adaptations (by specifying an ambition that equals the choice) or by changes that influence related products (by specifying as ambition a partial feature configuration that delineates the set of logical versions where the change shall be visible)."
"commit message 1 -\"initial commit.\" 2 class graph \"realization of root feature graph.\" 3 class vertex, association has vertices \"realization of feature vertices.\" 4 class edge, association has edges \"realization of feature edges.\" 5 property edge::label \"realization of feature labeled.\" 6 property edge::weight \"realization of feature weighted.\" 7 association connects \"realization of feature undirected.\" 8 association starts at and ends at \"realization of feature directed.\" 9 class color, property color::name, association has color \"realization of feature colored.\" figure 3 . the mapping has been realized with the help of the mdple tool famile (buchmann and schwÃ¤gerl, 2012), where the mapping is defined on the abstract syntax tree of the mvdm. feature expressions are highlighted."
"2) arbitrary content inclusion: html provides tags (e.g., frame, iframe, object, and embed) to embed arbitrary content in a web page. the main purpose of the frame and iframe tags is to embed other html documents, and the main purpose of the object and embed tags is to embed multimedia, such as audio, video, and pdf files. the tags only support the onload event handler, so that attackers should guess the status of a url by measuring how much time a web browser spends before firing an onload event."
"workspace. a supermod workspace contains the currently selected version of the domain model, i.e., the derived product, in its domain specific representation within an ordinary file system. emf models are represented as instances of their custom ecore-based metamodel(s). plain text and xml files are represented in their custom format. this allows supermod users to utilize their single-version editing tools they are familiar with."
"appcache-based dns rebinding is a modification of the original dns rebinding attack [cit], which attempts to violate sop by changing domain-to-ip mapping with a short-lived dns entry. in the original form, when a victim web browser visits an attacker's web site, the attacker delivers some malicious scripts to the victim web browser while associating the domain name of the web site with a target ip address. subsequently, the malicious scripts can send arbitrary sameorigin requests to the target ip address because they have the same domain name. to mitigate this attack, modern web browsers maintain domain-to-ip mapping for a while (dns pinning). however, the two characteristics of appcache allow attackers to write a malicious script executed after domain-to-ip mapping changes [cit] : (1) allowing web sites to persistently cache arbitrary resources in web browsers and (2) supporting a javascript api to recognize whether a script comes from a local cache or a server. to eradicate the attack, [cit] suggest an x-server-origin response-header field that lists server-provided origin information."
"the export transformation has been implemented for each specific resource type to translate a modify. the user may modify both the feature model and the domain model within the workspace. for domain model resources, arbitrary editors available in the current eclipse installation may be used. for the feature model, the command edit version space is offered by supermod, which opens an emf tree editor for the current feature model revision. in addition to the modification of versioned resources, the commands add/remove to/from version control are provided, which adjust the corresponding entries within the metadata section accordingly."
"the web has become the most popular distributed application platform due to its high cross-platform compatibility. users can launch a web application on any web browser in any platform without modification or with negligible modification. therefore, many applications, including email, calendars, word processors, and spreadsheets, are being implemented as web applications."
"second, we can probe internal web servers located in the local network of a victim web browser. by using the url status identification attack, an attacker can probe any url including an internal url. probing internal urls allows an attacker to probe networked devices (or things) in a victim's local network, such as routers, network printers, network-attached storage (nas), smart tvs, and smart thermostats [cit] . thus, the attacker can fingerprint the victim and can conduct succeeding attacks (e.g., dns rebinding [cit] and router reconfiguration [cit] ). the danger of internal web server probing will increase as the internet of things (iot) becomes popular."
"however, the web's popularity has made it the most valuable attack target, so that users demand an in-depth security analysis of the web to prevent attacks before they rapidly spread. numerous researchers have considered various web attacks, such as clickjacking [cit], cross-site scripting (xss) [cit], cross-site request forgery (csrf) [cit], and domain name system (dns) rebinding [cit], that attackers can exploit to steal sensitive information or to make profits. despite the best efforts of researchers to reduce such security and privacy problems, unrevealed security threats probably still remain in web applications and web browsers due to undiscovered software vulnerabilities and problematic specifications. consequently, researchers should detect and remove new vulnerabilities before attackers recognize and widely abuse them."
"the cache-origin request-header field is a minor revision of the origin request-header field, so we believe that adopting cache-origin is not a big deal of the web standard. otherwise, using origin during appcache procedures is at least desired to prevent our attack."
"ambitions. an ambition denotes a set of versions as a subset of all available versions. ambitions are used as write filters in order to delineate the scope of a change performed in the workspace. in contrast to a choice, an ambition may contain unbound options, to which the change is immaterial."
"by using the cache-origin header field, a web application can identify other web applications that request to cache its resources. when the web application doubts the requesters or caching the requested resources can reveal sensitive information (e.g., access-controlled resources), the web application either assigns a no-store directive to its response header or returns an error code to abort an appcache procedure. attackers can no longer identify browser and url status because their appcache procedures always fail. even if some attackers bypass the cache-origin check, they cannot identify a browser status when the target web application disallows web browsers to cache sensitive resources."
"in this section, we first present the results obtained with our methodology using recently-published data from normal bone marrow samples (the data identified above as bm1-bm8, containing a total of 378k cells). additionally, we compare our cell type assignment to an existing identification of cell types from a large scrna-seq â¼68.6k cells pbmc dataset."
"bulk rna-sequencing has provided the bioinformatics community with a large volume of high quality data over the past decade. however, bulk measurements make studying the transcriptomics of heterogeneous cell populations difficult and provides limited insight on complex systems composed of interacting cell types. single cell rna-seq (scrna-seq) techniques promise to provide the field of bioinformatics with samples suf-mining the optimal therapeutic solutions in different hematological cancers."
"the metamodel realizes the sub-set of concepts presented in sections 3 and 4.1, which are relevant to the repository. as shown in figure 9, a supermod repository consists of a version space, a product space, and a visibility forest. the version space in turn is composed of several version dimensions, and the product space comprises a number of product dimensions, which in turn contain a hierarchy of versioned elements. these may reference a visibility, which is organized within a visibility forest. a visibility may be an option reference or a composed expression (e.g., and, or, not). choices and ambitions, which occur in supermod as temporary data structures (except for the choice in the metadata section), are represented by optionbinding, which maps options to selections."
"lastly, we can modify vulnerable web pages that conditionally redirect web browsers to login pages or that return error codes according to a login status. for example, we can use a login pop-up window instead of redirections and a custom error page with 200 ok instead of an error code. this countermeasure prevents a url status identification attack, but finding and modifying all vulnerable web pages are sophisticated tasks."
"visibilities. a visibility is a logical expression over the option set, which is attached to an element of the feature or domain model. in order to test an element's presence in a specific version, the bindings specified by the respective choice are applied. visibilities are modified automatically during the commit operation (see below)."
"2) updating cached web page: next, we describe the appcache update procedure for a cached web page and corresponding events fired during the procedure. a web browser initiates the following procedure to update corresponding resources when it visits a web page that has already been cached in its local storage."
"the first step to address these limitations is unsupervised clustering. after clustering, one can look at the average expression of markers to identify the clusters. several clustering methods have been recently used for clustering single cell data (for recent reviews see [cit] ). some new methods are able to distinguish between dropout zeros from true zeros (due to the fact that a marker or its mrna is not present) [cit], which has been shown to improve the biological significance of the clustering. however, once the clusters are obtained, the cell type identification is typically assigned manually by an expert using a few known markers [cit] . while in some cases a single marker is sufficient to identify a cell type, in most cases human experts have to consider the expression of multiple markers and the final call is based on their personal empirical judgment."
"choices. a choice denotes a single version by assigning a selection (selected, deselected) to each of the existing options. choices are used as read filters, i.e., they describe versions visible in the workspace."
"we have used the cell marker database cellmarker [cit], which is the most recent database available. there is another database created by the human cell differentiation molecules (hcdm) organization [cit], which is sponsored by a number of large companies. this database contains detailed information about each cd molecule, including structure, function, and cellular expression. the hcdm would be an alternative to cellmarker, that could be used to create a marker/cell type table to employ with p-dcs. we have observed that the overlap between these two databases is very strong, therefore we do not expect significant differences in the cell cluster assignments. finally, several deconvolution algorithms have been developed in the past for estimating the relative composition of complex tissues from bulk transcriptomics data. [cit] these methodologies are usually based on predefined signature matrices that contain the relative expression of markers, not just the presence/absence of a marker, for different cell types. regression methods are then typically used to infer the relative proportions in a mixture. these signature matrices have been validated on bulk data and their robustness to the characteristic scrna-seq noise has not been tested. however, in principle they contain additional information that could be integrated in our p-dcs to identify single cells."
"variability model. figure 3 shows the underlying feature model, which consists of a root feature graph with two mandatory sub-features vertices and edges. vertices may optionally be colored. for edges, the optional sub-features weighted and labeled are defined. furthermore, the features directed and undirected are mutually exclusive."
"using stepwise and incremental software development (apel and kÃ¤stner, 2009 ), a sub-discipline of feature-oriented software development, features are described as refinements or layers. this replaces the necessity of an explicit mapping in the form of presence conditions, but the increments need to be specified in a form that deviates from the \"normal\" implementation language, e.g., model transformations. in contrast, supermod enables for sisd using a familiar development environment."
"second, the conventional attacks are inefficient because they cannot measure the fetch latency of multiple urls in parallel. if attackers open more than one connection with target web applications, interference between multiple connections causes timing errors. accordingly, attackers should probe urls one by one."
"we depict how a web application announces that it uses appcache, and how the web application specifies which resources web browsers should store in their local storage. first, the web application declares the path of an appcache manifest file (example.appcache) that corresponds to an html document in its html tag (listing 1). the manifest file and the html document must belong to the same origin, and the content type of the manifest file should be text/cache-manifest."
"we aim to restrict arbitrary cross-origin appcache to protect browser and url status from the url status identification attack. one possible solution is to apply the origin requestheader field of cross-origin resource sharing (cors) [cit] to appcache procedures, although this approach can violate the principle of least privilege. the origin header field allows a web application to identify which web applications initiate cross-origin requests so that the web application can deny requests from unknown or blacklisted web applications. however, the origin header field further asks a permission to allow client-side scripts to access the requested resource, which is unnecessary for appcache. therefore, we require another method that only asks a web application whether it allows resource caching."
we modified a build of chromium (35.0.1856.0) to introduce a cache-origin request-header field during appcache procedures (listings 6). adding three lines of code was enough to enable this countermeasure with negligible performance overhead.
"we introduce as running example a product line of different domain models for graphs, a common example in spl literature [cit] . in this section, we conduct the example using a \"traditional\" tool chain: a state-of-the-art vcs supports the development of the platform in multiple iterations. next, a feature model is defined, and an mdple tool based on negative variability is used to annotate domain model elements with variability information. during application engineering, we derive one example product."
"second, our attack can concurrently identify the status of multiple target urls. attackers aim to develop a fast attack because they cannot guarantee that a victim spends a long time in their attack pages, so they have to obtain the victim's secrets as quickly as possible and as much as possible. however, conventional timing-based web privacy attacks [cit] third, our attack can correctly recognize whether a url redirection occurs when a victim web browser visits a target url, namely, it violates the requirement of atomic http redirect handling [cit] . to infer the status of a target url, conventional attacks [cit] load the target url via some tags (e.g., img, script, and link) and check when or whether onload or onerror events occur. such tags transparently follow url redirections for the atomic http redirect handling, so that attackers cannot accurately recognize whether redirections occur. therefore, identifying whether a url redirection occurs and determining a login status according to a conditional url redirection (section v-a) are only exact with our attack."
"to prevent this attack, a web browser should prevent external scripts from accessing its internal network. we also require dns pinning and host name authorization to prevent dns rebinding attacks [cit] ."
"to demonstrate the added value of supermod, we reconsider the example from section 2. now, we develop the platform, consisting of the domain model and the feature model (and the mapping in between, which is hidden now) together in multiple iterations, using feature configurations to describe the scope of changes to the domain model. figure 10 shows the subsequent iterations in which the model-driven product line is developed as described below. in each step i, revision i â 1 is evolved to revision i."
"the term bf refers to \"biologically-inspired filtering\", p is the considered pixel, is a threshold. in features extraction step, instead of using the input image i in, features are first extracted from two images, i â bf and i + bf, and are then combined together."
"the representation of sx used at test time is therefore a scattering transform. let p uc sx denotes the orthogonal projection of sx in the scattering space u c of a given class c. the principal components space u c is approximately computed from the singular value decomposition (svd) of the matrix of centered training sample sd j x c,i â Âµ c with all possible samples i dilated by 2 j for a given class c. the pca classification computes the classÄ(x) base on the minimum distance (id â p vc )( sx â Âµ c ) from sx to the space Âµ c + u c, (figure 2 )"
"in the recent decades, computer vision researchers have been witnessing a lot of researches drawn on texture because it is the fundamental appearance element of materials or objects. main problems consist of segmentation, shape from texture, synthesis, and classification. also, texture is a crucial clue for distinguishing things in the real world. for example, animals are able to be discriminated depending upon the texture of their skin, the category of soil, sand, rock and so forth can be differentiated base on the texture of their surfaces. texture classification is a research field that categorizes image data into more readily interpretable information, which is able to be used in a wide range of applications such as industrial inspection, image retrieval [cit], medical imaging, remote sensing [cit], object recognition, and facial recognition [cit] . for this reason, it is a very inspiring subject."
"biologically inspired filtering (bf) [cit] imitates the human retina mechanism to extract more detail information of a given image when being used as a preprocessing step. it enhances performance of different features in terms of discriminative power for texture classification, including clbp."
"we analyze the effectiveness of our method by doing experiments on four popular texture databases and follow their testing protocols. in order to have reliable results, we run the pca classifier [cit] 100 times on each dataset by randomly choosing the training and testing sets, with an exception for outex and kth-tips-2b which have a predefined sets of those. then the average results of all splits are reported. in addition to the bf preprocessing technique [cit], pca classifier [cit], and scaterring network (scatnet) [cit] which plays scattering transform role are used in our experiment."
"it can be vividly seen that conventional lbp features are vulnerable to noise because the most important part of this method is based on the center pixel threshold. a measure to this is that we make use of our previous works, bf preprocessing technique [cit], before extracting lbp features. the scattering transform method also has its weak point, a lack of small local structure information due to its feature dimension reduction strategy, downsampling. this can be compensated for using lbp features. we do not utilize the bf preprocessing technique for scatnet because this network contains band pass filters whose functionality are similar to those of bf. the overall idea is illustrated in figure 1 ."
"where g p, g c and s(x) are defined as in (1) . two operators called clbp-sign (clbp s) and clbp-magnitude (clbp m ), respectively, are proposed to encode them, where the clbp s is equivalent to the conventional lbp, and the clbp m measures the local variance of magnitude. the clbp m is defined as follows:"
a generative classifier called principal component analysis (pca) [cit] was proved to have decent performance for scatnet in case of small training dataset. pca classifier is described as following.
"in this paper, we have proposed a three-in-one handcrafted descriptor, namely an integrated descriptor. it takes full advantages of the bf preprocessing technique (our previous works [cit] ), the local lbp features, and the global ones extracted from scatnet. it is proved by experiments that this novel descriptor enhances distinctiveness of texture while preserving the robustness to variations in illumination, rotation, and noise. overall, scatnet and lbp are not concurrent, but complementary while the preprocessing technique makes the descriptor more robust. future study can be drawn on the same domain with scale variation tolerance by using multi-scale training technique."
measuring and quantifying the adverse effects of crowdsourced manipulation remains an unexplored area. we plan to conduct the following experiments to understand the effect on osn's ecosystem -
"users, spread and consume information, and even build and strengthen social connections. due to the growing rise of osns as a platform for mass communication, brands, celebrities and political parties have started using them extensively to engage with users. some of the most popular and influential celebrities have garnered millions of users following their activities on osns. 2 the user following and crowdsourced ratings give an osn user a sense of social reputation which she tries to maintain and boost to be more influential in the network and attract more following. various osns have a different measure of user following and social reputation, like followers on twitter and instagram, likes on facebook and ratings on yelp. however, these reputation metrics can be manipulated in several ways."
"information privacy protection has become an important issue in the field of information security, and secret sharing is a promising technology among privacy protection schemes. in real life, it is dangerous to keep some sensitive and important information, such as passwords of opening bank safes or launching missiles, by a single person, because the information is easy to be damaged, lost or tampered. therefore, it is urgent to establish novel key dispersion schemes. in the custody system, a secret sharing system is established, which is an important method to protect information security and data security. since blakley [cit] and shamir [cit], secret sharing has been extensively investigated in the literature. in a secret sharing the associate editor coordinating the review of this manuscript and approving it for publication was corrado mencar . scheme, the dealer divides the secret into several shares and distribute them among a group of participants. in general, each share alone cannot reveal any useful information about the secret, and some specified subsets of participants are able to re-establish the original secret information in a cooperative manner. meanwhile, those unauthorized subsets of participants are unable to reconstruct the original secret information."
"we identify three main sources of bulk social reputation manipulation viz. (i) online blackmarket, (ii) scratch-back services, (iii) browser malwares. there has been some work done to identify online blackmarkets by finding websites on search engines using keywords like buy cheap followers and purchase facebook likes [cit] . online blackmarkets are however not hard to find since they advertise themselves to get more hits. we focus on the more challenging task of profiling and measuring the impact of these websites. we identify market leaders that inject the network with most of the sybil nodes and hence have a more damaging effect on the network."
"the first part involves landscaping the sources of crowdsourced social reputation manipulation. as a preliminary study, we have been able to effectively study the underlying structure of online blackmarket which sells twitter followers. we purchased over fake followers from over 60 most popular (by alexa ranking) merchant websites and discovered the following -"
"effect of crowdsourced manipulation of social reputation. there have been several reports stating the adverse effects of manipulated social reputation on popular osns like generating fake following by politicians [cit], misleading product reviews [cit] and fake hits on facebook advertisements [cit] . researchers have shown that blackmarkets for crowdsourced social reputation on osns have a potential to generate revenue between $40 to $360 million [cit] . however, the effect of social reputation manipulation of users on osn's friend recommender system, prioritization of search results and advertising revenue of osn is largely unexplored and not quantified. as the final part of this work, we would measure the adverse effect of manipulated social reputation on the osn's ecosystem."
"every online social network provides mechanisms to its users to become socially visible and influential. this work explores the sources of crowdsourced manipulation of social reputation and detection of the same. table 1 gives a brief overview of popular osns and correspondingly, which met-ric gives the concerned osn entity a sense of social reputation. the larger goal of this work is to be able to effectively detect and measure the effects of manipulation of such social reputation metrics. to achieve our goal as described in section 3, we divide our work into a three phases. we describe our current and future approach to complete each phase in more detail as followed -"
"the multi-secret sharing scheme provides a practical technique for sharing multiple secrets among a group of participants. verifiable multi-secret sharing schemes not only share multiple secrets among a group of participants, but also detect cheating by dealers or participants. they are important tools for preserving multiple secrets (such as cryptography keys) and designing secure multi-party protocols. in this study, we present an information privacy protection based on verifiable (t, n)-threshold multi-secret sharing scheme. moreover, we seamlessly integrate the verification algorithm to our secret sharing scheme so that one can verify the correctness of the shares. the new scheme we designed can be used in the environment without secure channels. we evaluate the performance of the proposed verifiable multi-secret sharing scheme through theoritical analysis and a series of simulations. results prove that the scheme can provide effective information privacy protection. in addition, we assessed the time efficiency of the proposed programme and simulation results illustrate that the scheme is also very efficient."
"in this paper, we have studied the uas of dsltv systems. some stability conditions have been proposed to verify the uas of dltv systems by using function-dependent lmis. comparing with the existing results, the obtained conditions allow the norm and rate of variation of system matrix are unbounded. furthermore, the stability condi-tions have been given for dsltv systems by combining the methods of function-dependent lmis with average dwell time. finally, some further discussions and two relaxed stability condition that allows all subsystems are unstable also have been proposed."
"the verifiable secret sharing (vss) scheme is a modification of the traditional secret sharing scheme, and it is mainly used to address issues of non-honest distribution center. vss schemes are designed based on the usual secret sharing schemes with additional verification operations. in the vss scheme, the dealer distributes not only the shares of the secret but also some extra information about the secret fragments to the participants. once a member receives his shares, he can verify whether the shares are correct based on the broadcasted information. in the secret reconstruction phase, each participant uses the same method to verify correctness of secret fragments of other members. it can be observed that vss is able to resist the following two kinds of active attack:"
"case 1: it is impossible for an adversary to construct f (x) directly from the n + l â t points disclosed by the dealer, because the number of public points is less than the power of the n + l â 1-th degree polynomial f (x). thus, the adversary will not get any information about l secrets."
3) it realizes the sharing of multi-secrets in parallel through one-time secret sharing process; 4) it allows participants to reuse their shares in different rounds of multi-secret sharing.
"in this section we move towards the more challenging task of effective detection of users with manipulated social reputation. we conduct our initial experiment on twitter. the perceived social reputation is defined by the follower count of a user. to assess the credibility of this follower count, we use the following parameters: tweet-follower ratio, evidence of user being a topic expert and overlap of her interests with followers. to build our baseline, we collected a random sample of 1.6 million twitter users and used a multiple linear regression model to fit the three parameters. we define our model as -"
"one of the most prevalent methods to alter social reputation is online blackmarket that not only generates a misleading reputation but also injects the network with false identities. there exist several online services from where an osn user can purchase bulk followers and likes. such services are very cheap and let users choose from different packages like \"1000 followers for $3\". 3 security researchers have recently estimated that the revenue generated by blackmarket for twitter followers is between $40 million to $360 million. another host for fake accounts peddlers is supplydriven microtask markets like fiverr and seoclerks. account peddlers exploit these services to cater to osn users who inflate their social media metrics such as -followers, likes and shares in the hope to become more influential and popular on the network. these marketplaces often provide newly created or stockpiled, fake and inactive accounts to the users. such services are a big threat to the credibility of the social networks that rely on crowdsourced ratings and reviews for product recommendation and build user trust. recently amazon sued 1,114 sellers on fiverr for posting fake reviews of amazon products [cit] . infiltration of fake accounts and metrics also has a damaging effect on the advertisement revenue framework of osns. an advertisement might be shown to certain users with seemingly high popularity and reputation, but will most likely not get clicked by the expected number of real users due to the accounts being fake or bots. to address such issues and mitigate the damaging effect on the osn ecosystem, there is a dire need to build alternate social reputation systems and weed out users and entities on social networks with manipulated followers, likes and ratings. recent studies show that there exist a thriving blackmarket to create fake identities [cit] . there have also been strong evidence of browser malwares that attack social networking sites and trigger activities such as liking and sharing posts without user's knowledge [cit] . the attackers behind blackmarket or browser malwares either create fake identities or compromise existing users to manipulate social reputation in bulk. these sybil identities are used to manipulate social reputation, such as twitter followers [cit], facebook likes [cit] and yelp reviews and ratings [cit] . although there is a strong evidence of manipulated social reputation on various networks and its damaging effects [cit], there does not exist alternate social reputation systems to compute the difference between percieved and real reputation of an osn user. coupled with this gap, there exist a lack of understanding of how manipulated social reputation affects osn's ecosystem. an artificially inflated social reputation can adversely affect osn's recommender system, post engagement and advertising revenue framework, which we plan to study. therefore, my dissertation explores the followingbuilding a robust alternate social reputation system which does not get affected by manipulated social metrics of the osn user."
"in the view of the above shortcomings, this paper proposed a verifiable (t, n) threshold multi-secrets sharing scheme based on the difficulty of discrete logarithm, which has the following properties: 1) it can prevent cheating between dealer and participants; 2) it does not need to maintain the secure channel in the secret distribution, so that the scheme can be applied in the system where there is no secure channel;"
"as a class of important hybrid systems, switched systems have drawn considerable attentions and interests in control field due to their significance in both theory and applications. many significant results have been obtained for continuoustime and discrete-time switched systems, see [cit] and references therein."
"in particular, what are the conditions that this function and the former two ones (i.e., coefficient matrix and the switching signal of systems) need to meet together? this is a key technical issue."
"this work aims to detect and measure the deviation from perceived social reputation of an osn user. we start by landscaping the sources of such manipulation like blackmarkets and scratch-back services. preliminary results bring out the underlying structure of blackmarket which can be helpful to uncover the market leaders. eliminating or hindering their operations can significantly bring down crowdsourced manipulation of social reputation. initial results also show that a robust and adaptive technique can be built to detect social reputation manipulation. however, our proposed framework is at a very nascent stage and needs much more improvement and rigorous evaluation. much work yet remains to leverage this framework to build an alternate social reputation system and measure the effects of social reputation manipulation on osn's ecosystem."
"in recent years, most secret sharing schemes [cit] have strong limitations on the secret sharing process and they can only share one secret at one time among a set of participants, which is of low efficiency, as shown in fig. 1 . however, another drawback in their scheme is the assumption that both the dealer and the participant are unconditionally honest. in this way, the fraud of malicious dealer cannot be discovered. the deceptive behavior of malicious participants is unavoidable in the process of reconstruction. verifiable multi-secret sharing enables a dealer to share multiple secrets volume 8, 2020 this work is licensed under a creative commons attribution 4.0 license. for more information, see http://creativecommons.org/licenses/by/4.0/ among a group of participants such that the deceptive behaviors of the dealer and the participants can be detected."
"browser malwares is one of the other primary sources of social reputation manipulation which we plan to study. the infected users are unaware of the likes, shares and comments triggered from their accounts and unknowingly become a proxy for a manipulated social reputation of the attacker. this area is largely unexplored with only evidence of browser malwares attacking osns [cit] . we plan to collect a dataset of rogue browser extensions that trigger such attacks and study the propagation of infection in the network."
"proof: if the participant p v j follows the protocol accurately, then we have the following: theorem 1 shows that if the participant has deceptive behaviors, the other participants cannot pass the verification. this completes the proof. theorem 2: an adversary cannot obtain any useful information about secret s 1, s 2, Â· Â· Â·, s l from public information."
"motivated by the above reasons, in this paper we will study the stability of dsltv systems by using the methods of function-dependent lmis. the main contributions of this paper include: i) two stability conditions are proposed for dltv systems by using function-dependent lmis (compa-ring with the existing results, the conditions obtained allow the norm and rate of variation of system matrix are unbounded), ii) the stability condition is given for dsltv systems by combining the methods of function-dependent lmis with average dwell time, iii) some further discussions and two relaxed stability conditions that allows all subsystems are unstable are also given."
"on the other hand, to ensure the stability of systems, the results of theorems 3 still require all subsystem to be stable. by relaxing this restriction, we further have the following results."
"another source of social reputation manipulation, scratchback services are the ones where users do not need to pay money to boost their social reputation. instead, they can do favors to other interested users to get back the same and hence become part of a collusion network. since each individual node involved in such a collusion network can be a genuine user trying to manipulate its social reputation, such manipulation is very hard to detect. there do not exist any studies with tangible results to uncover large-scale collusion networks involved in manipulating social reputation. however, we plan to take cues from seminal research been done on social spam campaigns [cit] to detect social reputation manipulation by scratch-back services."
"we propose a robust approach to detect crowdsourced social reputation manipulation by leveraging the basics of each osn used to gain true attention and reputation in a network. for instance, a twitter user will genuinely garner followers when she posts interesting content (or is a celebrity in the real world). therefore, to validate her follower count, we build a heuristic to determine her real reputation based on various other factors like tweet-follower ratio, evidence of being a topic expert and overlap of her interests with followers. the initial results of our approach seem promising and based on our findings we later plan to propose an alternate social reputation score."
"for a particular user under suspicion, we find the deviation of its distribution based on the same three parameters from that of the random sample and define suspect ratio as the inverse of the deviation. therefore, smaller suspect ratio indicates higher suspicion towards the perceived social reputation (follower count) of the user. we then conducted an in-the-wild experiment over 1% twitter stream data and labeled a user as suspicious if her suspect ratio was significantly low below a certain threshold. so far we have been able to label over 56,000 users as suspicious. a small sample of the users detected by our proposed methodology can be seen at http://bit.ly/fakefollowproj."
the method of sharing secret s among the n participants is perfect secret sharing scheme if it can realize the access structure with the following two properties: 1. the secret s can be determined by an authorized subset of participants when their shares are placed together; 2. nothing about secret s can be determined when the shares of an unauthorized subset of participants is pooled.
"working of intrusion detection system: there are 4 steps in the working of ids. they are collecting data, selecting features, analyzing the data, and last the actions to be performed."
the kdd-99 cup dataset is the most used dataset for training the algorithms. it is the subset of darpa-98 dataset. kdd-99 dataset is a multi-variety dataset. the dataset contains 4.8 million instances. the characteristics of the attributes used in the dataset are categorical and integer in nature. it has forty-two attributes. thekdd-99 cup dataset consists of the following:
"1. load the data. 2. initialize the value of k. 3. for getting the predicted class, iterate from 1 to total number of training data points."
where is a normalization factor that ensures the sum of all instance weights is equal to 1. after m iteration we can get the final prediction by summing up the weighted prediction of each classifier.
"another simple concept that has been applied to sediment transport is tracing particles in order to identify the displacement after specific time spans. this technique is particularly suitable to coarse-clastic beaches, where the coarser grain-size makes it easier to mark single samples. after attempts made by painting gravel particles [cit], using allochthonous lithologies [cit], and coupling pebbles to electronic devices [cit], the technology that produced better results in terms of recovery rate was the radio frequency identification (rfid). individual pebbles would be unequivocally identified by a passive transponder activated by the low frequency radio signals transmitted by an antenna [cit] . though useful for many purposes, this method does not provide any clue about the trajectories the tracers underwent between the recoveries."
"recognizes intrusions based on a standard pattern of the malicious activity. it can be very helpful for known attack patterns. also, the rate of misplaced report is high. one disadvantage of misuse detection over anomaly detection is that it can only notice intrusions which contain known patterns of attack. an ids monitors the activities of a given environment and decides whether these activities are malicious or normal based on system integrity, confidentiality and the availability of information resources. when building ids, one needs to consider many issues, such as data collection, data preprocessing, intrusion recognition, reporting, and response."
"in order to analyze the actual power consumption, a sensor node was powered with such a kind of battery applying the suggested duty cycle: the node operated continuously for 22 days, thus in the same order of magnitude of the analytical results, with a â¼ 54% reduction. applying this reduction to the value calculated in (14), the actual life time would be:"
1. takes the test features and use the rules of each randomly created decision tree to predict the outcome and stores the predicted outcome (target) 2. calculate the votes for each predicted target 3. consider the high voted predicted target as the final prediction from the random forest algorithm
"research and study of the possible techniques for the implementation and configuration of the monitoring network, formed by a network coordinator and several sensor nodes; 3."
"low cost and reduced energy consumption are the two key requirements of the whole system, thus the choice of tools and instruments for the development of the monitoring devices has fell on the arduino community, being an open-source project offering small dimension printed circuit boards with a micro-controller and related components, to which several accessories can be connected (e.g., wired and wireless modules, gsm boards for data traffic), providing a wide range of functionalities, in particular for wireless connectivity. the objective was then the realization of a complete system, inexpensive and ready to use, which would allow a user, through an internet browser, to visualize and manage relevant data in the monitored area."
"delivery of data to the server: after the eight transmissions, the processing phase for calculating the dune's height is concluded, and the calculated value is sent to the server. after the successful connection to the server, a get request to the web application is performed, and data processed by the coordinator are sent through the gprs channel. from this moment, it is possible to access a web page through user-password authentication and then to verify that the data have been actually sent; 4."
"according to the previous considerations, the main objective of the design was to keep low production costs and to create a sensor with very low energy consumption, which could be powered with simple aa batteries. then, the overall list of components responding to all the previous requirements is the following:"
"in supervised learning, algorithms learn from labeled data. after understanding the data, the algorithm determines which label should be given to new data based on patterns and associating the patterns to the unlabeled new data. supervised learning can be divided into 2 categories i.e., classification and regression."
"an mlp can be viewed as a logistic regression classifier where the input is first transformed using a learnt non-linear transformation. an mlp consists of at least three layers of nodes: an input layer, a hidden layer, and an output layer. except for the input nodes, each node is a neuron that uses a nonlinear activation function. mlp utilizes a supervised learning technique called back propagation for training."
"the second measurement campaign was carried out between 16 [cit] and 22 [cit] and saw the deployment of a two-node network configuration on the marina di tirrenia beach, near pisa, italy, close to a bathhouse that provided a video-surveillance service. figure 6 shows the two deployed sensor nodes, while figure 7 shows the overall network: between the two poles, it is possible to notice a structure for the measure of wind speed and direction, as well as the solar cell with the coordinator. the sample rate was set at one sample per hour: however, a small subset of the datasets went lost due to the weak gprs network coverage on the site that turned into some short disconnection periods of the coordinator from the network. nevertheless, the amount of received packets is however widely sufficient to define the sand level variation trend. in order to verify the correctness of the acquired data, they where checked manually daily directly on site by counting the number of emerging ldrs on the poles: this number allowed knowing the amount of buried ldrs that allowed checking the correctness of the value measured by the sensor. each ldr count performed on site confirmed the accuracy of the measurement carried out by the sensors. the trend of the data collected by the two sensor nodes for the whole measurement period can be seen in figure 8 . the value range is between â60 cm and 60 cm assuming as zero the midpoint of the poles: indeed, once the system is fully operational, the poles are expected to be positioned half immersed in the sand, in order to measure both negative and positive variations of the sand level. however, for the test phase, in order to speed up the experimentation avoiding the digging of deep holes in the sand, the initial level was set at â45 cm for pole 1 and at â55 cm for pole 2. while the final values for the two poles are â50 cm for pole 1 and â60 cm for pole 2, it is however possible to conclude that during the test period, the sand level remained almost constant, as wind speed recorded during the same time span did not reach values able to consistently entrain and move sand particles of such a grain size. while the system proved its operation following the field tests, some limitations can still be addressed. in particular: (i) measurements can only be carried out at fixed locations as the sensors are not free to move (this can be overcome by deploying as many nodes as possible to cover wide areas or by moving the nodes to different locations for every specific time span); (ii) the accuracy of the measurements is about 5 cm, which is lower relative to lidar surveys (this can be overcome by a reduction of the distance between consecutive sensors along the same pole); (iii) the system does not transmit the data if there is no gsm coverage in the area; (iv) the system does not record any measurement during the night (to overcome this limitation, the adoption of photoelectric barriers could be studied). nevertheless, these limitations do not impact the overall effectiveness of the proposed solution."
"in this phase, several tests have been performed both in the laboratory and at the monitored site. the objective was to verify that the data were correctly transmitted in real time, so as to allow their consultation and monitoring any time and anywhere through an internet connection. data related to the first couple of active nodes are transmitted at the same time to the server, leaving a time distance of 2 min before the transmission of the second couple of nodes (the entire system has been tested with two couples of nodes). moreover, in the first phase, in order to reduce test times, the sleep period of the single nodes has been reduced from 4 h to 7 min."
"the idea at the base of the proposed monitoring architecture comes from the need for a tool capable of measuring the temporal variations of sandy beach and dune height variations at different points, in order to assess their progressive morphological modifications, possibly evaluating also the sand volume variations. the developed tool is expected to remain installed on site for a prolonged time; it should be characterized by a certain stability; and it should resist atmospheric conditions that are typical of a harsh environment such as the marine one. according to these requirements, a sensor has been designed for measuring sand level variations with a 5-cm resolution (i.e., a tolerance that has been identified to be adequate for such scenarios according to geologists' opinion)."
"intrusion is a set of actions that attempts to compromise the integrity, confidentiality, or availability of any resource on a computing platform. an intrusion detection system (ids) is a system that monitors network traffic for doubtful activity and matters alert when such activity is exposed. while anomaly discovery and reportage is the primary function, some interruption detection systems are capable of taking actions when malicious activity or irregular traffic is detected, including blocking traffic sent from suspicious ip addresses. an ids is a combination of hardware and software that detects intrusions in the network. it is used to track, identify and detect the intruders. it is a combination of both hardware and software that detects intrusions in the network. ids is used to detect unauthorized intrusions that occur in computer systems and networks. feature selection for intrusion detection is most important factor for the success of intrusion detection system. the objectives of ids are confidentiality, integrity, availability and accountability. intrusion prevention system is classified into four types: a. network based intrusion prevention system -monitors the entire network for suspicious traffic by analysing the protocol activity. b. wireless intrusion prevention system -monitors the entire network for suspicious traffic by analysing the wireless networking protocols. c. network behaviour analysis -examines the network to identify the threats such as distributed denial of service."
"two to three nodes: modification of a specific cross-section of the beach or dune (figure 1b ). this kind of analysis can be crucial if carried out at specific points as, for example, the depression between two adjoining dunes, which are usually the most sensible spots for the erosive processes. while the first two analyses are simply carried out by comparing the different sand level values and possibly by structuring the data in a gis, the second two allow the measurement of a wider range of data. in particular, variations of the volume, as well as of the mass of sand can be calculated. cases 3 and 4 can be considered together since the architecture described in point 4 is just an extension of the one proposed in point 3. combining the measures of the sand level acquired through the sensor network with the exact gnss position and elevation of the nodes, it is possible to create dynamic 3d maps of the area under study in real time. indeed, figure 2a shows that a portion of beach or dune covered by a large number of sensor nodes can be considered as composed by an array of solids (figure 2b ), enclosed by four sensor nodes placed at their vertices, that are characterized by a square or rectangular lower base, four trapezoidal side faces that are perpendicular to the base and an irregular upper surface: this one could be flat (when the four upper vertices lie in the same plan), but also curved, concave or convex. the lower base of this solid coincides with the surface of the beach at the deployment instant of the network, while the upper surface is the one delimited by the new sand layer levels measured by the sensor nodes after a predefined span of time."
"the data to be collected by the proposed monitoring infrastructure can be used to retrieve different kinds of information according to the number of available sensor nodes and to the different layouts according to which they can be deployed. in any case, in order to process the data with a geographic information system (gis), each deployed node needs to be georeferenced with a high precision global navigation satellite system (gnss), also measuring the exact elevation of the point: this value is crucial for every kind of data analysis to be carried out. for this purpose, when each sensor node is positioned on the beach or dune, its position is recorded by a dgps-rtk instrument, which provides accurate values of the coordinates and elevation of the ground at the beginning of data collection."
"on each pin of the counter (referred to as output buffers q3-q9 and q11-q13), we have a clock cycle that doubles at each following output. for example, if q3 needs 2 s to complete a cycle, q4 and q5 will employ respectively 4 and 8 s, and so on. the counter outputs control the logical signals for the selection of the mux: at each change of bit status, the signals select a mux input to which each single ldr is connected; light intensity perceived by that resistor is measured and acquired through a suitable analog pin of the xbee transmission module. this module delivers the measured value to the network coordinator in wireless mode through the zigbee protocol. at each transmission, data related to three resistors are delivered (since xbee radio modules are provided with three analog inputs); therefore, for the entire acquisition of the sensed data, eight transmissions are needed. moreover, there are three logical signals for the selection of the mux; hence, only three out of 10 possible counter outputs are utilized."
"a decision tree is one of most frequently and widely used supervised machine learning algorithms that can perform both regression and classification tasks. the intuition behind the decision tree algorithm is simple, yet also very powerful. it requires relatively less effort for training the algorithm and can be used to classify non-linearly separable data. it is very fast and efficient compared to knn and other classification algorithms. entropy and information gainare the most commonly used attribute selection measures."
semi-supervised learning algorithms represent a middle ground between supervised and unsupervised algorithms. the semi-supervised model combines some aspects of both into a model of its own.
"the logic part represents the heart of sensor node, completely defining its operation modality. it allows the sequential selection of the ldrs, allowing the data acquisition from 24 different data sources using only three input lines. moreover, it manages the duty cycling of the sensor node, allowing power consumption regulation. finally, an xbee series 2 radio transmission module allows the data transfer to a gateway in charge of forwarding it to a remote data acquisition center through an internet connection (see section 6)."
"an xbee series 2 transmission module with a related shield to connect it to the arduino uno, for wireless (zigbee) communication between network nodes and the coordinator; 3."
"the main objective being the realization of a low-cost and low-energy wsn, we can envisage simple sensor nodes not equipped with a micro-controller: data are sent to the coordinator as soon as they are acquired, without any processing. consequently, complexity is shifted to the coordinator side, which should be able to process multiple data. the arduino board implementing the coordinator has some hardware limitations, among which is a maximum receiving buffer size of 64 byte; therefore, if data are received from multiple sensors, some of them could be discarded due to buffer overflow and then become lost. for each single transmission, a sensor node delivers to the coordinator a 26-byte frame (raw data, not pre-processed); hence, it is not possible to handle more than two nodes at a time if they are transmitting simultaneously."
a. [cit] at lincoln labs. seven weeks of traffic resulted in five million connection records used for a training set. a further two weeks of network traffic generated a test set with two million examples. the complete schedule can be found at the mit website [cit] . kdd-99 is a filtered version of this data.
"this is obviously an ideal value coming from simple calculations. moreover, xbee modules require a minimum of 2.8 v of voltage to operate, but aa batteries have a discharge curve that limits their usage due to a constant reduction of the output voltage. nevertheless, field tests performed reducing the duty cycle from 4 h to 20 min (i.e., with a 180-s active time each hour) have proven that the actual achievable life time is lower than the ideal one, but still in the same order of magnitude. in particular, with this duty cycle, the battery consumption grows to:"
"as sand transport processes are crucial to better define the sandy beach environment, the proposed system would offer the chance to collect a huge amount of data about real-time particle movement, which is an element that coastal managers often lack while discussing decisions to be made to manage coastal areas most appropriately."
"entropy calculates the homogeneity of a sample. if the sample is completely homogeneous the entropy is zero, and if the sample is an equally divided it has an entropy of one."
"the svm has been chosen because it represents a framework both interesting from a machine learning perspective and from an embedded systems perspective. an svm is a linear and nonlinear classifier, which is a mathematical function that can distinguish two different kinds of objects. these objects fall into classes, which is not to be mistaken for a java class. training a svm can be illustrated with the following pseudo code:"
"regarding the housing of the sensors, in the marine environment, the hot summer temperatures and the presence of corrosive elements like sand and sea water can easily deteriorate the devices. hence, the pole has been inserted inside a rubber pipe that is 0.5 cm thick, in order to be protected and insulated: the pipe is totally sealed, and no sand can filter inside; thus, the ldrs are totally protected. even if a thin layer of sand could stick to the pole, this will not affect the correctness of the measurement since the ldrs proved to be able to detect the presence of sunlight even when covered by a small amount of sand. obviously, also the related circuit part has been opportunely secured through ip67 protection boxes, appropriate for such scopes."
"the smartest device, essential for communication among the different nodes of a wsn, is the zigbee network coordinator. the coordinator manages network creation and coordination by selecting a suitable radio channel and defining operating parameters (e.g., personal area network (pan) id, a 16-bit number for network identification). once the network is started, it takes the responsibility of maintaining and controlling it and of receiving and processing data coming from all nodes. the hardware part constituting the network coordinator is formed by:"
"where gain (t, x) is the information gain by applying feature x. entropy(t) is the entropy of the entire set, while the second term calculates the entropy after applying the feature the disadvantage of a decision tree model is over fitting as it tries to fit the model by going deeper in the training set and thereby reducing test accuracy."
a. select the best attribute using attribute selection measures (asm) to split the records. b. make that attribute a decision node and breaks the dataset into smaller subsets.
"wireless sensor networks (wsn) represent in many cases a good option for in situ monitoring infrastructures since they allow continuous, remote and real-time data acquisition. wsn-based monitoring infrastructures have been developed for a wide range of scenarios [cit], and several applications can be found for environmental monitoring [cit] . the deployment of wsns in coastal and marine environments has been widely discussed [cit] : anyway, the greater part of the applications focus on the measurement of water quality [cit] and atmospheric parameters [cit], while to our knowledge, no solution exists focusing on beach morphology as the one proposed in this paper."
"a comparative analysis about the state of the art methods and the proposed wsn solution, focusing on both the advantages and drawbacks, is provided in table 1 . table 1 . advantages and limitations of state of the art methods to measure sediment transport."
"basically, the proposed system relies on existing solutions, enhancing them by adding several technological features and wireless network management and by providing remote, real-time, data control and analysis tools. in this context, particular attention has been devoted to the design and development of the sensor network for the monitoring of height and volume variations of sandy beaches and dunes, and this is the innovative feature of the system herein proposed."
"classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, based on a training set of data containing observations (or instances) whose category membership is known."
"the data acquisition circuit is powered by two 1.5-v aa batteries in series, providing a total 3 v of voltage. at the circuit activation, the hef4060bp counter is started. the oscillation frequency being equal to"
"the training dataset is made up of 21 different attacks out of the 37 present in the test dataset. the known attack types are those present in the training dataset while the novel attacks are the additional attacks in the test dataset, i.e. not available in the training datasets. the attack types are grouped into four categories: dos, probe, u2r and r2l as in kdd dataset."
"the first measurement campaign was carried out over a 24-h time frame on 17-18 [cit] on the coastal sand dunes of the migliarino san rossore massaciuccoli regional park close to pisa, italy, and saw the deployment of three sensor nodes positioned according to a linear layout, perpendicular to the coast line: such a configuration would allow one to evaluate the evolution of a specific cross-section of the sand dune. the three sensor nodes were positioned at a 10-m distance from each other, with the coordinator placed on a pole on the top of the dune, powered with a 10-w solar cell connected to a 12-v lead-acid rechargeable battery. the sampling time of the sensor nodes was reduced from 4 h to 20 min in order to collect a larger number of datasets. the correct functioning of the sensor nodes was confirmed since all three nodes measured the exact sand level value and transmitted it to the remote data collection center. the accuracy of the measured value was manually checked."
"in order to extend the battery life further, the circuital part (i.e., counter and mux), which remains constantly active when the xbee is in sleep mode, must also be put asleep. xbee pin 13 controls the sleep state; therefore, connecting a led between this pin and the ground, it is possible to verify whether the xbee is active or sleeping. in particular, the led is on when the module is awake, and it switches off when the xbee terminates its activity period. xbee pin 13 can also be used to control the power of the circuital part that remains always active. for this purpose, a bs170 mosfet has been integrated in the circuit. in fact, connecting the gate of the mosfet to xbee pin 13, the source to the ground and the drain to the return signal of the circuit, the mosfet acts as a simple switch, which is turned on when the gate receives a high signal (xbee is active), that is when the voltage v gs between the gate and source is higher than the threshold voltage v th . in this way, as for the xbee module, also the rest of the circuit remains active for only 6 min per day. this means that the battery consumption of the circuital part (without the xbee) drops to:"
"the sensor is composed of three main parts: one part, represented by the pole and the array of sensors (formed by elements 1-3), carrying out the physical measurement, a logic part, which is formed by elements 4-7, and the data transmission part, composed of the xbee radio module."
"implementation of techniques for the transmission (through zigbee technology and xbee modules) of data measured by sensors to a coordinator located on the beach and use of gprs network for real-time data delivery to a web server (data collection center), easily accessible by end users; 4."
"the naive bayes algorithm is an intuitive method that uses the probabilities of each attribute belonging to each class to make a prediction. it is the supervised learning approach used to model a predictive modeling problem probabilistically. naive bayes has become one of the most efficient learning algorithms [cit] .naive bayes simplifies the calculation of probabilities by assuming that the probability of each attribute belonging to a given class value is independent of all other attributes [cit] . this is a strong assumption but results in a fast and effective method. the probability of a class value given a value of an attribute is called the conditional probability. by multiplying the conditional probabilities together for each attribute for a given class value, we have a probability of a data instance belonging to that class. to make a prediction, we can calculate probabilities of the instance belonging to each class and select the class value with the highest probability. naive bayes is often described using categorical data because it is easy to describe and calculate using ratios. a more useful version of the algorithm for our purposes supports numeric attributes and assumes the values."
"we take the output(z) of the linear equation and give to the function g(z) which returns a squashed value, the value will lie in the range of 0 to 1. to understand how sigmoid function squashes the values within the range, let's visualize the graph of the sigmoid function."
"the unsupervised machine learning algorithm is used for exploring the structure of the information, extracting valuable insights, detecting patterns and implementing this into its operation to increase efficiency."
"b. kdd-99 has five classes of patterns like normal, dos (denial of service), u2r (user to root), r2l (remote to local) and probe (probing attack). each intrusion category is further subclassified by the specific procedure used to execute that attack. we listed those attacks in table 2 . [cit] investigates anomaly-detection techniques on kdd-99, accounting for this problem by segmenting the dataset and using a stationary partition for training."
"the best configuration for the xbee transmission module (the most demanding part, in terms of energy consumption) has been implemented by making it operate in sleep mode and by allowing a cyclic switching of the entire control logic. considering that the developed system finds its more typical application in a scenario lacking other power sources, in the described way, it was possible to reach the prefixed objective, that is to guarantee for the sensor node, powered by simple aa batteries, as long a life time as possible."
"where a is the area of the lower base, which can be easily calculated since it is a right triangle, and a, b and c are the lengths of the side edges ( figure 3 ). the volumes of the two pentahedra can be summed to obtain an approximated value for the volume of the whole solid enclosed by the four sensor nodes. since two possible subdivisions of the solid are possible, two different approximate values can be calculated: a larger one for the convex subdivision and a smaller one for the concave. the best approximation for the volume can then be obtained by calculating the two volumes and averaging the two values:"
"small cables have been added to connect the gsm shield to the arduino through an icsp (in-circuit serial programming) connector. while this device is characterized by higher production costs than the sensor nodes (around 100$), this does not affect the overall cost of the system since only one coordinator is required for each network."
"specifically, the developed sensor nodes are basically \"sensing poles\", whose height is 220 cm and diameter 3 cm, which are expected to be placed in the sand measuring the variation levels through an array of light-dependent resistors (ldrs) (the sensors' functioning principle will be described in detail in section 5.1). the poles are not free to move: actually, they are expected to be stuck into the ground about 100 cm. thus, in no way could the local dynamics (either wind or wave) offset the sensor nodes. since the diameter of the poles is very small (3 cm), no significant difference is expected to occur on the two sides: in particular, since the accuracy of the measured value is in the order of 5 cm and a maximum expected difference value of 4 cm is expected, this will not affect the measurements. recording the exact position of the nodes by a dgps-rtk (differential global positioning system-real-time kinematic) instrument is crucial since repetitions of data collection campaigns are to be performed periodically in order to compare the results coming from the very same spots. the sensor nodes are expected to be deployed on the backshore of the beach at a level tide fluxes would not reach; the runup of storm waves might reach the sensors, but they would only determine sediment volume displacements, which is what the sensors actually measure. as the system is not invasive, the main factors driving the morphodynamic evolution of a beach, such as wind and waves, are not affected by the sensors. as mentioned, the poles are so thin that sand accumulation because of their presence is negligible, waves reach the sensors for a few seconds only during major storms."
"nevertheless, the buffer overflow problem can be managed by activating two sensors at a time, leaving a time interval of at least two minutes between one activation and the next (two minutes is the time needed by the coordinator to receive, process and deliver the data coming from two sensor nodes to the server). by configuring the network as specified above, each node should be active for just one minute and then remain asleep for a predefined span of time, which in the proposed system has been set to four hours (time shift remains constant during the functioning period)."
2. information gain: it measures the relative change in entropy with respect to the independent attribute. it tries to estimate the information contained by each attribute. constructing a decision tree is all about finding the attribute that returns the highest information gain.
"realization of a web application in java that allows one to receive and store acquired data in a database, for the purpose of allowing the user to monitor network status any time, anywhere."
"by contrast, in order to achieve low costs and reduced power consumption, the sensor node has not been equipped with a data processing unit. this fact has influenced the choice of the network structure to be used. two nodes at a time are activated for data acquisition and transmission to the network coordinator; then, the coordinator processes them and delivers the calculated values to the web server, allowing the user to visualize and control them in real time. each successive pair of nodes is activated at a distance of about 2 min from the previous one, so as to allow the coordinator to carry out its work without interference. when fully operational, once all nodes have been activated, the system can work properly."
"k nearest neighbor (knn) is a managed machine learning algorithm useful for classification problems. knn is a non-parametric and lazy learning algorithm which means there is no supposition for fundamental data distribution. non-parametric means there is no assumption for underlying data distribution. in knn, k is the number of nearest neighbors. the number of neighbors is the core deciding factor. it calculates the distance between the test data and the input and gives the prediction accordingly. the distance is calculated using the euclidean distance formula."
"the system has been tested on two beaches along the tuscany coast (italy) in two different measurement campaigns, a short-term one to check the field operation of the sensors and the overall infrastructure and a mid-term one, to analyze its functioning for a prolonged period. the deployment of a permanent network has currently been discarded due to security reasons since it would require a forbidden-access portion of the beach or dunes to avoid theft and acts of vandalism."
"2) the number of selected records from each difficult level group is inversely proportional to the percentage of records in the original kdd dataset. 3) nsl-kdd has fewer data points than kdd-99, all of which are unique. it is thus less computationally expensive to use for training machine learningmodels."
"intrusions can be defined as the set of actions that attempt to compromise the confidential harmony, integrity or availability of a computer resource. when intruders deliberately gain unauthorized access of the resource, they try to access information, manipulate data, or render information in a system to make unreliable or unusable. an ids is a union of hardware and software components that detect harmful or malicious attempts in the network. ids can monitor all the network activities and hence can detect the signs of intrusions. the main aim of ids is to inform the system administrator that any doubtful activity happened. there are two kinds of intrusion detection techniques:"
"semi supervised learning algorithms work as follows 1. semi-supervised machine learning algorithms use a limited set of labeled sample data to shape the requirements of the operation (i.e., train itself). 2. the limitation results in a partially trained model that later gets the task to label the unlabeled data. due to the limitations of the sample data set, the results are considered pseudo-labeled data. 3. finally, labeled and pseudo-labeled data sets are combined, which creates a distinct algorithm that combines descriptive and predictive aspects of supervised and unsupervised learning."
"to achieve this goal, a wsn has been realized: in particular, ad hoc sensors have been designed and developed for measuring the height variations of the sand layer; network organization and communication protocols have been defined; and finally, a data storage and analysis system has been developed through the implementation of a database (mysql), a web server (glassfish) and a web application (in java language)."
the apriori algorithm is a categorization algorithm. some algorithms are used to create binary appraisals of information or find a regression relationship. others are used to predict trends and patterns that are originally identified. apriority is a basic machine learning algorithm which is used to sort information into categories. sorting information can be incredibly helpful with any data management process. it ensures that data users are apprised of new information and can figure out the data that they are working with.
algorithm uses supervised learning in which sets of data should be labeled for training. +1 is used for normal data and -1 is used for attacked data.
"random forest algorithm is a supervised classification algorithm. we can see it from its name, which is to create a forest by some way and make it random. there is a direct relationship between the number of trees in the forest and the results it can get i.e., the larger the number of trees, the more accurate the result. but one thing to note is that creating the forest is not the same as constructing the decision with information gain or gain index approach. the difference between random forest algorithm and the decision tree algorithm is that in random forest, the processes of finding the root node and splitting the feature nodes will run randomly. there are two stages in random forest algorithm, one is random forest creation, the other is to make a prediction from the random forest classifier created in the first stage. for applications in classification problems, random forest algorithm will avoid the over fitting problem and for both classification and regression tasks, the same random forest algorithm can be used. the random forest algorithm can be used for identifying the most important features from the training dataset."
"since, we are trying to predict class values, we cannot use the same cost function used in a linear regression algorithm. so, we use a logarithmic loss function to calculate the cost for misclassifying."
"a. collecting data: in order to do ids, we need to collect the information about the network traffic like kinds of traffic, hosts and protocol details. b. selecting features: from the large amount of data collected we need to extract the features which we need. c. analyzing the data: the selected features data is analyzed to find whether the data is anomalous or not. d. actions to be performed: ids alarm or alert is made by the system administrator when an attack has occurred, and it tells about the type of the attack. ids also participates in controlling the attacks by closing the network port and killing the processes."
the data point which is located at the minimum distance from the test point is assumed to belong to the same class. the advantage of knn is simple implementation and makes no prior assumption of the data. the disadvantage of knn is the prediction time is quite high as it finds the distance between every data point.
"a.k-means: k-means is an unsupervised clustering algorithm that is used to group data into kclusters. k means it is an iterative clustering algorithm which helps to find the highest value for every iteration. initially, the desired number of clusters are selected. in this clustering method, the data points are clustered into k groups. a larger k means smaller groups with more granularity in the same way. a lower k means larger groups with less granularity."
"since the exact calculation of this volume is impossible because the exact shape of the upper surface is not known, an approximation is required. two ways can be followed: an analytic way based on the interpolation of the four points and then on the volume calculation, or a geometric way based on the approximation of the volume with solid figures and then the use of geometric formulas. the first option would require hard calculations that cannot be carried out by resource-constrained devices such as microcontrollers, forcing then the volume estimation to be moved to the server side of the system, rather than being performed directly on the network coordinator. moreover, the interpolation carried out on the four points placed at the vertex of a quasi-rectangular surface is expected to lead to an approximation of the surface in the best case only slightly better than the one achievable through the geometric method (but possibly also worse). for this reason, a geometric approach has been chosen, based on the split of the solid into two: in particular, connecting two opposite vertices of the upper base, this one is divided into two surfaces that can be approximated to two triangles. two subdivisions are possible according to the pair of vertices that is chosen as shown in figure 3 : a convex one and a concave one. in both cases, the solid is approximately subdivided into two pentahedra with two non-parallel triangular bases and three trapezoidal side faces perpendicular to the lower base. the volume of this figure can be easily calculated through the following formula:"
"in this survey we have introduced an overview of different machine learning algorithms for intrusion detection system (ids) and different detection methodologies, and classifiers for kdd-99 and nsl-kdd dataset. as per the studied of techniques suggested by various authors, the ways it can detect the intruder are presented here. the experiment results show that knn is having high false rate and detection rate but adaboost algorithm has a very low false rate with high detection rate and run speed of algorithm is faster when compared with other supervised algorithms. our further goal is to implement the unsupervised algorithms and find out is there any algorithm which is better than adaboost."
1. entropy: entropy is the degree or amount of uncertainty in the randomness of elements or in other words it is a measure of impurity.
"1. denial of service (dos): dos is an attack category, which depletes the victim's resources thereby making it unable to handle legitimate requests -e.g., syn flooding. 2. remote to local (r2l): unauthorized access from a remote machine, the attacker intrudes into a remote machine and gains local access of the victim machine -e.g., password guessing. 3. user to root (u2r): unauthorized access to local super user (root) privileges is an attack type, by which an attacker uses a normal account to login into a victim system and tries to gain root/administrator privileges by exploiting some vulnerability in the victime.g., buffer overflow attacks. 4. probing: surveillance and other probing attack's objective is to gain information about the remote victim -e.g., port scanning."
"1. activation of the network coordinator: when switched on, the device connects to the gsm network (it requires about 15 s) through the gsm shield and the connected sim card configured with the required parameters (i.e., access point name (apn) and credentials of the network operator). if connection fails, the device is reset and the procedure repeated; 2."
"tracing techniques have also been applied to sand beaches, even though the results were not as successful as on coarse-clastic beaches. in particular, sand composition [cit] and bioclasts [cit] have been used as natural tracers; fluorescent [cit] and radioactive [cit] tracers have been used as artificial tracing techniques. another method to infer sediment transport is frequent topographic surveying [cit] . the most intense modifications to the beach profile are achieved during storms, as high-energy waves deeply rework the subaerial and underwater portions of the beach. therefore, the comparison between the topographic configuration before and after storms would provide the basis for volume shift calculations. the same end results can be obtained with remote sensing technologies, such as lidar [cit] and satellite imagery [cit] . as for sediment tracing, these techniques do not provide sensible information about sand movement during the periods when it should occur the most: the high-energy events. for all these reasons, instruments able to record data series in real time and for prolonged periods, including storms, would provide significant improvements for a better definition of sediment transport."
"the system described in this paper provides a low cost and low energy-consuming technique to measure sand level change in a sandy beach setting. in order to obtain these features, the design has been focused on a careful selection of components (counter, mux and mosfet) and on their circuital connections."
"the obtained value is obviously an approximation since it does not take into account small-scale morphological variations. nevertheless, it is useful to broadly assess the variations in the mass of sand using its density value."
"according to sung and mukkamala [cit], kayacik, [cit] and lee, [cit] most published results observing feature reduction on the kdd cup '99 datasets are trained and tested on the '10%' training set only."
"a. calculate the distance between test data and each row of training data. here we will use euclidean distance as our distance metric since it's the most popular method. the other metrics that can be used are chebyshev, cosine, etc. b. sort the calculated distances in ascending order based on distance values. c. get top k rows from the sorted array. d. get the most frequent class of these rows. e. return the predicted class."
"as the hard structures (groynes, breakwaters) built to protect the coastal areas often turned out to be a failure in the long term (because of side-effects that emphasize localized retreats), recently, coastal managers have preferred to apply softer approaches involving sand redistribution [cit] . artificial replenishment and sand backpassing/bypassing are increasingly used to restore a suffering beach [cit], because they are intended not to overly modify the previous configuration of the coast. the downside of beach fills is the frequent (seasonal to annual) need to integrate the replenishment as waves and currents keep on moving and redistributing the sand, because they do not act to solve the main problem, which is river sediment load diminution. for all these reasons, increasing the knowledge about sediment transport is crucial to gain more information about particle redistribution along and across the beach. in particular, a better definition of transport pathways on sandy beaches would provide an optimization of the interventions in terms of durability, not to mention the possible savings generated by a more accurate sizing of the replenishment. in addition, it would be of major help for the conservation of dunes, both as geomorphological features and as a species habitat. as a matter of fact, lately, dune restoration has been often utilized as an additional soft approach along with replenishment and sand redistribution [cit] ."
"by analyzing and calculating clock cycles at the counter output buffers, it has been considered worthwhile to connect outputs q6, q7 and q8 (i.e., counter pins 6, 14 and 13) to the logical selection signals of the three mux (a, b and c) in the following way:"
"several tests focused on the acquisition times of data collected by sensors and on the processing times of data received by the coordinator. in particular, laboratory tests show that in order to guarantee reliability when receiving data, it is important to acquire data from sensors at intervals of 7 s. smaller intervals would provoke packet losses, because during the processing phase performed at the coordinator, data wait to be processed in the limited input buffer, and additional transmissions coming from multiple nodes would rapidly lead to buffer overflow."
"in this section various machine learning techniques have been utilized to measure classification performance on the datasets and highlight their characteristics. after selecting the features from data, we needed to convert text into binary values and then label the data."
"nevertheless, the solar cell proved to be undersized for the power requirements of the coordinator, not being able to power it continuously for the whole night. however, this fact does not constitute a problem since the sensor nodes can operate only with sunlight. even if the coordinator turned off during the night, the presence of the solar cell allowed him to turn on again in the morning ensuring the continuity of the acquired data."
"the consequences of the erosion affect also the environment, because several habitats are endangered by coastal retreat [cit] . for instance, coastal dunes are most affected by sediment loss from the beach, because wave run-up reaches higher distances on the backshore up to the base of the dunes [cit] . this implies huge volume loss and dune retreat or destruction [cit] and consequently severe problems for the vegetation [cit] . in general terms, coastal erosion occurs in sediment-starving settings: the particles naturally feeding a specific sector of coast no longer compensate those that are wiped out by waves and currents. river bedload is the most important source of beach sediments: due to multiple factors (e.g., river damming, bank armoring, sediment quarrying from the river bed), during the last century, river sediment load has decreased exceptionally, which ultimately had led to the first erosion processes."
"the communication infrastructure is based on a zigbee network where all the sensor nodes act as zigbee routers, in order to fully exploit the advantages in terms of reliability and efficiency coming from the implementation of a multi-hop mesh topology: these features allow the network to be resilient with respect to malfunctions of single nodes and to extend the connection coverage to wider areas than the ones achievable with a single-hop ieee 802.15.4 network."
"1. for each data item, assign it to the nearest cluster center. nearest distance can be calculated based on distance algorithms. 2. calculate mean of the cluster with all data items."
"a set of tests has been performed to analyze the data processing and transmission timings of the devices, as well as to measure power consumption and then to calculate the average life time of the sensor nodes. in this phase, the developed system has been tested according to three different steps."
"the only components whose cost is significant are the xbee module (around 30$) and the rubber pipe (around 10$), while the sum of the price of all the other components is lower than 20$: this means that the overall node can be built with less than 60$. while this is only a prototype, the price could be notably lowered with an engineered solution to be produced in large quantities."
"triggering the orthosis: for this experiment the orthosis was fixed in different positions in a range of [90, .., 180] degrees in 10 degrees steps. in these positions the user performed three different action:, flexion, relaxation and extension of the arm. each action period took 5 s. the routine started with a relaxation phase. afterwards two alternating action combinations (1. flexion/relaxation 2. extension/relaxation) were executed 10 times each. the experiments were recorded separately for each angle starting at 180 degrees. between two measurements a short break of 2 minutes was given to the subjects."
"in contrast to the previously explained experiment, here forces were applied in both movement directions. the force was directly provided by the orthosis in a range of [â2.0, .., 2.0] nm in 0.5 nm steps. note that a negative torque extends and a positive torque flexes the orthosis, a torque of zero is complementary to the free-running-mode."
"the main problem of a paresis is located in the lack of necessary force, associated with concomitant reduction in range of motion and speed of the affected muscles. furthermore, the muscle coordination is impaired; this applies to the muscle chain coordination and to the internal muscle coordination. the muscles tend to a prolonged contraction time and a delayed end of the contraction. moreover, the affected muscles fatigue much faster with respect to the nonaffected muscles [cit] by motor learning the brain is able to adjust to new situations due to massive functional reorganization. this phenomenon is known as neuronal plasticity and characterized by the ability of the brain to restructure itself by forming new neural connections. however, the ability of the brain to restructure is limited since it is not plastic in every of its regions. long term evaluations will reveal the magnitude of the rehabilitation success."
"however, solving this equation is similar to solving a boolean satisfiability problem. thus, we simply leverage the most recent gibbs sampling result Ï * z, obtained during em, for instantiation. this is defined by a function decide as follows:"
"in the first 3 to 18 days after stroke certain neurotransmitters can be detected. it is believed that these substances are important for the neural plasticity. therefore, the first weeks are an ideal time to ensure optimal functional and structural reorganization of the brain. hence, a early active training of the disturbed functions leads to functional recovery [cit] ."
"effects of missing user input. a user may skip the validation of a claim due to being unsure or preferring to check another claim first. we consider such scenarios by a probability pm with which a claim is skipped, meaning that the second-best claim is validated. we test pm ranging from 0.1 to 0.5, while running the validation process until a precision value of 0.7, 0.8, or 0.9 is reached. fig. 8 shows the saved efforts (%), computed as the relative difference in user effort between the normal process and the one with skipping, needed to reach the respective precision. as expected, skipping at the beginning of the validation process (precision level of 0.7) affects the saved effort, as selecting the second-best candidate leads to worse inference results. later, this effect becomes smaller."
"equations (7) and (8) indicate that the sum of logarithmic marginal probabilities in the function exp(Â·) defines statistical independence. lastly, by using u -multiplication, we can generalise definition 3, as follows."
"due to the aging society and probably significant increase in chronic diseases of the musculoskeletal and the nervous system, the need for innovation in assistive technologies for everyday and rehabilitation is judged as very high [cit] ."
"multi-input, single output (miso) model structures were chosen for identification. the learning set always consisted of the first half of samples available, and the validation set of the second half. both muscle (emg) and device information where used: the four inputs of the model are the pre-processed emgs of the triceps and biceps emg tri and emg bi, the torque applied on the orthosis' joint measured via the displacement of a disc spring Ï spring and the angular position values of the orthosis Î¸."
"the above formulation is motivated by the crf being a special case of log-linear models, which, extending logistic regression, are suitable for structured learning tasks [cit] . in our setting, the data has an internal structure via the relations between sources, documents, and claims. exploiting these relations, however, means that the inference of model parameters becomes complex. hence, the potential function needs to be computationally efficient to enable user interactions in the validation process. a log-linear model enables efficient computation, while, at the same time, provides a comprehensive model, in which the features of sources and documents are discriminative indicators for the credibility of the related claims. the weights enable tuning of feature importance, as features vary between applications and shall be learned from labelled data. handling opposing stances. documents may link the same claim with opposite stances-support or refute it [cit] -and a source is considered trustworthy, if it refutes an incorrect claim. a model that only captures that a claim is part of a document would neglect this aspect. yet, incorporating such information via a new type of random variable would mean that the number of variables is larger than or equal to the number of documents, which is much larger than the number of claims (see Â§8). we therefore introduce an opposing variable Â¬c for each claim c. then, model complexity increases only slightly: configurations of c include opposing claims, w contains a doubled number of parameters, and any document connects only to the positive or negative variable of a claim. as c and Â¬c cannot have the same credibility value, we enforce a non-equality constraint:"
"we first measures the response time, denoted by ât, of our approach during one iteration of alg. 1, i.e., the wait time of a user. this includes the time for inference and claim selection. fig . 2 shows the observed response time, averaged over 10 runs, when using the plain algorithm (origin), with uncertainty estimation as introduced in Â§4.1 (scalable), and with the computational optimisations of Â§5.1 (parallel+partition). with larger dataset size (wiki to snopes), the response time increases. however, with computational optimisations, the average response time stays below half a second, which enables immediate user interactions. fig. 3 further illustrates for the largest dataset, snopes, how the response time evolves during validation when averaging the response time over equal bins of relative user effort. the response time peaks between 40% and 60% of user effort, since at these levels, user input enables the most conclusions on credibility values."
a first heuristic to guide the selection of claims for validation aims at the maximal reduction in uncertainty under the assumption of trustworthy sources. it exploits the benefit of validating a claim using the notion of information gain from information theory [cit] .
"to recover or improve motor function, a preferably early and intensive rehabilitation is recommended, since a positive relationship between treatment intensity and outcome exists [cit] . however, this requires a high and efficient deployment of personnel, which can be a limiting factor. in this context the use of robot-aided-therapy is worthwhile [cit] ."
"note that we do not need to rank the opposing claim Â¬c of a claim c, as their conditional entropies in eq. 14 will be equivalent."
"in the statistical inference, we naturally use arithmetic operators, such as multiplication or division, for probability values. for instance, statistical independence can be defined with multiplication of marginal probabilities. we can generalise these operators with an appropriate monotonically increasing function u(Â·) and its inverse function Î¾(Â·)."
"this method is widely used in engineering for identification of several dynamic systems, e.g., electromotors. for a more detailed description and an example of the use of this algorithm for identification of a nonlinear model, please see [cit] ."
"we follow common practice [cit] and use the ground truth of the datasets to simulate user input. model parameters are initialised with 0.5, following the maximum entropy principle."
"the log-likelihood optimisation is convex, since the logarithm is monotonically increasing and the probability distribution is in exponential form. however, the problem becomes intractable due to the exponential number of configurations to consider for the random variable c u . moreover, upon receiving new user input, c l and c u, and hence c l and c u change, so that re-computation is needed."
"another device group compensates physical limitations in daily life of patients. a representative example of these systems is the commercially available wilmington robotic exoskeleton (wrex), which relieves the weight of the arm by elastic slings. thus, the user is able to operate with exclusion of gravity [cit] ."
"anyhow, in the first step we decided to design and develop an orthotic system which accounts with one active degree of freedom (dof) capable of flexing and extending a patients arm in parallel to the upper body exoskeleton. this gives us the opportunity of analyzing and developing physiological data driven control strategies for an exoskeleton in a easier to handle setup compared to full upper body system. therefore, in this paper we introduce the concept and medical background of a support and rehabilitation system for the upper limb in the form of an active elbow orthosis."
"in this paper we conduct experiments to improve the control of the system. we derived a model with the recursive least square (rls) algorithm which takes into account emgs from the upper arm as well as sensor data from the orthosis (position and force induced to the device). off-line gained results for the triggering of the orthosis are provided. in addition, we present results of an experiment giving the orthosis the ability of adjusting the level of support provided to the patient. again a model is formulated with the rls involving the same input data. having the possibility of automatic support adjustment during rehabilitation can have positive effects since an increasing muscle activity results in a lower support. in this way the support is constantly reduced until the point the patient does not need any external help for moving his arm."
"using this notion, we chose the claim that is expected to maximally reduce the uncertainty of the probabilistic fact database. this yields a selection function for information-driven user guidance:"
"nevertheless, in future we will investigate the transferability of a model from one subject to another, or work into the direction of adaptive models. further, the model reliability for more natural movements will be evaluated. it can be expected that the parameters of the presented models derived with data acquired under a controlled setting will vary for natural movements. however, the model structure might remain the same. followup experiments with daily life task, e.g., grasping a glass, getting up from a chair or lifting items, are planned to verify this."
"together with the patient, specialised everyday and (if necessary) job-relevant therapy goals are determined. the treatment team selects an appropriate therapy concept [cit] . the various therapeutic measures for arm rehabilitation which can be transferred to a robotic system, include repetitive training, uniand bilateral training, the effect of training at distal positions, task-oriented training and mirror therapy [cit] ."
"exoskeleton or orthotic systems for medical applications in rehabilitation should support scientifically founded training principles. furthermore, these systems should optimally be integrated into the rehabilitation routine and must support the therapist and patient in a useful way. the general aims of rehabilitation robotics relate mainly to the increase of efficiency, accuracy and reproducibility of the treatment methods while ideally improving the economic situation in rehabilitation [cit] . in addition to the general objectives, concrete goals have to be formulated. these goals are: -imitation of natural and patient-specific motion -high degree of compliance control (this promotes a safe man-machine interaction) -self-initiated movement support and patientcooperative control strategies (this promotes motor learning processes in the brain) -solid monitoring of the treatment-progress -early intensive training"
"finally, we investigate practical issues when deploying our validation framework. a challenge for such an evaluation is that it is difficult to find experts that are knowledgeable in the domains covered by the annotated datasets. therefore, we consider a setting that features supporting information for the validation. to derive this supporting information, we queried the google search engine with the text of each claim and extracted the first ten search results as a list of documents. the list of documents is then shuffled for each validation task to avoid biases by the search engine or the user. due to budget constraints, we selected 50 claims randomly for each dataset. then, we considered two different types of users:"
"we now lift our approach to a streaming setting. instead of checking a large set of claims from scratch, we consider a potentially infinite stream of claims to validate. upon the arrival of new documents, sources, and claims, the model structure and its parameters need to be updated. however, evaluating the parameters periodically based on the complete database is not a viable option, as the database grows continuously. limiting the number of considered claims, in turn, may induce a loss of all claims provided by a source. since only a (small) subset of documents is observed per source, operating on a subset of claims increases the risk of discarding trustworthy sources and documents."
"the resulting model of the triggering function y1 mod, given by equation 6, fits the real triggering function excellently (mean absolute error less than 0.001) and can be seen in figure 6 . here, a value of â1 represents a triggering of an extension movement, 1 represents a triggering of a flexion movement and a value of 0 corresponds to the resting state (no triggering). one can see from these figures, that the biggest model errors are reached at the phases with no active movement. we assume this is due to sensor noise in the used magnetic position encoder. further, a second model that can help to determine the level of support needed from the orthosis was identified. the input signals chosen are the same as in the first model. in order to determine the real level-of-assistance-function, in a first step experiments were performed with different external weights carried with the hand while moving the arm with the device (see section 4.1)."
"our complete validation process for fact checking is defined in alg. 1. it instantiates the general validation process outlined in Â§2.3 to address the problem of effort minimisation (problem 1). as long as the validation goal is not reached and the user effort budget has not been exhausted (line 6), selection of the claim for which user input shall be sought is done either by the source-driven or the information-driven strategy. the choice between strategies is taken by comparing factor ziâ1 to a random number (line 8), which implements a roulette wheel selection. the second step (lines 10-13) elicits user input for the selected claim and computes the error rate. the third step incorporates the user input in the probabilistic model (line 14) and then conducts credibility inference by means of our icrf algorithm (line 15). this yields a new probabilistic model pi, along with the gibbs sampling result Ï * i of the last e-step. based thereon, in a fourth step, we decide on the new grounding gi capturing the facts that are considered credible (line 16). the ratio of unreliable sources ri is calculated to compute score zi (lines 17-18), used in the next iteration to choose between the selection strategies."
"truth finding on the web. given a set of claims of multiple sources, the truth finding (aka fact checking) problem is to determine the truth values of each claim [cit] . existing work in this space also considers mutual reinforcing relations between sources and claims, e.g., by bayesian models [cit], maximum likelihood estimation [cit], and latent credibility analysis [cit] . however, these techniques neglect posterior knowledge on user input and rely on domain-specific information about sources and data, such as the dependencies between sources and temporal data evolution [cit] . the fact checking literature, however, focuses on the classification of claims by credibility, based on a fixed training data. this can be seen as the starting point for our work: we put an expert user in the loop to clean the results obtained by automated classification. our guidance strategies therefore complement the literature on classifying claims in identifying which potential errors of a classifier are most beneficial to validate by an expert user. at the same time, our approach can also support an expert user in building up a fact database from scratch, in a pay-as-you-go manner. moreover, our approach goes beyond recent work on offline fact checking, e.g., [cit], by including a streaming process to incorporate new claims on-the-fly."
"the current version of the system has one active degree of freedom and four passive joints that are required to compensate misalignments and one actuated joint to support the flexion/extension movement of the elbow joint (see figure 3) . the active joint is driven by a 24 v maxon a-max 22 dc-motor with a 333:1 maxon planetary gear and a 4:1 worm wheel gear. for a natural force interaction, safety reasons, and to measure the applied force interaction, the actuated joint is compliant. this compliance is generated via serial elasticity in the worm wheel gear set-up. the worm is axial moveable and centred in the gear via disc springs. in case a load is applied, the worm is pushed to one side and thus, the spring is compressed on this side. the position of the worm wheel is measured with a bahluff inductive sensor. in this way the applied load can be calculated. the position of the joint is measured with an ic-haus-mh position encoder. furthermore, the used electronics consist of a stm32f103ve microcontroller, offering several data acquisition (gpio) and communication (usart, can-bus) ports, and a bd6232 custom made pwm h-bridge driver. the used dc-drive can generate a torque of about 16 nm."
"in this section the experimental results are presented. first, the triggering function of the orthosis, which defines when and in which direction the system should move was approximated."
"all experiments where performed by one subject in an upright position with emg electrodes and orthosis equipped to the subjects right arm. a monitor on a table in front of the subject was used to give commands (flexion, relaxation, extension). this stimulation was implemented with the presentation software (neurobehavioral systems, inc., albany, usa). the given commands were marked in the measured emgs. we designed three different experiments which are explained in the following:"
this experiment was conducted to formalize a model which can modulate the level of assistance provided by the orthosis in dependence of the weight the operator has to lift.
"there is a trade-off between the information-driven and the sourcedriven strategy for user guidance. focusing solely on the former may lead to contamination of the claims from trustworthy sources by unreliable sources. an excessively source-driven approach, in turn, may increase the overall user efforts significantly. thus, we propose a dynamic weighting procedure that to choose among the two strategies. this choice is influenced by two aspects:"
"in this work we presented some background information and facts that support the use of robotic systems for rehabilitation processes. further, an overview in exoskeleton technology was given. starting from this knowledge, concepts on the feasibility of exoskeleton technology for home rehabilitation were given and dicussed by means of a demonstrator, an active 1-dof-elbow orthosis. its possible application, design and mechanics, and control were presented. to summarize, the system allows to support self initiated movements that are normally executed by both upper arm muscles m. biceps brachii or m. triceps brachii."
"nevertheless, in contrast to the two above mentioned studies, where the torque produced by the muscles was computed, in the present work, control signals as well as the torque needed for assisting the user were modelled."
"an example for an end-effector-based approach, which is also widely used in modern therapy, is the inmotion arm robot (former mit manus). this system simulates the classical hand-to-hand therapy of a therapist with a continuous measurement of position and force applied to the arm of the patient. it is also equipped with a visual feedback which allows to address even complex tasks [cit] . a drawback is that the system is stationary and restricted to planar movements."
"combining the techniques for credibility inference and instantiation of a grounding ( Â§3) with those for user guidance ( Â§4), we define a comprehensive validation process ( Â§5.1). we further outline how robustness against erroneous user input is achieved ( Â§5.2)."
"the extended nb models with empirical marginals are called empirical u -nb models in this paper. in the next subsection, we experimentally evaluate empirical u -nb models by using benchmark datasets."
"furthermore Ï(t) is the predictor regressor vector, or the vector of measurable signals, in which the real input values u(t) and the real output values y(t) are fed:"
"in the field of system identification the recursive least squares (rls) method is a basic estimation method [cit] . its principle is simple and relatively easy to use. in most of the cases the algorithm delivers high accuracy, fast convergence of parameters and high modelling efficiency. what also makes this algorithm attractive is the fact that it can easily be extended for identification of more complex and nonlinear models. let t be the actual time step. the general structure of the rls algorithm is given by:"
"we evaluate our approach experimentally, using real-world datasets. we first discuss the experimental setup ( Â§8.1), before turning to an evaluation of the following aspects of our approach:"
"nevertheless, approximately 35% of stroke survivors live in long term with a significant leg paresis and 65% are not able to use the affected hand at daily activities. to this already high level of suffering, patients often suffer from depression, a so called poststroke depression. the resulting psychological problems can prevent a successful motor therapy, since the motivation and cooperation of the patient plays a crucial role [cit] ."
"user guidance. guiding users has been studied in data integration, data repair, crowdsourcing, and recommender systems [cit] . most approaches rely on decision theoretic frameworks to rank candidate data for validation. despite some similarities in the applied models, however, our approach differs from these approaches in several ways. unlike existing work that focuses on structured data that is deterministic and traceable, we cope with web data that is unreliable and potentially non-deterministic. also, instead of relying on two main sources of information (data and data provider), we incorporate individual features as well as direct and indirect relations between data types (sources, documents, claims)."
"in general, independent living and acting is strongly connected with the motor skills of the individual. the proper function of the arm and hand in everyday activities -at work or at home -are of vital importance [cit] ."
"in this paper, we proposed an approach to overcome the limitations of existing methods for automatic and manual fact checking. we introduced an iterative validation process, which, based on a probabilistic model, selects claims for which validation is most beneficial, infers the implications of user input, and enables grounding of the credibility values of claims at any time. we further proposed methods for early termination of validation, efficient batching strategies, and a streaming version of our framework. our experiments showed that our approach outperforms respective baseline methods, saving up to a half of user effort when striving for 90% precision."
"-reproduction of the human kinematics with a high number of active degrees of freedom -haptic features: the exoskeleton / orthosis design allows transferring haptic functions at certain points throughout the patient's arm. possible types of haptic feedback are kinaesthetic feedback (force-feedback) and tactile feedback (e.g. vibrations) -modular design: exoskeleton / orthosis with a segmental structure allows to provide a patientspecific system which meets the requirements and needs of the patient, in the sense that an exoskeleton including only the affected joint would be used. this can reduce costs and deliver a differentiated training, following the slogan: \" as much as necessary, as little as possible\" -force intensification: due to the active degrees of freedom and mobility, an exoskeleton/orthosis can be used as a power amplifier. through this option, it is possible to deliver a system that supports activities of everyday life and acts in the same time as a training system."
"this section describes how muscle activity was measured and processed in order to use it as a control signal for the orthosis. since the orthosis is designed to actively flex and extend the operators forearm, emgs were measured at the biceps and triceps, which are the muscles mainly involved in the process of flexion and extension. ag/ag-cl electrodes were placed in a bipolar arrangement on the middle of the muscles in direction of the muscle fibers. the signals were acquired with a sampling frequency of 1000 hz using a brainamp exg mr amplifier (brainproducts gmbh, gilching, germany). the emgs were preprocessed in two consecutive steps. first a variance based filter [cit] was applied. this filtering step eliminates motion artifacts and enhances the signal-to-noise ratio of emg signals. the length of the filter was set to 50 ms. basically a sliding window of length 50 ms is passed to the signal with a stepwidth of 1 ms. the variance of the whole window is assigned as the new value of the last sample inside the window, resulting in the filtered signal. in a second step the root mean square (rms) of the signal was calculated. for the calculation, again a window of 50 ms was used, but in this case the stepwidth was chosen as 50 ms so that the windows did not overlap. the resulting signal had a frequency of 20 hz. in this way we obtained the same sampling frequency as the sensors of the orthosis."
"the next step is to integrate the obtained models into the orthosis' control system, turning it into a model-based control scheme. it is expected that with the predicted triggering-and torque-assistive function the control system of the active orthosis will improve in two points: in the discrimination of time and direction of movements, and on the other side in the precision at defining proper assistive set torque values, according to the current state of all measurable signals in the system."
"truth finding is also known as knowledge verification [cit] and credibility analysis [cit] . existing automatic techniques mostly look at features of data, such as number of relevant articles, keywords, and popularity, which are noisy and can be easily dominated by information cascades [cit] . again, posterior knowledge on user input cannot be incorporated. also, approaches based on gradientdescent [cit] only optimise model parameters, but neglect external probability constraints. fact extraction may be performed by diverse data representations, e.g., knowledge bases [cit], web tables [cit], semi-structured data [cit], or free text [cit] . other work uses cooccurrence information and evidential logs [cit], but is limited to quantitative information such as identifying unpopular facts based on the number of mentions [cit] . our work is orthogonal to all the above mentioned. by relying on an abstract data representation, our model is not specific to a particular domain. our principles of user guidance can further be adapted for many of the above techniques, exploiting its generic notion of uncertainty."
"the requirements on a social, well functioning and modern health care system -including elderly care -are demanding: it must be flexible enough to encounter the increasing process of change and the related challenges. these changes and challenges are triggered, among other things, by the demographic changes, the increase in chronic diseases, the rising costs and the impending skills shortage [cit] ."
"the active range of motion of the elbow orthosis corresponds to the anatomic workspace of the human joint and is individually adjustable to each subject. if the position of the joint exeeds the workspace limits defined for the user, the reference torque is automatically set to zero and the system can only be controlled via buttons."
"when relying on accurate facts, incorporating manual feedback is the only way to overcome the limitations of automated fact checking. however, eliciting user input is challenging. user input is expensive (in terms of time and cost), so that a validation of all claims is infeasible, even if one relies on a large number of users (e.g., by crowdsourcing) and ignores the overhead to resolve disagreement among them. also, claims are not independent, but connected in a network of web sources. an assessment of their credibility thus requires effective propagation of user input between correlated claims. finally, there is a trade-off between the precision of a knowledge base (the ratio of credible facts) and the amount of user input: the more claims are checked manually, the higher the precision. however, user input is commonly limited by some budget. this paper presents a comprehensive framework for guiding users in fact checking, adopting a pay-as-you-go approach. we present a novel probabilistic model that enables us to reason on the credibility of facts, while new user input is continuously incorporated. by (i) inferring the credibility of non-validated facts from those that have been validated, and by (ii) guiding a user in the validation process, we reduce the amount of manual effort needed to achieve a specific level of result precision. credibility inference and user guidance are interrelated. inference exploits mutual reinforcing relations between web sources and claims, which are further justified based on user input. moreover, a user is guided based on the potential effect of the validation of a claim for credibility inference."
"in a classification task based on a dataset with large m, the model given by equation (19) has some computational advantages. since u(Â·) is a monotonically increasing function, the map estimate of the class y for given x is derived as follows,"
"in the following the torque control system of the proposed active orthosis will be presented. this can be visualized in the simplified block diagram in figure 5 . compared to the systems mentioned above, the torque that is applied to the orthosis' joint is measured making use of the compliance in the joint itself, as it will further explained below. the general control structure is designed to be cascaded, while the main and inner loop of the control architecture is a torque control loop. the dcdrive of the device is provided with two disc springs performing the serial elasticity of the drive. these springs deflect when load is applied to the joint. one is used for movements that are directed upwards and one for movements that are directed downwards. the inductive sensor detects this deflection d. with these measurements it is possible to obtain a nearly linear function between the spring deflection and the actual torque applied to the joint, Ï a . a feauture of this method is that it gives an accurate measurement for closed-loop actuator force / torque control without the need of calculating or measuring the armature current of the motor [cit] . the set (desired) joint torque Ï s is fed externally via usart port. at this time the reference values are defined in a common pc, later Ï a will be calculated online using dynamic models presented in section 5. the difference between these two torques is the control error e, which is propagated into an anti-windup pid controller. the performance of the control system was verified with weight discs in order to simulate values for Ï a, and giving the corresponding Ï s to the system, resulting in an acurate balancing of the weights. furthermore, the resulting measured torque was compared with the deflectiontorque curve depicted above."
", where the i-th vector element denotes the credibility of claim ci. instantiation will return [cit] as this configuration appears most often, so that its probability is maximal."
"in the long term view, our superior goal is to design and develop a full home rehabilitation system, composed of an exoskeleton, physiological data acquisition and processing in terms of eeg, emg and gaze-direction, which can be operated in real as well as in virtual environments. for this purpose we will make use of our aquired expertise, gained with the development of exoskeleton systems in recent projects [cit] . the exoskeleton shall be lightweight and comfortable to wear, while having enough force to move both plegic arms and the upper body of a patient. with the use of physiological data movement intentions of the patient can be predicted and therefore the interaction between subject and rehabilitation device can be improved. the system is meant to support a therapist in the daily routines during the rehabilitation phase of a patient. when using the system in a virtual environment it is possible for the therapist to design and change tasks for the patient without being on sight and in addition the training/rehabilitation success can be monitor via physiological data, e.g., emg signals."
"to address the problem of effort minimisation, we argue that a user shall be guided in the validation of claims. in essence, user input shall be sought solely on the 'most promising' unverified facts, i.e., those for which manual validation is expected to have the largest impact on the estimated credibility of the resulting grounding."
"where â and â are multiplication and division operators based on the function u . in this paper, the generalised operators â and â are called u -multiplication and u -division."
"in practice, users that validate claims face significant set-up costs, implied by the need to familiarise with claims of a particular domain. it therefore increases user convenience and efficiency if the validation process considers a batch of claims per iteration. we support such batching by a greedy top-k strategy to select a set of claims with a high joint benefit for credibility inference."
"for example, the swiss company hocoma ag provides a therapy concept with three different rehabilitation systems for upper limb rehabilitation. this therapy concept is based on task-oriented train-ing scenarios in a virtual environment, which facilitates treatment of neurological diseases of different severity. 3 the three therapeutic robots are: armeopower (former armin) -a robotic arm exoskeleton [cit], armeospring -an exoskeleton with integrated spring mechanism (emerging from t-wrex exoskeleton) [cit] and armeo-boom -an overhead sling suspension system (emerging from the robar project) [cit] ."
"to avoid any danger for the user, various safety aspects are considered. to this end, the orthosis' working range is limited by mechanical stops. furthermore, at too high forces the forearm interface will release from the orthosis."
"the bregman divergence is a pseudo-distance between two functions, which is characterised by a convex function. conventional statistical inference is based on the kl divergence that is a special case of the bregman divergence, but sometimes results in poor estimation, especially when the sample set is very small or includes outliers. some divergences belonging to the bregman class show robustness against small sample sets and outliers, and with such divergences, statistical algorithms like the boosting or the em algorithm are generalised and improved [cit] ."
"since an additional and unilateral load can represent a major influence on, e.g., neurological patients, the orthosis' weight with respect to the user must be kept as low as possible. therefore, the orthosis' materials are a combination of carbon reinforced plastics and polyamid pa6, for a lightweight, robust and stiff design. additionally, a carrying system was developed, which distributes the weight of the device on both shoulders. figure 2 and 4 show the current design concept of the orthosis."
"this experiment was conducted to record data to build a model that enables to trigger the movement direction which was intended by the user and is supported by the orthosis as well as the relaxation phase where the orthosis stays in a fixed position. the different starting positions were used, since in each position the muscles are contracted to a different amount, which leads to diverse signal shapes."
"greedy selection. exploiting the monotonicity and submodularity of the utility function f, we define a greedy algorithm with a performance guarantee of (1 â 1/e) â 0.63 [cit] . we iteratively expand the set of claims in k iterations. in each iteration, we traverse all unlabelled claims to identify the claim c * to maximise the gain â(c"
"the nb has some convenient properties, such as simple structure, easy estimation and scalability. and it is also known as a simple but robust classification tool [cit] . with the empirical joint distributionp x y, the ml estimate of the nb model is given by"
"alternatively, the orthosis can be manually operated via two buttons at any time, supplying a constant voltage of Â±15 v. this allows corrections and re-positioning of the joint if needed."
"as an example, consider the snopes dataset [cit], a collection of 4856 claims derived from 80421 documents of 23260 sources, such as news websites, social media, e-mails, etc. for instance, this dataset comprises the claim that eating turkey makes people especially drowsy. this claim can be found in documents of various web sources, among them earthsky.org [cit], webmd.com [cit], and kidshealth.org [cit] . in the snopes dataset, claims have been validated by expert editors, which corresponds to the user input in our model. it labels the aforementioned example claim as non-credible [cit] ."
"this section describes the conducted experiments. the idea behind the experiments was to obtain models, that can help to study and to predict important aspects of the behavior of the active orthosis."
"to assure the achievement of these objectives in medical care, the publicly financed science plays a major role. in this context robotics research is an important element which is increasingly gaining significance [cit] ."
"using an own matlab implementation of the rls algorithm, and following the experiment descriptions in section 4.1, models of important dynamic relationships of the active orthosis were identified. the parameters of the models were adapted, in specific, with equation 1. the motivations for the identification of the models presented in the following paragraphs were that particularly in rehabilitation, on the one side, the need of an accurate discrimination of movement direction is of major importance, and on the other side, the requirement of a correct computation of the level of support currently needed by the user, according to all measurable states of the system, including muscular states."
"the resulting model of the support-function y2 mod is given by equation 7. the performance of the modelling algorithm can be seen in figure 9 . one can see the prediction error is bigger at the beginning of the measurements. [cit] samples. this initial error could probably be reduced with further fine-tuning of initial model parameters. throughout the validation set, only isolated deviations, which are not significant (caused by sensor noise), can be seen."
"two dynamic models, identified with the rls algorithm were shown. the first one computes the triggering function of the system from the muscle-and sensor information from the orthosis, while the second one computes a function describing the level of support needed to counteract external forces. both models show excellent performance in matching the real signals."
"further these signals can be used to move the patients arm in a self motivated way. in later stages of treatment the patient should regain more and more muscle strength. therefore, the orthosis has to adjust its assistance level via the measured muscle activity, in a way that higher muscle activity leads to a lower level of assistance."
"as denoted in the previous subsection, the joint expression of the u -independent distribution is affected by the form of the function u, however it is reduced to Î½ x for any function u in the case that all the marginals are uniform distributions. this fact indicates that the space of empirical u -independencep â (u) is not a rich subspace in p x if the empirical marginals are close to uniform. on the other hand, when the marginals are far from uniform and have extremely high (or low) probabilities because of small sample sets or outliers, empirical u -independent models can be flexible and convenient candidates."
"we introduced generalised multiplication based on the monotonically increasing functions which is deeply related to the bregman divergence and proposed some extensions of independence in statistical models. to reduce computational cost, we also proposed the empirical u -independent model which has robust property attributable to the approximated bregman divergence. in addition, we show effectiveness of u -independence in the nb model for simple classification tasks by using some benchmark datasets. when we use the u -nb model as a classifier in a practical scene, we have to handle very small probability values, particularly, when the dimension of x, given as m, is very large. in the conventional nb model, the log-transformation is a convenient tool to handle these small values because the nb model is decomposable by using the logarithm function as follows, however, the log-transformation is ineffective for u -nbs like equations (17) and (18) since we still need to handle u -multiplied small probabilities in these models. one way to avoid this problem is given as follows; we can extend the u -nb in the following way,"
"in this section, the basic elements of rehabilitation robotics are presented. this includes the medical background for rehabilitation, the basic functions of robotic systems and applications, target groups and transferable treatment modalities which can be suited to adapt on rehabilitation devices."
"user guidance as introduced above assumes that sources are trustworthy-an assumption that is often violated in practice. to tackle this issue, we model source trustworthiness by explicitly aggregating over all claims made by a source. more precisely, the likelihood that a source is trustworthy is measured as the fraction of its claims that are considered credible. the latter is derived from the grounding gz instantiated in the last, the z-th, em iteration:"
"level of assistance (flexion): for this experiment the orthosis was operated in free-run-mode. the operator had to lift weights in the range of [1, .., 4] kg in 1 kg steps. the experiment started with the users arm fully extended. the subject had to flex and extend his arm for 10 times, each action period was again 5 s. the experiments were recorded separately for each weight starting with the lightest. between two measurements again a short break of 2 minutes was given to the subjects."
"update time. we measure the response time during one iteration of alg. 2, i.e., the update time of the model when a new claim arrives. we run the update process from 0% to 100% of claims in the order of their posting time, for each dataset. the average update time for the wiki, health, and snopes datasets are 0.34s, 0.61s, and 1.22s respectively. as such, the response times turn out to be similar to those of alg. 1, as implied by prop. 2 and prop. 3. preservation of validation sequence. as explained in Â§7, the algorithms for streaming fact checking (alg. 2) and validation (alg. 1) run in parallel and update the model parameters. this leads to the question of how to interleave both algorithms: validating claims early may not be beneficial as later arriving claims help in user guidance. to answer this question, we compare the validation sequences between the offline setting and the streaming setting as follows. we run the streaming algorithm from 0% to 100% of claims in the order of their posting time, and periodically invoke the validation process, where a claim is selected from the existing claims for validation (hybrid strategy, current model parameters provided by the streaming algorithm). we record the validation sequence and compare it with the offline setting using kendall's Ï Î² rank correlation coefficient [cit] . it ranges from â1 (reverse order) to 1 (same order), quantifying the similarity of the ranking in two validation sequences. table 2 presents the result when varying the validation period from 5% to 30% (e.g., validation is invoked after every 5% of new claims arrive). increasing this period, the validation sequence of streaming fact checking becomes more similar to the static setting."
"having discussed (i) inference based on user input and (ii) instantiation of a grounding, we turn to strategies to guide a user in the validation. this corresponds to the first step of the validation process presented in Â§2.3, i.e., the selection of a claim for validation. we first define a measure of uncertainty for a probabilistic fact database ( Â§4.1). then, two selection strategies are introduced ( Â§4.2 and Â§4.3), before they are combined in a hybrid approach ( Â§4.4)."
"in addition to the emgs, we recorded sensor values from the orthosis. these are the position and the force induced into the orthosis by the operators arm. the signals were acquired with 20 hz and send via rs-232 to a computer where they were stored. in order to synchronize the emg and sensor data, we marked the beginning and end of the orthosis measurements in the emg. since the preprocessed emgs have the same sampling frequency as the orthosis sensor data, the corresponding emg part could be cut and merged into one time series together with the sensor data. [cit] (the mathworks inc., natick, usa). for loading the emg data the eeglab-toolbox (swartz center for computational neuroscience, san diego, usa) was used."
"to assess the credibility of claims, automated methods rely on classification [cit] or sensitivity analysis [cit] . while these methods scale to the volume of web data, they are hampered by the inherent ambiguity of natural language, deliberate deception, and domainspecific semantics. consider the claims of 'the world population being 7.5 billion' or 'antibiotics killing bacteria'. both represent common-sense facts. yet, these facts have been derived from complex statistical and survey methods and, therefore, cannot easily be inferred from other basic facts."
"in order to reach the goals in rehabilitation described in section 1, and to study and transfer first rehabilitation concepts to exoskeleton technology, a demonstrator which is presented in this section, was designed."
"applying alg. 1 in practice, the computation of the information gain for the information-driven or source-driven selection strategy becomes a performance bottleneck. therefore, we consider two optimisations for this step:"
"in practice, we can improve efficiency by terminating the validation process upon convergence of the results. below, we define several criteria that indicate such convergence and, therefore, may be employed as additional termination criteria."
"for a variety of diseases a motor rehabilitation by means of a robotic device is in general conceivable. we are of the opinion, that an application is particularly suitable and economically useful, where long rehabilitation periods are necessary. this applies, for instance, to chronically or chronic-progressive neurological diseases in which a regular, persevering training a rapidly progression of symptoms and sequelae of diseases might be prevented. these are in addition to the surgical and orthopedic diseases, such as elbow and shoulder joint prosthesis, brain tumor surgery, and muscle weakness due to immobilization and surgical follow-up care (mobilization), in essence, the neurological diseases. examples of such neurological diseases are multiple sclerosis, peripheral nerve lesions, traumatic brain injury, infantile cerebral palsy and as mentioned stroke. stroke is a sudden onset of dysfunction of the motor, sensory and cognitive functions of the brain. depending on the location and severity of the injury, the functional limitations may vary."
"proof. the time complexity of the iteration of alg. 1 is dominated by the icrf algorithm, which infers the implications of new user input. yet, icrf runs in linear time in the dataset size (prop. 1)."
"going beyond this trade-off, we aim at minimising the user effort invested to reach a given validation goal. we consider fact checking as an iterative process with a user validating the credibility of a single claim in each iteration. this process halts either when reaching a validation goal or upon consumption of the available effort budget. the former relates to the desired result quality, e.g., a threshold on the estimated credibility of the grounding. the latter defines an upper bound for the number of validations by a user and, thus, iterations of the validation process."
"adopting the above model, the grounding g to derive a trusted set of facts is partially derived from user input. however, manual validation of claims is expensive, in terms of user hiring cost and time. user input is commonly limited by an effort budget, which leads to a trade-off between validation accuracy and invested effort."
"in the online setting, we consider an em algorithm with stochastic approximation to update the likelihood with a new claim ct, a new source st, or a new document dt, rather than conducting re-computation. specifically, the update rule is defined as:"
"user guidance with mistakes. we further study the effect of user mistakes on the relation between user effort and precision. again, the confirmation check is triggered after each 1% of total validations. upon a detected mistake, the user reconsiders the input, which adds to the invested effort. fig. 7 illustrates that this implies that more user interactions are required to reach perfect precision. however, the precision curves obtained with our approach are still much better than with other baseline methods."
"the idea is to use the device in the early stage of treatment to passively move the patients arm. with therapy in advanced stages the residual muscle activity will be measurable again. this low residual activity may not be sufficient for moving the arm, but may result in myoelectric signals. by measuring these signals with emg, they can be used to detect the patients movement intent."
"the validation goal could be the precision of the final grounding gn, estimated by cross validation. note that, in theory, problem 1 could have no solution-the effort budget may be too small or the validation goal may be unreachable. however, for practical reasons, there needs to be a guarantee that the validation process terminates."
"the derived models are patient and session specific and therefore have to be trained individually. emgs of a single subject may vary from session to session, due to slightly different electrode positions or resistances and due to different levels of muscle fatigue. effects between subjects are likely even bigger, since the physical conditions of the muscles are completely different."
"statistical models based on some kind of independence, such as naive bayes (nb) models, bayesian networks [cit] or aspect models [cit], are broadly used in various situations; the assumption of independence is attractive in modelling relation between categorical variables with a lot of categories because the composed independence model may have a significantly smaller number of parameters than the model denoting dependence. technically, the assumption of independence in these models should be introduced by analysing a dataset. however, in practical scenes, the models are casually used without rigorous analysis; e.g., in classification problems, it is known that the nb model shows good performance even if the assumption is violated [cit] . in this paper, we introduce a generalisation of independence and propose an extension of statistical models based on statistical independence to express weak special dependence with the small number of parameters."
"based on the validation process introduced so far, this section presents methods to further reduce the required user effort. detecting convergence of our probabilistic model, we discuss when to terminate validation ( Â§6.1). reducing set-up costs of a user, we then target batching of claims ( Â§6.2)."
"where i 1 and i 2 are the number of elements in x 1 and x 2 . then, the following property holds with any types of u-function,"
"regardless of the type of rehabilitation device, most systems own one of the three basic classes of rehabilitative control strategies. these control strategies are referred to as passive-assistance, assist-asneeded and challenge-based [cit] . focus of current research are mainly assist-as-needed techniques, which support the user only as much as it is necessary [cit] . within this context there is even the possibility to act against the users movements and thus challenge the patient during task execution [cit] ."
"extracting factual knowledge from web data plays an important role in various applications. for example, knowledge bases such as freebase [cit], yago [cit] and dbpedia [cit] rely on wikipedia to extract entities and their relations. these knowledge bases store millions of facts, about society in general as well as specific domains such as politics and medicine. independent of the adopted format to store facts, extraction of factual knowledge first yields candidate facts (aka claims), for which the credibility needs to be assessed. given the open nature of the web, where content is potentially generated by everyone, extraction of claims faces inaccuracies and misinformation. hence, building a knowledge base from web sources does not only require conflict resolution and data cleansing [cit], but calls for methods to ensure the credibility of the extracted claims, especially in sensitive domains, such as healthcare [cit] ."
"experts (e): we implemented a validation interface for expert users, which records the time spent on validation and computes the average accuracy by comparing the answers with the ground truth. we asked three senior computer scientists to complete the validation tasks, with the option to pause between handling different claims."
"one possibility of a redesign of the system can be seen in figure 11 . figure 12 shows the design from the back perspective, including the battery packs."
"ratio of untrustworthy sources. if there is a high number of unreliable sources, the source-driven strategy is preferred. with little user input, detection of unreliable sources is difficult, though, so that the information-driven strategy is favoured in the beginning."
"to capture the impact of user input on a claim c, we define a conditional variant of the entropy measure introduced earlier. it measures the expected entropy of the database under specific validation input:"
"nonetheless, the presented results are very promising, in terms of the control accuracy of orthotic systems driven by myoelectric signals and sensor data from the device itself. especially the automatic adjustment of assistance regulated by the means of emg leads into the direction of an autonomous assist-asneeded home rehabilitation system. finally, the design of the next version of the system is currently under development. from the electromechanical point of view it is intended to use a brushless dc (bldc) motor in combination with a harmonic drive. advantages of harmonic drives are among others high torque capacity with compact and lightweight designs, as well as zero backlash. in this case the torque will be measured via the motor's current. an important point will be to give some kind of feedback to the user on the effects of the rehabilitation process. in order to give the system full mobility, the power will be supplied via battery packs, which will be installed in a decentralized module that may be carried at the lower back of the user. this is located close to the human center of gravity and, therefore, minimizes the load that is put on the user."
a demonstrator of the orthosis is developed and driven in terms of emg signals from the upper arm measured from the biceps brachii and triceps brachii [cit] . the control is based on threshold functions which are correlated to maximum amplitudes measured in both muscles.
"when validating claims, a user may make mistakes, not because of a lack of knowledge, but as a result of the interactions with a validation system [cit] . assuming that a user is confronted with the current inferred credibility of the claim to validate, along with an assessment of related sources and documents, any decision to deviate from the current most likely credibility assignment is typically taken well-motivated. common mistakes, thus, are accidental confirmations of a (wrong) inferred credibility value of a claim."
"crowd workers (c): while it is not the primary use case for our work, crowdsourcing enables scaling of manual validation tasks with the risk of lower result quality due to different levels of worker reliability [cit] . we used figureeight [cit] and its web templates to deploy our validation tasks. we prepared a budget of 1500 hits (human intelligence tasks) in total with a financial incentive of 0.1$/hit. we recorded the time spent on validation and computed the consensus of the answers among crowd workers using existing algorithms that include an evaluation of worker reliability [cit] . the consensus answer is then compared to the ground truth. table 3 summarises the obtained results. experts validate claims more accurately than crowd workers, but take more time to complete. moreover, the system also reports that the experts do not validate all the claims in one shoot; the validation process spanned 3-7 days. note that in our setting, experts and crowd workers already had supporting information in place. without it, they would have to retrieve such information on their own, which may further increase the validation time. the trade-offs illustrated in table 3, however, point to the potential benefit of combining the input of experts and crowd workers to achieve efficient, yet accurate fact checking."
"requirements for model inference. to be useful in our setting, an inference algorithm must meet two requirements. first, user input on correspondences should be a first class citizen. by propagating which claims have been validated, credibility probabilities can be computed for claims for no input has been sought so far. second, each iteration of the validation process changes the credibility of claims only marginally. hence, inference should proceed incrementally and avoid expensive re-computation of the credibility probabilities and model parameters in each iteration."
"we therefore propose an online expectation-maximization algorithm that reuses and updates the previous trained parameters, which accelerates convergence in the presence of new data. we operate on one claim at a time, and both the claim and the associated user input are discarded after validation. as such, we can only provide an educated guess on the credibility of the claim at a later stage. however, this is a minor drawback, since, in an online setting, claims are relevant only for a comparatively short interval. how to decide on which claims to discard in a more elaborated manner, is an interesting problem, see [cit], yet orthogonal to our work."
"the drawback of this experimental setup is that it is almost only useful to obtain biceps data, since the counterforce exerted by the weights is always acting in one direction ( figure 8 ). that is the reason why in a second step it was decided to generate counterforces in both up-and down directions (and thus, to compute the level of support in both directions) over the orthosis' torque control loop, obtaining again excellent results. here, 100% level of support corresponds to 2 nm and â100% level of support corresponds to â2nm."
"robot-aided rehabilitation of upper and lower limbs is currently a fast developing field that is also getting more and more acceptance by clinicians. as mentioned, to recover motor function, an intensive and early rehabilitation is recommended. for this purpose, a large variety of systems which are able to support therapists in their daily work, as well as control approaches have been developed and are more and more subject of current research. furthermore, clinical trials on various systems already show that robotic therapy can be useful and compared to traditional methods of therapy there are no disadvantages in the effectiveness [cit] . today, developed or applied robotic rehabilitation systems can be categorized according to their application focus. depending on the target group (patients needs), pathology, preferred method of treatment and place of installation, different conceptual approaches are possible. rehabilitation systems for the upper limbs can be classified into exoskeleton systems [cit] and end-effector-based structures [cit] which further divide into uni-and bilateral designs [cit], as well as distal and proximal approaches 1 [cit] . figure 1 shows the general design of the currently most used systems for upper limb rehabilitation, including portable haptic interfaces for use, e.g., in a virtual environment. in majority these systems are equipped with a fixed base and therefore, limited in mobility. all systems offer a so called \"massed practice therapy\"-paradigm 2 but nonetheless the individual systems have, due to their design, benefits and drawbacks, having in common that they are quite focused on their application scenario. essentially, restrictions can be found in the range of symptoms, which can be treated, as well as system mobility. generally, it can be stated that the systems are specialized for particular parameters and there is no system, which fits to all kinds of patients in the same way [cit] ."
efficient user guidance further requires to decide: (i) when to terminate validation to avoid wasting resources on marginal improvements of the quality of the knowledge base; (ii) how to group claims for batch processing to reduce the impact of set-up costs in validation (a user familiarising with a particular domain); and (iii) how to handle continuous arrival of new data to avoid redundant computation. our novel model enables us to address these aspects.
"once the user input of the z-th iteration of the validation process has been incorporated, a grounding is instantiated. this corresponds to the fourth step of the validation process in Â§2.3, i.e., deciding which claims are deemed credible. since claims are not independent, we take the configuration with maximal joint probability:"
"common causes of acquired and permanent physiological defects such as limited motor skills are mostly neurological diseases or injuries. in this case, one of the most common causes of permanent disabilities in western civilization is stroke [cit] . only about 40 % of all stroke survivors are able to return to normal employment and one third are permanently dependent of support and care. hence, the main goal of rehabilitation is the reintegration of affected back into normal life in an optimal way [cit] . in general, rehabilitation can achieve its goals in two ways: through compensation of motor dysfunction and/or through recovery of motor functions. here, a force exerting exoskeleton or orthosis for rehabilitation applications is used for compensation of motor deficits and/or for the recovery of motor skills. hereby the extremities with motor deficit -incomplete paralysis (paresis) or with total paralysis (plegia) -are actively supported (e.g., by gravity compensation)."
"the solution of equation (13) with respect toÃ» can be obtained by cross-validation (cv), that is much easier than solving equation (12) with respect toÃ»,p x 1 andp x 2 . we call the solution of equation (13) an empirical u -independent model. we show an interpretation of the ml estimation ofq â in figure 4 (c). this approximated inference withp â (u) is supported by the following fact. let us consider a minimisation problem of the bregman divergence d u with the same function u as applied to constructp â in u -multiplication. the minimiser of the bregman divergence in p u â is given as follows,"
"furthermore, by means of the movement prediction, the patient has the impression to control the arm himself, although the orthosis is actually moving the arm. this re-connects the movement planning phase of the brain with movement execution to re-establish the capability of the patient for freely and self-paced movements. therefore, the combination of the selfinitiated movement support and patient-cooperative control strategies can lead to a positive effect on rehabilitation and user-centered support in daily activities."
"moreover, in many applications, new sources, documents, and claims arrive continuously. we thus illustrate how the above process can be lifted to a streaming setting by exploiting online algorithms for inference and reusing parameters of our underlying model."
"figure 5(c) shows d kl (p x,q â ) which measures the discrepancy between the true distribution and empirical u -independent models with various Ï values. the result shows that the empirical u -independent model could be better than the conventional independent model even though the true distribution p x is strictly independent. figure 5 (d) shows d kl (Î½ x,q â ) where Î½ x is the uniform joint distribution of variables x 1 and x 2, and it depicts that the estimated model becomes closer to the uniform distribution as the Ï value becomes smaller. therefore, improvement of d kl (p x,q â ) in this experiment could be attributed to appropriate uniformalisation ofq â with u -multiplication when the empirical distribution has many sampling zeroes. to obtain the optimum Ï value with a given dataset, the cv or the bootstrap evaluation are available; especially the bayesian bootstrap [cit] ) is a useful tool for an extremely small dataset like this experiment."
"the goal of the orthosis system is to achieve a therapy session comparable to a guided session by a therapist, without having him at site and to motivate the patient for a constant training. figure 2 shows a possible training session which deals with different movement patterns."
"in addition, in the course of treatment the successes or failures are detected and evaluated to adjust the therapeutic measures, or to define new therapeutic goals. thus, the assessment procedures play an important role in motor rehabilitation. robotic systems which are equipped with assessment functions can make an important contribution to the quality of treatment, since they could ensure a simple and regular review of treatment effects."
"in the long term, this device could be used for the entire rehabilitation process, e.g., to improve motor recovery in patients with neurological or orthopaedic lesions. furthermore, the progress of therapy can be evaluated by monitoring and analyzing the muscle activity via emg."
"error rate. the grounding gi captures which claims are deemed credible in the i-th iteration of the validation process. if gi turns out to be mostly incorrect, we have evidence of unreliable sources and favour the source-driven strategy."
"this section presents a probabilistic model for fact checking ( Â§3.1), before turning to mechanisms for incremental inference ( Â§3.2) and the instantiation of a grounding ( Â§3.3)."
"the mpower 1000 4 is an example for an active elbow orthosis system with one degree of freedom, which is based on technology developments from mit. the device supports the elbow movement in extension and flexion and is designed for home and clinical use. it supports patients in their rehabilitation process who suffer from the consequences of stroke, spinal cord injury or multiple sclerosis. the system is controlled by residual signals of the biceps and triceps with three possible support levels [cit] ."
"we then address aspects of practical relevance, which are not captured in problem 1. validation may converge before the validation goal is reached and the effort budget has been spent. if so, further user input leads to diminishing improvements of the quality of the grounding and the validation process may be terminated. we show how our model enables the detection of such scenarios by decision-support heuristics."
"inference alternates between an expectation (e-step) and a maximization (m-step), until convergence. em-based inference is conducted in each iteration of the validation process, while each em iteration updates the model parameters w . hence, in the z-th iteration of validation, we obtain sequences w zâ1, with lzâ1 as the number of em iterations in the z â 1-th iteration of the validation process. in the l-th e-step of the z-th step of the validation process, credibility probabilities are computed as follows: (1) a sequence of samples Ï l z is obtained by gibbs sampling according to the conditional probability distribution:"
"the wotas orthosis is able to reduce symptoms of tremor in the upper extremity. the orthosis has three active degrees of freedom. with help of gyroscopes and force sensors, the system is able to discern tremor and apply force into the limb, in order to suppress it [cit] ."
"our setting is also different from active learning, as we do not require any training data for a user to begin the validation process. moreover, we incrementally incorporate user input without devising a model from scratch upon receiving new labels. however, stopping criteria for feedback processes have been proposed in active learning, e.g. using held-out labels [cit] and performance estimation [cit] . yet, these methods are applicable only for specific classifiers and do not incorporate human factors. using our probabilistic model, we have been able to propose several criteria for early termination that turned out to be effective in our experimental evaluation."
"nowadays, robotic systems are used in various medical disciplines and different highly specialized applications, e.g., in the field of minimally invasive surgery [cit] . furthermore, technical therapy approaches in physio and occupational therapy is given more and more importance. in this context, particular assistance and training devices are in the center of interest. these could be systems like powered exoskeletons, active orthoses or special end-effector based therapy robots [cit] . on the one hand, these systems could provide important support in medical rehabilitation for the therapist and patient, and on the other hand they could be a help in everyday activities for elderly or motor-impaired people in their home environment [cit] ."
"this step is realised by a l2-regularized trust region newton method [cit], suited for large-scale data, where critical information is often sparse (many zero-valued features)."
"moreover, we focus on reducing manual effort, assuming that there is a notion of truth. yet, user input may be uncertain or subjective [cit] . while we consider the integration of such feedback to be future work, we see two scenarios with different implications. first, if claims are validated by a single biased expert [cit], the grounding function is shifted to the expert belief. this angle can be extended to recommender systems, which recommend the most belief-compatible claim for a user. second, if claims are validated by multiple biased experts, differences in their belief suddenly have an impact. finding a common ground then requires negotiation and conflict resolution mechanisms [cit] ."
"the project ortho jacket of the karlsruhe institute of technology (kit) has the aim to develop an active orthosis for patients with spinal marrow lesion in cervical vertebras (c4 to c7). the orthosis is intended to enable movements of the shoulder, elbow and hand, in conjunction with a wheelchair. the control of the individual degrees of freedom carried out via a joystick or via residual emg signals of shoulder and arm muscles [cit] ."
"since our model is an acyclic graph with no self statistics, the partition function is computed exactly using ising methods [cit], which run in polynomial time."
-good stabilization and guidance of the limb: compared to end-effector-systems (usually with just one interface to the patient) an exoskeleton/orthosis can be connected to several points with the patient. with this specific structure the patient's arm is guided and stabilized at every joint.
this experiment was conducted to formalize a model which can modulate the level of assistance provided by the orthosis in dependence of induced force and movement direction.
"detecting erroneous input. we evaluate our approach to detect erroneous input by simulating user mistakes. with a probability p, we transform correct user input into an incorrect assessment. the confirmation check ( Â§5.2) is triggered after each 1% of total validations. table 1 shows the detected mistakes (%) when increasing parameter p. across all datasets, the majority of inserted mistakes is detected."
"this type of generalisation has been proposed in several contexts [cit], and is closely related to main-effect models in generalised linear models [cit], or the archimedean copula [cit] . in this paper, we characterise conventional multiplication and division by the kullback-leibler (kl) divergence and generalise them by introducing the bregman divergence [cit] which is deeply related with the monotonically increasing functions u and Î¾. we also generalise independence in statistical models by using generalised multiplication, and experimentally show the effectiveness of our proposed models. [cit], 2010) . this paper is composed as follows. at first, in section 2, the bregman divergence and some of its properties are introduced. then, the conventional product rule is generalised for statistical inference in section 3. an idea of generalised independence associated with a monotonically increasing function is also defined in this section. in section 4, some properties of generalised independence are given. in section 5, nb models are extended by implementing generalised independence in several ways. we also numerically evaluate extended nb models by using benchmark datasets in this section. lastly, in section 6, concluding remarks are given."
"here, q b denotes the probabilistic fact database constructed after incorporating the given configuration of b. note that a more complex cost model could be constructed based on validation difficulty (e.g., implied by logical relations between claims) [cit] . yet, this is orthogonal to our work. using this measure, our validation process incorporates batching of claims by choose the top-k claims with maximal information gain (breaking ties randomly):"
"where the claim probabilities are obtained after each em iteration (i.e., eq. 7 for unlabelled claims, or directly by the user input for labelled claims). however, this approximation neglects the mutual dependencies between claims."
"proof. the e-step is implemented by gibbs sampling, which takes linear time [cit] in the number of claims. the m-step is implemented by the trust region newton method, which also takes linear time in the dataset size [cit] ."
"in the following, we illustrate how stlda can accurately capture season-dependent topic clusters and im- table 4 summarizes the obtained topics and topic probability distributions using stlda and lda for these tourist attractions. the detected topics for each tourist attraction are arranged in descending order according to the probability values and the probability values of topics are shown in parentheses."
"from table 5, we can clearly see that the values of volatility indicator calculated from the results of stlda all larger than that of lda in different seasons, which indicates that the topic probability values generated from stlda in each season have remarkable difference. but the topics found by lda tend to have relatively uniform probability value and consequently there is no obvious discrimination between these topics. for simplicity, we take the yuntai mountain for example. the topic with the highest probability 0.645 detected from stlda in winter is snowscape and the standard deviation of topic probability distribution is 0.174, indicating that the snowscape topic highly belongs to the winter of yuntai mountain. in lda, the highest topic probability value is only 0.240 and the standard deviation of topic probability distribution is merely 0.052. when judging the prominent topic of yuntai mountain from the results of lda in practical applications such as recommending interesting attractions to tourists, the tour operators may get confused due to its relatively uniform probability distribution, and thus can not provide accurate recommendations for potential tourists."
"for the remaining parameters Ï k and Ï m using the same method, the approximate probability of word t in a topic k is formulated as follows:"
"where v is a lk x lk block-diagonal matrix with the i-th diagonal block being hsrtrxhifrl + a~ik. the first term in (12) is removed since it is a constant. although the precoder vector obtained in (13) is convex, the optimization problem has no closed form and it is rather prohibitive to obtain the optimum solution numerically. to simplify the complexity, we find a suboptimal solution by solving that is, if we collect the received signals at all destinations, the ratio of total strength of desired signals over total strength of noises is maximized. the numerator in (15) with simplification mentioned above, the suboptimal criterion in (15) can be expressed in vector form. on the other hand, we usually select the maximum allowable transmit power to achieve the best snr performance, that is, the equality in (14) usually hods. thus, the optimization problem is re-written as (mrc) to combine the signals received in both phases. the corresponding outage probability of the source sk is given by"
"as can be seen from table 3, the extracted topics apparently characterize some features of attractions, including both natural styles like woods (topic 7), snowscape (topic 16) and cultural styles like playground (topic 21). the representative words in these different topics are quite informative and coherent. for example for topic 4, words such as relaxing, entertainment, comfort, fun, vacation and resort are related to each other and semantically coherent, conveying the meaning about leisure and entertainment, and thus we name the topic accordingly. the words in topic 7 such as luxuriant, woods, reserve and verdant are closely related to woods, while topic 8 is mainly about cultural activities of ethnic minorities. in addition to the meaning of these topical words, we also refer to classification criteria of chinese national tourism resources [cit] and the related study of paper [cit] 39 ] to name all the topics that extracted from attraction corpus and list these topics for subsequent feature analysis of tourist attractions."
"similarly, each document d m is defined as a multinomial distribution Î¸ m over topics, drawn from a dirichlet prior distribution with parameter Î±. the full generative process for each document d m in a corpus is defined as follows:"
"in the proposed scheme, it takes k + 1 time-slots for the two-phase transmission, as shown in fig.2( c), thus, the outage probability of sk is given by"
"our experiment uses the english database of wikipedia to acquire the attraction description texts. meanwhile, attraction information is also collected from official websites of the attractions. since the acquired information regarding a specific attraction is not enough for topic detection, we make full use of abundant travel information from various travel-related websites such as wikitravel (http://wikitravel.org) and travelchinaguide (https://www.travelchinaguide.com). travelogues from wikitravel and professional descriptions from travelchinaguide are searched by the name of the attractions. it's worth noting that travelogues can serve as a reliable resource of attraction textual information, which is complementary to professional description texts because travelogues cover various travel-related aspects, including not only general scenery description, but also variety of cultural activities that travelers participated in specific attraction, which may be representative characteristics of that attraction. then a comprehensive attraction description document is generated by integrating all these related information, which contains abundant knowledge for topic detection."
"to show the dominancy of stlda over the basic lda model more intuitively, we evaluate the statistical properties of obtained topics and topic probability distributions of all 160 tourist attractions using these two models and the results are shown in table 5 . the topics whose occurrence probability larger than 0.1 are selected for each tourist attraction. in our experiments, we choose five statistical indicators, namely richness, coincidence, diversity, significance and volatility. richness indicator refers to the average number of topics detected from each tourist attraction. coincidence indicator denotes the average coincident number of topics generated from these two models for each tourist attraction. diversity indicator reflects the average number of extra topics generated from one model over the other model for each tourist attraction. significance indicator represents the average highest topic probability value of each tourist attraction. volatility indicator indicates the average standard deviation of topic probability distribution corresponding to each tourist attraction. the sp, su, au and wi in table 5 denote the four seasons spring, summer, autumn and winter respectively. and 0.563 respectively, all larger than the value 0.355 in lda. this result reveals that the topic with the highest probability is dominant in topics generated from stlda for most attractions corresponding to specific season."
"the rest of this paper is organized as follows. section 2 is devoted to the methods including the basic lda model and the proposed stlda model. in section 3, an inference algorithm using gibbs sampling for the parameter estimation of our proposed model is discussed in detail. section 4 illustrates the experimental results and analysis. finally, section 5 includes our conclusions."
"each document in the corpus is associated with a single famous tourist attraction in china, covering 160 unique attractions in total. the selected attractions including natural landscape and cultural landscape are mainly 5a or 4a tourist attractions evaluated by china national tourism administration, where 5a represents the highest level of tourist attraction in china. table 2 shows the summary of our data collection. since attraction textual information acquired from the internet is unstructured and usually contains much disturbance, it is necessary to perform preprocessing on the original attractions textual data before the subsequent experiments. firstly, punctuations, numbers and other non-alphabet characters are removed. secondly, all words are lowercased, stop words are removed based on a stop word list from natural language toolkit (nltk) [cit] . thirdly, for the purpose of reducing the vocabulary size, the low frequency words that appear less than twice in corpus are also filtered out. after preprocessing of the textual information in each attraction, the word distribution of a document can be obtained. finally, the corpus is further expressed with a data format that can be identifiable by stlda and lda model."
further analysis of yuntai mountain and zhangjiajie national forest park suggest a similar result. the results of stlda and lda for yuntai mountain and zhangjiajie national forest park are also visually shown in fig. 7 and fig. 8 respectively.
"in the first step, the optimal decoder vector at dk can be found by which is in a quadratic form in both numerator and denominator. it is easy to verify that the optimal decoding vector at dk is given by where fh is an arbitrary non-zero constant."
"in the case of our model, the target of inference is the posterior distribution of the hidden variables z and s, which is defined as follows:"
"and the precoding factors in b are obtained in (19) . when the number of source users increases, the transmission protocol seems more efficient since the rate reduction factor k / (k+1) is closer to one. however, the resource provided by the relays is shared by more sources, which may degrade diversity gain."
"besides, the description texts corresponding to different seasons for the same attractions show remarkable difference. it's apparently that none of the above mentioned topic models are applicable to deal with such unique attraction textual data because they may confound topics with respect to different time contexts in one document."
"monte carlo algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution, when direct sampling is difficult [cit] . in gibbs sampling, each parameter is sequentially sampled according to the full conditional distribution of the parameter conditioned on the observed data and all the other parameters. since this method can get global optimum solution in theory and can be easily implemented while other inference algorithms such as variational em algorithm is only guaranteed to find a local optimum [cit], it is commonly used in bayesian inference domain, especially for topic modeling."
"after deriving the joint probability distribution, the topic assignment z i and season label assignment s i for each word w i in the corpus can be sampled with the following full conditional probability distribution:"
"these comparative results indicate that the seasonal contextual information captured by stlda plays a very important role in forming better topic representation of tourist attractions. by including seasonal contextual information, stlda can model the variations of topic occurrence that reflect the changing seasonal contexts, which makes stlda more suitable and effective for topic detection of tourist attractions. an immediate and obvious merit of our proposed model is that this helps us understand more precisely what remarkable topic features are corresponding to specific seasons for a given tourist attraction, which can provide a good opportunity for both tourists and tour operators to fully utilize such information in various decision tasks that are season dependent. the most straightforward application is to apply stlda to the field of personalized attraction recommendation. for example, a tourist plans to travel during the labour day. the labour day of china is on may 1st which is spring. besides, according to the chinese law, the holidays last for three days, which is one of the peak travel seasons every year. at this time, the tourist intend to see flowers. taking the data shown in table 4 as example, the probability values of blossom topic in nalati scenic spots, yuntai mountain and zhangjiajie national forest park generated from lda are 0.102, 0.176 and 0.179 respectively and there is no distinct difference between these probability values. when providing recommendation for the tourist based on the results of lda, the tour operator would lack obvious pertinence. but according to the results of stlda, it is apparently that zhangjiajie national forest park should be recommended to him since the probability value of blossom topic up to 0.678 in spring. recommending different tourist attractions to users exposed at specific seasonal contexts on the basis of different seasonal topic features of attractions can achieve higher satisfaction for tourists and realize more profit for tour operators undoubtedly."
where v is a lk x lk block-diagonal matrix with the f-th diagonal block being hsr.erxh~r.e + a;ik. the first term in (12) is removed since it is a constant.
"when the set of relays cooperates with k sources, it takes k time-slots to forward signals to k destinations respectively, as shown in figure 2 (b), which leads to a waste of radio"
"where the weights labeled in the corresponding edges indicate the topic occurrence probabilities. for example for the attraction in spring, the detected topics are t7 with probability value 0.635 and t20 with probability value 0.208, while in winter the attraction corresponds to topics t16, t4 and t26 and the probability values are 0.613, 0.184 and 0.136 respectively. now, the likelihood function for the observed tourism attractions textual data can be formulated according"
"further deeply investigate the topic probability distributions of all 160 tourist attractions generated from stlda model, we find that not all tourist attractions have clearly seasonality in every season, but all exhibit seasonal characteristics to some extent. for example, wuyuan, the most beautiful village of china, have prominent topics in spring, summer and autumn. however, in winter it presents a uniform probability distribution over the topics and the corresponding probability values of topics all less than 0.1, indicating that it does not have remarkable seasonal topic features in winter. similarly, thousand islet lake shows seasonal characteristics in spring, autumn and winter and it does not present distinct seasonality in summer."
which is in a quadratic form in both numerator and denominator. it is easy to verify that the optimal decoding vector at di; is given by
"where Î¸ m,s, Ï k and Ï m denote model parameters to be estimated, z m and s m are hidden variables, w m refer to known variable. Î±, Î² and Î³ are the dirichlet priors for Î¸ m,s, Ï k and Ï m respectively. furthermore, the clear hierarchy struture of the model is also maked in the above formula."
"from table 4, we can clearly see that the topics and topics' occurrence probability of all these three tourist attractions change significantly with the alternation of seasons. specifically, we take nalati scenic spots for example and the results of stlda and lda for this attraction is also visually shown in fig. 6 . nalati scenic spots, located in xinjiang uygur autonomous region, is famous for its unique natural scenery, dynamic culture and ethic customs. as can be seen from table 4, the detected topics from stlda for nalati scenic spots are woods and blossom in spring and the corresponding probability value are 0.635 and 0.208 respectively. this shows that the representative topic features of nalati scenic spots are woods and flowers in spring, which are consistent with the common sense that the beautiful natural scenery is prominent in spring of nalati scenic spots. the topic generated from stlda with highest probability in summer for nalati scenic spots is cultural activity (0.494). further accessing the relevant information, we see that the temperature is agreeable in summer of nalati scenic spots, which is suitable for doing outdoor activities. another interesting finding is that summer is also the peak tourist season to visit the nalati scenic spots. thus, the local kazakhs, who are hospitable and excelling at dancing and singing, often hold a variety of folk activities in summer to show their colorful ethnic culture to the tourists. the topics found by stlda in autumn are woods, maple leaves and harvest, while in winter are snowscape, entertainment and ice sports respectively. these observations also reveal that topics corresponding to various seasonal contexts generated from stlda are capable of reflecting the features of the attraction with respect to different seasons in real life. for lda model, the detected topic with highest probability is snowscape, followed by woods, village, entertainment and blossom. it is obvious that the topics of nalati scenic spots generated from lda are different from those of stlda. even the same topics found by these two models such as snowscape, woods, entertainment and blossom, their probability values also differ."
"it is desperately needed to outline tourist attractions from massive travel-related information available on the web, with the aim of providing decision support for both tourists and tour operators. thematic analysis for a given tourist attraction provides us a good opportunity to obtain the high-level concepts that reflect the attributes of the attraction. however, the common sense that tourist attractions tend to show distinct features corresponding to different seasons has been neglected by aforementioned probabilistic topic models, which should be considered as a valuable reference for improving topic representation of tourist attractions. our work"
indicates the perplexity performance of stlda is significantly better than that of lda at the 95% confidence level. the statistical analysis demonstrates that seasonal contextual information contribute positively to the performance of topic modeling.
"in this section, we evaluate the performances of the proposed stlda model on real-world travel data, and compare the model with the basic lda model both qualitatively and quantitatively. it should be pointed out that as far as we know, no literature has conducted the similar research on seasonal topic features of tourist attractions. therefore, all experimental results of our proposed model are compared with the original lda model in this study. specifically, we present the data collection and pre-processing in section 4."
"however, from fig. 5 we can find that just like the lda model, stlda still has the linear time complexity and its running time grows linearly as the number of topics increases."
"by applying bayes' rule, the multinomial distributions Î¸ m, j, Ï k and Ï m with their dirichlet prior Î±, Î² and Î³ can be redefined as follows:"
"m + 1, n m + 1 end for end for if the markov chain has converged then for every 100 iterations do update matrices Î¸, Ï and Ï with new sampling results end for output matrices Î¸, Ï and Ï according to equation (23), (24) and (25) end if end while"
"in our proposed scheme, the relays forward all sources' symbols over a common channel, which is different from most existing cooperative strategies. one challenge of our scheme is that, while estimating the desired symbols at each destination, the signals forwarded through the relay links are interfered by other users' symbols, which is so-called multiple access interference (mal). by exploiting the spatial properties of the channel coefficients, the precoders and decoders can be designed to eliminate the interference as well as to attain higher diversity order for all source users. substituting (2) and (3) into (4), we have (8) where 13k is an arbitrary non-zero constant."
"although the precoder vector obtained in (13) is convex, the optimization problem has no closed form and it is rather prohibitive to obtain the optimum solution numerically. to simplify the complexity, we find a suboptimal solution by solving"
"given the parameters Î± and Î², the joint distribution over the random variables (w m, z m, Ï k, Î¸ m ) then can be derived from fig. 2(a), which is given by:"
"there are three sets of latent variables that need to be estimated in our model, including: the topic distribution of the corresponding per document-season pair Î¸ m,s, the per topic-word multinomial distribution Ï k and the per document-season multinomial distribution Ï m ."
topic detection and extraction is a well-studied research [cit] that aims at identifying a group of words that form topics from a collection of documents. thematic analysis has been actively investigated in feature extraction of tourist attraction and gradually become an important attraction profiling technique in recent years.
"we employ wikipedia (http://www.wikipedia.org) as the primary source of the experimental data from which attraction description information is retrieved. wikipedia, the collaboratively edited encyclopedia avaiable on the web with over 30 million articles written in 293 languages and more than 5 million english wikipedia articles, provides rich information on various aspects including plenty of travel-related knowledge."
vector of all words that appear in corpus excluding word w i z âi vector of topic assignments for all words in corpus except word w i s âi season labels vector for all words in corpus excluding word w i n (t) k the count of word t assigned to topic k in corpus n
"to evaluate the time complexity of our proposed model on attraction corpus, we summarize the running time of stlda and lda for different number of topics k in fig. 5 . from this figure, it can be easily observed that the running time of stlda are all longer than lda with different values of k. this is because stlda adds an additional season layer on the basis of basic lda model and this leads to its higher computational complexity."
"to demonstrate the effectiveness of our proposed model, we train stlda and lda model on attraction corpus to learn topics for following analysis respectively. the number of topics is set empirically to 30 according to section 4.2. by analyzing the learned topics of stlda and lda, we find that the representative topical words generated from these two models are very close. in order to fairly compare the results of stlda and lda model, we present the 22 topics that sharing the same meaning by these two models in table 3, where some of redundant and meaningless topics in each of these two models are abandoned. the topic number j denotes the jth topic discovered by the model. for illustrating the topics learnt by our proposed model, we also show the representative ten words of each topic in table 3 . note that here we use representative words in ice sports rimed, holiday, sliding, ski, iceboating, hockey, snowboarding, sled, ball, sports each topic rather than words with the highest probabilities to represent the corresponding topic, where the later method is commonly used in topic analysis. the reason for this is that the topic features of attractions often reveal high-level concepts, which is highly correlated to travel-related words. the words irrelevant with travel are removed for better illustrating the extracted topics. we manually assign topic labels to the extracted topics to reflect our interpretation of their meaning."
gibbs sampling will serially draw each variable of z i and s i from the full conditional distribution of each variable conditioned on remaining variables and the observations according to equation (18) and equation (19) .
"to provide a brief glance of the outage performance, we compare the proposed scheme with the direct transmission case and the conventional cooperative strategy. in the direct transmission case, as shown in fig.2(a), each source transmits signal during the designated time-slot without the assistance of the relays. in this case, the outage probability of the source in conventional cooperative scheme [cit], the set of relays forwards signal for each source accordingly, as shown in fig.2(b) . with full csi at relays, the transmit beamforming strategy is optimum in terms of snr at the destination [cit], and the destination applies maximum ratio combining (mrc) to combine the signals received in both phases. the corresponding outage probability of the source sk is given by"
"as previously mentioned, stlda considers both general description of attraction and seasonal features exisiting in attraction document in a unified manner and can detect the meaningful topics with respect to different seasons for each attraction. fig. 3 is a running example of stlda model. as can be seen from this figure, there is a clear hierarchy followed by the attraction document layer, season layer, topic layer and word layer. the words constitute a number of topics and the tourist attraction corresponds to various topics in different seasons,"
"in order to enhance the spectral efficiency of the conventional cooperative strategies, which takes twice time-slots to accomplish cooperation, we proposed a suboptimal set of precoders at relays and decoders at destinations to maximize the average snr subject to zero-forcing criterion and total power constraint. from computer simulations, it shows that, although the proposed scheme is not able to achieve full diversity order, it provides significant improvement on the outage capacity."
"specifically, in our model, the full conditional probability distribution for a word w i based on the topics and season labels of all other variables as well as the observed data is defined as follows:"
"that is, if we collect the received signals at all destinations, the ratio of total strength of desired signals over total strength of noises is maximized."
"in lda, there are two sets of parameters that need to be estimated from a collection of documents, one is the topic distribution in each document and the other is the word distribution in each topic. in reality, only the documents can be observed, while the topic structure including topics and topic probability proportions is hid- while the generative process of stlda has the similarity to a certain extent with some topic models in the text modeling domain, such as topic-aspect model [cit], topic-link lda model [cit] and author-topic model [cit], the logical structures of these models are totally different. for example, the author-topic model introduces table 1 ."
"where w âi, z âi and s âi represent vector of all words that appear in corpus excluding word w i, vector of topic assignments for all words except word w i and season labels vector for all words in corpus excluding word w i respectively."
"finally, the full conditional probability distribution for each variable z i and s i can be equally separated from the joint sampling distribution of topic assignment and sesaon label indicator for word w i, which are given by:"
"hence, it is necessary to develop a suitable approach to address the unique characteristics of the attraction textual data and precisely extract the topic features of tourist attractions with consideration of seasonal contextual information. however, to the best of our knowledge, so far no research has focused on this topic."
"the topics detected from stlda for the attraction are doubtlessly more comprehensive compared with lda, which is helpful for tourists to fully utilize such knowledge to plan their trip and tour operators to precisely grasp features of tourist attractions so as to provide more targeted publicity and recommendation for tourists."
"given a collection of documents, the posterior distribution of the latent variables including Î¸ m,s, Ï k and Ï m can be computed when the likelihood function of the whole corpus is maximized. the procedure of variable inference is to invert the document generative process and infer the latent variables from the observed corpus."
"in the following section, the precoders and decoding factors are designed to maximize the snr averaged over all estimates in (5) and the set of precoders has to satisfy total power constraint given by"
"where d denotes the observed corpus, n m,j, n k and n m refer to vector of topic observation counts for document m and season label j, term observation counts for topic k and season label observation counts for document m respectively. specifically, n m,j, n k and n m are denoted by (n"
"to apply a gibbs sampling algorithm, the joint probability distribution of the observed words, topics and season labels assignments of the whole corpus is first derived by dividing this joint distribution into three parts, which is given by:"
"to provide a brief glance of the outage performance, we compare the proposed scheme with the direct transmission case and the conventional cooperative strategy. in the direct transmission case, as shown in fig.2(a), each source transmits signal during the designated time-slot without the assistance of the relays. in this case, the outage probability of the source sk given a transmission rate r equals to"
"with summary of the analysis results of above tourist attractions using stlda and lda model, three observed conclusions can be made. firstly, the topics found by stlda and lda for tourist attractions indeed have a certain degree of similarity, but the topic probability distributions are prominently different in these two of lda for nalati scenic spots, we can see that these ignored topics whose probability values are 0.088, 0.043 and 0.0002 respectively. when considering the seasonal contextual information, the probability value of cultural activity topic increases from 0.0002 to 0.494, which makes the cultural activity topic prominent in summer of nalati scenic spots. this difference comes from stlda's assumption that takes the intrinsic seasonal features of each tourist attraction into consideration. therefore stlda can capture the potential season-dependent topics on a season level of fine-grained, while some of meaningful topics are filtered out in lda due to their extremely low probability value on a coarse-grained level."
"probabilistic topic models have been proposed for topic extraction from textual data and successfully applied to a series of text mining tasks in different research fields over the past decade, owing to their powerful capability of discovering meaningful latent topics from large collection of documents automatically and simultaneously representing documents with these discovered topics. topic models are usually based upon the assumption that documents are mixture of topics, where each topic is a probability distribution over words."
"in other words, the results of stlda can give the prominent topic that a tourist attraction belongs to. however, the results of lda are less significant compared with stlda. finally, we investigate the volatility indicator."
"in our proposed scheme, the relays forward all sources' symbols over a common channel, which is different from most existing cooperative strategies. one challenge of our scheme is that, while estimating the desired symbols at each destination, the signals forwarded through the relay links are interfered by other users' symbols, which is so-called multiple access interference (mal) . by exploiting the spatial properties of the channel coefficients, the precoders and decoders can be designed to eliminate the interference as well as to attain higher diversity order for all source users. substituting (2) and (3) into (4), we have in the conventional cooperative network with multiple relays, the relays forwards signal for one source simultaneously using specific cooperative strategy, e.g. distributed beamforming, selective relaying or distributed space-time coding...etc."
"despite recent progressions, these time-dependent topic models are mainly focus on the long-term evolution of topics in a whole corpus, while the topics of each document in the corpus remain constant. specifically, their research usually based upon the assumption that each document in the corpus is associated with one timestape and all documents are collected over time. then these topic models are applied to the document collections that sequentially organized to discover time sensitive topics. however, the hypothesis is oversimplified because one document may exhibit the feature of more than one time period. in the case of topic extraction for tourist attractions, the attraction textual data is significantly different from other common documents since the content of an attraction description text often reveal a strong seasonal pattern, which is an intrinsic feature of the attraction and should be considered as important contextual information with respect to this attraction. in order to clearly illustrate the seasonal characteristics existing in attractions description documents, fig. 1 shows snapshots of two famous tourist attractions in china. fig. 1(a) is the description text of east lake scenic area from its official website (http://www.whdonghu.gov.cn/english.htm) and fig. 1(b) is the description document of yellow mountains from travelchinaguide (www.travelchinaguide.com/attraction/anhui/huangshan/seasons.htm)."
"for the attraction corpus, we randomly allocate 75% of the attraction documents for training and the remaining for testing. fig. 4 shows the results of the perplexity comparison of stlda and lda with different number of topics varying from 10 to 100. as shown in fig. 4, stlda presents lower perplexity value than lda with different number of topics, which indicates stlda owns a better predictive power for unseen documents than the original lda model. further analysis shows that the perplexity performance is improved about 28.68% on average. this is due to the ability of stlda to detect meaningful topics corresponding to various seasonal contexts for attractions by taking the intrinsic seasonal features of attractions into consideration. therefore, stlda model can well represent the content of new attractions documents and this leads to its better perplexity performance. from fig. 4, we can also obtain the optimal number of topics extracted from the attraction corpus for both stlda and lda model. the perplexity values of these two models decrease rapidly with the number of topics increasing from 10 to 30, while the performances of these two models become worse when further increasing the latent topic number from 30 to 100. the experimental results reveal that the optimum number of topics for the attraction corpus is 30. the statistical significance of the difference between the stlda and lda model regarding the perplexity performance is further assessed by using the wilcoxon signed ranks test. the wilcoxon test is a nonparametric test method that is used when overall distribution is unknown [cit] . according to the test result, the value of z statistics is -2.803 and the concomitant probability Î± is 0.005, less than the significance level of 0.05, which"
"the probability of topic k asscociated with season label j in document m now can be approximated by using the expectation of dirichlet distribution, which is given by:"
"perplexity, widely used in the natural language modeling fields, is an important indicator to demonstrate the predictive power of a model [cit] . a lower perplexity value means that a higher likehood is achieved on a test dataset, thus indicates a better generalization performance of a model. given a test dataset d of m documents, the perplexity value can be calculated as follows:"
"with the rapid development of tourism market, the demand for intelligent travel services has been expected to increase remarkably. the prevalence of the internet enables everyone to easily access travel related information from various websites. however, the sustained growth of travel data on the web may be overwhelming for tourists when selecting tourist attractions that specific to their personalized requirements. meanwhile, tour op-erators need to present customized tourist attractions for potential tourists so as to survive in competitive market and make more profit. as an effective tool to achieve precision marketing for tour operators and assist decision marking for tourists, the personalized recommendation technique has attracted a great deal of attention over the past few years. personalized attraction recommendation focuses on identifying the most relevant attractions to recommend to tourists, where the content-based method is popularly used in this case since this method cater well to tourists' needs. the content-based attraction recommendation approach aims to maximize the relevance between the tourists' preferences and attractions' features. a critical challenge along this line is to get a comprehensive understanding of the characteristics of tourist attractions. therefore, it is highly desirable to produce a precise analysis and summary of online attraction information, with the objective of providing decision support for both tourists and tour operators."
"the algorithm iterates repeatedly until the markov chain has reached a stationary state, where the stationary distribution of the markov chain is our desired posterior distribution. according to gibbs sampling rule, a bunch of draws that are approximately from our desired posterior distribution are obtained once the markov chain has converged. then, the model parameters Î¸, Ï and Ï corresponding to the stationary state of the markov chain can be estimated based on these samples. the pseudocode of gibbs sampling procedure for our model is presented in algorithm 1, which runs over the three periods: initialisation period, burn-in period and sampling period."
"while the exact inference for posterior estimation is intractable in general, a wide variety of approximate inference algorithms are considered to estimate these parameters. in this paper, we adopt the gibbs sampling algorithm to perform approximate inference. gibbs sampling is a simple and widely applicable markov chain"
"to fill this gap, we present a novel probabilistic topic model to detect meaningful topics corresponding to various seasonal contexts for each attraction from a collection of attraction description documents. the proposed season topic model based on lda (stlda) is a generative probability model, which can capture the potential season-dependent topic clusters that naturally occurring in attractions documents. as a generative model, our learned topic model is substantially the joint probability distribution of seasonal contextual information as well as textual data, which specifies a probabilistic process to describe how words in attractions documents might be generated in particular when the seasonal feature in each attraction document is taken into account. by including seasonal contextual information, stlda can model the variations of topic occurrence that reveal the changing seasonal contexts, which is unable to capture using other probabilistic topic models. as a result, our proposed model can detect the representative and comprehensive attributes corresponding to various seasonal contexts for each attraction and well represent the content of each attraction description document."
"by integrating out the distributions of Î¸ m,s, Ï k and Ï m and summing over z m,n and s m,n, the likelihood of a"
"in particular we compared csumi to approaches using pearson and spearman correlation. to use correlation-based approaches we need real values (or ordered values) for our covariates. thus first we randomly label different tissue types with integers 1 through 13. we then compare how well our approach fig. 4 . clustering by csumi (solid line) is much more informative than the one using the naive approach (dashed line). for a varying number of dimensions, p, we take the data and project it onto a p-dimensional space using either csumi or by picking the p largest pcs, respectively. we then perform k-means clustering, compute the mutual information between the tissue type and the clustering, and plot the result. csumi, component selection using mutual information."
"it is worth pointing out that each of these first six pcs gives us different types of information about tissue type (fig. 1a) . take for example the first two pcs. we see that the first pc does not differentiate between muscle and heart or between lung and skin, whereas the second pc does exactly that. by the same token pcs three through six also give different types of information about tissue type than the first two pcs (compare fig. 1a to supp. fig. s3 )."
"0608. since lower scores mean clusters are better preserved, we see it is better not to use the first four components, just as our statistic suggested."
"our approach, csumi, works by first performing pca on rna-seq data. for a covariate of interest we use our statistic, mi c, to compare that covariate to each pc in turn and explore the highest scoring pcs."
"3.7.3. k-means clustering. we apply our k-means clustering measure (methods) to the brain samples using different covariates. first we consider tissue type (fig. 4) . for a varying threshold p (aka the number of dimensions we project down to), we calculated the mi between tissue type and the k-means clustering. we see that for most values of p our method achieves higher scores than the naive approach. when we perform the same procedure using age (supp. fig. 6 ) and hardy score (supp. fig. 7) as covariates, we also see that our method outperforms the naive approach."
"the above gives us a means of ranking how closely a given covariate relates to a given pc. we measure the significance of this relationship using a monte carlo-type approach. for each step in the monte carlo process, randomly permute the c i 's to get a new labeling c 0 1 â Ã¡ Ã¡ Ã¡ â c 0 n of the data points. let c 0 be the random variable that maps from each data point to the corresponding c 0 i . we can then calculate mi c (c 0 â j). repeat this process r times, and let r be the number of trials for which mi c (c 0 â j)qmi c (câ j). we can then approximate the p-value as r + 1 r + 1 [cit] ."
"for our second set of analyses we looked at all the tissue samples from different brain regions. focusing on the second pc, there is one clear outlier (supplementary fig. 1 ). this sample is labeled as being from a female brain, yet behaves much more like the testis (including when we look at y chromosome expression, data not shown). it seems likely that this is a technical artifact (perhaps a mislabeled sample?), so we exclude this sample, leaving us with 312 samples. the brain tissues are: caudate (basal ganglia), amygdala, cerebellum, hippocampus, frontal cortex, cortex, putamen (basal ganglia), hypothalamus, nucleus accumbens (basal ganglia), anterior cingulate cortex, cerebellar hemisphere, spinal cord, and substantia nigra."
"the smaller this number, the better, as more of the tissue types tend to cluster together. note that one could instead use more than three nearest neighbors, but our experimentation suggests changing this value does not affect our results qualitatively."
"luckily numerous techniques exist to help deal with technical artifacts [cit] . our approach allows us to understand which technical artifacts are having a global effect on expression, something that could be useful when deciding which technical covariates need to be corrected for. it may also be possible to use this knowledge to better correct for such problems (perhaps, for example, by projecting out the seventh pc or including it as another covariate)."
"text mining is powerful method to extract knowledge from huge text repository. automatic text classification is significant research topic in text mining. stemming is to find root word and instance selection is reduce the dimensionality reduction of dataset. in this work, we implement ecas stemmer and advanced multi class instance with pre-computed kernel based support vector machine using real-time research articles dataset. the classifier providing better performance in real-time research articles classification. the classifier shown 75% accuracy in research article classification. hence, ecas stemmer and eis shown better result in real-time research articles classification. in future, the classifier is extended more than two classes."
"if we are looking at multiple covariates and want to find which one best explains the pc, we can rank all of the covariates according to the p-value calculated from mi c (câ j) (see section 2.2). the higher up on the list a covariate appears the more likely it is to give us information about the jth pc. similarly, for a fixed covariate, we can score each pc to see how closely that pc relates to our covariate."
"gtex also provides information about a number of covariates for each sample. there are five main biological covariates: age (broken down into 10-year increments), sex, hardy score, and two different versions of tissue type (one more specific than the other-unless otherwise noted, when we refer to tissue type we mean the more specific one). the hardy score is an integer between 0 and 4 that gives information about how a patient died-zero corresponds to a patient who died on a ventilator, while one through four correspond to the continuum from fast, violent deaths up through slower deaths. there are also many technical covariates, but we limit ourselves to those with at most 13 possible values (since our statistic is designed to work when the number of values is much smaller than the number of samples). this leaves us with four technical covariates. these are the site that enrolled the donor (coded with its bss code), the time between death and final tissue stabilization, severity of autolysis, and technology used to isolate the dna/rna."
"stemming is a process to find the root word. they are several techniques are available for stemming such as lovins, porter and yass stemmer. the existing stemmer includes several drawbacks. the ecas stemmer shown better efficiency to find the root word. the proposed classifier uses ecas stemmer for stemming process. ecas stemmer consists less morphological rules and using extract from table (eft) method. due to this reason, easily identify root word from given word with less time consumption."
"the major role of instance selection techniques is to reduce the complexity of training svm. recently proposed multi class instance selection is used to gaussian kernel. the performance of mcis is not sufficient. hence, the advanced mcis is used precomputed kernel support vector machine and its performance is better. the main objective of the whole process the text document convert into sparse vector."
"next we consider the samples drawn from the brain (datasets) . surprisingly, our method shows that it makes more sense to look at later pcs as opposed to looking at the first few. this finding can be very informative; in particular, if one is trying to visualize the transcriptome of the brain, then instead of projecting onto the first two pcs, practitioners can project onto later pcs. we also use our statistic to investigate how other covariates, both biological and technical, affect the global transcriptional landscape."
"we focus on applying our hybrid approach to pca, though in theory it can be applied to most dimensionality reduction techniques. pca is one of the most commonly used dimensionality reduction techniques in the biological sciences. it has famously been used to help understand population structure [cit], a method that has become standard in the fields of statistical and population genetics [cit] . more recently, pca has been used to explore structure in gene expression array data [cit] . it was shown that the first few pcs are closely related to the tissue of origin and that projecting down onto the first two pcs provides us with an informative way to visualize this extremely high-dimensional data. a similar method has been used to study cancerous tissue and stem cells in standard gene expression data, an application that showed cancer tissues seem to be an intermediary between regular tissue and stem cells, supporting the cancer stem cell hypothesis [cit] . these examples are just the tip of the iceberg when it comes to applications of pca to biomedical data! as currently implemented, however, this approach is limited. since the pcs are ordered according to how much variance they explain-the first pc explains the most variance, the second pc explains the second most, and so onresearchers often choose the first few pcs, completely ignoring information that might be hidden in other pcs. previous methods to decide how many pcs to look at apply random matrix theory to see if the pcs are more informative than they should be at random, but give no information about what covariates might be contributing to this structure ( [cit] ) . we argue that researchers should alternatively be looking at the pcs related to their problem of interest, not simply those most related to the overall variation in a dataset."
we introduce a hybrid method to help us better understand the pcs derived from rna-seq data. this mutual information-based statistic gives us clues about what kind of biological information is unveiled by pca. we also see that this statistic can reveal important information about the effect of technical variation on a data set-information that might help investigators decide how reliable biological conclusions are likely to be.
"some approaches consider a large number of nodes (a dense network), which are deployed near the phenomenon that needs to be monitored. the strategy behind the deployment of a large number of cheap non-reliable nodes has several advantages: (i) better fault tolerance through distributed operation; (ii) uniform covering of the monitored environment; (iii) easy deployment; (iv) reduced energy consumption; and (v) longer network lifetime."
"clubs is an approach that form cluster through local broadcast, cluster are formed with a maximum of two hops. clubs is based on the following assumptions: every node must be connected to a cluster, the diameter of all clusters must be the same and the nodes in a cluster must be able to communicate with each others [cit] ."
"our rna-seq data was obtained from the genotype-tissue expression (gtex) [cit] . this data is freely available online. for each sample we get a vector for which the ith entry equals the expression of the ith gene, where expression is given as number of reads per kilobase per million reads (rpkm). note that, instead of looking at each gene we could have looked at the rpkm for each transcript-our experiments have shown that the overall picture we get by applying our method at the gene level is very similar, though not identical, to that at the transcript level."
"linked cluster algorithm (lca) aims the mobility support of nodes. the main goal of lca is the formation of a efficient network topology where chs are hoped to form a backbone network. the cluster members can communicate with this backbone infrastructure while they are moving. lca assumes that nodes are synchronized and that a time-based medium access is used. finally, the main lca objective is the maximization of network connectivity [cit] ."
"until until only one community remains 12: end procedure when communities i and j are merged, we label this new community as j, update every k element of jth row and column, and remove the ith row and column. to update the matrix âq ij is considered three cases. if community k is connected to both i and j, then"
"our method suggests that we should use the first and fifth pcs to visualize how tissue type relates to rna-seq expression levels in the brain, a fact that would have been missed by the naive approach. the naive approach for using pca is to project our brain samples onto the first two pcs. when we apply our method we see that the first pc is closely related to the brain region-it actually divides the cerebral tissues we see that enrollment center scores the highest, suggesting that the variance embodied in the seventh pc might largely be due to a technical artifact."
"intuitively we can think of mi c (câ j) as estimating the percentage of information contained in our covariate that is explained by the jth pc. one could use numerous other measures-mi(c, b), kullbackleibler divergence, conditional entropy, etc. it turns out that for most reasonable measures, including those mentioned above, the p-value we calculate will be exactly the same (this follows since all of them can be calculated knowing only h(c), h(b), and h(b, c), where h(b) and h(c) are kept constant during each step of the monte-carlo procedure used to estimate the p-value, see below). the only case that the choice is likely to have an effect is when we fix the pc and are looking at multiple covariates."
"it is also worth considering the relation between our statistical approach and clustering, something that we touched on here. previous work [cit] ) has argued against the common practice of fig. 5 . clustering by csumi (black line) is more informative than the one using the spearman (red) or pearson (blue) correlation. for a varying number of dimensions, p, we take the data and project it onto a p-dimensional space using either csumi or onto the p-dimensional space spanned by the principal component analyses with highest spearman or pearson correlation, respectively (the score for each correlation is based off the median taken over 100 random orderings of the tissue types, where the error bars range of the 25th to 75th percentiles). we then perform kmeans clustering, compute the mutual information between the tissue type and the clustering, and plot the result. we see that csumi greatly outperforms either correlation-based approach."
"complex networks is a young scientific field motivated by observations in real networks. these networks have a nontrivial topology that is different from regular graphs or random graphs but usually are observed in real graphs. very often, community structures may be observed in these networks. a community is a subset of nodes with a high number of connections between them and a few connections between other nodes. these structures may reveal some information about the dealing network."
"an interesting property observed in many real networks is the modular structure. in these networks, there are a lot of edges between nodes of the same subgroups and few edges between nodes from different subgroups. these groups of vertices, also defined as communities [cit], are illustrated in figure 2 . a real example of network is the world wide web that has many hyperlinks between related web pages and a few hyperlinks between unrelated web pages. the understanding of community structure present in complex networks is interesting to many research fields because it may reveal important information about the dealing problem. in order to understand and extract information of real networks, many algorithms of community detection was developed. these algorithms are classified as agglomerative (\"bottom-up\") or divisive (\"top-down\"). agglomerative algorithms start considering that every node is a community and merge them forming larger communities. on the other hand, divisive algorithms detect communities by considering at the beginning that the whole network as a community and divide it in smaller communities."
"the used communication model considers one master node (base station), ch n cluster head (ch) nodes and s n slave nodes. the data collected by slaves is sent to their respective cluster head nodes that perform the data fusion. all the slave nodes reach the cluster head using just one hop. after a sensing slot time window t, cluster head nodes send their messages to the master node. slave and cluster head nodes use the same frequency for wireless communication. we consider that cluster heads communicate with master node in a different radio frequency. this difference in radio frequency avoid interference between slave-ch and ch-master communications."
"jingnian [cit] offered multi class instance selection (mcis) method for choose instances nearest boundary. mcis has been used to speed up support vector machine. on the other hand, using this method selection of instances will be less. chih-fong [cit] introduced support vector oriented instance selection (svois) for instance selection to text classification. svois provided best performance compared with several states of art instance selection algorithms such as enn, ib3, icf and drop3. hence, this method is not enhanced to nonlinear regression plane and large scale experiments. [cit] discussed several kernel functions with support vector machine using micro array data sets. the pre-computed kernel support vector machine is providing better results comparing to other kernel functions."
"the above behavior could have many different interpretations. one is that in some sense the ''dimension'' of the space of tissues is roughly six dimensional, and the affect on pcs beyond the sixth are from noise and nonlinearities related to this structure. this interpretation is especially enticing since it suggests that, if we are interested in the structure arising in the data due to the tissue type of our samples, then it may suffice to look at only the top six or seven pcs instead of looking at hundreds of them. this gives us a principled way of determining how many pcs we want to consider when exploring the data."
"nowadays, the growth of it everything should be digitalized. in this reason of, the usage of text information is very huge. text classification is an innovative research topic in text mining. automatic text classifier is very useful to organize and retrieve the text documents. machine learning techniques are very useful to develop text classifier with better efficiency. support vector machine is popular technique in supervised machine learning techniques. text mining techniques are very useful to analyze, classify and retrieve necessary information from huge text repository. text mining [cit] techniques are successfully applied to several fields such as clinical text, newspaper, educational text, web text, information retrieval and so on. text classification process mainly divided into two category such as text preprocessing and classification task. the text pre-processing is used to dimensionality reduction for increase the efficiency of the classifier. in text pre-processing, ecas stemmer was used to find root word and efficient instance selection (eis) for reduce dimensionality of data set."
here is proposed a hybrid algorithm to cluster formation based on two approaches: community detection in complex networks and the traditional clustering technique k-means. this new algorithm takes advantage of community detection approach that is able to find clusters of different shapes and kmeans that is a good clustering technique for cartesian points.
"using pca for dimension reduction before clustering biological data since only looking at the first few pcs can lead us to throwing away a lot of information about clusters in the data. our results suggest an approach using our technique might avoid this drawback, allowing better clustering of high-dimensional biological data. our results show that, despite conventional wisdom, researchers can get better results out of pca by using later pcs instead of using the first few pcs. our technique provides a means of selecting which of these pcs are most useful for a given task. we believe our approach will be useful to the many biological researchers struggling to find structure in their data."
"we introduce a statistic, mi c (câ j), which measures the percentage of the information contained in covariate c that is also contained in the jth pc. we compute this measure using mutual information, a standard tool in information theory [cit] . though our work is not the first to apply mutual information to transcriptomic data [cit], to our knowledge we are the first method to use it as a means of investigating dimensionality reduction. given two random variables, x 1 and x 2, the mutual information (informally) measures the amount of information shared between the two variables. it is defined as:"
"moreover, even though the relationship with enrollment center seems to be a technical artifact, there may in fact be a biological reason for it-perhaps there is some biological difference between the patients in each of the centers that the authors are unaware of. either way, we see that our technique allowed us to find a source of global variation that we need to correct for in the future, one that might have been hard to find without a statistic of the kind introduced here."
"as with all other tools that allow users to explore a data set, ours comes with the caveat that users must be careful not to let the results of the analysis negatively bias later statistical analysis-in any application one must be careful to use multiple test correction or other approaches to avoid false findings."
"term frequency (tf) and inverse document frequency (idf) is numeric statistical value which is very useful to importance of word in a document or a collection and corpus. it is popular word weighting technique, which is very useful to search engines and document ranking system. term frequency and inverse document frequency is very useful to pre-processing task in text mining. term frequency is proportionate between number of times occurred in a word and total number of words in a document. inverse document frequency is logarithmic value of proportion between to total number of documents and number of document occurred in a particular word. the terms are occur computer network, data mining, computing, engineering, electric, electronic and so on the research articles are classified to engineering articles. and the terms are occurring cancer, diabetics, bio-medical, genetics, medicine and so on the research articles are classified to medical articles."
"note that the results in this section are not surprising since similar results have been found by the maqc consortium, among others (seqc/ [cit] ). this demonstrates that csumi is able to help us uncover the forces that are driving the variance in the dataset. csumi gives users a method for understanding which covariates need to be corrected for in a given dataset, something that could be especially important as new technologies with new biases are introduced."
"in addition to the method itself as a novel contribution, we apply csumi to rna-seq data taken from the genotype-tissue expression (gtex) project [cit] . we show that the most informative pcs are not always the first few, but instead that later pcs play an important role as well. not only does this insight allow us to better visualize the global structure in rna-seq data, for the first time to our knowledge, but it allows us to measure how much certain biological and technical variables actually affect the data, information that could help us understand the underlying biological and technical drivers in a given dataset. we also apply our statistic to rna-seq data from brain tissue. our statistic shows that the first few pcs are not the most informative about which region of the brain a sample is taken from, but rather that later ones carry a large percentage of the information of interest. for instance, when we look at brain tissue using our method we see that pc 5 can differentiate the basal ganglia from other tissues. this finding is in sharp contrast to pcs 2, 3, and 4, which are not nearly as informative about tissue type (see section 3.7.1). we also consider other biological covariates ( supplementary fig. 5 )."
"the increasing interest in studying and understanding real networks is motivated by many science fields. an example of these networks is social network [cit], where people are represented by nodes and their friendship represented by connections. the world wide web is another example, where each webpage is represented by a node and its hyperlinks denoted by edges between nodes [cit] . other examples are energy transmission [cit], neural networks [cit] and protein iteration [cit] ."
"3.7. what selected pcs tell us about the brain 3.7.1. tissue type. above we have looked at the seven tissues with the largest number of samples in the dataset. it is just as easy for us to apply our hybrid approach to some subset of tissues. in particular, we examine tissues that are drawn from the brain. we choose the brain because it is the most complex of the organs represented in gtex, with many of its samples drawn from different regions of the brain."
"clearly the tissue of origin is a large contributor to the variance in the first few pcs. how much information about tissue of origin is hidden in later pcs? to answer this question we apply our mi c statistic to look at how much information about each of the first 100 pcs is embodied by tissue type (see table 3 for the first nine pcs). the results show that tissue type has a large effect on even fairly distant pcs. for the first six pcs the mi c score is quite high (above 0.3). the score then drops sharply, and from the 8th value to the 100th it fluctuates between 0.02 and 0.08 (which, even though it is quite a bit lower than for the first couple pcs, still has an estimated p-value of Â£0.001). the one exception to this is the 73rd pc, where the score is 0.014, which corresponds to an estimated p-value of about 0.033. the sharp divide that occurs between the sixth and eighth pc can be nicely seen by projecting the data onto the eighth and ninth pc (supp. fig. s2 ). though our statistic tells us that there is a significant relationship between tissue type and pcs even this far out, we see that it lacks the obvious structure of earlier pcs, structure we see even when comparing the fifth and sixth pcs (supp. fig. s3 ). this sharp drop in the csumi score suggests that in many cases most of the most interesting information about tissue is contained in the first six dimensions (though clearly even higher dimensions contain some information as well)."
"in this paper, we propose a hybrid clustering algorithm based on community detection in complex networks and traditional k-means clustering technique, called qk-means. this new approach takes advantage of both techniques in order to detect better clusters and allow a better deployment of cluster head nodes in large networks. we also propose a network model to simulate the dynamics in a real wsn to make possible the comparison of different approaches. simulation results show that qk-means detect communities and sub-communities and, therefore, the lost message rate is decreased and wsn coverage is increased."
"as future works we could point some improvements in qk-means like calculating the best ch coverage radius by analyzing the number of nodes in each cluster and if it found a dense cluster, then the algorithm could break them into small clusters and use more chs with lower ch radius."
"random competition based clustering (rcc) also focus on node mobility and tries to stabilize the formed clusters. rcc applies the first declaration wins rule, this rule considers that the first node that claim being a ch will be elected. after receiving the message of the first node (that is broadcasted) all neighboring nodes join its respective cluster. the ch periodically broadcast a claim packet [cit] ."
"though tissue type seems to be the main driver of variance in the first few pcs, our method also finds covariates that seem to influence later pcs. moreover, many of these high scoring covariates are technical covariates-that is to say that they are not related to the underlying biology, but are instead related to the way data was collected. this knowledge is important to have before any analyses using this dataset, since otherwise it might be hard to tell what variance is due to biology and what variance is due to technical artifacts."
"depending on the situation it is common to transform rna-seq data to a log or square root scale. though our method could be just as easily applied to transformed data, for simplicities sake we decided to focus in the main article on the raw rpkm data, as opposed to the log and/or square root transformed version. some analysis of log-transformed data is provided in the supplementary material. (supplementary table 1 and supplementary fig. 10.)"
"this measure can be viewed as the percentage of the information contained in covariate c that is also contained in the jth pc. this is the tool we will use to measure how well a given covariate corresponds to a given pc. to apply this approach when dealing with another dimension reduction technique, replace the a i 's with the coordinates we get after dimension reduction."
"k-means(k(c i ), c i ); 14: end for 15: end procedure after the clustering process, one cluster head node is deployed in every cluster midpoint."
"we introduce component selection using mutual information, csumi, a method to choose the most relevant principal components for any given feature. this approach allows researchers to better utilize these later pcs when trying to understand and visualize their data, helping them decide which pcs are worth keeping and which can be thrown away-for example, it can show which pcs will be the most informative visually and statistically to project onto. though there exists supervised dimension reduction techniques (lda, partial least squares, etc.), to our knowledge our method is the first to use information from covariates to understand and better prioritize the results of unsupervised dimensionality reduction methods."
"all the experiments are performed with matlab with edited libstring interface. they are two types of classification are followed such as only title and title with keyword. the below results are found when the range is 0.082 and margin value is 0.039. the table. 1 shows the performance of the classifier. the table. 2 shows the result analysis of the classifier. the classifier is classified into two categories such as engineering articles and medical articles. the classifier classified out of 750 research articles are 562 and 525 is title and title with keyword respectively correctly classified. the fig.2 . shows the comparison of classifier for two types of classification. according to the representation of fig.2, the classifier correct prediction is 74.9% in title and 70% in title with keyword. hence, the classifier prediction is true in maximum occurrences. in this reason of, the classifier performance is better in real time datasets."
"measure of how well tissues cluster together when projecting onto different numbers of principal components. the x-axis tells us how many pcs we are projecting down onto (note that we are projecting down onto the first few pcs as opposed to projecting down onto the highest scoring pcs), whereas the y-axis is our measure of clustering, clust. recall that a lower clust score means that each tissue type tends to cluster together. we see that at first the clust score decreases as we add more pcs, but after we add roughly six pcs there is no further improvement. this is consistent with our method, which implies that the first six pcs are much more informative about tissue type than later pcs are."
we want to measure how well samples with the same tissue type cluster together after dimension reduction. to do this we use a nearest neighbor-type approach-we estimate how much loss we expect by guessing a sample's tissue type based off its nearest neighbors after dimensionality reduction.
"plot of the result we get by projecting the data from the brain onto the first and fifth pcs versus projecting onto the first and second pcs. we see that the first pc divides cerebral tissue (samples labeled with either braincerebellar hemisphere or brain-cerebellum) from the rest of the brain, and the fifth pc divides the basal ganglia from the rest of the brain. no such division is seen using the second pc."
"the above discussion leaves a few unanswered questions: why are the cerebral tissues so different from the rest of the tissues in the brain? why is the fifth pc so much more informative than the second (and even more informative than the third and fourth)? we do not attempt to answer these questions here, and instead leave them as open questions for researchers with more of a biological background."
besides setting k we also have to specify the number of trials we want to use when estimating our pvalues. using more trials will give us a better estimate of our p-value but will also take longer. in all our experiments we use 999 trials to estimate our p-values.
"preserves clusters compared to correlation. second, we calculate the p-value derived from pearson or spearman correlation and project down onto the pcs with the lowest p-values. since this method involves a random ordering of the tissues we repeat this procedure 100 times and record median scores. in figure 5, we show that csumi outperforms both types of correlation. in fact, by comparing figures 4 and 5 we see that simply ranking pcs by eigenvalue does better. even if we look at the 75th or 95th percentile among the correlation scores instead of the median (supp. figs. 8 and 9 ) we see that csumi does at least as well as either correlation measure."
"the remainder of this paper is organized as follows. first, we present some related works in section ii. then, in section iii are presented a brief introduction of community detection in complex networks. in section iv we describe the networks simulation model and the proposed qk-means algorithm. in section v are presented and discussed the results of comparisons between our proposed technique and others. finally, we enumerate some conclusions and future works in section vi."
"we are also able to use our method to study technical variation in the rna-seq data. our results agree with previous studies of how technical variation effects rna-seq data (seqc/ [cit] ). this validation demonstrates our method's ability to help discover signal due to technical variation that needs to be corrected for before analyses. this application is particularly useful as new technologies emerge that have undiscovered biases. thus, csumi can further be viewed as a verification platform for emerging technologies."
in this next experiment was observed the influence of the cluster head buffer (ch b ) on lost message rate. it was used the qk-means compared to other two algorithm: simple modularity (described in section iii) and expectation-maximization (em) [cit] using a small s b . figure 5 shows the results of this simulation. it is possible to notice that as long as the ch buffer increases the lost message rate decreases until reach a point that do not decrease anymore. for this and all simulations was considered that messages on buffer (slave or ch) are not counted as lost messages.
"we aim to overcome these limitations by introducing a statistical approach that relates the results of dimension reduction to the biology we are interested in. this allows a practitioner to better explore largescale, higher-dimensional data in order to further uncover global structure-structure that is apparent when looking at the expression levels of all or many of the genes at once, but not in the expression levels of a single gene."
"consider, for example, the seventh pc ( fig. 1b; table 4 ). using our mi c score we see that the highest ranked covariate is enrollment center-which is the bss code of the collection site-with a score of about 0.17. we can also see enrollment center scores fairly highly when compared to the fifth pc (a score of 0.048, which has an estimated p-value less than 0.001). if we project the data onto the 7th and 5th eigenvalues and color by enrollment center, we see a definite effect (fig. 1b) . one might wonder if this is just due to confounding based on the relationship between tissue type and enrollment center. however, this is not the case since the same relationship is present when we plot only one type of tissue, namely the whole blood samples (supp. fig. s4, where we choose whole blood because it is the tissue we have the most samples from)."
"wireless sensor networks (wsn) are composed of small communicating nodes that contain a sensing unit, wireless communication module, processor, memory and a power supply, typically a battery [cit] . the set of nodes can be composed by same sensors or some of them may have special characteristics, like different kinds of sensors. these nodes are able to collect data and communicate to each other forming a network with three main kind of topologies: star (one hop), mesh (a routing algorithm between nodes is considered) and cluster-tree (network is divided in clusters) [cit] ."
"where k i is the degree of a vertice v i, i.e., the number of edges incident with this node and m the total number of connections (or links, edges) between two different nodes of the network. after setting the data structures, the process of community detection is described in the following algorithm:"
"n is the data set we get by projecting onto the four-dimensional space spanned by the first, second, third, and fifth components, then we see"
it is our hope that the above approach will help put the use of pca for exploring biological on firmer ground. we provide a tool that allows us to uncover potentially important sources of variance hidden in later eigenvalues and allows us to measure how significant these findings are.
"after the deployment, the monitoring phase starts. slave nodes begin to generate messages, cluster head nodes receive them and send them to master node. the monitoring phase is described in the following algorithm:"
"in this work, we collect technical papers from annual conference proceedings to determine the categories of the paper. the total number of papers in the collection is 750, while the total number of classified categories is two such as engineering and medical articles. the articles collection consists 586 engineering articles and 164 medical articles."
"note that there are some samples that do not cluster with their tissue types, which could be for many reasons-random noise, underlying medical conditions of the patients, or possible problems with the data point (such as mislabeling, contamination, etc). indeed, this work may better classify such samples."
"technical systematic reviews answer certain questions within a very specific area of expertise by selecting and analyzing the recent pertinent literature. in several research domains, huge amount of research papers are published. hence, the classification of research articles is a challenging issue. the remaining of these papers is organized as follows. section 2 discuss background study followed by implementation is elaborately explained in section 3. section 4 is analysis of results and followed by conclusion and future directions is section 5."
"t he influx of noisy, high-dimensional data into the biological sciences has made the ability to tease out lower-dimensional structure from that data crucial. recent years have provided numerous examples of how such low-dimensional structure can give insight into the underlying biology, serving both as a tool for understanding and visualization [cit] . one of the main techniques applied in these results is known as unsupervised dimensionality reduction. as the name suggests, this technique involves taking high-dimensional data and projecting it down onto a much lower-dimensional subspace. techniques such as principal component analysis (pca) and nonnegative matrix factorization [cit] attempt to perform this reduction so that very little information is lost. though such approaches have proven valuable, they do have limitations. in particular, it is left up to the user to unravel the biological meaning of the results on their own. moreover, these methods can lead to users ignoring important information hidden in higher dimensions. how can one decide which of the reduced dimensions are biologically relevant or which can be traced to artifacts or noise?"
"fast local clustering service (floc) is a distributed technique that tries to form equal-sized cluster with minimum overlap [cit] . the non-ch nodes are classified based on their proximity to ch into inner (i-band) and outer (o-band). nodes classified as i-band will suffer low interference during the wireless communication with the ch, and o-band nodes may lose most of their messages due to high interference level. the i-band membership is preferred in order to increase intracluster traffic."
"for our first set of analyses we discarded tissue types represented by few samples, leaving only the seven most common tissue types: thyroid, whole blood, artery (tibial), lung, sun-exposed skin, muscle (skeletal), and heart (left ventricle). note that samples from the artery are referred to as blood vessels in all our figures (this is due to the naming scheme of gtex, which has two different labelings for each tissue type, one more specific). this gives us a total of 809 samples. for more details about this data see the gtex website or reference [cit] ."
"a plot of the rna-seq data projected onto the first two pcs, where each spot corresponds to a sample and each color to a tissue type of origin. we see that, as expected, the samples from the same tissue cluster together. (b) a plot of the rna-seq data projected onto the fifth and seventh pcs, now colored by enrollment center (i.e., bss center codes c1, b1, and d1). we see that there is an obvious relation between the seventh pc and enrollment center. pcs, principal components."
"wsn with cluster-tree topology commonly use a special sensor node, called cluster head, that collects the data of each cluster in order to increase coverage and decrease lost messages. thus, one problem of these networks is the process of cluster detection in order to deploy cluster head sensors. moreover, when a large number of nodes is used, the connections between these nodes usually create a non-trivial topology resulting in a complex network. in these networks the clustering process, called community detection, becomes a more difficult task."
"we demonstrate that our approach, csumi, informs practitioners which pcs give the most information about their biological question of interest. we begin by validating our approach on rna-seq data from the seven most common tissues (datasets) and show that the first six pcs are the most informative about tissue type. this finding tells us that, if we want to use pcs to better understand tissue type (for example if we want to perform clustering or something along those lines), then we should consider the first six pcswithout our measure we might think it suffices to consider only the first two or three pcs."
"clustering algorithms are necessary to organize large wsn in clusters in order to decrease the number of lost messages and increase the network coverage. in this section, we present some related clustering approaches for wsn."
"convergence property. we conduct an experiment to see the effect of the number of the loopy belief propagation iterations. figure 13 illustrates the convergence analysis results of the learning algorithm. we see on both test cases, the learning algorithm can converge in less than 200 iterations. after 120 learning iterations, the prediction performance of trifg on both test cases becomes stable. this suggests that the learning algorithm is very efficient and has a good convergence property. figure 11 already shows the distribution of follow-backs in different time stamps. now, we quantitatively examine how different settings for the time span will affect the prediction performance. figure 14 lists the average prediction performance of trifg in the two test cases with different settings of the time span. it shows that when setting the time span as two or less timestamps, the prediction performance of trifg drops sharply; while when setting it as three timestamps, the performance is acceptable. the results are consistent with the statistics in figure 11 : more than 90% of follow-back actions are performed in the first three timestamps, and only about 80% of the follow-back actions are in the first two timestamps. pagerank, #degree, and (Î±, Î²) algorithm, to find elite users. now we examine how the different algorithms would affect the prediction performance. table ii shows the prediction performance of trifg with different elite user finding algorithms in the two test cases. interestingly, though trifg with the (Î±, Î²) algorithm achieves the best performance, the difference of performance among the three algorithms, especially in the second test case, is not that pronounced (with a difference of 1%-4% in terms of f1-measure score). this confirms the effectiveness and generalization of incorporating the status homophily factor into our trifg model."
"two interesting questions arise: how is a reciprocal relationship developed from a parasocial relationship and how do pairwise relationships further develop into a triadic closure? employing twitter as the basis of our analysis, we try to answer these questions. in particular, when you follow a user on twitter, how likely is it that the user will follow you back? some users only follow back those who are real \"friends\" in their physical world, while some other users (even some top users with tens of thousands of followers) will follow everyone back."
"in this section, we propose a novel triad factor graph (trifg) model to incorporate all the information within a single entity for better modeling and predicting the formation of reciprocal relationships and triadic closure. for easy explanation, we will mainly use the follow-back prediction in our explanation and the extension to the triadic closure prediction is straightforward."
"now we present a case study to demonstrate the effectiveness of the proposed model. figure 16 shows an example generated from our experiments. it represents a portion of the twitter network from the 10th-13th timestamps. black arrows indicate following links created 4 timestamps (we use 4 timestamps as the time span for prediction) before. blue arrows indicate new-following link in the past 4 timestamps. dash arrows indicate follow-back links in our dataset (a), predicted by svm (b), and predicted by our model trifg (c), with green color denoting a correct one and red color denoting indicates there should be a follow-back link, but the approach does not detectit."
"tableaux and naÃ¯ve databases relational patterns are known under the name of tableaux if one views them as queries, and as naÃ¯ve tables if one views them as data. the instance below on the left is a usual relation, and the one on the right is a tableau/naÃ¯ve table:"
"note that the satisfiability problem for relational patterns expressed via naÃ¯ve databases -whether the set d is not empty -is trivial, the answer is always yes. in the presence of constraints on the schema it can become a fairly complicated problem, sometimes even undecidable."
"triadic status. we examine the correlation between users' social status and triads associated with them. we divide users into two categories (elite users and ordinary users). for simplicity, we select the top 200 users with the highest indegree as elite users, and the others as ordinary users. then we study the probability of a creating a new follow link to b's followee c, when a follows back b, conditioned on the status of a, b, and c. figure 8 shows the analysis result. the three digits on the x-axis represent the status of the three users (a, b, and c, with 1 indicating elite user and 0 indicating ordinary user) and y-axis represents the probabilities of different categories of users who formed triadic closure. we find a striking pattern that the highest probability is resulted by 101 (high status, low status, high status), which means that it is very likely a high-status user spends time investigating whom a low-status user follows, when she/he follows back the low-status user, and finally follows some high-status followees of the low-status user. the likelihood is almost ten times higher than chance. another interesting phenomenon is that when a low-status user a follows back another lowstatus user b, the likelihood of a following a low-status followee of b is very low (about 0.005%), while the likelihood of a following a high-status followee of b is much higher (4 times higher). some other interesting patterns can be summarized as follows."
"in our previous work, we study the problem of reciprocal relationship prediction. in this work, we extend this work from the following aspects. first, we further investigate how closure triads are formed from pairwise relationships, and how the formation is correlated with factors such as link homophily and social status. second, we extend the factor graph model to infer the triadic closure formation. last, we evaluate the proposed model on the dataset twitter."
"in addition to conjunctive queries (sometimes abbreviated as cqs), we shall consider their unions and boolean combinations. the former class, denoted by ucqs sometimes, is obtained by closing cqs under union (i.e., if q 1, q 2 are ucqs producing relations of the same arity, then q 1 âª q 2 is a ucq). for boolean combinations of conjunctive queries (abbreviated bccqs), the additional closure rules are that q 1 â©q 2, q 1 âª q 2, and q 1 â q 2 are bccqs."
"prediction setting. we use the dataset described in section 3 in our experiments. to quantitatively evaluate the effectiveness of the proposed model and compare with other alternative methods, we carefully select a dynamic network which consists of a completely historic log of link formation information among users, that is, each user is associated with a complete list of followers and followees at each timestamp. the network is comprised of 112,044 users, 468,238 following links among them, and 2,409,768 tweets. on average, there are 40,943 new-follow links and 3,337 new-followback links per day. we divide the subnetwork into 13 timestamps by viewing every four days as a timestamp."
it trains a conditional random field [cit] ] model with attributes associated with each edge. the difference of this method from our model is that it does not consider structural balance factors.
"in relational databases, it is common to have an exponential gap between data and combined complexity: for instance, data complexity of all first-order queries is very low (ac 0, i.e., a subset of dlogspace), while combined complexity is np-complete for cqs and pspace-complete for first-order."
"-geographic distance. do users have a higher probability to follow each other when they are located in the same region? -homophily. do similar users tend to follow each other? we make the analysis for both follow-back and triadic closure predictions. -implicit network. how does the following network on twitter correlate with other implicit networks, for example, retweet and reply network? -social balance. does the reciprocal relationship network on twitter satisfy the social balance theory [cit] ? to which extent?"
"-containment is the most basic query optimization task. indeed, the goal of query optimization is to replace a given query with a more efficient but equivalent one; equivalence of course is testing two containment statements. -containment can be viewed as finding certain answers over incomplete databases, using the duality between queries and patterns. a pattern Ï describes an incomplete database; if, viewed as a query, it is contained in a query q, then the certain answer to q over Ï is true, and the converse also holds. this correspondence is well known for both relations and xml. -finally, containment is the critical task in data integration, specifically in query rewriting using views [cit] . when a query needs to be rewritten over the source database, the correctness of a rewriting is verified by checking query containment."
"as most querying tasks for xml have to do with navigation through documents, reasoning/static analysis tasks deal with mechanisms for specifying interaction between navigation, data, as well as schemas of documents. navigation mechanisms that are studied are largely of two kinds: they either describe paths through documents (most commonly using the navigational language xpath), or they describe tree patterns."
"factor contribution analysis. for the triadic closure prediction, we mainly consider three factor functions: structural balance correlation (b), status homophily (s), and link homophily (l). here we examine the contribution of the different factors defined in our model. again, we first rank the individual factors by their predictive power, and then remove them one by one in reversing order of their prediction power. in particular, we remove structural balance correlation denoted as trifg-b, followed by further removing the status homophily denoted as trifg-bs, and finally removing link homophily denoted as trifg-bsl. we train and evaluate the prediction performance of the different versions of trifg. figure 15 shows the average f1-measure score of the different versions of the trifg model. we can observe a clear drop on the performance when ignoring each of the factors."
". it represents all trees in which there is an a-node holding value 1, with a b-descendant holding some value, that has two children: a c-node with the same value, and an a-node with value 3, about which we also know that it appears after the c-node in the sibling order."
"due to the complicated hierarchical structure of xml documents and the many ways in which it can interact with data, reasoning about xml data has become an active area of research, and many papers dealing with various aspects of static analysis of xml have appeared, see, e.g. [1, 6, 12, 16-18, 24, 26, 27, 29] ."
"for instance, [cit] showed that when schemas are given by nested-relational dtds, the complexity of sat sm (â, â, ) drops to pspace-complete. if, in addition, all the queries used in Ï st are from cq(â), then âsat sm (â) can be solved in polynomial time."
"the proof shows that the problem sat-bccq(â, â, ) is in Ï p 2 . satisfiability of bccqs can easily be reduced to simultaneous satisfiability of a cq and unsatisfiability of a ucq. for this, we need to guess a witness tree t; then satisfiability of a cq can be done in np and unsatisfiability of a ucq in conp, giving us the bound. one can still use the cutting technique to reduce the size of this tree; however, this time the size is not polynomial but rather exponential. however, the only objects of exponential size are long non-branching paths in the tree, so they can be carefully re-labeled and encoded by polysize objects in a way that checking satisfiability or unsatisfiability of cqs and ucqs can still be done with the same complexity as before."
"to solve the objective function, we adopt a gradient descent method (or a newtonraphson method). we use Î¼ as the example to explain how we learn the parameters. specifically, we first write the gradient of each Î¼ k with regard to the objective function (eq. (5)"
"we then introduce pattern-based queries, specifically analogs of conjunctive queries, their unions, and boolean combination, and survey results on their containment. using those results, we derive bounds on finding certain answers for queries over incomplete documents. finally, we deal with reasoning tasks for pattern-based schema mappings, which also rely on a form of containment statement."
"structural balance. based on structural balance theory, as in figure 6, we define four features capturing all situations of structural balance theory for each triad."
"since our patterns are essentially tree-shaped, the problem as formulated above is trivial for them: the answer is trivially yes as one just turns a pattern into a tree. what is more interesting for us is satisfiability with a schema."
"thus, we already see a big difference in the containment problem for xml patternbased conjunctive queries, which is Ï p 2 -hard, and relational cqs, for which the problem is in np. the question is then when we can lower the complexity of containment to np, to match the relational case."
"in this section, we first describe our experimental setup. we then present the performance results for different approaches in different settings. next, we present several analyses and discussions. finally, we use a case study further to demonstrate the advantage of the proposed model."
"a tableau has a list of variables, among those used in it, selected as 'distinguished' variables; that is, formally it is a pair (d,x), where d is a naÃ¯ve database andx is a tuple of variables among those mentioned in d."
"we now turn to discuss the performance of triadic closure prediction by the different methods we considered. table iii shows the results in the two test cases (prediction performance for the 9th-12th timestamps and that for the 10th-13th timestamps). in the task of triadic closure prediction, the labeled data is very unbalanced (a large portion of instances are negative instances, i.e., a follows b back, but does not follow b's followees), thus it is more difficult than the reciprocity prediction task. even the best performance of the baseline methods on the first test case is only 10% by f1 and 22% on the second test case. our proposed trifg significantly improves the performance (+18.6% in terms of f1-score). the situation is similar on the second test case. comparing with the other three graph-based methods, trifg also results in an improvement of 23-34%. the advantage of trifg mainly comes from the improvement on precision."
"the plan of the survey is as follows. we first explain the basic relevant notions in the relational case, particularly the pattern/query duality and the connection with incomplete information. we then define tree patterns, present their classification, and explain the notion of satisfaction in data trees, i.e., labeled trees in which nodes can carry data values. after that we deal with the basic pattern analysis problem: their satisfiability. given that patterns are tree-shaped, satisfiability per se is trivial, but we handle it in the presence of a schema (typically given by an automaton)."
"1 this problem also implicitly exists in other social networks such as facebook and linkedin: when you send a friend request to somebody, how likely will she/he confirm your request? how likely will two connected pairwise friendships finally form a closure triad?"
"thus, it appears that we can lift results for containment to state results about certain answers. however, this is only partly true. when we deal with query answering, we are interested in a finer classification of complexity, namely:"
"in our analysis, we consider the geographic location of each user. specifically, we first extract the location from the profile of each user 2, and then feed the location information to the google map api to fetch its corresponding longitude and latitude values. in this way, we obtain the longitude and latitude of about 59% of users in our dataset. more detailed analysis, source-code, and an online demonstration are publicly available. http://reciprocal.aminer.org/"
"link prediction. our work is related with link prediction, which is one of the core tasks in social networks. existing work on link prediction can be broadly grouped into two categories based on the learning methods employed: unsupervised link prediction and supervised link prediction. unsupervised link predictions usually assign scores to potential links based on the intuition that the more similar the pair of users are, the more likely they are linked. various similarity measures of users are considered, such as preferential attachment [cit], and the katz measure [cit] ]. [cit] present a flow-based method for link prediction. a survey of unsupervised link prediction can be found in liben-nowell and kleinberg [cit] ."
"in this section, after presenting several definitions, we formally define the targeted problem in this work. we formulate the problem in the context of twitter to keep things concrete, though adaptation of this framework to other social network settings is straightforward."
"homomorphisms can also be used to give semantics of incomplete databases. it is assumed that a naÃ¯ve database d represents all complete databases d â² (i.e., databases over c) such that there is a homomorphism h :"
"geographic distance. figure 2 shows the correlation between geographic distance and the probability that two users create a reciprocal relationship. interestingly, it seems that online social networks indeed go global: figure 2 different time zones. clearly, the geographic distance is already not a major issue to stop users from developing a (reciprocal) relationship. figure 2 (b) shows another statistic which indicates a different perspective that the twitter network (in some sense) still stays local: the average number of reciprocal relationships between users from the same time zone is about 50 times higher than the number between users with a distance of three time zones."
notice that the semantics of patterns allows different Âµ i to be mapped into the same nodes in a tree. we also consider a class pat(â) of patterns which is a restriction of the most general patterns to downward navigation only. these are defined by the grammar
"(1) geographic distance has a pronounced effect on the number of reciprocal relationships created between users, but little effect on the likelihood of users following back each other. (2) users with common friends (reciprocal relationships) tend to follow each other. (3) elite users have a much stronger tendency (status homophily) to follow each other than ordinary users. (4) the implicit networks of retweet or reply links have a strong correlation with the formation of two-way (reciprocal) relationships. (5) the network of reciprocal relationships on twitter is balanced (88% of triads satisfying the structural balance property), while the network of parasocial relationships is unbalanced (71% are unbalanced). (6) elite users play an important role for developing triadic closure. the probability of an elite user developing a closure triad is almost ten times higher than chance."
"then we consider directed links and define another 9 features (three features for each pair among the three users): the number of common reciprocal links, number of common followers, and number of common followees."
"we have already said that the containment provides a way to address the problem of finding certain answers to queries over incomplete databases. in the relational case, we saw the equivalence certain(q, in the case of xml, the standard view of incomplete documents is that of tree patterns [cit] . for instance, a pattern in pat(â) specifies the child relation, but no nextsibling relation, and nodes may contain incomplete information as data associated with them. in a tree in pat(â, â, ), structural information may be missing too. consider, for instance, a pattern"
"another type of related work is social behavior analysis. [cit] study the difference of the social influence on different topics and propose topical affinity propagation (tap) to model the topic-level social influence in social networks and develop a parallel model learning algorithm based on the map-reduce programming model. [cit] investigate how social actions evolve in a dynamic social network and propose a time-varying factor graph model for modeling and predicting users' social behaviors. the proposed methods in these works can be utilized in the problem defined in this work, but the problem is fundamentally different."
"that, though the three algorithms present different statistics, \"elite\" users have a much stronger tendency to follow each other: the likelihood of two elite users following back each other is nearly 8 times higher than that of ordinary users (by the (Î±, Î²) algorithm). the (Î±, Î²) algorithm seems able to better distinguish elite users from ordinary users in our problem setting. this is because besides the global network structure, the (Î±, Î²) algorithm also considers the community structure among elite users."
"implicit structure. on twitter, besides the explicit network with following links, there are also some implicit network structures that can be induced from the textural information. for example, user a may mention user b in her tweet, that is, \"@b\", which is called a reply link; user a may forward user b's tweet, which results in a retweet link. we study how the implicit links correlate with the formation of the follow-back relationship on twitter. figure 5 clearly shows that when users a and b retweet or reply to each other's tweets, the likelihood of their following back each other is higher (3 times higher than chance). another interesting phenomenon is that compared with replying to someone's tweet, retweeting (forwarding) her tweet seems to be more helpful (15% versus. 9%) to win her follow-back."
"we have developed and deployed a web application for reciprocal prediction based on the proposed trifg model 7 . figure 17 shows a screenshot of the reciprocity prediction system. the system trains a trifg model offline using all the follow and follow-back relationships in our dataset. when a user wants to know how likely another user will follow him back, he first inputs his twitter id and the other user's id. then the system analyzes his social circle and the other user's social circle, and extracts features defined in our approach. next, it makes the prediction based on the trained trifg model (refer to section 4.2). as the example in figure 17 shows, the user \"jian pei\" has a probability of 92% to follow back user \"jietang\"."
"in the six methods, svm and crf-balance only consider attribute factors; wtrifg further considers unlabeled data. crf considers all factors we defined, but does not consider unlabeled data. our proposed trifg model considers all factors as well as the unlabeled data."
"status homophily. we also test whether each pair of users have similar social status, and define the following 12 features (four features for each pair among the three users): whether or not the two users are both elite users, an ordinary and an elite (two features), and both ordinary users."
"link homophily. similar to the analysis to follow-back, we test whether users who share common links (followers or followees) will have a tendency to form a closure triad. figure 9 shows the probability of user a following user c, conditioned on the number of common neighbors. it clearly shows that when the number is one or zero, the probability is very low, while there is a sharp increase when the number becomes two. after that, the sublinear behavior takes over. the deviation at 0, 1, 2 can be seen as a slight \"s-shaped\" effect: the plots mainly show sublinear increase, while we observe a superlinear between 1 and 2."
"a tree pattern presents a partial description of a tree, along with some variables that can be assigned values as a pattern is matched to a complete document. for instance, a pattern a(x)[b(x), c(y)] describes a tree with the root labeled a and two children labeled b and c; these carry data values, so that those in the a-node and the b-node are the same. this pattern matches a tree with root a and children b and c with all of them having data value 1, for instance; not only that, such a match produces the tuple (1, 1) of data values witnessing the match. on the other hand, if in the tree the b and the c nodes carry value 2, there is no longer a match."
"in this article, we try to conduct a systematic investigation on the problem of predicting reciprocity and triadic closure formation. we precisely define the problem and propose a triad factor graph (trifg) model. the trifg model incorporates social theories into a semisupervised learning model, where we have some labeled training data (reciprocal relationships) but with low reciprocity [cit] . for reciprocity prediction, given a historic log of users following actions from time 1 to t, we try to learn a predictive model to infer whether user a will add a follow-back link to user b at time (t + 1) if user b creates a new follow link to user a at time t. for triadic closure prediction, we try to infer, when a follow back b at time t, whether user a will add a new follow link to b's followee c at time (t + 1). figure 1 shows an illustrative example of the addressed problem. figure 1 results. we evaluate the proposed model on a twitter data consisting of 13,442,659 users and their profiles, tweets, following behaviors (new following or follow-back links) for nearly two months. we show that incorporating social theories into the proposed factor graph model can significantly improve the performance (+22-27% by f1-measure) for predicting reciprocity and (+20-28%) for predicting triadic closure compared with several alternative methods. our study also reveals several interesting phenomena."
"finally, one can extend queries with inequality comparisons of data values. this makes the problem undecidable for bccqs and all the axes, or for cqs with â and â * under schemas."
"we first engage in some high-level investigation of how different factors influence the formation of reciprocity and triadic closure, since one major motivation of our work is to find the underlying factors and their influence to this task. in particular, we study the interplay of the following factors with the formation of follow-backs (or triadic closure)."
crf. it trains a conditional random field model with all factors (including attributes and structural balance factors) and predicts edges' labels in the test data.
"organization. section 2 formulates the problem. section 3 introduces the dataset and our analyses on the dataset. section 4 explains the proposed model and describes the algorithm for learning the model. section 5 presents experimental results that validate the effectiveness of our methodology. finally, section 6 reviews the related work and section 7 concludes this work."
"online social networks (e.g., twitter, facebook, myspace) significantly enlarge our social circles. the structure of the networks governs the dynamics of the networks (e.g., information propagation and users' behavior changes). in a social network with directed links such as twitter, the relationship between users often starts by one user (a) creating a \"follow\" (parasocial) relationship to another user (b). user b can choose to \"follow\" a back, which results in a reciprocal relationship between them. on the other hand, if a follows b, and continues to follow b's followee c, then (a, b, c) forms a directed closure triad. this phenomenon is also referred to as \"link copying\" [cit] ."
"we now describe the performance results for the different methods we considered. table i shows the results in the two test cases (prediction performance for the 9th-12th timestamps and that for the 10th-13th timestamps). it can be clearly seen that our proposed trifg model significantly outperforms the four comparison methods. in terms of f1-measure, trifg achieves a +27% improvement compared with the (svm). comparing with the other three graph-based methods, trifg also results in an improvement of 22-25%. the advantage of trifg mainly comes from the improvement on recall. one important reason here is that trifg can detect some difficult cases by leveraging the structural balance correlation and homophily correlation. for example, without considering the two kinds of social correlations, the performance of wtrifg decreases to 70-72% in terms of f1-measure in the two test cases. another advantage of trifg is that it makes use of the unlabeled data. essentially, it further considers some latent correlations in the dataset, which cannot be leveraged with only the labeled training data. now, we perform several analyses to examine the following aspects of the trifg model: (1) contribution of different factors in the trifg model; (2) convergence property of the learning algorithm; (3) effect of different settings for the time span; and (4) effect of different algorithms for elite user finding."
"we look at specific examples to study why the proposed model can outperform the comparison methods. \"a\", \"b\", and \"c\" are three elite users identified using the (Î±, Î²) algorithm [cit] . svm correctly predicts that there is a follow-back link from \"c\" to \"b\", but misses predicting the follow-back link from \"c\" to \"a\". our model trifg correctly predicts both the follow-back links. this is because trifg leverages the structural balance factor. the resultant structure among the three users by svm is unbalanced. trifg leverages the structural balance factor and tends to result in a balanced structure."
"the next obvious question is about matching lower bounds. they can be shown with the simplest form of navigation, or, alternatively, with all the navigation but just for cqs [cit] ."
"the semantics of a mapping m, denoted by m, is the set of pairs of trees (t, t â² ) so that t â² is a solution for t. the second condition is a containment statement, albeit a bit unusual one. it does not say that the cq âÈ³q s (x,È³) is contained in the cq âzq t (x,È³) but rather that the result of the first cq on t is contained in the result of the second cq on t â² . another, more conventional way, to read that statement is as follows: for all values x,È³ making q s true in t, there exist valuesz so that q t (x,z) is true in t â² . the basic reasoning tasks about schema mappings relate to their consistency, or satisfiability:"
"(1) elite users (opinion leader) tend to follow each other. the likelihood of an elite user following back another elite user is nearly 8 times higher than that of two ordinary users and 30 times that of an elite user and an ordinary user. (2) reciprocal relationships on twitter are balanced, but parasocial relationships are not. more than 88% of social triads (groups of three people) with reciprocal relationships satisfy the social balance theory, while parasocial relationships are unbalanced (only 29% of them satisfy the balance theory). (3) social networks are going global, but also stay locally. no matter how far a user is from one by geospatial distance, the likelihood that she/he will follow one back is almost the same, while on the other hand, the number of reciprocal relationships between users within the same time zone is 20 times higher than the number of users from different time zones. (4) elite users play an important role for developing triadic closure. the likelihood to form a closure triad when an elite user follows back an ordinary user is 10 times higher than that of an ordinary user following back an elite user."
the difference of wtrifg from trifg is that we do not consider status homophily and structural balance here. we use this method to evaluate how social theories can help this task.
"all algorithms are implemented in c++, and all experiments are performed on a pc running windows 7 with intel(r) core(tm) 2 cpu 6600 (2.4 ghz) and 4gb memory. the proposed algorithm has the tractable running times on networks of 112,044 size/order of magnitude. our reciprocity predictions required 2 to 5 minutes, and triadic closure predictions required 2 to 18 minutes."
"svm. it uses the same attributes associated with each edge as features to train a classification model and then employs the classification model to predict edges' label in the test data. for svm, we employ svm-light."
"we aim to find a large set of users and a continuously updated network among these users, so that we can use the dataset as the gold standard to evaluate different approaches for our prediction. to begin the collection process, we select the most popular user on twitter, that is, \"lady gaga\", and randomly collect 10,000 of her followers. we take these users as seed users and use a crawler to collect all followers of these users by traversing following edges. we continue the traversing process, which produces in total 13,442,659 users and 56,893,234 following links, with an average of 728,509 new links per day. the crawler monitors the change of the network structure from 10/12/2010 to 12/23/2010. we also extract all tweets posted by these users and in total there are 35,746,366 tweets."
"thus, to compute certain answers, all one needs to do is to run a query on the incomplete database itself. this is referred to as naÃ¯ve evaluation. note that the data complexity of finding certain answers is tractable, as it is the same as evaluation of conjunctive queries. the fact that naÃ¯ve evaluation works for boolean conjunctive queries extends in two ways: to ucqs, and to queries with free variables [cit] . in some way (for the semantics we considered) the result is optimal within the class of relational algebra queries [cit] . in particular, naÃ¯ve evaluation does not work for bccqs (even though it was shown recently that data complexity of finding certain answers for bccqs remains polynomial [cit] )."
"structural balance. now, we connect our work to a basic social psychological theory: structural balance theory [cit] . let us first explain the . structural balance correlation. y-axis: probability that a triad creates two-way (reciprocal) relationships, conditioned on whether the resultant structure is balanced or not. structural balance property. for every group of three users (called triad), the balance property implies that either all three of these users are friends or only one pair of them are friends. figure 6 shows such an example. to adapt the theory to our problem, we can map either the reciprocal relationship or the parasocial relationship on the friendship. then we examine how the twitter network (only reciprocal relationships or parasocial relationships) satisfies the structural balance property. more precisely, we compare the probabilities of the resultant triads that satisfy the balance theory based on reciprocal relationships and parasocial relationships on twitter. figure 7 clearly shows that it is much more likely (88%) for users to be connected with a balanced structure of reciprocal relationships, while with parasocial relationships, the resultant structure is very unbalanced. this is because two users are very likely to follow a same movie star, but they do not know each other, which results in a unbalanced triad (figure 6(c) ). we now present some observations of the formation of triadic closure. we focus on studying how users' status and activity influence the formation of the triadic closure."
"definition 2.1. new follow and follow back. suppose at time t, user v i creates a link to v j, who has no previous link to v i, then we say v i performs a new-follow behavior on v j . when user v i creates a link to v j at time t, who already has a link to v i before time t, we say v i performs a follow-back behavior on v j ."
"for these classes containment is still decidable, and the complexity stays in np for ucqs given explicitly as unions of cqs, and goes up to Ï p 2 -complete for bccqs [cit] . certain answers and naÃ¯ve evaluation now suppose we have a naÃ¯ve database d and a query q; assume that q is boolean. the standard notion of answering a query on an incomplete database is that of certain answers:"
"it is also worth looking at the situation of users 9 and 10. trifg made a mistake here: it does not predict the follow-back link, while the link is correctly predicted by svm. users 9 and 10 have a similar social status (similar indegree) and also they are from the same time zone, thus svm successfully predicts the follow-back link. however, as the resulting structure is unbalanced, trifg make a compromise and finally results in a mistaken prediction."
"we deal with patterns that are naturally tree-shaped. this is contrast with some of the patterns appearing in the literature [cit] that can take the shape of arbitrary graphs (for instance, such a pattern can say that we have an a-node, that has b and c descendants, that in turn have the same d-descendant: this describes a directed acyclic graph rather than a tree). in many xml applications it is quite natural to use tree-shaped patterns though. for example, patterns used in specifying mappings between schemas (as needed in data integration and exchange applications) are such [cit] . it is also natural to use them for defining queries [cit] as well as for specifying incomplete xml data [cit] ."
is the expectation of factor function h k (y c ) given the data distribution (essentially it can be considered as the average value of the factor function h k (y c ) over all triads in the training data); and
"in this article, we study the novel problem of predicting reciprocity and triadic closure in social networks. we formally define the two subproblems and propose a triad factor graph (trifg) model, which incorporates social theories into a semisupervised learning model. we evaluate the proposed model on a large twitter network. we show that with the proposed factor graph model it is possible to accurately infer 90% of reciprocal relationships in a dynamic network. we also demonstrate that the proposed model significantly improves the performance (+22%-27% by f1-measure) for triadic closure prediction comparing with several alternative methods. our study also reveals several interesting phenomena. of common neighbors, percentage of common neighbors of the two users (respectively), and the average percentage."
"the set of all trees accepted by a is denoted by l(a). a data tree is in l(a) iff its \"data-free\" part is in l(a) (i.e., the tree obtained by simply dropping the data-value assigning function Ï)."
"our general task is to predict whether a user will follow another user back (or follow another user's followee so as to form a closure triad) at the next timestamp when she follows back the user. by a more careful study, however, we find that it is very challenging if we restrict the prediction just for the next timestamp. figure 11 shows the distribution of time span in which a user performs the follow-back action, which indicates that 60% of follow-backs are performed in the next timestamp though 37% of the follow-backs would be still performed in the following three timestamps. for the triadic closure formation, it is the similar case, that is, 59% of formed triadic closure happens in the next timestamp and 37% in the following three timestamps. a further data analysis shows that active users often either perform an immediate follow-back (at the next timestamp) or reject to follow back; while some other (inactive) users may not frequently login into twitter, thus the time span of follow-backs varies a lot. according to this observation, in our first experiment, we use a network of the first 8 timestamps for training and predicate follow-back actions in the following 4 (9th-12th) timestamps (test case 1). then we incrementally add the network of the 9th timestamp into the training data and again use the following 4 (10th-13th) timestamps for prediction (test case 2). we respectively report the prediction performance of different approaches for the two test cases."
"factor contribution analysis. in trifg, we consider five different factor functions: geographic distance (g), link homophily (l), status homophily (s), implicit network correlation (i), and structural balance correlation (b). here we examine the contribution of the different factors. we first rank the individual factors by their predictive power 6, and then remove them one by one in reversing order of their prediction power. in particular, we first remove structural balance correlation denoted as trifg-b, followed by further removing the implicit network correlation denoted as trifg-bi, status homophily denoted as trifg-bis, and finally removing link homophily denoted as trifg-bisl. we train and evaluate the prediction performance of the different versions of trifg. figure 12 shows the average f1-measure score of the different versions of the trifg model. we can observe a clear drop on the performance when ignoring each of the factors. this indicates that our method works well by combining the different factor functions and each factor in our method contributes improvement in the performance."
"in social science, relationships between individuals are classified into two categories: one-way (called parasocial) relationships and two-way (called reciprocal) relationships [cit] . the most common form of the former are one-way relationships between celebrities and fans, while the most common form of the latter are two-way relationships between close friends. twitter and facebook are respectively typical examples of the two types of social relationships. relationship is the basic object in social network analysis [cit] . it forms the basis of the social structure. understanding the formation of social relationships can give us insights into the microlevel dynamics of the social network, such as how an individual user influences her/his friends through different types of social ties [cit] ], how friendships have been created across different networks [cit] b], and how a user's opinion spreads in the social network [cit] ."
"the average probability of 1xx is three times higher than that of 0xx. here x indicates any status (ether high status or low status). number of midpoints probability fig. 9 . number of midpoints correlation. y-axis: probability that relationship can be established, conditioned on number of midpoints."
"homophily. the principle of homophily [cit] suggests that users with similar characteristics (e.g., social status, age) tend to associate with each other. in particular, we study two kinds of homophilies on the twitter network: link homophily and status homophily. for the link homophily, we test whether two users who share common links (followers or followees) will have a tendency to associate with each other. figure 3 clearly shows that the probability of two users following back each other when they share common neighbors is much higher than usual. when the number of common neighbors with two-way relationships increases to 3, the likelihood of two users following back each other also triples. the effect is more pronounced when the number increases to 10. but it is worth noting that this only works for reciprocal relationships and does not hold for the parasocial relationship (as indicated in figure 3 )."
lrc. it uses the same attributes associated with each edge as features to train a logistic regression classification model [cit] and then predicts edges' labels in the test data.
"in database theory, there is a well-known duality between partial descriptions of databases (or databases with incomplete information), and conjunctive queries. likewise for us, patterns can also be viewed as basic queries: in the above example, the pattern returns pairs (x, y) of data values. viewing patterns as atomic formulas, we can close them under conjunction, disjunction, and quantification, obtaining analogs of relational conjunctive queries and their unions, for instance."
"in contrast to the previous work, we are the first to examine the applicability of lsa in query/pattern semantics and to discover semantically similar context patterns in user queries, inspired by the success of using lsa for lexical similarity estimation (20) . furthermore, compared to (21)'s single drug side-effect pattern recognition, we automatically discover bio-relational patterns related to diverse semantics of #c compared with #c, #c in combination with #c, #c #c interaction, #c induced #d, treatment of #d with #c, #d #c deficiency, dietary #c and #d, etc. simply by using bio-entities in pubmed queries as knowledge. the unsupervised nature of our framework makes it highly scalable: needing no seeds, it can easily be extended to cover various entity types (e.g. genes) and to understand the semantics of corresponding relations (e.g. #g responsible for #d where #g denotes genes)."
we propose to address the problem of finding semantically similar context patterns in an unsupervised manner by finding patterns with high distributional similarity in lsalearned latent topics. figure 1 outlines the procedure to transform pubmed queries into patterns in entity space and lsa space for this purpose. algorithm 1 shows the corresponding steps. note that we consider sip unsupervised in that sip does not require any training/seed data for pattern semantics understanding.
"suppose now that every pair of nodes in a network exchanges a message using the shortest path between them, and that we measure the performance of the network by the fraction of successfully transmitted messages, that is, the total throughput of the network. betweenness centrality can equivalently be thought of as the fraction of all messages sent through the network that pass through a given node. the removal of this node will cause these transmissions to fail, until the node's removal is detected and the messages can be rerouted. betweenness, thus, measures the contribution of each node to the total throughput of a network where traffic between all pairs of nodes is approximately uniform."
"using the same strict-match criterion and figure configuration in figure 3, figure 4 summarizes the results on figure 3 . system performance on the cc task with different lsa topic numbers (10-150) and different numbers of the most frequent entity pairs (500-3000). strict match is required. the solid line represents best-performing sip while the dotted line represents the baseline. discovering semantically similar chemical-disease semantic relations. similar to the cc task, sip generally benefited from more entity pairs in the cd task and the 500-1000 entity pair increase led to sip's largest margin of improvement. again sip significantly outperformed the baseline by a large margin. one thing worth mentioning is that, compared to the cc task, both sip and the baseline yielded lower performance: while sip dropped from a mrr of 0.86 to 0.73 and a ndcg of 0.87 to 0.74, the baseline drastically dropped to a mrr of 0.28 and a ndcg of 0.28. this is mainly because our cd task contained a broader spectrum of semantic contexts/relations (i.e. the chemical-disease relations in pubmed queries were more diverse)."
"after summarizing some important background material on centrality and zero-sum game theory in section ii, we state the problem under consideration in section iii. in section iv, we introduce a linear-time centralized algorithm to compute saddle-point equilibrium strategies for both the attacker and defender in the case of single-node attacks and multiple-node defense. these algorithms provide a network administrator with the probabilities that each node should be protected. we then extend the results for single-node attacks to the distributed setting in section v, using a finite-time distributed averaging algorithm to compute the same result as in the centralized case. finally, when multiple nodes are attacked simultaneously, we propose a centralized approximation algorithm in section vi based on a sequential simplification of the attack strategies, which performs well in a pair of simulation studies."
"1) a strategy x is a best response to y if there is no other available strategy that will improve the payoff when played against strategy y. 2) a strategy is strictly dominated if it is not a best response to any strategy of the opponent. 3) a nash equilibrium is a state of a game in which both players are playing best responses to each other's strategy. nash equilibria of zero-sum games are commonly referred to as saddle-point equilibria since the players share a common utility function. in the rock-scissors-paper example, each pure strategy is a best-response to a different pure strategy, but there is no pair of pure strategies that are best responses to each other. however, there is a saddle-point equilibrium in mixed strategies, when both players play each of the three strategies with one-third probability. we can express this by the mixed-strategy pair y"
"1) as duplication information turn up in the former items of pointer table, the first step is combine the pointer table whose subsequence node of former items are same as one item."
"to answer this question, it is necessary to know the importance of each node or link to the network. as a result, the use of centrality measures, first introduced to study the most influential people in social networks [cit], is starting to gain traction in the communication and control literature. for example, [cit] studied the effects of coordinated attacks to wireless mesh networks and showed that targeting nodes with the highest betweenness centrality results in a more effective attack than targeting nodes with the highest degree, a result supported by earlier studies on the attack vulnerability of complex networks [cit] . however, this strategy might easily be predicted and thwarted by a smart defender. game theory is ideally suited to such competitive settings and for this reason has become widely adopted in the study of network security [cit] . despite these emerging research trends, the use of centrality measures toward the game-theoretic solution of practical network security problems remains largely unexplored."
"in fact, two-player games with finite strategies always admit a unique saddle-point equilibrium in mixed strategies [cit] . this result illustrates another key concept in game theory, which is the principle of indifference, the idea being that if i am playing a saddle-point equilibrium mixed strategy, my opponent can pick from several strategies without seeing any change in payoff, and is, thus, indifferent to which strategy to play."
we begin with a brief review of two subjects that are fundamental to understanding the problem and solution approach: node centrality and zero-sum game theory.
"it is quite difficult to derive analytical measures for how closely algorithm 3 approximates the saddle-point equilibrium defense strategy in general, but we can test it on a model of a fig. 2 . results of algorithm 3 applied to the ucsb meshnet data from a particular time instant. the shading of the nodes is proportional to the adjacent numbers which are the probabilities that each node will be defended as part of a three-node mixed defense strategy against a three-node attack."
"if multiple performance metrics are of concern, for example, throughput and latency, these can be weighted and combined into a single performance metric to be used in the security problem, provided that each constituent metric can be decomposed into individual node contributions as described above."
"remark 2: in general, it may not be known exactly how many nodes will be attacked. if this is the case, the following algorithm can be run several times for different numbers of attack nodes and the results can be weighted based on a probability distribution on the expected number of nodes that will be attacked."
"as table 12 shows, the above pattern-matching approach assisted by sip output outperforms co-occurrence baselines relatively by 47 and 10% where co-occurring chemical-disease pairs in abstracts and sentences are proposed as cid candidates. we believe that, without the computational overhead of stemming and machine learning/training, such approach can be the first step to help accelerate biocuration and that its performance in relation extraction can be further improved if incorporated more cid patterns and/or co-developed with machine learning techniques."
"the experimental results indicated that when the size of chord ring is small (such as the first group of experiments), basic chord algorithm, chord algorithm improved by other researchers and nrfchord algorithm designed by us in this paper has little difference on searching hop count, as shown in table 2 . that is because when the size of chord network is smaller, the redundancy of the pointer table entry is not large and the effect of small-world is not obvious. table 2 also shows a phenomenon: when the network size increasing, but the scale is not big, the average number of query hops in chord algorithm and nrfchord algorithm, that is because the network size is not big at present, the number of actual nodes is not more, the pointer table information which increased with the m value increased reduce the number of query jump in chord. after completing the three groups experiment, we can know by analyzing the experimental data that when the network size is small, the needing hop count of query in chord algorithm and nrfchord algorithm has small difference, that is because when the number of node is small, the small-world phenomenon is not obvious, the content of pointer table maintained by the two algorithms is much the same. with the network size increasing, small-world phenomenon has appeared gradually, the redundancy of pointer table increased, the advantage of nrfchord algorithm on query hop count has emerged."
"a s society grows more reliant upon networked and cyber-physical systems for communication, transportation, sensing, control, and other applications, these systems occupy larger and more complex networks and are, thus, increasingly vulnerable to attack by malicious adversaries. with consequences ranging from costly inefficiencies to catastrophic failures, it is critical to understand how to secure these networks against such attacks. the field of research related to the security of such systems has consequently seen rapid growth in recent years [cit], and there are many angles from which to approach these problems. the first line of defense against attacks is to make the network as secure and resilient as possible, from physical and cybersecurity at the node level to robustness and redundancy at the network level [cit] . nevertheless, there will always be some vulnerabilities in large networks, and resource limitations may prevent every node in the network from being defended to the fullest extent. the question then becomes how to optimally distribute limited defense resources around the network to achieve some security objective."
"step 4a and 4b), thus understanding the semantic relations between entities. the wisdom of search crowds and searchers' perception, encoded in search queries, are also valued in (26, 27), and our experiments in experiments section suggest user entity pairs in web queries serve as good knowledge to capture query/pattern semantics."
"the key insight behind our approach is that the expected payoff resulting from a mixed attack strategy is monotone in the addition of pure strategies or nodes. in other words, when choosing between two nodes to add to the support of a mixed attack strategy, it is always better to choose the node with the higher centrality. this means we can incrementally construct a mixed defense strategy by adding support nodes in order of decreasing centrality until all remaining pure strategies are dominated. this approach is made possible by the following propositions."
"in the experiments, we compared sip with our baseline to highlight the importance of lsa space transformation. we did not directly compare our method with other pattern recognition methods such as (21) that are based on entity co-occurrence at either sentence or abstract level because our goal was to discover information needs (relations) that are frequently sought by pubmed users. additionally, sip was designed to work in query space where semantics (in queries) tend to be clear and specific, and entities and relations (in queries) tend to bond in proximity, which may not hold for some entity relations in literature space. finally, sip was designed to discover semantically similar patterns without supervision. that is, no training/seed data are required for the purpose of pattern semantics understanding. therefore, it is not straightforward to compare sip with traditional methods which require and start with entity seeds describing the pattern (e.g. (21) leverages chemical-disease entity seeds having chemical-induced-disease relation to recognize the said relation). nonetheless, when examining the output of (21), we observed complementary results for the chemical-cause-disease relation. for instance, sip had query-specific #d from #c but missed #d during #c."
"2) is p a fixed value? if we solve the problem 1), that is to say, we could find a p value, with this value to select remote node, then we could get a small-world network. but, as is known to all, the character of p2p network is high dynamic, if we add or reduce node frequently, then the optimal value p now may not be the optimal value the next moment."
"strict match. a pattern pair is considered to be strictmatch if, in biomedical context, its patterns are semantically the same (e.g. #c induce #d and #d due to #c) or highly similar (e.g. #d child #c and pediatric #d #c). relaxed match. a pattern pair is considered to be relaxed-match if, in biomedical context, its patterns are semantically related and one of its patterns entails or contextually subsumes the other. for example, #c reduce #c and #c effect on #c are relaxed-match semantically similar patterns since #c reduce #c entails #c effect on #c, whereas #c induce #d and #c induce #d in rat are relaxed-match since #c induce #d subsumes the contexts of #c induce #d in rat (the same applies to #c induce #d and #c induce #d treatment). no match. a pattern pair is considered to be no-match if it is neither one of the above."
"remark 1: in the centralized algorithm, nodes are added to the support set one at a time, whereas in the decentralized algorithm, multiple nodes can drop out simultaneously. the result is that when there are many nodes in the support set, the decentralized algorithm is likely to terminate in fewer iterations than the centralized algorithm, and indeed that is what we observe in simulation. however, since the decentralized algorithm includes an extra averaging computation for k, it will generally lead to longer total computation times."
"in this paper, we refer to sip as an unsupervised framework because it requires no specific manually annotated seeds or training data for pattern semantics analysis. although the open-source entity recognition tools (i.e. (23) (24) (25) ) used in step 1 need entity annotations, such annotations and these tools are not designed and re-trained for the purpose of discovering context patterns with similar meaning, and entity recognition can always be achieved by less-satisfying dictionary methods."
"encouragingly, sip with small topic numbers significantly outperformed the baseline which tends not to benefit from using more entity pairs either. sip achieved the highest mrr score of 0.86 and the highest ndcg score of 0.87 when as few as 20 lsa topics were used with 3000 entity pairs. and a mrr and ndcg above 0.85 indicate that the first-ranked candidate pairs were almost always correct."
"following the strategy to rebuild the table pointer, the redundant information of the pointer table in figure 8 will be eliminated, while add the appropriate number of remote node information, through the new algorithm it obtained the pointer of the node 8 of the table which is shown in figure 10 : figure 10 pointer table which node 8 maintains in new algorithm"
"indicates the reference exists, 0 otherwise. our cc/cd task has its own m, ensuring subsequent lsa transformation and semantically similar pattern finding are confined to a specific entity type pair. table 3 shows sample m for cc task while table 4 shows the m for cd task. as we can see in these two sample m's, the contextual entity pairs (reflected by zeros and ones) coarsely categorize the patterns into upper-left and bottom-right groups. this is genuinely how sip learns to discern pattern semantics. learning pattern semantics by patterns' specific participating entities, however, come with issues of data sparseness and specificity: a certain entity pair could only be mentioned in a handful of patterns, and entities may be topically-related (e.g. carcinoma and tumor are related to cancer, malignant melanoma to skin cancer, simvastatin to statin and simvastatin to lovastatin). therefore, we further transform entity space into latent topic space to avoid these issues (step 4b). specifically, we leverage lsa (31) to learn entity pairs' semantic topics and to reduce dimensionality from the number of distinct entity pairs (j) to the number of distinct lsa semantic topics (t) where t ( j. this equates to transforming pattern representations in entity space, m, into pattern representations in lsa topic space, m 0 ."
"first, matrix sim i Ã¢ i is initialized to record (semantic) similarity scores between patterns and list i Ã¢ n to store each pattern's top-scored n patterns in similarity. similar to space transformations, finding candidates of semantically similar patterns is done independently from one entity type pair to another. as a result, the similarity calculation of chemical-chemical patterns does not concern that of chemical-disease patterns, and i refers to the number of the unique patterns in our cc task or that in our cd task."
t denote the quantities we wish to average. the finite-time averaging procedure begins by computing up to n iterations using a standard distributed averaging algorithm
"however, one can approximate the solution to the problem while greatly reducing the computation. the key observation is that similar to the single-node attack case, it is likely that in most practical networks, only a few of the large number of possible attack combinations will play a role in the equilibrium solution. if we could somehow identify the most effective attacks without computing the entire payoff matrix, we could save a substantial amount of computation. the method we propose in the next section computes the most effective sequences of attacks in order to predict the most effective parallel attack strategies from the full payoff matrix."
"nowadays, the service discovery algorithms for mobile application mainly are concentrated indexing algorithm, directory server mechanism. this algorithm has single point of failure problem, namely, the directory server will become to the bottleneck of the whole system. once the central server goes wrong, it will lead to the collapse of the whole system. unstructured algorithm takes gnutella as represent which used flooding or class flooding algorithm. each user message will be broadcasted to give a number of other users linked directly. after these users received message, they broadcast equally the message to give their connected users respectively, and so on, until the request is answered or message ttl (time to live) value is reduced to 0. the algorithm is unreliable and having big consumption of network resource, low security, easy to spread a lot of junk files and virus. the typical representative of structure algorithm is dht (distributed hash table) [cit] data structure, and they decide the way of maintenance hash table in network nodes through different algorithms [cit] ."
"with the node addition or reduction we compute the optimal p, that is to say, p is alternative constantly. and then you can imagine, in this case, the maintenance of pointer tables are complex. the computing mission of nodes may increase a lot. updating of pointer tables may appears to be more complicated than before."
"today's search engines typically treat natural language queries as lists of terms and retrieve documents containing those terms. however, documents with different words but similar semantics may be overlooked. take the search engine in biomedical domain, pubmed (1), for example. semantically similar as the queries chlorthalidone vs hydrochlorothiazide and chlorthalidone versus hydrochlorothiazide are, pubmed returns 2.5 times more relevant articles when users compare these two drugs using versus than using vs. such performance difference in retrieval effectiveness may be reduced and/or the levels of user satisfaction may be maintained if queries of similar semantic meaning were presented at search time. in this regard, this paper learns to discover semantic relations between bio-concepts (such as chemicals and diseases) on the web for possible help of biocuration and retrieval effectiveness. specifically, this paper aims to identify semantically similar context words (like the vs and versus example), referred to as context patterns thereafter, in pubmed queries that assert specific relations between two entities. we focus on semantically understanding pubmed queries with exactly two bio-entities as bio-nlp research in entity relations has long focused on relations between dual entities: chemical-disease relations (2), protein-protein interaction (3), gene events (4), drug-drug interaction (5) and disease co-morbidities (6) ."
"in concentrated indexing algorithm, the using mechanism is based on the directory server. this algorithm has a single point of failure problem, namely, directory server will become to the bottleneck of the whole p2p system. once the directory server has problems, it will lead to the collapse of the entire system. unstructured algorithm adopts the way of flooding forwarding. the algorithm is unreliable and having big consumption of network resource. with the expansion of network size, positioning the peering point by diffusion and query information method will be result in dramatic increase in network traffic, causing network congestion. therefore, the network scalability is not good and don't suitable for large networks. in addition, the security is not high and vulnerability to malicious attacks, such as sending spam check information by attackers will lead to network congestion and so on."
"similar to many bio-nlp challenge tasks such as chemical-disease relation extraction (2), protein-protein interaction extraction (3), drug-drug interaction extraction (5) and identification of gene events (4) and disease co-morbidities (6), sip focuses on two-argument, dual-entity, relations. in this subsection, we examine sip applicability in a real-life relation extraction problem and compare sip table 9 . pubmed responses to query submission (a) albuterol vs levalbuterol and (b) albuterol vs levalbuterol or albuterol versus levalbuterol where (a) is the original user query while (b) is (a)'s new query expanded using sip pattern knowledge effectiveness in helping biocuration with simple co-occurrence method. [cit] biocreative chemical-disease relation extraction subtask (2): extraction of chemicalinduced-disease (cid) relations. [cit] official development set, consisting of 500 pubmed abstracts and extract chemical-induced-disease (cid) relations in a number of steps. first, starting with a representative cid pattern #c induce #d (words are stemmed), we consolidate its sip strict-match patterns from different settings. this process examines newly-discovered patterns and adds their synonymous patterns iteratively. in total, we collect 24 sip context patterns associated with the cid relation including #c cause #d, #d due to #c, #d associate with #c, #d cause by #c and #c and the risk of #d. second, for any chemical-disease pair in a pubmed abstract, we extract its context (or contextual words) in the abstract and manage to best match the contextual words to a sip pattern out of the 24, if any. table 11 shows example pubmed contextual words surrounding chemical-disease pairs and contextual words' best-matched sip cid patterns. note that in this step we require the chemical-disease pair to appear in the same sentence. finally, we consider the chemical-disease pairs whose pubmed contextual words have matched our sip patterns to be candidates having cid relation."
"in order to know how many nodes have dropped out after each iteration, the nodes must also be able to compute the new value of k. to update k, the nodes simultaneously compute k n using distributed averaging, after initially setting Îº i to 0 or 1 depending on whether they have dropped out of the support at a given iteration."
"in the basic chord algorithm, the scope which the node pointer table maintains greatly reaches the half chord ring. the maintained node in the pointer table is regarded as the neighboring node. if we can choose the suitable nodes that do not maintain in the other half chord link from the pointer table to increase to the pointer table, the chord network will show the small-world character, thus reduces the searched hop count. the following proof theoretically first after the adding long-distance node, the searched hop count will drop."
"where the weights w ij form the matrix w . to guarantee convergence, w must have one simple eigenvalue at 1 and the rest strictly less than 1, with both left and right eigenvectors being equal to 1 n (the vector of length n containing all ones) [cit] . it is always possible to find such a matrix w since the underlying network is undirected and connected. one standard choice that meets these requirements is the following [cit] :"
"the construction process: firstly, according to the keyword idn hashing on all the adding nodes in the network, the nodes could arrange from small to big. so it builds a ring of which the address space is 0--23 -1."
"service discovery technology for mobile application allows network nodes can effectively find and use these services from other nodes. with the continuous expansion of network size, the network computers are increasing rapidly, also the demand for service and the service can be provided of each node in network. therefore, how to search available service efficiently from masses of network services for mobile application is the key of service discovery technology. the main work of most of researchers is focused on searching and storing of service resource, which makes the research on service discovery algorithm become popular [cit] ."
"3) the attacker and defender make decisions independently, without knowledge of each other's decisions. assumption 1 holds particularly true in the case of physical attacks to a network. when security is simply a matter of implementing code, resource constraints would most likely not be the key limiting factor for network security. the second assumption will hold for networks in which speed is critical or recovery is slow. although the problem is still solvable if the network is allowed to reroute traffic, it becomes significantly more complex, and we leave these dynamic extensions for future research. the third assumption establishes that this is not a sequential game but a simultaneous game; in fact, the sequential game is less interesting in this case, because the choices for either the attacker or defender become quite clear once the opposing player has acted. mixed strategies, therefore, capture the players' ability to independently randomize their choices. since a worst-case attacker wants to minimize the same performance metric that the defender wants to maximize, this problem fits well into the setting of a two-player zero-sum game. in fact, we can already express the problem in general form as the following minimax problem:"
"our sip framework is unique as it targets biomedical queries, gaining importance in web searches and biomedical research (1, 7) . second, sip leverages search crowds' wisdom (i.e. user entities in web queries) to discern context patterns' semantics and estimate patterns' semantic similarity. this makes sip unsurpervised requiring no training/seed data for related pattern discovery. third, sip serves as one of the pioneering work to analyze pattern semantics based on realworld user queries in either nlp or bio-nlp community. last but not least, sip exploits lsa to project entities in queries into lower-dimension latent topics, avoiding specificity in entity mentions, and sip transforms the problem of finding semantically similar patterns into one of finding patterns with distributional similarity in lsa topics."
"in this section, we apply sip strict-match pattern pairs to two specific biomedical tasks: biomedical document retrieval and bio-entity relation extraction (see figure 1) . we show that sip output can benefit the process of biocuration and semantic information retrieval (ir)."
"lsa constructs the t-topic semantic space by a number of steps, namely, performing rank-reduced singular value decomposition on the matrix in entity space, retaining t largest (significant) singular values and approximating the matrix in the least-squares sense. finally, a lower-dimension i-by-t matrix approximation (m 0 ) to the original i-by-j matrix (m) is learned in an attempt to model pattern semantics in terms of t lsa topics. note that although similar method such as probabilistic lsa (plsa) (32) could also be used for rank reduction, plsa does not outperform lsa in both our tasks."
"where bracketed sets of paths indicate that there are multiple shortest paths connecting two nodes. the betweenness of each node is obtained by counting the total number of shortest paths in which a node appears, dividing by k when the path is one of k shortest paths connecting two nodes. finally, we divide by the total number of node pairs in the network so that the result lies between zero and one. checking each path in (1), we see that node b lies on 1 + 1 + 1 + ( 5 out of the ten node pairs, or 0.75 of the shortest paths in the network, supporting the intuitive observation that b is an important node in this network. note that this definition of betweenness differs slightly from standard betweenness definitions because we count being an endpoint of a path as being on the path while standard definitions typically do not. the reason is that for our purposes, the endpoint nodes are just as important to the communication link as any intermediate node. this also prevents nodes from having a betweenness of zero. our methods still apply if some nodes have centrality zero, but the result is trivially that these nodes should never be attacked or defended since they are negligible to the network with respect to the corresponding performance metric. we do restrict the centrality measures to take non-negative values, but this is quite a mild assumption since the vast majority of standard centrality measures is non-negative by definition."
"we present a novel unsupervised framework, sip (semantically similar pattern finder), that discovers twoargument context patterns that are semantically similar but lexically different. table 1 shows example sip discovery of synonymous context patterns associated with semantic bio-relations involving chemicals/drugs (denoted as #c) and diseases (denoted as #d). sip leverages the semantic information of biological entities in web queries to differentiate pattern semantics, based on observations that semantically similar patterns such as #c induced #d and #d due to #c share significantly more chemical and disease pairs among web queries than patterns like #c induced #d and treatment of #d with #c which are not semantically similar. intuitively, sip estimates patterns' semantic similarity by their distributional similarity, whether their distributional contexts are participating entities or semantic topics. in specific, the sip framework discovers patterns of similar semantics in three main steps. first, it determines patterns' participating entities which constitute entity space. next, sip transforms entity space into latent topic space for pattern semantics analysis/understanding. it learns the transformation by analyzing pubmed queries using latent semantic analysis (lsa). finally, sip yields pattern pairs with high distributional similarity in lsa topics and proposes them as semantically similar patterns."
"after removing the redundant items, we compute and select corresponding items from another area of finger table which is un-lapped, and add to the rear of basic finger table. thus the finger table could cover the whole chord ring, and improve the availability of finger table. and thus increase the search speed. the selected node from the other half area may posses the feature of smallworld network."
"in this context, a collective mixed strategy can be interpreted in multiple ways. in the first interpretation, a mixed strategy prescribes probabilities that each node should be protected, but since nodes make these decisions independently, it may happen that more than Î´ nodes request protection, which is a violation of the resource constraint implied by the problem statement. however, since the game is likely to be repeated many times, it is reasonable to relax this constraint to a limit on the average amount of resources consumed per iteration over a large number of repeated games. since this is a game of complete information, the extension to repeated games has no effect on the saddle-point equilibrium and the resulting expected payoff of a long-term distributed implementation of a given strategy is equivalent to the one-shot expected payoff in the centralized case. as an alternative interpretation, one could assume a linear relationship between defense resources applied and success probability of the defense. that is, a node that is defended with half resources has a one-half probability of successfully defending an attack and similarly for arbitrary fractions of resources. the result is again mathematically equivalent to the centralized case, although it can be computed in a distributed way."
"in the case that traffic varies significantly between node pairs, one can still define the importance of a node as the fraction of all flow from source to destination that passes through a given node. this is a kind of flow centrality. other notable centrality measures include degree centrality, which corresponds to the fraction of all links in the network that connect to a given node, and closeness centrality, which is the average distance of a node to all other nodes in the network and is related to how long it takes to broadcast a message to the entire network, also referred to as the efficiency of a network."
"3) successor(idk): the successor of resource idk, the first node which idn is greater than or equal to the keyword; 4) finger table: pointer table. it is pointer table storing in each node for maintaining m items of node information, the details described in table 3-1; 5) successor(idk) (successor node); 6)predecessor(idk) (predecessor node): refers to the first node of this value's successor in chord ring by counter-clockwise, used to update the pointer table when the node joins in the network."
"in accord with network's topology at present, proposed the application of the small-world theory in the network is necessary [cit] . we usually discuss the network topology mainly has two kinds: regular and random. in the regular network, the connected number of each node and the other node is basically consistent, the way average length l between nodes in the network is also basically consistent, l~n/(2k), n is the network node number, k is the connection number of each node and other node, l expands along with the network size expansion. while in the random network, the network way average length l is quite small. some scholar carried out the change to the regular network in the experiment, chose certain nodes that did not to connect in this node from some nodes by probability p carry on the connection, then obtained small-world network. in fact, in the real life many networks are the small-world networks, like social personal relation network, worm's neural network, electric power supplied network and so on, and present internet is also \"small-world\" network. after the reconnection by the choice of probability p, the nodes maintains quite high connectivity, like the regular network, but the network average path length l became small, and the random network is similar, namely in the network any node that only needed through the few in number several nodes will be possible to arrive at the goal node."
"we constructed our test set semi-automatically in two steps. we first ordered pubmed context patterns according to their frequency and the diversity of their participating entity pairs in our query log. we then manually examined the top-ranked patterns and considered a pattern suitable for testing if it is a common, general biomedical pattern (in contrast to specific ones such as #c oxidase #c and #c transporter #c) and it should not be ambiguous about entity relations. our final test set consisted of 68 chemicalchemical and 120 chemical-disease testing patterns (see table 6 for examples). for each of these patterns, we performed the evaluation on the list of top-ranked similar patterns returned by sip."
"system settings for sip we evaluated sip framework on different numbers of lsa topics: 10, 20, 40, 60, 80, 100, 150, 200 and 300. we started with a small topic number of 10 and increased the number faster to 300 because of the fact that 300 6 100 topics have been used to analyze lexical semantics of general documents (33) and that, compared to full-text general documents, we had a much smaller and constrained vocabulary. on the other hand, to avoid possible noise in web queries, we restricted sip to the most frequent 500, 1000, 1500, 2000, 2500 and 3000 chemical-chemical/ chemical-disease entity pairs in pubmed queries when constructing cc/cd task's entity space in figure 1 ."
"since discovering relaxed-match patterns can also be beneficial, we further examined system performance with both strict-and relaxed-match patterns allowed. figure 5 reports corresponding ndcg results on our cc and cd tasks. as expected, sip gained from relaxing the matching criterion and achieved an improved performance of ndcg closer to 0.9 and 0.85 in semantically understanding the chemical-chemical and chemical-disease patterns, respectively."
"the main contributions of this paper are fast algorithms for allocating defense resources to the nodes of a network with the goal of maximizing a centrality-based performance metric in the presence of attacks. for single-node attacks, we presented both centralized and distributed algorithms to compute the saddle-point equilibrium defense policies against worst-case attacks. the case of simultaneous attacks to multiple nodes is computationally complex, so we proposed a centralized approximation algorithm based on a sequential simplification. simulations demonstrated that this approach very closely approximates the equilibrium strategies for a variety of cases at greatly reduced computation. interesting directions for future research include extending the results to the case when links are attacked in place of or in addition to nodes, developing distributed solutions to the multiple-node attack case, and investigating the effect of adversarial nodes in the distributed case."
"2) according to computing method of the flow chart of algorithm is shown in figure 9 as followed: figure 9 flowchart of node's joining in chord therefore, the node joins the network, nrfchord network will complete the following five tasks: 1) initialize the predecessor of the newly added node n and the finger table. 2) update other nodes of the network's predecessor and finger table. 3) tell the successor nodes of the node n, sending idk information which be responsible by the n index sent to n. 4) remove redundant pointer table entry of the node. 5) add m-logn remote links to the end of the pointer table."
"in the light of the problems as mentioned above, our task decide not to use the basic method of selecting remote node with p. thus, we analyze the pointer table, and eliminate the redundancy of pointer table. we remain use the formula of pointer table construction approach to add new table item in the tail of tables. in the observation of finger table, we could find out that the skip span of former item node is little, and this could lead to the existence of redundant section. thus, the repetition of table items is mostly in the former of finger table. considering remove the redundant items in finger table, and add new items in the rear of it."
"the algorithm flow of the three steps that chord node n join in the network is shown in figure 5 : figure 5 flow sheet of node joining if a node n wants to join the chord network, firstly it detected whether it exists guiding node n' by broadcasting. if n' exists, use n' to initialize your own pointer table, at this time it needs to call init_finger_table(n') function, at this point it has achieved the task 1) of the above three tasks. after initialized there own pointer table, call update_others() function to update other node's pointer table, at this point it has achieved task 2) of the above three tasks. at last, contact the successor of n successor(n), give the keyword resources that n should be responsible in successor(n) to n to maintain, at this point it has achieved task 3) of the above three tasks. if n is the first node to join the network, it only needs to set set the successor information and predecessor of its own pointer table to itself."
"nowadays, the way to construct small-world networks is mainly the mode of selecting remote node connection with probability p [cit] . for instance, in chord network, in order to increase remote nodes, we need to build two pointer tables: neighbor pointer table 1 . when building the remote pointer table, we need to select the remote node with probability p, that is to say, the node which does not in the neighbor pointer table will selected as remote node of certain node with probability p."
"when node n receive a request to query a resource's idk, firstly checking whether the idk equal to idn or belongs to (idn, successor (n)) on this node, if it belongs to, returning information to the initiate requesting query node, else transmitting the query message to successor (n), repeating this process until they successful search or the query message sent back to the launching query node(failed search)."
"once context patterns are semantically recognized in lsa space as m 0 i Ã¢ t, instead of their lexical forms, sip estimates patterns' semantic similarity by their distributional similarity in lsa latent topics. sip proposes semantically similar candidate patterns using the procedure in figure 2 . algorithm 2 shows the detailed steps."
"where y and z are mixed strategies of the attacker and defender, respectively, a is a payoff matrix whose entries represent the performance change in the network resulting from each pair of pure attack and defense strategies, and v * is the saddle-point equilibrium value of the game. we denote by Î´ a and Î´ d the mixed-strategy simplexes over pure strategy attack and defense sets a and d, which depend on the number of nodes attacked (Î±) and defended (Î´) as described in the following sections. finally, we say that a pure strategy is contained in the support of a mixed strategy if the corresponding element of the mixed strategy is strictly positive."
"we first consider the problem from the perspective of a network administrator who has access to full information about the network. problem (2) is in a standard form that admits an lp solution, and although there are algorithms that can solve lp problems in polynomial time [cit], this can still pose a computational burden for very large networks. moreover, the fact that payoff matrices grow combinatorially with the number of attack and defense nodes provides even stronger motivation to develop faster solution algorithms. we present here a simple and linear-time algorithm to compute the saddle-point equilibrium strategies in the case of single-node attacks."
"many natural language queries are submitted to search engines on the web every day, and an increasing number of online search engines target domain-specific search services. for example, yelp (www.yelp.com) facilitates restaurant searching while pubmed (www.ncbi.nlm.nih.gov/ pubmed) retrieves scholarly publications in biomedicine."
"for both interpretations, the objective is a distributed method for arriving at the solution of (2) . we see in algorithm 1 that the quantities each node needs to compute are"
"with the following theorem, we show that the computational complexity of algorithm 1 is linear in the number of nodes in the network. for comparison purposes, polynomial-time algorithms for lp have worst-case computational complexity in the neighborhood of o(n 3.5 ) [cit] . the reduction in complexity is primarily a result of the monotonicity of the expected payoffs in the pure attack strategies. we also note that the computation of the algorithm itself is likely to run faster than computing and sorting the centrality measures. proof: the inside of each loop in algorithm 1 contains a fixed number of operations and is repeated a maximum of n times resulting in a total number of operations that is in the worst case o(n)."
"the performance of sip on finding strict-match chemicalchemical patterns (i.e. the strict-match cc task) is summarized in figure 3 . in this figure, histograms represent the (a) mrr and (b) ndcg performance of different sip settings concerning the lsa topics and the most frequent entity pairs, and colors are used to differentiate lsa topic numbers. for instance, green bars, labelled as t60, denote the sip performance when set with 60 lsa topics. and 60-topic sip (i.e. green bars) performed differently when accompanied with different numbers of frequent entity pairs (i.e. 500, 1000, 1500, 2000, 2500, 3000): 60-topic sip achieved around 0.4 mrr using 500 frequent entity pairs but achieved around 0.8 mrr using 3000. the results of t200 and t300 are omitted as system performance degraded drastically after t100 (i.e. t150, t200 and t300). figure 3 also plots sip's best performance (i.e. the solid lines) with respect to each number of entity pairs used. [cit] frequent entity pairs, sip achieved the best 0.81 mrr with 20 lsa topics, thus t20 0.81 labelled. for comparison, the dotted lines represent the performance of our baseline, which simply estimated patterns' semantic similarity by the cosine similarity of their specific participating entity pairs in the queries without using lsa topic information. in other words, our baseline is basically sip framework excluding the component of latent semantic analysis (i.e."
"in the field of biology and life sciences where entities have abundant alias, retrieving documents containing the exact user search words may not be sufficient. as a result, pubmed (1) uses medical subject headings (i.e. mesh terms) expansion by default (9) and searches for query words not only in documents but also associated mesh headings. by doing so, pubmed alleviates the issue of biomedical term mismatch between document words and query words. take the query albuterol for example. pubmed will return documents containing albuterol and documents without albuterol but (annotated) with the same mesh heading as albuterol. thus, documents not containing albuterol but containing the synonyms of albuterol such as proventil, salbutamol and ventolin, will also be returned. nonetheless, since general terms/phrases are out of the scope of mesh headings, pubmed can still suffer from general-purpose vocabulary mismatch during search. consider a real user query albuterol vs levalbuterol. pubmed's retrieval effectiveness could be improved if pubmed semantically understands the query by exploiting sip synonymous pattern pairs, (#c vs #c, #c versus #c) in this case, and returns the accumulated search results from both the original query albuterol vs levalbuterol and sipmotivated counterpart albuterol versus levalbuterol (see table 9 ). as shown, pubmed retrieves relatively 118% more documents for the new query (35 documents vs 16 documents). in addition, examining the retrieved pubmed titles shows that with sip's query expansion, albuterol versus levalbuterol for albuterol vs levalbuterol, one can obtain relatively 100% more relevant documents (22 vs 11) in this case. retrieving more relevant documents is essential to biocuration, semantic ir, and article triaging of many biomedical shared challenges (36) ."
"we now turn our attention to the case in which there is no central administrator and the nodes must come to an agreement on the best defense strategy while communicating only with neighbors. in this case, the collection of nodes can be considered as one distributed player whose objective is to implement the saddle-point equilibrium defense policy through coordination of individual decisions. here, we require that the network performance metric be based on a centrality measure that can be computed in a distributed way. this is trivially true of strictly local measures such as degree centrality and flow centrality (local throughput), but it also turns out to be possible for betweenness and closeness centrality, though the communication costs for these will be higher [cit] . since we are not aware of a distributed method for computing the centrality of node sets, we must also restrict our focus in the distributed case to singlenode attacks. finally, we restrict our attention in this section to undirected connected networks."
"the solid point in the ring shows there is a specific corresponding node. in figure 1, the nodes adding to the network is 0,1,3 and the keywords of resource is 1,2,4,5. keyword value of resource idk is assigned to the ring's specific node which equal to idk or is the first encountered in clockwise. the node is called successor node of idk and can be expressed as successor(k). that is to say, successor(k) is starting from the node that equal to idk or less than idk in the ring of which address space is 0-23 -1 and arranged to the first node encountered after this node in clockwise. as shown in"
"the simulation experiments show that nrfchord algorithm designed by us in this paper can effectively reduce the average query path length. it can be known by simulation experiments and theory analysis, the main reasons of reducing the query path length are: 1) increase the pointer table, when the node is finding resource, it can query directly in its own machine's finger table, the using time is smaller than query simply in chord ring without finger table. through pointer table, it can across directly many nodes to reach the target node or the node closer to target node. 2)we have our finger table, the finger table in chord algorithm only can cover half of the whole ring, after our, remove the redundancy of finger table, increase the node information that is not cover the other half ring before. in the query, the calculation volume of this node became bigger, but the query time reduced."
"curating relationships between biological entities and concepts is an active task carried out by many groups such as ctd (gene-disease-chemical) (10), biogrid (proteinprotein interaction) (11) and pharmagkb (drug-gene) (12) . the proposed work could potentially contribute to improved curation quality and productivity in two main ways: a) our discovered patterns could be directly used by curators to locate relevant papers more effectively (i.e. with better coverage and precision) in their routine literature search; and b) our patterns could be integrated into automated textmining systems for assisting relation curation. semantic search, or searching with semantics, has been an area of active research for improving keyword-based retrieval systems by taking semantics into account. semantics of the documents to be searched or semantics of the search terms may be leveraged in the process. in biomedicine, understanding the semantics of user queries has received much attention since (13, 14) . for instance, (15) analyzes query length, query specificity and query clarity of trec and clef shared tasks. another interesting work (16) imposes position constraint on search terms in retrieved documents. such in-proximity constraint aims to preserve semantic relations of search terms in multi-word queries. moreover, past research has studied the effectiveness of semantically expanding queries on biological entities, concepts, or controlled vocabulary for improved retrieval performance (17, 18) . following this line of trend and term disambiguation (19), here we aim to understand the semantics of biomedical queries on a deeper level than individual concepts, but in the form of context patterns and entity relations."
"the results of our work can benefit biocuration and semantic information retrieval. for example, the automated semantically similar patterns can be used by biocurators for assisting bio-relation curation and article triaging (e.g. (8)), or can be passed on to search engines to expand search results for better recall of relevant documents (e.g. (9)). this paper focuses on discovering semantically similar patterns and its evaluation, together with its real-life applications in two use cases (see application section for more details)."
"extend to the distributed case for a range of different attack and defense scenarios. closely related to this topic are interdiction problems, which involve identifying the most critical nodes or links in a network with respect to various flow or path-distance metrics [cit] . in particular, when one party is responsible for choosing the paths and/or flows along which information or products are transported in the presence of full or partial disruptions to the nodes or links, this is commonly referred to as an interdiction game [cit] . typically, interdiction games are multistage, with players acting in turn based on observed changes to the network. here, we assume that the attacker knows only the topology of the network and not the level of defense at each node. conversely, the nodes of the network do not know where the attacker will target, so we effectively have simultaneous play between the attacker and defender. this scenario is both highly plausible and, in many cases, allows for relatively fast computation of the solution, as we will show later."
"basic chord algorithm is a search algorithm which based on dht (distributed hash table) [cit] . in chord algorithm, network construction, node adding and resource discovery need to use a hash function for processing nodes and resource information. the hash function can transform the target (node information, resource information) into a fixed length output. hash function h(x) has the following properties [cit] . 1) h(x) can act on an arbitrary length data."
"the centrality of a node can be thought of as the importance of a node in the context of the network. there are many different centrality measures related to various notions of importance. we are particularly interested in centrality measures that represent each node's contribution to a network performance metric. one notable example is betweenness, which is the fraction of all shortest paths in the network on which a node lies."
"firstly provide the relevant terms used in our method based on basic chord algorithm: 1) idk: hash value of resource information; idn: hash value of node information (ip, port);"
"for each pattern p, sip yields a set of patterns whose similarity scores are among its top n as its semantically similar candidates. at last, sets of paraphrasable pattern pairs are obtained. table 1 shows example discovery of semantically similar context patterns on our working prototype."
"the latest research results reflect in structuring network topology by using the fully distributed of distributed hash table (dht). in order to solve how to enhance the discovery efficiency and coverage, based on dht (distributed hash table) and small world theory [cit], we put forward a new algorithm of service discovery for mobile application. based on dht (distributed hash table), we put forward a new algorithm of service discovery for mobile application."
"the benefit of sip in semantic ir, alleviating vocabulary mismatch that is not covered by mesh, can also be observed in another real user query methotrexate combined with tofacitinib where sip proposes pattern #c combine with #c and #c in combination with #c are synonymous (see table 10 ). based on the first-page pubmed responses shown in table 10, pubmed clearly achieves better retrieval performance with the expanded query methotrexate combined with tofacitinib or methotrexate in combination with tofacitinib. in the near future, the applicability of sip patterns in pubmed literature search as query expansion will be examined more extensively and quantitatively."
"and in order to remove the redundancy of pointer table and build small-world network, when new node n added, chord should finish a new task, that is to check the redundancy of pointer table and remove it, and then add remote node information. when added, the strategy of selection may follow as below:"
"the first three tasks and the nodes which in the chord join network have the same way, eliminate redundancy and add the remote link by calling dele_ redundancy () and add_lf () to complete."
"as shown in figure 3, the performance of smaller topic numbers (t 80) tends to improve with increasing entity pairs and their performance becomes steady at 2500-3000 entity pairs: increasing the number of frequent entity pairs from 500 to 1000 gave mrr and ndcg the largest margin of improvement whereas increasing from 1000 to 1500 yielded the second largest. nonetheless, with larger topic numbers (t ! 150), sip did not always benefit from the entity pair increase and did not perform well."
"all 54 system settings for sip (9 different numbers of lsa topics Ã¢ 6 different numbers of frequent entity pairs) were evaluated in our cc and cd tasks. in evaluation, candidate semantically similar pattern pairs were pooled from the 54 sip alternatives and our baseline, and were manually judged for semantic similarity. as the authors concurred on each other's semantic judgement most of the time (85%) in prior-experiment analysis, only one of the authors examined the pooled results blindfolded. in total, 1687 unique pattern pairs in cc task and 3609 unique pairs in cd task were manually evaluated and annotated as:"
"we now formally state the problem that we are addressing: we are given a collection of pubmed queries ql and a context pattern p that specifies a biological relationship between two entities. our goal is to automatically discover a reasonable-sized set of patterns in ql that are semantically similar to p in biomedical search context. for this, we represent queries in ql as context patterns in entity space and project such representations into latent topic space using lsa, such that patterns' semantic similarity can be estimated by their distributional similarity among lsa latent topics and those patterns having high lsa topic similarity with p can be proposed as its paraphrases. figure 1 summarizes the workflow of our method while figure 2 elaborates on semantically similar patterns identification at run-time. detailed process is discussed in the following sections."
"suppose now that an attacker has the ability to disable one or more nodes, effectively removing them from the network, but that a limited number of nodes can be protected and, thus, immunized from attack. whether from the perspective of a central administrator or the distributed collective of nodes, the critical question becomes which nodes to protect in order to maximize a particular performance metric in the presence of such an attack. depending on the specific network application, there will be many different means of attack and potential security measures. the approach we present here is applicable to contexts in which the following assumptions hold."
"knowledge source: pubmed queries a total of six-month's worth of 35 968 309 pubmed queries (24.3 million unique queries) was collected for our experiment of pattern semantics understanding. queries with exactly two entities were stemmed, entity-tagged and re-formulated into context patterns following the procedure in figure 1 for semantically similar pattern finding in figure 2 . table 5 shows some frequent dual-entity context patterns or entity relations in pubmed queries. frequent chemical-chemical patterns cover relations of drug/chemical comparison (e.g. #c versus #c), interaction (e.g. #c and #c interaction) and so on, whereas frequent chemicaldisease patterns cover semantics of chemical-induced side effects (e.g. #c induce #d), drugs' therapeutic effects (e.g. treatment of #d with #c), etc."
"sip is designed to learn the semantics of context patterns by entities involved. although both scholarly publications and web queries provide such information (i.e. the entities that patterns keep), we prefer web queries because user queries tend to bond entities in proximity. as such, sip is trained and evaluated over web queries. in this section, we first present our pubmed query data, for discovering semantically similar entity relations or context patterns and the process to construct our test set. then, we describe the parameter settings for sip and outline the evaluation process. finally, experimental results are reported and discussed."
"we will construct small-world in the chord ring with the remote node link, which are added with the table space of external pointer [cit] . it not only improves the effect of the research, the external pointer tables are not added."
"the pointer table algorithm of initializing n involved in figure 5 achieved by function init_finger_table(). in function init_finger_table(), node n first request node n' calling function find_successor() to find the successor which finger [cit] maintains and update its predecessor and predecessor of n'. then repeatedly call function find_successor() to update pointer table from second entry to the last one. the pointer table algorithm of updating the other nodes involved in figure 5 achieved by function update_other(). when updating the other node's pointer table (such as node p), it only needs to update the nodes which affected by node n joining, namely whether node n become the first i items of node p's pointer table. the basis of judging whether the pointer table of node p replaced by n is: 1)node p is located before node n at least 2 i -1, in other words, node p is the predecessor of node (n-2 i -1); 2)in the pointer table of node p, the first i-item information is behind n. satisfy these two conditions node p by calling function update_finger_table(n, i) to update first i-item information of its own pointer table."
"the search applied the principle of the greedy algorithm: when each time transmit inquiry news, the choice closest goal node regard the next transmission inquiry news node, like this, each time receive the inquiry news node that leave the goal node to be more and more near. according to the length with the goal node, the distance between the search news node and the goal node divide into certain stages in the search process. the proof is completed. obviously, the small-world network is constructed through connecting the long-distance link in the chord network, and it is able to reduce the search hop count and enhance the search efficiency."
"in a game, two or more players choose from a set of strategies in pursuit of different and often competing objectives and receive payoffs that may depend on the strategies of all players. a twoplayer zero-sum matrix game is a game in which one player's gain is equal to the other player's loss and can be modeled using a payoff matrix where one player is the minimizer and the other is the maximizer. a familiar example is the rock-scissors-paper game, where players choose between rock (r), scissors (s), and paper (p), knowing that rock beats scissors, scissors beats paper, and paper beats rock. a typical payoff matrix is as follows:"
"in this paper, we use the pointer table that has deleted the redundancy to add the remote node to the empty pointer table, which made the network have small-world effect. due to the shortage of basic chord algorithm and chord algorithm improved by other researchers, we propose the new algorithm nrfchord based on dht and small world theory. the simulation experiments have proved that during the service discovery of mobile application, the node's needing hop count of the algorithm nrfchord has reduced compared with the former two chord algorithms, and effectively reduce the average query path length. the new algorithm can enhance the discovery efficiency and coverage. it is useful for service discovery of mobile application."
"prior proposals suggested the use of similarity between patterns estimated from separate scanning runs (cross-run rsa) [cit], in order to overcome \"pattern drift\" [cit], which can be seen as being caused by an interaction between the study design and auto-correlated fmri noise. the inner product between noise pattern estimates from separate runs is indeed theoretically unbiased. cross-run rsa was also proposed for assessing the extent to which brain patterns can discriminate between different conditions [cit] . in our simulations, crossrun rsa generally performs better than within-run rsa, but worse than brsa. however, after spatial whitening of the point estimates of activity patterns, cross-run rsa outperforms brsa at high snr, with the results of both methods very close to the true similarity structure in this case. on the other hand, at low snr, brsa performs better. the limitation of cross-run rsa is that even though the cross-run covariance matrix is unbiased, the magnitude of crossrun correlation is underestimated. additionally, the high noise in the pattern estimates can also lead to results that are difficult to interpret, such as an anti-correlation between estimated patterns of the same condition from different runs. in general, it is not guaranteed that crossrun rsa will obtain a measure of distance that satisfies the triangle inequality. in contrast, although brsa is not fully unbiased, it does guarantee that the estimated similarity matrix is positive semi-definite, and that an interpretable distance metric can be derived by subtracting the similarity matrix from 1. in sum, it is difficult to predict whether brsa or cross-run rsa with spatial whitening will be more suitable for any specific study and brain area of interest since their performance depends on snr and both methods have limitations. nonetheless, based on our results, both approaches should always be favored over traditional within-run rsa."
a web transfer consists of the request of one or more objects from the client to the server and the transfer of requested objects from the server to the client. figure 1 depicts a web session between a pair of the client and the server machines.
"the notion of what constitutes educational quality has been researched by many scholars. in the context of the developing world, educational quality presents more complex issues given the increasing access to education and the associated demands that go hand-in-hand with an increasing inflow of learners into the schooling system. the call for educational quality is therefore a serious one."
"where (c) refers to a chromosome (candidate solution), and (w) refers to the weight assigned to the factor (k), where (k) refers to any of the factors from 1 to 4 in table 3 . the weight figure 2 chromosomes' snapshots to illustrate utilization crossover logic."
"according to table 11, it is obvious that approximately 41% of the rooms available were saved instead of 19% in the old approach. forty percent (40%) of the total working hours available weekly to build the schedule were saved as well. applying uga also excluded two days for a weekend (thursdays and fridays) when building timetables and reduced the number of potential lecture periods per days to five instead of six."
"the algorithm in this paper is publicly available in the python package brain imaging analysis kit (brainiak), under the brainiak.reprsimil.brsa module. our previous version of bayesian rsa method [cit] with newly added modeling of spatial noise correlation is in the brsa class of the module. the new version described in this paper is implemented in the gbrsa class and can be applied to either a single participant or a group of participants."
"subsequently, other popular web browsers were also tested. tests were performed on a laptop equipped with windows xp operating system. four web browsers were tested: internet explorer 8, firefox 3.6, google chrome 4.1 and opera 10.51. all of these web browsers support persistent tcp connections."
"estimating shared representational similarity across participants. as mentioned above, brsa can be extended to jointly fit the data of a group of participants, thus identifying the shared representational similarity structure that best explains the data of all participants. this is achieved by searching for a single u that maximizes the joint probability of observing all participants' data (group bayesian rsa; gbrsa). the rationale of gbrsa is that it searches for the representational structure that best explains all the data. using all the data to constrain the estimation of u reduces the variance of estimation for individual participants, an inspiration from hyper-alignment [cit] and shared response model [cit] . fig 3i shows that the similarity structure recovered by gbrsa has slightly higher correlation with the true similarity structure than the average similarity structure estimated by other methods, across most of the snr levels and amounts of data. cross-run rsa with spatial whitening performs better only at the highest simulated snrs. however, low average snr is common in many brain areas and this is where (g)brsa offers more power for detecting the true but weak similarity structure."
"first, rsa on the raw activity patterns suffers from the second contributor to the bias in rsa that comes from the temporal properties of fmri noise. to understand this, consider that estimating activity pattern by averaging the raw patterns, for instance 6 sec after each event of a task condition (that is, at the approximate peak of the event-driven hrf) is equivalent to performing an alternative glm analysis with a design matrix x 6 that has delta functions 6 sec after each event. although the columns of this design matrix x 6 are orthogonal and Ã°x t 6 x 6 Ã¾ Ã  1 becomes diagonal, the bias term is still not a diagonal matrix. because of the autocorrelation structure s Ïµ in the noise, the bias term Ã°x"
"where l is the total number of multipath components including the los path when it exists, Î± l is the attenuation linked to the path loss and the reflection associated with that multipath component. in this model, we assume that the phase change comes from the delay Ï l being 2Ïf c Ï l and deterministic phase change Î¾ after the reflection, where f c is the carrier frequency. fig. 2 shows an example of ray-tracing result from the transmitter located at (1, 1, 1) m to the receiver located at (3.1, 3.2, 1) m, with the bandwidth limited to 400 mhz. the number of taps of the simulated cir is about fifteen and the cir is normalized to the strongest channel tap (fig. 2(a) )."
"finally, passive measurements were performed on an operational network of an isp with real users. we identified different browsers from the traffic traces with the help of their user agent strings. after identifying the web browsers, we carried out further analysis to extract table 2 summarizes the different termination types seen from all the experiments. the type of termination here refers to the sequence of terminating flags that were seen at the end of a tcp connection. five different types of terminations are observed. sequences of these termination flags occurred as a result of different actions as listed in table 1, performed in the web browser."
"put differently, the type of information and how that information is presented plays a significant role in the manner that the information is firstly received, and secondly, utilised or used and then implemented. [cit] includes this component of 'use' as a central concept in the way in which he theoretically articulates school performance feedback systems. here, use can be defined as the process of applying knowledge received toward either the solution of a problem or alternatively the attainment of a predetermined goal [cit] . furthermore, utilisation is thought of in terms of a continuum from direct use to mere informational purposes without resulting in actual use [cit] . utilisation in the context of this research refers to the process of applying received knowledge and information with the aim of finding a solution to a problem or the attainment of a predetermined goal [cit] . the application of the information may include direct use (instrumental use), delayed use or diffused conceptual use [cit] ."
the remainder of this paper is organized as follows. section 2 provides some background information on the web transfers and the tcp protocol. section 3 describes the related work. section 4 gives the methodology used. section 5 shows the results and discussion on the sequences of the tcp termination flags. section 6 presents results from passive measurements. section 7 proposes a set of user-interruption criteria. section 8 concludes the paper.
"https://doi.org/10.1371/journal.pcbi.1006299.g003 that except when studying primary sensory stimulation, the snrs expected in real studies are likely in the lower range in our simulation, where brsa shows the most benefit."
"twenty primary and 21 secondary were then included in the next phase of development. the grades 1 and 8 learners completed the assessments, as part of samp and sassis in english, afrikaans and sepedi, depending on the language of instruction of the school. for both primary schools and secondary schools, a prototype report was generated and a feedback session held. during this session, schools received their reports, had the opportunity to ask questions and engage with the research team and the other schools. as the primary school feedback sessions took place first, a nominal group interview was arranged. however, due to difficulties experienced with the nominal groups in terms of attendance, a different method, the delphi technique, was used which essentially provided the opportunity to capture the same information as a nominal group."
"most of the studies conducted to construct university timetables did not consider timeslot availability for each professor as a hard constraint [cit] . some others did not consider it even as a soft constraint [cit] . the main focus is often directed toward avoiding the infeasibility of having two lectures at the same time; for a professor or a group, the capacity of rooms is considered based on a predicted number of attendees [cit] without encompassing more real life hard constraints such as professors' preferences and their timeslot availabilities. few contributions considered professors' preferences as a soft constraint [cit] . badri [cit] constructed a departmental timetable that considered professors' preferences in two phases: first, a matrix that maximizes the faculty member's preference for courses; and second, he maximizes the faculty member's preferences for timings of the lectures at the university of united arab emirates. some researchers suggested the introduction of utility functions for professors' preferences that can help in building timetables [cit] . schniederjans and kim [cit] argued that building such functions requires a lot of effort and time that can practically be an obstacle. [cit] considered professors' preferences and adopted a mining technique only to substitute the process of collecting information from every professor at the beginning of every semester. to avoid using complex functions and mining algorithms, this paper considers a simple to implement and a practical weighted sum formula to prioritize professors' preferences. respecting professors' preferences aims at reducing the effort needed to reallocate assignments in later stages and to make major timetable modifications more than once over few days."
"representations play a role in the performance of the algorithm during the search process [cit] . different chromosome representations are used to represent course timetables [cit] suggested that the best representation of a chromosome differs from a problem to another depending heavily on the constraints and requirements of each problem. for simplicity purposes, some researchers represent timetables in the form of binary strings [cit] stressed that this simple representation is not an ideal approach and that there is no clear alternative to it."
"one of the significant findings of this research speaks to how feedback can be used as a management tool. the reports provide a management tool as well as an opportunity in which discussion between heads of departments and teachers could take place at classroom level. the participants highlighted that the school's task is made more difficult due to combining special needs and mainstream schooling. participants also indicated that the amount of information provided in the reports was overwhelming; for example, gender and class comparisons. they suggested that if this information is needed, it should be requested on a school-by-school basis. principals suggested that feedback of school information should be clustered by school type as this would provide a more realistic picture and perhaps reflect more equally the demographics of the schools. however, as a positive, the participants indicated that they found the way in which data was shared with the school was empowering and would feed into their practice."
"it can be seen that in (6), Î¾ m takes values in the range [cit] . when the cfr between the ap and the ut matches the one represented to the position m in the database, Î¾ m is close to 1. however, as aforementioned the estimated cfr under the impact of i/q imbalance is modified, resulting in a change of the localization metric Î¾ m . assuming that the phase rotating vector in (6) is correctly calculated and that the considered position of the ut associated with the cfr g is matched with the desired position of the cfr h m in the database, from (4), (6) and removing the index for simplicity, the corresponding localization metric is presented as follows"
"before considering the tcp rst flags as being the indication of user behavior, it is important to make sure that a tcp rst flag is generated only when the user interrupts a tcp connection. therefore, it calls for a detailed classification of tcp end flags, which allows identification of those tcp rst flags that are generated as a result of user interruptions."
"reports are currently provided to each school and school level results are presented in comparative graphs where schools are represented anonymously. schools with the same medium of instruction are thus able to make a comparison of results with other participating schools. however, concern was raised by one of the schools about these comparisons as it was felt allowing comparison could lead to friction. the overwhelming response to this aspect was that the comparison allows for an examination of the school level overall and provides valuable information. more than 80% of the schools supported the idea that schools be subgrouped during the comparisons in terms of district or area to inform comparison with information about the environment and resource availability. [this grouping] will offer a better comparison of result due to influences of environment, expertise and distribution of resources [which] differs from area to area. (school 4, english, round 2)."
"end while 29. end while 30. output: the best chromosome achieved for the problem instance i the professor is an associate professor or higher, a weight of 0.2 is given. the age of the professor is another important factor where elder professors' preferences are a priority. if the age of the professor is between 50 and 75 years old, a weight of 0.1 is added. since, no weighted sum approaches for prioritization were proposed in the literature, the weights assigned to the factors in this work are proposed based on their level of significance as concluded by the authors from the practices followed in building timetables at the faculty of commerce, alexandria university. these weights are thus adjustable according to the problem under study. it is also possible in following semesters that weights change in their level of significance for the same faculty."
"in the first step, we evaluate the localization metric in (6) in the absence of the i/q imbalance. fig. 3 shows the localization metric values of 50 position points within the area of interest. the distance between two points in the grid is equal to the carrier wavelength, i.e. 12.5 cm. each point is consecutively considered as an intended location, by moving the intended location within the grid of interest we get the focusing gains for each intended point in the los (fig. 3(a) ) and nlos ( fig. 3(b) ) cases. it can be observed that the nlos case provides a better focusing gain compared to the los case, thanks to the more uniqueness of the cfr information. as the localization in the nlos indoor environment is in general more challenging, in what follows we focus only on the database-aided localization in the nlos case."
"two subsequent web transfers in a session are separated by the inactive off time, which is also called the user think time. this is the time taken by the user before launching the request for the next page or a file from the same server. the web traffic is carried by the tcp on the transport layer, hence making tcp the most widely used protocol on the internet for almost two decades. a tcp connection goes through several states from the connection establishment to the data transfer to the connection termination."
"evaluation of the user-perceived performance of network services is a complicated task. user perception is very much subjective, which vary heavily from one person to the another. there have been efforts made in order to evaluate and model the user perception. many of these efforts are limited to the user perception of voice and video applications [cit] . literature on qoe shows that it has either been drawn as qualitative nature subjective evaluation or quantified as a function of the qos parameters."
"the most common termination flag sequence observed with the internet explorer is the following: the server sends the data, which is immediately followed by one or more fins from the server. the client then responds the server's fin with one or more rst flags to tear down the connection. this is quite similar to what we observed in our active tests. there are also significant numbers of connections that are terminated with one or more rsts from the client side. we infer that these connections are the mix of both terminated by the client-side web browser (in the case of video transfers), as well the user to abort a transfer."
"in the main study, assessment instruments were used to assess the level of literacy and numeracy, but in this phase of the study only the feedback report, which contains the results is of relevance. for this phase, information was required on the current presentation and feedback sessions and thus drew on the interviews and delphi questionnaires."
"while rsa has been widely adopted in many fields of cognitive neuroscience, a few recent studies have revealed that the similarity structure estimated by standard rsa might be confounded by various factors. first, the calculated similarity between two neural patterns strongly depends on the time that elapsed between the two measured patterns: the closer the two patterns are in time, the more similar they are [cit] . second, it was found that because different brain regions share some common time courses of fluctuation independent of the stimuli being presented (intrinsic fluctuations), rdms between regions are highly similar when calculated based on patterns of the same trials of tasks but not when they are calculated based on separate trials (thus the intrinsic fluctuation are not shared across regions). this indicates that rsa can be strongly influenced by intrinsic fluctuation [cit] pointed out that the noise in the estimated activity patterns can add a diagonal component to the condition-by-condition covariance matrix of the spatial patterns. this leads to over-estimation of the variance of the neural pattern and underestimation of correlation between true patterns, and this underestimation depends on signal-to-noise ratio in each roi, making it difficult to make comparison of rdms between regions [cit] ."
"in this section we describe the connection terminations that we observed on the operational network of an isp. we only present the web browsers that we considered in our active tests, i.e. internet explorer, firefox, google chrome and opera. we observed a large the most common termination type: the server sends a fin flag after transferring the data, which is followed by a fin flag from the client side. generally, this is the most common termination flag sequence observed on the tcp connections launched on the network."
"after the similarity matrix between all pairs of estimated activity patterns is calculated in a region of interest (roi), it can be compared against similarity matrices predicted by candidate computational models. researchers can also convert the similarity matrix into a representational dissimilarity matrix (rdm, e.g., 1 â c, for similarity c based on correlation) and visualize the structure of the representational space in the roi by projecting the dissimilarity matrix to a low dimensional space [cit] . researchers might also test whether certain experimental manipulations change the degrees of similarity between neural patterns of interest [cit] . to list just a few applications from the field of visual neuroscience, rsa has revealed that humans and monkeys have highly similar representational structures in the inferotemporal (it) cortex for images across various semantic categories [cit] . it also revealed a continuum in the abstract representation of biological classes in human ventral object visual cortex [cit] and that basic categorical structure gradually emerges through the hierarchy of visual cortex [cit] . because of the additional flexibility of exploring the structure of neural representation without building explicit computational models, rsa has also gained popularity among cognitive neuroscientists for studying more complex tasks beyond perception, such as decision making."
"y k is the time series of voxel k. x is the design matrix shared by all voxels. Î² ï¿½k is the response amplitudes of the voxel k to all the task conditions. Ïµ k is the residual noise in voxel k which cannot be explained by either x or x 0 . we assume that Ïµ is spatially independent across voxels, and all the correlation in noise between voxels are captured by the shared intrinsic fluctuation"
"the needs analysis, incorporating interviews, and the dephi technique elicited rich data with similar ideas emerging from both primary and secondary schools, probably because the focus was on transition periods of schooling. overall, schools felt the process of feedback implemented was fit for the purpose intended. however, some schools felt that additional information could be provided. this suggestion is contrary to the expert review and findings from literature, which suggest streamlining and focusing information. there is evidence that schools are engaging with the information; however, the extent of actual usage of the information, and associated factors, in terms of instrumental, symbolic and conceptual use, still needs to be investigated."
"we conducted a set of active tests to observe the sequence of tcp termination flags exchanged in both directions. to execute the tests, we established an isolated environment. these tests were conducted by accessing a webpage on a smart phone or a laptop. the webpage was located on a local web server."
"after some manipulation, we can get the expression of the localization metric as in (11). based on the cauchyschwarz inequality [cit], the following relation holds"
"tcp connection termination handshake employs the control field in the tcp header to flag the end of a connection. to signal the end of the connection, a segment is sent from either side, with the finish (fin) flag set in the control field of the tcp header. the other side then responds with a fin segment to confirm the receipt of the fin segment. this handshake confirms that the data transfer is completed, and the connection could be closed. sometimes, the fin segments also contains last chunk of data in it."
"in line (6), the algorithm uses the largest enrollment (le) and the largest degree (ld) heuristics to generate a predetermined number of chromosomes (y) to form the population. in line (7), fitness values of the chromosomes produced are calculated and the best half of the chromosomes (y/2) are selected for crossover, while discarding low fitness value chromosomes. in line (13), crossover takes place between two chromosomes to produce new two offsprings. the old parents are also added to the new population. then, their fitness values are sorted in a descending order to choose the first chromosomes with highest fitness values and exclude the rest. the new generation of chromosomes becomes the input for creating the next population following the same steps."
"the appraisal tool comprises of a questionnaire which is divided into four sections. the sections consist of questions which evaluates the teacher depending upon (i) subject knowledge; (ii) teaching skills and assessment methods; (iii) behaviour towards students; and (iv) communication skills. each section comprises of minimum three to a maximum of five questions with a total of 15 questions. all the four sections are evaluated using 4 parameters which is set up according to the need of the question. at an outset the parameter a is assessed with 10 marks, b with 8, c with 6 and d with 4 marks. the percentage is calculated using the summation of all the parameters / (total no of questions * total number of students evaluated) * 100. the aim of this paper is to find out the effect of each parameter in the total score of the teacher."
"in addition to comparing the performance of brsa with traditional rsa, we also evaluated the performance of a few other approaches that have been proposed to improve the performance of rsa. one such approach is cross-run rsa [cit] . cross-run rsa calculates the similarity between patterns estimated from separate scanning runs, in contrast to the traditional rsa (here denoted as within-run rsa) which uses patterns estimated from the same scanning runs: Ã°mÃ¾ andÎ² Ã°nÃ¾ ) are independent, the expectation of the numerator of eq 8 does not contain the bias term (x t x)"
"we executed tstat on our interrupted and uninterrupted transfers. we found out that the user interruption criterion was not working accurately for those video transfers, in which internet explorer was used as client-side web browser."
"burke and varley [cit] made many efforts toward defining and discovering the different dimensions and requirements of the space allocation problem inside ninety-six universities and academic institutions in the united kingdom. they collected the information from universities using questionnaires. the questionnaires stressed on three main aspects: the size and diversity of the space allocation process, the tools used to automate the space allocation process and the constraints considered when allocating these spaces. the main purpose of this survey was to discover whether a generic solution to the problem could be articulated or the variance among universities will prohibit such approach. they have concluded that a generic system for the space allocation must be able to satisfy all the requirements specified by a university."
"a web session is a combination of one or more web transfers. a web transfer starts when a user requests for webpage or a file. a web page may consist of multiple embedded objects. each embedded object is retrieved after the client-side web browser requests for the respective object, automatically. the transfer of each object is shown as the on time in the figure 1 . two objects in a web transfer are separated by the active off time; the time taken by the client-side web browser to launch the next request automatically, after the transfer of the previous object."
"perhaps the expectation of instrumental use as part of the data-driven decision making process is unrealistic as the effective use of performance information is a gradual process especially within the context of south africa. therefore, it is important for researchers to uncover obvious and less obvious examples of use. to this end, methods should be used which distinguish between partial and complete use and therefore deeper exploration and understanding of the behaviour of participants in terms of the complex process behind data use is needed [cit] ."
"an idea was put forward that multiple copies of the reports be provided to schools so both the principal and educators would have copies readily available. a few schools indicated this would be useful, while the majority of schools indicated it was not necessary. some schools expressed concern that educators might think that the two reports supplied differ which would cloud the process with doubt. one school indicated that the principal would prefer controlling the data and decide what information should be provided to educators. this is a school management issue. (school 4, english, round 2). half of the schools indicated that they felt the reports were relevant and appropriate for their needs. a request was made to ensure the turn-around time for the reports be shortened, to allow educators more time to alter their planning and practices according to the results."
"monitoring of the user-perceived performance has two main requirements. first, a model is required, that takes into account all the parameters that influence user-perceived performance of a service; second, a method, which estimates the performance by measuring the above parameters in a fast and scalable fashion [cit] . unfortunately, it is not so easy to measure all these parameters online in real-time, from the network-level."
"data mining techniques can be categorized into classification, clustering and association mining. classification is the task of assigning objects to one of the several predefined categories that encompasses many diverse applications. some popular classification methods include decision trees, logistic regression.. rep tree and m5p trees are used in this paper to predict the factor affecting the teacher appraisal. the general procedure for the application of the trees is given in figure4. the output (tree) has the following types of nodes."
"we have studied the impact of i/q imbalance in the time-reversal based indoor positioning systems (trips). it has been analytically shown that the localization metric reduces in the presence of i/q imbalance. as a consequence, the spatial focusing effect of the trips also reduces whereas the error in the positioning estimation increases compared to the case of no i/q imbalance. by numerical simulations, the reduction of focusing gain is about 0.5 db and the average positioning estimation error is about 5% subject to the gain and phase imbalances of 1.1 and 10"
"different approaches that were used in solving the space allocation problems were also used in solving the timetabling problems. these include simulated annealing [cit], tabu search [15, [cit], integer programming [cit] and genetic algorithms [cit] . some contributions are as follows."
"generally, from all the above observations of interrupted connections, one thing is common that at least one rst flag from the client-side is seen regardless of the platform. the other important evidence about the user-generated interruption is that more than one consecutive rst flags were seen in most of the cases as soon as the user performs an interruption in the web browser."
"moreover, tests were first performed on three popular mobile platforms: windows 6.5 (htc hd2), android 2.2 (htc desire hd) and symbian 3.0 (nokia n8). built-in web browsers were used on each of these platforms, as external browsers were not supporting the video content. the web browser used by windows 6.5 is microsoft internet explorer 6.0. the user agent string in the html header reports android's web browser as mobile safari and symbian's web browser as browser ng, which is used on the nokia mobile phones. on android and symbian platforms, the built-in web browsers use webkit as the html rendering engine, which is an open-source web browser engine [cit] . each of the tests on mobile platforms was conducted with 40 repetitions."
"this assignment process is repeated until all high weight professors are assigned. afterward, the rest of professors with weights less than 0.4 and more available timeslots are assigned."
"the goal of rsa is to understand the degree of similarity between each pair of spatial response patterns (i.e., between the rows of Î²). but because the true Î² is not accessible, a point estimate of Î², derived through linear regression, is usually used as a surrogate:"
assessment plays a key role in how teachers teach and how students learn. there are different types of assessment and not all are about results and exams. the best forms of assessment will combine data analysis with ways of getting to know pupils and understand their educational needs.. effective assessment enables teachers to:
"reduced error pruning (rep) is a model based tree proposed by quinlan. starting at the leaves, each node is replaced with its most popular class. if the prediction accuracy is not affected then the change is kept. reduced error pruning has the advantage of simplicity and speed. it uses a pruning set to evaluate the goodness of the subtrees of a model tree t. the pruning set is independent of the set of observations used to build the tree t, therefore, the training set must be partitioned into a growing set used to build the tree and a pruning set used to simplify t. the algorithm analyzes the complete tree t and, for each internal node t, it compares the mean square error (mse) computed on the pruning set when the subtree tt is kept, with the mse computed on the same set when tt is pruned and the best regression function is associated to the leaf t [cit] . the attributes are fed to the rep tree without applying pruning. it has applied on with a tenfold cross validation. the following rule has been applied by the algorithm to build the table. from the given rule it is clear that the parameter -aâ and -dâ has played a vital rule in building the teacher's feedback. the parameter -bâ and -câ has minimum values which can be ignored while building the tree. the diagrammatic representation of the rep tree is given below. the figure 4 is the tree produced by the algorithm. it indicates that parameter -aâ and -dâ are taken as leaf nodes and they end as dead nodes at first iteration only. the data set is then pruned and analysed using the rep tree without changing all other constraints. the rule obtained after pruning the data set is given below: the rep tree has produced a decision tree of 5 leaves after pruning. the tree stopped growing after the first iteration for the parameter -aâ and took parameter -dâ for second iteration. also. the algorithm is analyzed using various measures like co-relation co-efficient, the various errors generated and the time taken for generating the tree. it is also clear from the tree that parameter -aâ plays a major role than the parameter -dâ. the table1 shows the various values obtained while processing the data in the algorithm. the time taken by the algorithm before and after pruning is same ie zero seconds, the other factors differ from each other. it is evident that pruning has reduced the effect of error in the algorithm and the tree is also terminated after the generation of two leaf nodes . the graph in figure 6 indicates that all the comparison measures are relatively high before pruning the data and it is relatively low when the data is pruned. therefore it is obvious that pruning the data in rep tree makes the decision tree very effective."
"on the other hand, a single rst flag is seen if the transfer is not interrupted by user, on which the rst flag is sent automatically by the client-side software."
"this project started with a needs analysis of what information schools felt was essential to capture in the report and feedback sessions. six primary and secondary schools participated in the first phase from which the first prototype of the report and feedback session was developed. the needs analysis comprised interviews with the principal, head of department and selected teachers from grade 1 and grade 8."
www.macrothink.org/npa 57 transfer. the tcp connection also goes into the slow start phase once again. the client-side software should avoid this kind of behavior as the opening of multiple tcp connections per transfer may degrade the performance of the transfer.
"there is also large number of cases in which the connection termination handshake starts with a fin flag from the client side followed by a fin flag from the server side. this is the most common in the case of opera and second most common in the cases of firefox and google chrome. the client in such cases detects the end of the data transfer and hence, it sends a fin immediately along with an ack of last data segments from the server. one or more fins from the client followed by one or more fins from the server followed by one or more"
"1. the faculty does not follow any automated approach when constructing timetables. thus, it was a fresh ground to test the contributed approach. 2. the problem size is very large and complex in terms of the following: the number of students enrolled each year, the number of groups, the number of departments, the number of professors, and the limited number of teaching spaces. 3. the large number of common lectures among student groups from different departments. 4. the faculty works seven days a week including fridays to be able to find a room to assign events. 5. lectures' time durations are not standardized; some are three-hour and others are two-hour duration. this makes it harder to assign events while avoiding intersections."
"the samp project sampled 22 schools, of which eight were english medium school, six schools were afrikaans medium schools, seven sepedi medium schools and one was a dual medium english/afrikaans school. two grade 1 classes in each school undertook the baseline assessment. all 22 principals and selected heads of department and teachers were included in the study. while this article reports on the 22 schools that participated in the study, at the time there were schools who decided to withdraw."
"to test other popular web browsers, we further continued our tests on a laptop equipped with the windows xp operating system. tests were performed with four web browsers: internet explorer, firefox, opera and google chrome. figure 6 presents the total number of rst flags observed from all our tests. this figure gives an overall picture of the web browser figure 6 confirms the results we retrieved from mobile web browsers. the internet explorer web browser generates more tcp rst flags than any other web browser. the rst flags on internet explorer are not only generated when the user aborts a transfer, but also in the cases when a transfer is not aborted by the user. another observation we got from the experiments is that, the generation of tcp flags is not dependent on the operating system, but on the web browser. we performed these tests on the same operating system and we observed different behavior in terms of tcp termination flags. termination initiation from the client side: in the case of google chrome web browser, connections termination starts with a fin from the client instead of a fin from the server for uninterrupted tests. after the data transfer, the client initiates termination handshake, by sending a fin segment. the server then responds with a fin flag. hence, the client does not wait for the server time-out but starts tearing down the connection proactively."
"several recommendations related to the improvement of support to schools were made. many schools have difficulty with consent from parents for the assessments as learners often do not convey the messages, or the parents are not accessible. to ensure consent, it was suggested that the consent letters for assessment be sent out to educators the year before testing. then when parents register learners for schooling, they sign the forms. many learners have to be reminded all the time. this would be a faster way of getting forms back. (school 23, english, round 2). the idea enjoyed great support and schools added that it would have the additional benefit of informing the planning of the school calendar which can take testing dates into consideration."
"luÂ¨and hao [cit], developed an adaptive tabu search, in which an initial timetable was constructed using a greedy search heuristic. this greedy search starts with an empty schedule and starts assigning lectures by selecting an unassigned lecture and a suitable period-room event. however, greedy search heuristics tend to work efficiently at the beginning of the timetable construction process while causing conflicts in assigning later events [cit] . the objective of the method was to minimize the number of soft constraint violations in a feasible timetable. although teachers' timeslot availabilities were considered a main hard constraint, no prioritization approaches were incorporated to handle this issue if the number of available slots for more than one teacher is the same."
"while spatial whitening might be desirable for the purpose of cross-run rsa since it improves the performance, one should take caution when interpreting activity pattern after whitening. this is because spatial whitening remixes data across voxels and risks moving taskrelated signals from voxels responding to a task to non-responsive voxels. instead of performing spatial whitening, brsa estimates several time series x 0 that best explain the correlation of noise between voxels, and marginalizes over their modulations of activity in each voxel. therefore, brsa can capture spatial noise correlation without the risk of misattributing signals across voxels."
"hence, our contribution in this paper is two-fold. first, we present the different sequences of the tcp termination flags observed with a number of web browsers. with the help of these results, cooperation between web browsers and web servers could be improved to raise the performance of web transfers. second, we develop a set of criteria, which could be used to identify the user action performed in the web browser. it may help service providers to monitor the user-perceived performance."
"x 2 r n t ï¿½n c is the \"design matrix,\" where n c is the number of task conditions. each column of the design matrix is constructed by convolving a hemodynamic response function (hrf) with a time series describing the onsets and duration of all events belonging to one task condition. the regressors composing the design matrix express the hypothesized response time course elicited by each task condition. each voxel's response amplitudes to different task conditions can differ. the response amplitudes of one voxel to all conditions forms that voxel's response profile. all voxels' response profiles form a matrix of spatial activity patterns Î² 2 r n c ï¿½n v, with each row representing the spatial pattern of activity elicited by one task condition. the responses to all conditions are assumed to contribute linearly to the spatio-temporal fmri signal through the temporal profile of hemodynamic response expressed in x. thus, the measured y is assumed to be a linear sum of x weighted by response amplitudes Î², corrupted by zero-mean noise Ïµ."
"first, a decision-tree induction algorithm is used to build a tree, but instead of maximizing the information gain at each inner node, a splitting criterion is used that once the tree has been built, a linear model is constructed at each node. the linear model is a regression equation. the attributes used in the equation are those that are tested or are used in linear models in the subtrees below this node. the attributes tested above this node are not used in the equation as their effect on predicting the output has already been captured in the tests done at the above nodes. the linear model built is further simplified by eliminating attributes in it. the attributes whose removal from the linear model leads to a reduction in the error are eliminated. the error is defined as the absolute difference between the output value predicted by the model and the actual output value seen for a given instance. the splitting procedure in m5p stops if the class values of all instances that reach a node vary very slightly, or only a few instances remain."
"timetabling is an np-hard optimization problem [cit], for which a good solution needs to be found among a set of complex variables and constraints. the problem is to assign a feasible tuple of variables which optimizes a set of metrics and indicators such as minimizing time gaps, maximizing space utilization, and minimizing cost relevant to the use of resources [cit] . finding an efficient algorithm for such problems is hard and complex especially when the problem gets larger. according to tovey [cit], good solutions can be provided when the problem is better understood in terms of its hardness or simplicity. organizations such as educational institutions use timetables to schedule classes and/or lectures by assigning times and places to future events in a way that makes optimal use of the available resources [cit] . universities increasingly deal with a large number of courses, groups and professors. poorly designed timetables are not only inconvenient, but also result in significant losses in terms of time, effort and money."
"all the previously presented tests were done on a personal computer core i5 2.40 ghz cpu and 6 gb ram using java 6 as a programming language and postgressql 9.3 as a database management system. it is obvious from testing the algorithm on different datasets, that uga algorithm is a robust generic algorithm that generates good utilization rates for the allocated spaces. however, the computational time required by uga algorithm to test dataset 1 was slightly longer than bwas and bawcs, and it consumed less time in dataset 2. this is still acceptable because constructing timetables is not a problem solved every day. it is a process that takes place long before academic semesters begin. additionally, the authors have tested the benchmarks based on constraints that are more complex and with a more elaborate objective that were not taken into consideration in their original problem formulation."
"this question refers specifically to the following two aspects: the report and the feedback sessions, in terms of logistics and additional support provided so that the data could be interpreted and used effectively by the schools. however, schools were invited to also offer any other ideas pertaining to the programme."
"the various notions of 'use' appropriate for this research are instrumental use, conceptual use and symbolic use. instrument use is the concrete application of the research information in a specific and directed way (such as decision-making) [cit] . conceptual use is about using the information for general enlightenment, which means that thinking about the feedback information may be changed, but does not result in action. finally, symbolic use is when information is used to legitimise practice or defend a position and to persuade to lobby for resources [cit] . [cit] suggests that use should be studied in terms of what is used, who uses it, how immediate the use is and the effect of the use."
"after each crossover, a mutation operator is applied to the best solution found so far to obtain a better fitness value. the number of mutations in a chromosome depends on the number of the underutilized events in the solution. this process is repeated until the maximum number of iterations (iter) is reached."
"for this research, design research was employed which focuses on designing and exploring innovations to test particular interventions in order to support specific theoretical claims. the aim of design research is to understand the complex interplay between theory, designed artefacts and practice [cit] . design research is iterative in nature (as reflected in figure 1, [cit] ), with the aim of improving the reporting format and feedback sessions, in addition to improving the design principles [cit] )."
"to get the utilization rate of a room in a day, the averages of both the frequency and occupancy rates of that day are multiplied. similarly, when calculating the utilization rate of a room in a week, the averages of the frequency and occupancy rates of that week are multiplied. in this work, the fitness function is a composite function that calculates the fitness of a certain chromosome based on a number of important factors. these factors are as follows:"
the peak height of task-triggered response is often in the range of 0.1-0.5% of fmri signal magnitudes in cognitive studies [cit] while the noise level is often a few percents. this means that of the noise (the time series of the resting state fmri data):
"validity in qualitative research is described in terms of the trustworthiness, relevance, plausibility, credibility, or representativeness of the research [cit] . the validity of the research is located with the representation of the participants, the purpose of the research and the appropriateness of the processes employed [cit] . validity for the qualitative component of this research has to do with the adequacy of the researcher to understand as well as represent the participants' meaning. thus, validity becomes a quality of the knower in his/her relation to the data, enhancing different vantage points and forms of knowing [cit] . credibility is similar to the concept of internal validity [cit] . it refers to procedures aimed at ascertaining whether the interpretations of the data are compatible with the constructed realities of the participants [cit] . peer debriefing and member checking (by means of the delphi process), is used in the process of this research. validity in qualitative research is personal, relational, as well as contextual in nature. how the research was conducted was of importance in terms of whether the researcher was aware of her own perspective, processes, and the influence of these on the research [cit] . reflexivity, which is the examination of how one's own truth influences the research process, is also an important component of this research [cit] ."
"in addition to inferring the representational similarity structure, our method also infers activation patterns (as an alternative to the traditional glm), snr for different voxels, and even the \"design matrix\" for data recorded without knowledge of the underlying conditions. the inferred activation patterns are regularized not only by the snr, but also by the learned similarity structure. the inference of an unknown \"design matrix\" allows one to uncover uninstructed task conditions (e.g., in free thought) using the full bayesian machinery and all available data."
"multiple tcp connections per transfer: subsequently, when the video-based webpage is downloaded from the windows platform, there is another interesting pattern seen in the connection termination process. after receiving the base file, the client makes a get request for the video player. it then immediately terminates the connection with a rst flag and initiates the new connection with a syn handshake. the get request for the previous file is thus repeated once again and then the video is played in the web browser. the second connection is terminated similarly as was observed in the case of text-based webpage. hence, two connections are opened for playing video in the web browser. the connection establishment procedure creates extra overhead, which affects badly the overall speed of the"
"to signal the error conditions, a segment with reset (rst) flag is sent. the rst segment can be sent from one of the sides to deny a connection, if a connection was requested to a nonexistent tcp port. it is also sent when one of the sides aborts an existing tcp to signal an abnormal situation. figure 2 . the client acknowledges one or more data segments from the server with an ack segment. once the transfer of requested file is completed, the client may request for another file with another http get request in the same tcp connection, if the client and the server are both supporting persistent tcp connections. once all the requests from the client are served, the client or server then starts the connection termination handshake procedure, which is shown by fin segment. the other end then responds with a fin to terminate the connection."
"many aspects of flexibility may be incorporated to brsa. for example, the success of the analysis hinges on the assumption that the hemodynamic response function (hrf) used in the design matrix correctly reflects the true hemodynamics in the region of interest, but the hrf in fact varies across people and across brain regions [cit] . jointly fitting the shape of the hrf and the representational structure using brsa may thus improve the estimation. in addition, it is possible that even if the landscape of activity patterns for a task condition stays the same, the global amplitude of the response pattern may vary across trials due to repetition suppression [cit] and attention [cit] . allowing global amplitude modulation of patterns associated with a task condition to vary across trials might capture such variability and increase the power of the method."
"functional magnetic resonance imaging (fmri) measures the blood-oxygen-level-dependent (bold) signals [cit], which rise to peak ï¿½6 seconds after neuronal activity increases in a local region [cit] . because of its non-invasiveness, full-brain coverage, and relatively favorable tradeoff between spatial and temporal resolution, fmri has been a powerful tool to study the neural correlates of cognition [cit] . in the last decade, research has moved beyond simply localizing the brain regions selectively activated by cognitive processes and the focus has been increasingly placed on the relationship between the detailed spatial patterns of neural activity and cognitive processes [cit] ."
"in our derivations above, point estimates ofÎ² introduce structured noise due to the correlation structure in the design matrix. one might think that the bias can be avoided if a design matrix is not used, i.e., if rsa is not performed after glm analysis, but directly on the raw fmri patterns. such an approach still suffers from bias, for two reasons that we detail below."
"to simulate the fmri noise in fig 4, we first estimated the number of principal components to describe the spatial noise correlation in the 24 resting state fmri data from hcp database using the algoritm of gavish and donoho [cit] . the spatial patterns of these principal components were kept fixed as the modulation magnitude Î² 0 by the intrinsic fluctuation. ar (1) parameters for each voxel's spatially indepndent noise were estimated from the residuals after subtrating these principal components. for each simulated subject, time courses of intrinsic flucutations were newly simulated by scrambling the phase of the fourier transformation of the x 0 estimated from the real data, thus preserving the amplitudes of their frequency spectrum. ar(1) noise were then added to each voxel with the same parameters as estimated from the real data. to speed up the simulation, only 200 random voxels from the roi in fig 3b were kept for each participant in these simulations. among them, 100 random voxels were added with simulated task-related signals. thus, each simulated participant has different spatial patterns of Î² 0 due to the random selection of voxels. 500 simulated datasets were generated based on the real data of each participant, for each of the three snr levels. in total 36000 subjects were simulated. the simulated pool of subjects were sub-divided into bins with a fixed number of simulated subjects ranging from 24 to 1200. the mean and standard deviation of the correlation between the true similarity matrix and the average similarity matrix based on the subjects in each bin were calculated, and plotted in fig 4a. all snrs in figs 3 and 4 were calculated post hoc, using the standard deviation of the added signals in the bounding box region devided by the standard deviation of the noise in each voxel, and averaged across voxels and simulated subjects for each level of simulation."
"with the increasing usage of the internet, the expectations of users are also evolving. while the computational power of the devices, intelligence of applications and speed of networks are increasing with time, following moore's law, expectations of users are following \"the more's law\" [cit] : users want more in less time. they are becoming increasingly strict and intolerant about the quality of network and application services. this is because the users now have to rely on the internet for their everyday tasks. due to these growing expectations, the margin of error is getting smaller and the network protocols and algorithms need to perform smartly and accurately."
"the authors introduce a crossover type that they named ''utilization crossover\". this crossover focuses on the utilization rates of teaching spaces. observations from running the algorithm revealed that, in many cases, chromosomes hold close utilization rates of some events although they differ in their placements in the timetable from a chromosome to another. utilization crossover aims at reducing the number of the under/over-utilized events (with occupancy rates less than 75% and greater than 100%, respectively) as much as possible to increase the number of well-utilized events in a chromosome. this is achieved through obtaining a list of all the underutilized and over-utilized events from one chromosome and randomly selecting 50 percent of the events from this list to be assigned to other random rooms, within the same timeslot, in the other chromosome. when selecting any event, its utilization rate, in the other chromosome, is obtained so that an accepted move takes place if the utilization rate of the moved event is improved in the new place without violating any hard constraint. if a successful move takes place, the event is then removed from its old place in the new chromosome to avoid duplication (see fig. 2 )."
"several schools were purposefully selected to participate in this project for maximum variation in their characteristics and background. as the aim of the research is to develop a monitoring system, which would be appropriate for south african schools regardless of the variation in schools, it was imperative to include schools from various backgrounds. due to financial constraints, a limited number of schools were accommodated in the sample, as discussed below."
"time reversal based indoor positioning system (trips) operates based on two phases: (i) offline cir (or cfr) estimation probing phase to build the database and (ii) localization estimation based on the correlation between the online estimated cir (or cfr) and the ones in the database. usually, the access point (ap) stores the cir database for the localization in order to reduce the cost of the user terminal (ut). while the reference terminal can generally assume to not be prone to hardware impairments, it is not the case for the ut to be located. one of the major source of impairments in wireless systems is the imbalance between the inphase (i) and the quadrature (q) branches, referred to as i/q imbalance, where the up-down complex frequency conversion by the sinusoidal oscillators happens. typically the i/q imbalance originates from the mismatch between the i and q branches compared to the ideal case, in which the phase difference is 90"
"in this paper, we study for the first time the impact of i/q imbalance on the trips. we investigate the impact of i/q imbalance on the localization metric proposed for trips [cit], and on the spatial focusing effect as well as on the failure ratio of the positioning estimation."
"the monitoring process aims to lead to informed decision-making and improvement strategies, given the complex embeddedness of societal systems such economics and politics [cit] . generally, educational quality can be thought of in the following terms: schools being able to transform inputs into outputs [cit], that the objectives identified have national and societal relevance (scheerens, glas & thomas, td, 9(1), [cit], pp. 81-93., [cit], fairness in the distribution of resources, as well as the value of the certificates received which verify that knowledge and skills have been mastered [cit] ."
"while this interruption criterion determines the connections interrupted within t gap, it does not identify the user action performed in the web browser that resulted in the interruption of ongoing transfer. the identification of the user action in the web browser is also important to know, as there could be different motivation behind each user action. for example, users usually press the stop or reload button when they are annoyed. however, they might also follow a link before the completion of a page when they have already seen enough information on the page, which may not be the result of anger or dissatisfaction."
"additionally, we also showed some of the abnormal behaviors by the web browsers. we believe that there is a need of a better mechanism for communication between web browsers and the web servers, in order to improve the performance of tcp connections and raise the user experience."
"from fig. 2, three underutilized events with the ids (4, 40 and 300) were detected in chromosome one. in the second chromosome, these events' occupancy rates were found to be underutilized as well. thus, random suggested rooms were introduced aiming at achieving improved rates without violating the event's professor timeslot preference. successful moves took place for event id (40) from room 201 with 36% occupancy rate to 85% in room 302. similarly, this happened for event ids (4 and 300) where occupancy rates have improved from 53% in room 302 to 66% in room 503 for id (4) and from 55% in room 403 to 77% in room 507 for event id (300)."
"schools across the board agreed with the idea of giving information and supporting preschools and schools for preparing learners for the move into primary or secondary education. the idea of providing workshops for educators already involved in the project met with some ambivalence as some welcomed the suggestion, but others did not feel the need. some schools, however, expressed the need for intervention materials to help support individual atrisk learners identified in reports. it was suggested that educators be allowed to observe fieldwork. schools noted that this would allow educators to build confidence in the assessment and see how well the fieldworkers build rapport with the children. educators will be able to judge learner's reaction towards an unknown person as some learners don't simply respond to a strange face as learner is familiar to educator. (school 23, english, round 2) it was also mentioned that the educator presence may have a reassuring effect on learners, especially those new to schooling. whilst schools advocated for observations of assessments, they added that this should be an opportunity to be extended, but not a requirement as it may be time consuming for educators."
"pling of the temporal covariance structure of noise at the distances of the inter-event intervals. in this way, timing structure of the task and autocorrelation of noise together still cause bias in the rsa result."
"our brsa method is closely related to pattern component modeling (pcm) [cit] . a major difference is that pcm models the point estimatesÎ² after a glm analysis while brsa models fmri data y directly. the original pcm [cit] in fact considered the contribution of noise in pattern estimates to the similarity matrix, but assumed that the noise inÎ² is i.i.d across task conditions. this means that the bias in the covariance matrix was assumed to be restricted to a diagonal matrix. we showed here that when the order of task conditions cannot be fully counterbalanced, such as in the example in fig 1, this assumption is violated and the bias cannot be accounted for by methods such as pcm. another difference is that brsa explicitly models spatial noise correlation, which can improve the results (see figure 3 of s1 material)."
it is obvious from table 6 that the number of events with poor occupancy rates (less than 75%) is 99 events compared to the number of events with good occupancy rates that are 12 only. there are 10 events with occupancy rates that exceeded 100%. this means that the corresponding rooms were overutilized. the number of events left idle and unoccupied is 103 events.
"the aim of monitoring and providing information based on a monitoring system is to improve teaching and learning. [cit] are of the opinion that there are a number of reasons for gathering performance data, namely, for information needs in terms of functioning and learner performance so that adequate decisions can be made, for accountability purposes and to stimulate discussions with stakeholders. here a distinction is drawn between what data is, namely, objective facts with no meaning attached or information where the data is interpreted [cit] . if schools and teachers are to use the data they receive as part of monitoring systems, they need to transform the data into information with which they can work effectively."
"for instance, in fig 3f, at the lowest snr and least amount of data (top left subplot), the true similarity structure is almost undetectable using brsa. is this due to large variance in the estimates, or is it because brsa is still biased, but to a lesser degree than standard rsa? if the result is still biased, then averaging results across subjects will not remove the bias, and the deviation of the average estimated similarity structure from the true similarity structure should not approach 0. to test this, we simulated many more subjects by preserving the spatial patterns of intrinsic fluctuation and the auto-regressive properties of the voxel-specific noise in the data used in fig 3, and generating intrinsic fluctuations that maintain the amplitudes of power spectrum in the frequency domain. to expose the limit of the performance of brsa, we focused on the lower range of snr and simulated only one run of data per \"subject\". fig 4a shows the quality of the average estimated similarity matrix with increasing number of simulated subjects. the average similarity matrices estimated by brsa do not approach the true similarity matrix indefinitely as the number of subjects increase. instead, their correlation saturates to a value smaller than 1. this indicates that the result of brsa is still weakly biased, with the bias depending on the snr. it is possible that as the snr approaches 0, the estimated Ã» is gradually dominated by the impact of the part of x 0 not orthogonal to x. this bias is not due to underestimating the number of regressors in x 0 (see part 6 the effect of the number of nuisance regressors on brsa performance of s1 material). we leave investigation of the source of this bias to future work. empirically, the algorithm [cit] we use to estimate the number of regressors in x 0 yields more stable and reasonable estimation than other methods we have tested (e.g., [cit] ). it should be noted that brsa still performs much better than standard rsa, for which the correlation between the estimated similarity matrix and the true similarity matrix never passed 0.1 in these simulations."
"in a few interrupted transfers on symbian and windows, and in the majority of transfers on android, connections were terminated with a fin flag followed by one or more rst flags from the client. by looking at the interrupted traffic traces, we found out that, when the client starts the termination process with a fin flag, then the server responds with the retransmission of previous unacknowledged segments. upon receiving the retransmitted segments the client tears down the connection by sending one or more rst flags. this kind of anomaly may result in the wrong estimation of loss rates on the network. on the windows platform, the majority of the connections were terminated with one or more rst flags from the client without any fin flag."
"in the case of interrupted tests, we observe a similar behavior in the case of all web browsers. the client terminates a connection with a rst flag as soon as the user aborts a connection in the middle of the transfer. if the client still receives a data segment from the server, after sending a rst flag, then it repeatedly sends rst flags to enforce the termination of the connection."
"the steps involve extracting the data from the data base, cleaning data to remove noise and duplicate observations and selecting records and features relevant to the data mining tasks. it is one of the time consuming step in the data mining process."
"we start by assuming that the shared intrinsic fluctuation across voxels can be explained by a finite set of time courses, which we denote as x 0, and the rest of the noise in each voxel is spatially independent. if x 0 were known, the modulation Î² 0 of the fmri signal y by x 0 can be marginalized together with the response amplitude Î² to the experimental design matrix x (note that we still infer u, the covariance structure of Î², not of Î² 0 ). since x 0 is unknown, brsa uses an iterative fitting procedure that alternates between a step of fitting the covariance structure u while marginalizing Î² 0 and Î², and a step of estimating the intrinsic fluctuation x 0 from the residual noise with principal component analysis (pca). details of this procedure are described in part 2 model fitting procedure of s1 material."
"indeed, the severity of confounds in traditional rsa is not yet widely recognized. rsa based on neural patterns estimated within an imaging run is still commonly performed. furthermore, sometimes a study might need to examine the representational similarity between task conditions within an imaging run, such that cross-run rsa is not feasible. the taylor expansion approach to model the effect of event-interval can be difficult to set up when a task condition repeats several times in an experiment. there also lacks a detailed mathematical examination of the source of the bias and how different ways of applying rsa affect the bias. researchers sometimes hold the view that rsa of raw fmri patterns, instead of activity patterns (Î²) estimated through a general linear model (glm) [cit], does not suffer from the confounds mentioned above. last but not least, the contribution of noise in the estimated neural patterns to the sample covariance matrix between patterns may not be restricted to the diagonal elements, as we will demonstrate below."
"as expected, when the snr approached zero, the model over-fit to the noise and the bias structure increasingly dominated the estimated structure despite increasing the number of simulated participants (fig 4b) . this observation calls for an evaluation procedure to detect the average correlation between the off-diagonal elements of the estimated and the true similarity matrices (mean Â± std) as the number of simulated subjects increases. each simulated subject had one run of data. legend shows average snr in taskresponsive voxels. half of the voxels do not include any signal related to the design matrix. the correlation reaches asymptotic levels slightly below 1 with increasing numbers of participants except when the snr is extremely low (0.07), indicating that the bias is not fully eliminated. (b) the average correlation between the estimated similarity matrix and the expected bias structure assuming white noise. the estimated similarity structure is most dominated by the bias structure at the lowest snr simulated (0.07). the negative correlation at the highest snr reflects the weak negative correlation between the true similarity structure and expected bias structure (-0.055)."
"in the recent years, the internet has witnessed the mushrooming of networks and applications. among all the applications, the web application is the most dominant one. the popularity of web is further fueled by the migration of video on the web. according to study [cit], web traffic accounts for the major part of the traffic volume on the internet."
"for sassis, 21 schools were included. instead of selecting schools according to language groups, sassis schools were selected according to previous department of education dispensation. the breakdown per previous dispensation was eight former model c schools and department of education and training, three former house of delegates and two house of representatives. two classes from every school were randomly selected 4 by means of winw3s. all 21 principals and selected heads of department and teachers were included in the study."
"currently, reports are only produced in english. some schools suggested that the reports be provided in both afrikaans and english. this raised the issue of reports being made available in sepedi. the cost and labour implications of reports in all three languages are however large and schools indicated that while this would be good, it is not a main priority in terms of improvement. a consideration of reporting through the use of electronic reports which allows the schools to perform their own further analysis, was proposed by a secondary school. reports should be in cd form (school 1, english, round 1)."
"as shown above, the covariance structure of the noise in the point estimates of neural activity patternsÎ² leads to bias in the subsequent similarity measures. the bias can distort off-diagonal elements of the resulting similarity matrix unequally if the order of task conditions is not fully counterbalanced. in order to reduce this bias, we propose a new strategy that aims to infer directly the covariance structure u that underlies the similarity of neural patterns, using raw fmri data. our method avoids estimatingÎ² altogether, and instead marginalizes over the unknown activity patterns Î² without discarding uncertainty about them. the marginalization avoids the structured noise introduced by the point estimates, which was the central cause of the bias. given that the bias comes not only from the experimental design but also from the spatial and temporal correlation in noise, we explicitly model these properties in the data. we name this approach bayesian rsa (brsa) as it is an empirical bayesian method [cit] for estimating u as a parameter of the prior distribution of Î² directly from data."
"assuming that the signal Î² is independent from the noise Ïµ, it is then also independent from the linear transformation of the noise, (x t x)"
"according to a hypothetical covariance structure for the \"16-state\" task conditions (fig 3a) for voxels assumed to respond to the task, and then weighting the design matrix of the task in fig 1a by the simulated response amplitudes. the resulting simulated signals were then added to resting state fmri data. in this way, the \"noise\" in the test data reflected the spatial and temporal structure of realistic fmri noise. to make the estimation task even more challenging, we simulated a situation in which within the roi (fig 3b; we took the lateral occipital cortex as an roi in this simulation, as an example) only a small proportion (ï¿½ 4%) of voxels respond to the task conditions (fig 3c) . this is to reflect the fact that snr often varies across voxels and that an roi is often pre-selected based on anatomical criteria or independent functional localizer, which do not guarantee that all the selected voxels will have task-related activity. we evaluated the performance of brsa at different levels of snr and on different amounts of data (1, 2 and 4 runs of data, where each run includes fmri data of 182Â±12 time points. the simulated roi includes more than 4000 voxels.) the results of additional simulation in which signals were added in all voxels at lower snr are similar, and provided in part 4 performance of all methods when all voxels have task-related signals of s1 material."
"interrupted transfers are those in which a user aborts an on-going transfer by manually performing any of the three actions (before the end of the download) in web browser: pressing the stop or reload button, exiting the browser or clicking a hyperlink on the webpage. the results in figures 3-5 illustrate that the connection termination pattern is similar when the interruptions are made from android and windows platform, while it is slightly different in the case of the symbian platform."
"a key aspect that motivated this research is that previous studies focused much on the computational time of the algorithms proposed [cit], even when considering a small problem instance and few constraints. in contrast, the reality of the egyptian public universities, with the limited resources available and the large numbers of students and professors they have, forces us to assign priority to the real problems together with their relevant constraints. in fact, the computational time and power should no longer be the focus for this type of problems, where solutions are not instantly needed. even if the developed algorithms take relatively longer time, they would at least be capable of the following: (i) responding to a real problem with its constraints, (ii) addressing big size problem instances and (iii) taking less time than the manual process because constructing timetables starts a long time before they are actually needed [cit] . this paper presents a genetic algorithm for solving a university course-timetabling problem. the studied case is of the faculty of commerce, alexandria university, egypt, where building undergraduate semester timetables start six weeks before the academic semester begins. this paper is organized as follows: section 2 reviews the related work on the timetabling and space allocation problems. section 3 describes the contributed solution methodology. testing and results are presented in section 4. section 5 includes the conclusion and future work and an acknowledgment is made in last section."
"where fin s is a fin flag from the server and rst s is the rst flag from the server. data s is the data transfer from the server and rst c is the rst flag from the client. \"Â¬\" represents \"not\", \"â¨\" represents \"or\" and \"â§\" represents \"and\" as logical signs. hence, although eligible connections represent those terminated by the client, they do not tell whether the connection is terminated by the user or not. the eligible connections could be terminated by the client-side software when the data is already transferred, and the server is idle, waiting for the time-out. in order to identify the connections interrupted by the users during a data transfer, the authors also consider the connection termination time. the interruption criterion is thus expressed by: (2) where is the time elapsed between the last data segment from the server and the actual flow end. and are the mean and the standard deviation of the rtt per connection, respectively. hence, if a connection is eligible and is less than one rtt time, then the connection is said to be interrupted by the user."
"the estimation of Î² is based on a glm. we denote the fmri time series from an experiment as y 2 r n t ï¿½n v, with n t being the number of time points and n v the number of voxels. the glm assumes that"
"the british cem research centre has developed a number of monitoring systems for various stages of the united kingdom's schooling system, of which the cea chose two: pips (performance indicators in primary schools), implemented at the beginning of primary school, and midyis (middle years information system) implemented at the beginning of the secondary school. pips and midyis were chosen because of the lack of monitoring systems in south africa that focused specifically on the beginning of the primary and secondary school. pips was renamed the south african monitoring system for primary schools project (samp) for the south african context and midyis, the south african secondary school information system (sassis). the initial funding was earmarked for validation of the monitoring systems, which took place over a period of four years."
one of the most important problems that arise in classification trees is to understand where to terminate the nodes. pruning is a technique used to reduce the size of decision trees by removing sections of the tree that provide little power to classify instances. the dual goal of pruning is reduced complexity of the final classifier as well as better predictive accuracy by the reduction of overfitting and removal of sections of a classifier that may be based on noisy or erroneous data.
"server-side rst flag: while using the symbian platform, a large ratio of tcp connections were terminated with one or more rst flags from the client, followed by a fin flag from the client and then a rst flag from server. the reason why the server responded with a rst flag is that when it received a rst flag from client, it assumed the connection was already closed and therefore, when it received an additional packet from client (containing a fin flag) on the same port, it responded with a rst flag to once again signal the end of the connection."
future research will focus on applying uga with flow optimization considerations inside campuses. this means that it is better to allocate consecutive events for a group to the same room or to rooms that are close to each other to avoid logistical problems inside academic institutions. another avenue of research that seems very promising is the use of emerging technologies in order to be able to report actual numbers of students attending lectures and to avoid predicted numbers in calculating utilization rates. attendance patterns vary a lot in egyptian public universities and the number of attendees needs to be tracked in order to be able to calculate accurate utilization rates. revealing such information will help dynamically improving the initially constructed timetables and freeing unneeded resources.
"developments made in the area of timetabling resulted in launching the practice and theory of automated timetabling (patat) series of conferences, which sponsors the international timetabling competition (itc) [cit] . this competition aims at encouraging research in the area of timetabling to bridge the gap between theory and practice in real-world applications. this paper proposes a novel approach to tackle a real world big size timetabling problem. due to the complexity of the timetabling problems, a wide range of heuristics is used to find feasible solutions [cit] . this paper contributes to a genetic algorithm approach that allocates teaching events to spaces and timeslots. it uses a number of heuristics to find initial feasible solutions and it considers the following set of hard and soft constraints: or equals to 0%."
"in order to make fair comparison with brsa which considers temporal auto-correlation in noise, all the point estimates ofÎ² by other methods in fig 3 were performed with restricted maximum likelihood estimation, which model the auto-correlation in noise. ar(1) parameters of each voxel were estimated after initial regular regression. the ar(1) parameters were used to re-compute the temporal noise covariance matrices for each voxel andÎ² were calculated again assuming these noise covariance matrices. to account for task-irrelevant time courses in the data, extra nuisance regressors were included in all methods based on point estimates of activity patterns. these included legendre polynomial functions of volume index, up to the fourth order, to model slow drift of fmri signals, and the first three principal components of the fmri data in each of white matter and ventricles, to capture intrinsic fluctuations. the principal components were also included as nuisance regressors for brsa. when spatial whitening ofÎ² was performed, residuals of fitting from all runs of data from the same simulated subject were concatenated in time. one covariance matrix s spatial across voxels was then estimated from these residuals using the optimal shrinkage method [cit] implemented in scikitlearn [cit] . this covariance matrix was then used to whiten the estimated pattern of all the runs. that is,Î²Ï Ã  1 2 spatial of each run were treated as the spatially whitened pattern estimates. when performing within-run rsa, the estimated patterns of each run (being spatially whitened or not) were averaged over the simulated runs before subjecting to a pearson correlation. when performing cross-run rsa, the correlations were calculated between the estimated patterns corresponding to any two conditions from two different runs. this calculation was repeated for each combination of two runs in the data. and finally all the correlation coefficients between the two conditions calculated over all pairs of runs were averaged (e.g., 12 cross-run similarity matrix would be calculated from 4 runs of data and averaged to generate one matrix)."
"monitoring of the transmission control protocol (tcp) connection terminations on the web is one of the ways to monitor user-perceived performance degradation of a service [cit] . normally, users press the stop or reload button in the web browser to abort an on-going transfer, when it is much slower than their expectations. these interruptions result in early termination of the tcp connections with a reset (rst) flag from the client side. these rst flags can be monitored passively on the network-level to observe the user behavior."
"this projection appears to show clear grouping of the states in the orbitofrontal cortex consistent with the 3 categories, suggesting that this brain area represent this aspect of the task. however, a similar representational structure was also observed in other rois. in addition, when we applied the same glm to randomly generated white noise and performed rsa on the resulting parameter estimates, the similarity matrix closely resembled the result found in the real fmri data (fig 1c) . since there is no task-related activity in the white noise, the structure obtained from white noise is clearly spurious and must reflect a bias introduced by the analysis. in fact, we found that the off-diagonal structure obtained from white noise (fig 1c) explained 84 Â± 12% of the variance of the off-diagonals obtained from real data (fig 1b) . this shows that the bias introduced by traditional rsa can dominate the result, masking the real representational structure in the data."
"the bar charts in figures 3-5 highlight the number of each terminating sequence observed as a consequence of different user actions performed in different web browsers. on symbian and android platforms, all the tcp connections ended with a proper fin handshake. after the data transfer, a fin from the server is sent which is followed by a fin from the client-side to end the connection. this type of termination follows the rules as described by the standards [cit] ."
"from table 3, both occupancy rates and frequency rates are included in the calculation of the fitness function because they are used to calculate utilization rates. however, the weight assigned to the events with best occupancy rates (equals to 4) is greater than the frequency rates (equals to 3), because the efficient utilization of spaces is more important than the frequent usage of rooms. finding gaps between two lectures for a professor or a group is given a penalty of two. finally, assigning more than two lectures per day for a professor or a group is given a penalty of 1. the factors and the weights can be changed if the level of significance of any of the factors changes. this is calculated by the equation mentioned below:"
the expected bias structure when spatial noise correlation exists is difficult to derive. we used (x t x) â1 as a proxy to evaluate the residual bias in the estimated similarity using brsa.
"feedback session logistics also emerged as an important finding. the majority of participants expressed satisfaction with the current arrangements in terms of the timing of the feedback sessions, the venue and the directions to the venue. whilst some principals indicated that they would like feedback sessions to occur earlier in the day, educators expressed concern that such a move would mean educators would not be able to attend. no [do not move feedback sessions earlier], we as educators cannot be there so early, our first responsibility is to the children in our classes. this may be possible for the principal to attend. (school 19, afrikaans, round 2). although most schools were comfortable with the centrality of the current venue at the university of pretoria for feedback, some indicated that it was a long journey. suggestions were made for having more than one session focusing on the particular regions, possibly hosted by the participating schools. individual feedback for each school was also suggested."
"to help understand this observation, we provide an analytic derivation of the bias with a few simplifying assumptions [cit] . the calculation of the sample correlation ofÎ² in traditional rsa implies the implicit assumption that an underlying covariance structure exists that describes the distribution of Î², and the activity profile of each voxel is one sample from this distribution. therefore, examining the relation between the covariance ofÎ² and that of true Î² will help us understand the bias in traditional rsa."
"the ray tracing technique is used to simulate the multipath-component channel [cit] . in order to save time, we limited our simulations to the third-order reflection and the diffuse scattering was ignored. in the simulation, we use 3-d model with bounded planes in combination with the image method to compute the rays. both lineof-sight (los) and non-line-of-sight (nlos) are considered. in the nlos case, the direct path representing for the los is removed, which could physically represent a situation where an obstacle hides the transmitter to the receiver. the cir assumed to be temporally static can be modeled as follows"
"the largest enrollment first (le) heuristic deals with the number of groups each professor teaches. accordingly, professors with the greatest number of groups are given priority in assignment. by applying this, the authors are able to narrow the search space and deal with the assignments that might increase the chance of getting an infeasible solution."
"t, m is the number of the subcarriers. the slope is compensated in the cfr of the database and in the online estimated cfr."
"the data for the case tested using uga are presented in table 7 . the same parameters of the manual case were used. however, a five days week was considered as well as shorter working day. the solution obtained used less rooms."
"abstract-time reversal has been shown as a promising technology for the indoor positioning. instead of mitigating the multipath channel, the time reversal based indoor positioning system (trips) exploits the rich multipath propagation in indoor environments as a specific signature for each location. in order to do this, a database should be first constructed by channel probing. the devices used in this process are assumed to have no hardware impairments or be well calibrated. however, the low cost terminal to be located can exhibit impairments, such as i/q imbalance at the front end transmitter known to create an interference image of the signal that can particularly impact the performance of time reversal. in this paper, we investigate the impact of the i/q imbalance on the trips. we analytically show that the i/q imbalance modifies the metric used for the localization and hence reduces the spatial focusing gain of trips. numerical simulations are carried out to evaluate this observation. the results further show that the i/q imbalance creates errors in the positioning estimation. more specifically, at the gain and phase imbalances of respectively 1.1 and 10"
"allocation of spaces inside university campuses is growing more and more important. spaces inside universities include rooms, halls, amphitheatres, offices, parking lots. the increasing number of students flowing every year increases the need for managing and utilizing these spaces. space allocation problems are also np-hard as timetabling problems. there exists p e ways of allocating e events to p places when searching for an optimal solution [cit] . this implies that no efficient algorithm exists to solve large instances of these problems in a reasonable time [cit] . variations in the size of the problem, its constraints and objectives will also affect the time required to perform space allocation while guaranteeing good utilization levels of spaces [cit] ."
"to demonstrate the spurious structure that may appear in the result of traditional rsa, we first performed rsa on the fmri data in one roi, the orbitofrontal cortex, in a previous dataset involving a decision-making task [cit] . the task included 16 different task conditions, or \"states.\" in each state, participants paid attention to one of two overlapping images (face or house) and made judgments about the image in the attended category. the transition between the 16 task states followed the markov chain shown in fig 1a, thus some states often preceded certain other states. the 16 states could be grouped into 3 categories according to the structure of transitions among states (the exact meaning of the states, or the 3 categories, are not important in the context of the discussion here.) we performed traditional rsa on the 16 estimated spatial response patterns corresponding to the 16 task states. to visualize the structure of the neural representation of the task states in the roi, we used multi-dimensional scaling (mds) [cit] to project the 16-dimensional space defined by the distance (1-correlation) between states onto a 3-dimensional space (fig 1b) ."
"in this paper, we proposed a set of criteria to monitor the user actions in the web browser. the monitoring of these actions can provide indications about user reaction to the network performance. these are based on the tcp interruptions and http requests. in order to study whether tcp rst flags could be used to monitor the user behavior on the web, we conducted several experiments with different web browsers. we found out that some web browsers send tcp rst flags without any interruption by the user. therefore, tcp rst flags alone could not be used to monitor the user actions in the web browser. however, tcp rst flags along with the knowledge of http request and response messages can allow us to passively monitor the user-perceived performance."
"two types of tests were performed: uninterrupted and interrupted. in the uninterrupted tests, the user issues a webpage request and then allows the transfer of the webpage to finish completely. in the interrupted tests, the user aborts an ongoing transfer of webpage by performing some action in the web browser. the user action could be either pressing the stop or the reload button, exiting the web browser or clicking a hyperlink on the webpage. these actions are further mentioned in table 1 ."
"note that, whenever the receiver realizes a channel probing request for the localization, it starts estimating the cfr that incorporates a possible channel time delay. we hence introduce the phase rotating vector Ï (âÏ) in (6) to represent the timing offset compensation coming from the fact that the preamble frame is not time synchronized. the timing offset in the time domain will create a linear slope of phase in the frequency domain. instead of using two-dimensional searching for the problem in (6), we use here a linear least square fitting solution to find the angle of this slope as follows"
"the quality perceived by the users is mainly affected by the several network-dependent and application-specific factors. however, there are many other factors that may influence the quality as perceived by the users such as, the prior experiences, expectations, the context of use, etc. users from different geographical backgrounds may have different expectations regarding the service quality, based on previous experiences. a user surfing at work could probably be more intolerant about the bad quality of service, as compared to a user using the service on leisure."
1. no of events with best occupancy rates. 2. no of rooms with best frequency rates. 3. any gap between two lectures per day for a professor or a group.
"an important tool for characterizing the functional architecture of the brain is representational similarity analysis (rsa) [cit] . this classic method first estimates the neural activity patterns from fmri data recorded as participants observe a set of stimuli or experience a set of task conditions, and then calculates the similarity (e.g., by pearson correlation) between each pair of the estimated patterns. the rationale is that if two stimuli are represented with similar codes in a brain region, the spatial patterns of neural activation in that region would be similar when processing these two stimuli. when using pearson correlation as a similarity metric, the activity profile of each voxel to all the task conditions is essentially viewed as one independent sample from a multivariate normal distribution in a space spanned by the experimental conditions, which is characterized by its covariance matrix. recently, it has been pointed out that rsa and two other approaches for understanding neural representational structure, namely encoding model [cit] and pattern component modeling (pcm) [cit], are closely related through the second moment statistics (the covariance matrix) of the true (unknown) activity patterns [cit] ."
"in this paper, we first compare the result of performing traditional rsa on a task-based fmri dataset with the results obtained when performing the same analysis on white noise, to illustrate the severe bias and spurious similarity structure that can result from performing rsa on pattern estimates within imaging runs. by applying task-specific rsa on irrelevant restingstate fmri data, we show that spurious structure also emerges when rsa is performed on the raw fmri pattern rather than estimated task activation patterns. we observed that the spurious structure can be far from a diagonal matrix, and masks any true similarity structure. we then provide an analytic derivation to help understand the source of the bias in traditional rsa."
"english, round 2). other schools expressed a preference for conveying information to the feeder schools and educators themselves as the feeder areas are often diverse. some schools proposed a more interactive format for the feedback sessions. this would mean schools would facilitate some of the presentations themselves. this idea was acceptable to some schools, while others stated that the current discussion sessions allowed for valuable interactions. a third group of schools expressed concern that school facilitation would increase their workload."
"rakesh and gupta [cit] built a university course timetable using a hybrid algorithm. they used the genetic algorithm and iterated local search to avoid getting trapped in local optima. the objective of their approach was to satisfy the set of hard constraints and minimize the violation of the soft constraints. the hard constraints include having students attend one event at any timeslot, assigning all events to suitable spaces with adequate number of seats, and assigning only one event to any one room in any timeslot. the soft constraints were about avoiding scheduling events in the last day slots, avoiding scheduling more than two consecutive events in a day for students, and ensuring scheduling more than one event in a day for all students. their hybrid algorithm did not take into account the utilization rate in the proposed objective function."
"although fig 3 shows that brsa reduces bias, it does not eliminate it completely. this may be due to over-fitting to noise. because it is unlikely that the time course of intrinsic fluctuation x 0 and the design matrix x are perfectly orthogonal, part of the intrinsic fluctuation cannot be distinguished from task-related activity. therefore, the structure of Î² 0, the modulation of intrinsic fluctuation, could also influence the estimatedÃ» when snr is low."
"we see similar termination flag sequences observed with firefox and google chrome. the majority of the connections are terminated with a fin from the server, followed by a fin from the client. however, in many cases, the server sends a rst flag after sending a fin to flag the end of the connection. in these cases, we do not observe any termination flag from the client side."
"in the following subsections, the authors introduce a genetic algorithm by examining its representation, the selection process of chromosomes, its fitness function and the crossover and mutation operators."
"the quality of education has been a recurrent theme in the educational landscape. part of the debate is the role of principals and teachers in making adequate decisions based on performance data of learners [cit] . while different monitoring systems on learner performance may exist within the school, there may also be systems which are external to the school which could be used as a resource. school performance feedback systems (spfs) are information systems, developed external to the school that provide schools with information learner performance. the goal of such feedback systems is to maintain and improve the quality of education by identifying patterns of learner performance in terms of strengths and weaknesses so that revisions to current teaching programmes can be made [cit] . with this in mind, the objective of the research reported in this article was to explore whether the reporting format and feedback sessions within the primary and secondary school monitoring system is appropriate and whether improvements can be made."
"there is no one unified standard method of crossover. it has various types and forms [cit] . some populationbased algorithms do not employ the crossover operator [cit] . the reason behind this may be due to the complexity of its application while keeping the solution feasible [cit] . simply, crossover means; recombining parts from two parent chromosomes in order to form new offsprings that may have better fitness values than the two parents [cit] . a common crossover strategy followed by many researchers is the point crossover method [cit] . in point crossover, a chromosome is split at a point (gene), that is usually randomly selected [cit] and exchanged by the same split part of the second chromosome to swap the two parts together [cit] . crossover techniques also differ according to the representation of the chromosome itself."
"the tree built can take a complex form. the tree is pruned so as to make it simpler without losing the basic functionality. starting from the bottom of the tree, the error is calculated for the linear model at each node. if the error for the linear model at a node is less than the model sub-tree below then the sub-tree for this node is pruned. in the case of missing values in training instances, m5p changes the expected error reduction equation to"
"this section investigates the proposed methodology. it is divided into four subsections. the first is on the description and the evaluation of the case of the faculty of commerce in alexandria university, which is an important real complex problem rich with constraints that motivated the work presented in this paper. the second subsection demonstrates the solution of the problem using the uga contributed in this paper. the third subsection discusses the applicability of the uga to other problems including other soft and hard constraints. lastly, testing uga against benchmark problems, to prove its generality and capability to deal with the hardest timetabling problems, is presented in the last subsection."
"without compensation for the i/q imbalance, the estimated cfr is modified by an independent term that depends on the pilot sequence and on the image of the cfr and its consequence in the time reversal based indoor positioning systems (trips) is studied in the next section."
"higher education is becoming a major driver of economic competitiveness in an increasingly knowledgedriven global economy. as higher education systems grow and diversify, society is increasingly concerned about the quality of courses offered. much attention is given to public assessments and international rankings of higher education institutions. these comparisons tend to overemphasise research, using research performance as a yardstick of institutional value. if these processes fail to address the quality of teaching, it is in part because measuring teaching quality is challenging. institutions may implement different evaluation mechanisms to identify and promote good teachers through which they can achieve a global recognition. to get global recognition institutions should provide quality education and educators to stakeholders. teachers play a major role in building a good institution as well as a student community. therefore it becomes important for the education institutions to identify good teachers to provide better education. teachers can be identified through their educational qualification in the initial stage and can be evaluated through various methodologies for further improvement. teachers can be evaluated using their technical knowledge, teaching ability, communication skills, attitude towards students etc. [cit] explains the key stages in a teacher evaluation cycle represented through the figure 1. the first phase is evaluate the aim and need of the process itself. the second stage is to create an appropriate tool for designing a feedback and then implemented by collecting the data. the collected feedback is analysed by the institution first and given back to teachers to make necessary action if any."
"feedbacks of research results, and the use thereof, are not new concepts and researchers have been grappling with these concepts for decades, especially in the field of evaluation [cit] . feedback on learner performance should be about particular qualities of learners (strengths and weaknesses), learners' work and how the learner can improve [cit] . for feedback of performance data to be effective, both positive and negative aspects need to be highlighted [cit] in order to motivate recipients of feedback to fulfil educational purposes (siebÃ¶rger & [cit] ) . evidence suggests that feedback can be as harmful almost as often as it can be helpful, which can have a substantial effect on the improvement of task performance [cit] . very often it is not the information itself that is of importance, but instead the manner in which such information is mediated and conveyed [cit] . in sum, the impact of feedback depends on the interaction between the feedback message, the nature of the task performed and situational variables [cit] ."
"the global positioning system (gps) has been successful to achieve a good accuracy of the order of meters for outdoor localization. however, the gps generally fail to localize the objects in the indoor environments because the gps signals become too weak after propagation through the concrete walls. with the increase in demands of location-based information services like tourist guidance in a museum, advertisement in the smart market, etc., a high accuracy indoor positioning system (ips) needs to be developed. a lot of researchers investigate the use of the current local area network (lan) infrastructure integrated with the indoor positioning techniques in order to obtain a good ips."
in the original formulation of the two chosen datasets some constraints of the problem studied in this paper were not included. these are as follows:
"the fmri data used in the current manuscript came from two sources. part of them came from a previously published study, which was approved by the princeton institutional review board and all subjects gave informed written consent prior to participation. the other part came from open access data of the human connectome project (https://www. humanconnectome.org/)."
"the paper is divided to three sections viz section ii explaining the background investigation and related work, iii explaining the various applications of data mining techniques in evaluating teachers feedback followed by section iv comprising of result and conclusion."
we assume that the covariance of the multivariate gaussian distribution from which the activity amplitudes Î² k are generated has a scaling factor that depends on its pseudo-snr s k :
"the authors introduce a generic two-dimensional chromosome representation that can easily fit with different university course timetabling problems (see fig. 1 ). a list of all the rooms is presented in the first column. the first row of the chromosome represents all the timeslots. timeslots consist of two main parts: the day and the timeslot order in that day represented in the form of two numeric values. for example, saturdays are assigned number ''1\" because it is the first day in the week. events are scheduled over only 10 h. lecture durations can be of two or three hours. each day consists of a maximum of five timeslots in case only two hours lectures are scheduled and a maximum of three slots in case only three hours lectures are scheduled. the two-hour duration lectures are assigned to a subset of the large rooms. smaller rooms are dedicated to the three-hour duration lectures. this is because large rooms at the faculty of commerce are few and frequently needed. each timeslot is given an id. if a slot id is ''11\", this means it is the first timeslot (from 8.00 am to 11.00 am) on ''saturday\"; an id of ''12\" refers to the second timeslot (11.00 am-2.00 pm) on saturday and so on. the numbering system also continues to represent the two hour duration slots."
"in the proposed methodology, professors whose summed weights were 0.4 or greater are referred to as ''high weight\". population initialization starts with this list of high weight professors, who require priority assignments. this list is then sorted in a descending order based on the weights. if two professors have the same weight, the number of their available timeslots is compared so that the professor with less availability is selected first. if the number of available timeslots for both is the same, their academic ranks are then compared. if the academic rank of one professor is greater than the other, his preference is considered first. if both ranks are the same, the number of groups each professor teaches are then compared using the ''largest enrollment first\" (le) heuristic [cit] . in the case where both share the same number of groups, the age is checked so that the oldest is selected."
"schools indicated that the current content of the feedback session was applicable, but offered suggestions for further improvement. schools who had been participants in the project for a number of years expressed a concern that they were familiar with much of the content of the presentation and that accommodation should be made for such schools. the schools expressed a need for more information on the assessment items so that educators would be better equipped to focus on learner preparation. the idea was mooted with a concern that such action would lead to teaching to the test behaviour. on the other hand the test would not be successful as learners will be prepared [specifically for the test]. (school 23, english, round 2). a key concern from the researchers was that teaching to the test would take place and thus distort the purpose of the monitoring system. however, the aspect of understanding what would be expected of learner is important and thus materials linked to the content covered by the items were developed and given to schools."
"if the covariance structure of the noise s Ïµ were known, the diagonal component of the noise covariance structure assumed in pcm [cit] could be replaced by the bias term (x t x)"
"the aim of conducting interviews with principals was to collect data on what information related to the project that they felt was needed, as well as to probe how the reporting format and feedback sessions could be improved upon. the interview schedule was semi-structured in that although the questions had been formulated and the order determined, the order as well as the questions was able to be modified during the interview, as deemed appropriate. working hand-in-hand individual interviews were the delphi technique."
"when scheduling an event, the search process starts with a list of suggested rooms identified by their capacities so that best fit room capacities are suggested. for example; if the number of attendees in a group is 300, the list of suggested rooms will be those with capacities between 300 and 400 maximum, while, greater capacities (500-600) will not be proposed to ensure efficient resource utilization. when a suitable room is found, the search space is then explored for a timeslot that matches the professor's preference. if none of the suggested rooms is available at any suitable timeslot, greater room capacities can then be proposed. the genetic algorithm proposed is illustrated by the pseudo-code in table 2 ."
"thematic content analysis is an analytical method that makes use of a set of procedures to draw valid inferences from text [cit] or to analyse the content of text where the content refers to words, meanings and themes and where text refers to anything written, visual or spoken [cit] . in this research, thematic content analysis was chosen for the analysis of curriculum documents and interviews because it provides the tools necessary for the chunking and synthesising of data for the creation of a new whole. through this process, interviews and delphi questionnaire data that had been captured verbatim were coded according to different units of meaning [cit] )."
model tree based algorithms are efficient in predicting the parameter leading to the appraisal of the teacher. the efficiency of the algorithms are compared using the mean squared errors. the table 2 explains the efficiency of the pruned rep tree and m5p tree algorithms. table 2 from the table2 it is clear that rep tree performs better than the m5p tree in terms of absolute error as well as the time taken to derive the tree. it is also clear that pruning plays an important role in deriving a tree with lesser time. the chart identifies the difference between both the classification models using the various parameters. it is clear from the chart that pruned rep tree algorithm is better in terms of accuracy and time taken to generate the tree than m5p tree. the algorithms are very efficient in identifying the parameters which plays a vital role and least role in analyzing the learner's response. this identification of parameter will definitely help the teaching community to identify their strength to improve the learner. it will also help the management to identify better teachers and bring out quality education. data mining techniques can also be used in various other domains of education to improve the teaching and learning community.
"for an internet service provider (isp), it is extremely important to monitor and keep track of the service quality as perceived by the user. there are several competitors in the market and a user may easily switch to another service provider as a result of dissatisfaction, taking several other users with her. hence, there is need of a mechanism in order to learn about the user experience over time, in order to provide better services. however, being certain about user experience is a complex task for the following reasons."
"finally, our simulations revealed that brsa is not entirely unbiased, that is, the results cannot be improved indefinitely by adding more subjects. this bias is not a consequence of the number of components estimated by the algorithm we chose [cit] (see part 6 of s1 material), and further investigation is needed to understand the source of this remaining bias. the residual bias occurs when snr is very low and may be due to overfitting of the model to noise. fortunately, the cross-validation procedure we provided helps to detect overfitting when the snr is too low. when this happens, it is advisable to focus on taking measures to improve the design of study. ultimately, task designs that are not fully counterbalanced and low snr in fmri data are two critical factors that cause bias in traditional rsa and impact the power of detecting similarity structure in neural representations. carefully designing tasks that balance the task conditions as much as possible, using different randomized task sequences across runs and across participants, and increasing the number of measurements, are our first line of recommendations. in the analysis phase of the project, one can then use brsa."
"the detailed derivation of the generative model, the model fitting procedure, model selection based on cross-validation and decoding task-related signals are in the parts 1, 2 and 3 of s1 material. here we provide the major assumptions and the formula of the likelihood of fmri data in our model."
"xÎ² instead of Î². intuitively, x temporarily smears the bold patterns of neural responses close in time but Ã°x t 6 x 6 Ã¾ Ã  1 x t 6 only averages the smeared bold patterns without disentangling the smearing.Î² 6 thus mixes the bold activity patterns elicited by all neural events within a time window of approximately 12 sec (the duration of hrf) around the event of interest, causing over-estimation of the similarity between neural patterns of adjacent events. if the order of task conditions is not fully counterbalanced, this method would therefore still introduce into the estimated similarity matrix a bias caused by the structure of the task."
"the bias demonstrated in this paper does not necessarily question the validity of all previous results generated by rsa. it does call for more caution when applying rsa to higher-level brain areas in which snr of fmri is typically low, and when the order of different task conditions cannot be fully counterbalanced. counterbalancing a design could be done at different levels. the lowest level is randomizing the order of task conditions within a run. at higher levels, task sequences can also be randomized across runs and participants. if there are few measurements of each task condition within a run, randomizing trial order only within a run does not guarantee a perfectly counterbalanced design. the bias structure can then deviate from a diagonal matrix. if the same random sequence is used for all runs, or for all participants, small biases can persist across runs and participants and become a confound. therefore, it is also important to use different task sequences across runs and participants when possible. although such counterbalancing at different levels is desirable, it may not be achievable when studying high-level cognition, for instance in decision making tasks that involve learning or structured sequential decisions. these tasks often require (or impose) a specific relationship among conditions and events cannot be randomly shuffled."
"for isps, it is easier to measure the network-dependent factors than asking the each user about her experience subjectively. however, retrieving the application-specific and user-related factors is not simple. it is because isps do not have control on the user-end devices. therefore, conducting subjective experiments in a lab environment, with real users, has been a common practice to model the user-perceived performance. although, they provide control at the user-end, subjective lab experiments have proven to be away from reality [cit] ."
"the delphi technique was conducted through faxes to and from schools. the technique proved more appropriate than the nominal group technique with at least a third of schools in the sample contributing to each round of questioning. a great diversity of ideas was generated and discussed in relation to the feedback sessions, reporting and support for the projects."
"from table 7, it is clear that the number of working days was reduced to five instead of seven. this does not contradict the professors' preferences as professors may have thursdays off or use them in other activities. this also means that there are still two more working days to assign more lectures if needed. after applying and running uga, the utilization rates obtained (see tables 8 and 9) ."
"space utilization rate measures are used to calculate the fitness function of the problem studied in this paper. space utilization rates are used to measure the efficiency in using a certain space relative to its capacity as well as its availability. utilization rates (see eq. (1)) are the product of two other rates called the frequency rate and the occupancy rate [cit] . frequency rates are rates that measure how often a room is used relative to the total number of hours during which it is available (see eq. (2)) [cit] . on the other hand, occupancy rates are rates that measure the extent to which a room is fully occupied relative to its total capacity (see eq. (3)) [cit] ."
the uga contributed in this paper is intended to solve university course timetabling problems where the objective is space utilization optimization. it was applied on the faculty of commerce dataset where the real problem under study was identi- [cit] .
"traditional rsa translates structured noise in estimated activity patterns into spurious similarity structure traditional rsa [cit] first estimates the response amplitudes (Î²) of each voxel in an roi to each task condition, and then calculates the similarity between the estimated spatial response patterns of that roi to each pair of task conditions."
"rst flag from the client side: the internet explorer replies with a rst flag instead of a fin flag, after receiving a fin flag from the server. after the data transfer, the server sends a fin flag, and the client then responds with a rst flag. this behavior indicates the abnormal condition according to the tcp standards. these results indicate that the internet explorer does not seem to follow the standards. these tests with the internet explorer also confirm our observations we discussed in the case of mobile web browsers. the internet explorer in the case of video transfer opens two tcp connections. the first connection is terminated as soon as the video player is requested. the request for the video player is then repeated in the second connection. due to such behavior of internet explorer, we observe a larger number of rst flags in the case of internet explorer as compared to other web browsers as displayed in figure 6 ."
"tcp is a reliable stream-oriented protocol [cit] . before the transfer of the stream of bytes, a virtual connection is established. after the connection establishment between two tcps, a request is made by the client to the server, followed by data transfer from the server. after the end of the data transfer, the connection is closed by going through a proper termination handshake."
"the analysis was facilitated through the use of a computer-aided qualitative data analysis programme, atlas.ti [cit] . atlas.ti allows for the analysis of textual, graphical and audio data [cit] and facilitates the use of direct quotations to enrich the data representation. the use of computer-aided qualitative data analysis is specifically indicated when dealing with large amounts of unstructured textual material, which could cause serious data management problems [cit], p.129) . the tool also provides a platform for making the raw data, audit trail and process notes available, which facilitates trustworthiness of the data."
"our generative model of fmri data follows the general assumption of glm. in addition, we model spatial noise correlation by a few time series x 0 shared across all voxels. the contribution of x 0 to the k-th voxel is Î² 0ï¿½k . thus, for voxel k, we assume that"
"1. a root node that has no incoming edges and zero or more outgoing edges 2. internal nodes, each of which has exacting one incoming edge and one or more outgoing edges 3. leaf or terminal nodes, which has one incoming edge and no outgoing edges."
"in order to characterize the impact of i/q imbalance to the metric of trips, we define a focusing gain reduction (fgr) as the ratio between the localization metrics without (Î¾ wo ) and with (Î¾ w ) i/q imbalance"
"1. rooms might not be available in certain periods, and they must not be suitable for specific lectures. in comparison with the studied case, these are considered hard constraints. 2. rooms have the same capacities. in comparison with the studied case, rooms had different capacities and they have to be matched against the number of attendees in a lecture. 3. teacher preferences on periods and rooms are only included as soft constraints. in comparison with the studied case, preferences for periods are considered hard constraints. table 13 avg. occupancy room rates per days using uga (dataset 1)."
"in fig. 1, intersections between rooms and timeslots represent the potential combinations where events can be assigned. intersections are filled with the event id. event ids are composite attributes that represent three aspects at a time: the group id to be assigned, the professor id involved in teaching this group and the course id the professor teaches to this group. for example, event id ''294\", in the intersecting cell between room ''201\" and timeslot ''12\" represents professor id ''7\" who teaches group id ''33\" the course id ''66\". having more than one event id in a cell like the intersecting cell of room ''503\" and timeslot ''11\" means that a number of event ids share the same course and professor and should be scheduled together. in reality, this corresponds to two or more groups, maybe from different departments, who attend the same lecture. room capacity constraints are respected in this case."
we also characterize the correctness of the positioning estimation under the impacts of both i/q imbalance and awgn by calculating the ratio between the number of position estimation failures and the total number of position estimations.
"mutation is the last step of improving a chromosome [cit] . mutation is similar to crossover except that it makes changes in a chromosome itself. most of the mutation strategies follow a random selection approach (e.g. roulette wheel) to make modifications in a chromosome [cit], in which it randomly selects a timeslot and a room for a certain course or lecture to be assigned to."
"in this paper, we demonstrated that bias can arise in the result of representational similarity analysis, a popular method in many recent fmri studies. by analytically deriving the source of the bias with simplifying assumptions, we showed that it is determined by both the timing structure of the experiment design and the correlation structure of the noise (and task-unrelated neural activity) in the data. traditional rsa is based on point estimates of neural activation patterns which unavoidably include high amounts of noise. the task design and noise property induce covariance structure in the noise of the pattern estimates. this structure, in turn, biases the covariance structure of these point estimates, and a bias persists in the similarity matrix. such bias is especially severe when the snr is low and when the order of the task conditions is not fully counterbalanced."
"finally, we characterize the fgr due to the impact of i/q imbalance. the snr is set to 24 db. fig. 6 presents a contour plot of the fgr (in db) for different values of gain and phase imbalances. it is clearly seen that the fgr increases when the values of i/q imbalances become bigger, implying the positioning estimation errors also increase. this result suggests that the i/q imbalance should be compensated for in the trips system in order to ensure the correctness of the positioning estimation. the i/q imbalance estimation and compensation can be integrated into the online channel estimation phase, using the fact that the training sequence is known beforehand. a lot of studies on i/q imbalance compensation have been reported in the literature, i.e., [cit], however, the optimum i/q imbalance compensation algorithm for trips in terms of the complexity and its effectiveness is left for future study."
"university timetabling is a hard to solve problem, especially, in large universities. constructing timetables is an important task that consumes time and effort of the involved personnel. this paper reviewed some of the work related to timetabling and space allocation problems. it was obvious that different evolutionary algorithms were developed aiming at reducing the computational time required to solve these problems without considering much of the real-world constraints. it was also reported that genetic algorithms have proven success in solving many timetabling problems. in this work, a utilization-based genetic algorithm is proposed to solve a real course timetabling problem with a number of soft and hard constraints including professors' preferences, which is considered a novel contribution overlooked by the previous literature. a simple weighted sum formula is used to prioritize professors according to their availabilities. the authors developed a new utilization-based crossover type that aims at reducing the number of underutilized/over-utilized events in a timetable. moreover, a mutation operator that targets under/over-utilized events was integrated with a simple descent local search heuristic to improve the solution. the fitness function developed in this work considers utilization rates along with other factors to evaluate the fitness of chromosomes. weights are assigned to each of the factors used in calculating the fitness value based on each factor's level of significance to the institution constructing the timetable. the case study used to test the algorithm was of the faculty of commerce, alexandria university in egypt. this case study was chosen because of its big problem size and the limited resources available. a comparison between the timetable generated by the old manual approach and the timetable generated using uga was made to highlight the number of resources saved. the results showed that applying uga enhanced the occupancy rates of the allocated events and saved many resources. moreover, to prove the robustness of the developed algorithm, it was tested against two medium and big size benchmark datasets. results of the testing showed that uga took less computational time for solving the big size problem and slightly more time was required with the medium sized benchmark dataset. however, testing the two datasets was not on their original simplified formulations since the constraints defined in the faculty of commerce case study were incorporated. this shows that uga outperforms the previously contributed approaches to solve these problems even with slightly more computational time for the medium sized dataset. the overall performance of uga with the constraints elaborated in the paper and the objective proposed prove that it is an effective tool for generating timetables in big universities."
"genetic algorithms were chosen to solve the problem of this paper because they are known for their robustness [cit] in solving complex combinatorial problems. genetic algorithms are characterized by their flexibility and ability to search in complex, large spaces [cit] . they are considered one of the most powerful tools in solving course and exam timetabling problems [cit] . moreover, genetic algorithms have the advantage of being adaptive search algorithms [cit] ."
"room/day sat (% 0 507 72 33 50 67 67 83 601 83 83 100 91 94 83 603 0 0 0 0 0 0 605 0 0 0 0 0 0 705 0 0 0 0 0 0 707 0 0 0 0 0 0 91 91 93 96 93 93 93 92 0 0 0 0 0 0 93 93 95 96 100 100 100 94 0 0 0 0 0 0 95 0 0 0 0 0 0 1 100 83 100 75 100 0 2 0 0 0 0 0 0 3 0 0 0 0 0 0 4 0 0 0 0 0 0 5 0 0 0 0 0 0 6 8 3 8 3 8 3 8 3 8 3 0 7 100 100 100 100 100 100 8 0 0 0 0 0 0 in order to maintain the same level of problem complexity when testing the algorithm, the authors added these removed features as hard constraints. additionally, they assumed that all rooms are available in all the allowed time periods. the maximum number of students in a lecture was set to be 600 when testing with these datasets because nothing was mentioned about the number of students in the original formulation of the benchmark problems. the authors did not assume that rooms are of similar capacities, but rather used the different capacities of the faculty of commerce's spaces, which makes the problem harder to solve and requires more computational time. the proposed composite fitness function that focuses on the utilization rates of spaces was used to evaluate the solution quality. details about solving the twobenchmark datasets are shown in tables 12-16 ."
"sutar and bichkar [cit] introduced a genetic algorithm to solve a real university timetabling problem in india. an initial population is randomly generated and parents are selected for crossover based on their fitness values. all the produced offsprings from crossover are subject to mutation; however, the crossover and mutation operators' implementation applied were not clear in their work. in the set of hard and soft constraints, learning space capacities were not mentioned. lectures' timeslot availabilities were not prioritized but minimum and maximum limits of weekly working hours for each were set."
"to address the aforementioned shortcomings, we propose a set of criteria. the set of criteria are specified by a finite state machine diagram in figure 9 . there are two types of transitions shown in the diagram in figure 9 . one is the tcp flow transition triggered mainly by the tcp control flags. it is shown with the solid black arrows in the diagram. another type of transition is the http request transition, which is initiated each time by the arrival of a new http request from the client side. http request transitions are represented with dotted arrows in the diagram. additionally, there is one more transition shown in the diagram, which takes both tcp and http into the idle state. this transition is shown by dashed arrow. the machine comes out of this state when the user requests for the next web page from the same web server. we will first describe the tcp flow states in detail, followed by the description of the http transitions. state 5. interrupted: if the rst flag from the client is seen when the data transfer from the server is still going on (shown by transition 5a), then the connection is called interrupted. however, there are two types of such interruptions: one made by the client-side web browser and another one by the user. it is not possible to differentiate between both of them only by observing the tcp flow. let's recall the video page download case, which was performed on the ie web browser. we observed that the client terminates the data transfer each time after it requests for the video player. in this case, although the tcp connection termination met the interruption criteria, the transfer is not interrupted by the user. in order to identify the tcp connections interrupted by the user and not by the browser, we need to take into account the http request and response messages. for instance, if a tcp connection interruption is followed by the arrival of the last http request, then it shows that the tcp connection is interrupted automatically by the web browser (see internet explorer case with the video transfer). conversely, if the tcp interruption is followed by either of the transitions 2e, 2f and 6b then the connection was interrupted by the user and not by the browser. when a transfer is reloaded by user, request for the last base file h b is repeated. the transition 2e shows that the web transfer is reloaded by the user. similarly, when a user follows a new link or a bookmark (link-follow), before the completion of the previous transfer, clients sends the http message containing the request for a completely new file. the transition 2f shows this link-follow behavior. finally, if the user interrupts a transfer by pressing the stop button or by killing the web browser, then it does not immediately trigger any new http request message but followed by the user think time (the time user takes before launching the new request for a page). it is illustrated by the transition 6b in the state diagram. state 6. idle/user think time: let's recall the on-off model of the web transfer described previously in figure 1, where each web transfer is followed by inactive off time. this state could be reached by either of the two possibilities. first, if a transfer is completed without any interruption by the user, the user takes some time for reading the page or thinking about the next link before launching the request for the new page. second, when the user interrupts the previous transfer by pressing the stop button or killing the web browser, then it takes a period of silence time before the user requests for a new page."
"in order to study the impact of the content type, three web pages were developed. one webpage had simple text, the second one had an image and the third one had a flash video, played in a shockwave player on the webpage. since the results of the tests with text and image web pages were almost similar to each other, we only present the results related to text and to video in the remainder of this paper."
"the next finding focuses on the process, namely that feedback sessions be open to educators involved in the preparation of learners for primary and secondary schooling in the feeder areas."
"in her report ''the black holes of space economics\", shove [cit] has pointed out the interdependent and conflicting interests of students, lecturers, timetablers and administrators. she highlighted that each has his own interest in utilizing spaces and his own definition of what effective space management means. shove explained that overestimating the need for more spaces is a rational response to uncertainty. she confirmed that this is the case when space is being viewed as a free good in the absence of a proper plan and vision to target good utilization levels. poor space allocation can force the decision maker to provision unneeded resources. when solving the space allocation problem in higher education, the different spaces available, the different uses of each space, the timeslots in which it is available and all other constraints need to be identified. in academic institutions, manual approaches to solve resource allocation problems may result in wasting a large number of resources that could be better managed and utilized."
"similarity is then calculated between rows ofÎ². for instance, one measure of similarity that is frequently used is pearson correlation. the similarity between patterns of condition i and j is assessed as"
"it was also suggested that the reports note the revised national curriculum outcomes to show the link between the curriculum and the skills assessed. additional information could also be included in the reports higlighting trends across the various schools as well as additional demographic characteristics of schools such as number of home-language learners, pre-school attendance of learners and number of repeating learners."
"the studied case is of the faculty of commerce-alexandria university, egypt, where building undergraduate semester timetables starts six weeks before the academic semester begins. personnel build the timetable based on a semester course plan communicated by the academic departments. there are four main divisions at the bachelor level inside the faculty: the arabic (ar) division, the affiliated arabic division (aa), where students are evaluated using different methods, the english (e) division and the french (f) division. each division can study in seven specialties starting from the third academic year in a four-year based education system where the academic year consists of two semesters. [cit], the number of professors involved in teaching was 131 and the number of courses taught was 150 (including courses taught in the three different languages). thirty-two learning spaces were available and 337 events were considered to construct a timetable over a seven-day week. the academic week starts on saturdays and the number of lecturing hours per day is 12 from 8:00 a.m to 8:00 p.m. the faculty builds the timetable centrally for the seven departments and all the student groups. the faculty of commerce is one with a very large number of students (between 10 and 2650 [cit] ) and a total of 57 groups (see table 4 ). hard constraints include assigning professors to timeslots of their preferences, respecting room capacities, avoiding lectures' conflicts for professors and groups, excluding fridays and end of day timeslots, and assigning all the lectures to the timetable. professors' preferences are collected from professors before the semester begins. the set of soft constraints includes respecting a maximum number of lectures per professor and per group in one day, decreasing gaps between lectures for a professor and for a group and targeting either no less than 75% or 0% occupancy rate for each allocated event. the objective pursued in the paper is reflected in the fitness function that considers the soft constraints together with the best frequency rates for a room per day. occupancy and frequency rates are defined in section 3.3."
"the prototype, in the context of this discussion, is the report provided to schools, based on the performance data and the feedback sessions. the feedback sessions were arranged in consultation with the participating schools. the sessions normally took one and a half to two hours and included a presentation of the project, the assessment and the results overall. the schools were given an opportunity to discuss the research and ask questions, which is seen as a participatory process. because of the rich information received, the sessions were recorded. at the feedback session, the schools were also provided with comprehensive reports which are tailored to their specific school."
"the need for informed decision-making has been on the increase especially in light of the fact that schools are becoming more autonomous [cit] . furthermore, research on the use of performance data of learners has shown that this can be an effective mechanism to improve learner outcomes [cit] . apart from the relevance on the classroom-level, data-driven decision making is proving to be an effective management tool on the school-level and beyond. the reason for this is that data-driven decision making implies the collection, analysis and interpretation of data to inform practice and policy within educational settings [cit] ."
"client-side rst flag: on the windows platform, however, the text-based webpage transfer is finished with a fin from server, followed by a fin and then a rst flag from the client. this rst flag appears to be a reaction of the client to the ack received from the server, which triggers the client to immediately shutdown the connection by sending a rst flag. this behavior is found to be consistent in all the transfers."
"network operators and web service providers can use this knowledge to passively monitor the behavior of users over time, and manage their resources accordingly to guarantee high-quality user experience. the research community working on network quality of experience can use this study to validate it against the subjective experiments with real users. finally, web users can also use results to choose web browsers that are operating according to the rules defined by the standards."
"during the algorithm-testing phase, the authors discovered that the fitness values of some of the generated timetables were the same while the table structure of one could be better than the other. then, the authors decided to assign weights to the factors they consider more significant and to calculate a corresponding weighted sum. the weights assigned to the factors are shown in table 3 ."
"a useful conceptualisation on how data should be provided is that the data has to speak to a measurable attribute, and different reference levels need to be included for stakeholders to make sense of the data. information on different levels and years of administration need to be provided to the stakeholders and should be followed by a discussion on the discrepancies in years of administration which may be present. what is clear is that there has to be an additional step for researchers to engage in and this step entails what interventions can be put in place. it is important to identify strengths and weaknesses in learner abilities but it is vital to provide information and guidance on how the weaknesses can be addressed and how the strengths can be built upon. the impact of the data provided to schools and the use thereof is dependent on the ability to engage in complex behaviour tasks and this has to be facilitated with care in order to obtain the buy-in of stakeholders as well as their commitment and collaboration."
"average pooling aggregation. the fully connected fc6 layer from the original vgg-m is replaced by two layers Ã  a fully connected layer of 9 Â£ 1 (support in the frequency domain), and an aggregation layer Ã  global average pooling along the temporal axis. the benefit of this modification is that the network becomes invariant to temporal position but not frequency, which is desirable for speech, but not for images. it also helps to keep the output dimensions the same as those of the original fully connected layer, and reduces the number of network parameters (for our given input size this reduction is fivefold, i.e., from 319m in vgg-m to 67m in our network) which helps avoid overfitting. table 6 vgg style architecture. the data size on the right is the output data size for each layer. here we assume input spectrograms of size 512 Â£ 300, and up to fc6 the sizes have been calculated for an input with a temporal dimension of 300, but the network is able to accept inputs of variable lengths. note that the first layer also has zero padding. netvlad aggregation the cnn trunk architecture maps the input spectrogram to frame-level descriptors, as described in the thin-resnet shown in table 7, the output feature is downsampled by a factor of 32. the netvlad layer then takes dense descriptors as input and produces a single k Â£ d matrix v, where k refers to the number of chosen cluster, and d refers to the dimensionality of each cluster. concretely, the matrix of descriptors v is computed using the following equation:"
"in this section we describe our neural embedding system, called vggvox. our aim is to move from techniques that require traditional hand-crafted features, to a cnn architecture that can train end-to-end for the task of speaker recognition. the system is trained on short-term magnitude spectrograms extracted directly from raw audio segments, with no other pre-processing. a deep neural network trunk architecture is used to extract frame level features, and the features are aggregated to obtain utterance-level speaker embeddings. the entire model is then trained end-to-end."
"we use 2d cnns as feature extractors and treat 2d spectrograms as single-channel images. it is perhaps unnatural to treat spectrograms in this manner where the same convolution is used at every point since, unlike in a visual image where an object may appear at any location, a pattern can appear at any point on the time axis but we would not expect patterns to also be frequency independent. however, deep networks can potentially learn frequency-specific filters if they are needed for solving a downstream task; for instance, some filters can only fire on specific patterns existing in the low frequency region, whilst fully connected layers can be position dependent. therefore, even if a 2d cnn uses shared filters on the spectrogram, the model has the capability to divide the filters into low/high frequency groups."
"comparing with the baseline methods that are based on traditional methods, e.g., gmm-ubm, i-vectors+plda, achieving 15.0% eer and 8.8% eer on the standard voxceleb1 testing set, respectively, most of the neural networks (nn) based methods have shown clear advantages, for instance, one of our earliest vgg-m models [cit] trained with softmax and contrastive has outperformed the traditional methods (obtaining 7.8% eer)."
"datasets obtained from multi-speaker environments include those from recorded meeting data [cit], or from audio broadcasts [cit] . these datasets usually contain audio samples under less controlled conditions. some datasets contain artificial degradation in an attempt to mimic real-world noise, such as those developed using the timit dataset [cit] : ntimit, (transmitting timit recordings through a telephone handset) and ctimit, (passing timit files through cellular telephone circuits). table 1 summarises existing speaker identification datasets. besides lacking real-world conditions, to the best of our knowledge, most of these datasets have been collected with great manual effort, other than [cit] which was obtained by mapping subtitles and transcripts to broadcast data."
"pre-training for contrastive loss. our first strategy is to use softmax pre-training to initialise the weights of the network. the cross entropy loss produces more stable convergence than the contrastive loss, possibly because softmax training is not impacted by the difficulty of pairs when using the contrastive loss. to evaluate the identification performance, we create a held-out validation test which consists of all the speech segments from a single video for each identity."
"we released the dataset in two stages, as voxceleb1 and voxceleb2. voxceleb1 contains over 100,000 utterances for 1251 celebrities, while voxceleb2 contains over 1 million utterances for over 6000 celebrities extracted from videos uploaded to you-tube. the datasets are fairly gender balanced, (voxceleb1 Ã  55% male, voxceleb2 Ã  61% male). the speakers span a wide range of different ethnicities, accents, professions and ages. videos included in the dataset are shot in a large number of challenging visual and auditory environments. these include interviews from red carpets, outdoor stadiums and indoor studios, speeches given to large audiences, excerpts from professionally shot multimedia, and even crude videos shot on hand-held devices. crucially, all are degraded with real-world noise, consisting of background chatter, laughter, overlapping speech, room acoustics, and there is a range in the quality of recording equipment and channel noise. we also provide face detections and face-tracks for the speakers in the dataset, and the face images are similarly 'in the wild', with variations in pose (including profiles), lighting, image quality and motion blur. table 2 gives the general statistics, and fig. 1 shows examples of cropped faces as well as utterance length, gender and nationality distributions."
"this section describes our multi-stage approach for collecting a large speaker recognition dataset, starting from youtube videos. our pipeline involves obtaining videos from youtube; performing active speaker verification using a two-stream table 4 overlap between development and test sets for voxceleb1, voxceleb2 and sitw. n refers to there definitely being no overlap, y refers to the possibility of overlap between the sets."
"in order to ensure that our system is extremely confident that a person is speaking (section 4.4), and that they have been correctly identified (section 4.5) without any manual interference, we set very conservative thresholds in order to minimise the number of false positives. this conservative threshold allows us to operate in a high precision low recall regime. the large number of videos downloaded initially allows us to discard many, and only keep the ones with extremely high confidence. precisionrecall curves for both tasks on their respective benchmark datasets [cit] are shown in fig. 3, and the values at the operating point are given in table 5 . employing these thresholds ensures that although we discard a lot of the downloaded videos, we can be reasonably certain that the dataset has few labelling errors. since voxceleb2 is designed primarily as a training-only dataset, the thresholds are less strict compared to those used to compile voxceleb1, so that fewer videos are discarded."
"we use short-term magnitude spectrograms as input to our deep cnn architecture. mean and variance normalisation is performed on every frequency bin of the spectrum. no other speech-specific preprocessing (e.g., silence removal, voice activity detection, or removal of unvoiced speech) is used. precise implementation details are provided in section 6.3."
"this technique is also used in video summarization. first, the input video is split into frames. each frame is converted into gray color image. then each frame is transformed using discrete cosine transform. the difference between two adjacent frames is computed. finally, pixels of interest (Æ¥) and key frames are computed."
"speaker recognition under noisy and unconstrained conditions is an extremely challenging task. applications of speaker recognition vary from authentication in high-security systems and forensic tests, to searching for persons in large corpora of speech data. all such tasks require high speaker recognition performance under 'real-world' conditions. this is extremely difficult due to both extrinsic and intrinsic variations; extrinsic variations include background chatter and music, laughter, reverberation, channel and microphone effects; while intrinsic variations are factors inherent to the speakers themselves such as age, accent, emotion, intonation and manner of speaking, amongst others [cit] ."
the histogram of an image represents the relative frequency of occurrence of the various gray levels in the image [cit] . an image histogram is type of histogram which acts as a graphical representation of the tonal distribution in a digital image [cit] . it plots the number of pixels for each tonal value. the existing histogram based video summarization (h-vs) splits video into frames. each frame is converted into gray image. then histogram is generated for each frame. the difference between two adjacent frames based on pixel distribution is computed. pixels of interest (Æ¥) are calculated using the threshold filter value. key frames with maximum Æ¥ values are identified.
"the goal of this stage is to determine the audio-video synchronisation between mouth motion and speech in a video in order to determine which (if any) visible face is the speaker. this is done by using 'syncnet', a two-stream cnn described in zisserman (2016b, 2018) which estimates the correlation between the audio track and the mouth motion of the video. for voxce-leb2, the syncnet model is replaced with a multi-view variant, so that talking faces can be detected even when the face is off-frontal. this method is able to reject the clips that contain dubbing or voice-over."
"for the sake of comparison, we also implement some traditional non-cnn methods and train them on the voxceleb1 dev set. gmm-ubm. the gmm-ubm system uses mfccs of dimension 13 as input. cepstral mean and variance normalisation (cmvn) is applied on the features. using the conventional gmm-ubm framework, a single speaker-independent universal background model (ubm) of 1024 mixture components is trained for 10 iterations from the training data."
"datasets. many existing datasets are obtained under controlled conditions, for example: forensic data intercepted by police officials [cit], data from telephone calls [cit], speech recorded live in high quality environments such as acoustic laboratories [cit], or speech recorded from mobile devices [cit] . [cit] consists of more natural speech but has been manually processed to remove extraneous noises and crosstalk. all the above datasets are also obtained from single-speaker environments, and are free from audience noise and overlapping speech."
"related models are compared with our proposed esi and 670 npb algorithms from some properties in in the following simulation experiments, the number of 707 cloud providers is varied in the range of 10 to 100. table 4 708 lists the entire system parameters and the corresponding rithms. the parameters are outlined in table 5 . the experi- a certain number of iteration. in fig. 8, it is shown that the"
"on the challenging voxceleb1-h test set, we outperform the previous best architecture (eer of 5.17% vs. 7.33%), which is by a larger margin than on the original voxceleb1 test set. we note that training a softmax loss based on features from temporal average pooling (tap) yields extremely poor results (eer of 10.48%). we conjecture that the features trained using a softmax loss are typically good at separating different speakers), but not good at reducing the intra-class variation (i.e., making features of the same speaker compact). therefore, contrastive loss with online hard sample mining leads to a significant performance boost, as demonstrated in for tap."
"a siamese network is constructed from two standard classification models, i.e. two branches share the same network and parameters (thinresnet is fixed until conv4_x, refer to table 7 ) that have been pretrained for speaker classification based on standard softmax, and the small relation module is then trained to distinguish if two voice samples are from same identity or not (binary classifier, implemented as a softmax with two classes). a separate netvlad/ghostvlad aggregator is incorporated for the classification and relation network paths. as most of the feature extractor are fixed, the relation module only costs a very limited additional computation. during inference the output scores of a voice pair is computed as the average of the cosine similarity (between feature embeddings) and the classification score (from the small relation module)."
the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
"dev/test splits are given in table 3 . for clarity, we also provide a summary of the possible overlap between the developement and test sets of voxceleb1, voxceleb2 and sitw in table 4 . this is useful for researchers wishing to train on one of these datasets and test on another."
"we take the model pre-trained on the identification task, and replace the classification layer with a fully connected layer of output dimension 512. this network is then trained with the contrastive loss."
"we experiment with different trunk architectures, aggregation strategies as well as training losses. we describe the trunk and aggregation architectures here, and the losses in section 6.2."
"the first stage is to obtain a list of speakers. voxceleb1. we start from the list of people that appear in the vggface1 dataset [cit], which is based on an intersection of the most searched names in the freebase knowledge graph, and the internet movie data base (imdb). this list contains 2622 identities, ranging from actors and sportspeople to entrepreneurs, of which approximately half are male and the other half female."
"hard voxceleb1-h test set Ã  within the same nationality and gender.this is a 'hard' evaluation set consisting of 552,536 pairs with the same nationality and gender, sampled from the entire voxceleb1 dataset. there are 18 nationality-gender combinations each with at least 5 individuals, of which 'usa-male' is the most common."
"where, c is a Æ¥ in the domain [1 to 10]. the datacube contains ten numbers of adjacent frames. soâs' value ranges from 1 to 10. if the s value increases then the number of key frames of the video will also decrease. so s value is proportionally inverse into the number of key frames in the video summary. the output contains key frames of a video clip with all events of interest of the input video. it also contains small movement of water animal such that crab which is not possible with the existing dwt. this modified dwt algorithm can easily and quickly detect key frames."
"active speaker face tracks are then classified into whether they are of the speaker or not using the vggface and vggface2 cnns for voxceleb1 and voxceleb2, respectively. verification is done by directly comparing the cosine similarity of the face embedding from the pretrained networks Ã  the face classification networks have been trained on images of the same set of speakers (the vggface cnn is trained on the vggface image dataset, and voxceleb1 starts from the same list of speakers, similarly for vggface2)."
"where, n is number of frames in video in the final step, apply local maxima and local minima (ÄºÄ¼) to select key frames. fig.3 . demonstrates local maxima and local minima concepts clearly [cit] . local maxima and local minima may not be the minimum or maximum for the whole function, but locally it is. first, the interval values of the function are determined. let âf' be a function defined on an interval (a,b) and let âp' be a point in (a,b) . the height of the function at p is greater than or equal to the height anywhere else in that interval. it is called as the local maximum value of the function. the local minimum at p if f(p) is less than or equal to the values of f. the f(p) should be inside the interval, not at one end or the other."
"where cr is the compression ratio, and t keyframes is number of key frames in the result video and t is the number of frames in the original video."
"we experiment with both vgg [cit] and resnet style cnn architectures. vgg-m: [cit] . this architecture is a modification of the vgg-m [cit] cnn, known for high efficiency and good performance on image classification. the modification concerns the addition of an aggregation layer, and is described below. the complete cnn architecture is specified in table 6 . fig. 3 . precision-recall curves for the active speaker verification (using a 25-frame window) and the face verification steps, tested on standard benchmark datasets [cit] . operating points are shown in circles. resnets: the residual-network (resnet) architecture [cit] b ) is similar to a standard multi-layer cnn, but with added skip connections such that the layers add residuals to an identity mapping on the channel outputs. in this paper, we experiment with three variants of resnets, e.g., resnet-34, resnet-50 and a thin-resnet which contains fewer parameters. we modify the layers to adapt to the spectrogram input. the architectures are specified in table 7 ."
"whilst our models are based on 2d convolutions applied to spectrogram inputs, further work will involve investigating alternatives that may be more efficient, such as 1d time convolutions with the frequencies of the spectrogram arranged as input channels, or 1d convolutions applied to raw waveforms directly."
"baseline: here we use variable average pooling where we evaluate the entire test utterance at once, by changing the size of an average pooling layer during test time according to the length of the test sample;"
"following the continuous development of new architectures in computer vision, we also experiment with different trunk architectures, ranging from vgg to resnet. in this comparison, we fix all the experimental settings and only vary the network architecture, i.e., we compare three models (vgg-m, resnet-34, resnet-50) trained with temporal average pooling (tap), and softmax+contrastive loss on voxceleb2. evaluation is done on the standard voxceleb1 test set without any test-time augmentation. similar to the observations found in computer vision research, deeper networks lead to better generalization, therefore, resnet-50 (4.19% eer) outperforms the resnet-34 (5.04% eer) and vgg-m (5.94% eer), despite the fact that the vgg-m model uses a higher dimensional embedding (1024d)."
"in this work, five underwater videos of various sizes are taken as input. these are downloaded from youtube table 1 . it shows the number of frames (n) and target frames (t) in the input video file. the metrics including number of detected frames (d f ), detected target frames (d t ), false negative ratio (f n ), compression ratio (c r ) and computation time (c t ) are computed. it shall be noted that the proposed work identified desired key frames. the number of detected frames (d f ) increases with the increase in input file size. the target frames (t) of all five videos are manually identified by the user to watch the video frame by frame. the target frames are shown in the fig.4 . three target frames are identified for fishnew.mp4, fishmp1.avi and fishmov12.avi. six and seven target frames are identified for dataset6.mp4 and sanimal.avi. fig.5 shows the detected target frames (d t ) of the proposed method to all five video files. proposed method gives two detected target frames for fishnew.mp4 file and fishmov12.avi file. it presents all desirable target frames for fishmp1.avi, dataset6.mp4 and sanimal.mp4 file. the percentage of target frames detections are 67%, 100%, 67%, 100% and 100% respectively for the video fishnew.mp4, fishmp1.avi, fishmov12.avi, dataset6.amp4 and sanimal.avi files."
"traditional methods.for a long time, speaker identification was dominated by gaussian mixture models (gmms) trained on low dimensional feature vectors [cit] . the state of the art in more recent times involves both the use of joint factor analysis (jfa) based methods which model speaker and channel subspaces separately [cit], and i-vectors which attempt to model both subspaces into a single compact, low-dimensional space [cit] . these methods rely on a low dimensional representation of the audio input, such as mel frequency cepstrum coefficients (mfccs). however, not only does the performance of mfccs degrade rapidly in real-world noise [cit], but by focusing only on the overall spectral envelope of short frames, mfccs may be lacking in speaker-discriminating features (such as pitch information). [cit] ."
"this section describes our experimental setup for speaker verification, loss functions, baselines, and implementation details. along with releasing the voxceleb dataset, we also release a number of different evaluation benchmarks for testing speaker verification. these have been used extensively by the speech community to compare methods. in particular, we provide both easy pairs and hard pairs for testing; for the hard pairs, speakers with the same nationality and gender are chosen which makes distinguishing between them more challenging. this is described in more detail in the next section."
"voxceleb2. the cnn face detector based on the single shot multibox detector (ssd) [cit] ) is used to detect face appearances on every frame of the video. this detector is a distinct improvement over [cit], allowing the detection of faces in profile and extreme poses."
"a caveat of using youtube as a source for videos is that often the same video (or a section of a video) can be uploaded twice, albeit with different urls. duplicates are identified and removed as follows: each speech segment is represented by a 1024d [cit] as a feature extractor. the euclidean distance is computed between all pairs of features from the same speaker. if any two speech segments have a distance smaller than a very conservative threshold (of 0.1), then the speech segments are deemed to be identical, and one is removed. this method will certainly identify all exact duplicates, and in practice we find that it also succeeds in identifying near-duplicates, e.g., speech segments of the same source that are differently trimmed."
"elsdsr [cit] clean speech @ 22 198 mit mobile [cit] mobile devices Ã  88 7884 swb [cit] telephony Ã  3114 33,039 polycost [cit] telephony Ã  133 1285z icsi meeting corpus [cit] meetings Ã  53 922 forensic comparison [cit] telephony @ 552 1264 andosl [cit] clean speech Ã  204 33,900 timit [cit] y clean speech Ã  630 6300 mgb challenge dataset [cit] broadcast data ** unknown 1600 hs sitw [cit] multi-media @ 299 2800 [cit] clean speech Ã  2000+ * voxceleb1"
"the top 50 or 100 videos for each of the speakers are automatically downloaded using youtube search for voxceleb1 and voxceleb2, respectively. the word 'interview' is appended to the name of the speaker in search queries to increase the likelihood that the videos contain an instance of the speaker speaking, and to filter out sports or music videos. no other filtering is done at this stage."
"for both datasets, the shot boundaries are detected by comparing colour histograms across consecutive frames. within each detected shot, face detections are grouped together into face tracks using a position-based tracker. this stage is closely related to the tracking pipeline of chung and zisserman (2016a) [cit], but optimised to reduce run-time given the very large number of videos to process."
"since voxceleb1 is intended to be used as a test set for speaker verification, the data is checked manually for any errors. this is done using a simple web-based tool that shows all video segments for each identity. [cit] and our own model trained on voxceleb2, respectively, and those with lower confidence are highlighted with a different colour. by running this check, we discovered around 300 label errors, which account for around 0.2% of the voxceleb1 data."
"the implementation was done using the programming language matlab. the performance of the proposed work is analyzed using the metrics such as false negative, compression ratio and processing time. the main aim of this work is to detect all events of interest in the input video and to eliminate all redundant frames. to achieve this, it is desired to get a minimized false negative ratio, compression ratio and processing time. the false negative (f n ) is defined as (7) where f n denotes the original event of interest (measured in terms of frames) that is not included in the result and t is the number of frames in the original video. compression ratio is computed by dividing the number of key frames in the result by the number of frames in the original video."
"as a fair comparison to our original model [cit], we only train the relation module on the voxceleb2 dataset. however, as explored in previous sections, we expect that incorporating both voxceleb1 and voxceleb2 can further boost the performance of all of our models. overall, our thin-resnet with a ghostvlad layer and a relation module currently holds the state-of-the-art result on the voxceleb1 dataset (table 10) ."
"in this paper we have introduced a scalable method to automatically generate a speaker recognition dataset, and used it to produce the voxceleb1 and voxceleb2 datasets, which are several times larger than any other speaker recognition dataset. these datasets have become a standard for the speech community to train and evaluate speaker recognition performance on. [cit], introducing theÃ¢vast partitionÃ¢ in sre18, comprising the voxceleb and sitw datasets, represents a 'new initiative towards speaker recognition in the wildÃ¢, since 'a signature feature of the vast partition is multi-speaker conversation with considerable background noise.Ã¢ [cit] . we believe that the use of these datasets in challenges has allowed a paradigm shift in speaker recognition efforts in the community, encouraging the development of systems under noisy and 'in-the-wild' conditions."
"originalvoxceleb1test set.the original verification test set from voxceleb1 consists of 40 speakers. all speakers with names starting with 'e' are reserved for testing, since this gives a good balance of male and female speakers."
"synchronization convolutional neural network (cnn), and confirming the identity of the speaker using cnn based facial recognition. using this fully automated pipeline, we have obtained over a million utterances for thousands of different speakers. the overall pipeline is the same for both voxceleb1 and voxceleb2, but the methods used in key stages differ since we selected the state-of-the-art face recognition systems at the time of dataset curation. the pipeline is summarised in fig. 2, and key stages are discussed in the following subsections:"
"mining hard examples. a key challenge associated with learning embeddings via the contrastive loss is that as the dataset gets larger, the number of possible pairs grows quadratically. in such a scenario, the network rapidly learns to correctly map the easy examples, and hard negative mining is often required to improve performance to provide the network with a more useful learning signal. we use an offline hard negative mining strategy, which allows us to select harder negatives (e.g. top 1-percent of randomly generated pairs) than is possible with online (in-batch) hard negative mining methods [cit] limited by the batch size. we do not mine hard positives, since false positive pairs are much more likely to occur than false negative pairs in a random sample (due to possible label noise on the face verification), and these label errors will lead to poor learning dynamics. while training the relation module, a similar strategy is applied, we pre-compute the feature embeddings for all the voice samples in the entire voxceleb2 dataset. in addition to negative pairs, we mine both hard positive and negative pairs for training relation modules."
"video abstraction is a short summary of the original video. this is widely used in video cataloging, indexing and retrieving. shot, scene, frame and video are the important terms in video processing. video is collection or sequence of frames. frame represents a picture image. shot represents sequence of frames in a single camera operation. scene is a collection of consecutive shots that have semantic similarity in object, person, space and lame. there are two types of video abstraction, video summary and video skimming. video summary, also called a still abstract, is a set of salient images (key frames) selected or reconstructed from an original video sequence. video skimming, also called moving abstract, is a collection of image sequences along with the corresponding audio from an original video sequence. video skimming is also called preview of an original video, and can be classified into two sub-types: highlight and summary sequence. a highlight contains the most interesting and attractive parts of a video, while a summary sequence renders the impression of the content of an entire video [cit] ."
"nationality labels are crawled from wikipedia for all the celebrities in the dataset. we crawl for country of citizenship, and not ethnicity, as this is often more indicative of accent. in total, nationality labels are obtained for all but 428 speakers, who were labelled as unknown. speakers in the dataset were found to hail from 36 nationalities for voxceleb1 and 145 for voxceleb2. the voxceleb2 is a far more ethnically diverse dataset, with a smaller percentage of u.s. speakers (29% in voxceleb2 compared to 64% in voxceleb1)."
"we have also introduced new architectures and training strategies for the task of speaker verification. our learnt identity embeddings are compact (512d) and hence easy to store and useful for other tasks such as diarisation and retrieval. the relation module, also introduced in this paper, has been shown to outperform all previous models by a significant margin on the voxce-leb1 dataset."
"we next explore different aggregation methods and loss functions in this section. once again, other experimental settings are fixed, for instance, we train the same thin-resnet on voxceleb2 and only vary the aggregation strategy (tap, netvlad, ghostvlad) and training loss (softmax vs. amsoftmax). as shown in table 9, the thin-resnet trained with standard softmax loss and netvlad aggregation layer outperforms the previous model by a significant margin (eer of 3.57% vs 4.19%). the fact that the thin-resnet is actually shallower than the resnet-50 (table 7), and contain fewer number of parameters, further illustrates the benefits of the netvlad aggregation layer. by replacing the standard softmax with the additive margin softmax (am-softmax), a further performance gain is achieved (3.32% eer). the ghostvlad layer, which excludes irrelevant information from the aggregation, additionally makes a modest contribution to performance (3.22% eer)."
"extended voxceleb1-e test set Ã  using the entire dataset.since the above test set is limited in the number of speakers, there is a danger that models achieving high performance on this test set might not generalise to other sets of speakers. hence we also propose a larger test set of 581,480 random pairs sampled from the entire voxceleb1 dataset, covering 1251 speakers."
"typically, such systems are trained end-to-end for classification with a softmax loss [cit] or one of its variants, such as the angular softmax [cit] c) . in some cases, the network is further trained for verification using the contrastive loss [cit] or other metric learning losses such as the triplet loss . similarity metrics like the cosine similarity or plda are often adopted to generate a final pairwise score."
"discrete wavelet transform (dwt) decomposes each video frame into four sub band images with different properties as shown in fig. 1 . among them ll corresponds to a smooth version of the original image. hl, lh and hh are called detail coefficients [cit] . any sudden change in the original image will affect these three sub bands. this work is implemented in four steps. in the first step, two successive frames are read and transformed using dwt. the hl, lh and hh sub-bands are used to detect key frame. for each sub-band, difference between the current frame and the next frame is calculated. in the second step, mean and standard deviation are computed from the difference vectors. then the threshold value for each sub-band is calculated by adding the mean and standard deviation. in the final step, the threshold and difference values of each band are compared. if the difference value exceeds the threshold then the second frame is considered as a key frame."
"deep convolutional neural networks (henceforth, cnns) have given rise to substantial improvements in speech recognition, computer vision and related fields due to their ability to deal with real-world, noisy datasets without the need for handcrafted features [cit] . one of the most important ingredients for the success of such methods, however, is the availability of large training datasets."
"in this section, we show all the evaluation results on three publicly available test sets created from voxceleb1, i.e. voxceleb1 test-set, voxceleb1-e, voxceleb1-h. discussions of our main observations from these experiments are included, e.g., benefits from the end-to-end trained cnn, size of training data, network architecture, different loss functions, and choice of aggregation strategy. we also compare performance of the cnn architectures to a number of other deep learning methods and more traditional state of the art methods."
"in this paper, we focus on the price bidding mechanism for 91 cloud providers resource provision competition from the 92 perspective of non-cooperative game. our main contribu-93 tions are listed as follows: 94 with the perspective of non-cooperative game, a 95 mechanism of pricing strategy for resource provision 96 is constructed to maximize the profits of both the 97 cloud customers and service providers. 98 regarding the quantity of the resource provision 99 from each provider as a fraction to get continuous 100 benefit functions, we prove the existence of nash 101 equilibrium solution for the proposed game model. 102 an esi algorithm is proposed to compute the nash"
the methods are evaluated on a number of different test sets. these are described below and summarised in table 8 . all test set lists can be found on the voxceleb website 3 .
"the second contribution is a deep cnn based neural speaker verification system, named vggvox, which is trained to map voice spectrograms to a compact embedding space. we then use the cosine distance between vectors in this embedding space to measure the similarity between speakers. besides speaker recognition and verification, clustering and novel speaker discovery can be straightforwardly implemented using standard techniques, with our embeddings as features. in developing vggvox we investigate current popular cnn architectures, e.g. variants of vgg-m [cit] and resnet [cit] ), different aggregation strategies, e.g. global average pooling, netvlad [cit], ghostvlad [cit], and different loss functions for training the model, e.g. standard softmax classification, large-margin softmax and the contrastive loss. our methods achieve state-of-the-art performance on the voxceleb1 speaker verification task, outperforming all other traditional methods and recent deep learning methods. [cit] into a single coherent document. in addition, we have added new results based on a new relation module, and added a more detailed comparison to previous work and the discussion of results."
"voxceleb2. the list of candidate names are drawn from the vggface2 dataset [cit], which has greater ethnic diversity compared to vggface1. this list contains over 9000 identities, ranging from actors and sportspeople to politicians. there are a number of overlapping identities between vggface1 and vggface2 Ã  these are excluded from the development set of voxce-leb2, so that any models trained on voxceleb2 can be tested on voxceleb1."
"features produced by the trunk cnn architecture are then aggregated in time to produce a single fixed length representation for each audio input. we experiment with two aggregation strategies: simple non-trainable average pooling, as well as a trainable aggregation layer based on the netvlad layer. here we provide a brief overview of both the average pooling aggregation layer, and also the netvlad [cit] ."
"unfortunately, large-scale public datasets in the field of speaker identification with unconstrained speech samples are lacking. while large-scale evaluations are held regularly by the national institute of standards in technology (nist), these datasets are not freely available to the research community. the only freely available dataset curated from multimedia is the speakers in the wild (sitw) dataset [cit], which contains speech samples of 299 speakers across unconstrained or 'wild' conditions. this is a valuable dataset, as the speech samples have been manually annotated, however, scaling it further, for example to thousands of speakers across tens of thousands of utterances, would require the use of a service such as amazon mechanical turk (amt). in the computer vision community amt like services have been used to produce very large-scale datasets, such as imagenet [cit] ."
"during training, we randomly sample segments from each utterance. for the vgg based model, we use 3-s long segments with a 1024 fft giving us spectrograms of size 512 Â£ 300, and for the thin-resnet model we use 512 point ffts giving us spectograms of size 257 Â£ 250 (frequency Â£ temporal). earlier models (resnet34 and resnet50) are trained using the deep learning toolbox matconvnet [cit], and the latest models (thin-resnet) are in keras (tensorflow). 4 the models and training code are publically available. 5 the model is trained using a fixed size spectrogram corresponding to a 2.5 s interval. all audio is first converted to single-channel, 16-bit streams at a 16khz sampling rate for consistency. spectrograms are then generated in a sliding window fashion using a hamming window of width 25ms and step 10ms. we normalise the spectrogram by subtracting the mean and dividing by the standard deviation of all frequency components in a single time step. no voice activity detection (vad), or automatic silence removal is applied. we use the adam optimizer with an initial learning rate of 0.001, and decrease the learning rate by a factor of 10 after every 36 epochs until convergence. fig. 4 . relation module. the relation module is added to a siamese network in training and inference. during training, two sets of aggregation modules are used: v 1 and v 2 are computed from a shared netvlad/ghostvlad and trained for identity classification; v 1l and v 2l are computed from a shared netvlad/ghostvlad and used to train the relation classifier for identity matching."
"in this paper, key frame is detected using modified dwt in underwater video. the rest of the paper is organized as follows. section ii describes the concepts behind the existing video summarization techniques and section iii discusses the design of the proposed video summarization technique. in section iv, experimental results and analysis are presented. finally, concluding remarks are furnished in section v."
"this work is the extension of dwt based summarization [cit] . first, the input video is split into adjacent datacubes. then dwt is applied to each datacube and statistical features are extracted. this result is used to select pixels of interest in each frame in the datacube. key frames are identified by local maxima and local minima. the proposed work outperforms the existing dwt method in terms of identifying all events of interest in the input videos. fig. 2 depicts the overall diagram of the proposed transform based video summarization."
"dwt-vs provides only one key frame but it is also undesirable. hist-vs and dct-vs give one desirable target frame and two desirable target frames. so it is clearly demonstrated that, the proposed work gives better result as compared to the other existing methods. table. 2 illustrates the performance comparison of the proposed work for a video with fast moving animals. the video file fishmp1.avi with 42 frames is considered for comparative analysis. from table.2 it shall be noted that the percentage of the detected target frames (d t ) is 100% for the proposed work. dwt-vs detects no target frames (0%). h-vs detects 33% and dct-vs detects 67% of the target frames. similarly the proposed work clearly identifies even the smallest water animal. the video file sanimal.avi with 195 frames contains a small frog which is correctly detected by the proposed work. table. 3 depicts the detection of smallest animal movement. proposed method detects all desirable target frames. dwt-vs and hist-vs detect six and five target frames. dct-vs detects only two target frames. it shall be showed that the target frame detection percentage is 100% for the proposed work. whereas, the percentage of target frames detections are 86%, 71% & 29% respectively for the existing dwt-vs, hist-vs and dct-vs techniques. the graph in fig.7 depicts that the average false negative ratio (f n ) for the proposed work is less in comparison with that of the other existing works. the graph in fig.8 clearly demonstrates that the proposed work consumes less computation time in comparison with that of the other existing works. compression ratio represents the number of key frames in the video summary. it is directly connected with theâs' value in the local maxima and local minima. if the s value increases then the compression ratio of the video will also decreases. so âs' value is proportionally inverse into the compression ratio. based on the experimental results, the value of s is eight. it gives less number of key frames and the key frames should be target frames. the graph in fig.9 clearly demonstrates that the proposed method has less compression ratio in comparison with that of the other existing works. a new threshold based statistical video summarization scheme has been designed and developed for the automatic detection of events of interest in underwater video. due to automated process this scheme easily detects smallest water animals which are not possible with the existing schemes. also the role of statistical analysis enables the proposed scheme to detect even fast moving animals efficiently. in future, we will do research for summary of specific events of interest such as particular fish motion activity, fish availability, etc. the decimation techniques will reduce video processing time. so, another future step would be to apply decimation techniques to reduce video processing time."
"evaluation metric. we evaluate the models with equal error rate (eer) and the minimum detection cost function (mindcf). eer measures the value at which the false-reject (miss) rate equals the false-accept (false-alarm) rate, and mindcf is defined as a weighted sum of false-reject and false-accept error probabilities. these are common metrics used by existing datasets and challenges, such as nist sre12 [cit] ."
"we make two contributions towards the goal of speaker recognition under noisy and unconstrained conditions. the first contribution is to propose a fully automated and scalable pipeline for creating a large-scale 'real-world' speaker identification dataset. benefiting from the success of face recognition research in computer vision, our method circumvents the need for human annotation completely. we use this method to curate voxceleb 2, a large-scale dataset with over a million utterances for over seven thousand speakers. since the dataset is collected 'in the wild', the speech segments are corrupted with real-world noise including laughter, cross-talk, channel effects, music and other sounds. the dataset is also multilingual, with speech from speakers of 145 different nationalities, covering a wide range of accents, ages, ethnicities and languages. [cit] ), speech separation [cit], cross-modal transfer from face to voice or vice versa [cit] b;, emotion recognition [cit] and training face recognition from video to complement existing face recognition datasets [cit] . [cit], the dataset has already been downloaded over 3000 times."
"both datasets contain development and test sets, with disjoint speakers. the development set of voxceleb2 has no overlap with the identities in the voxceleb1 or sitw datasets. since we have created a number of evaluation benchmarks using the vox-celeb1 dataset for testing (section 6.1), we encourage others to use the development set of voxceleb2 only to train models for the speaker recognition task (sections 5Ã 7) so that they can evaluate their methods fairly on voxceleb1. the voxceleb2 test set should prove useful for other applications of audio-visual learning for which the dataset might be used. the statistics for all the table 1 comparison of existing speaker identification datasets. cond.: acoustic conditions; utter.: approximate number of utterances. y and its derivatives. z number of telephone calls. * varies by year. ** only available to participants of the challenge. this dataset was mainly used for speech recognition (asr)."
"where p(Â·) denotes the probability. given the channel knowledge at the receiver side, the performance of signal detection (or classification) is dominated by two factors: 1) the white gaussian noise v, and 2) the uncertainty of cfos. the former has been extensively investigated, and the latter is the problem of interest."
"the following hidden layer and the output layer are jointly responsible for mud. basically, the mud procedure follows the map principle in (4) . note that every mud component is independent to each other and able to detect all users' signal i.e., the q th mud component detects all users' signals and outputs the recovered signal in the form ofx"
"in the literature, the cfo estimation and mui-cancellation problems are often considered together. most of the muicancellation approaches assume the availability of cfos estimates at the users' side (i.e., the transmitters), which facilitates mui cancellation at the receiver through the use of transmitterside cfo pre-compensation (i.e., [cit] . such trades off feedback overhead and point-to-point communication latency for the mui-cancellation complexity. however, this approach is not suitable for 5g or beyond communication systems which are demanding to communication latency and feedback overhead (i.e., [cit] ). given the receiver-side cfo knowledge, it is trivial to take the maximum-likelihood (ml) approach aiming at minimizing the following euclidean distance"
"although there is lack of rigorous proof due to mathematical intractability, the similar result has been reported in other experimental works [cit] . hence, we can summarize the observation in previous works into the following statement proposition 2. suppose: c1) âÏ is sufficiently small; and c2) the relationship (10) holds. there exists a unique combination of g and s given z, i.e.,"
"our experimental results show that the dnn determines the cfo range based on a certain voting mechanism, and this is where the diversity gain comes from. however, the voting algorithm exploited by the dnn is hard to be observed; and this could be a piece of interesting future work. as far as fading channels are concerned, the multiuser diversity gain appears to be less significant because the fading channel effect corrupts the amplitude and phase of the received signals, and this could confuse the cfo classifier. as illustrated in fig. 4, the multiuser diversity gain appears from 12 db and saturates at 20 db. different from the phenomenon shown in the awgn channel, increasing the number of users does not show a remarkable contribution to the cfo classification accuracy in the rayleigh fading channel, i.e., nearly identical probabilities of misclassification observed in both 4-user and 8-user scenarios throughout the snrs."
"v. conclusion in this paper, a novel unsupervised deep learning approach has been presented to handle the blind multiuser frequency synchronization problem in ofdma uplink communications. the proposed approach replaced the conventional cfo estimation and compensation procedure with a novel cfo classification and multiuser detection procedure. it has been shown that the dnn-based cfo classifier and mud components can be trained separately, and they can operate together to offer a remarkable mud performance. moreover, it is found that the cfo classifier can exploit the multiuser diversity to improve the cfo classification performance."
"it is perhaps worth noting that the dnn-based cfo classifier is able to identify the cfo range for ofdm signals in the presence of cps, but not for those having their cps removed, which means that it was actually exploiting the correlation between cps and ofdm symbols for the cfo classification. due to the channel effect, the cfo classification accuracy is significantly degraded. fig. 5 illustrates the histogram of the 4-ue cfo classification result for a specific cfo sub-range, i.e., Ï 4 . it is observed that nearly 57% of the classifications correctly fall into Ï 4 while nearly 38% are dropt into the adjacent subr-ranges, i.e., Ï 3 and Ï 5 . unideal cfo classification may lead to the incorrect assignment of the corresponding mud component to conduct the mui cancellation and detection. in order to ensure the correct mud component to be assigned, a parallel-mud (pmud) scheme based on the existing cat-mud structure is proposed, where numbers of mud components is activated simultaneously. for example, based on the classified cfo sub-range Ï 4, the corresponding mud component for Ï 4 and its adjacent mud components responsible for Ï 3 and Ï 5 are simultaneously activated to work in parallel. by doing so, the correct mud can be guaranteed with a probability of 95%."
"where Ï is the user specific carrier-frequency-offset (cfo) normalized by the symbol duration, and l cp is the length of cyclic prefix (cp); the superscripts [Â·] t, [Â·] h stand for matrix/vector transpose and hermitian transpose, respectively. note that the linear model (1) describes the cp-removed version of the ofdma signal to facilitate our discussion. given the channel knowledge h, â, the aim is to determine x with minimum detection errors."
"it is worthwhile to highlight that the multiuser diversity gain comes from the cfo diversity of different users. the users, with their cfos located at the edge of the cfo range Ï q, have higher probability of misclassification, and those, with their cfos at the centre of the cfo range, have much lower probability of misclassification."
where Î³ represents the linear transform in the first linear layer and Î» â1 is the channel inverse matrix with its diagonal submatrix entries defined by
"this problem has been extensively studied in the last two decades, with many results already reported in the literature. however, such seemingly saturated problem faces new challenges arising from emerging and future wireless networks. specifically, the multiuser frequency synchronization problem involves two sub-problems. one is the estimation of multiuser cfos, and the other is the multiuser detection (mud) or multiuser interference (mui) cancellation given the estimates of cfos. the former has been well investigated concerning either the best way of using pilots/preambles [cit] or statistical behaviors inherent in signal waveform [cit] . however, the existing cfo estimation approaches face increasing challenges from the continuous growth of the user density; as it becomes increasingly difficult to provide sufficient pilot resources to support more and more users, while the signal's statistical behaviors also become hard to be exploited due to the increasing mui. we argue for the need of novel multiuser-cfo estimators or equivalent approaches."
experiment 3: this experiment aims to evaluate the performance of the proposed cat-mud procedure. the baseline used for comparison is the conventional pic cfocompensation method [cit] . fig. 6 illustrates the user-averaged block-error-rate (bler) performance as a function of e b /n 0 (db) in the ofdma uplink communication over rayleigh fading channels. it is observed that each user has nearly identical performance and only the averaged bler results are shown. the bler is measured for each individual users' signal block. we observed from the result:
"the proposed cat-mud procedure is realized by employing a feed-forward deep neural network (ff-dnn). as depicted in fig. 2, the ff-dnn consists of two functional layers. one is called the cfo classification layer, which plays the role of cfo classification. the other is called the muicancellation layer, which takes the role of joint mud and frequency synchronization. the motivation of employing ff-dnn lies in the fact that:"
"given that the random cfo varies continuously, for those located at the edge of the cfo sub-range i.e., Ï q, inevitably, there exists the non-zero probability with which the cfo classifier drops the received signal into the sub-range i.e., Ï qÂ±1, adjacent to the ideal cfo sub-range. thus, the unideal mud component is assigned to handle the mui-cancellation. it is obvious that this cfo misclassification probability can be minimized by sufficiently enlarging the cfo sub-range âÏ. however, the classification ambiguity in the mud procedure becomes consequently more significant which degrades more the system performance. therefore, âÏ needs to be carefully set up so as to well balance the trade-off between the cfo classification precision and mud reliability. fig. 2 (a) . specifically, it consists of three fully-connected layers. the target of the cfo classifier is to tell the sub-range Ï q where y falls in. note that the block index n in (11) is omitted here since the cfo classifier is able to give the prediction based on only one received block. therefore, the classification procedure follows the simplified map principle.q"
"with f i s representing cumulative distribution functions of inter-event times, change detection under this framework amounts to choosing one of the following competing hypotheses:"
"thus, if departure from stationarity leads to process deterioration, i.e., if events seem to occur more and more frequently in recent times, the t i /t n values will tend to cluster around 1, inflating the value of z b and deflating the value of z . if the process improves, z will be high while z b will be low. ho [cit] examined these versions in the context of a poisson process setting and found situations under which the use of z b in detecting non-stationarity could be advantageous. one such instance is if the intensity of the underlying stochastic process happens to be a series of gradually increasing steps. but there are other situations, notably when the process improves (through maybe a gradually decreasing step intensity) when z b 's statistical power drops in comparison to z 's. noting both versions perform well under certain disjoint conditions and that the goal of the present paper is to detect non-stationarity broadly defined (as opposed to only deteriorating or improving systems), we combine z and z b to propose a new trend ratio statistic defined by"
"noting such drawbacks, this work takes stock of a host of statistics, both parametric and non-parametric, and examines their applicability under a wide array of change scenarios. we have proposed two new ways in which non-stationarity might corrupt a stable flow -a strong way, in which the post change density is deterministically governed and a weak way, in which it is a random mixture of deterministic densities. we have explained the advantages of each and formulated a new trend ratio statistic z tr combining ideas of ordinary trend (when time flows from the left to the right) and reversed trend (when it is switched to the right to the left). to replicate non-homogeneous environments, we have conducted extensive simulations, both under the deteriorating and improving framework, with the changes placed early, midway, and late into the process. to invite further intricacies, within each category, we have generated easily identifiable non-stationarities, when the corrupted parameters are extremely different from the stable parameters, and difficult-to-identify non-stationarities, when they are not. the established cusum, its eight prevalent competitors and our proposal were subsequently employed to address two crucial aspects of unstability research: detection -investigating whether a change happened at all, and estimation -guessing the true time of change, in case it did. this rewarding exercise revealed that in every possible corrupted scenario, at least one of the newer proposals outperforms cusum with regards to both aspects mentioned previously. high sensitivity and specificity values imply efficient detection, and distributions for the estimated change times hovering around the true change, suggest accurate estimation. we have constructed recommendation tables, summarizing the apt proposals, for the ease of applicability and the benefit of practitioners. weak corruptions were then considered, with the non-stationary piece construed as a probabilistic mixture of an improving and a deteriorating system, and the supremacy over cusum was observed to be retained. our newest proposal z tr was found to occupy most of the cells in these recommendation tables, with some shared by other non-cusum options. interestingly, however, z tr was found to be the only undisputed choice in case a suspected change happened late into the process, arguably the hardest detection scenario. such classification power holds practical benefits too since in general, we are more concerned about changes in the immediate future as opposed to ones in the distant past. the rationale behind z tr 's accuracy has also been explained."
3.4. availability and applicability of in vitro-in silico methods to study transporters in the context of an animal-free chemical risk assessment strategy 3.4.1. gap-analysis of available transporter in vitro methods and database information to support animal-free chemical risk assessment
"3.1. country, professional sector and expertise of the participants a total of 73 participants completed the online survey from 21 countries: austria, belgium, canada, croatia, denmark, finland, france, germany, greece, hungary, ireland, italy, netherlands, norway, portugal, singapore, spain, sweden, switzerland, united kingdom, and united states of america (fig. 1) ."
"related to transporters of interest, the participants could choose between the main \"drug transporters\" namely (with gene name): mdr1 also called p-gp (abcb1), bcrp (abcg2), oatps (slcos), oats (slc22a), octs (slc22a), mates (slc47), bsep (abcb11) and mrps (abccs). their relative investigations among participants are represented in fig. 5 . uptake and efflux transporters were equally considered. besides, participants mentioned studying various supplementary atpbinding cassette (abc) and solute carrier (slc) members; such as cholesterol efflux transporter (abca1), abca7 transporter, excitatory amino acid transporter (eaats, slc1a), amino acid transporters (slc7), sodium-taurocholate cotransporting polypeptide (ntcp, slc10a1), monocarboxylate transporters (mcts, scl16), vesicular glutamate transporters (vgluts, slc17a), vesicular monoamine transporter (vmat2, slc18a2), reduced folate carriers (rfcs, slc19a) urate transporters (urats, slc22a2), concentrative nucleoside transporters (cnts, slc28a) and equilibrative nucleoside transporters (ents, slc29a)."
"the rest of the paper is formatted as follows. section two is devoted a review of previous methods and the establishing nine new methods for the change point detection problem. section three is devoted to analyzing the proposed methods on simulated data using two new ways of introducing nonstationarity into an evolving environment, while section four concerns analyzing real data sets. finally, in section five we summarize our conclusions along with future directions."
"thanks to the 73 respondents from different working sectors and with various kinds of expertise, the results reported that transporters are investigated during drug development first, but also for human and environmental risk assessment purposes and in fundamental research. with the exception of few working on basic science, all participants studied transporters for toxicity predictions mainly to support ivive, considering species differences and sensitive population as well as real internal exposure, but also to characterize and design in vitro methods used in toxicology or pharmacology, to support screening and prioritising and to support substances grouping and read across. these results are in line with recent recommendations of how tk data can be used in regulatory decision making [cit] . however, recipients from some regulatory agencies declared not having transporter expertise in house, supporting that transport tk data are not sufficiently considered so far within a regulatory context."
"attention was then turned to harrowing ordeal inhabitants of the hawaiian volcano, kilauea are going through, and the confidence gained through the simulation studies was channelized both to fuel and resolve an ongoing geological debate -whether kilauea and its close neighbor mauna loa are inversely related. the proximity of the estimated change points of these neighbor provides an answer in the affirmative. such statistical validation should aid government authorities to relocate kilauea inhabitants close to mauna loa. lives may be saved and moving costs may be minimized, owing to the geographical closeness. interactions between strong and weak hurricanes originating in the west atlantic basin, striking the us east coast, was then considered and the change detection results found through this study was observed to be in agreement with some of our other studies published recently, using different interaction ideas."
"when questioned about the available information in transporter databases in the context of chemical risk assessment, around 80% of the respondents declared that important data are currently missing and specified which in free-text format. first, several respondents highlighted the need of databases based on environmental chemical datasets (pesticides, industrials, cosmetics, excipientsâ¦) and not only with respect to pharmaceuticals as it is mostly the case currently. besides, the need of publicly accessible databases was also raised. then, quantitative data on kinetics and on transporters abundance were particularly cited. this latter encompasses transporter expression and functionality in the various cell lines used compared to tissues, in different species and in specific diseases. it was stated by many that it would benefit the community to make available the raw in vitro, in silico and in vivo data and to report the negative results. ideally supplemented with the methods or protocols used to ensure high quality of the data. finally, non-mammalian data, in vitro-in vivo scaling factors, substrates specificity, mechanistic data, effects of co-exposure on the internal concentrations and toxicity were also mentioned."
"in a simulation scenario, experimenters have control over the time at which they want to corrupt a stationary process and, subsequently, measure the average difference between the estimates furnished by the competitors and the true change point. in a real data example, however, the ''truth'' is unknown. in this section, therefore, we shall investigate two processes and confirm the change points found by the alternatives proposed here with those generated using different techniques in some of our previous works."
"for a fixed Î±. the average run length (arl) is defined as the mean number of observations scanned before sounding a false alarm. under the null assumption, this can be proved to be 1/Î±, while under the alternate assumption of a change, this quantity should preferably, be high. summarized versions of the above conditional distributions (created using monte carlo simulations) are stored as lookup tables within the cpm package in r."
"has been examined by rigdon and basu [cit], among others. ho [cit] made a slight modification to z, introducing a backward version of it, as"
"these tables have been constructed to store the strongest candidates, defined by detection accuracies (evidenced by high sensitivity and specificity values) and closest average proximity to the red broken line, and may be used as follows: if one wants to guard against a deteriorating process (suggesting events are happening more and more frequently in recent times, which, in turn, could spell disaster, especially with examples studied in the next section) and suspects that the change happened midway into the process with parameters hugely different from the stable one (implying an easy detection), the best two sample statistic d k,n to use in the cpm framework would be the kolmogorov-smirnov one. other conditions remaining the same, if the change is suspected to happen in more recent times (implying a late change), the best option to implement would be the new proposal z tr . it is interesting to seen that this trend ratio option occupies most of these twelve cells. we also implemented one of these recommendations with one isolated simulation shown in figure (6) representing a hard, midway improvement. table (6) prescribes either the energy-divergence or the z tr option. the black steps represent a stationary and stable flow, while the red departure signals an improvement somewhat midway into its evolution. in addition to the evident closeness of the newer candidates (z tr and e-div) to the true time of change and our latest proposal z tr 's better performance, another observation merits mention: we have found in almost all the cases possible, z tr and the cpm-based candidates suggest estimated times of change that are after the true time of change. this is not always true for the established cusum, as figure (6) shows, suggesting its extreme sensitiveness and tendency to sound too many false alarms."
"with the mann-whitney test designed to detect location changes and the mood test to detect scale shifts, a need is often felt to combine the two and create a test efficient for both aspects. lepage-type tests offers (see [cit] ) an alternative by using"
"the survey contained a total of 20 questions including yes/no, multiple choice, and free-text answers type questions. in most of the cases, the respondents could specify their answers in a free-text format. for multiple choice questions, participants could select one or multiple answers. the survey was divided into 4 sections: (1) respondent's profile (country, professional sector and expertise), (2) use of transporter data and knowledge (fields and applications of transporter studies as well as mechanisms, organs/barriers and individual transporters under study), (3) tools and methods used to study transporters (decision-support tools and experimental methods: in vivo, in vitro and in silico) and (4) opinions on the availability and applicability of the nonanimal methods to study transporters, especially for chemical risk assessment and biomedical research. the full questionnaire is available in the supplementary material. responses were aggregated for statistical analysis and individual responses were kept anonymous."
"here Î± represents the usual probability of type-i error, set to 0.05 in this study. we shall next implement each of these change detection options along with an array of realistic synthetic scenarios to discover the best choice under every situation."
"assuming normal-like conditions of the probability distributions of the inter-event times, two sample t or f tests are often used, in case the Î¸ s represent location or scale parameters. in the absence of such knowledge, mann-whitney or mood tests can be employed to detect possible location or scale updates, and other non-parametric options such as lepage, kolmogorov-smirnov or cramer von-misses, to unearth more intricate structural changes. these statistics have been elaborated in section (2.3). in theory, a two sample statistic d k,n is devised, and subsequently studied for signs of large values. this, due to its construction described later, signals dissimilarity between the pre-change sample (those before the kth observation) and the post-change sample (those after the kth observation), and consequently, a drift in the underlying model. a tolerance level h k,n helps to quantify alarmingly large or small values. for ready implementation, a working statistic d n is created by choosing the largest of the d k,n 's:"
"the issue of non-stationarity intrigues several decision making processes on a daily basis, with consequences both profound and mild. examples originate from myriad sources such as industrial engineering, where a production manager might be curious about detecting a change in the quality of the items manufactured, the financial sector, where one might be concerned with the rate of bank failures, or health care, where the prevalence of a rare disease might be the quantity of interest. the current state of art relies on some variant of the cumulative sum (cusum) charting technique, where departure from an insipid flow is usually flagged by a chain of points, documenting the flow, venturing beyond a specified threshold. while the principle sounds alluring and works reasonably well in theory, there exist several strong, often untenable assumptions, perhaps the most striking one being complete knowledge of the pre and post-change parameters, necessary to come up with the bounds. in addition, the time discretization method involved invariably loses information that could offer insights into the time and nature of a change."
"the national oceanic and atmospheric association (noaa) classifies hurricanes based on their maximum wind speeds attained (described in table (8) below) and following is an analysis on whether the frequencies of such storms have changed in recent times. following the recommendations of leading weather scientists like emanuel (2003 emanuel (, 2006 emanuel (, 2007 [cit] and also because of the damage they inflict, we have agreed to define hurricane categories 3 through 5 as the strong group and 1 and 2 as the weak group. data on their past/projected tracks, origination dates, etc. are freely available from the noaa web page. [cit] . over this period and over this region, we have found 32 h5 storms, 84 h4 storms, 87 h3 storms, 93 h2 storms, and 150 h1 storms. the cumulative counts, through steps, have been plotted in figure (13) below. scientists ( [cit] believe that with a continually warming climate, the likelihood of a strong hurricane drops due to a decline in moisture deficit, however, if it gets started somehow, it has the potential to become deadly. thus, although the number of hurricanes might increase in recent times, the proportion of those that are strong should fall."
"table (1) above represents a situation when the difference between the stationary parameter (2) and the non-stationary parameter (0.6) is noticeable. thus, although the change corrupts the process late into its evolution, the difference in parameter values makes it easily detectable, evidenced by the high sensitivity values for most of the competitors. table ( 2) below makes the non-stationary parameter (1.8) closer to the stationary one and records similar results."
"in the simulation results to follow, 10 6 such processes were generated and each was classified by the ten competing tools (described in the previous section) into one of two types -stationary or not. to make our simulations as realistic as possible, we next propose two new ways in which a stable and stationary process might get corrupted."
"the survey was created by the european commission -joint research centre (jrc), eurl ecvam (https://eurl-ecvam.jrc.ec.europa. eu/) as an online questionnaire using the public european tool eu survey (https://ec.europa.eu/eusurvey/home/welcome). the survey was available for the participants from the 24th of january until the 16th [cit] ."
"the answers collected by the eu survey tool were exported to a microsoft excel file for analysis and interpretation. [cit] (microsoft corporation, redmond, wa, usa) and via http://eulerr.co/ for the venn diagrams. geographical distribution of the survey responses was elaborated by means of an interactive infographic using graphical mapping built on google chart api as a html5 and javascript web app by klimeto (http://klimeto.com/). this visualisation approach was chosen as it allows for a simplified and clear illustration of the spread in demographics of the survey participants while keeping their identities and contact information anonymous."
"next, we turn our attention to the case of estimation. this is important since detecting a change (what we have done in the previous part of this section) is only the first hurdle a classifier has to overcome. once a change gets detected (if it gets detected) the next step is to inquire when did it occur. such estimation and comparing those estimates with userdefined change points form the purpose of the last portion of this subsection on strong corruption."
future decisions and plans are made based on the assumptions of an underlying model. when the parameters of the the associate editor coordinating the review of this manuscript and approving it for publication was lei wu.
"finally animal studies were primarily performed in the context of fundamental research by the survey participants. conversely, half of respondents declaring fundamental sciences as their field of interest, mentioned working with non-animal approaches. however, the transition away from animals seems to be less engaged in basic research than in the application-centric toxicological field. considering the widely recognized inter-species differences in transporter expression and activity, human cell-based data implemented into computational model describing human physiology and validated with human biomonitoring data may be more relevant than animal studies. several researchers pointed out that in vitro methods will never be able to replace in vivo studies, especially not to unravel complex pathways involving multi-transporters or transporter-metabolic enzyme interplay. [cit] /63/eu could still be considered, namely the reduction and refinement of the use of animals for scientific purposes. in vitro and in silico tk data as well as imaging can allow to (i) reduce the number of animals used to obtain the same information or with the same number of animals expand the information collected, (ii) refine animal experimentation by moving from severe to mild suffering by favouring non-invasive approaches as an example [cit] . besides, the most commonly used [cit] in vitro models are often reproached to lose their tissue-specific physiology including their transportome profile. the recent fields of 3d cultures and organs-on-chips are proposed by some to have promising potential to improve these issues by offering more complex systems notably with co-cultures [cit] . in addition to the role of transporters in tk, they can also play a role in toxicodynamics. this could be more extensively captured in the context of adverse outcome pathways (aops), which provide a simplified organizational construct linking molecular initiating events and early key events at lower levels of biological organization to disease outcomes. transporters may be part of molecular initiating events or key events, such as in the aop:27 describing cholestatic liver injury induced by inhibition of bsep transporter (aop wiki https://aopwiki.org). as such, aops could facilitate the compilation of information to increase mechanistic understanding of human (patho)physiological pathways [cit] that involve the role of transporters. ultimately, better funding of alternative-based projects would encourage basic researchers to replace, reduce and refine animal studies and applied researchers to adopt more standardised non-animal methods in toxicity testing and regulatory decision-making."
"transporters are expressed at the external barriers (intestinal, dermal and lungs) and major blood-tissue barriers (brain, placenta, testes) as well as at the hepatic and renal excretory organs. there, transporters affect the adme processes of various compounds. whether for drugs or for chemicals, intestinal, hepatic, renal and blood-brain barriers were predominantly considered by participants as shown in fig. 4b . in this survey, brain and lungs were mentioned to be investigated more for drugs, while skin, placenta and testes more for chemicals (fig. 4b) . some participants also mentioned working on the cardiovascular system, eyes, adipose tissues and on fish gills and embryos. altogether, the participants rank-ordered the impact of transporters first on absorption, then on excretion, followed by distribution, and to a lesser extent on metabolism."
"volcanic eruptions can be disastrous situations in terms of the safety of the population, the environment and the local economy. the current situation in hawaii with the continued eruption of the kilauea volcano proves this point. at this writing over 100 acres and 35 structures have been destroyed. the eruption has displaced hundreds of residents. one explosion sent gas and ash over 30,000 feet in the air. [cit] kilauea eruption to estimate the cost in today's dollars, we are looking at about 20 million dollars."
"ultimately, the confidence in unravelling transporter (patho)-physiology without animal studies is low for basic researchers. it was recurrently mentioned that in vitro and in silico models have limited in vivo relevance in that context and cannot mimic complex pathways, such as enterohepatic circulation. lack of funding of alternative-based projects was also expressed as a barrier to encourage scientists to move toward studies not requiring animals."
"this paper reported the current state-of-play and challenges in the application of non-animal (in vitro and in silico) methods to study transporters for chemical risk assessment purposes and in biomedical research. the results provide insights into the future improvements needed to gain regulatory and scientific acceptance, including in vitro methods amelioration and standardization, missing database information as well as uncertainty characterization at the different steps. future guidelines or guidance documents should as a priority contain standardised protocols preceded by an overview of in vitro and in silico tools available and relevant to evaluate active transport. overall, the extensive existing knowledge on drug transporters from the historical interest of the pharmaceutical sector could provide essential pieces of information to support chemical risk assessment and to gain greater confidence in the predictive value of non-animal data."
"this research is still in its infancy, but with its promising prospects, the road ahead looks enticing. we are especially excited about ways of putting bayesian priors on the number of mixture components involved, their biasing intensities, and corruption locations. the sixth author's ph.d. dissertation (forthcoming) [cit] ) offers the relevant intricacies. average run length calculations may be carried out as well and multiple changes may be detected simply by restarting the algorithm. our proposals are not tethered to the suffocating cusum assumptions brought to the fore and provide better change estimates without paying a hefty price in terms of model complexity. their computational simplicity and intuitive appeals should also help them find a way into every modeler's arsenal."
"where h n is chosen to satisfy the level Î± condition, typically as the upper Î± point of the null density of d n ."
"in case the post-change density deterministically switches to a gamma(0.5, 1), the shocks would occur more frequently, represented by the red curve and the process will be deteriorating. similarly, if the post-change density deterministically switches to a gamma(5, 1), the shocks would occur less frequently, represented by the green curve and the process will be improving. on the diagrams in the first row, the jump from a stationary flow to a non-stationary one happens early in the process and as we move to the right, the differences between the pre and post change parameters reduce, thereby making the change detection harder. along the second row, the change happens midway into the process, and along the third row, it happens late into the process. strong corruption is thus, generally characterized by improvement or deterioration lines clearly showing, never deviating to the other side. from a practical viewpoint, strongly corrupted processes would typically have one cause, or more than one that acts in the same direction, that make(s) them non-stationary. the appointment of a new ceo who wants to push sales, for instance, might make the stochastic process of making sales deteriorate (i.e., sales would occur more frequently). both the weibull and gamma densities are reasonable models to work with since they can account for a wide array of skew scenarios and the inter-event times are non-negative, by definition. for our simulations, thus, at some predetermined time, the parameters either improve or deteriorate deterministically (as opposed to randomly, described in the next subsection) and stays that way throughout the rest of the process. this corruption may happen at different points in the history and becomes harder to detect the later it occurs. with this, in the spirit of figure ( 3), we categorize them into early, midway, and late corruptions. early and midway corruptions are easy to identify because deviation from stationarity occurs very soon after the process starts, and has an algorithm has a large portion of the it to pick up a change. late corruptions, by contrast, are harder to detect, as is evidenced in fig (3) above."
"government officials may exploit this dependence in several ways. relocating inhabitants seems the most pressing. moving people from dangerous zones remains crucial. as does doing so in a cost-efficient way. shifting people from the now-active kilauea to a great distance from it might save lives but will be forbidding financially. the inverse dependence established through this change point research might suggest a prudent alternative -shifting them close to mauna loa. it is significantly dormant currently and simultaneously, not a great distance away from kilauea, either, suggesting minimal moving costs."
"in this distribution, the probability density under the null assumption of no change (i.e. stationarity) is often intractable for several popular choices of the two sample statistics d k,n s. hawkins [cit] and pettitt [cit] provide large sample approximations for the t and mann-whitney choice of the d k,n s. asymptotic bounds for a class of other choices may be had from worsley [cit] . the cpm framework detailed later, exploits numerical simulations to estimate the null densities for small sample sizes."
"additionally, it was particularly emphasized here the need to report transporter abundance in cell lines versus tissues and between species in order to address a major gap in the ability to mathematically describe the fate of a chemical within the tissue. besides, abundance data represents an essential characterization of the biological system used to generate in vitro data. [cit] . current work at oecd is ongoing to issue a guidance document on good method in vitro method practices (givimp) where this aspect is stressed (http://www.oecd.org/chemicalsafety/ testing/draft-guidance-review-documents-monographs.htm). in line, several respondents acknowledged that it would be beneficial to share the raw data and many declared being in favour of sharing them."
"we have implemented the ten options described previously on both the strong and weak series with the estimates collected in figure (13) . while some candidates (especially cusum on the weak series) fail to identify any changes at all, our low confidence in those (evidenced through their simulation performances) leads us to turn to the more reliable ones mentioned in the recommendation tables (such as z tr, e-div, etc). we find that for the strong series, most of the candidates (including the newly proposed z tr ) [cit] (the heavy dotted vertical red line) while for the weak series, most (again, including the newly proposed z tr ) [cit] . it is interesting to note that using entirely different modeling techniques (through a statistic termed empirical recurrence rates ratio), in one of our previous works [cit] ) [cit] . the advent of global warming around this time was noted in this work and offered as a possible reason. [cit], 1965 (obtained from the current study) [cit] (obtained from the previous), derived using different strategies, strengthens the conclusion that changes did occur both in the ways the strong and weak hurricane count series propagate individually, and hence in the pattern of their interactions."
"underlying statistical model change, enacting the future plan can have significant consequences. [cit], there was strong statistical evidence suggesting that the rate of mining accidents in the uk was increasing. however, because no action was taken, this resulted in a mining accident that took the lives of over 400 people [cit] . detecting and estimating the changes in parameters of an underlying model is known formally as the change point detection and estimation problem."
"as our first exercise, we have looked at the detection problem, i.e., checking whether a change from stationarity has occurred at all. it it worthwhile to recall that sensitivity and specificity are two established performance indicators of almost any binary classifier. in our context, sensitivity is defined as the probability (estimated through a relative frequency) of correctly identifying a non-stationary process as non-stationary while specificity is taken as the probability of correctly classifying a stationary process as stationary. higher these probabilities, better is the classifier. the shape parameter for the stationary weibull density was held at 2 and the tables below describe the different non-stationarity parameters. since late changes are the most difficult ones to identify, through tables (1) -(4) below, we have recorded performance measures under this condition. we, however, offer summary tables below that takes into account the other possibilities too."
"eruption data related to dates and lava volumes were collected from the united states geological survey (usgs) and the smithsonian institute's global volcanism program, spanning the period from 1750 till the present day. figure ( shows the annual eruption count time series for these volcanoes while figure (11) keeps a record of the cumulative running averages for these yearly counts. the change detection results are shown in fig (12) above. the step graphs track the volcanoes cumulatively -one additional eruption leads to one additional step. the global times on the horizontal axis have been recorded in days. the vertical lines represent different change points obtained through different tools -the blacks for changes in kilauea, while the reds for those in mauna loa. one might notice that two of these vertical lines are heavy while the rest are faded. this aspect represents the strength of our confidence in them. for instance, through the simulation analyses carried out previously, we found that our latest proposal z tr performs the best under the widest array of scenarios. thus if a change point is estimated through this tool, it gets coded through a heavy vertical line. in this instance, however, several good competitors agree on the same change point, which increases our confidence in it even further. it is interesting to note the proximity of the heaviest change points of mauna loa and kilauea. one feels due to such closeness, that a downward slope (implying process improvement, rarer eruptions) from mauna loa almost causes an upward slope (implying process improvement, increased eruptions) for kilauea. [cit] through a different function termed empirical recurrence rates ratio and was explained through the possibility of a connected magma reservoir. thus, the volcanoes fight for the same resource causing ones restlessness imply the other's dormancy. additionally, it is our conjecture that the change points bands will alternate between ''red-black'' and ''black-red''. to clarify, as one traverses from the left to the right on the horizontal axis, there will be a chunk of period where the mauna loa change points will appear first, closely followed by those for kilauea. the next clustering will start with kilauea change points, followed by mauna loa. [cit] . this pattern would not have happened had the two processes been independent of each other in which case the red and the black vertical lines would have been randomly mixed."
"humans and ecosystems are continuously exposed to various environmental chemicals, such as pesticides, manufactured chemicals, cosmetics ingredients or food contaminants. their potential toxicity is of public concern. however, clinical trials are not conducted for pollutants, as compared to drugs, depriving the toxicologists of human in vivo data to rely on. currently, the safety assessment of environmental chemicals for regulatory purposes mainly involves animal testing. however, costs, scientific and ethical concerns have created the need to develop reliable, relevant and economically feasible tools based on alternative (non-animal) approaches. [cit] /63/ [cit] directive 86/609/eec on the protection of animals used for scientific purposes. the aim of the new directive is to anchor in eu legislation the principle of the three rs: replace, reduce and refine the use of animals for scientific purposes. under this directive, the eurl ecvam was established to contribute to the development, validation, and international recognition of alternative methods. [cit] eurl ecvam published a toxicokinetic (tk) strategy proposing kinetics as the cornerstone in an integrative in vitro-in silico risk assessment [cit] . kinetics determine what amount of an external exposure dose of a compound reaches the systemic circulation and the target organ(s) by providing essential information on the adme processes [cit] . tk here defines kinetics of environmental toxicants in contrast to pharmacokinetics related to drugs. despite the usefulness of tk information, there are only few legal requirements in eu chemicals legislation for the generation of tk data. however, the use of tk data to support the assessment of systemic toxicity is widely recommended in regulatory guidance and various scientific opinions [cit] . tk data are proposed for use to evaluate cross-species differences, to waive specific in vivo studies when applicable, and to support the development of novel approaches in chemical safety assessment [cit] . moreover, tk data are valuable for the development of mathematical models, such as physiologically based kinetic (pbk) models and could increase the accuracy of in vitro fate models [cit] ."
"the null assumption of similarity is rejected for exceedingly high values of this divergence. matteson (2013, 2014) [cit] introduce a binary tree based bisection algorithm called ''e-divisive'' for hierarchical divisive change point estimation. the significance of an estimated change point and its corresponding p-value is found through permuting the observation collected thus far."
"the survey was circulated to a target group of experts in membrane transporters from the academic, industrial, enterprises and regulatory communities, starting with eurl ecvam collaborators, and selected scientists having publications on transporters (the survey was sent to around 250 contacts). the survey was also circulated to the members of the european society of toxicology in vitro (estiv) (http://www. estiv.org/), the european partnership for alternative approaches to animal testing (epaa) (https://ec.europa.eu/growth/sectors/chemicals/ epaa_en), the virtual physiological human institute (vphi) via their newsletter (http://www.vph-institute.org/), the eu-tox-risk consortium (http://www.eu-toxrisk.eu/). furthermore, the recipients were encouraged to share the survey with their own network of experts. the goal was to reach as many scientists in the transporter field as possible. the exact number of recipients of the survey is therefore not known."
"in line with previous results section, the respondents identified some missing appropriate in vitro methods to generate transporters data without animal studies. here are summarized the replies that were reported in a free-text format. first, transporter induction assessment is understudied and remains a big challenge as already stated. further in vitro systems need to be developed to gain greater insight into mechanisms and signals inducing transporter expression. then with regard to barrier models, it was mentioned by several respondents that holistic in vitro models are not fully developed yet and that improvement of current methods are needed. particularly, it was mentioned a need for quantifiable and reproducible kidney and blood-brain barrier models and additional methods to study placental transport. for individual transporter, methods for all species and polymorphisms were stated as lacking. furthermore, it was mentioned that most of the data are on mdr1 (pg-p), other transporter methods are available mainly as vesicles and this tool is not appropriate for permeable compound. it was also said that reliable high-throughput methods to assess transporter functionality are needed in association with a necessary understanding for data interpretation. besides, some mentioned that nonanimal methods to study transporters are highly costly and time consuming, suggesting that economically feasible methods are missing. finally, it was reported that standardised protocols are lacking for these methods."
the views expressed in this paper are those of the authors and do not necessarily reflect the views of the author/s institutions. authors declare no conflicts of interest.
"in the four tables above, we have observed how in separating stable flows from unstable ones, our new proposal z tr outperforms both its established competitor cusum, and a host of other recent ones. certain options (such as the divergence-based measure) do not detect deteriorations as efficiency as they do improvements. the new candidate z tr however doesn't suffer seriously from such asymmetries. this is expected since we constructed it in (33) combining and borrowing strength from components designed to work best in one specific direction. most notably, z tr should be the preferred choice in case the change detection seems hard, either in the sense of a late occurrence or in the sense of close similarity of the post-change probability density to the prechange one, or both."
"the red broken line in figure (4) above represent the true time of change and the boxplots represent the estimated change time distributions from the different competitors. the jump was placed early into the process (after the 12th observation out of 50) and the pre (2) and post (0.6) change parameters were held markedly different. these imply an easy detection scenario, which help explain the good performance from almost every change-detection choice -the medians from most of the ten time-of-change distributions hover around the true change time. the distribution from our new proposal z tr, however, is the closest to the truth, along with the tightest spread, promising accurate and consistent inferences."
"keeping the change position unaltered (i.e., early into the process), we next make the detection harder by moving the non-stationary parameter (1.8) closer to the stationary one (2) . figure (5) records those results. the difficult situation gets reflected both through the widening of the boxlengths and the farther drifting of the medians away from the true change dotted line. nevertheless, z tr still retains its superiority."
"the problem of checking whether a poisson process (a special type of a point process, where the number of events in a given interval follows a poisson density) is stationary or trend-infected, is well studied. in recent times, the problem has attracted attention both from the theoretic (brodsky [cit] ) and pragmatic (chen and gupta [cit] ) viewpoints. antoch and jaruskova [cit], and lindqvist [cit] may be consulted for a review. it is time for us to survey different possible candidates for the two sample statistic d k,n mentioned previously and check their performance against the established cusum technique."
"the preceding subsection assumes that the type of departure from stationarity is known, which requires considerable mastery and field knowledge about the process unfolding. if, by any chance, we are uncertain of the nature of an alteration, we can combine improvement and deterioration setups probabilistically to generate a mixed distribution for the inter-event times. by way of illustration, let us consider a case where a certain set of individuals buy cigarettes in a smoke shop over a period of time. if we gather data on how much cigarettes was bought per day and graph it through a histogram, in all likelihood, it will exhibit several peaks. the reason being, there exists an inherent categorization among smokers -heavy, moderate, occasional, etc. and the buying habits vary considerably across these groups. in such cases, it is useful to introduce the notion of a mixture density."
"to the final question on how to gain greater confidence in describing transporter kinetics in the absence of in vivo data, the participants strongly acknowledged a better understanding of the mechanisms (80%), the development of new uncertainty assessment (21%) and a combined in vitro-in silico approach (65%) as key elements. however, with the aim of using transporter data for animal-free risk assessment, respondents from the regulatory sector specified that they would only gain confidence in the predictions if they are shown to match in vivo data, at least in some cases such as with known compounds or with literature data. similarly, another respondent proposed to first obtain transporter tk data via in vitro and in silico tools to parametrise pbk models and then use available human data to evaluate the model performance and further refine the pbk parameters obtained from initial in vitro/in silico methods. this resounds to other comments stating that \"because we do not know how to use the in vitro data for further use in kinetic models, it may limit the end value of the data and therefore, it will be difficult to gain confidence in describing transporter tk in the absence of in vivo data\" and \"that guidance as to how much in vitro data is good enough to parameterize a transporter function in silico would be important to characterize at an earlier stage so that the transporter-specific parameters do not end up becoming fudge-factors say in a pbk model\". additionally, a quantitative understanding of interspecies and in vitro-in vivo differences in transporter expressions and activities is very important for the development of pbk models and for risk assessment as already stated in this paper."
"during drug development, particular attention is paid on transporters expressed at the intestinal, hepatic, renal and blood-brain barriers considered as pivotal for pharmacokinetics . similarly, these barriers are predominantly investigated for tk in the context of estimating the internal concentration that corresponds to the external exposure levels to environmental chemicals. in this survey, brain was particularly considered for drugs, probably because brain-to-blood efflux represent a major obstacle in drug penetration and efficacy (lÃ¶scher [cit] . [cit], promoting alternative methods. finally, placental and testes barriers are of importance for reproductive and developmental toxicology and for the evaluation of window of sensitivity for exposure-based adverse effects in reproductive life-stages. the individual transporters studied for drugs as for chemicals reflect the recommendations of drug regulatory agencies [cit] . interestingly, while mrps are not required but should only be eventually considered, more than half of the respondents declared studying them. it is important to note that the list of known drug transporters listed here is neither exhaustive nor complete as the transporter field is rapidly advancing. literature on additional membrane transporters of clinical and toxicological relevance is continuously emerging. this is notably reflected by the additional transporters cited by the participants and detailed in the results section. furthermore, there exists the possibility for some transporters to be more relevant for chemicals than for drugs. this should be kept in mind when assessing relevant transporters for chemical risk assessment. however, considering first the intensively studied transporters offer extensive data and knowledge to start our assessments with."
"where l 0 and l 1 denote the likelihoods under the null and the alternate hypothesis, respectively. m k,n can then be taken as d k,n under the general cpm framework."
"finally, when questioned about the need for further guidelines and guidance documents to conduct studies on transporters, half of the participants said yes while 12% (all from academia) thought that it is not necessary for fundamental sciences (fig. 7d) . the others did not have an opinion on this question. when asked to specify in a free-text format about the content of these recommendations, experts replied: to know the right controls and standardised protocols, to inform on risk assessment and pbk model development for industrial chemicals, to provide an overview of the options (tools and methodologies) to study transporters and to guide on which transporters are relevant to study. several respondents also mentioned the need for harmonization across geographical regions."
"the sensitivity values have dropped considerably owing to the proximity of the non-stationary and stationary parameters, however, similar to its performance in table (1) above, our new proposal z tr offers the best classification accuracy even under this difficult scenario. while the two tables shown above investigate departures to the left of the stationary parameter (suggesting process deterioration), the two to follow will consider deviations to the right (hence process improvements). table ( 3) above represents improvement detections that are harder to detect owing to the facts that the change appeared late into the process and in addition, the non-stationary parameter was extremely close to the stationary one, while table (4) represents improvement detections that are relatively easier due to the large difference between the non-stationary and stationary parameters."
"iii) complete ignorance about process parameters [cit] developed their cpm formulation under this scenario of complete darkness, arguably, the most realistic of all. they prescribe conducting another generalized likelihood ratio test as follows:"
"this process is known as weak corruption in our specific context, where we generally introduce varying degrees of both improvement and deterioration. to be concrete, the inter-event time distributions in this subsection will not be deterministically governed (like in the previous on strong corruption), but will be generated according to (37) with some appropriate choice of the mixing probabilities and contributing densities. this will enable the process to follow an alternating deterioration-improvement pattern and incorporate our possible lack of complete knowledge on the nature of its history. the mixing was carried out using the ''uni-varmixingdistribution'' function within the ''distr'' package in r and the random generation of inter-event times for our simulation studies was done using functional programming. figure (8) portrays both strong (i.e., deterministic, introduced in the last subsection) and weak (i.e., random, introduced here) corruption simultaneously. the black line represents a stable, stationary process. the red represents a (deterministically) deteriorating case where, at a certain point of time, admittedly, early in the process, the process transitioned from a stationary to a non-stationary environment. the green line represents an (deterministically) improving case where, also, the line transitioned early in the process. the blue process, however, represents a mixed case in which we combined (20%) improving and (80%) deteriorating cases to produce a weak non-stationarity. gamma densities have been used as the contributing distributions for both improvement and deterioration. as shown in the illustration, the deterministic changes from stationarity to non-stationarity is very noticeable, as both the red and green curves demonstrate. however, as a glance at the black and the blue steps shows, such differentiation gets tough if the update through weak non-stationarity, gets random."
"overall, to gain scientific and regulatory acceptance of non-animal transporter data, the approach should be integrative and iterative similarly to integrated approaches to testing and assessment (iata) [cit] . first, computational models combined with decision trees and/or public databases could be used to determine if a chemical would interact with a transporter and with which transporter(s). this first step would help to guide experimental work to focus on the appropriate in vitro methods related to the transporter of interest and it will also specify what confidence there may be in the predictions that lack any active transport kinetics. then the data generated by specific human in vitro methods could be implemented into pbk models, built on mechanistic understanding from literature. the in vivo predictions obtained could then be evaluated with human biomonitoring or volunteer study based data when available. the pbk model equations and parameters as well as the initial in vitro or in silico generated data and models could then be adjusted accordingly to obtain matching with in vivo data. through iterative calibration, the model predictions would gradually describe the human data better, increasing confidence in predictions from alternative methods. importantly, the focus of the study here was active transport. however, passive as well as facilitated diffusion via membrane channels or via binding to human serum albumin, have to be taken into consideration [cit], especially for lipophilic contaminants, as most of the industrial chemicals are."
"in addition to the sample size type (i.e., whether fixed or not), it is imperative to have some insights into the nature of model parameters. [cit] introduces this cpm framework with a normal choice for f, i.e. with"
"3) outage probability and throughput: we first consider the signal sent from node a to b through the relay r. the snr of this signal at r, Î³ r, can be calculated from (4) as"
"see http://www.ieee.org/publications_standards/publications/rights/index.html for more information. relay r receives signal from the node a, and it uses its power splitter to divide the signal power into two parts: one for harvesting energy and the other for processing signal (see fig.2 )."
"during the first two time slots, r decodes signals x a and x b from a and b, respectively. during time slot t 3, r broadcasts the normalized signal, x r, to the two destinations."
"after simplification, we finally obtain p 1 [10, eq.(3.324.1)] as follows, where k 1 (.) is the first order modified bessel function of the second kind [cit] :"
"substituting (5) into (6), node b already knew its own information therefore it easily discards x b to get x a from x r . here, we assume that the channel state information (csi) and other system parameters are available at all nodes. we can estimate the signal received at b (from a via r) as"
"this paper has proposed a two-way df relay that exchanges information between a pair of nodes in three steps. the performance of the proposed relay is compared with a work on multiplicative af relay. the expressions for the outage probability of the proposed relay, and its upper and lower bounds have been evaluated. the simulation and analytical results show that the proposed relay outperforms multiplicative relay when the nodes are equidistant from the relay. however, when the distances are unequal, the performance of the proposed relay depends on the power splitting factor."
"if temporal and visual information processing reflect similar principles [cit], it is possible that the receptive field in our model could also reflect the adaptation of our mechanism of temporal information processing to statistical properties of the world."
where * Ï â¡ k/s is a parameter that indexes the nodes inf 2 and k is some integer that we later identify to be related to the precision of the inverse laplace transformation. the only time-varying part in eq. 5 is f . it will turn out that the value of * Ï specifies the time that each unit inf has its peak activation following a delta function input.
"there are 9 groups of layer i neurons in total, each neuron within a group has the same time constant. within each group there are 12 layer i neurons with the same parameters."
"here g can is the maximal value of the ion conductance measured in mho cm 2, e can is the reversal potential of the can current ion channels and m is a dimensionless quantity between 0 and 1 that is associated with the activation of the can current ion channels."
"there are two primary results in this paper. the first is that the simulated output layer neurons, like time cells, fire sequentially in response to a delta function input and the sequential firing is approximately scale-invariant. this property comes from receptive fields that can be understood as off-center/on-surround in the projections to the output layer units. the second is that the simulated neural sequence can be rescaled by adjusting the gain of the layer i neurons. before describing those results, we first describe the activity profile of each of the layers in turn."
"although can current is prevalent in pyramidal neurons, the amount of persistent spiking in different populations of entorhinal cortex neurons varies, with less persistent spiking in stellate cells [cit] . thus, the can current may be present in different magnitudes in different neuronal populations. we have used simple integrate and fire neu-rons in the output layer as a simplified initial representation, but future implementations could include a more complex range of membrane currents."
"we proposed a neural circuit that encodes the laplace transform of an input function and then approximately inverts the laplace transform to produce a series of sequentially firing cells. the time constants and the peak firing times of the time cells range from a few seconds to almost 100 seconds. critically, the firing rates of the sequentially-activated time cells are scale-invariant. this provides a possible neural substrate for the scalar timing behavior observed across a wide range of timescales in behavioral tasks, and also approximates the neurophysiological recordings of sequentially activated cells that have been observed across a wide range of regions of the cortex including the hippocampus [cit], pfc [cit] and striatum [cit] ."
"the neural circuit presented in this paper is built upon a mathematical framework that has been proposed to construct a representation of the recent past in a distributed, scale-invariant way [cit] ). this mathematical model has two layers of nodes. nodes in the first layer integrate the input stimuli with an exponential kernel, equivalent to performing a laplace transform on the stimuli. the activity of the nodes in the second layer is obtained by inverting the laplace transform using the post approximation [cit] . after presenting a delta function as input to the first layer, the activity of units in the second layer resembles the firing rates of scale-invariant time cells. the model can be implemented as a two layer feedfoward neural network where the weights can be explicitly computed as a function of the time constants of the nodes in the first layer. here we give a brief overview of the mathematical model and emphasize the connection to our neural circuit that will be introduced later."
"there are other neural circuit models that produce sequentially activated neurons, but to our knowledge, the present model is the first one that has the additional feature of scale-invariant neuronal activity. however, functionally identical models with different biological realizations of the same equations might also be possible. for example, rather than implementing long functional time constants via intrinsic currents, one could construct an analog of eq. 2 using recurrent connections. for example, gavornik and shouval showed that in a spiking recurrent neural network trained to encode specific time intervals, units exhibit persistent spiking activity [cit] . other neural circuits for computing the inverse are also possible. the computation of the inverse laplace transform amounts to a suitable linear combination of inputs from cells with exponentially decaying firing rates. [cit] showed, in a detailed compartmental model of a hippocampal ca1 pyramidal cell, that the dendritic tree functions as a two-layer artificial neural network. thus a single dendritic tree could implement something like w l used here. it is also within the realm of possibility that the exponentially decaying firing rates could be replaced with slow dendritic conductances. table 1 list of the parameters of the model"
"this property ultimately arises from the central limit theorem. consider a chain of n neurons where every neuron is modeled by a same synaptic kernel k(t). that is, the activity of every neuron is the convolution of its synaptic kernel with the activity of the previous neuron."
"a biological constraint on the neural circuit that can cause deviation from scale-invariance is the input-output function (f-i curve) of the layer ii neuron. only when the layer ii neurons are in their linear regimes can they faithfully relay the temporal information from the presynatic neurons to the output layer neurons. since we modeled the layer ii neurons as leaky integrate and fire neurons, their f-i curves are discontinuous near the threshold input value. thus some background firing is required for the layer ii neurons to be in their linear regimes. also some steady background firing for the layer i neurons would not change the scale-invariance property, since a constant shift in f (s, t) would not affect the derivative that contributes to the inverse laplace transform."
"the values of * Ï inf are controlled by the values of s in f . it remains to specify the distribution of values of s and thus * Ï . in order to preserve scale-invariance, equate the information content of adjacent nodes [cit] and enablef to implement weber-fechner scaling [cit], we choose the values of * Ï to be logarithmically spaces as shown in figure 3c . this is equivalent to choosing the number density of s to go down like s â1 . power law distributions of time scales emerge in physical systems under quite general circumstances (e.g., [cit] ) a biophysical model implementing this mathematical framework"
",v m is the average membrane potential during a spike and q is the total charge influx during a spike. note that this expression allows infinite values of the functional time constant. this expression holds when the neurons are in the linear regime and when the change of m is much faster than the change of calcium i.e., 1 a[ca 2+ ](t)+b Ï p and the interspike interval is much less than Ï p, the decay time constant of the calcium concentration. in the simulation we choose the parameters to satisfy the two conditions above, so that the firing rates are well approximated by exponential decaying functions with a broad range of time constants."
"since the functions k(t) and g i (t) are bounded, we can treat them as probability distributions up to a scale factor. then the activity of the ith neuron g i (t) is proportional to the probability distribution of the sum of the random variables described by k(t) and g iâ1 (t)."
"to keep the firing rates of the layer ii neurons in a reasonable regime, the psps (postsynaptic potentials) coming from 3 layer i neurons from the same group are used as the input to one layer ii neuron. layer ii neurons are modeled as leaky integrate and fire neurons."
"behavioral scale-invariance requires that the neural system supporting behavior is also scale-invariant. recent neurophysiological recordings in behaving animals show spiking activity at specific temporal intervals by individual neurons, referred to as time cells. these experimental data provide a possible neural substrate for timing behavior and various forms of memory [cit] ."
"the classic power-law of forgetting [cit] indicates that a single mechanism may underlie both short and long term forgetting. in free recall, subjects are given a list of words and are asked to recall them in any order. the recency effect refers to the phenomenon that words from the end of a list are more easily recalled. this effect has been observed over a wide range of timescales, from fractions of seconds [cit] to several minutes [cit], indicating that a single memory mechanism with a scale-invariant representation of time may serve under different timescales."
"we will demonstrate that this is indeed neurally realistic by constructing a biologically detailed neural circuit that utilizes a biophysical model of exponentially decaying persistent firing neuron [cit] to perform the computation of this mathematical model, thereby generating a set of scale-invariant time cells."
"the post-synaptic cells shown in the top layer of figure 4 are also modeled as leaky integrate and fire neurons. when driven by the psps from the layer ii neurons, their firing rates resemble the mathematical expression of equation 7. as shown in figure 6 their peak firing times scale with the width of their firing fields. when rescaled according to peak firing times, their firing rates overlap with each other. this indicates that the output layer neurons fire sequentially and have a firing rate profile that is time-scale invariant."
"numerous behavioral experiments in humans and other animals suggest that time is represented in the brain in a scale-invariant fashion. for example, in interval timing experiments, the variability of the reproduced interval is proportional to the duration of the interval [cit] . the distributions of the response to different intervals are scale-invariant in that they overlap when rescaled by the duration of the interval, a phenomenon termed the scalar property [cit] ."
"after we have the matrix representation of the kth derivative, the activity for a givenf nodef (t, s) can be approximated by a linear combination of the f node activities f (s, t)."
"time cells exhibit phenomena that are suggestive of time-scale-invariance. the firing fields of time cells that fire later in the delay period are wider than the firing fields of time cells that fire earlier in the delay period ( figure 1) . moreover, the number density of time cells goes down with delay. although there is not yet quantitative evidence that time cells are scale-invariant, these findings imply that the representation of the past is compressed [cit] and are at least qualitatively consistent with a scale-invariant representation. if it turns out that sequentially-activated time cells support timing behavior, and if time cells are scale-invariant, then the neurophysiological mechanisms that endow time cells with scale-invariance are of critical importance in behavior. scale-invariance of time cells is important because it provides the temporal basis for an animal to use the same set of mechanisms to integrate information and make decisions over different time scales. because the natural world's choice of scale is not known a priori, treating all scales equally is adaptive. it can be shown that a logarithmically spaced one-dimensional receptor optimally represent a function when the statistics of the stimulus function is unknown [cit] . just as in visual system the acuity decreases further away from the fovea and facilitates saccades, a scale-invariant representation of time where the temporal acuity decreases as we recede into the past would potentially facilitate retrieval of episodic memory [cit] ."
where c k is a constant that depends only on the choice of k and we have substituted * Ï â¡ k s in the last step.
2 note that this definition of * Ï differs from the notation in some previous papers where * Ï was defined to be negative. we adopt this convention for convenience here.
"the layer i neurons as shown in the bottom layer of figure 4 are driven by the input to the network. they are equipped with an internal can current and have persistent, exponentially decaying firing rates. the decay time constants are logarithmically spaced between 2.04 s and 83.49 s. this is achieved by adjusting the maximum can current conductance g can and initial calcium concentration according to table 1. the dotted lines are exponential functions; the degree to which the firing rates align with these theoretical functions confirm that the firing rates indeed decay exponentially. this is in accordance with the activity of the f nodes in the mathematical model."
"scale-invariance is also often observed in the associative learning rate in animal conditioning experiments. for instance, it has been shown that the number of trials needed for animals to develop a conditioned response increases when the reinforcement latency is increased and decreases when the intertrial interval is increased [cit] ."
"to better understand the properties off ( * Ï ), consider the form it has if f (t) is a delta function at time zero. then, each node in f (s) decays exponentially as e âst and eachf node is given by:"
"in the above simulation we presumed that the time cells fire during a delay period, whose start is signaled by some stimulus which we abstracted as a delta function. in reality we would be interested in the response of the network to a temporally extended stimulus."
"sequentially-activated time cells have been observed in a wide range of behavioral tasks and in many brain regions. time cells were observed when an animal is performing delayed match to sample [cit], delayed match to category [cit], spatial alternation [cit], or temporal discrimination tasks [cit] . time cells have been found in various parts of the brain including the hippocampus [cit], prefrontal cortex (pfc) [cit] and striatum [cit] . a recent study suggests that neurons in the amygdala are sequentially activated during the intertrial interval of a conditioning task [cit] ."
the activity of the f nodes is transformed by a second layer off nodes. thef nodes are in one-to-one correspondence with the f nodes. at each moment the activity of the f node labeled by s is transformed in the following way:
the factors in the parentheses account for the fact that the accuracy of the slope further away from the point s 0 is a less accurate estimate of the derivative. 3
at very long times they maintain a background firing rate of around 1 hz due to the [cit] and the inhibitory neurons have gaba a receptors with a time constant of 15 ms [cit] ).
"when rescaled, the neuronal activity for the neurons that get activated later will be more concentrated. this is indeed what is observed in figure 2 in order to construct a scale-invariant neural system, it is essential that it be endowed with a range of characteristic time scales."
"this mathematical framework produces a set of functions that resembles the firing rates of time cells. moreover this mechanism gives rise to time cells that are scale-invariant, which would be a desirable property for the brain to possess. however, it is not clear whether it is possible for neural circuits in the brain to actually implement this hypothesized mechanism."
"the layer ii cells shown in the middle layer of figure 4 are leaky integrate and fire neurons, with parameters given in table 1 . driven by upstream neurons with exponentially decaying firing rates, they also display firing rates that decay exponentially, at least initially."
"in particular, the activity of neurons that are activated later is more concentrated when rescaled. this can be shown in an asymptotic analysis using the central limit theorem (see text). c. the heatmap for the activity of 50 neurons in the chaining model. the number of neurons coding later time is the same as the number coding for earlier times."
"indeed as shown in figure 9a,b changing the slope of the f-i curves changes the time constants of layer i neurons. figure 9c shows the firing rates of the same time cells before and after the change in Î±. their firing fields appear rescaled by the scaling factor 1 Î±, in accordance with observation [cit] ."
the same time basis functions rescaled by the peak time along the x axis and by the maximum activity along the y axis. they deviate from each other systematically. it is clear that the neurons at different points along the chain do not obey scale-invariance.
"f (s â1 ) with coefficients determined by s 1, s 0 and s â1 . therefore if we represent the function f (s) by a discretized vector of values"
"alternatively, any type-i model neuron with a linear f-i curve would satisfy the biological constraint imposed by scale-invariance, with or without background firing. by appropriately modeling an adaptation current, a log-type f-i curve could be transformed into a linear one [cit] ."
"moreover, as long as the ratio between the intertrial interval and the reinforcement latency is fixed, the number of trials needed to develop a conditioned response is fixed, again indicating scale-invariance in the animal's timing behavior."
"layer i neurons implementing f nodes. layer i consists of 108 integrate-andfire neurons driven by the can current described above, modeling the f nodes from the mathematical model. the set of model neurons spans 9 different values of s with 12 neurons for each value. to model the experimental finding of logarithmically-compressed timeline, the time constants 1/s were chosen to be logarithmically spaced between 2 s and 83 s by adjusting maximum can current conductance g can and initial calcium concentration in the can driven persistent firing neuron model above according to table 1. all the model parameters are summarized in table 1 layer ii neurons relay the activity and ensure dale's law. the expression for the synaptic weights w l in eq. 15 requires both positive and negative connections."
"in the following sections, first, we explain the model of a stochastic neural network with dynamic synapses. next we derive the corresponding macroscopic mean field models that approximate the dynamical properties of the stochastic model. furthermore, we analyze structural details of the dynamical system in the macroscopic mean field model, and we show how the network behavior and the memory-retrieval performance are influenced by noise intensity and the properties of the dynamic synapses. finally, we discuss the results of the analyses from a viewpoint of neuroscience as well as possible future studies."
"using the described criterion function to evaluate features, the next component of our feature selection algorithm is its search strategy. there are many approaches to searching the space of possible feature subsets, from the naÃ¯ve exhaustive search to more sophisticated search strategies such as genetic algorithms. in this work, we apply sequential forward selection (sfs), also referred to as sequential forward search [cit] . this is a greedy search technique that begins with an initially empty set and adds features one at a time such that the feature added at each round is the one that best improves the criterion function. note that one can utilize other search strategies, such as backward selection and sequential forward floating search, or applying sparse optimization [cit] . for the purposes of this paper, which presents the ability of the proposed feature selection criterion in enhancing outlier detection, we find it sufficient to utilize a simple search strategy that takes feature interaction into account, as in sequential forward search. we name our proposed method local kernel density ratio (lokdr) feature selection."
"the efficacy of synaptic transmission is determined by the product of x j (t) and u j (t); the efficacy decreases (short-term depression) or increases (short-term facilitation) according to the parameters Ï r, Ï f, and u se . associative memory networks work well if the memory patterns are mutually orthogonal, but otherwise it does not necessarily work well. moreover, in the associative memory network with depression synapses, the appearance of the oscillatory states is influenced by the similarity among the memory patterns [cit] . to evaluate the influence of the similarity among memory patterns in the network with both depression and facilitation synapses, we construct the associative memory network with correlated memory patterns by considering a parent memory pattern Î¾ and p child patterns Î¾ Î¼ [cit] as follows:"
"stochastic neuron models with dynamic synapses and the corresponding mean field models have been proposed in previous studies, and their dynamical properties and possible roles of the dynamic synapses have been intensively investigated [cit] . synaptic depression is known to enable neuronal gain control [cit], and to contribute to the destabilization of the network activity and generation of an oscillatory state [cit] . synaptic facilitation is believed to enhance the working memory function in the prefrontal cortex [cit] . furthermore, in a network with both depression and facilitation synapses, changes in the efficacy of dynamic synapses are suggested to reorganize the effective network structure, thereby contributing to flexible information processing in the prefrontal cortex [cit] )."
"to analyze the macroscopic properties of the associative memory network with stochastic neurons, we consider dynamical mean field theory with the sublattice method [cit], which allows us to analyze the mean field model with the non-homogeneous network structure of the associative memory network. first, we derive the microscopic mean field model by taking the noise average of each variable in the stochastic neural network model. we get the following equations from equations (1) to (3):"
"the svm is a typical pattern recognition method, before the svm model is trained, the type and parameter ï³ of kernel function, and penalty factor c are determined. therefore, the parameter"
"another observation from the speedup figure is the similarity in speedup of the cns and prost datasets. these datasets have comparable sample space sizes and differ in their feature set sizes, yet the common pattern of their speedup confirms the ability to benefit from parallel processing of data points. as more features are added, their speedups slowly diverge because prost has fewer features and thus requires evaluating fewer candidate subsets during each subsequent round of the feature selection algorithm."
we assumed that neurons within the same sublattice i Î· follow the same dynamics and that the variables in the microscopic mean field model (equations 17-19) can be described as
"most of the work on dimensionality reduction for outlier detection have tackled the problem using a feature transformation approach. in these methods, a projection of the feature space is sought which can be used to detect outliers. while this approach subsumes feature selection (i.e., projecting onto the original feature axes is equivalent to selecting a subset of the features), there is a case to be made for the understandability that comes with feature subset selection as opposed to linear or non-linear combinations of the features. retaining a subset of the original features goes a long way towards understanding the nature of the underlying data and the features that contribute to an outlier's deviant behavior. this can be advantageous in domains such as fraud detection and intrusion detection, in which anomalous activity can be narrowed down to a subset of the collected features."
"to investigate the stability of the system given by equations (24-26) around the steady state given by equations (30-32), we consider the locally linearized equations with small perturbations Î´m Î· (t), Î´x Î· (t), and Î´u Î· (t) around the steady state as follows:"
"in this work, we presented a novel feature selection criterion catered to outlier detection problems. the proposed method is non-parametric and makes no assumptions about the distribution of the underlying data other than the fact that outliers are different than the normal data. it selects features that best capture the behavior of normal data while making outliers more distinct from the normal. we applied a forward search strategy to our feature selection criterion and compared its ability to detect outliers with other popular feature selection methods. experiments on real datasets showed that our local kernel density ratio (lokdr) feature selection algorithm does very well to discern features that facilitate the detection of outliers. by taking advantage of its parallel nature in terms of the number of data points and features, we also achieved great speedups with an implementation on a graphics processing unit (gpu)."
"2) performance experiments: we also evaluate the performance of a gpu-based implementation of our proposed feature selection algorithm as compared to a serial (cpu only) implementation. the gpu version was written using nvidia's compute unified device architecture (cuda) and the serial version was written in c. cuda extends the c language with new programming constructs and provides libraries and a platform for the efficient execution of generalpurpose applications on gpus. our serial experiments were run on an intel xeon cpu e5405 running at 2.00 ghz. [cit] which is based on the nvidia fermi gpu architecture. [cit] modules are performance-optimized, high-end products which offer 6 gb of gddr5 ecc-protected memories on board with a 1.566 ghz memory clock and a 384-bit memory interface. the features supported by a cuda hardware are described by the compute capability, and fermi architectures have the support for cuda compute capability 2.0."
"in this work, we present a novel optimization criterion inspired by outlier detection problems where the data is highly imbalanced and outliers comprise a small portion of the dataset. the criterion tries to find the set of features that maximize the density of normal data points while minimizing the density of outliers. in other words, it seeks feature subsets wherein normal data points fall in high-density regions and outliers fall in low-density regions of the feature space. the goal is to make outliers stand out more prominently from normal data points, which allows outlier detection algorithms to more easily identify them."
"is introduced into the pheromone updating rule in order to control the updating proportion of the pheromone concentration in one iteration, which will greater strengthen the pheromone concentration on the better path. then the pheromone concentration is used to react the path information. the pheromone concentration on the optimal path is globally updated: ï³ is near one."
"the evaporation coefficient of pheromone ï² in the conventional aco algorithm is a constant. the value of pheromone ï² directly relates to the global search ability and convergence speed. in the aco algorithm, if the pheromone is too large, the selected probability of the visited path will be large, it will affect the global search ability of the algorithm. so it is the key to set the pheromone value for controlling the pheromone releasing and evaporating. the dynamic evaporation coefficient strategy is proposed. this strategy sets a large dynamic evaporation coefficient ï² at the beginning of the algorithm for enhancing the global search ability. it can not only increase the global search capability, but also accelerate the convergence."
"in addition to proposing a feature selection algorithm that seeks to intrinsically enhance the quality of outlier detection, an important contribution of this paper is the implementation of the algorithm on a graphics processing unit (gpu) and the substantial speedup that is acquired. gpus are massively parallel floating point processors attached to dedicated high speed memory, at a fraction of the cost of traditional parallel processing computers. they are designed for compute-intensive, highly data-parallel computation and rely on multithreading to provide throughputoriented performance. therefore gpus are well suited for data-parallel applications, resulting in large improvements in running time."
"in this section, we describe the datasets and outlier detection algorithms used in this study to evaluate the quality of features selected by the lokdr algorithm. we also briefly describe other feature selection techniques with which we compare our results. we then present the outlier detection results for each of the feature selection methods."
"the arrhy dataset shows the most significant improvement for even a small number of features due to the performance gain of concurrent data point processing on the gpu. as more features are selected, the cost of performing distance calculations (and hence -nn queries) increases, so there is more performance to be gained when they are done concurrently. this, coupled with the small sample space of the microarray and mass spectrometry datasets, means that more data points can be processed and used to calculate the optimization criterion within a shorter amount of time, which is reflected in the large rise in speedup for the cns, lymph, ovary, and prost datasets."
"in order to verify the effectiveness of the proposed eruraco-svm method, the experiment data from uci data set are selected in this paper. the detail describing of the experiment data is shown in table 1 . and the svm and aco-svm based on aco and svm are selected as the contrast models in order to more accurately compare the effectiveness of the eruraco-svm method under the same experiment data sets. the experiment environment: intel(r) core i5,2.40ghz, 2g [cit] . the experiment selects the different initial samples, which are regarded as the training samples to train the svm model. table 2 and table 3 . from the experimental results of table 2, it can be seen that the aco-svm method has the premature convergence, and poor local search ability. and the eruraco-svm method takes on the search abilities of the local optimal solution and the global optimal solution. for yeast and adult samples, the svm method can only obtain the classification accuracy (87.1% and 89.2%), and the eruraco-svm method can obtain the classification accuracy(95.3% and 94.6%). so the classification accuracy of the eruraco-svm method is better than the svm and aco-svm methods. the results show that the eruraco-svm method takes on the strong generalization ability and the higher classification accuracy."
"our criterion function tries to optimize the ratio of local kernel density estimates for normal and abnormal points (outliers). in the numerator, we sum the local kernel density of all normal data points and in the denominator, we sum the local kernel density of all outliers. by maximizing the ratio of the two, the goal is to find the subset of features that maximizes the density of normal data points and simultaneously minimizes the density of outliers. intuitively, we would like to find a lower dimensional subspace that corresponds to a subset of the features wherein normal data points are in closely compacted regions of the space while outliers are dispersed, allowing them to be more easily distinguished as anomalous with respect to the normal data. by using a local density approach, our criterion can aid outlier detection algorithms in detecting local, as well as global, outliers [cit] . in particular, we are already thinking in terms of local neighborhoods, as reflected in the kde calculations. this notion can be carried over to the outlier detection phase, especially in the case of a local density-based outlier detection algorithm such as lof which calculates the density of each point within a local neighborhood. this allows the detection of data points that seem to be outlying when considered within the scope of their local neighborhood, not just on a global scale."
"for outlier detection, we use one-class classifiers which are trained on only normal data (inliers). for each data point, the classifier produces a decision value that represents its confidence in that point being an outlier. we apply a threshold on the decision value as a cutoff point for decision making. a data point is flagged as an outlier if the decision value exceeds a threshold. varying the threshold varies the number of correctly classified outliers (true positives) and incorrectly classified normal data (false positives). using this information, we plot a curve of the true positive rate versus the false positive rate, known as the receiver operating characteristic (roc) curve [cit] . in section iv-d, we perform an evaluation of several feature selection techniques in terms of the area under the roc curve (auc) achieved by the outlier detection algorithms on different feature subsets chosen by the feature selection techniques."
"because of the non-convexity of the response function g Î² and the excitatory feedback connection, the network can stabilize the selfsustained active states [cit] . similarly to equation (11), we obtain the following equations corresponding to equations (4) and (5):"
"in the mem state (figure 2a), one of the memory patterns or inverted memory patterns is retrieved. the state of the network converges to a steady state, which corresponds to a stable fixed point in the macroscopic mean field model. the steady state can be represented with the overlaps as e.g., figure 2b shows a typical time course that the network converges to the smix state [to the mixture of the stored patterns sgn(Î¾ 1 + Î¾ 2 + Î¾ 3 ) in the figure 2b ]."
"where (x, x â² ) is the distance between the two points. the local neighborhood density of a data point x can be thought of as a measure of the similarity of points within that neighborhood to x. since a kernel function can be used as a measure of the similarity between two data points [cit], in equation 2 other kernel functions can be used in place of the gaussian kernel. we use the gaussian kernel and effectively perform kernel density estimation (kde), also referred to as the parzen-rosenblatt window method [cit] . this provides a standard, non-parametric notion of density for the data points."
"in the amix state ( figure 2c), one of the asymmetric mixture of the memory patterns is retrieved. the amix state can be represented as e.g.,"
"ant colony algorithm (aco) [cit] . it is a metaheuristic inspired by the behavior of real ants in their search for the shortest path to food sources. the aco algorithm consists of a number of cycles (iterations) of solution construction. during each iteration, a number of ants construct complete solutions by using heuristic information and the collected experiences of previous groups of ants. these collected experiences are represented by the pheromone trail which is deposited on the constituent elements of a solution. small quantities are deposited during the construction phase while larger amounts are deposited at the end of each iteration in proportion to solution quality. pheromone can be deposited on the components and/or the connections used in a solution depending on the problem. each ant randomly starts at one city and visits the other cities according to the transition rule. after the ants complete their routes, the system evaluates the length of the routes. then, the system uses the pheromone update rule to update the pheromone information. the learning procedure is to update the pheromone information repeatedly."
"the size of the local neighborhood of a data point x is determined by the distance to its â -nearest neighbor, referred to as its -distance. all of the data points whose distance to x is less than this distance comprise its -distance neighborhood, (x). the -distance neighborhood of a data point x is formally defined as follows:"
we now derive the mean field model that describes the macroscopic dynamical properties of the associative memory network. here we use the sublattice method [cit] ) with
"the classifiers used to evaluate the feature subsets are nearest neighbor (nn), local outlier factor (lof), and one-class support vector machines (ocsvm). the (oneclass) nearest neighbor classifier is a distance-based outlier detection algorithm wherein a data point's decision value is the distance to its nearest neighbor. the greater the distance, the more likely that point is an outlier. the lof algorithm takes a density-based approach to detect outliers; the greater the density of a point's nearest neighbors compared to its own density, the more outlying the data point. the decision value assigned to a data point is its local outlier factor."
"in this study, we investigated the dynamical properties of an associative memory network composed of a stochastic neural network with both short-term depression and facilitation synapses on the basis of the macroscopic mean field model. we analyzed the behavior of the network in broad ranges of parameters that specify the noise intensity and the properties of the dynamic synapses. we found that the associative memory network exhibits the variety of dynamics, including the memory state, smix and amix, and several modes of the oscillatory states, and that its properties change with various types of bifurcations."
"since there are no dependencies between individual -nn queries, the -nearest neighbor search is highly task-parallel, as all queries can be performed in a simultaneous manner, each one independent of any other query. the process is also known to be data-parallel, meaning the data values required to calculate pairwise distances are used for several different calculations. this makes it an excellent candidate for implementation on a gpu. general purpose computing on a graphics processing unit (gpgpu) has become a popular, cost effective approach for high performance parallel computing. [cit] have shown that implementing the -nearest neighbor search on a gpu accelerates the search by up to a factor of 400 compared to the brute force cpu-based (serial) implementation."
"support vector machine (svm) introduced by vapnik [cit], it is one of the popular tools in a supervised machine learning method based on structural risk minimization. the basic characteristic of the svm is to map the original nonlinear data into a higherdimensional feature space where a hyperplane is constructed to bisect two classes of data and maximize the margin of separation between itself and those points lying nearest to the support vectors."
"although, we have considered the properties of the steady state and the oscillatory state as the attractors in the present study, properties of a transient process of memory retrieval should be evaluated. the relation between the stability of the memory retrieved states and irregularity of the neural activity [cit] remains to be further investigated. in the present study, we used a simple neuron model, namely the discrete-time and binary neuron model. meanwhile, the behavior observed in the present model should be qualitatively and/or quantitatively evaluated in more realistic neuron models e.g., integrate-and-fire or hodgkin-huxley model in the future."
"in addition to the search strategy, the other important component of feature selection is the criterion to be optimized. a brute force way to evaluate features is to utilize the classifier that will ultimately be used. such a method is called a wrapper approach [cit] . another, less computationally expensive, way is to evaluate features based on some criterion function. this is referred to as a filter method. existing criteria include measures based on distance, information, dependency, and consistency [cit] . a third approach is to perform feature selection as part of the learning task, known as an embedded method, such as a decision tree."
"as we add more features to each dataset, the arrhy dataset shows a linear increase in the running time whereas cns, lymph, ovary, and prost show a somewhat superlinear increase. contrast this to figure 5(b), which presents the running time of the gpu-based implementation of lokdr and where arrhy displays a somewhat sublinear increase and the results of the other datasets are more linear. also in the gpu implementation, the prost dataset shows a slightly lower running time compared to the lymph dataset. this can be attributed to the improvement gained from performing -nearest neighbor queries for several different data points concurrently on a gpu, which reduces the running time required for a larger sample size."
"to see how well our feature selection algorithm performs on a more general dataset that does not have imbalanced data or a small sample size, we also ran experiments on the arrhy dataset and present results in figure 4 . the results show that while features selected by bahsic produce the highest average auc for most of the feature subsets, the lokdr algorithm still performs well and produces auc using the paired student's t-test, we confirmed that our experimental results showing the superiority of lokdr over the other feature selection methods are statistically significant at the 95% confidence level with respect to all methods, except bahsic on the prost and arrhy datasets, where they perform comparably. from our results, we conclude that the lokdr feature selection algorithm chooses features that enable outlier detection algorithms to do consistently well across all the datasets, from those which are very high-dimensional with imbalanced class labels and that suffer from the small sample space problem, to a more general dataset without these properties."
"lof has been shown to perform well in detecting outliers using a ratio of densities. however, it uses a heuristic notion of density; the density of a data point is the inverse of the average reachability distance between the point and its neighbors, which is the maximum of the actual distance between two points and the -distance of the latter point [cit] . we utilize the success of the main notion in lof with the goal of producing a cleaner, simpler model that requires no heuristics and is based on kde, which has a solid statistical foundation. by maximizing the ratio of densities, we emphasize the differences between outliers and normal points, enhancing the ability of outlier detection algorithms to correctly identify outliers."
"in the latter approach to reducing the size of the feature space, the original set of features remain unchanged and a subset of those features are selected. a simple way to perform feature selection is to use a feature evaluation function, such as relevance [cit], to rank the features on an individual basis. then, a subset of the features are selected by taking the top-ranked features (for example, the top features). another feature selection methodology is to search the space of feature subsets and select the subset that optimizes a criterion function. for a dataset with features, there are 2 possible subsets of features. for even a moderate value of, an exhaustive search would be too computationally expensive, so it is common to use a greedy search strategy such as sequential forward selection or backward elimination [cit] ."
"to test this hypothesis, we developed a criterion that measures the quality of features based on the density induced for normal and outlier data points. more specifically, to maximize the density of normal data points while minimizing the density of outliers, the criterion function takes the ratio of the two, with a focus on the local neighborhood density of each data point. to measure density, we make no assumptions about the form of the underlying distribution of the data. instead, we take a non-parametric approach and calculate the kernel density estimate of the data points with a gaussian kernel. the objective is to find the optimal set of features w * that maximizes the described criterion function (w):"
a sublattice is defined as a set of neurons belonging to a given pattern vector. the sublattice belonging to the pattern vector Î· is defined as
"the performance of the memory retrieval can be characterized by the appearance of the mem state in which the state of the network successfully converges to one of the memory patterns. in addition to the mem state, in the relatively-lownoise range, there exists smix and amix states that correspond to pseudo-memory patterns. in this parameter range, the retrieval of the memory pattern is not assured and depends on the initial state of the network. in the high-noise range, the network tends to the para state, which corresponds to the state in which the pattern of neural activity is disrupted and randomized because of the noise. we classified the oscillatory states into three modes according to the ed. the os1 state corresponds to oscillation between the pseudo-memory patterns, and it appears in the relatively high noise range. the os2 state is the oscillation between one of the memory patterns and its inverse pattern, and it appears next to the mem state. the os3 state is the transitive state between memory patterns and their inverse patterns. such transitive dynamics is related to the itinerant dynamics in terms of chaotic dynamics [cit] . the appearance of the above mentioned states of the network depends on the properties of the dynamic synapses (figure 6) and on the correlation level between memory patterns (figure 7) . in the pseudo-constant region (figure 4a), the state of the network converges to one of the fixed points like the conventional associative memory model [cit] . in the depression-dominant region, which is archived by increasing the recovery time constant Ï r from the pseudo-constant region, the area of successful memory retrieval shrinks, whereas the oscillatory states appear as shown in figure 6a . increase in the fraction of neurotransmitterrelease u se intensifies the influence of the depression. as u se increase, the area of the para state expands, whereas the areas of other states shrink ( figure 6d ). in the facilitation-dominant region, which is archived by increasing the time constant Ï f from the pseudo-constant region, the area of the memory retrieval expands (figures 6b, 7c), which suggests that the facilitation synapses contribute to the memory retrieval [cit] . as the correlation level among memory patterns increases (figure 7), the network loses the ability to retrieve the memory pattern, and the state of the network tends to become the pseudo-memory pattern. in the region of the oscillatory states, the oscillatory state among the memory patterns shrinks, whereas the oscillatory state between pseudo-memory patterns remains ( figure 7b ). these results have implications regarding brain functions. the distribution of facilitation and depression synapses in the brain varies according to the region of the brain. many facilitation synapses exist in the prefrontal lobe and, whereas many depression synapses appear in the parietal lobe [cit] . the facilitation synapses may form a synaptic working memory and contribute to the prefrontal function, which requires a flexible executive function. conversely, the depression synapses might be involved in memory search or mental rotation, which requires to imagine to handle an object in the parietal cortex [cit] . the oscillatory states os3 observed in the present model correspond to the states that the neural network sequentially retrieves stored memory patterns. the oscillatory state appears with the incorporation of depression synapses. furthermore, the area of the oscillatory state expands with increase in the time constant of the facilitation process. these findings imply that the depression and facilitation synapses contribute to various brain functions e.g., a generation of sequential actions or the flexible information representation [cit] ."
"for the training phase of the outlier detection algorithms, we take a one-class (or semi-supervised) learning approach and train only on normal data points. during testing, both normal and outlier data points are used to see how well the model is able to detect outliers. we perform 10-fold cross validation by dividing the normal data points into ten folds and training on nine of them while testing on the tenth. since no outliers are used during the training phase, we use all outliers during the testing phases of the cross validation. in what follows, we present two sets of experiments: in the first, we evaluate the quality of the solution achieved by our novel feature selection algorithm compared to other stateof-the-art methods. in the second set of experiments, we quantify the speedup obtained with a parallel gpu-based implementation of the lokdr feature selection algorithm, compared to a serial cpu-based implementation."
we evaluate the quality of the outlier detection results using the area under the roc curve (auc). the roc curve is a plot of the true positive rate (fraction of outliers correctly detected) versus the false positive rate (fraction of normal points misclassified as outliers). it represents the behavior of a classifier across a range of thresholds on the decision values.
"an associative memory network retrieves a memory pattern according to their network dynamics in which the memory patterns are stored in their synaptic connections. associative memory networks have also been well investigated [cit] . dynamics of memory retrieval can be characterized as the convergence of the state of the network to a fixed-point attractor that corresponds to a stored memory pattern [cit] . in this type of conventional model of an associative memory network, the state of the network usually remains in the attractor. in contrast to this, in an associative memory network with the depression synapses, the memory retrieved state can be destabilized and the state of the network can move to another attractor that corresponds to another memory pattern. such transitive dynamics among several memory patterns has been also investigated [cit] . although stochastic neural networks with depression and facilitation synapses have been studied [cit], a comprehensive understanding of the dynamics of associative memory networks with dynamic synapses has not yet been achieved."
"in figure 5, we present the performance results. figure 5(a) shows the running time (in seconds) of the serial implementation of lokdr on all the datasets with increasing number of selected features. from this figure, we see that the arrhy dataset requires a longer running time than the microarray and mass spectrometry datasets. although it has far fewer features than the other datasets (276 versus 6000 and 7129), its larger sample size increases its runtime more dramatically than its features. this validates our analysis that the number of data points has a greater impact on the computational complexity of the serial implementation than does the number of features. another confirmation of this is found in the running time of lymph versus prost. while the number of features in lymph is greater than prost, the number of data points in prost is greater than lymph which leads to its slightly longer running time."
"in this section, we present the results of simulation in the stochastic model and of analyses of the macroscopic behavior in the associative memory model with dynamic synapses. in particular, we analyze the changes in the structure of the dynamics, depending on the parameters t, Ï f, Ï r, and u se ."
"from these results, it is clearly evident that exploiting the parallelism that exists in the lokdr algorithm can lead to great rewards in terms of computation time. data mining problems where adding more features can enhance the quality of the final solution further add to the appeal of a gpu-based implementation, with its ability to achieve vast speedups and performance improvements. the implementation of our proposed feature selection algorithm for outlier detection on a gpu gains from its ability to perform processing on data points and feature subsets concurrently, allowing the performance to scale nicely with respect to both."
"an integral part of any data mining task is having a good set of features that can be used to accurately model the inherent characteristics of the data. in practice, the best set of features is not known in advance. therefore, a pool of candidate features are collected and processed to removed irrelevant and redundant features. this can improve both the memory and computational cost of the data mining algorithm, as well as the accuracy of the learner. reducing the space of possible features is done in two ways: feature transformation and feature (subset) selection. in the former, the original space of features is transformed into a new feature space, as in principal components analysis (pca)."
"in future work, we will incorporate other search techniques using our novel feature selection criterion. in particular, the backward search method can reap massive rewards in terms of running time savings since it begins with all the features and removes them one by one. this necessitates a great number of computations for distance calculations and a gpu implementation can perform them concurrently, yielding significant speedups."
"here, we assume that the correlations among variables s j (t), x j (t), and u j (t) are negligible on the basis of the following considerations. the correlations among the variables s j (t), x j (t), and u j (t)"
"dynamic synapses change their transmission efficacy depending on the activity of the presynaptic neuron, and the postsynaptic response can be decreased (short-term depression) or increased (short-term facilitation) [cit] . synaptic transmission is carried out by the flow and diffusion of chemical components. activation of a presynaptic neuron and generation of an action potential causes influx of calcium ions into the presynaptic membrane. a chemical reaction with the calcium ions triggers the release of the neurotransmitters and induces the post synaptic current. if many action potentials are generated in a short period of time, the calcium concentration and the fraction of releasable neurotransmitters change, and the transmission efficacy increases or decreases transiently. change in the transmission efficacy is modeled by variables that represent the releasable neurotransmitters and the utilization parameter that defines the fraction of the neurotransmitter release by each action potential, reflecting the calcium concentration."
"in particular, we target stability analysis on the associative memory network with correlated memory patterns. the properties of the dynamic synapses are characterized by parameters that specify the time constants of recovery from an active state to a resting state of synapses. in the models of short-term plasticity the difference between depression and facilitation can be specified using theses parameters. we investigate how the dynamics of the associative memory network depends on these parameters."
"the remainder of the paper is organized as follows. in section ii, we discuss related work and in section iii, we describe the details of our proposed local kernel density ratio feature selection algorithm. in section iv, we present the results of our feature selection algorithm on several real-word datasets, including an analysis of the performance gained by a gpu-based implementation. finally, section v concludes the paper and presents directions for future work."
"note that the independency between x j (t) and u j (t) is reported to hold if there is no facilitation . thus, we evaluate the validity of this assumption by comparison between the simulation and the mean field model derived by this assumption. as we show in \"results\" section, the mean field model shows good approximations. by using these relations (15) and (16), the microscopic mean field model is derived as"
"the phase diagrams in figures 6, 7 show sets of bifurcation points that switch the stability of the fixed points and the distribution of the oscillatory states obtained by the brute-force methods. we calculated the time evolution of the macroscopic mean field model on each parameter points; the parameter points where the orbit converges to the oscillatory states are indicated by colored dots in figures 6, 7 . in the higher-noise boundary of the oscillatory state, the oscillatory states are separated by the supercritical type of the ns bifurcation; the region of the oscillatory states is well separated by the sets of the ns bifurcation. on the other hand, the oscillatory states appear with the subcritical type of ns bifurcation in the lower-noise boundary. thus, the oscillatory states and the steady states coexist as multi-stable states in this region. similar bifurcation structure is found in the uniformly connected network [cit] ."
"data classification is an important task in the field of data mining and machine learning [cit] . it is to choose the classified training set from the data and use the classification technologies of data mining for the training set in order to set up the classification model, which is used to classify the data without classification data. the data classification methods has a wide range of applications, and can be further derived from the new algorithms. in recent years, there proposed a lot of data classification methods of data mining, such as decision tree, bayesian, neural network, support vector machine, genetic algorithm and so on [cit] . these data classification methods have good effect for low dimension data. however, for high dimension data, the application ability is obviously reduced. so it has actual significance for researching the data classification, which is a hot research in the field of data mining."
"the rest of this paper is organized as follows. section 2 briefly introduces the ant colony optimization (aco) algorithm. section 3 briefly introduces the strategies of the pheromone evaporation rate strategy and pheromone updating rule for improved aco algorithm. section 4 briefly introduces support vector machine (svm) model. section 5 proposed parameter optimization model of svm based on improved aco algorithm. section 6 gives experimental results and conclusions. finally, the conclusions are discussed in section 7."
"in order to better explore the decay model of the evaporation coefficient, the curve decay model is selected according to the experiment in this paper. the curve decay model is described as follow:"
"the local outlier factor (lof) algorithm [cit] solves the outlier detection problem using a ratio of densities. in the lof algorithm, the density of a data point is compared to that of its neighbors and based on this, the point is assigned a degree of being an outlier, known as its local outlier factor. the lof of a data point is calculated as the average density of data points within its neighborhood divided by its own density. when a data point has a low density compared to points in its neighborhood, it is more likely to be an outlier. conversely, outliers should have a lower density compared to its neighbors. therefore, it stands to reason that a feature set which emphasizes this phenomenon would facilitate the detection of outliers."
"to quantify the similarity between the state of the network s(t) and the Î¼th memory pattern Î¾ Î¼, we use an overlap given by"
"although these proposed data classification methods can better realize the data classification, but the classification accuracies of them are low and slow speed. in allusion to the existing shortcomings of the svm and improved svm in classification, an improved ant colony optimization (aco) algorithm is introduced into the support vector machine(svm) model in order to propose a new data classification(eruraco-svm) method. the pheromone evaporation rate strategy and pheromone updating rule are use to improve the optimization performance of the aco algorithm, which is used to find the optimal combination of parameters of the svm model in order to improve the learning performance and generalization ability of the svm model and establish the optimal data classification model."
"in the os2 state shown in figure 3b, the network oscillates between one of the memory patterns and its inverse pattern; one of the overlap (m 1 in the figure 3b ) oscillates with larger amplitude than others. the remaining two overlaps oscillate in phase. because the model is symmetric, three possible patterns of oscillation exist and the realization of the oscillatory pattern depends on the initial state of the network."
"the (t, Ï r ) phase diagram in figure 6a shows changes in the dynamical properties of the network from the pseudo-constant region to the depression-dominant region. as Ï r increases, the regions of the stable fixed point of mem, smix, and amix shrink, while the regions of the para state and the oscillatory states expand. the (t, Ï f ) phase diagram shown in figure 6b illustrates the dynamical properties from the pseudo-constant region to the facilitation-dominant region. as Ï f increase, the regions of mem, smix, and amix expand, while the region of the para state shrinks. furthermore, the oscillatory states appear. as Ï f increases from the depression-dominant region (figure 6c ), the regions of the oscillatory states expand. as u se increases, the region of the para state expands, while regions of other states shrink."
classification parameter to balance the fitness error and model complexity. the optimization problem transforms into its dual space. lagrange function is introduced to solve it. the corresponding optimization problem of the ls-svm model with lagrange function is shown:
"(1 (2) the pheromone update rule in order to improve the solution, the pheromone trails must be updated. trail updating includes local updating and global updating. the local trail updating formula is given by:"
"the (t, b) phase diagrams in figure 7 show that the dynamical properties of the network depend on the correlation level between the memory patterns. as b increases, the region of the smix state expands, while regions of the other states shrink. in the depression-dominant range (figure 7b), as the correlation level b increases, the region of the os3 state shrinks but that of os1 state remain, which corresponds to the oscillatory state between smix states. in the facilitation-dominant range (figure 7c), the overall bifurcation structure is similar to that of the pseudo-constant range, but the region of mem states expands."
"the inspiration for our feature selection algorithm came from an approach taken to solve the outlier (or anomaly) detection problem. hawkins [cit] describes an outlier as \"an observation that deviates so much from other observations as to arouse suspicion that it was generated by a different mechanism.\" in outlier detection problems, outliers typically comprise a small portion of the dataset. examples include intrusion detection data, where malicious attacks are less frequent than normal activity, and certain tumor datasets where patients with a particular type of cancer are few compared to the number of healthy patients. in some cases, obtaining samples from the outlier class may be difficult or expensive, such as a fault detection system where it can be expensive to obtain outlier data representing the faulty situations of a machine. therefore in the outlier detection domain, learners must deal with highly imbalanced data. next, we describe our criterion function for feature selection which caters to outlier detection problems and is insensitive to the degree of imbalance in the data as it is based on a ratio of average normal to outlier densities."
"the results of our experiments using nn, lof, and ocsvm show that in general, the classifiers perform comparably across the various feature selection algorithms and datasets with no clear winner. as an example, in figure 2 we present the average area under the curve (auc) results as a function of the number of selected features using 10-fold cross validation for all three outlier detection algorithms on the cns dataset with features selected by lokdr, bahsic, and lasso. on the -axis, we vary the number of selected features and on the -axis, we plot its corresponding average auc."
"in the conventional aco algorithm, the global update rule only strengthens the pheromone concentration on the shortest path in each iteration. but the pheromone concentration of the shortest path is excessively strengthened to easily fall into local optimal solution. the optimal path in each iteration will be make full use. so the adaptive dynamic factor"
data classification is an important concept and technology in the data mining. it includes establishing the classification model and classifying new data. support vector machine (svm) is a machine learning method with the good generalization ability and prediction accuracy. the svm model can effectively realize the classification for small sample and nonlinear problem. but the parameter of svm model seriously affects the generalization ability and prediction accuracy on the great extent. so an improved ant colony optimization (aco) algorithm is introduced into the support vector machine (svm) model in order to propose a new data classification(eruraco-svm) method. the pheromone evaporation rate strategy and pheromone updating rule are introduced into the aco algorithm to improve the optimization performance of the aco algorithm. then improved aco algorithm is used to find the optimal combination of parameters of the svm model. and the experimental data from the uci machine learning database are selected to validate the classification correctness of the eruraco-svm method. the results show that the eruraco-svm method takes on the strong generalization ability and the higher classification accuracy.
"in this study, we use an associative memory network comprising n binary neurons. the state of the neuron is determined stochastically depending on inputs to the neuron."
"one of the main components of calculating the criterion function is the -nearest neighbor ( -nn) search. a simple brute force approach is to calculate the distance between all pairs of points, requiring"
"as the goal is not to compare the outlier detection algorithms themselves, but rather to compare the proposed feature selection algorithm (lokdr) with previous feature selection methods, for the remaining figures of this section we shall present the outlier detection results using the lof classifier. the other classifiers produce similar results (cf. [cit] ). in figure 3, we show the auc results of the feature selection algorithms on the microarray and mass spectrometry datasets. with a horizontal line, we show the auc obtained when using all of the features. this displays the importance of performing feature selection, as all of the feature selection algorithms are able to surpass the auc achieved with the entire feature set. the figure also highlights the strength of the lokdr algorithm in selecting features for outlier detection. across the datasets, the features chosen by lokdr enable the outlier detection algorithm to identify outliers with a high detection rate and few false positives, as reflected in the high average auc. for the cns, lymph, and ovary datasets, as the number of selected features increases, the average auc for lokdr rapidly exceeds that of the other methods. for the prost dataset, the performance of lokdr starts out the strongest and continues to be competitive with the other methods."
the stability of the system can be determined by the eigenvalues of the jacobian matrix on the steady state; the stability is distinguished by the absolute value of the eigenvalues. elements on the jacobian matrix k are given as
"feature selection is a very important and well-studied problem in data mining. most of the work have focused on the area of feature selection for classification and regression [cit], and as far as we know, there has been no work done to create a feature selection algorithm that caters specifically to outlier detection problems. in our work, we propose the first feature selection algorithm that takes the subsequent task of outlier detection into consideration and chooses features to intrinsically enhance the identification of outliers."
"essentially, the two fields in a table entry store both the qualitative and the quantitative evaluations for each candidate spawning point. they are used by the dynamic dispatching policies described next."
we utilize the continuity analysis of sub-strokes to determine whether a pair of sub-strokes jointed with the same singular region will be contiguous into a stroke. the local continuity analysis is defined as follows:
"the whole stage of parsing in a chinese character is divided into two sub-stages, i.e., single component matching and all components matching. the first sub-stage can be solved by heuristic search algorithm to find out the best set of components in the input character. the second sub-stage is categorized as the variant knapsack that demands us to find the best combination of the components found by the first stage."
"3.3.1. evaluation using the correctness measure. for a loop invocation, if t pseq and t seq are both greater or both smaller than t t ls, the prediction is considered correct, otherwise it is incorrect. we categorize loops into three groups: (i) loops whose parallel and sequential performance differ by less than 5%, the rest of the loops that are (ii) correctly predicted, and (iii) incorrectly predicted. the first loop category is excluded in the correctness measure since whether or not these loops are parallelized contributes little to the final performance."
"where s c i is the ith sub-volume sample from activity c; s c i ( : ) is a lexicographical operation on s c i to form a column vector; ( ? ) is a nonlinear convex function (e.g., smooth l 1 penalty function ( ? )~log cosh( ? ) [cit] in this paper); and k, l are the number of features (rows of w c ) and a balancing parameter, respectively."
"scenario iii. multiple speculative threads concurrently executing on multiple cores can further complicates l1d cache performance. when two threads on different cores load from the same address, they incur two l1d cache misses, as shown in figure 4 (a). however, in sequential execution, it is likely that the logically later miss on the second thread becomes a hit since the threads are executing on the same core. thus, to correctly predict sequential performance, latency associated with l1d caches misses that return the data item in shared state must be omitted from the prediction."
"in the evolution process of chinese character, the shape and topology have had the huge variation at different writing stages. for example, this legibility stems from the highly rectilinear structure, the structure tends to be square, and so on. but the forms and meanings of chinese character is preserved and passed down by using the components and the component functions. so, all the chinese characters on the same historical diachronical level have their own component elements, and all the chinese characters on the different historical diachronical level are connected into a sign system by using the semantic relevance between components. for example, figure 2 shows the correspondence between components of chinese character``$'' in differen scripts."
"where d, r a and r b are reduced from the svd [cit], d is a diagonal matrix, r a and r b are two unitary matrices. we can compute a(t) by linearly interpolating the free parameters in above factorizations:"
"upon a first level data (l1d) cache miss, we must check whether the cache access has found an invalidated cache line with a matching tag. if so, it indicates that this cache line has been loaded into l1d, but then speculatively modified by a thread that is eventually squashed. at this point, the original data must be reloaded from the next level cache. this cache miss would not have occurred if this program is executed sequentially (figure 3.2) . thus, the stall cycles in the accumulator are discarded."
"in this section, we propose an approximate sparse coding scheme and compile a set of sparse histograms. any sample x can be sparsely represented by w as following"
"6.4.2. art. differences in the performance of art, as shown in figure 10, partly come from source code in scanner.c: static analysis chooses the inner loop starting at line 589, whereas dynamic mechanism favors the outer loop starting at line 584. figure 13 (a) and figure 13(b) show the code snippets of these two loops and contrast their performance. bars are labeled as in the previous section."
"based on the chinese character structure theory, the chinese character teaching system is proposed. the teaching system includes two main phases, i.e. the offline phase and the online phase. figure 3 shows the system architecture and the relationships between technologies."
"the offline phase implements the digital representation of the chinese character structure theory and constructs the knowledge database, including the component models database and the components correspondence database. the component models database mainly is used to store the digital representation of the component for each script. based on the component models database, we can parse the input chinese character into a set of component, as shown in figure 2 . to this end, we input the binary images of components, extract the strokes in given components, and represent the strokes and their relationships by using the statistical structure method. all above information are stored in the component models database."
where s is the coefficients of sample x. the first n s (400 in this paper) largest coefficients are kept and the rest coefficients of s are set to zero to make s sparse.
"scenario i. speculative threads that bring data into l1d can be squashed and then reexecuted. when this occurs, a straightforward classifier would count all the cycles, from the point of thread starting to the point of squashing toward squash. note that this classifier counts all stalls due to l1d cache misses as part of squash. however, this simple classification can be inaccurate if the data is used when the thread reexecutes. this is because when the squashed thread reexecutes and accesses the data for a (a) in tls execution, a data item loaded into the l1d cache by a speculative thread that is eventually squashed is reused when the thread reexecutes with no additional cache miss. in sequential execution, this access incurs one initial cache miss. (b) in tls execution, when data modified by a squashed speculative thread is reused, this access incurs an additional cache miss. in sequential execution, there will be no additional cache miss. second time, it will not incur another cache miss, as shown in figure 3 (a). however, since squash cycles are discarded in the prediction, the latency associated with this memory access is inadvertently not counted towards the sequential execution time. to rectify this counting inaccuracy, cycles stalled due to l1d miss during the squashed execution must be recorded separately and nevertheless contribute to the predicted sequential execution time."
"a general trend of increasing performance is observed as the prediction scheme is amended to model tls execution in more details. base yields the lowest performance because the inaccuracy in prediction causes the runtime system to often target on wrong loops. execution stall scaling and data cache behavior classification prove to be useful in correcting this inaccuracy. for example, in twolf incorporating execution stall scaling recovers most of the performance loss; while in art, data cache behavior classification is key to prediction accuracy; nevertheless, gcc benefits from both amendments. mcf is a memory-bound benchmark. only applying execution stall scaling happens to expose more inaccuracy; however, further incorporating data cache behavior classification ultimately corrects this error."
"to present the evolution of chinese characters in different scripts, the chinese character is parsed into the components by using the stroke-based representation. the component models database need be built in the offline phase. then a chinese character is decomposed into the components in the online phase. we introduce the component modeling and the component recognition, respectively."
"to evaluate the performance of our system, we invite two types of persons to use and evaluate our system from different perspective. the one type is the chinese character teachers who have rich experience about the ancient chinese character. the database of components correspondences is very important in the system, it mainly depended on the knowledge of chinese character teachers. so, these teachers are ask to use the correspondence labeling tool to construct the database. after that, they were asked to complete a questionnaire with designed questions to evaluate the labeling process and the morphing result. the questions are listed in table 2 ."
"parallel code overhead can also be mitigated in several ways. using simple heuristics, the compiler or programmer can filter out some loops first. for example, loops with tight dependencies (such as pointer chasing or short reduction) are unlikely to benefit from tls. a runtime reoptimization system can also eliminate such overhead for all the loops not selected for tls execution. we do not explore these optimizations and leave their integration as future work."
"there is also a large body of previous work on runtime performance optimization for parallel applications (such as openmp [cit] ]) [cit] . stomp [cit] ] selects among multiple specialized versions of parallel regions based on parameters; [cit] experiment with different openmp scheduler configurations at different parallel regions. [cit] peel parallel loops and collect performance profiles using the first few iterations of the loop to reoptimize the program dynamically. however, performance optimization for openmp is very different from that for speculative threads. first, the optimization goals are different: for openmp system, the tuning knobs are the number of threads [cit], shared variables [cit] ], etc., while our system optimizes tls performance by redeciding where to speculate. second, the performance models for openmp and tls differ significantly: in openmp, all threads perform useful work, while in tls, work done by speculative threads can be wasted when speculation fails. to summarize, the knowledge learned from openmp performance optimization cannot be directly applied to tls. however, we believe that our performance analysis technique (cache behavior classification, execution stall scaling, etc.) and dynamic dispatching policy can be applied to openmp threads to understand their performance and select which loop level to parallelize."
"we choose two representative scripts in the experiment, i.e. seal script and clerical script. the seal script is called ancient script, while the clerical script is known as modern script. the frequently used chinese characters in seal script and clerical script are used as test data of the system and several evolution animations of the chinese characters are generated by using the system. the rest of the paper is organized as follows. firstly, the related work is discussed, and then the chinese character teaching system is introduces briefly. moreover, the strokes extraction, the chinese character parsing and the chinese characters morphing are described in detail. finally, the experimental results and the conclusion are presented."
"with the quant+statichint policy, almost all benchmarks benefit. inaccurate static loop selections are overridden in gzip and mcf, so the performance becomes similar to quantitative. at the same time, twolf enjoys the benefit of accurate compiler annotations. more importantly, for ammp, art, gap, gcc, mesa, and vpr-r, loop levels with higher performance are discovered and parallelized, which leads to better results than both quantitative and quant+static. in section 6.4, we will look into a few benchmarks as case studies to illustrate how this dispatching policy searches the loop nest and to offer insights into why it outperforms static analysis. across all the benchmarks, quant+statichint generally yields the best performance among all dispatching policies, while negligible overhead is observed in some benchmarks due to tentatively trying out nonoptimal loop levels for comparison. the average speedup to sequential execution is 1.371x. 6.1.5. summary. performance gradually improves as we build up the desired dispatching policy, rising from the most basic that achieves 1.216x to the most sophisticated that achieves 1.371x."
"given a chinese character, its contour and corners are extracted. then the contour is evenly divided into a set of discrete points by uniform sampling between any two corner points. based on the discrete points, the chinese character is represented as a set of triangular meshes by using the constrained delaunay triangulation (cdt) [cit] . an example is shown in figure 4 (a, left). according to the number of internal edges, there are three types of triangle meshes in the representation. the junction triangle owns three internal edges, while the normal triangle owns two internal edges and the terminal triangle owns one internal we use the point to boundary orientation distance (pbod) curve [cit] to merge the junction triangles. the pbod curve is a vector, and the k -th value in the vector is the distance from the central point of triangle to the boundary along the k -th quantized orientation, where k~1,2,:::,m and m is an integer that denotes the quantization number from 0 0 to 360 0 . the number of the crests in the pbod curve is used to determine the degree of the central point of junction triangle. if d k wv Ã£ max(a,b,c), a crest is determined, where a, b and c are edges of junction triangle, v is a constant value. if degree of the central point of junction triangle is less than 3, the junction triangle is a spurious junction triangle. then we delete it from the junction triangle set. we compute the distance between central points of two junction triangles, p 1 and p 2 by using the equation d~p 1 p 2 3 . if dvt d, the two triangles need to be merged. when all junction triangles are merged, the singular region is generated, as show in figure 4 (a, right)."
"after all the singular regions are detected, a chinese character is divided into two types of regions, i.e., the sub-strokes and the singular regions. the sub-strokes are defined as a segment separated by a series of singular regions and a stroke is considered as the concatenation of sub-strokes and singular regions."
"our performance prediction relies on cycle breakdowns of speculative parallel execution. note that the goal is not to build accurate tls execution cycle breakdowns, but to generate a set of components to help reconstruct the sequential execution cycle. obtaining cycle breakdowns in an out-of-order processor is difficult due to the overlap of multiple on-the-fly instructions. examining the instructions at the head of reorder buffer (rob) gives us some clues [cit] ] to the causes for stalls. we propose a cycle classification scheme that works on programmable hardware counters that is common on modern processor chips. figure 7 depicts our use case that aims to obtain the following breakdown components: -useful: cycles spent graduating useful instructions, that is, instructions not related to speculative thread management; -ilpstall: cycles stalled due to lack of instruction-level parallelism; -instfetch: cycles stalled due to instruction fetch penalty; -datacache: cycles stalled due to data cache misses; and -nconflict: the number of conflict misses that need to be compensated when predicting sequential execution time."
"the first policy follows an inside-out search order, that is, from the innermost to the outermost, to evaluate the impact of tls for each level in a loop nest. each level runs in tls mode for several invocations, and the number of times tls performs worse than sequential execution is recorded in the saturation counter. similar to branch prediction, once this counter exceeds a certain threshold, the loop is predicted as not suitable for tls. this decision is stored in the decision table, so this loop will be serialized whenever it is encountered again, yielding resources to its outer loop levels. otherwise, the current loop is continuously parallelized, and the search for the loop nest may stop at the current level. we name it insideout since it is the fundamental search order for other policies."
"in figure 10, static and dynamic are both normalized to the execution time of the original sequential executable where no instrumentation is made. over all the benchmarks, dynamic dispatching (1.371x) outperforms static analysis (1.253x) by 9.4%."
"if a cache miss is served by the l1d cache of a different core (feedback from underlying cache coherence protocol), it is possible that this cache miss would not have occurred in sequential execution, as illustrated in figure 3 .2. this cache miss behaves like a coherence miss. since coherence misses cannot occur during sequential execution, the stall cycles in the accumulator is also discarded. figure 4 (b) depicts a different scenario, where a cache line brought in by an earlier load is evicted due to replacement. rereferencing to this cache line will cause a cache miss, and it is possible for this cache miss to be served by a neighboring l1d cache. in this case, the same coherence messages will be exchanged between the caches as before; however, this cache miss is likely to incur a conflict miss in sequential execution. in other words, conflict misses in sequential execution can behave similarly to coherence misses in tls execution. however, stall cycles that correspond to coherent misses are always discarded for predicting sequential execution time. thus, a new mechanism must be introduced to compensate this effect. we propose to augment each l1d cache line with one bit and introduce a new counter to count the number of occurrences of such cache misses in each core. we refer this bit as extrashared and the counter as nconflict. the scheme works as follows: (i) extrashared is initially set 0 for all cache lines when tls execution starts; (ii) when a cache line is fetched to serve a cache miss from another cache, set the extrashared bit for that cache line; (iii) when a cache line is replaced, increment the nconflict counter if the extrashared bit is set, and reset extrashared bit; (iv) when the nonspeculative thread commits, the value of nconflict is forwarded and accumulated as other cycle counters."
"our tls system focuses on parallelizing loops from sequential programs. many such programs contain multiple nested loops, and thus the dynamic dispatching policy is required not only to identify and parallelize loops that can benefit from tls, but also to select the right level of loop to maximize the overall performance gain. a straightforward mechanism is to first tentatively parallelize each loop, measure the performance impact, and then serialize the ones for which tls execution is ineffective. however, there can be various ways to determine the order in which loops in a loop nest are evaluated and to decide the precedence among different loop levels. in this section, we first examine the design issues for the dynamic dispatching system and then build the most effective policy in incremental steps."
"in this paper we proposed a real-time algorithm that makes use of joint information to recognize human activities. in the first step of the algorithm, videos of 3d movements of body joints are sampled to obtain a set of spatialtemporal 3d volumes, which entail the complex spatial-temporal relationships of joints of human activities at a data size that is much smaller than that of a rgbd volume. second, rica is performed on the spatial-temporal 3d volumes to obtain a set of dictionaries of codes that form a sparse representation of human activities. an approximate spare coding scheme is then used to compile a set of spare histograms as features for activity recognition. finally, a multi-class svm is used to perform activity recognition. we performed extensive tests on this algorithm on three widely used datasets of human activities. our results show that this algorithm produces so far the best recognition accuracy on these datasets. our algorithm automatically learns discriminative features for activity recognition and is very fast and easy to implement. since joint information can be obtained by low-cost cameras such as the microsoft kinect systems, our algorithm can be used in smart environments and deployed in robots for human-robot collaborations. this model can be improved by the rich information in depth images. to include this information, we will extend the model presented here and our recent model of activity recognition based on multi-scale activity structures [cit] ."
"the study was approved by the human research ethics committees for non-clinical faculties, in the beihang university and the beijing normal university. the proposed system was supported by state key laboratory of virtual reality technology and system, beihang university. the data of different writing stages of chinese characters were obtained from the school of chinese language and literature, beijing normal university. the all of authors from the two institutions declare that there is no actual or potential conflict of interest in this paper. in the study of user experience, there were 102 students and 5 chinese character teachers who volunteered to participate in the experiments. this experiment was affirmed to cause no potential injury."
"the objective function in (7) is a smooth function. the optimization problem (7) can be easily solved by any unconstrained solvers (e.g., l-bfgs and cg [cit] )."
"the quality of compatible triangulations is unlikely to be acceptable. therefore, optimization techniques must be applied to improve the quality. we simultaneously use the edge-flips, the mesh smoothing and the mesh refinement to generate the high quality compatible triangulations with a small number of triangles [cit] . the optimization result is shown in figure 6 (b)."
the system is implemented on a pc with intel core 2.66 ghz processor and 4.00 gb memory by using the development environment of visual c++6.0. the main module interfaces are showed in figure 7 and figure 8 .
"judged by the execution time breakdown of 584 tls4 alone, this loop should not be parallelized since the cost of speculation failure (the squash segment) is high. however, when compared against the sequential execution 584 seq, the failed speculative threads help to fetch useful data into the l2 data cache and reduce the data cache stalling (dcache) to a large extent. while this cache effect (discussed in section 3.2) benefits parallel execution, its impact is hard to accurately estimate at compile time. our compiler uses dependence profiles to estimate speculation failures, so loop 584 is determined not ideal for tls due to possible speculation failures from runtime aliasing. this is why the static analysis chooses the inner loop 589 for tls. unfortunately, the performance of inner loop 589 is not up to the expectation due to the smaller coverage and insufficient thread count (part of others in the breakdown). insufficient thread count means the total number of thread is less than the number of available processing cores (four in our experiment), so some cores are left idling when this loop is being executed. this contributes most of the others segment in the breakdown. dynamic dispatching policy is able to compare the compiler-selected inner loop 589 with the outer loop 584 and ends up selecting loop 584 for better performance gain."
"due to the cross between strokes in chinese characters, there are many ambiguous zones called as singular region in the stroke extraction. we propose a novel method to detect the singular region."
"we propose a method to learn a set of sparse codes that can be used to represent human activities. sparse representation is useful for object recognition [cit] . a number of algorithms have been proposed to learn sparse features, including restrict boltzmann machines [cit], spare auto-encoder [cit], independent component analysis [cit], sparse coding [cit], and rica [cit] . since rica works well on approximately whitened data and is fast [cit], we use rica to learn a dictionary of codes from a set of sub-volumes s i,i~1,2, Ã¡ Ã¡ Ã¡,n s for each activity. the learned dictionary is called ''joint-dictionary''. to the best of our knowledge, this is the first work on feature learning from 3d movements of body joints."
"automatic thread extraction done by the compiler is desirable, but is challenging due to ambiguous memory aliasing and its effectiveness has been limited for generalpurpose applications. in the presence of complex control flow and ambiguous pointer usage, traditional parallelization schemes must conservatively ensure correctness by synchronizing all potential dependences, which inevitably limits parallel performance."
"in this section, we evaluate how accurate we are able to predict sequential performance from speculative parallel execution. to simplify our evaluation, we focus only on loops that are selected for parallelization by the compiler [cit] . these loops are normally important and representative. for each loop invocation, three sets of execution cycles are obtained: the sequential execution cycle t seq, the tls execution cycle t t ls, and the sequential cycle predicted from parallel execution time breakdown t pseq . we use two metrics for evaluation: (i) the percentage of total loop execution we are able to correctly determine whether or not parallel execution outperforms sequential execution (correctness measure); and (ii) the difference, in cycles, between the predicted sequential execution and the real sequential execution cycle (similarity measure). we evaluated both metrics on three increasingly complex schemes: base corresponds to the baseline prediction scheme described at the beginning of the section; base+exestall incorporates the execution stall scaling described in section 3.1; and base+exestall+dcache further incorporates the data cache behavior classification described in section 3.2. performance evaluation on how these three schemes work with the rest of the system can be found in section 6.2."
"where s is the sparse coefficients of sample x represented by dictionary w. a number of algorithms have been proposed to solve the above problem of sparse representation [cit] . instead of solving the optimization problem (9) for each video, which is prohibitively time consuming, we propose to project any sample x onto w via"
"we have evaluated the performance of our method on three public datasets. our method has four steps: generating samples, learning dictionaries, constructing sparse histograms, and classifying via svms. in this section, we replace the ricabased dictionary learning in our method with the k-means clustering. we cluster samples with the k-means algorithm and take the clusters as words in the dictionaries. we call this method as a baseline method. the results of this baseline method and our original method on the three datasets are shown in table 6 . both methods perform well, with our original method being slightly better. thus, the joint dictionaries and sparse histograms in both methods are responsible for the good performance."
"the clerical script also is an archaic style of chinese scripts which evolved in the warring states period to the qin dynasty, was dominant in the han dynasty, and remained in use through the wei-jin periods. due to its high legibility to modern readers, it is still used for artistic flavor in a variety of functional applications such as headlines, signboards, and advertisements. this legibility stems from the highly rectilinear structure, a feature shared with modern regular script (kaishu). in structure and rectilinearity, it is generally similar to the modern script; however, in contrast with the tall to square modern script, it tends to be square to wide, and often has a pronounced, wavelike flaring of isolated major strokes, especially a dominant rightward or downward diagonal stroke. some structures are also archaic."
"after the completion of learning, the students were asked to complete a questionnaire with designed questions. there are five choices for each question to evaluate the effect of the proposed system. the questionnaires are listed in table 3 . the students responses to the questionnaire are shown in figure 13 . it can be observed that 58% students think that the proposed chinese character teaching system is useful for their learning, while only 9% of the students showed negative opinions. moreover, 69% of the students think it is very or somewhat interesting and 55% of the students are willing to recommend other students to try this system. in addition, 74% of students think that it is easier to study ancient chinese by this system. in a word, the proposed system on ancient chinese character teaching is deemed useful and interesting for most students."
"we build our simulation infrastructure based on a trace-driven, out-of-order superscalar processor simulator. the trace-generation portion is based on the pin instrumentation tool [cit], and the architectural simulation portion is built on simplescalar [cit] ."
"4.1.2. aggregating counters across multiple cores. speculative parallel threads are distributed across multiple cores, and thus performance counters must be aggregated to obtain the complete cycle breakdown. there is only one nonspeculative thread and only the nonspeculative thread is allowed to commit. at thread commit time, the nonspeculative thread forwards its performance counters to its successor, and makes the successor nonspeculative. the new nonspeculative thread adds the forwarded values to its own corresponding counters. therefore, when a speculatively parallelized region completes, the counters on the core that commits the last thread contain the complete breakdown."
"the overhead of dynamic thread dispatching, such as trying an ineffective loop, is faithfully included in our simulation. to reduce simulation time, we have adopted the simpoint-based sampling technique [cit] ] with 100 million instructions per sample and up to 10 samples per benchmark. up to one billion instructions can be simulated for each benchmark. with the sample size of 100 million instructions, the side effect of warming-up is negligible."
"there are a number of cases where dynamic dispatching is worse compared to static analysis, such as in bzip2, crafty, perlbmk, twolf, vortex, and vpr-r. we find that it is the parallel code overhead that causes most of the performance downgrade. we run the static and dynamic executables sequentially, and their execution time is shown in figure 11 (a) as bars staticseq and dynamicseq, respectively. the higher the bar is, the greater the overhead is. in figure 11 (b), static/staticseq and dynamic/dynamicseq are normalized to their corresponding sequential baselines. in this rescaled comparison, most previously under-performing benchmarks has shown comparable or even better performance under dynamic dispatching. overall, dynamic dispatching (1.475x) could potentially outperform static analysis (1.271x) by 16.0%."
"n a general dictionary-based framework that automatically learns sparse, highdimensional spatial-temporal features of 3d movements of joints, n an efficient method that constructs sparse codes and histograms, n a real-time system for human activity recognition that can be easily implemented, n extensive evaluations on the proposed model and superior results on three datasets of human activities."
"figure 5(a) shows the degree of correctness for all benchmarks. overall, the correctness measure improves as the proposed execution stall scaling and cache miss cycle classification are incorporated. with execution stall scaling, the correctness is significantly improved in mesa, perlbmk and vpr-r. with cache miss cycle classification, the correctness is further improved, especially in bzip2, crafty, gzip, mesa, parser and perlbmk. there are two exceptions (ammp and vortex), however, that do not follow this trend of improvement, and base+exestall+dcache yields noticeably lower correctness than base. the reason is that base in these two benchmarks heavily biases towards one side of prediction that happens to match the actual situation. the extensions to the prediction remove such biases but result in lower degree of correctness. nevertheless, base+exestall+dcache always yields the highest similarity in these two benchmarks (next subsection), showing its effectiveness. overall, base+exestall+dcache correctly predicts 93.4% of the loops, a 9.7% improvement compared to the base scheme."
"this section contrasts our most successful dispatching policy (quant+statichint) with the state-of-the-art static analysis. before presenting a detailed comparison, we first explain the overhead introduced to tls. to parallelize a loop, necessary special instructions (thread spawning, synchronization instructions such as signal and wait, and thread committing) are generated. these extra instructions are tls overheads. for static approach, the compiler selects only a subset of loops to insert these instructions. but for dynamic approach, all the loops are instrumented so that any loop may be eligible for parallel execution. when the code is running in sequential mode, tlsspecific instructions are executed as nops, but still incur extra performance penalty. if a program has a large number of small loops that are instrumented but eventually serialized, the slowdown due to this overhead can be significant. we referred to this as parallel code overhead for tls."
"this research used hardware-performance-counter based techniques to help determine where to create parallel speculative threads. in this section, we will discuss related work in two areas of research: dynamic optimization and determining where to parallelize."
"section 3 has proposed three increasingly complex schemes to predict sequential execution and evaluated their accuracy. in this section, we will evaluate their performance impact for the dispatching system. for the purpose of this comparison, we will use the most successful dispatching policy quant+statichint. figure 9 contrasts the performance of these prediction schemes. base corresponds to the baseline prediction scheme described in section 3; base+exestall incorporates the execution stall scaling described in section 3.1; and base+exestall+dcache further incorporates the data cache behavior classification described in section 3.2."
"the experiment of component recognition and parsing also is tested in the chinese character. we compare our method with representative works on component recognition. the experiments are executed on two datasets, i.e., the test 1 includes 500 chinese characters in different scripts, while the test 2 has 1000 chinese characters. the compared method is the nonlinear pca method and the stroke-based method [cit] . the comparison results are shown in table 1 ."
"finally, an effective mechanism should adapt to program phase changes. when a program enters a different phase, loop behaviors can change substantially. loops that are previously serialized could potentially benefit from tls in the new phase. therefore, it may be necessary to reevaluate and reselect the best performing loops when phase changes. we describe our dispatching mechanisms in four incremental policies, each extending the previous one with higher complexity."
"however, these factors are often difficult to estimate even with accurate profiling information. for example, the rate of speculation failures not only depends on the number of interthread data dependences, but also on the timing of its occurrence. for loops with complex control flow, it is difficult to determine whether the load/store instructions in consecutive iterations are dependent; and if they are, which loads will cause dependence violations at runtime. probability-based data and control dependence profiling, which is used in many tls compilers, is insufficient to come up with such estimations."
benchmark vpr-r exhibits some abnormality as incorporating data cache behavior classification results in lower performance. this is due to a simplification when predicting conflict misses in sequential execution. we will discuss this problem and its possible remedy in further detail in section 6.3.
"we propose to learn a set of dictionaries of sparse codes to represent the complex spatial-temporal relationships among body joints. for this purpose, we introduce some notations first."
"dynamically managing speculative threads is an attractive alternative. when the execution of speculative threads is monitored, it is possible for the runtime system to accurately determine their impact and adjust their behavior accordingly. to build such a system, the following issues need to be addressed:"
"and other overheads. the performance impact of speculation is determined by the number of useful execution cycles in the speculative threads that can overlap with the execution of the nonspeculative thread. to determine this overlap, the compiler must determine the size of consecutive speculative threads, the cost of speculation failures and the timing of their occurrence, the cost of synchronization, and the cost of managing speculation."
"there are three features of the connected region are used in the process of continuity analysis. the connected region is a part of the sub-stroke and close to the singular region, as shown in figure 4 (c). the mean width w of the connected region is approximated as:"
"6.1.1. innermost vs. insideout. comparison between the first two bars indicates that insideout generally outperforms innermost, however with the exception of gzip, mcf, and vpr-r. a close examination of the execution traces reveals that insideout missed some profitable inner loop levels in these benchmarks due to premature serialization (section 5.1), whereas innermost persistently parallelizes the innermost loop levels regardless of their performance. however, for many other benchmarks, our first dispatching policy greatly outperforms innermost by a large margin, especially in ammp, art, mesa, and vpr-p, showing its usefulness. insideout achieves an overall speedup of 1.216x, while innermost achieves 1.146x. 6.1.2. simple vs. quantitative. quantitative weights different loop invocations by the cycles saved from sequential execution, and is more accurate than insideout in identifying profitable loop levels. it resolves the problem of premature serialization, as the performance is improved in gzip, mcf, and vpr-r."
"the trace generator instruments all instructions to extract information such as instruction address, registers used, memory address for memory instructions, opcode, and etc. the entire trace of the instruction stream is output to disk files."
"for the character recognition, there are two methods to represent and analyze the chinese characters, i.e., the statistical method and the structural method. the statistical methods usually process the chinese characters as a feature vector so that their similarity can be calculated by the distance between two vectors. various statistical methods have been proposed for chinese characters recognition, such as k-nearest-neighborhood classifier [cit], k-means clustering and gaussian distribution selector [cit], nonlinear active shape models [cit], contextual vector quantization [cit] and mahanalobis distance [cit] . the structural methods extract the structure information from the chinese characters which emphasizes the relationship between strokes. chinese characters are often decomposed into strokes to analyze. the stroke and theirs relation information both are used to distinguish two chinese characters. a combined method is also proposed for the character recognition. in this method, the stroke is represented by using the distributions of its length, position and angle [cit] . the position and the shape of strokes are statistically modeled and their distributions are estimated from training samples. this statistical modeling is adopted to represent both statistical and structural information of characters. the mrf-based method proposed by zeng [cit] makes use of the energy of cliques to measure the similarity between the target character and the model character. it provides a much more enriched vocabulary to describe character structures."
"in online phase, the input chinese characters firstly are split into components by matching the extracted strokes with the component models from the component models database. then, the correspondence between components is searched and built by using the components correspondence database. finally, the evolution animation of chinese characters is generated by morphing method. therefore, the online phase mainly includes two key modules, i.e., the chinese character parsing and the chinese character morphing. the chinese character parsing is used to split the given chinese character into components, while the chinese character morphing is to generate the evolution animation. to implement the system, four key technologies are proposed in the system, i.e., the stroke extraction, the component modeling, the chinese character parsing and the chinese character morphing. we will introduce these key technologies in the following sections, respectively."
"where ie i is the internal edge that in the connected region, w i is the angle between ie i and contour, n is the number of the ies. the orientation angle of the connected region is defined as:"
"in this section, we propose a methodology to quantitatively determine the efficiency of speculative execution at runtime. our techniques build on execution cycle breakdowns that can be obtained through hardware-based, programmable performance monitors. details of these monitors and how to obtain the breakdowns will be addressed in section 4."
"1.3.1. thread monitoring. the performance profile of speculative threads must be collected dynamically. such profiles can be application-dependent, such as loop iteration count; architecture-dependent, such as memory access latency; or both, such as cache miss rate. hardware-based performance counters [cit] are programmed to collect such information."
"furthermore, an effective mechanism should identify a set of loops that lead to maximum performance benefits. in many programs, it is common to have multiple nesting loop levels that all benefit from tls. since the optimal parallel loops are most likely neither the outermost nor innermost loop, finding the right set of loops is essential to maximize performance."
"figure 1(b) illustrates the concept of speculative execution. the threads are numbered according to their original sequential order. if no dependence is violated, the speculative thread commits (thread 2); otherwise, it is squashed and reexecuted (thread 3). tls empowers compilers to parallelize program regions that were previously nonparallelizable."
"3.3.2. evaluation using the similarity measure. the similarity measure quantitatively evaluates the accuracy of our prediction for all loops parallelized. we first define dissimilarity as the accumulative difference between t pseq and t seq over the sum of t seq for all loop invocations; that is,"
"6.1.3. quantitative vs. quant+static. the quant+static policy incorporates compiler annotations to prioritize the search in the loop nest. we found that it selects better loop levels for benchmarks art, bzip2, and twolf, but it greatly degrades ammp, gcc, gzip, and mcf. and the performance downgrade in mesa remains. the problem is that compiler annotation often fails to point to the most profitable loop levels. quant+static respects the compiler decision and does not attempt to look at other levels as long as the annotated loop level does not worsen performance. in another word, the problem of local optimality is still at large. overall, this policy shows 1.224x speedup on average. 6.1.4. quant+static vs. quant+statichint. the quant+statichint policy treats compiler annotation only as hints and evaluates both the annotated loop level and its neighboring levels. the compiler-annotated loop will be compared with its inner loops. if an annotated loop level is the innermost, it will be compared with the immediate outer loop level. in both cases, this policy can select a loop level that outperforms the compiler's decision and try to avoid local optimality."
"the full adaptation to phase change is beyond the scope of this dissertation. in our implementation, a simple mechanism is used for all the policies: the decision tables are reset periodically, so that the impact of speculative threads can be reevaluated. table ii summarizes the features of the four dispatching policies."
"numerous execution models involving the creation of assistant threads have been proposed to utilize the emerging multicore to satisfy diverse performance or nonperformance requirements. for many such threads, their execution does not necessarily affect the correctness of the application, but introduces significant performance variations and resource competition. thus, judicious utilization of these threads is key to application performance. such threads not only include performance-enhancing speculative threads and helper threads [cit], and other workload sharing parallel threads [cit] ], but also monitoring threads and verification threads that aim to improve nonperformance metrics of a system. this article focuses on one class of assistant threads that aims to enhance program performance by creating speculative parallel threads. however, the proposed techniques can be deployed to evaluate the performance impact of a large variety of assistant threads and make decisions on how to best deploy them. let us take helper threads as an example. a helper thread improves the performance of an application by bringing data into the shared cache before they are needed. however, if deployed improperly, prefetching threads can also degrade application performance by polluting the cache or saturating shared resources, such as the off-chip pin bandwidth or the bus, to create a performance bottleneck. we can apply the following steps to dynamically optimize the performance of a helper thread: (i) executing the program with a helper thread and configuring the hardware performance monitors to dynamically collect a performance profile that contains information regarding how prefetched lines are used by the main thread, whether prefetched data displaces useful data, and whether the helper thread contributes to reduced data cache stalls; (ii) isolating the performance impact of the helper thread using the dynamically collected performance profile, estimating the performance of the main thread in the absence of the helper thread using the profile information; and (iii) enabling/disabling or reoptimizing the helper-thread based on its performance impact."
"the components correspondence database is used to store the semantic relevance between components from the different scripts. then these correspondence will be used in the process of chinese character morphing. to this end, the experts of chinese character are invited to build the sematic correspondence between components from different script, and they use the interactive labeling tool to manually label the rough correspondence based on the extracted strokes. finally, the fine correspondence will be established automatically by using the interpolation on the key points."
"note that all tls-related overheads, including thread idling and squashing, are not attributed to any counter because they would not be present in sequential execution."
"the wide deployment of multicore and multithreading architectures has brought forth significant volume of computation power. efficiently utilizing these parallel this work is supported in part by a grant from national science foundation under cns-0834599, eia-0220021; [cit] -tj-1819, and gift grants from hp, ibm and intel. authors' addresses: y. luo, advanced micro devices, 1237 east arques avenue, sunnyvale, ca 94085; email: yangchun.luo@amd.com; a. zhai, department of computer science and engineering, university of minnesota, 200 union street, minneapolis, mn 55455; email: zhai@cs.umn .edu. permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. permissions may be requested from architectures and exploiting their performance potentials demand efficient extraction of thread-level parallelism (tlp) in diverse applications. however, automatically extraction of tlp is challenging."
"for obtaining the compatible triangulation, the fine strokes correspondence firstly are computed based on components correspondence. a stroke is a closed contour and it has a start point and an end point. when the start point and the end point of two strokes are corresponded rightly, the rest of the contour points also are corresponded by using the sampling principle. the points are used to construct the triangulations by applied the minimum link interior path method [cit], are shown in figure 6 (a) ."
"in this section, we will first compare the performance among the four dispatching policies (section 5), then evaluate how the different performance prediction schemes (section 3) impact the performance of the dispatching system. finally, we will contrast our dynamic dispatching mechanism with the state-of-the-art static thread management."
"pr(s)~pr(s 1,s 2,:::,s n ): 5 where s i represents the stroke. the joint distribution contains all information about the strokes and their relationships. due to the time complexity and the proper accuracy, we tend to select the necessary relationship to simplify the model by considering the importance of neighbor relationships. the joint probability is transformed to conditional probability multiplied together as follows: we select the most important relationship by minimizing the kullback-leibler measure. the more details of derivation process can be found in reference [cit] . at last, the component model are stored by using the geometrical and neighbor information of every stroke in the component."
"in equake, almost all points fall above the dotted line, but form a linear relationship with the same slope as the dotted line. this indicates that our prediction is systematically optimistic, that is, predicting less cycles for sequential execution, due to the same reason of not compensating for all conflict misses. we have also observed that the l1 cache replacement rate is very high. this is a strong indication that parallel execution could have reduced conflict misses compared to sequential execution. also note that these benchmarks achieves near perfect prediction in terms of correctness measure, thus similarity measure is only used as tie breakers. therefore, as long as the prediction is consistently pessimistic or optimistic across all loops, the dynamic dispatching system is able to make proper decisions."
"although this policy could use the opposite outside-in search order instead, outer loops are much larger than inner loops and attempting outer loops would significantly prolong the time needed to reach the best level. in extreme cases, the outermost loop covers the entire program execution; by the time the outermost loop level is evaluated, the execution of the program is almost done."
"to isolate the performance impact of the speculative threads, we attempt to predict the sequential execution time from the execution time breakdown of the tls execution. a straightforward prediction, shown as the pseq s bar in figure 2, is to subtract squash and others cycles from the total aggregated cycles of parallel execution. each segment in pseq s is just four times of its counterpart from tls4, since the latter is scaled down by four. in this prediction, busy can be accurately predicted because the amount of useful work done in tls mode and sequential execution is similar; ifetch is also similar in these two execution modes. however, execution stall and cache behaviors can change dramatically when the sequential program is decomposed into multiple threads. to improve our prediction, a more accurate model is developed to address the inaccuracies in predicting exestall and dcache segments."
"4.1.1. basic cycle classification. the cycle classification scheme is explained in the following. as a simplification, we describe cycle classification for a processor that commits one instruction per cycle. for processors with higher commit width, multiple counters can be incremented in a single cycle, based on the activity of each graduation slot. at each cycle, the classifier examines the head of the rob. if the rob is empty, the instfetch counter is incremented by one. at the head of the rob, if the instruction is unable to graduate, a hardware accumulator is incremented by one to keep track of the number of cycles this instruction stalls. when the instruction finally commits, no counter is incremented if this instruction is related to tls thread management. otherwise, the useful counter is incremented by one, and the value in the accumulator is added to the datacache counter if this is a memory instruction and results in a cache miss, or added to the ilpstall counter otherwise. the accumulator is reset whenever an instruction graduates."
"the hardware-based decision table is built in each processor core. it is a contentaddressable memory (cam) indexed by a unique identification number associated with each candidate spawning point, namely its instruction address. each table entry contains two fields: a saturation counter, which is incremented if the tls execution outperforms the predicted sequential execution and decremented otherwise, and a performance summary, which contains the cumulative difference in execution time (i.e., cycles) between the tls execution and the predicted sequential execution. note that the performance summary can be a negative value."
"insideout is prone to serializing a loop prematurely, while the loop could lead to overall speedup. loop invocations with different execution times should not be treated equally. we devise a new policy named quantitative that quantitatively evaluates every loop invocation and uses the performance summary, that is, cycles saved from sequential execution, as the weights of different invocations. under this policy, a loop is serialized if both the saturation counter exceeds the threshold and the performance summary becomes negative."
"scenario iv. speculative threads run on multiple cores, and thus are able to utilize multiple l1d caches. this can increase the effective associativity of the first level cache compared to the sequential execution. thus, conflict misses that occur in the sequential execution may or may not occur in the parallel execution. as shown in figure 4b, the conflict miss served by the second level cache in the sequential execution does not occur"
"where vector Â½3,2,1 indicates that the second dimension of s i stays where it is but the first dimension is swapped with the third dimension. from equation (1), it can be seen that the same coordinate components of each joint form the columns of the permuted sub-volume s p i by the above permutation operation. as a result, either x, y, or z coordinate components of a joint in the sampled frames in subvolume s i form one column of the permuted sub-volume s p i . for example, the x coordinate components of the head joint in different frames in sub-volume s i are one column of s p i . this is illustrated by the horizontal color bars in fig. 3 since body joints in neighboring frames tend to have similar coordinates. to examine the sub-volumes, we form a new matrix s u by by reordering the permuted subvolumes s p i lexicographically."
"-we propose an execution framework that allows the runtime system to evaluate speculative thread execution and dynamically adjust their runtime behaviors. -we propose and evaluate dynamic performance evaluation methodologies that analyze execution cycle breakdown of speculative threads to determine their efficiency. we also discuss how hardware counters could be programmed to collect such cycle breakdown. -we propose, implement and evaluate various dynamic performance dispatching policies that determine the priority in which the runtime system evaluates speculative threads for parallel execution. by evaluating these policies, we identify important runtime information and compiler annotations that are key to optimizing speculative threads."
"when extracting speculative threads for 256.bzip2, we collected profiles using the train input set and decided which dependences to speculate on. when the program executes with the three ref input sets source, graph, and program, we found that the percentage of total execution cycles that are wasted due to speculation failure was 25%, 40%, and 31%, respectively. therefore, speculative threads that are created to improve performance under one workload may potentially degrade performance when the input set changes."
"we propose a method that learns automatically sparse representations of human activities. specifically, we treat 3d movements of joints as space-time volumes and densely sample the volumes along the time axis to obtain a set of sub-volumes. we then use the reconstructed independent component analysis (rica) [cit] to learn a dictionary of over-complete codes from the sub-volumes for each activity. in this learning procedure, the sub-volumes are represented by the learned codes in a sparse manner. from the coefficients of the sub-volumes projected to the sparse codes, we construct a sparse histogram for each activity. finally, we concatenate the sparse histograms and use them as inputs to a multiclass support vector machine (svm) to perform activity recognition."
"in static approaches, compilers often analyze extensive profile information to estimate the performance impact of speculative threads and then determine where and how to create speculative threads. being able to perform global and even interprocedure analyses, compilers can extract coarse-grain parallelism in which speculative threads may contain several thousand instructions. however, it is difficult and sometimes impossible for compilers to accurately estimate the performance impact even when extensive profile information is available. we have identified four key reasons."
we tested this model on three widely used databases of human activities and found that it outperforms the state-of-the-art algorithms. the contributions of this paper to joint-based activity recognition are:
"4.1.4. cache miss categorization. when a load instruction that stalls due to a cache miss finally commits, section 3.2 describes four scenarios each requiring a different classification mechanism based on information returned from the cache."
"essentially, the nconflict counter predicts how many additional conflict misses might have occurred in sequential execution. during performance evaluation, we compensate the predicted sequential execution cycle with the value in nconflict multiplying the second level cache access latency."
"for each activity c,c~1,2, Ã¡ Ã¡ Ã¡,n a (n a is the number of activities), we obtain a dictionary w c . suppose n c is the total number of sub-volume samples from activity c. then the class-specific dictionary w c can be obtained by solving the following optimization problem [cit] w c~a rg min"
"in this section, we examine in more detail on benchmarks where dynamic approach performs much better than static approach and explain why this can only be achieved through runtime decision making. many cases have similar behaviors, so we highlight on the cases for ammp, art and mesa in the next sections."
"1.3.3. thread adjustment. once the effectiveness of speculative threads is determined, the dispatching system can decide where to create speculative threads to maximize application performance. we will experiment the design space by creating various dispatching policies and compare their effectiveness in optimizing application performance."
"we propose to learn a class-specific dictionary w c for each activity c and we pool all the learned class-specific dictionaries w c,c~1,2, Ã¡ Ã¡ Ã¡,n a to form a code book w as follows"
"the frequently used chinese characters are contain in the system wrote with above two scripts. 543 components in seal script and 527 components in clerical script are digitized for constructing the component model database. in the component correspondence database, the sematic relevance between components from two scripts is assigned by the experts of chinese characters. for the rough correspondences on the components, average 10Â±15 key points are manually labeled on the components by use the interactive labeling tool."
"the another type is the students from the beijing normal university and the beihang university. 102 students is invited in the age group of 20Â±30, 76 female and 26 male. especially, there are 10 foreign students in our experience. the majority of students are familiar with the common chinese character in the regular script, but they almost do not know the corresponding ancient script, not to mention the evolutions between different scripts."
"depends on the underlying hardware configuration. because speculative threads must share the underlying hardware resources, the configuration of the underlying hardware can change the behaviors of these threads. in particular, interaction between the speculative threads and the cache components has a profound impact on performance. on the one hand, speculative threads, even when they fail, can potentially bring data items into the cache and improve the performance of the nonspeculative thread. on the other hand, speculative threads can modify data items that are shared with the nonspeculative thread and introduce misses that otherwise do not exist. furthermore, the chip-multiprocessor (cmp) architecture effectively increases total cache size. however, as data is spread across multiple caches, cmp introduces coherent misses that do not exist in sequential execution. the impact of such cache behaviors is difficult for the compiler to determine even with accurate profile information."
"the paper is organized as follows. in section 2, we briefly describe related work and how our model is different. in section 3, we describe the procedures of data processing and learning dictionaries of codes of 3d movements of body joints. in section 4, we propose a set of sparse histograms of the codes of human activities. in section 5, we present an algorithm for activity recognition via a multi-class svm with sparse histograms as input features. in section 6, we report the recognition results of our model on three datasets of human activities and compare them to the state-of-the-art algorithms. in section 7, we briefly summarize the main points of our model and address several aspects of the model that can be improved."
"all components matching. we modify the matching result to find the most possible components. especially, the situation of using the reduplicate components in a chinese character needs to be processed, such as,``Ãª'',``''. in a chinese character, not only the possible components are different in the composition of strokes, but also the unselected strokes in the input characters are minimized. we regard this optimal problem as variant knapsack problem. the knapsack's size is equal to the size of strokes in input character. every matching result has the attribute to describe the usage of input strokes. through the solution of variant knapsack problem, we find the best combination in matching result produced by the single matching process to fulfill the knapsack. the goal is to make the collision of stroke usage as less as possible."
"we then obtain the histogram h of nonzero coefficients of samples of a video u by counting the number of occurrences of nonzero coefficients for each word in w. thus, the ith component of h is the number of occurrences of the ith word that appears in video u. fig. 5 shows the histograms of ''talking on the phone'' and ''writing on the white board'' of the cad-60 database. the two histograms are quite different upon a careful visual examination. we define the degree of sparsity of a histogram as the ratio of the number of non-zero bins to the bin size"
"figure 8 compares the speedup with respect to sequential execution among the four increasingly sophisticated dispatching policies. in addition, the scheme that blindly parallelizes the innermost loop level (bar innermost) is also included in the comparison. all speedup numbers are summarized by geometric mean (g.m.)."
"programs exhibit phase behaviors. during different phases of execution, speculative threads may behave differently. thus, the dynamic optimization system must adapt to these changes and potentially reoptimize the assistant threads accordingly. in this research, a simple mechanism is used to adapt to phase changes. the performance and decision tables are reset periodically, and the performance impact of speculative threads is reevaluated. this mechanism can potentially introduce unnecessary overhead if the performance characteristics of the speculative threads remain unchanged. many researchers have been working on detecting phase changes. our system can adopt such phase change detection mechanisms so that speculation effectiveness is reevaluated only when a phase change is observed."
"to support dynamic thread dispatching, the execution framework needs to be able to adjust the behaviors of speculative threads. in this section, we describe the execution framework that (i) programs hardware performance counters to collect the necessary information and (ii) store the dispatching decisions about where to spawn speculative threads."
"to get outstanding animation of chinese characters, this paper also focuses on 2d morphing technology. in general, morphing problem includes two steps. the first step is to establish the correspondence between two models. the second step is the interpolation of corresponding models. the correspondence is extremely important in these two steps. as 2d contour can well retain the appearance of chinese characters, so the morphing problem in this paper is based on 2d contour. for the correspondence problem, a collection of 2d correspondence methods were developed in [14Â±18]. these methods can be divided into two categories. one is to establish correspondence based merely on geometry properties of the shapes [cit] . the other one adds some user interactions [16Â±18] to establish the semantic correspondence for satisfy human perception, then the further treatment is still based on the geometry properties. all these methods compute the correspondence by using the optimization techniques, such as, dynamic programming [cit], graph cuts [cit], etc."
"in figure 1 (a), the compiler attempts to partition the sequential execution into two parallel threads t1 and t2. as a result, two pointer-based memory accesses, a logically earlier store and a logically later load, are allocated to two different threads, and their relative order is inverted in parallel execution. this partitioning is safe only if these two instructions access different memory locations. however, their memory addresses are unknown at the compilation time. hence the compiler is forced to give up parallelizing this code region. traditional compilers have very limited capability in parallelizing pointer-based code."
"phase change is a natural phenomenon in real-world applications, and can occur as a result of ordinary programming constructs. for example, in algorithms that search for the maximum or minimum in a large data set, the frequency of updating the global variables decreases as the algorithm progresses. thus a loop that is not fit for speculative execution earlier in the program can become a good candidate during later phases of the execution."
"in this paper, a chinese character teaching system is proposed to show the evolutions, which are constructed based on the chinese character structure theory and the 2d contour morphing technology. the chinese character structure theory provides the systematically theoretical foundation for describing chinese characters [cit] . in this theory, the elements of chinese character writing system are not characters but components which bring certain meanings, when they were used to compose characters. after many years of research, the experts have obtained a lot of data of chinese characters at different writing stages, and systematically sorted out the component database of chinese character at different writing stages. for example, there are 543 components in seal script, 527 components in clerical script and 514 components in regular script, respectively [1Â±3] . based on the chinese character structure theory, the system firstly divides every chinese character into the set of components. then, the correspondence relationship between different script stages of the same chinese characters is built on the set of components. all components are represented by using the 2d contour formation, and the correspondence relationship between components has been constructed in the special knowledge database. in the end, the evolution animation of chinese characters is generated by using the 2d contour morphing technology."
"we construct the component model by using the statistical structure modeling method. the strokes of the component are extracted by using the strokes extraction algorithm introduced in section 4. the component model is assumed to obey the multivariate normal joint distribution x~n n(m,s). given a component and its stroke set, s, the component model c is defined by using the joint probability of all strokes as follows:"
"where p i is the mid-point of ie i and p s is the start point of the connected region. the curvature of the connected region is represented by the orientation angle of the contour, is defined as:"
"so, there are four stroke paths in figure 4 (b), i.e., (a, c, j, g, h), (b, c, d), (e, g, f ), and (g, i)."
"chinese character is well known as an ideographic and hieroglyphic system, which is the oldest writing system and is still used at present. for example, the chinese character``'' denotes that an active energy source represented by a dot manifest in the space. during the development history, chinese characters experienced different writing stages and there are five major scripts, i.e., oracle bone script, bronze script, seal script, clerical script and regular script. as shown in figure 1, the evolution of`' ' in the five major scripts could have been influenced by the introduction of the brush as a writing tool, which made lines easier to draw than circles. these scripts are intrinsically related, although the shape and the topology have had the huge evolutions. if understanding the process of evaluation, it would be very help for systematically learning the chinese character [cit] ."
"the work related to ours is the eigenjoints that describe positional differences between joints within or cross video frames and are used for action recognition via a naive bayes nearest neighbor classifier [cit] . the eigenjoints are simple and easy to compute and so are the features of our model presented below. our model is different in two ways. first, a set of dictionaries of codes of human activities are learned. second, an approximate sparse coding is performed to obtain a set of sparse histograms for action recognition via a multi-class svm."
"their complex runtime behaviors greatly increase the difficulty of estimating their performance impact using existing static analysis. managing threads at runtime is a natural alternative. this article presents an execution framework that monitors and evaluates the performance of speculative threads under the context of tls, and then adjusts their creation accordingly. we utilize hardware performance monitors to generate an accurate execution cycle breakdown for tls and dynamically analyze the cycle breakdown to determine the efficiency of tls. the proposed analyses are capable of correctly determining whether tls is able to improve the performance for loops that correspond to over 93% of total loop execution time across all benchmarks. this article also proposes, implements and evaluates various dynamic dispatching policies to adjust the exploitation of speculative threads according to their performance profile. the best policy achieves an overall speedup of 1.37x compared to sequential execution and outperforms static thread management by 9.4%. therefore, our dynamic optimization system proves to be effective in exploiting the performance potential of tls and enhancing their capabilities. the performance counters that are used in this article are available on some high-performance processors, and the additional hardware overhead is moderate. with proposed hardware performance monitoring capability and a set of dynamic dispatching policies, tls may reach a performance level not previously attainable, making it more attractive for multicore architectures. the future work is discussed in the following."
"finally, the evolution animation is generated fast base on the component correspondence. the fine correspondence process of each example takes less than 2 seconds in all the experiments. the intermediate shapes can be computed in real time. we can generate more than 25 intermediate shapes per second for all examples shown in this paper. as the topology of the chinese characters are rather complicated, there are mainly three kinds of evolution sequences among chinese character: (a) the number of components and strokes become more in the process of evolution; (b) the number of components and strokes between chinese characters are the same; (c) the number of components and strokes become less in the process of evolution. some examples are shown in figure 10 ."
we believe that our performance prediction mechanisms are accurate enough for the runtime system to make parallelization decisions. our experiments have also shown that being able to accurately understand cache behaviors and the effects on execution stall is crucial for determining the performance impact of speculative threads.
"the simulator reads the trace file and translates the itanium code bundles generated by the compiler into alpha-like codes. the simulated pipeline model is based on simplescalar [cit] . in addition to modeling register renaming, reorder buffer, branch prediction, instruction fetching, branching penalties, and memory hierarchy performance, we also extend the infrastructure to account for different aspects of speculative thread execution, including explicit synchronization through signal/wait, cost of thread commit/squash, and etc. table i shows the architecture parameters of the tls-enabled chip multiprocessor (cmp). private l1 caches, each belonging to one core, are connected with a shared unified l2 cache through a bus."
"before a candidate spawning point is executed, this table is consulted for whether to grant or decline this spawning request. a decline would fail the spawning instruction and serialize the requesting candidate, given precedence to other overlapping candidate threads. when speculative threads complete execution, the processor that commits the last thread has the aggregated information from all other processors and updates the corresponding table entry: (i) increase or decrease the saturation counter, and (ii) add the difference between the tls execution and the predicted sequential execution cycle to the performance summary. the table update is then broadcast to other processors. this operation is infrequent; it only happens when the entire invocation of speculative threads finishes."
"the msr daily activity 3d dataset contains 16 activities each of which was performed twice by 10 subjects [cit] (dataset s3). the dataset contains 320 videos in each of 3 channels, rgb, depth, and joint. there are 20 body joints recorded whose positions are quite noisy due to two poses: ''sitting on sofa'' and ''standing close to sofa''."
"where p e are end point of the connected region. after computing the three features above, we estimate the continuity of two sub-stroke v i and v j which jointed with the same singular region v n using the function below:"
"this mechanism is similar to the performance monitors in ibm power5 [cit] ]. however, the following situations require special handling for tls execution: (i) aggregating counters across multiple cores; (ii) redistributing counters when a thread is squashed due to speculation failure; and (iii) categorizing cache misses."
"note that the histogram bins in fig. 5 have more or less the same height (about 0.3). this may be due to similar words in the dictionaries for the activities in the dataset. since a dictionary is learned from each activity independently, it is likely that there are words that are shared by more than one activities. it is worthy to point out, though, that shared words do not impair the performance of our algorithm."
"served by l2 conflict miss, (b) a conflict miss served by lower level cache may be served by another l1d cache in parallel execution. this leads to a lower cache access latency. in parallel execution; rather, it is served by a neighboring l1d cache. this results in a different latency compared to accessing l2. therefore, we need to compensate the predicted sequential execution time with an l2 access latency each time this scenario occurs. to count the number of such occurrences, we extend each cache line with one additional bit to track whether this cache line is loaded by a logically later thread. we refer to this bit as extrashared. if a cache line with the extrashared bit set is evicted, it is likely that the load in a logically later thread would result in a conflict miss in sequential execution, so the count is incremented by one. details regarding this extension will be discussed in section 4.1.4. this method is simple and with minimal hardware support. it can capture most of the conflict misses, but not all situations. for instance, if the eviction happens in thread 2 (figure 4(b), it will not be counted because the extrashared bit is associated with the cache on which the first thread executes. moreover, if a cache line loaded by more than one logically later thread is evicted, this method will only increase the count by one, in which case multiple increments are desired."
"this article explores the feasibility and effectiveness of a runtime system that dynamically adjust the behavior of speculative threads based on their performance impact. under the context of dynamic performance optimization for tls, this article makes the following contributions:"
"both insideout and quantitative stop searching once a loop level that could benefit from tls is reached. since multiple loop levels can all benefit from tls, previous"
"note that this scheme cannot catch all conflict misses that occur in sequential execution but not in speculative parallel execution, as observed in section 3.3.2. more sophisticated approaches can potentially alleviate this situation, such as replacing the extrashared bit with a counter in each cache line. however, our simple scheme can provide accurate prediction in most situations, thus we did not pursuit alternatives with higher hardware costs."
"yet art and mesa experienced significant performance downgrade in this new policy. this is due to the existence of multiple overlapping loop levels that are all profitable. quantitative stops searching once a profitable level is found, without checking whether an even profitable one lies outside. we refer this problem as local optimality. insideout and quantitative both suffer from it in a number of benchmarks. for the cases in art and mesa, however, insideout prematurely serializes the inner loop level by mistake and reaches a more profitable outer level, making it outperform quantitative. quantitative has an overall speedup of 1.232x, slightly better than insideout."
"to evaluate the performance of teaching system, each student learn the chinese character by using our system. these students first watch the morphing of selected character from the system, then they are asked to do a test which is called linkgame of chinese characters, as showed in figure 12 . in linkgame, a set of common chinese characters are selected and averagely each group has about 7Â±10 chinese characters. different groups can have the repeated chinese characters. the chinese characters of each group have two writing stages, i.e., the seal script and the clerical script. after learning the set of chinese characters, we require that these students link the same chinese characters in different scripts by using the line in each group. moreover, these students also are randomly selected into an experiment group and a control group, and each group contained the same students. the students in the experiment group are trained with the proposed teaching system, while those in the control group were trained by self learning. compared with the control group, the average accuracy and the average speed in the experiment group are improved."
"of the seven parameters, the sampling size n s f, the number of words n w, and the number of the largest coefficients n s are new in our algorithm while other parameters appeared in other published studies [cit] . therefore, we explore how to choose the values of these three parameters while setting other parameters to the values recommended by other researchers [cit] . we run our algorithm with different parameter values on the cad60 dataset. fig. 11 shows the average table 6 . performance of our model and the baseline method on the three databases. accuracy as a function of the sampling size f s when n w~4 00 and n s~4 00; fig. 12 shows the average accuracy as a function of the number of words n w when n s f~1 1 and n s~4 00; and fig. 13 shows the average accuracy as a function of the number of the largest coefficients n s when n s f~1 1 and n s~4 00. these good results on action recognition obtained under a wide range of parameter settings show that our method is not sensitive to parameter values. therefore, setting the parameters in our algorithm for good recognition performance is not challenging."
"we now consider a triangulation t~ft fi,j,kg g rather than a single triangle. each of the source triangles p fi,j,kg~fp p i,p p j,p p k g corresponds to a target triangle q fi,j,kg~fq q i,q q j,q q k g. for each pair of triangles, we compute a mapping a fi,j,kg t . since most of the vertices belong to more than one triangle, a mapping of all vertices could not satisfy all the individual ideal transformations a fi,j,kg t . we define an intermediate shape v (t) as the vertex configuration which minimizes the quadratic error between the actual matrices b i,j,k (t) and the desired ones a i,j,k (t). this quadratic error functional is expressed as where is the e.e norm. in order to have a unique minimizer to e v (t), we predetermine the location of one vertex, say v 1x (t),v 1y (t). so the functional e v (t) can be expressed in matrix form."
"1.2.3. speculative thread behaviors are input-dependent. the performance of speculative threads is often dependent on the characteristics of the input data. tls takes advantage of probabilistic data dependences by speculatively assuming that these dependences do not exist. this mechanism is beneficial only if these data dependences are infrequent. frequently occurring data dependences should be synchronized. choosing the threshold that separates frequent and infrequent dependences is a delicate matter, since a high threshold leads to excessive speculation failures, and a low threshold leads to serialization of the threads. however, once this threshold is chosen and the set of frequently occurring dependences are synchronized, this decision is compiled into the binary, even if the decision is not proper for some input sets."
"there are two types of components. one type is the primitive component, it is not composed by other components, such as`' ',``'', etc. the other type is compound components, it is figure 2 ."
"as shown in the table, our method took 0.50, 0.03, and 0.10 seconds/per video to classify the activities of the cad-60 dataset, the msr action3d dataset, and the msr daily activity 3d dataset respectively. the training time was 513.43 seconds, 73.02 seconds, and 125.60 seconds on the cad-60 dataset, the msr action3d dataset, and the msr daily activity 3d dataset respectively. this time performance can be improved significantly by optimized c++ codes running on much faster cpus. therefore, our model is a real-time method that can be used in smart environments and deployed in robots for human-robot collaborations."
"the main contribution of the paper firstly is that a complete chinese character teaching system is constructed, including the knowledge databases, the key technologies and the user experience study. secondly, the system automatically generates the evolution animation on the screen, when given two or several writing stages of the same characters. in contrast with the flash animation, the system avoids building the evolution animation for each chinese character in the different writing stages, as a result of using the chinese character structure theory. finally, some key technologies in the system is proposed, including the strokes extraction, the component modeling, the chinese character parsing and the chinese character morphing."
"speculative threads have a significant impact on the first-level data cache (l1d) performance. because speculative states are buffered at l1d, and l1d cache misses observed in speculative parallel execution may or may not occur during sequential execution; and vice versa. on the other hand, the second level unified cache (l2) is responsible for caching the working set for the entire process. since the aggregation of all committed instructions on all cores in speculative parallel mode is the same as in the sequential mode, the two modes generate similar access patterns to l2. thus, the performance of l2 remains mostly unchanged and we do not consider the performance impact of speculation on l2. our main approach is to classify l1d cache misses that occur during speculative parallel execution, and predict whether they would occur during sequential execution. we have identified the following scenarios that require special handling: (i) data brought into the l1d cache by a speculative thread that is eventually squashed; (ii) data modified by a speculative thread that is eventually squashed; (iii) data brought into a neighboring l1d cache; and (iv) data evicted from one l1d cache but available in other l1d cache(s). in the rest of this section, we will describe the four scenarios in detail."
"in this section several evolution animations of chinese characters are generated by using the proposed system. we choose two representative scripts in the experiment, i.e. the seal script and the clerical script. the seal script is an ancient style of chinese script. it evolved organically out of the bronze script, arising in the warring state of qin. the qin variant of seal script became the standard and was adopted as the formal script for all of china in the qin dynasty, and was still widely used for decorative engraving and seals in the han dynasty. ever since, its predominant use has been in seals, hence the english name. most modern-day chinese people cannot read seal script, so it is generally not used outside the fields of seals and calligraphy. but, the emergence of seal script laid the foundation of modern chinese characters structure and became the watershed between ancient scripts and modern scripts."
"based on these databases, the experiment consists of two parts. 1) in the system test, we quantitatively evaluate the key module in the proposed system, and correctness of component modeling and character parsing; 2) in the user experience study, we invite two types of groups to use and evaluate our system."
"the i-th sub-volume s i described above contains several video frames (n in following section, we propose to learn a set of dictionaries of codes that can be used to represent complex human activities. the words (i.e., codes) in the dictionaries should be components whose concatenations in the space and time domains constitute representations of human activities. thus, n s f should be neither too small nor too big so that the sub-volumes are samples of components of human activities. unfortunately, it is difficult to set a fixed value for n s f for all human activities, which may have components of a variety of spatial and temporal scales and may be captured by cameras of a range of imaging parameters. therefore, we set the values of n s f via a learning procedure for the three datasets tested in this paper."
"the chinese character structure theory provides the systematically theoretical foundation for describing chinese characters [cit], which is proposed by ning wang based on the``mf'' (liushu). the theory uses the components to analyze chinese character. the elements of chinese writing system are not characters but components which bring certain meanings, when they were used to compose chinese characters. the vast majority of chinese characters are phono-semantic compounds, with a semantic component giving a broad category of meaning and a phonetic component suggesting the sound. for example, those chinese characters``c'' (peach),``Â¨'' (pear),``t'' (orange),``b'' (laurel),` '' (plum), which have a common semantic component``('' (wood), are all used to record a name of woody plants, when the phonetic components are different."
"after the rescaling, vpr-r is the only benchmark in which dynamic dispatching performs noticeably worse than static analysis. the reason is due to inaccurate performance prediction discussed in section 3.3.2. for a profitable loop in vpr-r, our model does not predict the full extent of conflict misses that would have occurred in sequential execution. therefore, sequential execution cycle is predicted less than what it should be, and also less than the tls execution cycle, leading to an incorrect conclusion that this loop does not benefit from tls and consequently, this profitable loop is serialized. static approach benefits from parallelizing this loop. we have observed significant count of conflict cache misses when this loop is serialized and run sequentially. this observation could lead to a potential remedy to this problem: if a serialized loop incurs high rate of conflict misses, we should give it another chance to be parallelized, taking into account the benefit of parallel execution to reduce conflict misses from sequential execution."
"-busy: cycles spent graduating non-tls instructions; -exestall: cycles stalled due to lack of instruction-level parallelism; -ifetch: cycles stalled due to instruction fetch penalty; -dcache: cycles stalled due to data cache misses; -squash: cycles wasted due to speculation failures; and -others: cycles spent on various tls overheads, including thread spawning, committing, and synchronization; as well as idling cycles due to unbalanced workloads. figure 2 shows the execution time breakdown, normalized to the execution time of the sequential execution, of a loop executing in tls mode on four cores (tls4) and executing sequentially (seq). each segment in tls4 is the aggregated cycles scaled down by four to show the relative speedup compared to seq."
"quant+static can potentially find the best level if static analysis is accurate. however, it can fall into the same sub-optimality if static analysis targets to the wrong loop level. our final policy strives to protect against the situation where the compiler's choice is wrong. it explores both the compiler-annotated loop level and the neighboring levels and compares them quantitatively to increase the chance of reaching the best loop level. it is named quant+statichint since we treat static analysis only as hints and could override them."
"however, it is a difficult task to develop features to representation human activities based on 3d information. this is because depth images have much less textures than 2d images and are sensitive to occlusion [cit] . adopting recognition algorithms developed to work on 2d images and videos is not trivial either. for example, interest-point detectors such as dollar [cit] and stip [cit] perform badly on 3d videos. currently, there are two approaches in using depth data for activity recognition, depth based and skeleton/joint based methods [cit] . a recent study showed that relative joint positions carry significant information about activities [cit], but these features are difficult to extract without human intervention. thus, although several recognition algorithms that use manually selected joint-related features have been developed [8, [cit], there is no consensus on what jointrelated features should be extracted and how they should be used for activity recognition."
"we tested our algorithm on three publicly available datasets: the cornell activity dataset-60 (cad-60) [cit], the msr action3d [cit], and the msr daily activity 3d [cit] . our results show that the model proposed here is better than the stateof-the-art methods."
"when the original program executes sequentially on one core, instructions from multiple iterations of the same loop are available for scheduling, and thus the core is able to effectively exploit instruction-level parallelism (ilp). however, when the same code is decomposed into multiple threads that are distributed to multiple cores, execution stall may increase since fewer instructions are available for scheduling. this effect correlates with the average number of dynamic instructions per thread (defined as thread size). when the thread size is much greater than the reorder buffer (rob) size, the variances of execution stall between sequential and tls execution can be negligible. however, when the thread size is smaller than or comparable to the rob size, execution stall will increase considerably. thus, to accurately predict execution stall in sequential runs, the execution stalls in parallel runs must be scaled down by a factor that is correlated to the rob size of the processor and inversely correlated to the thread size. in our experiments, the following estimation is demonstrated to be effective for approximating this relationship:"
"once the profile is collected, it is analyzed to determine the efficiency of speculative threads. the interaction between the main and the speculative threads often complicates the evaluate process. for instance, data brought in by the speculative threads can be used by the main thread but can also displace useful data needed by the main thread. therefore, we need a robust methodology. the evaluation results are stored in a hardware-based table that is kept in sync with all processor cores."
"if a cache line is speculatively loaded, it enters the sps or spe state, and becomes susceptible to dependence violations. if an invalidation message arrives from a logically earlier thread for that cache line, speculation fails; and the thread is squashed. to recover a speculation failure, all cache lines in the spe state are invalidated and all sps lines transit into the shared state; the thread is then reexecuted from the beginning. when a thread is spawned to a busy core that is executing an earlier thread, the new thread is suspended and will be resumed when the earlier thread commits. the compiler estimates the parallel performance of each loop based on the cost of synchronization and the probability and cost of speculation failure, using loop nesting profile, edge frequency profile, and data dependence frequency profile. the compiler then chooses to parallelize a set of loops that maximize the overall program performance based on such estimations [cit] ."
"for each question, there are five choices, the corresponding scores are 5,4,3,2,1 which respectively means very, somewhat, neutral, rarely and not at all. the average score of each question is show in the figure 11 . we can see that the experts gave higher scores in the first two questions and the last two questions but trend to be neutral in the third question. that result shows that the proposed system is effective in the process of ancient chinese character teaching."
"the values of the parameters for all the experiments are listed in table 7 . for simplicity, we set the parameter values the same for the three databases except n s f, the sampling size along the z-direction, which may depend on the speed of the activities and the frame rate of the videos. as shown in tables 1, 3, 5, and 6 and figs. 8-13, there are a range of parameter values in our method that lead to very good performance, which may be further improved by finely tuned parameter values."
"almost all researches of chinese character focus on the character recognition, including the offline handwriting recognition, online handwriting recognition and the optical character recognition (ocr). some key technologies are discussed in the following."
"we build an undirected graph g~(v,e) to model each chinese character [cit], where v and e are the sets of the nodes and edges. each node in g represents one of the sub-strokes or singular regions of the character, as shown in figure 4 (b) . a node represents a singular region, while its degree is equal to or more than three. similar, a node represents a sub-stroke, while its degree is less than three. according to the degree information, sub-strokes can be divided into three types: jointed with singular region at one end (degree 1), at both ends (degree 2), and without any jointed singular region (degree 0)."
"static analysis chooses the inner loop 730, which turned out to have a low thread count per loop invocation. thus, tls execution is unable to utilize all the cores efficiently. it is shown as a significant others segment in the breakdown. this also correlates with the loop coverage in figure 5 (b) that most loops selected by the static approach perform no better than 5% compared to their sequential execution. dynamic dispatching approach attempted to parallelize both loops, and found it more beneficial to parallelize the outer loop. we noticed that dcache segment increases from sequential to parallel execution for loop 897. this is because parallel execution needs to load the same date item to multiple l1d caches while sequential execution for the same code segment may need to load only once. nevertheless, this minor overhead does not overcome the benefit of speculative parallelization in this case."
"the proposed algorithm was implemented in matlab without any optimization in programming. we evaluated the time performance of our method using intel(r) core(tm)2 duo cpu e8600@3.33 ghz with 64 bit windows 7 professional sp1 os. only one core (2 cores available) was used based on single thread programming. we report 4 measures, i.e., the average training time (att), the average testing time per video (attpv), the average number of training videos (antv), the average number of test videos (anotv), and the average number of training classes (antc) on the three datasets in table 8 ."
"one way to completely eliminate this overhead is to generate two versions of code for each loop, a parallel version and a sequential version, and switch between these versions as needed. however, we are unable to experiment with this scheme due to limitations in our infrastructure. to estimate the performance after eliminating this overhead, we attempt to normalize the tls runs to the sequential execution of their respective parallelized code."
"6.4.1. ammp. the performance improvement of ammp from static to dynamic in figure 10 comes from the different loop levels selected by them. one instance of such difference is located in source file rectmm.c. the outer loop starts at line 562 and the inner loop starts at line 995. we named loops by their starting line number. their code snippets and execution time breakdowns are shown in figure 12 (a) and figure 12(b) . the bars are labeled with loop name and its execution mode. for example, 995 seq and 995 tls4 correspond to loop 995 running sequentially and in parallel, respectively. bars are normalized to sequential executions with respect to the same loop. the static analysis believes the outer loop 562 has a greater performance benefit than the inner loop 995. however, loop 562 incurs frequent speculation failures (i.e., is squashed), which cannot be predicted by the compiler. although the inner loop 995 would also incur speculation failure if selected for tls, it achieves a better speedup than parallelizing loop 562. our dynamic dispatching policy quant+statichint uses cycle-saving as the measurement. in this case, it works as follows: since the outer loop 562 is selected by compiler, quant+statichint policy first parallelizes inner loop levels within loop 562 at its first invocation, and innermost level loop 998 is parallelized for comparison. since loop 998 degrades performance, it is quickly serialized. in the next step, loop 995 is parallelized and the cycle-saving is recorded. at the second invocation of loop 562, this compilerselected loop is parallelized while all of its inner levels are tentatively serialized and the cycle-saving is also recorded. from the third invocation on, loop 562 and loop 995 are compared and the one with greater cycle-saving is selected for tls execution. in this case, loop 995 wins the competition."
"the module of stroke extraction first is evaluated. we collect and select 1000 chinese characters as the test objects. these chinese characters are written by different artists with the different scripts. by the experiments, we find that the singular regions detection (98:52%), the continuity analysis (96:63%) and the stroke extraction (95:45%) all obtain the prominent results. these higher accuracy will ensure the good results in the subsequent steps. some examples of stroke extraction using our proposed method are shown in figure 9 ."
where i t i j is the t i j th coordinate image. the third dimension of sub-volume s i can be permuted with the first dimension by a permutation operation
"to summarize, the impact of speculative threads is often multifold: they can commit speculative work, move data between various memory and cache components, and compete for shared resources with the nonspeculative thread. a crucial task in creating speculative threads is to allocate work to each thread, taking into consideration interthread dependences and resource requirements. such decisions are not unique and it is difficult for the compiler to make decisions that are optimal for all programs under all workloads on a large variety of machines."
"scenario ii. when a data item is modified by the speculative thread that is eventually squashed, the cycles must be counted differently. as shown in figure 3 (b), the squashing mechanism invalidates the modified copy in the cache to ensure correctness. during reexecution, this data item will be reloaded into the l1d cache, and thus incur another cache miss. in sequential execution, the second cache miss would not occur. this case can be identified in speculative parallel execution, since the second miss would find an invalidated cache line with a matching tag."
"in this paper, we propose a novel chinese characters teaching system by generating the evolution animation to show the evaluation between different scripts of the same chinese characters. the teaching system is based on two important knowledge databases, i.e., the component models database of each script and the components correspondence database for different scripts. moreover, four important technologies of the system are implemented for generate the evolution animation automatically, including the stroke extraction, the component modeling, the chinese character parsing and the chinese character morphing. finally, the teaching system is tested on two representative scripts and the user experience studies are reported."
"s u represents the sub-volumes from one video with each column corresponding to one sub-volume. one s u is shown in fig. 3, where the ''sample index'' axis indicates the indices of all the sub-volume samples and the ''coordinate index'' axis is the row index of matrix s u . as shown in fig. 3, gradual changes between samples occur along the ''sample index'' axis (corresponding to time axis). thus, the configurational relationships among body joints update in the time domain, as they should in human activities."
"the rest of this article is organized as follows: we first describe the compiler and simulation infrastructure in section 2, and then discuss how to determine the performance impact for speculative threads in section 3. section 4 describes the necessary runtime support that facilitates performance profile collection and decision making. dynamic thread dispatching policies are proposed in section 5 and their performance evaluated in section 6. related work is discussed in section 7. finally we present our conclusions and future work in section 8."
"after optimization of compatible triangulations, the problem is transformed into the interpolation problem. in single triangle case, let the source vertices be p~p 1 3,p 2 3,p 3 3 Ã  Ã¡ and the target vertices be q~q 1 3,q 2 3,q 3 3 Ã  Ã¡, where vertices with the same index correspond. an affine mapping represented by matrix a transforms p into q:"
"4.1.3. handling thread spawning and squashing. when a speculative thread is spawned to a processing core, all counters on that core are reset. when a speculative thread is squashed, all counters except datacache and nconflict on that core are reset. note that we do not attempt to record the cycles wasted due to squashing, but need to preserve these two counters for the reasons explained in figure 3(a) ."
"6.4.3. mesa. the performance difference between static and dynamic approach mainly comes from the code snippet shown in figure 14(a) . in vrender.c, an outer loop at line 897 calls a function that contains an inner loop at tritemp.h:730. the execution breakdowns of these two loops are shown in figure 14(b) ."
"1.2.4. speculative threads experience phase behavior. for some applications, it has been reported that the same codes may exhibit different performance characteristics as the program enters different phases of execution [cit] . we refer to this behavior as phase behavior. in the context of tls, phase behavior can be manifested as changing the effectiveness of speculative threads: speculative threads that improve performance during certain phases of execution can potentially degrade performance during other phases of execution. probabilistic profiles cannot capture this behavior, as speculative decisions that are compiled statically into the binary cannot adapt to this behavior."
"a smart environment is a place where humans and objects (including mobile robots) can interact and communicate with each other in a human-like way [cit] . it has a wide range of applications in home and office work, health care, assistive living, and industrial operations. current pervasive computing technologies and low-cost digital imaging devices make feasible the development of smart environments. in smart environments, accurate, real-time human activity recognition is a paramount requirement since it allows to monitor individuals/ patient's activities of daily living [cit], such as taking medicine, dressing, cooking, eating, drinking, falling down, and feeling painful, to keep track of their functional health, and to timely intervene to improve their health [cit] . fig. 1 shows several human activities in the dataset cad-60 [cit], including ''wearing contact lens'',''talking on the phone'', ''brushing teeth'' and ''writing on the white board''."
"however, there are many places need to improve in the teaching system, including to reduce the user interaction and to introduce the machine learning methods. meanwhile, the interpolation methods should be improved for the distortion and shrink still appears in some cases. in future, the feedback study of the user experience study will be strengthened, the components knowledge database will be enriched, and the teaching system will be promoted in the teaching process."
"it is worth pointing out that cache behaviors make it difficult for static analysis to derive the impact of speculative threads accurately. if speculation failure often helps to fetch useful data into the l1 cache, a high failure rate can be benign, but if failed threads often invalidate useful data, even a moderate failure rate can be detrimental. in section 4.1.4, we will provide a detailed description of the hardware performance monitors needed to classify the misses described above."
"figure 5(b) shows the dynamic coverage in execution time of the three loop categories: close represents loops whose performance differs by less than 5% after being parallelized; correct and wrong stand for the rest of the loops that is correctly and incorrectly predicted by the last scheme base+exestall+dcache, respectively. note that the sum of the three categories shows the percentage of speculative parallel execution in each benchmark. our prediction scheme generally works well: on average, only 2.8% of total execution time suffers incorrect prediction."
"the proposed multitier heterogeneous adaptive vanet (mhav) framework incorporates high tier nodes (htn) and low tier nodes (ltn). htns are the authority owned vehicles such as public buses, taxies, council lorries, etc. while ltns comprise all the other private vehicles. both htns and ltns are assumed to be equipped with lte and dsrc interface, integrated with the help of a control device [cit] ."
"in the light of these previous works, we propose a framework and a gateway selection algorithm that fully integrates lte and dsrc in order to achieve a robust system for multiple its applications. contributions of this paper include:"
"where icn-enabled switches receive icn-specific flows directly, flows between icn-enabled switches that are ini tially unreachable due to intermediate legacy ip and ethernet switches are realized by setting up ip-encapsulated tunnels. the legacy switches are configured by the legacy openflow controller to forward those tunnels accordingly. hence, the configuration procedure consists of 4 steps shown in figure 1, where a doubly-circled node represents a switch capable of both icn and openflow."
"in order to implement our proposal, we have extended the pox (branch betta) controller [cit] by designing an additional custom icn module. we use the native pox discovery module to perform topology discovery and learn a switch adjacency matrix. we reuse the opennetmon [l7] forwarding module to perform legacy path computation and enable end to-end ip forwarding. additionally opennetmon may be used to perform fine-grained monitoring of flows. finally, we im plement a ccnx specific plug-in that is added to communicate with icn-enabled switches and perform icn-specific path computations. the implementation of ndnflow is published open source and can be found at our github web page [cit] ."
"in this application-specific layer, all colmnunication and path computation regarding the icn are handled separately from the regular ip and ethernet plane. by separating the icn layer from the regular openflow layer, we introduce sdn functionality independent of changes and restrictions in the standardization of the openflow protocol. this prevents inter dependencies on versions of protocols, easing the deployment and future maintenance. as shown in figure 1, switches that are icn enabled set up a communication channel, parallel to their regular openflow communication channel, to the icn module of the openflow controller. this icn channel is then used to announce icn capabilities, information availability and requests for unreserved flows. the controller's icn module computes paths for icn flows, and configures both the icn capable and legacy ip and ethernet switching fabric to allow icn flows to pass through the network. hence, we introduce a separate sdn control mechanism for the application-specific osi layers 5 to 7 (icn), independent from osi layers 2 to 4, reusing the separation of layer responsibilities to maintain overall network manageability."
"the network modeled is a 2x2 km 2 area of glasgow city center with varying density of vehicles evaluating both rush hours when there is high presence of htns and less busy hours with lesser htns available. both ltns and htns are assumed to be equipped with fdd lte transceivers with 20 mhz bandwidth, uplink carrier frequency 1715 mhz and downlink carrier frequency 2115 mhz (band 4) [19, table 5 .5-1] integrated with ieee 802.11p compliant dsrc interface operating at 5.9 ghz with 10 mhz bandwidth [cit] . these nodes are assumed to be moving in urban model created using routes mobility model [cit] . fig. 2 illustrates the service area modeled in ns-3 [cit] . nodes move at an urban average speed matched to the 3gpp extended vehicular a (eva) model radio environment designed using matlab [cit] . simulation parameters used are given in table ii. furthermore for htns, predefined bus routes are modeled with an interval of 10 minutes [cit] . for the enodebs (enbs), mast data for operator ee has been implemented [cit] . the enbs are connected to the mobility management entity (mme) through their s1-ap interface and to the serving gateway (s-gw) and packet data network gateway (p-gw) through their s1-u interfaces. interconnection from the p-gw to the tcc server and vsa server is modeled using an error free 10 gbps point-to-point link and tcp/ip version 4. the packet payload for htns is assumed to be 1500 bytes including the registration tables, locations and safety application data."
"propagation loss model employed for ieee 802.11p is nakagami loss model with the path loss factor (m) of 4 [cit] on top of friis propagation loss model. multiple simulation runs are undertaken in order to obtain reliable results. the results from all these simulations are then averaged using 95% confidence interval. ltn velocity is set to 30 mph while the htns are assumed to be moving at 20 mph, according to the city speed limits enforced in glasgow city center."
"in the future, we plan to implement a message dissemination scheme for the proposed framework, expanding it to meet requirements for all the vehicular applications. we will also evaluate other scenarios such as highways and suburban areas where the transmission range parameter in our algorithm is speculated to be significant. lte has been a promising candidate for vehicular networks. however with the current growth in cellular users, catering vehicular networks would require much more capacity. we plan to implement techniques that would integrate lte and dsrc while minimizing the impact of capacity issue."
sdn allows applications to request qos parameters on-the fly from a network control agent. this enables scenarios in which applications can offer guaranteed quality by negotiating the service they need from the network and ultimately pay for that service for the time they need it.
"due to the fact that both the icn-enabled switches and the icn controller module are aware of the specifics of the icn forwarding mechanism, they have equal understanding of an icn flow and its details. furthermore, their communication protocol can be extended to contain flow-specific parameters, such as the needed bandwidth and the expected duration of a flow, without changing the openflow protocol."
"next we compare our proposed algorithm with the previously proposed [cit] lrta for bus vanets. fig. 4 shows the average number of registration switches that occurred during the 300 sec of simulation. the average is taken for each ltn and then 95% confidence interval is calculated over multiple simulation runs. it can be seen that the number of switches decrease by about 30% to 35% when our proposed registration scheme is in place. the reason behind this decrease is the selection of threshold parameters. for lrta, the threshold chosen is the delivery delay, i.e., whenever this delay falls below a certain threshold, the ltn registers with the htn and stays connected until this value crosses the threshold. for a fast vehicle in a radio environment, this delay can arise from various sources and multiple htns can have similar delays, specially when multiple htns are present at the same location. our selection of transmission range instead of delivery delay shows higher robustness, since the ltn tends to stay registered with one htn till the time it is present in the predefined transmission range."
"after authentication, the icn-enabled switch propagates the information it has access to using the availablecontent message. each item can be stored locally or accessed else where, for example via a network outside of the scope of the sdn, and additional costs can be added which are taken into account by routing discovery. absolute backup replicas, which are only to be accessed when the primary replicas are unavailable, can be announced by increasing the value of the priority field. hence, robustness can easily be implemented by placing redundant copies of information across the network. after computing the appropriate actions, the controller is sues an installfiow message to all the switches along the path to install the correct forwarding rules, reducing the original interest name to match the name prefix of a complete flow or segmented piece of information. the facetype and action parameters can be used to configure flow-specific parameters. among others, we use them to set up ip-encapsulated tunnels between icn nodes that are separated by one or more icnincapable switches to enable flow exchange between them."
"this paper proposes a multitier heterogeneous adaptive vanet framework and a gateway selection algorithm. mhav architecture consists of htn and ltn. all the vehicles are assumed to have lte and dsrc capabilities while ltns register with htns to enable v2i and v2v communications over dsrc while the htns connect to the lte network in order to provide infrastructure communications to its registered ltns. we also propose to have a fall-back to lte sai in the case where there's no htn present for registration. simulations are carried out in glasgow city center, a dense urban environment, in order to evaluate our proposed algorithm. results show that the proposed scheme outperforms the traditional bus vanet registration technique by 30-35% in terms of switching while offloading more than half of the vehicular traffic from cellular networks and maintaining a pdr above 85%. having authority owned gateways tend to make the network more secure and addresses the privacy issue raised by many private car owners."
"similarly, we at first intended to wrap or encapsulate icn streams in ip packets containing a reserved ip anycast address to allow fine-grained control beyond the scope of icn-capable switches. we found this method to be less trivial than it appears. where ccnx is already capable of performing udp over ip encapsulation, it uses static ports for each connection, disabling a switch to differentiate between different flows. as the ccnx application is openflow unaware by nature, changing it by generating different tuples of source ip address and the 4 bytes of the udp source and destination ports implied a drastic change to the internal functions of the ccnx daemon. more generically, forcing developers to create port tuples in such a specific way in order to benefit from sdn functionality is in contrast with the open philosophy of sdn. furthermore, openflow switches may not send the complete incoming ccnx packet to the controller, they may apply buffering to recreate the original message when necessary, but instead might only forward the first part of the message. this implies possibly losing parts of the icn name, information necessary for the icn controller to perform path computation."
"we have conducted our experiments on a testbed of phys ical, general-purpose, servers all having a 64-bit quad-core intel xeon cpu running at 3.00 ghz with 4.00 gb of main memory and 1 gbps networking interfaces. openfiow switch functionality is realized using the open vswitch 2.0.2 software switch implementation, named data networking functionality by installing ccnx 0.8.2, both running in parallel on ubuntu server 14.04.1 lts with gnu/linux kernel version 3.13.0-29-generic. the ccnx is connected to open vswitch via a socket to the internal bridge interface to realize connectivity, hence data is forwarded to and from ccnx through the openfiow local port."
"in this section, we summarize named data networking (ndn) [cit] and its implementation ccnx [cit] . ccnx im plements an information centric network (icn) by using a route-by-name principle. in contrast to identifying by source and destination ip addresses, ndn identifies interest and contentobject packets by one or more name components (for example, bob could publish his holiday photos under the name ibob.eu/holidayphotos). a user-client requests content by sending out an interest containing a name describing the desired information. intermediate nodes on the path from client to server forward the expressed interest hop-by-hop to the generator responsible for the requested name. when the interest reaches a node that has a cached copy satisfying the description of the interest, the interest is dropped and the data is delivered from cache. if not, the interest travels the path to the content generator, which creates a contentobject and delivers it to the client accordingly."
"throughout, we use a 2-switch topology on which the discussed switching fabric and additional plug-ins from section vi are configured. a third server is configured as controller using the pox controller and modules discussed in section vi."
"even if standardization would include an icn protocol, we foresee a rise in application-specific forwarding schemes in general to optimize the internet for the most frequently used applications. one application-specific adaptation of openflow, or any sdn paradigm for that matter, would exclude other application-specific forwarding schemes facing identical prob lems."
"abstract-cooperative intelligent transportation systems have become an active topic with the introduction of smart communications between vehicles, increasing driver safety, traffic efficiency and ultimately paving way for autonomous vehicles. these vehicular communications have stringent transmission requirements. among various proposed communication protocols, use of heterogeneous networks, combining long term evolution (lte) with dedicated short range communications (dsrc), have shown promising results. this paper proposes an lte gateway selection procedure that enables multitier hybrid architecture. the proposed multitier heterogeneous adaptive vanet (mhav) framework consists of two tiers, the high tier consist of authority owned vehicles or public transport operators such as buses, lorries and taxis, while low tier consist of privately owned vehicles. having an authority owned gateway addresses the security and privacy concern raised by private car owners on sharing their information with other cars. results from implementation of our proposed algorithm using extensive system-level simulations showed an increase in coverage area for dsrc while minimizing the number of gateway switches made by 30-35% in comparison with previously proposed multitier registration techniques. traffic on lte network in our simulations is also reduced by 50%."
"in this paper, we have presented and designed a mechanism and implemented a prototype to realize application-specific forwarding schemes in openflow-controlled software defined networks (sdns). specifically, we have implemented a popular information centric networking proposal, named data networking and its implementation ccnx. compared to other application-specific sdn implementations, we argue that our implementation is architecturally less complex to implement, easier to extend and furthermore applicable to multiple application-specific forwarding schemes due to the stricter separation of functionalities. with this implementation, we provide the tools to control and manage application-specific flows in sdns."
"in section ii, we first discuss the initial principles of sdn and openfiow. in section iii, we explain the functionality of the icn implementation ndn. section iv presents our two initial proposals toward application-specific sdns and reasons why we think these approaches are infeasible for standardization. section v proposes our mechanism in which we have divided the sdn in two layers: the regular openfiow layer based on traditional forwarding mechanisms, supple mented with an application-specific layer. section vi presents the exact details of our implementation. section vii presents experimental measurements performed on ndnflow. finally, section viii concludes this paper."
"data delivery in the proposed framework is carried out with the corporation of htns, traffic control center (tcc) and vehicular safety application (vsa) server. the tcc and vsa are situated at the core of lte network and are also accessible via internet. all the ltns get registered with htns, which then enables v2i communications. if an htn is not available, ltn falls back to using lte network. htns consistently communicate with the lte network, updating the traffic conditions and their registered ltns table with the tcc and vsa. htns broadcast beacons every second consisting of their location, velocity and id using dsrc. ltns receiving these broadcasts run our proposed registration algorithm in order to register with the most suitable htn. once the ltn is registered, all v2v communications are carried out via the registered htn, acting as a message relay. the basic architecture of mhav framework is shown in figure 1 ."
"cooperative intelligent transportation systems (c-its) enable different forms of communications, such as vehicle-tovehicle (v2v), vehicle-to-infrastructure (v2i) and vehicle-topedestrian (v2p). in order to provide v2i communications, an entirely new infrastructure comprising base stations along road side are required. installation of such an infrastructure will incur extra costs. considering this economical issue, a number of studies have proposed the use of already installed cellular infrastructure [cit] . with the presence of evolved universal mobile telecommunications service terrestrial radio access networks (e-utran, referred as long term evolution (lte)), service providers and mobile network operators (mno) have achieved high data rates with lower latencies. a number of lte performance evaluations for the feasibility of use with vehicular ad hoc networks (vanets) have suggested a reasonable suitability, however, without any centralization; vanets can pose enormous network capacity issues on the cellular network [cit], it can be argued whether the present network can accommodate vanets or not, since a slight delay in message delivery, especially for autonomous vehicles, can be disastrous."
"our software currently runs on general purpose x64 archi tecture servers running ubuntu server. on these servers, we have installed stock open vswitch 2.0.2 [cit] to enable config uration of operation by the openflow protocol. in addition, we enable switches with icn capabilities by installing the ccnx daemon [cit], the open-source implementation of ndn."
"when an ltn receives a broadcast beacon from htn, this node is placed in the candidate registration set (s). using the information in the broadcasted beacon, ltn calculates the connection delivery delay (t ) for every htn in the candidate registration set. this delay is calculated using htn's transmission range (r) which is predefined, distance between the htn and ltn (d) and relative velocity (v lt n â v ht n ). negative value of this delay means that the htn is moving in the opposite direction to the ltn, therefore if t is negative the htn is placed in the discard set. out of all the htns residing in s, the one with the highest t is selected for registration. once ltn has registered with the htn, it stays connected with it until the distance between ltn and htn remains below the predefined transmission range. this is elaborated in algorithm 1."
"we compare our results with the previously proposed longest registration time algorithm implemented in busvanets [cit] . the primary performance measures used are the average number of registration switches, ieee 802.11p coverage and the packet delivery ratio (pdr). the registration switches are calculated for each ltn whenever it switches from an htn to either another htn or to the lte network. ieee 802.11p coverage is the percentage of ltns registered with htn. furthermore, the pdr is the number of packets successfully delivered to the ltns after they have registered with the respective htn."
"recently, software-defined networking (sdn) has gained the interest of both research and industry. for research, sdn opens up the possibility to implement optimizations that previously were theoretical in nature due to implementation complexity. for industry, sdn delivers a way to dynami cally monitor and control the network beyond the capabilities of self-organized distributed traffic engineering and fail over mechanisms."
"we chose to use the javascript object notation (json) to facilitate communication between the ccnx and sdn module due to its generic implementation and high support in different programming languages. currently, we imple ment the following abstract messages to support our actions."
"in terms of using just dedicated short range communications (dsrc) for vehicular communications, less latency is experienced as compared to lte. however, successful message delivery in dense urban and sub-urban scenarios is not evident. for the centralization of dsrc, again there are some proposed techniques and frameworks. among these, clustering [cit] and various routing protocols [cit] are some of the promising dsrc techniques. however, again with clustering or direct vehicular communication arises the concern of privacy and security."
"the resulting content is encapsulated in a contentobject and forwarded along the exact reverse path back to the client. functionally, each intermediate node administers the following three tables in its memory:"
"due to the frequent and fixed routes of public transit, studies have suggested the use of buses as mobile gateways (mg) instead of fixed road-side units (rsus) [cit] . many advantages such as their tall structure exhibiting higher transmission range, covering most parts of urban areas, no requirement of privacy mechanisms and avoiding the cost of installing a new infrastructure, contends public buses as a good substitute for fixed base stations. [cit] are the first researchers who proposed using public buses for message ferrying in vanets."
"in order to use sdn and openflow to set up icn networks, we have evaluated multiple techniques before coming to our final proposal in section v. in this section, we discuss previous initiatives and parts of our early work and argue why we think these are not feasible for standardization."
"using the described testbed and tools, we have performed 4 types of experiments to evaluate the suitability and stability of ndnflow. in our experiments, we differentiate between the proactive and reactive sdn approaches in which flows are respectively configured in advance, or on-the-fly. as the decision between proactive and reactive configuration can be made independent for both the ccnx and ovs specific forwarding fabric, we perform the following 4 experiments: (l) we determine a baseline by measuring rtts using a statically configured ccnx over classic ip. (2) we determine the overhead of open vswitch and the ndnflow ccnx-plug in by measuring rtts in a proactively configured ccnx and open vswitch network. since the biggest difference between proactive and reactive configuration lies within the delay of setting up the flow (measurable by the delay of the first packet), we continue measuring the delay of the first packet of every new flow with a reactive configuration of ccnx in both a (3) proactive and (4) reactive configured ovs network. while experiment (3) shows the delay invoked by comput ing and configuring the path of the content flow, (4) shows the delay invoked by additionally configuring the legacy ip part of the openflow network. we measured 10,000 samples for each configuration. figure 2 shows the results for (1) and (2), while figure 3 shows the results for (3) and (4) . figure 4 shows the relative averages and 95% confidence interval, giving: (1) a baseline of 1.534Â±0.101 ms for ccnx over ip networks, (2) 1.834Â±0.115 ms for a fully proactive configuration of ccnx and ovs, and (3, 4) 64.006 Â± 5.028 and 304.630 Â± 34.191 ms for a reactive ndnflow configuration in a proactive and reactive ovs configuration, respectively, to determine the additional costs of configuring the content flows in ccnx and ovs."
"the ccnx daemon is extended by implementing an addi tional sdn plug-in, which sets up a connection to the pox icn module, parallel to open vswitch's regular openflow connection, and announces its icn capabilities, capacity and information availability. the extension is realized similarly to our plug-in solving global ndn routing table complexity [cit] . whenever a ccnx daemon receives an interest for which no flow or previously defined forwarding rule exists, it forwards this interest to the pox icn module. in turn, the pox icn module looks up the appropriate location or exit-point of that interest, calculates the appropriate path based on the topology information learned from the discovery module and announcements from ccnx-enabled switches and configures the intermediate ndn nodes accordingly. finally, the open vswitch is configured by the controller as shown in figure 1."
"in order to measure the delay time between requesting and receiving content we use ccnping [cit], an ndn alternative to the popular application ping that can be used to confirm connectivity and measure round-trip times in classical ip net works. similar to ping, ccnping sends an interest per interval to a given destination prefix concatenated with a random value. when sending, ccnping stores the timestamp of creation and computes the round-trip time (rtt) upon arrival of the appropriate contentobject. the ccnping server and client are installed on the 2 switches and connect to the ccnx switch fabric using the application interface."
"software-defined networking is often associated with the network configuration protocol openflow [cit] . openfiow is a vendor-independent protocol which can configure network nodes both in advance, and in a reactive fashion. openflow enabled switches connect to a single controller entity, which configures the switches based on their topological properties and predefined rules concerning routing, firewalling and qos. additionally, when a switch receives a packet for which it has no installed flows yet (i.e., it is a new connection not matching any predefined rules), it sends this packet to the openfiow controller. the openflow controller performs access control and computes the appropriate path for the new data flow and configures all switches accordingly."
"the measured values show that, on average, ovs adds an additional delay of 0.300 ms, while configuring a new content flow using ndnflow costs an additional 62.172 ms at the ccnx daemon and another 240.624 ms at the ovs daemon. although setting up new flows can be considered costly, the additional delay only applies to the first packet of a new flow. once a flow has been installed, the delays of experiment (2) apply. using a completely proactive configuration would remove the additional delay of methods (3) and (4) altogether, though at the cost of losing the flexibility of computing flow specific paths."
"in this paper, we discuss our experiences in setting up an sdn for the application-specific forwarding mechanism of named data networking (ndn) [cit], a popular icn imple mentation. although this paper is dedicated to setting up an 978-1-4799-7899-1/15/$31.00 Â©2015 ieee sdn-supported icn, our experiences and decisions also apply to other forwarding mechanisms that may emerge."
"in its initial form, software-defined networking (sdn) concerns separating the control plane (decision functions) from the data plane (forwarding functions) in networks. this enables a more flexible form of networking in which abstract business rules in terms of robustness, security and qos can be translated into a network configuration policy. in turn, the configuration policy can be configured in the networking devices using either an abstract configuration interface (such as openflow [cit], openflow config [cit], ovsdb [cit], forces [cit] or netconf [cit] ) or vendor-specific configuration parameters."
"the three-phase opf tool employs a mathematical technique (optimization solver) to navigate the solution space outlined by the problem formulation. a converged solution ought to satisfy the equations governing power flow. in stage i, this optimization is provided in (1)- (8) . the objective can be loss minimization, voltage profile improvement, voltage unbalance improvement or a combination of these objectives. in (1), ns is the number of scenarios indexed by s. vi opt is the optimal voltage that should be followed by inverter i, to achieve objective (1) . the voltage between phase Îº and neutral (n) at bus b in scenario s, should be within the predefined limits (2) . the current of line l on phase Ï in scenario s should be lower than the maximum allowable limit (3). voltage-dependent nature of the loads is modelled using zip decomposition method which combines the effects of constant power, constant current and constant impedance load components. Î±, Î², Î³ are the coefficients of zip model. each scenario vector includes the characteristics of the load model (Î±, Î², Î³, pd0 and qd0) and active power production of iicds, i.e., pgi for inverter i. equation (5) gives the total active and reactive power consumption at bus b, where iicdb is the set of iicds connected to bus b. the apparent power relationship is given in (6) for bus b. equations (7) and (8) enforce kcl and kvl, respectively, where buses m and n are the sending and receiving ends of line l."
"as displayed in those tables, though greedy can generate the smallest test suite almost all the time, its consumed time is much longer than that of all the other algorithms. da-ro generates the smallest test suites for more than half cases (9 of all 16 inputs), and da-fo generates smallest test suites 7 times. in the aspect of time performance, the consumed time of da-fo is always less than that of da-ro, which supports the theoretical results about the time complexity of each algorithm. we can also find out that for some inputs, the paraorder and tvg can also generate small test suite with an excellent time performance. and the sizes of test suite generated by the reqorder and union are always much bigger than others."
"for simplicity, we call the density of coverage requirement as the local density. and based on the definition of local density, we define the global density for the whole system as:"
"as demonstrated in table 4, we find out sa generates the smallest test suite for all inputs. when ignoring the data about sa, da-ro generates the smallest test suites for 17 of all 22 inputs, and da-fo generates the smallest ones for 12 inputs. besides, the pict, tvg and paraorder can generate the smallest test suites for 1, 4 and 7 inputs respectively, and greedy can generate the smallest ones for only 1 of 8 valid inputs. therefore, it could be concluded from experimental result that, the test suites generated by two proposed algorithms are much smaller than that generated by pict, tvg, paraorder, and even greedy, though their performances are worse than that of sa in the field of \"narrow sense\" variable strength combinatorial test generation."
"first, the results are presented assuming that there is no limitation on the reactive power provided by these inverters except for the inverter maximum capacity (s max ). the offline modelling phase is repeated for voltage unbalance minimisation as the first objective and loss minimisation as the second objective. the vvc of each v2g system is found using the method discussed in section 2."
"with the optimal voltages obtained for voltage unbalance minimisation, the voltages at system load points (bus 1-11) are closer to each other. this leads to a lower voltage unbalance and shows the effectiveness of the optimisation algorithm used to find the optimal voltages. to analyse the effectiveness of the avm technique, the minute by minute active and reactive power demands at all load points and also the other required data are collected for one week and for each minute a three-phase unbalance power flow has been conducted to find the three-phase voltages. in this week-long time-series power flow, the v2g systems on this lv feeder are tasked with following their assigned vvcs. table 1 summarizes the average performance metrics found using the minute-by-minute one-week-long time-series power flow simulations utilising the vvcs found in the offline-analysis for voltage unbalance and loss minimization. the results are also presented for the fixed power factor scenarios. with minimisation of the voltage unbalance as the objective, the algorithm reduces the voltage unbalance by about 26% and 37% comparing to the voltage unbalance found with loss minimization as the main objective and the best voltage unbalance gained with the fixed power factor assumption. on the other hand, under loss minimization objective, the weekly energy loss is reduced by 11% and 5% compared to the energy loss with minimization of the voltage unbalance as the main objective and the best voltage unbalance found under the fixed power factor assumption."
"in this sub-section, we introduce a new model of variable strength combinatorial testing, which considers the actual interaction relationship more sufficiently than \"narrow sense\" variable strength combinatorial testing."
"to obtain the vvcs in case of the outage of each inverter a centralised offline simulation is conducted. for each possible connection state, a set of vvcs are extracted. after extracting the vvcs in each connection state, the results of applying the proposed adaptive avm technique are compared with those obtained by applying the fixed vvcs."
"by analyzing the time complexity of algorithm da-ro, we can conclude that the time performance of da-ro is not as good as expected. therefore, a concrete test generation algorithm with a better time performance is required. this sub-section proposes the da-fo, another variable strength combinatorial test generation algorithm. the new algorithm is similar to dda, which is a pairwise test generation algorithm, for they both generate single test case by fixing value one by one as a given order of factors."
"there are limitations in twitter sentiment analysis. first of all, the twitter search api can only get user tweets a maximum of 7 days, which is a limitation for the predictive analysis. also, the twitter sentiment is not effective for detecting sarcasm, it will detect it as a negative sentiment. the code is only limited to query only 1500 tweets at a time without authenticating via oath. also giving hashtag under wrong category will also give positive, negative or neutral results . twitter word cloud analysis also has some limitations. in such data visualization like word cloud, the main downside is that we lose context. we cannot differentiate between positive and negative words. in word cloud analysis, we basically lose the ability to derive the meaning from it. for example, from the word cloud we generated, there is one term \"rohingycrisis\". this is a major issue for bangladesh in recent times. [cit] . so, in this context, it is difficult to learnt whether \"rohingyacrisis\" used in positive or negative reference. generated word clouds need to be used carefully but they present a quick and easy way of data visualization."
"generating optimal fixed strength combinatorial test suite has been proved to be np-c [cit], and many heuristic strategies were proposed. the one-test-at-a-time strategy is one that has been most widely used for its simplicity, accuracy, efficiency, and consistency. rather than test generation, it could provide some additional functions such as seed test cases, constraint handling, and test prioritization, etc. therefore, we apply one-test-at-a-time strategy on problem of variable strength combinatorial test generation."
"most of the above models are constructed on non-negative networks, where the effect of non-negative relations is stressed, while the impact of negative relations has been ignored. in reality, information diffusion among individuals is not only spread by trusting or cooperating relationships, but may also be subject to relations that involve controversy or conflict [cit] . for example, if you heard from some of your friends that they joined a club, then you might join too. but if someone you dislike or some of your opponents joined this club, then you might be less likely to join. instead, you might join another club or not join any club. several studies have pointed out that negative interactions also play important roles in collective dynamics [cit] ."
"microblogging or social media sites have developed to wind up plainly a source of fluctuated sort of data. this is because of nature of social media on which individuals post constant messages about their opinion on an assortment of points, examine current issues, grumble, and express positive feeling for items they use in day by day life. actually, organizations assembling or manufacturing such items have begun to survey these microblogs or social media to get a perception of general slant for sentiment for their items or product. commonly these companies think about client responses and answer to clients on social media. one test is to develop technology to distinguish and outline a general sentiment. while there has been a considerable amount of research on how assumptions are communicated in genres, for example, online reviews, blogs and news articles, how feelings or sentiments are communicated given the casual language and message-length requirements of microblogging or social networking has been substantially less studied. highlights, for example, programmed grammatical form labels and resources, for example, sentiment vocabularies have demonstrated valuable for sentiment examination or analysis in different areas, yet will they likewise demonstrate helpful for sentiment analysis in twitter? in this paper, we start to explore this question. word clouds produced for a collection of text can fill in as a beginning stage for a more profound analysis [cit] . for example, they help to decide whether a given text is applicable to a particular data require. one of their downsides is that they give a simply factual rundown of disengaged words without considering phonetic information about the words and their relations. subsequently, word clouds are utilized rather statically as a way to outline message in many frameworks and they regularly give no or just restricted collaboration abilities. we think there is a bigger potential to this straightforward yet capable visualization worldview in numerous analyzing con-texts. in this work, we, along these lines, investigate the potential outcomes by utilizing word clouds at the exceptionally focal point of text mining."
"according to the case studies of this paper, the main factor that affects the vvcs is the system configuration. these means the network structure and also availability of the system controllable devices widely affect the vvcs. in other words, without tuning the vvcs, the voltage control objectives may never be realized. it is quite possible in operation of a lvds that the system configuration is changed due to many reasons such as forced outages of the system components and operation strategies. these may drastically change the voltage intercepts of vvcs (target voltages at pccs. in this paper, a simplified impedance identification technique is considered as an option to develop a more accurate and effective active voltage management (avm) scheme. the effects of the network configuration and also the availability of the system components on the avm algorithm are analysed. the practical limitations of controllable devices are appropriately modelled and taken into account."
"in the model of io relationship testing, each output variable is influenced by a group of input variables, which could be considered as a coverage requirement in the model of variable strength combinatorial testing. people need an optimal test suite, which covers all value combinations of input variables that influence each output variable. however, generating an optimal test suite to satisfy a io relationship has been proven to be np-c [cit] . therefore, we can conclude again that generating an optimal variable strength combinatorial test suite is also a np-c problem, by mapping each coverage requirement to a output variable, which is influenced by a set of input variables that corresponding to factors in such a coverage requirement."
"in this paper, we analyze one such prevalent microblog or social media called twitter and build r models for characterizing \"tweets\" into positive, negative and unbiased sentiment and also create word cloud to find out the most frequently used term. for twitter sentiment, we assemble models for twitter authentication, and then we will pull the data from twitter. here we will use a political figure to analyze sentiment what type of words are being used by him in everyday life to figure out actually what is happening in his mind. by using the r models, we will basically create a graph of positive, negative and neutral words used by the twitter user. to generate word cloud, we will first use r model to authenticate twitter. then we will pull twitter data of a famous phone company. then we will process the twitter data in a way that we can create a word cloud based on the dataset. the finalized word cloud will picture what the company is actually thinking. meaning which words are being used frequently on this particular twitter account."
"variable strength combinatorial testing, which has been proposed in our earlier paper, may avoid some limitation of existing classic combinatorial testing models including fixed strength combinatorial testing and \"narrow sense\" variable strength combinatorial testing. the reason is that such a new model makes more sufficient consideration on actual interaction relationship in software. to address the problem of variable strength combinatorial test generation, this paper proposed two test generation algorithms based on one-test-at-a-time strategy. the experience results show the advantages of two proposed algorithms in both the aspect of size of generated test suite and the aspect of time performance. and rather than the variable strength combinatorial test generation, two proposed algorithms are also available in fixed strength and \"narrow sense\" variable strength combinatorial test generation. above all, many works on combinatorial testing have been done in recent years, but there are also many problems to study in the future. the first one is test generation for different model of combinatorial testing, and there is a limitation that most works in this field focus on pair-wise testing. secondly, the combinatorial testing techniques for test prioritization, constraint handling, and fault location are also important. furthermore, automatic integration tool for combinatorial testing, which need to support the automation of test generation, execution, measurement, and fault location etc, is also required to be developed."
"in another study, a maximum power factor is set for the inverter operation. in this study the inverters can only absorb the reactive power signifying an always lagging power factor. this is the common engineering practice in most of distribution system operator (dsos) across europe. here this maximum lagging power factor is set to 0.92. it should be noted that in this study, the power factor is not assumed to be fixed. the value of the reactive power injection of each inverter is still found using the regarding vvc, but the maximum reactive power injection is restricted by this maximum power factor assumption. now it can be discussed how these constraints affect the performance of the proposed avm algorithm. according to table 1, with the accurate constraint modelling, the results of applying the proposed avm algorithm for optimising the reactive power dispatch to minimise the voltage unbalance are even better than the case with capacity constraint as the only constraint on the operation of the system inverters. it may seem a little bit surprising, but can be justified as follows."
"the remainder of this paper is organized as follows. section 2 describes definitions. section 3 reviews related works. section 4 describes the framework of one-test-ata-time greedy strategy. and two concrete test generation algorithms which are based on one-test-at-a-time strategy are proposed in section 5. section 6 gives experimental results. finally, a conclusion remarks is given."
"to fix value in the order of factors, the order of factors must be determined firstly, so we should define a priority number to measure the priority of different factors and determine such an order. we define a factor density, which could be described as the summation of local densities of coverage requirements that contain such a factor, to measure the priority of factors:"
above operation will repeat until all factors have been selected and the values of all these factors have been fixed. the process of generating a single test case is also described as the algorithm 3. the problem of tiebreaking in steps that selecting factor and fixing value can be treated similar as the da-ro. calculate global density by assuming the value of f i is fixed as v; end for select a value v that takes the greatest global density; fix factor f i as the selected value v;
"here we give a simple demonstration of this claim. when the threshold t ï¿½ min 1 Ã°n pv Ã¾lï¿½n nv Ã¾, only one edge can make an active node activate a neighboring node. if there is an inactive node in a balanced network, its positive-link neighbor will persuade it to take the same action, while its negative-link neighbor will push it to take the opposite action. as a result, all nodes can be activated if there are no isolated nodes. on the other hand, if there is a pair of active +1 (-1) nodes connected by a negative edge, they should belong to two opposing clusters according to the definition of structural balance; then there must be another node to activate one of these two nodes. if the node is a +1 (-1), this node will link positive paths to both +1 (-1) nodes. then there will be positive links between the two clusters, which is contradicted by the structural balance. if the node is a -1 (+1) node, this node will link negative paths to both +1 (-1) nodes. then there will be negative links within one of the clusters, which also contradicts the structural balance. if there exists a positive edge connecting a +1 node and a -1 node, they should belong to the same cluster according to the definition of structural balance, while another +1 node will link a positive path with the +1 node and link a negative path with the -1 node, which means there are negative links within the cluster; or another -1 node in another cluster will link a negative path with the +1 node and a positive path with the -1 node, which means there are positive links between the two clusters. as a result, active nodes with the same state must connect positive edges with each other, while inactive nodes with different states must connect negative edges with each other."
"networks consisting of positive and negative relations are called signed networks, where the sign \"+\" denotes a positive relation and \"-\" denotes a negative relation [cit] . these signed relations can simply signal positive or negative impact on information diffusion, and thus provide information that individuals may use to activate their states during the evolution of collective behaviors."
"comparing the vvcs obtained for loss minimization to those obtained for minimization of the voltage unbalance, one can say the intercepts of the vvcs obtained for loss minimization in this system are usually lower, since due to the load-to-voltage dependence, the higher voltages lead to the higher active and reactive power demand. higher active and reactive powers lead to higher line currents, which in turn cause higher power loss. no meaningful relation was observed for the slopes of the vvcs for these two objectives."
"1) the performance metrics will be improved (compared to the fixed power factor and uncontrolled dispatch). however, the results are not globally optimal. actually, the globally optimal results cannot be achieved using any decentralized control system. the results cannot be treated as the results of an optimization framework that is applied to find the global optimal settings. 2) after applying the vvcs to find the optimal reactive power injection of all inverters, the set of voltage measurements will be updated. it is quite possible that according to these new voltage levels, further corrections are required. this indicates the need of applying a supervised closed loop voltage control based on the vvcs that is out of the scope of this paper. in three separate studies, three different fixed power factor operation strategies are also assumed for the v2g systems, i.e., 0.95 inductive, 1 and 0.95 capacitive. a fixed power factor is typical for an inverter based controllable device connected to a lvds to reduce the voltage-rise effect caused by the excessive active power injections. the results of applying the vvcs to find the set-points are compared to the results of this fixed power factor strategies."
"above all, to increase the effectiveness of existed combinatorial testing, it is necessary to mine the actual interaction relationship among factors and make more sufficient consideration on such interaction relationship. a new model of variable strength combinatorial testing (or named \"interaction relationship based combinatorial testing\" in ref. [cit] ) was proposed by us previously [cit] . to generate combinatorial test suite for the new model of variable strength combinatorial testing, two heuristic algorithms, which are based on one-test-at-a-time greedy strategy, are proposed in this paper. the theoretical analysis and experimental results show there are many advantages of the proposed algorithms."
"it is evidently that the factor, which involved in large number of coverage requirements that with high priorities and great local densities, will have a great factor density. and such a factor should have a high priority to be handled when generating a single test case."
"in first experiment, in order to assess their practicality in variable strength combinatorial test generation, we compare two proposed algorithms to union, greedy, pict, and tvg. besides, the reqorder and paraorder, two algorithms that were proposed in our earlier paper [cit], are also included in this experiment. the tools pict and tvg are both downloaded from internet. the algorithms union and greedy are implemented according to their description [cit] . in the implementation of union, when constructing test suite for a single coverage requirement, the values of factors that excluded in such coverage requirement will be selected randomly. and in the implementation of greedy, although the problem reduction technique is not included, but the time and space performance of this version has been improved to be much better than that of earlier version [cit] ."
the ability of the system inverters to measure the grid impedance is important in order to evaluate the stability of the power electronic interface. offline impedance identification techniques are not viable in practice due to the ever-changing nature of grid impedance over time. the online methods of impedance measurements are being used in order to monitor system stability in a decentralized manner in real time and synthesize corrective actions in case they are necessary [cit] .
"to generate a single test case with one-test-at-a-time strategy efficiently, the value of factors should be fixed in turn. in the first proposed algorithm da-ro, the values will be fixed in the order of coverage requirements, and such order will be determined by the local densities of each coverage requirement."
"to validate the ability of the adopted 3-phase power flow algorithm with many controllable devices connected to the lvds under study, and to assess the voltage controllability of the inverters, the three-phase voltages at all system buses are presented in fig. 12 . the voltage set-points of each inverter is fixed on the target values found under voltage unbalance minimisation as the main objective. considering the voltages at buses 12-20 (pcc of v2g systems), it can be seen that the voltages of the phases on which the v2g systems have been installed match the target values presented fig. 10 . the same study is conducted under loss minimization as the main objective. the three-phase voltages are depicted in fig. 13 . as discussed earlier, the optimal voltages are usually lower with this objective. the adopted unbalanced three-phase power flow algorithm is effectively converged to stable solutions."
"in the basic diffusion model discussed here, the effects of threshold distribution and link weight distribution are not included. the initialized activation is also an ideal operation where nodes connected by positive edges are activated to the same state and those nodes connected by negative edges are activated to different states. if this condition is not met, structural balance may not remove the path-dependence. moreover, there may exist a non-zero probability of change in the active states in the real world. the transition between +1 and -1 and transition from active to inactive states should be also considered in future work. for example, someone participating in a collective action may quit for some reason or a consumer may change their preference to another product. including these possibilities will produce a more realistic diffusion."
"in today's global marketplace, the social media data become one of the rich sources for companies to understand the market value and customer diversity [cit] . in addition, political parties competing for state elections, may also find such text analyzing of social media very useful to understand the opinion of mass people and to make new political strategies for the elections. however, there are certain legal concerns in relation to the data mining from the social media. the are some major legal issues facing by people when they plan to undertake the text data mining which are protected and governed by the database law, copyright law and contract law [cit] . the copyright law gives protection automatically over the work which one not copied from elsewhere by recording and creating in a way it establishes a new inventory work [cit] . the copyright protection somehow provides the owner the right whether to approve or decline on third party's' restricts acts which includes such as adaptation of the work, copying, re-dissemination for publishing it on the web, or any substantial part of the original copyright work, as well as translation of the work into other languages."
"the futuristic lvdss include various res technologies. the thevenin equivalent of the network seen by each individual res is different and depends on the network characteristics and also the behaviour of other ress in the network as shown in fig. 3 . the thevenin impedance seen by each res depends on several factors such as those as described in fig. 4 . among these factors, the impact of network parameters, as well as the demand characteristics on vvc, are already captured in operating scenarios (section 2)."
"actually, the network in fig 4(a) is a balanced network. heider first defined structural balance to detect the origin of conflicts in networks. according to his theory, a complete signed network is balanced if and only if every triangle has an even number of negative edges [cit] . the lt in signed networks reflects homophily where active nodes with +1 (-1) will have a tendency to persuade their friends to take the action +1 (-1), while pushing their enemies to the cluster of -1 (+1) [cit] . when the network structure is balanced and the initialized active nodes are assigned to a balanced state (nodes with the same state are connected by positive edges, while those with different states are connected by negative edges), the path of information diffusion can be predictable and this model tends to activate nodes in the direction of cluster assignation of structural balance, i.e. the active state of a node s i will gradually evolve to its cluster assignation in structural balance c i . then all nodes will become active on a balanced network if the threshold t ï¿½ min 1 Ã°n pv Ã¾lï¿½n nv Ã¾, where n pv is the number of v's positivelink neighbors, and n nv is the number of v's negative-link neighbors. the final active +1 (-1) nodes will connect positive edges with each other, while connecting negative edges with active -1 (+1) nodes."
the technique capitalises on the iicds by engaging them in the provision of reactive power. offline stages i-iv consist of a multi-scenario three-phase ac-opf analysis.
"and in another aspect, only the factors, whose values have not been fixed, should be counted when calculating density. so we could construct a sub coverage requirement by collecting these factors, and then calculate density for the sub coverage requirement to instead original one. the number of factors in sub coverage requirement is less than that in original one, so the denominator of density should also be modified to a smaller value, in order to increase the density of coverage requirement to a balanced level."
"as a measure of goodness-of-fit to show how close the data are to the fitted regression line, r-squared test is done for all the vvcs extracted in the case studies. r-squared measure is the percentage of the response variable variation that is explained by the linear model. for 73% of the vvcs, this measure is higher than 70%. even though the interpretation of r-squared measure is hard, such level of this measure assures the validity of the vvcs. if after conducting the linear regression, the value of this measure was less than expected, the boundary scenarios and outliers in the scenario set were removed and the linear regression was conducted again. in the case that the linear regression does not pass the r-squared test, more complicated curves can be fitted to the data or other control schemes should be followed."
"above operations will repeat until all coverage requirements have been handled in current test case. the process of generating a single test case is also described as the algorithm 2. after run such algorithm, there may still be some independent factors, which are not involved in any coverage requirement, have not been assigned. note that these factors can not reduce the coverage ability of generated test suite, so we can fix values for them after all test cases have been generated, to guarantee all valid values of each independent factor appear at least once. the step, which selecting the coverage requirement with the greatest local density, may suffer from the problem of ties that there exist more than one coverage requirements with the equal greatest local densities. there are several methods to break ties, such as the first strategy that selecting the first one that with the greatest local density, the mostfactor strategy that selecting the one that with most fixed factors, and the random strategy that selecting one from all that with greatest local density randomly. another step that may suffer from ties is the step that selecting combination for the selected coverage requirement to increases global density as great as possible. the possible available tie-break methods in such step are similar to above three strategies. we did some experiments to test tie-break methods in above two steps, and the results showed that there are not obvious differences between different methods in aspect of size of generated test suite. to make algorithm to be simple and deterministic, we usually adopt the first strategy in both two steps."
"after the twitter analysis on r, it generated the following chart which shows the positive, neutral and negative sentiments the user used in his twitter account. we can see from december 08, 2017 to december 14, 2017, the user used more than 60 neutral words, almost 40 positive words and almost 10 negative words in his tweets. so, we can easily determine the users' sentiment that he is thinking neutral, positive or negative."
"batteries of electric vehicles (evs) have a considerable potential not only to provide energy for the locomotion of evs, but also to dynamically interact with the lvdss. thereby, through the energy stored in the batteries, these vehicles can be used to regulate the active and the reactive powers. it should be noted that low voltage v2g systems typically consume active power. the owners of the electrical vehicles may raise an argument about discharging their vehicles' batteries in the course of time that they left their vehicles to be charged. here, it has been assumed that the charging station only consumes active power. however, reactive power can be easily exchanged between the v2g system and the grid. to elaborate v2g inverters can work in all four quadrants of p-q plane (see fig. 2 ). for the aforementioned reasons, it is assumed that the v2g systems only absorb active power. the v2g system can increase the reactive power injected to the grid by increasing the voltage at the ac terminal of the inverter with respect to the voltage at pcc and can also absorb more reactive power by decreasing the terminal voltage with respect to the voltage at pcc. it should be noted that the proposed algorithm for decentralized avm can also work with no change under bidirectional active power exchange assumption."
"with total energy loss as the objective, the value of weekly energy loss is lower considering the actual constraints on the operation of the system inverters comparing to the energy loss obtained for the optimised reactive power dispatch considering the capacity limit as the only constraint."
"this work was partially supported by the european commission ireland, by funding the reserve consortium under grant 727481 and also supported in part by science foundation ireland (sfi) under the sfi strategic partnership programme grant number sfi/15/spp/ e3125. the opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the science foundation ireland"
"we have generalized the earlier linear threshold model in signed networks. we discuss the difference between the classic linear threshold model and the proposed model, and analyze the impact of network structures on the diffusion of information in signed networks. a signed network may generate more randomness, which has the opposite effect of positive links, and thus generates path dependence. structural balance may affect information diffusion in signed networks, and a balanced network seeded with a balanced initialized active state can remove the path dependence. simulation experiments show that when the effect of negative links is the same as that of positive links, it is more difficult to achieve information diffusion. a greater proportion of positive links in signed networks is more likely to activate nodes and remove information diffusion in signed networks path dependence, but positive edges reflect a +1!+1!+1 or a -1!-1!-1 diffusion mechanism, which can make the active states less equal. moreover, based on the predictability analysis, the critical threshold point is more unpredictable, while negative links help to improve predictability. we also explore the effect of structural balance on the dynamics and find that information spreads more easily in a balanced structure, causing more activated nodes at a higher speed. structural balance can also help remove path dependence but may cause polarization."
"second study: in a week-long horizon, the v2g systems are tasked with following their assigned vvcs. it means during each day, the set of vvcs are updated according to the outage of other v2g systems. tables 2 and 3 show the system energy loss and voltage unbalance for the first and second studies, respectively. each study has been repeated for the objectives of minimisation of the voltage unbalance and loss minimisation with different assumptions for the limitations that should be considered for the reactive power support capability of the system inverters. the details of such assumptions are provided in subsection 4.1."
"after introducing the concept of \"density\", we will present two different algorithms to generate single test case. and when generating a single test case, we endeavor to take the global density as great as possible, to make the generated test case cover uncovered combinations as most as possible."
"sentiment analysis is a developing area of natural language processing with research extending from document level characterization [cit] to taking in the extremity of words and phrases [cit] . given the character impediments on tweets, characterizing the sentiment of twitter messages is most like sentence-level sentiment analysis [cit] . be that as it may, the casual and specific dialect utilized as a part of tweets, and in addition the very idea of the microblogging or social networking domain make twitter sentiment analysis is altogether a different assignment. how well the features and strategies utilized on more structured information will transfer to the microblogging or social media area. just in the previous year there have been various papers taking a gander at twitter sentiment [cit] . different data scientists and researchers have started to investigate the utilization of grammatical features but the results are still mixed. features regular to microblogging or social networking are likewise normal, yet there has been little examination concerning the handiness of existing sentiment resources created on non-microblogging information. specialists have likewise started to examine different methods for naturally gathering training information. a few analysts depend on emojis for characterizing their training information [cit] . barbosa and feng [cit] misuse existing twitter sentiment sites for gathering training information or data. [cit] likewise utilize hashtags for making training information, however they restrain their examinations to sentiment/nonsentiment grouping. being \"born outside the universe of pcs\" [cit], word clouds ended up plainly well known in the specific community oriented websites, for example, photo sharing website flickr, advertising firm technorati or social bookmarking website delicious, that utilize tagging as an ordering strategy [cit] . in the meantime, they have developed as a core system of data representation that is applied in a wide range of contexts. one famous application range for tag clouds or word cloud is text outline [cit] . here, word cloud is utilized to give a natural and outwardly engaging diagram of a content by delineating the words that used regularly within it. such a synopsis is useful to find out about the number and sort of topic introduce in an assortment of text. this statistical outline is accomplished by decidedly corresponding the font size of the delineated words with the word recurrence. at the point when a word cloud representation is utilized along these lines, the words in the word cloud are words from a content. therefore, the term word cloud is regularly favored over the term label cloud in these specific circumstances. although many assessments of text analyzing in social media written by the data scientists are highly persuasive, however, they often neglect the importance of text/data analyzing of social media as a tool to understand social and political sentiment of any given region. on the other hand, books, thesis and reports written by social and political scientists only focus purely on social and political assessment of social media and always ignore the scientific aspects of using text analysis to understand the social behavior of the people. as this research attempts to provide a brief scientific assessment of the social media of bangladesh by using text analysis methods on the political aspects in bangladesh and avoid unsubstantiated assertions, such study undoubtedly stands as a major reference work on the subject."
"the effectiveness of the adaptive voltage control framework should be validated. to this end, two new studies are conducted. for both studies the proposed avm algorithm is implemented in a one week period to keep the reactive power supports up-to-date (see subsection 4.1). during each day of the study week a certain inverter is assumed to be unavailable. v2g systems 8 and 9 are always available."
"in general, social media analytics are used for future forecasting for better business decision making and planning. in this study, we obtained and modified numerous source codes from different sources and run the algorithms by rendering a new technique to generate twitter sentiment and word cloud for further forecasting on political context. although some of the results are already prevailing in scattered manner -we tried to put those in place -to illustrate the whole process of text mining for business analytics professionals or researchers."
"in studying the effect of network structure on contagions in signed networks, it is difficult to define a specific number for the critical threshold because of the unpredictability discussed in section 2. moreover, it is hard to determine how to assign edge signs (+1 and -1) on a random or a regular network to form a completely random signed network or a regular signed network. this is because edge signs and edge positions are not in the same dimension. for example, given a non-negative random network, if we randomly assign signs on the network based on a uniform bernoulli distribution multiple times, the characteristics of the resulting networks can be quite different. if more positive or negative signs are assigned to edges with greater betweenness centrality, then the generated network is definitely not a random signed network. however, some kinds of signed network structure may indeed influence the diffusion. for example, for the diffusion on the network in fig 4(a), the inactive nodes will be activated as the figure shows, and this will be predictable. but for the diffusion on the network in fig 4(b), in which three edges have different signs from fig 4(a), then only node 4 is certain to be activated to +1, while the other nodes' states cannot be predicted."
"word cloud generated after text analysis from \" [cit] \" twitter account after the word cloud analysis, we can see the user \" [cit] \" used \"seikhhasina\" most frequently as the prime minister of bangladesh and the leader of the bangladesh awami league party is seikh hasina. the other most frequently used words by this user are \"govt\", \"hpm\" meaning honorable prime minister, \"power\", \"dhaka\", \"infrastructure\", \"marching\" and so on. so, we can clearly get a visual understand of what actually is going on in the user's mind by looking at those frequently used words."
the results of the d in section offline and online studies are presente 4 to validate the effectiveness of the proposed avm method . the concluding remarks are presented in section 5 based on these results.
"software can be considered as a complex logic system, which may be affected by many factors or parameters, such as system configurations, internal events, external inputs etc. rather than the single factors, the interaction of multiple factors may also affect the work of software systems. combinatorial testing (or interaction testing) uses a small test suite that cover all needed parametric values and their combinations, to detect the faults that may triggered by these single factors or parameters in software and even the interactions of them. many applications of combinatorial test approach have shown that, a carefully designed test suite which contains small number of test cases can yield high fault detection ability with reduced test cost. therefore, combinatorial testing is an important and effective method for software testing, especially for those high-configurable systems."
"we here explore the predictability or unpredictability of the dynamic process. for each state s and initialization setting r, the unpredictability can be formulated as the mechanism of path dependence must be tested with many experiments with different paths, and if all of the trials with different diffusion paths result in a consistent result, then the process is path-independent. we compute here the inconsistency between the different trials in order to explore the problem of path-dependence. for each initialization setting r, the inconsistency between different trials can be formulated as"
"according to the results, the vvcs enable the decentral operation of inverter-based control devices for voltage control in an online setting satisfying the performance criteria. the optimal target voltages of inverters should be identified in a wide range of diverse conditions including the entire range of demand-to-voltage sensitivities. even with these diverse conditions, the optimal voltages can be found to effectively satisfy the dso's objective. another point of note is revealed when considering the small relative errors between the optimal voltages and the intercept of the vvcs. in fact, a small relative error indicates that the reactive power supports proposed by the vvcs for the iicds better complies with the globally optimal reactive power supports."
"a decentralized avm algorithm was discussed in section 2. in the case studies, this algorithm is first tested on a real-life lvds to demonstrate its performance for effective voltage control in unbalanced lvds with high penetration of iicds. this high penetration leads to both new challenges (such as voltage deviation and reverse power flow), and also new opportunities for control of these systems."
"social media or microblog mining includes a three-organize process: gather, analyze, and visualize. \"gather\" includes getting applicable web-based social media or microblog data by observing different social media or microblog sources, documenting significant data and delivering relevant data. this procedure should be achievable by an institution itself or a data analyst. not all information that are caught will be valuable. \"analyze\" chooses important information for demonstrating, expels noise, low-quality information, and utilizes different propelled data analytics techniques to investigate the information restore and pick up bits of knowledge from it. \"visualize\" manages showing discoveries from \"analyze\" in every significant way for a business occupied with social network or microblog analytics, \"gather\" enables it to recognize discussions on social media or microblog sites identified with its interest and activities. this is finished by gathering huge measures of applicable information crosswise over hundreds or thousands of social networking sources utilizing news feeds or apis. \"gather\" stage covers mainstream platforms, for example, foursquare, linkedin, facebook, twitter, instagram, youtube, tumblr, pinterest, google+, and so forth. to set up a data set for the \"analyze\", different pre-handling steps might be performed, including data modeling, record, linking from other source, part of speech tagging or other syntactic and semantic techniques that help analysis. once a firm has accumulated the comments and posts identified with its products and operations, it should next study their influence and create metrics necessary for decision making. this is the \"analyze\" stage. since the \"gather\" stage accumulates information from numerous users and sources of social media or microblog, a sizeable bit might be noisy and may should be expelled before playing out any meaningful analysis. straightforward, run based text classifiers or more modern classifiers prepared on marked noisy information might be utilized for this cleaning capacity. assessing the importance of the cleaned data can include various statistical methods and different methods got from text mining, machine learning, or and system inquiry. [cit] . this stage gives data about users' sentiment. numerous valuable measurements and patterns about users can be delivered in this stage, interests, covering their experiences concerns, etc."
"the solution of this optimization gives the optimal voltages that should be followed by the inverter controllers (vi opt ). however, according to the standards and network codes, the single-phase voltage-sourced converters are usually neither equipped with voltage controllers nor permitted to regulate the system voltage [cit] . they can regulate their reactive power supports. therefore, instead of following a voltage set point, each inverter is tasked with injecting (or absorbing) a value of reactive power which pushes the pcc voltages towards the optimal voltages. in stage ii, another optimization is conducted separately for each scenario. using this optimization, the change in the reactive power support provided by all inverters are found in a way that minimize the average deviation of the pcc voltages from the optimal voltages found for each scenario. ns separate optimizations are conducted. the objective function of these optimizations is provided in (9) . the operational constraints of the inverters are also taken into account (10)- (12) ."
"in one-test-at-a-time strategy, a set of combinations that should be covered by combinatorial test suite is required, which is just the set combset that has been mentioned before. the process starts with an empty initial test suite. then at each time, one single test case will be selected and added into test suite, and the covered combinations will be removed form set combset. such step repeats until set combset becoming empty. the framework of one-test-at-a-time strategy is described as algorithm 1."
"in a conclusion, it is clear that two proposed algorithms have some advantages in variable strength combinatorial test generation. besides, they are also competitive in fixed strength and \"narrow sense\" variable strength combinatorial test generation. the experiment results also suggest that da-ro usually generate smaller test suite than da-fo in variable strength (include \"narrow sense\" variables strength) combinatorial test generation, while da-fo is usually better in fixed strength combinatorial test generation."
"the intercepts of the vvcs obtained for loss minimization are usually lower. the reason is that due to the load-to-voltage dependence, the higher voltages lead to the higher active and reactive power demand, higher line currents and higher energy loss. therefore, with loss minimisation objective, the optimal voltages are lower."
"the analysis of information diffusion plays an important role in understanding and predicting information flows, and has been widely discussed in various disciplines including sociology, economics, physics and computer science [cit] . one of the applications is the analysis of collective action, in which a small group of individuals have been initially deprived of a right and ask for the return of this right. their behaviors may then influence their friends or colleagues, triggering a cascade in the social network."
"positive-link neighbors, while n nv is the number of v's negative-link neighbors). each node v will be assigned a threshold 0 ï¿½ t v ï¿½ 1, which represents the fraction of v's neighbors that must become active in order to activate v. the dynamics of the process proceed as follows."
"here, all v2g systems are disconnected one by one and a set of vvcs are extracted for the system inverters. it is not possible to show the vvcs for all connection modes. instead, fig. 15 shows the optimal voltages found for the remaining inverters in the case of the n-1 outages. as shown in fig.15, the optimal voltage levels at pccs of the system inverters depend on the availability of the other system inverters. this indicates the necessity of developing an adaptive decentralised control scheme."
"however, when we generalize lt to signed networks, things can be different. even if we fix all the necessary parameters, the diffusion results cannot be predicted because diffusion by negative links will contradict that by positive links, which increases uncertainty in the dynamic process. as shown in fig 3, if the threshold t is assigned as 0.5, and node 5 is first activated to -1, then node 4 cannot be activated, because node 1 and node 2 want to activate node 4 to +1, which is contradicted by node 5, which wants to activate node 4 to -1. then node 6 is activated to -1 persuaded by node 5. however, if node 4 is the first to be activated, then node 5 cannot be activated, and node 6 is activated to +1. compared to the classic lt, the signed network entails more unpredictability and the process is not path-independent."
"1. if v's state is active, it will remain active in the same state, i.e., if it is +1 active, it will remain +1 active and if it is -1 active, it will remain -1 active;"
"here, we explore the diffusion model in signed networks via a set of simulation experiments. these simulations are carried out by matlab on a 2.40 ghz cpu and 4.00 gb memory computer, windows 10. the experiment for each parameter set is carried out for 50,000 iterations and all experiments converge within 50,000 iterations."
"the theory of structural balance plays an important role in the evolution of signed networks [cit] . ic or lt models consider how individual behaviors evolve to a collective behavior, while structural balance focuses on how an individual's behavior can affect others' behaviors as a result of social interaction. combining structural balance with diffusion models may produce a more natural set of choices. in this paper, we focus mainly on diffusion over static networks, generalize the diffusion model in signed networks, and examine the impact of structural balance on the propagation. the rest of this paper is organized as follows. a linear threshold model for signed networks is proposed in section 2; section 3 explains the relationship between the threshold model and structural balance; simulations and experiments are shown in section 4, and our conclusions are presented in section 5. each node v will be assigned a threshold 0 ï¿½ t v ï¿½ 1 uniformly at random, which represents the fraction of v's neighbors that must become active in order to activate v. the dynamics of the process then proceed as follows. at each time t, every node v is selected once."
"avm algorithm should be modified to include the effects of variability of the grid configuration and also availability of the system controllable devices. in this paper, it will be shown that without an adaptive avm framework the voltage control objectives cannot be realized. however, based on just voltage measurement and without a strong communication system to exchange the data between the decentralized control agents, it is not impossible to keep the parameters of vvcs up to date. to keep the decentralized structure of the avm technique, a methodology is presented here to tweak the parameters of avm algorithm to best comply with the system topology and components' availability based on the available impedance measurement capability (subsection 3.1) of the current inverters."
"to show how the actual constraints restrict the reactive power support provided by each inverter in this study, fig. 14 shows the values of the active and reactive power injections of all 9 v2g inverters in this weeklong study with minimization of the voltage unbalance as the objective."
both d and q axes control signals can simultaneously be perturbed and the impedances can be measure after getting rid of cross-correlation using the stateof-the-art methods [cit] .
"for the classic lt, once the contagion spreads from the initial active nodes through the network, the system eventually reaches a stationary configuration if the threshold, the network structure, the diffusion parameters, and the focal nodes are fixed. in other words, the classic lt is path-independent if nodes can be repeatedly chosen, e.g., a node that is not activated at this time may be activated at one time in the future. in this model, a path-dependent process entails that the diffusion result may vary for different paths of diffusion, while a path-independent process is one for which the diffusion result is always consistent no matter the path of diffusion. fig 2 gives an illustration of path independence in the classic lt: if the threshold t is assigned to be 0.5, then it does not matter whether we first activate node 4 or node 5; the final result is the same with active nodes 4 and 5, and inactive nodes 6, 7, 8. however, it should be noted that lt may be path-dependent if inactive nodes can be only chosen once or a limited number of times."
"after the twitter analysis and generating a word cloud on r, the following word cloud is generated. it shows thee words that are frequently used in the twitter account of bangladesh awami league."
"both are focused on twitter sentiment analysis using r. but to satisfy the analysis we modified and run the codes using r to get the sentiment analysis of the twitter user \"sajeebwazed\". 9 word cloud analysis using r for generating word cloud using r, we used twitter account of famous bangladeshi political and ruling party awami league. the twitter account id of bangladesh awami league is \" [cit] \" which we used to get tweets. to analyze the twitter word cloud analysis to find out the frequently used words by a twitter user, we have to get twitter authentication from twitter like we have used in twitter sentiment analysis. then using r packages, we pulled 200 tweets of bangladesh awami league party and we got unstructured data of 200 tweets. then we analyzed the data using r and create a word cloud visualization to figure out the importance of each words used in \" [cit] \" twitter account. 10 model code on r for twitter word cloud analysis here, we cultivated and nurtured different source codes from github, r-bloggers, and stackoverflow [cit] in order to create the word cloud using r."
"a single active node cannot initiate an information diffusion, i.e., a single active node cannot activate a second or third node because of the threshold, and thus the information cannot diffuse from the first step. so we randomly select a focal node to be activated with all its positive-link neighbors being activated to the same state, and all its negative-link neighbors being activated to the opposite state. since activating a node to +1 and -1 have the same meaning in this paper, we activate the focal node to +1 in order to fix the initial state. the specific procedure for updating node status is shown in algorithm1. algorithm 1. the algorithm for updating node status."
"in this technique, there are three different options for injecting the prbs noise. the simulated white noise signal can be injected in the current reference or in the controllers' output. the other option is to inject the noise in current reference and the controllers' output simultaneously. the converter controller first decomposes the control signals for d and q axes. the prbs noise signal can also be decomposed and subsequently superimposed on the control signal on these axes. the tuned parameters of the avm algorithm are going to be applied for a relatively long time horizon, i.e., as long as the measured impedance change is limited. the prbs signal is firstly injected along d-axis and the d-axis impedances are measured. after measuring the measurement impedance, the prbs noise signal is injected along the controller q-axis and the associated impedances are measured."
"for the first objective, the sub-plots of fig. 10 are the reactive power set-points from the second stage in the avm algorithm and the voltage set-points from the third stage plotted against each other and also the vvc for each respective v2g system. for each subplot, the vvc equation for reactive power control is passed to the online implementation phase. for the second objective, the relative errors are less than 0.1%. the slopes of all vvcs are also negative. fig. 11 shows the pcc voltages plotted against the optimal values of the reactive power injection for each v2g system in 35 scenarios (after scenario reduction) and also the vvcs. the optimal reactive power injections and voltages are found according to the method presented in section 2."
"comparing the slope of the vvcs for different objectives, it can be said that the difference of these slopes is not significant at all. for the v2g systems of this study, the slopes of vvcs are the same to two-significant figures. this shows that the capability of an iicd to adhere to a set-point is more linked to the system topology, system impedance, and the location that this inverter has been installed at. comparing the optimal voltages for these two objectives shows different behaviour under differing objectives. for a lvds with high penetration of inverter-based ress, deploying the proposed avm algorithm results in a significant shift in the system operation when various objectives are taken into account."
"without updating the vvcs, according to the proposed decentralized resilient avm algorithm, the voltage control objectives are not satisfied. sometimes it is possible that without keeping the vvcs up-to-date, the degree of satisfaction of the operator's objective is even lower compared to a system for which the iicds are not allowed to contribute in the reactive power support provision."
"diffusion in signed networks can further explain some collective action. the balanced structure in fig 6 is typical of social conflicts [cit] . when two opposing groups are more different, the antagonism between them is strengthened, which leads to more instability. according to social identity theory, the stronger the group identity, the more willing group members are to take collective action [cit] . this group identity not only improves group unity, but also promotes comparison between groups, and thus generates more conflict [cit] . as a result, when an accident occurs in a population with balanced structure, the information will quickly diffuse and polarization will occur [cit], which corresponds to our simulation results."
"compared with the classic lt model, negative relations in signed networks may generate a completely different mechanism, because a negative relation can not only impede the regular diffusion, but also result in diffusion of opposing information. a common phenomenon is that people tend to do the same things as friends, while keeping a distance from people who belong to other groups [cit] . actually, homophily and xenophobia play an important role in neighbors' attributes in signed networks [cit] . fig 1 gives an example of how an individual is affected by its neighbors according to homophily and xenophobia. in fig 1(a), if one node is located with all its friends, then it will follow these friends and behave like them, e.g., voting for a political leader. but if the node is located with all its enemies, as in fig 1(b), then it will not follow its neighbors, and may vote for the opposing political leader instead. this is also consistent with social identity theory, according to which group identification may predict the likelihood of individuals' behaviors in social change decisions [cit] ."
"according to results, one of the main factors that affects the volt-var curves in a three-phase unbalanced lvds is the system configuration and availability of the system controllable devices. it is quite possible that in operation of a lvds, the system configuration is changed due to faults, scheduled maintenance, forced outages of the components and operation strategies. these change the parameters of vvcs, especially the voltage intercepts of vvcs. as will be discussed in the case studies, this intercept is approximately equal to the target voltage at pccs which is the voltages that the inverter should follow."
"as can be seen in table 2 and 3, with the objective of minimisation of the voltage unbalance, the value of the optimal voltage unbalance index is reduced at least by 14% in the second study compared to the first study. the value of the total energy loss is reduced at least by 1.66% in the second study comparing to the first study when the loss minimisation is chosen as the system level objective function."
"the basic foundations of the decentralized avm algorithm based on [cit] and the practical limitations that should be considered in the operation of iicds are discussed in this section. the main idea is to find a vvc for each iicd using offline calculations. then, in application mode, the iicds are tasked with following the assigned vvcs to find the change in their reactive power support according to the measured voltages at regarding pccs. a four-stage offline network analysis for obtaining vvcs is outlined below as a centralised solution. the decentralized implementation of the control technique is briefly discussed afterwards."
the remaining part of this subsection discussed the effects of the other limitations on the operation of the inverter-based controllable devices on the performance of the proposed active voltage management algorithm based on vvcs. table 1 also presents the results obtained in the implementation of the vvcs for avm considering two different set of constraints on the operation of the inverter of the v2g systems to show the effects of the constraints.
"with the practical limitations that should be considered in operation of the inverter-based control devices, the effectiveness of the proposed avm algorithm is still acceptable according to the results obtained in these studies."
"7 sentiment analysis using r for twitter sentiment analysis, we used twitter account of a famous bangladeshi businessman and politician. he serves as an information and communication technology advisor to the bangladesh government and he is also a member of bangladesh awami league. he is an influential person in bangladesh. to analyze the twitter sentiment, we have to get twitter authentication from twitter developer website (www.apps.twitter.com). then, using r packages we pulled 700 tweets of sajeeb wazed and analyzed the positive, neutral and negative sentiments he used in his official twitter account. we collected the positive [cit] and negative words [cit] datasets from github. according to process of social media mining, twitter sentiment also has gather, analyze and visualize stage."
"note that there may be intersection between two different coverage requirements. it means that, when handle a given coverage requirement, the value of some factors in such coverage requirement may have been fixed already. and it is reasonable that only the available combinations, in which the values of such factors are equal to the fixed values in current test case, should be counted when calculating density. and in the extreme case that the values of all factors have been fixed, there will be at most one available combination. if such available combination exists, which means that it covers a new uncovered combination, the density should be the upper bound 1; else it should be the low bound 0. therefore, after the values of all factors have been fixed in current test case, the density of each coverage requirement should be as big as possible to make test case cover more combinations."
"the vvc of each res is obtained and used for avm, as described in section 2. the idea of applying vvcs is based on the fact that the equivalent thevenin model of the network seen from pcc of each res does not widely change. many factors might cause changing this thevenin model. this will reduce the effectiveness of the avm algorithm. fig. 5 shows influence of res/inverter failure on avm performance is. fig. 5-a) shows the futuristic distribution network with several inverter based res connected to the grid. these technologies can be of pv, wind, storage or any inverterbased energy resources. fig. 5-b) shows the same futuristic lvds. for each res, a vvc is tuned. fig. 5 -c) shows the case when there is a contingency on one of the res units. in this case, the pre-tuned vvcs may not able to provide the required performance. this is due to the fact that the thevenin model seen by each res is changed and the vvc is no longer valid. in fig. 5-d), if there is any contingency on any of the res, initially, the contingency is identified for each res using a local impedance identification technique (subsection 3.1), the vvcs for the rest of them will then be updated to capture the changes in the network topology. this resilient avm technique remains robust against the contingencies. the framework for vvc extraction in resilient avm method is shown in fig. 6 . the following assumptions are made to obtain the resilient set of vvcs. the only contingency considered in this framework is the failure of the inverter based res units. only single contingencies are captured. at contingency row i, it is assumed that the inverter at res i is failed and is not able to inject power to the grid. once the vvcs shown in fig. 6 are found, the utilization of the resilient avm (ravm) algorithm is straight forward. the impedance seen from the pcc of each res indicates which row of fig. 6 should be used for selecting the appropriate set of vvcs."
"for the study with maximum lagging power factor of 0.92, the proposed avm algorithm leads to a total loss and voltages unbalance of 2.5% and 12.5% lower than the best energy loss and voltage unbalance attained with the fixed power factor assumption with loss minimization and minimization of the voltage unbalance as the main objective, respectively. this indicates the effectiveness of the proposed avm algorithm even with such restricted feasible region."
"the social networking scene is huge and evolving. indeed, even as some online networking sites detonate into utilization, rapidly turning into everyday tools, new stages are going along with them continually. by and large, there are dozens of destinations with no less than one hundred thousand enlisted clients, and numerous more special guests-including sites a large portion of us have never known about. indeed, even as organizations start to understand the danger in disregarding social networking content and, on the other hand, the open door it exhibits, their inquiries uncover how much stays obscure. real time analysis through social media is a serious challenge. social media data is so much noisy and unstructured and it's very challenging in the context of real time social media analysis from unstructured data and this analysis lags seriously when it comes to numeric analysis. social media analysis (sentiment analysis, word cloud analysis) using text mining can't be done by computer effectively categorized because in social media the word \"good\" might mean good or it might mean bad, depending on perception, relationship and other variables. despite all these challenges, new analytics tools with enough time can bring out meaningful comments and analysis across various social media sites."
"to assess the efficiency of proposed algorithms, we compare them to some existed algorithms and tools. we experiment with a computer consisting of 2.66ghz pentium iv processor and 1g memory."
"first study: in the week-long time-series power flow, the v2g systems are tasked with following their assigned vvcs found in subsection 4.1. in other words, the vvcs are not updated according to the availability of the v2g systems."
"in order to extend kempe's model to signed networks, we propose a new form of lt. in the real world, homophily and xenophobia can be observed in many types of interactions, including friendships, politics, international relations, etc. [cit] . since two nodes with a reciprocated tie are more likely to form a positive diffusion line, while those in conflict tend to construct a negative diffusion line, we design the linear threshold model as follows:"
"a series of studies have analyzed how network structure influences diffusion through the network. [cit] studied complex propagation on random, regular and small-world networks, and pointed out that random links connected to distant nodes can prevent contagions compared to a regular lattice. this contradicted the results for cascades with simple propagation [cit] . for a finite graph, the contagion has a critical threshold t c, and the active information diffusion in signed networks nodes in the stationary configuration will make up most of the system when their threshold is smaller than t c, while the number of active nodes will be very small when their threshold is bigger than t c ."
"before introducing the new model of variable strength combinatorial testing, we firstly review some definitions about existed fixed strength and \"narrow sense\" variable strength combinatorial testing [cit] ."
"reactive power scheduling and static voltage management in distribution systems is challenged by proliferation of distributed energy resources (ders), intermittent nature of ress, increasing demand of the consumers and complexities associated with new types of loads. though most ders (including renewables) have the ability to control their output, growing penetration level of these sources increases the chance of conflict between their control actions and conventional control devices installed at different locations of distribution systems. thus, these resources should be optimally schedule to hold an acceptable level of power quality (pq) and also to reduce the operation cost. with no control scheme, the optimal operation will not be achieved and excessive operation of the regulating devices and other damaging phenomena are likely to happen [cit] ."
"the copyright law is more complicated on the practice. a simple explanation of facts that includes more data or information is not subject to the copyright issues, however, to identify on the classification short sentence or a fact that still remains unclear and controversial for the court to decide to resolve the matters. there is one recent european court case confirmed that a sentence of 11 words was justified to be protected by the copyright even though this does not clearly imply that all similar sentences will be protected by copyright [cit] . the copying act alleged to infringe the intellectual property rights belongs to the original owners, where it will indicate to some legal ambiguity on the process of text data mining. legal aspects are of course country dependent. however, it should be noted that, posting something publically and accessing to the public information on the web is perfectly legal [cit] . when we reading a webpage it is similar like we reading a billboard or a street sign, therefore assessing the information that shared or broadcast to the entire world consist to have no legal restrictions. however, there are some restriction where the law provided on the access to unethical webpage likes the pornographic material by minors thus the service provider has to check the consumer age and enforcement of the law so that the viewer is not doing anything illegal by merely viewing a public web page. now the question remains open whether the text posted in social media stipulates the creation of any particular work or not. in most of the social media, people only tend to share the simple facts, which by its nature, can never considered as a copyright issues in legal arguments. therefore, copyright issues relating to social media needs serious concern from judges, lawyers and legal academicians. it is vital to note that, it is not a problem on accessing by viewing the freely and publicly posted data or information that we needed via certain webpage and use that data of the information by saving it or share it to others and there is no difference if the data or information is accessed interactively with a browser by downloading it automatically with others software. however, the condition will be different on the data or information that is not freely or publicly posted, but rather than it is being protected not for free viewing on the access on the webpage example like to have the password-protected account is required to access or to use the data or information on a web page. in most of the scenario, the web page provider will impose on some conditions and restrictions for the user such as non-disclosure to third parties, payment of a fee, or any other terms of use or terms of service (tos). this is more alike an agreement that exists between web page provider and user of the information or service. some have justified that if tos forbids some act it does not mean that it will be illegal. however, there are other laws binds with tos, where it can be a legally make that agreement to be binding, and therefore if you fail to follow as provided by the tos, then you will be violating the law that exists on the contractual arrangements, then give rise to contract law liabilities [cit] . even if it is lawful to gather information using social network site for the purposes of research, there may be ethical considerations that to make it inappropriate on the issue of privacy of personal data. there is information privacy policy which are protected by data protection laws that regulate the use of personal data information. however, a user who accesses to the personal data or information are protected not limited for one use of such information, except as specified by the tos. however, depending on what is considered as ethical behavior, it may have different views about the gravity of breaching a tos agreement, and the magnitude of violating a third party's right to privacy. it should typically consider such violation of privacy as unethical even it is not illegal. research on social network data has significant ethical consequences concerning the protection of the human subjects [cit] . it has been argued and recommended that the research on social media is to be reviewed by an independent review panel to make sure that it is protected from any discrimination, abuse, risk, privacy violations, and other potentially adverse factors. if there is any study conducted on online social network, it should be ensuring that no any personal identifiable information is to be publish however only to the information required which is about aggregate statistical analysis."
"to make the algorithm become more efficient, a color graph based problem reduction method was proposed [cit] . however, the reduction technique is suitable only when io relationships are \"simple\", which means that the number of edges in color graph is much smaller than that of complete graph. another limitation of such reduction technique is that it may lead to the redundancy of generated test cases."
"the system under study is considered as having an overshoot when the error is zero and the rate of change in error is any other value than zero. the system is considered oscillatory when the sum of the absolute values of the error taken over time does not equal the absolute values of the sum of the error over the same period of time. since the system is expected to overshoot during oscillatory behavior, the only switching criterion that needs to be considered is overshoot. however, in practice, it is more convenient to directly implement the control signal according to the control actions delivered by the controller. consequently, the fuzzy controller can be designed so that normal behavior (no oscillations or overshoot) results in a null fuzzy action. accordingly, the switching between the two controllers reduces to using pi if the fuzzy has null value; otherwise, the fuzzy output is used. in particular, the fuzzy controller can be designed so that a normal behavior fig. 6 shows the simulation model of the electrical block of bldc motor hall sensor signal generation"
"another unique feature for pure unicast scenarios is about routing. from lemma i (3), we know that the network is disconnected, and because every node performs a random walk, there is no way to predict contacts. therefore, the source or destination node cannot provide incentives to intermediate nodes except the direct next hop. as a consequence, we can only provide incentives to sustain two-hop relay [cit], i.e., every packet can at most take two hops from the source to the destination. in section iv, we will remove this constraint by considering human mobility traces from the real world."
"obviously, the interactions among autonomous and self interested entities can be modeled and analyzed as a socio economic system, and how to stimulate cooperative behaviors in such a system is an extensively studied topic in sociology and economics with a rich collection of analyzing techniques and promising solutions [cit] . therefore, it is not surprising that all proposed incentive mechanisms for mwns in the literature draw analogies from their counterparts in human this work was partially supported by the u.s. national science foundation under grants cns-0916391 and cns-072 1744, the national natural scie ? ce foundation of china under grant 61003300, and the chma iii project under grant b08038. the work of x. zhu was also partially supported by the fundamental research funds for the central universities under grant ly i 000090 i 021. ? currency used in (tit-for-tat) bartering [cit] wireless [ 4] - [cit] generous titreputation [1 6]-[ 1 8] networks for-tat [cit] social direct indirect reputation mechanisms reciprocity reciprocity societies. table i gives a glimpse of design space of incentive mechanisms for mwns and points out their relationships with economic and social mechanisms enabling cooperations in human societies. existing approaches for providing incentives basically can be classified into three categories as follows."
"a by sending back an e-cash at timeslot 3. however only one data packet is transmitted, and e-cash transfer is overhead. by mixing the ops, because with high probability each cp brings some new information, it can be treated as virtual commodity currency, i.e., generally acceptable in payment. for example, in fig. 2(b), at timeslot 3, node b can buy a packet from a by paying a cp (0,1,1)Â· x. after timeslot 3, node a can decode all ops, and node b only need one more cp. high exchange efficiency achieves without incurring any overhead."
"for every node u, its latency value dt (u) is a kind of tempo ral distance between itself and the infostation, and therefore can be used as a routing metric. all nodes have incentives to maintain a correct dt (u) for securing its own packet delivery and the information highway can be constructed in a fully distributed way. for unicasts from mobile nodes to the infostation, multi-hop relay can be performed in the following way. when a relay node u contacts a node v, and v has a -edge in contact graph (a) constructing infonnation highway (b) using infonnation highway to relay fonn the social contact graph packets to the infostation fig. 7 . information highway and multi-hop relay."
"we assume that all mobile nodes are self-interested and still rational. they have the non-cooperative behaviors mainly because they want to save resource such as bandwidth and battery power. we also assume that all infostations are under the control of one authority, and they will do all they can to encourage mobile nodes to use short-range links. unlike previous work [cit], our c4 does not make any assumptions about routing protocols used in mwns. our c4 is designed to support traditional store-and-forward routing schemes as well as new routing schemes in a dtn fashion (i.e., store-carry-and-forward [cit] )."
"proof\" we observe that for a given timeslot, whether a mobile node can obtain a new packet depends on two condi tions: (i) whether this node is selected into a communication pair? this condition is characterized by lemma i and is independent of the incentive mechanism used. (ii) when this node is selected into a communication pair, whether it can make a successful exchange with its partner? here, we assume that for an arbitrary node pair, there exists a probability pe of successful exchange which only depends on the incentive mechanism. we want a static, summarized characterization of pe, i.e., we obtain a value of pe which is averaged over all possible node pairs and all timeslots. we then simplify our analysis by assuming that given the incentive mechanism, the probability of successful exchange for any node pair at any timeslot is the same as this value. it is not hard to see that this key assumption is inconsistent with the interaction dynamics. nevertheless, our simulation results agree closely with the analytical results, indicating that this assumption works well in systems with moderately large number of nodes n ï¿½ 50."
"obviously, the best choice of k here corresponds to the best choice of grouping parameter g. here we develop a distributed scheme to find the optimal k and construct the corresponding k-clique-communities, we call the maximal complete subgraphs as cliques. in contrast to the k-cliques,"
"we know that if we decrease the group size (i.e., n/ g) to reduce costs, the effectiveness of c4 will also be reduced. this relationship is called effectiveness-cost tradeoffs characterized in corollary 2. however, all these results are based on the contact graphs without community structures. for the example given in fig. 9(a), we can directly observe that, when we group the yellow community and blue community separately, compared to grouping yellow and blue communities together, the effectiveness will not be affected. the reason is simple: most of nodes in the yellow community in fact have no chance to contact most of nodes in the blue community. therefore, when the vcc packets are only valid for the yellow community, there is no efficiency loss. so, every community can has its own valid vcc packets, just like every country has its own currency. compared to a global currency, the loss of efficiency is ignorable. therefore, when we shrink the group to the community, we can reduce overhead without losing effectiveness, based on above discussions, we conclude that the best choice of grouping should be based on community structures in the social contact graphs. the problem left is how to identify communities automati cally. the difficulty comes from the unique features of commu nity strictures in social graphs. because a node in a community is not necessarily linked to all other nodes in the community, the community is not a clique (i.e., a complete subgraph)."
"many drive systems today employ a conventional controller such as a pi-type controller. this method works well, but only under a specific set of known system parameters and load conditions. however, deviations of the system parameters or load conditions from the known values cause the performance of the closed-loop system to deteriorate, resulting in larger overshoot, larger rise time, longer settling times and, possibly, an unstable system. it should be noted that the system parameters such as the system inertia and damping ratio might vary over a wide range due to changes in load conditions. generally, a pi speed controller could be tuned to a certain degree in order to obtain a desired performance under a specific set of operating conditions. less than ideal performance is then observed when these operating conditions vary. thus, there is a need for other types of controllers, which can account for nonlinearities and are somewhat adaptable to varying conditions in real time. other methods are now being employed, such as fuzzy logic, in order to achieve a desired performance level. the model of speed controller has been realized using the simulink toolbox of the matlab software. the main function of speed controller block is to provide a reference torque which in turn is converted to reference current and is fed to current reference generator. the output of the speed controller is limited to a proper value in accordance with the motor rating to generate the reference torque. the speed controllers realized in this project are proportional integral (pi) controller, fuzzy logic (flc) controller and hybrid pi-fuzzy controller."
"based on this assumption, we can model the dynamics of obtaining packets for a given mobile node as a discrete time markov chain illustrated in fig. 4 . denote the state as the number of packets remaining to be obtained by a mobile node. initially a node is at state k. since the first two packets must be obtained from an infostation, the next state is k -2."
"the first category is barter based approaches [cit], which are based on direct reciprocity: node a would provide re sources/services for node b only if b simultaneously provides resources/services for node a. this kind of bilateral and synchronous resource/service trading makes barter extremely simple to implement. from a system perspective, there is no need to keep any long-term state information, in the form of either reputation or currency, and as a consequence the imple mentation cost of barter is almost zero. however, synchronous trading is easy to fail when an action and its reward are not simultaneous [cit] . the second category is virtual-currency based [cit], in which participating nodes would earn virtual currency by providing resources/services to others and spend the virtual currency to obtain resources/services from others. by taking virtual currency as a medium of exchange, nodes can then trade resources/services asynchronously, which overcomes the shortcoming of barters. virtual currency, how ever, incurs a high implementation overhead, e.g., billing and e-cash transfers, implementations of centralized bank and dispute-resolution mechanisms, etc. in the third category, i.e., reputation based approaches [cit], participants build up their reputation scores by providing services for others, and highly reputed participants receive preferential treatment when they need help. obviously, reputation scores here can be treated as another form of virtual currency. therefore, reputation based approaches share the same advantages and disadvantages as virtual-currency based ones. to sum up, existing incentive mechanisms are either less effective or incur high implementation costs, and therefore do not fit well with the requirements of mwns. a new design paradigm is needed."
"suppose this broadcast session is about distributing a movie stored in a web server in the internet. the infostation treats all network layer packets received from the web server as original data packets (ops). we assume each op has l bytes. at the infostation, random linear network coding (rlnc) [cit] each mobile node can either download packets from an info station or exchange packets with neighboring mobile nodes. in a non-cooperative network without any incentive mechanisms, the former is the only mechanism for packet dissemination. it only uses the high-speed channel between an infostation and a node near it, while wasting all the equally excellent channels between neighboring mobile nodes. a barter-like scheme (without network coding) alleviates this problem as follows: when two mobile nodes contact, they inspect the packet contents of each other. if each node identifies a packet that it wants, a bilateral packet exchange takes place. however, even for broadcasts, nodes can easily end up in a deadlock situation because of the requirement of mutual wants. in our c4, let s u ( t) and sv ( t) denote the subspaces spanned by the cps at neighboring node u and v, respectively, at the beginning of the times lot t. if s u ( t) ct sv( t), we say node v wants cps at u. if there exists mutual wants, then node u and v can successfully exchange cps as follows: node u sends a new cp p u e s u ( t) and p u tj-sv( t), and node v sends back a new cp p v e sv( t) and p v tj-s u ( t) as return."
"by assuming contacts between pairs of nodes are perfectly periodic, we can calculate the delay of the newest status updates from the infostation to every mobile node along this tree. the result is fig. 7(b), the information highway with every node knows the latency value from the infostation to itself. the concentric circles denote ball radii increasing by 5 minutes, and the distance of each node from the common center is its latency value from the infostation."
"the action of the current reference block is to produce three-phase reference currents, i ar, i br, i cr depending on the signal of current amplitude, is, and the position signal, then the reference currents are directly given to the current hysteresis controller block. table 2 shows corresponding relation between the rotor position and the three-phase reference current table. 2 relationship between reference current and rotor position"
"the use of community structures in our c4 is straightfor ward. when there exists no broadcast or multicast traffics, we need to group some unicast sessions destined to different nodes to perform group based inter-session network coding to generate vcc packets. the way of grouping will significantly affect our c4's performance as discussed in section iii-c."
"ops are all destined to node b, node a still has incentive to participate in exchanges before he collects three independent cps. in this way, we enforce node a to help relay one cp to node b at timeslot 3, which is impossible for bartering."
"when a mobile node receives a valid vee packet, in broadcast (or mixed traffic) scenarios this vee packet incurs no control overhead. however, in pure unicast scenarios, this vee packet only contains gin useful information on average. therefore, this vee packet incurs ( i-gin) control overhead, i.e., there is no vee packet without incurring any cost. this is the key feature emerging in pure unicast scenarios."
"based on the common requirements of future mobile com munication environment [cit], our c4 assumes the fol lowing generic model for mwns. as illustrated in fig. lea), there are two kinds of entities: mobile nodes and infostations."
"fuzzy logic control (flc) is a rule based controller. it is a control algorithm based on a linguistic control strategy which tries to account the human's knowledge about how to control a system without requiring a mathematical model. the approach of the basic structure of the fuzzy logic controller system is illustrated in fig.2 . the membership function is a graphical representation of the magnitude of participation of each input. there are different membership functions associated with each input and output response. here the trapezoidal membership functions are used for input and output variables. the number of membership functions determines the quality of control which can be achieved using fuzzy controller. as the number of membership function increases, the quality of control improves. as the number of linguistic variables increases, the computational time and required memory increases. therefore, a compromise between the quality of control and computational time is needed to choose the number of linguistic variables. the most common shape of membership functions is triangular, although trapezoidal and bell curves are also used, but the shape is generally less important than the number of curves and their placement."
"also our c4 allows all possible combinations of traffic patterns (e.g., broadcasts, multi casts and unicasts). the only requirement is that most traffics go through infostations."
"the processing stage is based on a collection of logic rules in the form of if-then statements, where the if part is called the \"antecedent\" and the then part is called the \"consequent\". the knowledge base comprises knowledge of the application domain and the attendant control goals. it consists of a data \"base\" and a linguistic (fuzzy) control rule base. the data base provides necessary definitions, which are used to define linguistic control rules and fuzzy data manipulation in an flc. the rule base characterizes the control goals and control policy of the domain experts by means of a set of linguistic control rules. decision making logic is the kernel of an flc."
"cliques cannot be subsets of larger cliques, therefore they have to be located in a decreasing order of their size. the largest possible clique size in the graph is determined from the degree-sequence. therefore, the infostation first finds the node with the highest degree in the social contact graph, then finds the clique containing that node. after recording this clique, the infostation deletes the node and its edges in this clique from the social contact graph. then the infostation repeats this procedure until no nodes are left. after that, the recorded set of cliques is a set of independent k-cliques in original social contact graph. fig, 9 (b) illustrates seven independent k-cliques in fig. 9(a), where different colors represent different k-cliques. then the infostation chooses several largest k-cliques as seed groups and broadcast the node lists for each seed group. other nodes can choose group memberships by themselves. the infostation will perform group-based inter-session network coding on packets from the same group to generate vcc packets, since all mobile nodes have the incentives to choose the most appropriate groups for themselves, after several iterations, the best community structures will emerges by themselves and the optimal k (or g) value will be automatically determined. the system itself. we can observe that there indeed exists an optimal value of g, which corresponds to the critical point at which when we further increase the cost cp( c4), the corre sponding packet delivery delay td ( c4) cannot be decreased."
"the objective of the hybrid controller is to utilize the best attributes of the pi and fuzzy logic controllers to provide a controller which will produce better response than either the pi or the fuzzy controller. there are two major differences between the tracking ability of the conventional pi controller and the fuzzy logic controller. both the pi and fuzzy controller produce reasonably good tracking for steady-state or slowly varying operating conditions. however, when there is a step change in any of the operating conditions, such as may occur in the set point or load, the pi controller tends to exhibit some overshoot or oscillations. the fuzzy controller reduces both the overshoot and extent of oscillations under the same operating conditions. although the fuzzy controller has a slower response by itself, it reduces both the overshoot and extent of oscillations under the same operating conditions. the desire is that, by combining the two controllers, one can get the quick response of the pi controller while eliminating the overshoot possibly associated with it."
"smaller dt (v), then packets destined to the infostation will be relayed to v. in fig. 7(b), one of such relay path from node a to the infostation is illustrated by red curves. the unicasts from the infostation to the mobile nodes will be even simpler. the routing path will follow the tree of information highway, because by the definition of information highway, this is the path with the minimum packet delivery delay from the infostation to the destination."
"in a multi-hop wireless network (mwn), when the source and the destination nodes are not within direct transmission range of each other, they must rely on intermediate nodes to forward packets for them. hence, the performance of a mwn heavily depends on the participating nodes' willingness to cooperate. if all nodes are cooperative, such as in military networks configured by a central authority, cooperation can be taken for granted. however, for most current and emerging mwns, participating nodes are owned and administered by different authorities such as different persons, and therefore are autonomous. when a node forwarding traffic for other nodes, it expends its own bandwidth and power resource without any direct benefit. a self-interested node therefore has a strong incentive to fre e ride, i.e., use the network resources of other nodes without contributing its own. if free-riding behaviors prevail, such networks even cannot function. therefore, the proper design of incentive mechanism for encouraging re source sharing at the network layer is essential for the success of any mwn in civilian or commercial environments [cit] ."
"as illustrated in table iii, this generic model includes several important multi-hop wireless network architectures which attract great interest from both academia and industry."
"current hysteresis controller block is to achieve hysteresis current control, in which input signals are three phase reference currents and practical currents, and output signals will act as the control signals of inverter. when the practical current is greater than the reference current and the error is greater than the ring width of hysteresis comparator, the corresponding phase will be conducted forward and turn off reversely."
"this paper presented the design of a hybrid pi-fuzzy control system for the speed control of a brushless dc motor the performance of the fuzzy logic controller is better under transient conditions, while that of the proportional plus integral controller is superior near the steady state condition. the combined advantages of these two controllers can be obtained with hybrid pi-fuzzy speed controller. mathematical model of the bldc motor is studied. based on this, the modeling and simulation of the proposed control system is done. the performance of the system using pi controller, fuzzy logic controller and hybrid pi-fuzzy controller are compared. it is shown through extensive simulation that the performance of the hybrid pi-fuzzy controller is better than using pi controller and fuzzy controller alone for the speed control of bldc motor."
"the membership function is divided into seven sets: nb: negative big, nm: negative medium, ns: negative small, z: zero, ps: positive small, pm: positive medium, pb: positive big."
corollary 1: suppose the infostation has k data packets to send to e' n randomly selected mobile nodes. the values of td ( im) and cp( 1m) remain on the same order as that in theorem 1 and 2 when e is a constant.
"the only problem left here is why our c4 can sustain two more-hop relay when the information highway emerges. for those intermediate relay nodes who do not contact with the source or the destination of packet pkt, the incentives for them to buy pkt come from the fact the the nodes which are closer to the destination will have more incentives to buy it. therefore, they can sell it a little later and from this buy-sell procedure they can obtain more vcc packets. for example, in fig. 7(b), on the path from node a to the infostation, node e will buy pkt from node d because he knows some nodes like e has the willingness to buy pkt, because e is closer to the infostation and has more chances to sell pkt to the infostation. fig. 8 shows how many hops the packets take to reach the destination. we can see that for both datasets, the hop count distributions of our c4 are the same as that of the cases when all nodes are fully cooperative. therefore, we can conclude that our c4 provides adequate incentives to sustain multi-hop relay when the information highway is available."
"there are only two kinds of elementary interactions when c4 puts no constraints on the underlying routing protocols or traffic patterns, and therefore achieves the maximum flexibility in supporting different application scenarios in mwns. the coding strategy adopted by infostations to generate vcc packets determines the performance of our c4. the key point is how to select sessions that need be mixed in generating vcc packets. there exists a tradeoff: if we mix the sessions interested by different mobile nodes, we can obtain vcc packets which are valid for a larger population of mobile nodes. then the vcc packets can achieve a better exchange efficiency. however, the useful data information contained in the vcc packet for a mobile user is also decreased, which means the overhead of using vcc packets becomes larger. therefore, a cautious choice should be made in order to obtain an optimized balance. a quantitative analysis of optimized coding strategies will be presented in section iii in detail."
"where p is the number of poles, Ï r is the rotor speed in mechanical rad/sec and Î¸ r is the electrical rotor position in electrical radian."
"for this scenario, in the central squarelet the elementary inter action is for the infostation to send two packets to a mobile node, while in regular squarelets the elementary interaction is packet exchanges between two mobile nodes."
"here, we just give two qualitative guidelines. (i) because all users are interested in broadcast data packets, they can be safely mixed without worrying about overhead. the only problem is that in order to prolong the valid time of vcc packets, the size of one generation, i.e. k, should be large enough. for traditional rlnc, this means the size of the packet header is pretty large, because the size of coding vector which should be included in the coded packet header is 8(k)."
"this optimal value for cambridge dataset is 6 while that for mit dataset is 8. this means that, for example, the community structures of social contact graph of cambridge dataset is most appropriately described by 6-c1ique-communities."
"the second kind of heterogeneity is called community structures. social contact graphs typically contain parts in which the nodes are more highly connected to each other than to the rest of the graph. the set of such nodes is usually called a community [cit] . fig. 9(a) illustrates four communities in an exemplary social contact graph, each with a different color. we can observe that (1) a typical member in a community is linked to many other members, but not necessarily to all other members in the community, (2) different communities may overlap. in fig. 9 (a) overlapping parts are emphasized by grey color."
"the proportional plus integral (pi) controller is widely used for industrial applications. the input to the pi controller is the speed error (e), while the output of the pi controller is used as the input of reference current block."
"the td (c4). because each coded packet only contains gin useful information for a particular mobile node, each coded packet introduces (1 -;7) control overhead. on average, each mobile node need exchange one coded packet with every relay node, and totally e (nlg) coded packets need be exchanged for the destination node successfully decoding one data packet."
"in recent years, brushless dc (bldc) machines have gained widespread use in electric drives. these machines are ideal for use in clean, explosive environments such as aeronautics, robotics, electric vehicles, food and chemical industries and dynamic actuation. using these machines in high-performance drives requires advance and robust control methods. conventional control techniques require accurate mathematical models describing the dynamics of the system under study. these techniques result in tracking error when the load varies fast and overshoot during transients. in lieu of provisions for robust control design, they also lack consistent performance when changes occur in the system. if advance control strategies are used instead, the system will perform more accurately or robustly. it is therefore, desired to develop a controller that has the ability to adjust its own parameters and even structure online, according to the environment in which it works to yield satisfactory control performance. an interesting alternative that could be investigated is the use of fuzzy logic control (flc) methods. in the last decade, flc has attracted considerable attention as a tool for a novel control approach because of the variety of advantages that it offers over the classical control techniques. unlike other conventional control schemes, flc is a model-free controller. it does not require an exact mathematical model of the controlled system and therefore, is less sensitive to system parameter changes. in addition, rapidity and robustness are the most profound and interesting properties in comparison to the traditional control methods."
"the only question left then is: what are the benefits of using virtual commodity currency (i.e., c4) in mwns? to fairly evaluate an incentive mechanism, we need consider three kinds of costs. as described in table ii, in the physical space, compared to the exchange of physical commodities, the im plementation cost of fiat currency (fc) or the mouth-to-mouth reputation (rep) can be ignored. barter has a high transaction cost; commodity currency (cc) has a high transportation cost; only fc can keep all three costs low, and therefore is the best choice. this explains the fact that nearly all contemporary economic systems are based on fiat currency. what happens in the digital space like a wmn? the key difference is that the goods in a wmn are information packets. virtual currency (vc) and reputation are also stored and transacted in the form of information bits. therefore, compared to the data packet exchanges, the implementation cost of vc/rep is pretty high."
"is the expected delivery time for unicasting one data packet from the infostation to a randomly selected mobile node (or in a reverse direction) under the incentive mechanism 1m, and cp( im) is the expected total number of control packets needed to support achieving td ( 1m) for that data packet."
"in this subsection, we assume there exist at least n unicat sessions, each of them is destined to one mobile node. we divide these unicast sessions into 9 groups ( 1 :::; 9 :::; n), each group (called coding group) containing n i 9 distinct sessions. only the data packets destined to mobile nodes in the same group will be mixed to generate vee packets, i.e., we perform group-based inter-session network coding (with grouping parameter g) to generate vee packets. there may exist other unicast traffics, but they will not be involved in generating vee packets. for pure unicast scenarios, td ( im)"
"the switching between the two controllers needs a reliable basis for determining which controller would be more effective. the answer could be derived by looking at the advantages of each controller. both controllers yield good responses to steady-state or slowly changing conditions. to take advantage of the rapid response of the pi controller, one needs to keep the system responding under the pi controller for a majority of the time, and use the fuzzy controller only when the system behavior is oscillatory or tends to overshoot. thus, after designing the best stand-alone pi and fuzzy controllers, one needs to develop a mechanism for switching from the pi to the fuzzy controllers, based on the following two conditions: 1) switch when oscillations are detected; 2) switch when overshoot is detected. the switching strategy is then simply based on the following conditions: if the system has an oscillatory behavior then fuzzy controller is activated, otherwise pi controller is operated."
here the inputs for fuzzy logic controller are the speed error (e) and change of speed error (ce). speed error is calculated with comparison between reference speed and the actual speed.
"the most important things in fuzzy logic control system designs are the process design of membership functions for input, outputs and the process design of fuzzy if-then rule knowledge base. fig 3 shows the membership function of speed error (e), change in speed error (ce) and figue.4 shows the membership function of output variable. in practice, one or two types of membership functions are table. 1 rule base of fuzzy logic controller enough to solve most of the problems. the next step is to define the control rules. there are no specific methods to design the fuzzy logic rules. however, the results from pi controller give an opportunity and guidance for rule justification. therefore after thorough series of analysis, the total 49 rules have been justified as shown in table 1 ."
the objective of the hybrid controller is to utilize best attributes of the pi controller and fuzzy controller to provide a controller which will produce better response than either pi or fuzzy controller. the superiority of both fuzzy and pi controller are integrated together by using a switch as shown in fig.24 . the speed error and simulation time control the switch such that it will be switched to fuzzy controller when the speed error is large and vice versa to pi controller when the speed error is small with appropriate time.
"we note that above model is the standard network model widely used in the literature for mwn performance analysis (see [cit] and references therein), and its behaviors are characterized by the following lemma when n -+ 00."
"a mobile node can establish a short-range wireless link (e.g., wi-fi) with other mobile nodes in its vicinity. the short range links tend to be intermittent because of node mobility."
"conversely, it will be conducted reversely and turn off forward. when selecting proper hysteresis ring width, the practical current will track the reference current continuously. by this means the current closed-loop control can be achieved."
"the implementation cost of c4 is always smaller than that of vc, because when e-cash is used as the medium of exchange, it only represents control overhead; while when coded packet is used, it also carries useful data. therefore, from table ii, we can see that c4 is the best choice in the digital space."
"in previous section, there are two problems left for our c4 for pure unicast scenarios, i.e., (i) two-hop relay constraints and (2) unavoidable effectiveness-cost tradeoffs. we note that these two problems are not caused by our c4, instead they are the consequence of oversimplified and unrealistic mobility model used for performance analysis. in this section we address these two problems by considering unique features in real-world user mobility patterns. this section also serves to validate our c4's performance with human mobility traces. in this section, we use two experimental datasets gathered by the haggle proj ect (referred to as cambridge) and the mit reality mining project (referred to as mit) . the characteris tics of these datasets are summarized in table iv . we first convert all these real life data into social contact graphs, with devices/mobile users as node sets and contacts between two nodes as edge sets. from these social contact graphs, we can easily find that they are heterogeneous both in terms of edges and community structures. note that from the random walk mobility model, we can only obtain a homogeneous contact graph, i.e., a complete graph with equal edges. there are no link heterogeneity or community structures because every node has the same probability to contact every other node. in what follows, we show how to utilize these two kinds of heterogeneities in reality to facilitate our c4 design. here we do not mean our c4 can work only if the information about social contact graph is available. we only want to show that c4 can even work better if this information is available. in fact, all dtn routing schemes try to collect and utilize this information [cit], and our c4 does not require anything more."
"[cell model] consider a square geography of area an with a fixed infostation at the center, as shown in fig. l(b) . we assume the geography wraps around each boundary, effectively creating a torus. we refer to this torus as a cell. a cell is intended to mimic a typical multi-infostation network in which an infinite grid of infostations populate an infinite plane. the area an relative to the single infostation serves to characterize the density of fixed infostations over the terrain."
"bldc motor can be modeled in the 3-phase abc variables which consist of two parts. one is an electrical part which calculates electromagnetic torque and current of the motor. the other is a mechanical part, which generates revolution of the motor. the electrical part of bldc motor can be represented in matrix form as follows:"
"based on above discussions, we conclude that when broad cast or multicast traffics exist in the network, all of them should be utilized to generate vee packets by performing intra-session network coding. vee packets generated in this way can facilitate packet/service exchanges without incurring any cost. however, the situation will be complicated for scenarios with pure unicasts."
"the rest of the paper is organized as follows. the design of c4 for a generic mwn is described in section ii. in sec tion iii we evaluate the performance of c4 through theoretical analysis and simulations. in section iv, we show how to utilize the social network formed by mobile nodes to further improve the performance of c4 with pure unicast communications."
"before introducing any new solution, the first question we should ask is: is there any room fo r fu rther exploration? if we go through the design space of incentive mechanisms, there do exist an unexplored area which is the counterpart of commodity currency in the physical world (i.e., the question mark in table i ). economists define currency (or money) as a generally acceptable medium of exchange [cit] . with the use of currency, the problem of \"double coincidence of wants\" is avoided, and the transaction cost of searching a successful trade is reduced. historically, currency originated as commod ity currency. when a physical commodity (e.g., compressed tea leaves in ancient china) has value to everyone and is 978-1-4244-9921-2/11/$26.00 Â©2011 ieee original data packets for different mobile users are mixed to produce coded packets. as a consequence, each coded packet has an intrinsic value to every node involved, and is ready to act as virtual currency to facilitate cooperations."
"according to source-destination relationship within the wireless domain, all original data packets in our c4 can be classified into two categories: (i) download data packets: the packets from the infostation to mobile nodes; and (2) upload data packets: the packets from mobile nodes to the infostation. our c4 take the whole network layer packet as an original data packet (including packet header). each original data packet has a unique packet id, and a session id indicating which session this packet belongs to. a mobile node can tell whether he/she is interested in it by checking its session id. in our c4, every transmitted packet within the wireless domain is a coded packet, and has a special format. at the infostation, some download data packets are selected and coded to produce virtual commodity currency (vcc). we call these coded data packets as vcc packets. as illustrated in fig. 3 (a), a coded packet consists of a header and a body. the body stores the coded data block. for a vcc packet, the header consists of three parts: (1) tpe field, which is set to 11 to indicate a vcc packet; (2) session field, which indicates session ids involved in the coded data block in the body part, and it can be implemented by a bloom filter; (3) cpvgc and does no collect enough coded packets to recover all ops of this session. download/upload data packets will not be directly transmitted in our c4. they will be carried by vcc packets. for a download data packet with packet id oid_q and destination node id d_id, which is not selected to produce vcc packet, the infostation first selects one cpvgc as the carrier, and then combines these two packets as shown in fig. 3(b) . the tpe field is set to 01 to indicate a coded download packet. the coded block is the linear combination of the coded block of cpvgc with the coding coefficient 1 and the download data packet with the coding coefficient c_q. at a mobile node with node id s_id, the coded upload packet is constructed in a similar way, as illustrated in fig. 3(c) . the tpe field is set to 00 to indicate a coded upload packet and the source node id s_id is also included in the packet header."
"when a step change at 0.5 was applied the pi controller failed to track the reference speed of 1000rpm which was well tracked by the fuzzy controller. during steady state conditions, the settling time of fuzzy controller was found to be more when compared to pi controller. in hybrid pi fuzzy controller this is compensated by the combined action of pi and fuzzy controller."
"the first kind of heterogeneity is called edge heterogene ity [cit] . as shown in fig. 7(a), edges are annotated with one or more times at which two nodes contact. we can see that not all edges in the social contact graph are with the same importance. for example, we assume that the infostation updates its status information every timeslot, and every mobile node exchanges its newest status information about the infostation with others during contacts. then, we can observe from fig. 7(a) that, although node g can contact the infostation, for most of timeslots, node g obtains the newest status information about the infostation from node h. so, given the existence of the information propagating highway \"g-h-infostation\", edge \"g-infostation\" can be deleted from the social contact graph. for the same reason, edge \"i-infostation\" and edge \"g-i\" can also be deleted. by continuing this deleting procedure, at last, we will obtain a tree rooted at the infostation. this tree is called the information highway, i.e., the structure of fast indirect paths from the infostation to all mobile nodes [cit] ."
"most symmetric cryptographic algorithms rely on s-boxes to implement non-linear (yet reversible) transformations that obscure the relationship between the key and the cyphertext. for example, the third s-box of the serpent cipher [cit] takes 4 bits of input and produces 4 bits of output. to implement this s-box in c, we would typically write:"
"above that, extensions and alteratives (beyond the scope of our analysis) can be found in the current literature: [cit] put forward the idea of using a cumulative risk measure based on the entropic value at risk (cevar). [cit] . second, Ï [cit] by replacing the relative entropy in the dual representation with different divergences as suggested in ahmadi-javid (2012c) first. [cit]"
"the capability of detecting smaller areas usually means more accurate localization, so it is important for an image forgery detection algorithm. inter-channel similarity based method is more capable of detecting small blocks, but it may fail when the image block 1) does not contain enough (e.g. in the all-black or all-white regions) or 2) contains too much (e.g. in the edge regions) high-frequency content. statistically, the values of the inter-channel similarity s in equation (10) are proportional to the means of diagonal-band wavelet coefficients. so if a hard-threshold strategy is applied, some untouched blocks in the edge regions will be incorrectly classified as enhanced ones, which will lead to the increase of false positive rate correspondingly. to solve this problem, a soft-threshold method is needed. from the experiments, the relationship between s and the mean of the wavelet coefficients d in an untouched image is relatively stable. therefore, we model this relationship by a linear function and then use it in the threshold process to determine whether the image block in question is enhanced or not. if the mean of the wavelet coefficients is:"
"in future work, we intend to broaden the range of our evaluation to these cryptosystems as well as consolidate our scheduling algorithm across the range of supported architectures."
these discussions of diversification can be extended by the notion of comonotonic additivity which describes the additivity of a risk measure for perfect positive dependent random losses: 6 [cit] .
the expectile value-at-risk (exvar) 12 has recently attracted a lot of attention in the discussion because it is the only known coherent and elicitable alternative. [cit] in the context of asymmetric least square estimation. its definition is given as the unique solution of an asymmetric quadratic optimization problem:
"the inter-channel similarity can be demonstrated by the constant-difference-based interpolation method (shown in figure 2 ). here we only take the reconstruction of red channel for example. suppose r s and g s are the color planes sampled by the cfa. for the bayer cfa sampling pattern, the size of r s is only 1/4 of the image size while g s is 1/2 of the image size. to obtain r s and g s with the full image size, zeros are filled at non-sample locations. assume that r and g are color planes reconstructed from sample values r s and g s, respectively. g is simply reconstructed from g s using bilinear or edge-directed interpolation. let g sr be the color plane that is produced by sampling g at red sample locations and filling in zeros at other color sample locations. the reconstruction of r then can be helped with g:"
"in this section, we provide a comprehensive review of recently suggested risk measures (beyond var) together with their key properties (from a financial, statistical and/or theoretical perspective). for this purpose, a risk measure in this paper is defined as usually as a mapping from the space of positive random variables or probability distributions to the real numbers in order to quantify the amount of capital that a company has to reserve against unexpected losses."
"the class of distorted risk measures follows a similar idea as the class of spectral risk measures. the original concept was introduced and studied in the context of actuarial pricing premium theory by wang (1995 wang (, 1996 wang (, 1998 . the basic idea of these risk measures is to distort the expectation of a loss l with an individual distortion function g, which should be non-decreasing and left-continuous. the general definition is"
"bitslicing consists in reducing an algorithm to bitwise operations (and, or, xor, not, etc.), at which point we can run the algorithm with bit-level parallelism, viewing a n-bits register as n 1-bit registers, and a bitwise and as n-parallel and operators. to execute such circuit, the inputs must be converted: the i-th bit of the j-th input become the j-th bit of the i-th register. this operation amounts to a matrix transposition transforming n m-bit inputs into m n-bit registers."
n p is the width of the region in histogram spectrum over which p(x) decays from 1 to 0. usually it is set to be around 8.
"usuba is a synchronous dataflow programming language we have designed to write bitsliced programs. it contains specifics constructs to address the specificities of cryptographic algorithms, in particular permutation and lookup tables (section 2). we presented the compilation steps to get from an usuba program to an efficient c program (section 3). the optimizer exploits the dataflow properties of usuba to make optimizations that most c compiler do not or cannot do as efficiently. we have focused our evaluation on des but we have also implemented other ciphers such as aes [cit], camellia [cit] and serpent [cit] ."
"the rest of paper is organized as follows. in section 2, we first review the original algorithms and list out some shortcomings that will be addressed in this paper. then we come up with the details of the improved counterparts accordingly. in the following section 3, experiments are setup to confirm the merits of the proposed approaches with detailed analysis and a comparison to prior methods in terms of both roc curves and real-world cut-and-paste image forgeries. finally, section 4 concludes this paper."
"due to observational noise [cit], sampling effects, complex lighting environments and cfa interpolation, the histograms of natural images are strongly lowpass and do not contain sudden zeros or impulsive peaks. while the smoothness of the original histogram will get disturbed or destroyed by contrast enhancement manipulations, which consequently cause the increase of high-frequency energy in the histogram spectrum. based on this observation, stamm and liu proposed a general contrast enhancement detection algorithm as follows [cit] :"
"inlining inlining enables further optimizations, as it may open opportunities for more cse or copy propagation. also, it saves the need to push and pop arguments on the stack, which has a significant impact when dealing with function with many parameters. unlike c compilers, usuba agressively inlines every function, exploiting the fact that sidestepping the call-stack outweighs the cost of executing a larger binary. the user is granted additional control through the usuba attributes _inline and _no_inline, which can be used to manually force a node to be inlined or not. table 1b shows the effect of inlining on des's code: it compares the code without inlining ((3), \"without\") from neither usuba nor clang, with clang's inlining ((2), \"clang\") and with usuba's inlining ((1), \"usuba\"). in all cases, constant folding and cse are active. without any inlining done by usubac (3), clang chooses to only inline the s-boxes (2), thus gaining 10% performance, but fails to inline the round function. however, usubac inlines every functions, thus increasing the throughput by more than 110% (2). this is mainly due to the fact that the round function takes in and returns a lot of variables, which causes unnecessary assignments through the stack. the code without inlining is not the smallest, as shown in column \"code size\", because it contains the round function and the numerous assignments related to its arguments."
"as can be seen from the above steps, there are some parameters or switches that need to be set carefully by users. the first one is n p, which controls the width of the region of interest in the frequency spectrum. setting it to be a small value around 8 eliminates the saturated effects, and meanwhile preserves the spectrum information as much as possible. the next parameter needs to be considered is the cut-off frequency t, which can be viewed as the dividing line between the low-frequency and high-frequency region. it is an important parameter as it directly affects the calculation of the averaged high-frequency measurement f . a large t means that only a small proportion of high-frequency components are taken into account, which gives rise to the oversensitiveness of detector for a fixed threshold. on the other hand, a small t may incorporate some undesired low-frequency components into the calculation and hence bring the detection accuracy down. for one specific contrast enhancement algorithm with fixed parameters, we can analytically or experimentally determine the optimal t for detection. but unfortunately, we do not have any prior information about the image being tested, and the optimal t varies from different forms of contrast enhancements. this can be demonstrated by the example of linear enhancement, which has the form of:"
"which leads to the useful practical and intuitive representation gluevar finally, alternatives to the var can be derived using the entropy concept, briefly denoted as entropy-based risk measures in the sequel. for instance, fÃ¶llmer [cit] introduce the entropic risk measure (erm) defined as"
"a flurry of c++ libraries provide a unified programming model for various simd architectures, such as boot.simd [cit], mipp [cit], ume::simd [cit], sierra [cit] or vc [cit] . these works are complementary to ours: our intent is to provide a language that (always) compiles to efficient bitsliced code while being amenable to formal verification. to this end, we have implemented specialized simd backends and could certainly benefit from piggy-backing on those libraries, much as we currently use openmp for multicore processing."
"we previously presented our work on the development of a physical phantom (wuphantom, us patent application) that can be used to seamlessly quantify the accuracy of a dir system. 21 here we used this physical phantom to evaluate the accuracy of dir for cbct with various scanning protocols and levels of image quality."
"we evaluate our compiler using our implementation of des presented in section 2. while des is outdated and should not be used for cryptographic purposes, it provides an interesting experimental platform. first, it was not designed with bitslicing in mind, which puts the expressivity of usuba to the test: our implementation of des is an almost literal translation of the des specification, unencumbered by bitslicing details. second, there exist several, publicly-available implementations of des, both in standard and bitsliced form for 64 bit architectures. we chose kwan's implementation because it is well-known and written in c, enabling us to compare our results across compilers."
"dsc values (means Â± standard deviation (sd)] of 0.89 Â± 0.07 (soft tissue) and 0.88 Â± 0.06 (bone) (for cbct pelvis vs ct); 0.83 Â± 0.07 and 0.90 Â± 0.06 (for cbct head vs ct), and 0.72 Â± 0.08 and 0.78 Â± 0.06 (for cbct image gently vs. ct) [ fig. 4(a) [ fig. 5(b) ]."
"recall that the only operations left in usuba0 are bitwise operations, function calls and binary constants. bitwise operations involving constants are simplified, following standard boolean algebra. doing this optimization in usuba produces slightly smaller c files, and guarantees that they are performed, irrespective of the level of optimization at which the c code is compiled."
"with cbct images in clinical practice. [cit] the performance of dir depends on numerous variables such as type of algorithm, implementation of that algorithm, and image modality and quality. 6 the clinical stability of dir is also influenced by factors such as the method of regularization 6 and user experience. 7 several methods have been used to evaluate dir algorithms, the three most common are contour outline comparison, landmark tracking, and simulating deformation with a phantom. 8 validating and commissioning dir are complex because of the lack of systematically documented processes for doing so. currently, means of validating the accuracy of deformable registration are being investigated at academic institutions. until the technology advances to allow production of a standard testable deformable phantom, the most common way to review deformation at present is by visual verification, 9 including tissue/voxel intensity overlay, viewing the deformable warp map, and displaying the difference map between two registered images. the american association of physics in medicine (aapm) recommends that formal image registration quality assurance (qa) programs be implemented at individual facilities. the program should include commissioning image registration and fusion software to ensure the accuracy of the tools used. 6 understanding the optimization approach used by the user's dir system is essential to appreciate how it converges, its limitations, and its potential pitfalls. last year, the aapm task group 132 reported 6 a new, downloadable virtual phantom to test dir accuracy and recommended using either a digital phantom or a physical phantom for dir tests. however, the digital phantom does not facilitate end-to-end testing of dir systems, in particular, facilitating the selection of optimal imaging parameters for dir systems. in addition, reports have shown that the validation procedure is more complex for digital phantoms. 10 the quality of images obtained with cone-beam geometry is known to be inferior to that of regular ct images because of the large solid angle receiving scattered radiation. 11 increased scatter from the patient obstructs the signal, degrading cbct image quality compared with standard ct, resulting in blurred images and changes in ct numbers. 12 for kv cbct, up to 2.5 times more photons arriving at a detector behind a normal-sized patient body are scattered as compared to fan beam ct. 12, 13 the accuracy of dir systems for cbct images under various levels of noise and dose has not been well studied. decreases in soft tissue ct number intensity have been noted from increased beam hardening and truncation of images due to smaller field of view (fov). some physical phantoms have been developed to assess the accuracy of dir. 4, 10, [cit] despite the existence of many methods to independently validate dir systems, none have been standardized and all demand a great deal of time and resources."
"the relationship between s and d relies heavily on image content, so it is difficult to exactly model it using an explicit mathematical formula. what's worse, it seems that this relationship varies from different cameras. the only useful information is that s is proportional to d statistically. in practice, any monotone increasing expression can be applied in the thresholding, but the simplicity of linear threshold makes it advantageous when compared with other quadratic or higer power threshold."
"obviously, the distortion function is not concave, which means that the rvar is not a coherent risk measure in general. however, it is robust and inherits some useful properties as a distorted risk measures. [cit] ."
"cryptographic applications put high requirements on the performance of the generated code. the c compiler will optimize the code produced by usuba but our experiments have shown that some optimizations produce better results when done by usuba's compiler itself. this is due to the unusual structure of our code: bitsliced programs tend to exhibit hundreds of live variables at any point, which is much more than what c compilers are used to deal with. optimizing usuba0 code offers several advantages. variables are assigned only once and there is no control structure, which means that the code is in single static assignment (ssa) form with no Ï node. moreover, information from the original usuba program is still available, which can be exploited to guide instruction scheduling for example."
"where x is the original pixel value, and p(x) is a pinch off function, whose role is to eliminate the low-end or high-end saturated effect in images:"
"one of the key risk drivers in a credit portfolio is concentration risk caused by an unbalanced distribution of the loans to individual borrowers (single-name concentration) or industry/country sectors (sector concentration). due to the requirement under basel ii pillar 2 to evaluate the impact of concentration risk in the portfolio, banks are interested in quantifying this particular impact in terms of additional risk capital. the following application investigates the practical usability of the aforementioned risk measures in order to quantitfy concentration risk."
"having defined our benchmark portfolio, we next explore the impact of concentrations in business sectors across the different risk measures. therefore, we increase the concentration in the business sector capital goods. we construct portfolio 1 (pf 1) from the benchmark portfolio by removing exposures from all sectors and adding them to capital goods. we then define the even more concentrated portfolio 2 (pf 2) by repeated application of this step. the exact sector distribution for pf 1 and pf 2 can be taken from table 3 . the increase in sector concentration is also reflected in the hirschmann-herfindahl index (hhi) (see table 3 )."
"the semi-parametric var estimator can be motivated by the results of the extremal value theory, where it was established that the distribution above a high threshold u can be approximated by a generalized pareto distribution gpd(Î², Î¾) [cit] )). let m denote the number of total observations and m u denotes the number of observations above the threshold u. then, the semi-parametric estimator admits the following form (assuming that the observation below the threshold are modelled by their empirical cumulative distribution function):"
"in order to carry out the investigation of the different risk measures in a suitable manner, the risk measures shall be adequately calibrated. the regulatory requirements to quantify credit risk is currently based on a confidence level of 99.9% for the var. therefore, this value is used as a benchmark henceforth to compare the effects of the remaining risk measures."
"observing that a bitsliced algorithm is fundamentally a circuit written in software, we designed usuba, a synchronous dataflow programming language for implementing circuits in software. this paper makes the following contributions: -we have designed usuba, a synchronous dataflow language targeting cryptographical and high-performance applications. while its syntax and semantics (section 2) are inspired by lustre [cit], it has been specialized to address cryptographical needs (lookup tables, permutations, binary tuples, arrays). -we have implemented a compiler, usubac, from usuba to c (section 3). it applies standard dataflow compilation techniques [cit] as well as domain-specific transformations, such as handling bitvectors, expanding permutations and lookup tables. -we have implemented several optimizations (section 4). this includes standard transformations such as inlining, constant folding, common subexpression elimination (cse) and copy propagation, as well as a domainspecific instruction scheduling algorithm. the latter is tailored to handle the unusual structure of our programs (large number of live variables, absence of control structures), for which c compilers have a hard time generating efficient code. -we evaluate the end-to-end performance of an usuba program, namely des, on several simd architectures and measure the impact of our optimizations (section 5). at equivalent word size, our usuba implementation is 15% faster than a functionally-equivalent, hand-tuned implementation of des [cit] . besides, the same usuba program can also be compiled to manipulate larger wordsizes (such as those offered by the avx-512 extensions), yielding a 350% improvement over the hand-tuned, fixed wordsize implementation. usuba is also trustworthy: we apply translation validation [cit] whereby each run of the compiler is followed by a verification pass that checks the semantics equivalence of the source program with the generated code. the same mechanism can also be used to validate an optimized usuba program (provided by the user) with respect to a naive usuba implementation (playing the role of a specification, also provided by the user). we shall not dwell on these techniques in the present paper, the description of the formal semantics and the evaluation of the verification framework being beyond the scope of the workshop."
"permutations are commonly used in cryptographic algorithm to provide diffusion. usuba offers syntactic support for declaring permutations. for instance, the initial permutation of des amounts to the following declaration specifying which bit of the input bitvector should be routed to the corresponding position of the output bitvector: the direct bitsliced translation of this permutation is a function of 64 boolean inputs and 64 boolean outputs, which consists of simple assignments. after copy propagation, a permutation is thus no more than a (static) renaming of variables."
"to overcome this criticism, the class of convex risk measures was introduced. every risk measure which is monotone, cash invariant and convex is called a convex risk measure:"
"another way to increase throughput is to use several cores, which gives (virtually) access to more registers. usuba can generate code exploiting several cores using openmp [cit] . we tested the openmp code generated by usuba on a 4x20 cores intel xeon e7-8870 v4 (figure 4b), reporting the speedup relatively to the monocore, non-openmp version. the overhead of openmp is negligible while the throughput of des is almost proportional to the number of cores used."
"despite of its popularity, several alternatives to the value-at-risk and the expected shortfall shot from the ground in the recent financial literature. we reviewed eight of them in the first part of this contribution. the second part of this contribution is dedicated to the measurement of unexpected losses in credit portfolios. thereby, the focus is on the allocation of the unexpected portfolio loss to sub-portfolios using euler's allocation scheme, which is well-established in credit risk management."
"a technical implementation (see, in particular, algorithm 1 [cit] ) can be found in the r package gcpm, which was used to generate the loss distribution for the hypothetical portfolios in the empirical part."
"generating c code from usuba0 is straightforward, assuming that the equations have been scheduled according to their data-dependencies. each node is compiled into a function, with return values passed by pointers. booleans are transformed to either int or vector types (depending on the underlying architecture). equations are converted into assignments. expressions are translated to the corresponding c expressions. function calls are converted to standard c function calls, where the left-hand side of the equation is passed by reference to the function. generating c code targeting a specific simd extension, such as intel's sse, avx and avx-512, arm's neon or ibm's altivec, only requires using the bitwise instructions specific to the vector extension, and embedding in the runtime an efficient transposition algorithm adapted to the architecture."
"2. differences in capital allocation for the considered risk measures: figure 2 illustrates the percentage distribution of the risk capital across all risk measures. above that, we highlighted in each pillar sectoral allocation for both the portfolio with a constant pd of 2% and 3.5%, respectively. whereas the upper diagram of figure 2 refers to the benchmark portfolio, the lower diagram corresponds to the portfolio with high sector concentration. consequently, across all risk measures, nearly 80% of the portfolio risk is allocated to capital goods. in general, the distribution seems to be relatively stable across all risk measures. however, evar and wang's measure allocate slightly less risk to the capital goods sector. in addition, the importance of the risk contribution for the sector transportation increases in the portfolio with higher pds driven by the correlation structure. although the allocation results are rather similar, the complexity of the it implementation of the risk measures differs: for instance, the implementation of exvar, evar, wang's measure and lvar requires some computational experience."
"this algorithm improves performance, as shown in table 1c : using usubac's scheduler instead of relying solely on that of the c compiler increases the performance by 10 to 30% on des. the c compilers struggle to keep the spilling low: from half to two thirds of the assembly instructions are moves. using usubac algorithm reduces the number of moves generated by the c compilers by 19% (clang) to 43% (gcc)."
"where f is the class of convex distribution functions on r + . the exercise of the risk manager is to choose a metric that fits the context of application. [cit] highlights the fact that risk measures which put emphasis on the tail of a distribution cannot be expected to be continuous with respect to topologies which completely ignore the tails like for e.g., the weak topology. consequently, for estimation processes of risk measures, another metric called wasserstein metric"
"to isolate the cost of encrypting data from the cost of converting data to a bitsliced format, we first evaluate our implementation of des without performing transposition of the data. figure 3a shows the throughput of such an implementation (compiled with icc 17.0.2 and running an intel skylake i9-7900x), depending on the type of registers used, general purpose 64-bit registers (gp-64), or simd registers: sse on 128 bits, avx on 256 bits, and avx-512 on 512 bits. we report the throughput of kwan's implementation of des with the label \"manual\". for a fair comparison, our implementation uses the same s-boxes as kwan (whereas smaller versions have been discovered since [cit] ): we are therefore comparing algorithmically equivalent programs."
"this property allows for breaking down the estimation process of the risk measure onto two steps. first, estimate the distribution function. second, calculate the risk measure from the estimated distribution function. this is the way we estimate the risk measures in section 5."
"the scaling of our code on vector extensions is slightly sublinear. while the c codes of the scalar and vector programs are very close, there are significant differences between the assembly codes: the scalar code is composed of about 12900 instructions and has an arithmetic intensity of 1.85, while the vector codes are composed of about 10400 instructions and have an arithmetic intensity around 4. the main difference between the two codes is the number of move instructions, which directly come from the amount of spilling. this is due to the fact that the vector codes use three-operand instructions while the scalar code only uses destructive two-operand instructions. one might then expect the vector codes to run faster than the scalar code, but this is not the case because skylake cpus have 4 scalar alu ports, and only 3 vector alu ports. furthermore, most memory operations can be performed at the same time as some arithmetic operations because they use 3 different ports (2 for loads and 1 for stores). it is worth pointing out that the vector codes are computation bound since their arithmetic intensity is 4, and the vast majority of their memory access are spill-related, and therefore in the l1 cache. hence, the memory is not the cause of the sublinear scaling on vector extensions. intelÂ® vtuneâ¢ amplifier reveals that the frontend, and in particular the micro instruction translation unit (mite), is limiting the scaling on sse and even more on avx. the instruction fetcher can only retrieve 16 bytes of instructions each cycle, which correspond to 4 instructions in the scalar code (whose sizes are around 3.7 bytes) but around 2 or 3 vector instructions, whose sizes are around 4.7 bytes. this is why the vector programs are executed at about 2.6 instructions per cycle (ipc), while the theoretical maximum is above 3.5. the mite is not an issue in standard vector codes, whose hotsopts are small loops (unlike our code, which is fully unrolled), and therefore benefit from the loopback buffer, which can issue up to 4 micro-operations per cycle."
"the high-level constructs offered by usuba boil down to a strict subset of the language, which we call usuba0. usuba0 can be understood as a core calculus for bitsliced algorithms (from a foundational perspective) or as a dataflow assembly language amenable to bitslicing (from a practical perspective). in usuba0, variables are only of boolean type. equations assign a single variable, unless they perform a function call in which case they assign a fully-expanded tuple of boolean variables. expressions consist only of bitwise operations and the only constants are of boolean type, i.e. 1 and 0. the front-end of the usuba compiler transforms usuba programs into (semantically-equivalent) usuba0 programs by converting lookup tables to boolean circuits, unfolding array definitions, expanding non-boolean constants, and inlining permutation tables, operators and tuples. the resulting usuba0 program can then be optimized (section 4) and eventually compiled to c (section 3.2)."
"as a result of our design, every usuba program can be compiled to an efficient bitsliced c program. the resulting imperative code is immune to cache-timing attacks since it cannot perform any input-dependent access to memory."
"where roi (1%) represents the mean hounsfield units (hu) in the roi of a 15-mm diameter, low-contrast object; roi (bk) represents the mean hu of the adjacent background; and sd (bk) represents the standard deviation of the background."
"finally, use of deformable phantoms for multimodality image registration adds complexity, as it requires phantoms to have components that are optimized for mri, pet, and single positron emission tomography. 6 the wuphantom was designed to faciliate tests of multimodality image registration. the dir accuracy for other image modalities, either same-modality or across-modality (e.g., 4d ct vs ct, mri vs mri, mri vs ct, pet vs pet) have not yet been validated. we plan to continue our studies in these areas in the near future."
"the focus of this contribution is on credit risk being one of the most important risk types in the classical banking industry. consequently, banks are urged from the supervisory authorities to reserve a certain amount of capital to cover unexpected losses from credit risk. typically, the unexpected loss of a credit portfolio is defined as the difference between a high quantile of the portfolio loss distribution (the so-called value-at-risk, briefly var) and the expected losses, which in turn are already been included in the customers' credit spread at the conclusion of the deal. due to the complex and nonlinear characteristic of the credit portfolio, the calculation of the portfolio loss distribution is usually performed with a credit portfolio model (details are provided in section 2). afterwards, the overall (portfolio) value-at-risk can be determined and the capital calculation step is completed. in the second step, the so-called capital allocation step, the unexpected loss of the portfolio is broken down to an obligor or transaction level in order to identify the essential/relevant risk drivers for the bank's portfolio. [cit] ."
ct and cbct image quality can be quantified in terms of the contrast-to-noise ratio (cnr). 22 head & neck protocol [ fig. 3(b) ] and cbct pelvis protocol [ fig. 3(c) ].
"nowadays, there is a lively discussion about the properties that a meaningful risk measure should satisfy. [cit] who postulate four theoretical properties for a risk measure:"
a risk measure is elicitable relative to a set of distributions function f if there exists a scoring function s for which the following expression holds:
"as the definition shows, it incorporates the tail of a loss distribution in both the frequency and the severity. [cit] shows the coherence of es. 9 the disadvantage of es 10 is its lack of accuracy statistical properties as robustness and backtestability. [cit] points out that the es is not elicitable and therefore has no intuitive backtesting method. [cit] confirms the statement in gneiting that es is not elicitable. [cit] introduce a new backtesting technique called ridge backtesting which is applicable for risk measures which are not elicitable/backtestable. in fact, backtesting for es is challenging and not as easy and intuitive as it is for elicitable risk measures, especially the var. [cit] introduces a new approach for the robustness of risk measures. if a risk measure is continuous with respect to the levy metric for a set c of distributions, it is called c-robust. against this background, one comes across a conflict between subadditivity and robustness."
"bitslicing is thus able to increase performance by exploiting data-parallelism, while improving security by disabling cache-timing attacks. historically, bitslicing is a manual process. for instance, here is a snippet of a bitsliced implementation of the data encryption standard (des) by kwan [cit] : the full implementation consists of hundreds of lines in the same, tedious style. debugging and maintaining such code is hard at best, and optimizing it even more so. furthermore, this code shows its age by failing to exploit modern vector extensions."
"from a practical point of view, law invariance is an essential property to transfer the issue from quantifying the risk of a random loss to quantifying the risk of the corresponding distribution function, which means 7. law invariance:"
"it is also well known as average-var, tail-var and conditional-var. 10 [cit] discuss multivariate extensions of es. 11 [cit] function, in fact, the exvar is a quantile of a transformation of the originally distribution. in addition, the exvar has the implicit representation"
"explicit spilling conversely, some variables have an inherently long lifespan and will inevitably be spilled. to help the c compiler perform register allocation, we explicitely spill such variables by storing them in arrays. on des, those variables are typically the outputs of each round, which are potentially not used until the call to the 8 th s-box of the next round. this reduces the register pressure, and allows the c compiler to find a better register allocation (with less spilling) for the variables exhibiting a shorter lifespan. table 1d shows the performance impact of explicitly spilling variables on des. this optimization heavily depends on the c scheduler and register allocator, which leads to a speedup varying from 0 to 20% depending on the compiler. the assembly code for des generated by clang (resp. icc) contains 1222 (resp. 1413) fewer move instructions when explicitly spilling some variables, which is directly reflected by the speedup."
"the popularity of var in the financial industry has started with the publication of riskmetrics developed by jp morgan. these days, var is widely used in theory and practice because of its central importance in the basel ii-and basel iii-regulatory, its intuitive interpretation, its easy implementation, its backtestability and its robustness property. on the other hand, there are several drawbacks like the lack of subadditivity and the fact that it completely ignores the severity of losses in the tail of a loss distribution."
"from a semantics standpoint, an usuba program textually embodies a dataflow graph: an unordered set of equations declaratively describe the \"wiring\" from input variables to output variables. for instance, the following usuba programs, which contains the same equations in a different order, are semantically equivalent: this equational approach offers referential transparency: any subterm of the program can be hoisted as a new equation with a fresh variable and the subterm replaced by that variable. the compiler backend eventually produces a valid scheduling, which consists in finding an ordering of the equations for which sequential execution of the instructions preserves the intended semantics. these particularities of the dataflow model make it extremely convenient for implementing and proving the correctness of compilation passes: new equations can be introduced at any point, term can be inlined or hoisted, and the scheduling pass eventually takes care of ordering them so as to generate efficient code."
"(not yet considered in the context of credit risk, with certain deviances in the former analysis). figure 3 again illustrates the percentage risk allocation to the relevant sectors for all five portfolios (i.e., with different levels of concentration) for the var (upper panel) and evar (lower panel). first of all, the visualization of the portfolio concentration is nearly identical for both risk measures. as the graphics for the other risk measures strongly resemble those of figure 3, we excluded them from the paper. referring to the portfolio construction summarized in table 3, the increase of risk due to increasing sector concentration (significant exposure is shifted from commercial services and supplies, and consumer discretionary to the capital goods sector) translates to the bars corresponding to pf1 and pf2. in addition, the differences between the second (third) and the fourth (fifth) bar solely arise from the assumed name concentration. in the case of constant portfolio quality and sector mapping, the importance of the sectors capital goods and transportation decreases, whereas that of consumer discretionary increases when the number of counterparties is reduced from 200 to 100. finally, figure 4 illustrates the different behaviour of expected loss and unexpected loss in the presence of risk concentrations. we contrasted the percentage distributions to the major sectors of the expected sector loss and the risk contribution(s) on the basis of the var (and the evar). per definition, the expected doesn't account for default correlation and would lead to an underestimation and mis-allocation of risk if it is used as allocation scheme. again, no significant deviations between the allocation based on var and evar can be detected."
1. how sensitive is the overall portfolio risk w.r.t. changes of the credit quality across the risk measures under consideration? 2. how sensitive are the risk contributions w.r.t. sector and name concentrations across the risk measures under consideration? 3. are there differences between the risk measures under consideration w.r.t. capital allocation?
"our goal in this testing was to evaluate the accuracy of dir in (a) cross-modality registration (ct-vs-cbct), (b) same-modality registration (cbct-vs-cbct and ct vs ct), and (c) these cbct registrations with different-sized volume of interest (vois)."
"it follows from the definition that all bld risk measures in general are monotone and law invariant. in general, they are neither convex nor subadditive nor positive homogeneous and comonotonic additive. in contrast to the lvar, rmblds are not elicitable. [cit] ."
"we quantitatively validated the accuracy of these findings imply that cbct can be used for adaptive planning, dose tracking, and so on, but only with selected imaging techniques that provide adequate cnr."
"3. sensitivity of capital allocation w.r.t. concentration effects: for reasons of clarity and with respect to the results up to now, the on-going analysis were restricted to var (standard case) and evar"
comparisons of the accuracy of cross-modality dir (dsc and mda) using large and small vois for soft tissue inserts are shown in the dir for small vois was significantly better than that for larger vois in all imaging protocols.
"economic capital (ecap) is a measure of risk, which is the amount of capital that a bank needs to ensure that the company stays solvent given its risk profile. economic capital is calculated internally, sometimes using proprietary models, and is the amount of capital that the firm should have to support any risks that it takes. once having calculated the bank's ecap, a process of capital allocation (ca) is carried out, whereby the ecap requirement is apportioned to different lines of business and sub-portfolios. several competing capital allocation methods reflect the different ways in which individual risks and sub-portfolios contribute to the total capital. there are several streams in the literature, respectively motivated by arguments from: (i) cooperative game theory [cit], (ii) performance and portfolio management (e.g., [cit] ), (iii) market valuation of assets and liabilities (e.g., [cit] ); and (iv) optimization (e.g., [cit] ). [cit] 18 . [cit] define two key properties for a feasible allocation method. the risk contribution should not exceed the stand alone risk and it should not fall below the minimum loss that can occur from this position. for the euler allocation, which is the predominated method for capital allocation in credit portfolios, the two key properties are fulfilled. if a risk measure is continuously differentiable and positive homogeneous, the euler contributions are given by"
"the most popular alternative is the expected shortfall (es), which is defined as the mean of the worst 100 Â· (1 â Î±) per cent of the possible losses"
"the risk contributions for the above-mentioned risk measures are listed in table 2 . for the var and the es, the risk contributions as partial derivatives are well-known as the expectation of the counterparty loss l i conditioned that the overall loss l is equal to the var Î± (l) or greater than or equal to the var Î± (l)."
"we assume that the density f l of l is strictly positive so that a differentiable inverse of the distribution function exists. applying the inverse function theorem, we can apply change of variables"
"the re-weighting function Ï is known as risk aversion function and should satisfy three properties: non-negativity, normalization and non-decreasing. if the properties are satisfied the corresponding risk measure Ï spec is risk-averse and coherent. 7 [cit] shows that spectral risk measures are all coherent risk measures with the additional properties of comonotonic additivity and law invariance. [cit] compare different spectral risk measures for several credit portfolios."
"in a narrow sense 1 and roughly speaking, credit risk comprises the risk of losses from a counterparty's default, i.e., by failing to repay principal, interest or provision in a timely manner. more formally and assuming that the credit portfolio consists of n counterparties, the total (or overall) portfolio loss l p reads as"
"if the confidence level is high and losses from the tail of the distribution are of special interest, importance sampling (is) might came to application in order to increase the speed of the calculation. with is, the future economic scenarios are not generated randomly, but the \"bad\" scenarios have a higher chance of being selected than the \"good\" [cit] ."
"this section presents usuba's compiler, usubac, and the generation of c from usuba. usuba programs are first simplified to a proper subset of usuba (section 3.1). several optimizations are then applied on the program (section 4), before generating c code (section 3.2)."
"in this paper, we provide a solution to the interoperability problem of using different xml-based metadata standards that allows to integrate semantic infor-mation in a vss. we introduce the application of semantic web technologies to deal with the above-mentioned problems. some metadata formats used in video surveillance already have a formal representation described using the web ontology language (owl) [cit], designed by the w3c web ontology working group. in this paper we will discuss such existing metadata ontologies and create new ones. additionally, we create a global ontology expressed in owl, specific for video surveillance systems, called the vss ontology. this ontology represents all relevant information and acts as a uniform interface for the end-operator. the metadata ontologies are linked to the vss ontology using rules and mappings, resulting in a layered metadata model. this model is consequently integrated in a semantic vss to show the benefits."
"the clinical promise of ct perfusion imaging of the liver has largely been unfulfilled, for two major reasons: the increased radiation dose and unclear clinical benefit. in our paper, we address both of these issues. we developed a simplified model of tumor blood supply that can be applied to standard triphasic scans, thus reducing the radiation dose relative to traditional perfusion imaging. we then show that blood volumes estimated from triphasic scans improve the specificity for diagnosing malignancy, compared to traditional criteria such as washout. improved specificity may be especially helpful in noncirrhotic livers, where imaging findings are currently considered insufficient for diagnosis of hcc, and biopsy is recommended [cit] . to our knowledge, this is the first time that liver perfusion parameters have been used to distinguish benign and malignant liver lesions."
"in the context of video surveillance, different metadata formats or standards have been proposed using the extensible markup language (xml) as underlying language. xml allows to structure data according to an xml schema (following the xml schema language [cit] ). the latter defines terms and constructs to represent the metadata and states the structure of the metadata. in the next section, we will show indeed that a number of different approaches exist in expressing the metadata associated with a video surveillance system, both in research initiatives as in actual surveillance installations. however, there is not one global metadata standard that is generally accepted for video surveillance, and most likely such a standard will not be introduced in the near future. consequently, different modules describe their information according to different metadata formats. as a result, combining different metadata schemes with each other seems to be the only solution to create interoperability between different modules and systems. however, the standards generally use different xml constructs to denote the same concept. as such, it can be hard to find similarities between annotations using these different standards."
"in the petsd2tec2 sequence, a first object (car) appears from frame 275 and leaves the scene at frame 403. both analysis modules have no problems in detecting this object, which is shown on the first row of fig. 7 . consequently, the corresponding bounding boxes are all available at the server side. when triggering the rule we see that the object is correctly tracked during the entire time. a second object appears at frame 562, this object is a walking person. the pixel-based detection technique has more problems in detecting this object due to fragmentation. this results in two bounding boxes, one for the upper and lower part of the body, respectively. this means that the system would wrongly think that two moving objects are present in the scene at that moment. the compressed-domain technique, being a more coarse detection, succeeds in detecting more parts of this object. as such, only one bounding box is represented in the cvml format, which more closely conforms to the moving object. this can also be seen on the second row of fig. 7 . since, our system integrates the different analysis modules, the detection results of the compresseddomain analysis module are mapped on the vss ontology. as a result, for instance in frame 586, three bounding boxes will be present as vss instances. the tracking rule will see that these overlap, and that they all refer to the same object. as such, when querying for moving objects, only one object is returned."
"sensitivity and specificity were calculated on a perlesion basis. statistical comparisons were performed using a two-tail, two-sample t test assuming unequal variances. confidence intervals of proportions were calculated assuming a uniform prior distribution [cit] . differences between proportions were evaluated using a two-tail z-test. figure 4 shows good agreement between predicted and measured hepatic vein enhancement. although predicting hepatic vein enhancement does not have clinical significance, the agreement between the prediction and reality provides some support for our simple model of liver perfusion."
"listing 4 shows such a rule to calculate the values of certain properties (we adopt the informal notation declared in swrl (semantic web rule language) to give a human readable form of the rules [cit] ). in cvml a bounding box is described by the coordinates of the centre (xc and yc), the width, and the height. the vss ontology also uses properties xc and yc, but these represent the coordinates of the lower left corner, so the cvml values need to be converted. the rule first looks for an instance in cvml that has values for the centre, width and height (this is an instance of the cvml box class and is stored in variable o1). next it calculates the coordinates of the lower left corner. finally, the new properties in the vss namespace are added to this instance. note that the mapping (as defined above in listing 3) states that an instance of the class box in the cvml ontology is also an instance of the class boundingbox in the vss ontology. so this instance indeed can get the properties xc, yc, width and height which are defined in the vss ontology."
"alternative initiatives to the interoperability problem include the standardization of new exchange formats for video surveillance. for instance, within the international standardization organization (iso) work is ongoing for a video surveillance format by the societal security technical committee (tc 223) [cit] . the work is focused on defining an interchange format for exchanging video, audio and metadata between different surveillance systems [cit] . related to this is the work of the psia (physical security interoperability alliance). 2 yet another initiative in the industry shows the relevance of the interoperability problem. onvif (open network video interface forum) 3 is an open industry forum existing of large companies in the network video domain like axis, bosh, canon, panasonic, and sony. they have defined a core specification to realize a fully interoperable network video implementation comprised of products from different network video vendors, in which they tackle the interoperability problem of video analytics by proposing an xml-based scene description language [cit] . objectvideo has created its own system to integrate different video analytics modules, called ov ready [cit] . this system also uses xml for analytic rules and communications. again, vendors of analytic modules need to adapt their systems to comply with this specification."
"note that even when using one single standard (e.g., mpeg-7) to describe a resource, issues in interoperability can exist due to a lack of precise semantics [cit] . as these examples show, using xml schema is not sufficient. consequently, we propose the use of semantic web technologies to deal with these issues by creating a layered semantic metadata model, discussed in the next section."
"within the metadata service, this xml annotation is converted to rdf triples, again by the x mltordf tool, and stored for future retrieval. the number of triples that need to be stored vary based on the used metadata format. table 1 shows the amount of triples needed to represent the metadata. the first row shows the triples that constitute the ontologies. the layered model includes the vss ontology, the two metadata ontologies, and the mappings. these ontologies are represented using pellet as reasoner. pellet generates different triples automatically, like the fact that properties are sub properties from and equivalent with themselves. the second and third row show the number of triples needed to represent a bounding box in cvml and mpeg-7, respectively. when these triples are added to the layered model, the reasoner creates new triples that take into account the mappings (including the rules). this way, the instances of the cvml and mpeg-7 ontology are mapped to instances of the vss ontology. finally, our vss offers the end-operator the possibility to search for images containing moving objects through a web interface. when this search is requested, the web service constructs a sparql query solely based on vss metadata as shown in listing 10. this query searches for image references (stored in variable z ) that are linked to segments which represent an object. as shown, the query only uses concepts of the vss ontology to retrieve the desired images."
"to address these limitations, we introduce the concept of the hepatic artery and portal vein blood supply coefficients (hac and pvc), which provide a simple way to characterize enhancement curves quantitatively on standard triphasic cts (arterial, portal venous, and delayed phases). we propose using the hac and pvc to classify hypervascular liver lesions and to increase diagnostic accuracy by computer-aided diagnosis."
"a conceptual overview of semvision is shown in fig. 3 . in this paper, we restrict ourselves to a subset of the isyss surveillance system, focusing on one camera monitoring a scene with two analysis modules attached. since the beginnings of digital video surveillance, compression is used to reduce the bandwidth and storage costs. an encoder is used to remove the redundancy in and between the video frames and a compressed video stream or bit stream is created. the analysis of the video stream can occur on a decoded version, or directly on the compressed video stream. in the next section, we will elaborate on these two analysis methods. the results of the analysis modules are represented as (xml-based) metadata and sent through a webservice to the semantic unit. the xml-based metadata is first converted to rdf, explained in section 4.2. the rdf triples are then processed and linked to the vss ontology in the rdf triple store within the semantic unit. section 4.3 will discuss this unit and the used technologies. lastly, end-users can communicate with a second webservice which offers query access to the metadata."
"a mapping ontology typically consists of basic owl and rdfs [cit] constructs (e.g., owl:equivalentclass and rdfs:subpropertyof ) between concepts of different ontologies. listing 3 shows an excerpt of a mapping ontology between the cvml ontology and the vss ontology. the mapping ontology links properties of the different ontologies to each other through the rdfs:subpropertyof constructs (lines 6 and 10). the listing also shows how the standard owl constructs are used to map a cvml box class on the conceptually equivalent vss boundingbox class using owl:equivalentclass (line 15)."
"the hac and pvc are equal to hepatic artery and portal vein blood volumes, in a simple perfusion model that assumes rapid blood flow (no delay between enhancement of the supplying vessel and the tissue) and no vascular permeability to contrast (fig. 2) . blood volumes or coefficients are the measured enhancement curves of the aorta and portal vein are plotted using thick lines. other enhancement curves (thin lines) are labeled with their calculated coefficients expressed in units of blood volume in a voxel (ml) divided by total volume of the voxel (ml) or as a percentage. lesions with high hac will show washout, compared to the background liver, which has high pvc. delayed enhancement (green line in fig. 1 ) cannot be explained by a model that assumes rapid flow and no leakage of contrast from vessels. however, since the enhancement increases in the delayed phase, whereas both the aorta and portal vein have decreased enhancement in the delayed phase, the green line has a negative calculated hac and pvc. in the case of delayed enhancement, the calculated hac and pvc describe the shape of the enhancement curve but do not correspond to actual blood volumes."
"the hac and pvc describe the enhancement curve of a liver lesion (which only contain 3 points in a triphasic scan) as a linear combination of the aortic and portal venous enhancement curves (fig. 1) . for example, an enhancement curve that has the same shape as the aortic enhancement curve, but only half the amount of enhancement, is considered to have 50 % hac. an enhancement curve that is halfway between the enhancement curves of the aorta and portal vein is considered to have 50 % hac and 50 % pvc. thus, hac indicates similarity of a lesion's enhancement curve to the aortic enhancement curve, and pvc indicates similarity of a lesion's enhancement curve to the portal venous enhancement curve."
"when the analysis modules send new metadata to the web service, these are added to the rdf triple store in the jena platform and, if appropriate, rules are triggered. all metadata is now present in the rdf triple store so standard approaches can be used for querying this information. for this purpose sparql is used to perform the queries on the metadata [cit] . finally, a web interface is provided to the end-operator that offers specific methods for querying the vss. internally these methods use sparql queries which are resolved by the jena framework. the results of these queries are then interpreted by the web service and presented to the end-operator in a suitable way."
"the first analysis module uses mpeg-7 to describe the detected objects. an example of such a metadata instance is shown in listing 8. as mentioned before, mpeg-7 is a complex metadata standard that is used in different domains. therefore, the semantic element and spatial decomposition element need to be combined to denote that a bounding box represents an object (in this case a person). when this mpeg-7 xml-based annotation is uploaded to the system, rdf triples are created that correspond to the mpeg-7 ontology using the x mltordf tool."
"hac and pvc calculated from pretreatment triphasic ct scans can be used to classify hypervascular liver lesions. hcc and hypervascular metastases, on average, both had increased hacs compared to the background liver, in agreement with prior results [cit] . compared to hcc, benign lesions, on average, had either greater pvcs (fnh or thad) or greater hacs (hemangioma)."
"the model captures important aspects of liver physiology. as expected, cirrhosis decreases the pvc, which corresponds to the reduced portal flow associated with cirrhosis. portal vein occlusion results in increased hac and pvc in the affected portion of the liver. we can speculate that this is analogous to the compensatory increased cerebral blood volume caused by vasodilation in an acute ischemic stroke [cit] ."
"the diagnosis was based on the radiology reports (including mri and follow-up studies) and pathology (if available). many of the lesions did not have pathologic confirmation. this allowed us to analyze lesions with classic imaging features that are not biopsied, as well as borderline lesions that are followed by imaging, but never biopsied. regenerative nodules that did not have pathologic confirmation had to meet all of the following criteria: resolution on follow-up imaging or stability for at least 250 days, normal afp, and no suspicious branches of the hepatic artery are assumed to have the same enhancement curve as the aorta. the enhancement of the liver parenchyma is due to small vessels that are below the resolution of ct. normal liver parenchyma contains both portal vein and hepatic artery branches, so the enhancement will be a linear combination of the portal vein and hepatic artery enhancement curves. a hypervascular tumor will contain a greater proportion of hepatic artery branches, and the enhancement curve will more closely follow the hepatic artery enhancement curve features on mri (if available). evaluation for cirrhosis was based on pathology (if available) or based on the ct appearance of the liver."
"note that, for practical implementations, a mapping ontology as presented above is not sufficient. rules are needed to create advanced conditional relationships, for example to declare instance equivalence when certain properties match, or to calculate new values for certain elements. within a vss the actual instances of the data (e.g., information on a specific image, or a detected person) are not known beforehand. hence, we cannot define relations on them in the pre-determined mapping ontologies. however, by defining rules, new instances can automatically be linked to those that are stored within the system. as such, a dynamic mapping is created since the rules are triggered when new (instance) data becomes available."
"finally, the liver deforms with breathing, making it difficult to align exactly across phases. we aligned the phases using translation in three dimensions, but more sophisticated nonrigid alignment algorithms are available [cit] . furthermore, in heterogeneous lesions, we measured the most arterially enhancing portion of the lesion, but the exact choice of roi is still somewhat arbitrary. both of these factors introduce uncertainty into the calculated coefficients."
"the rule, shown in listing 5, searches for an instance (stored in variable segment) that is linked through the mpeg7:regionlocator property to another instance (which is consequently stored in variable box). if this segment is related to an instance of the mpeg-7 person class (stored in variable o1) through the mpeg7:semantics and mpeg7:agent object properties, the rule infers that box is the subject and o1 is the object of the property represents from the vss ontology (line 5)."
"for this test we use the petsd2tec2 sequence (with a resolution of 384x288) provided by ibm research [cit] . the sequence shows an outdoor scene with moving objects of different sizes and speeds (people and vehicles). moreover, this is a challenging sequence since it holds gradual illumination changes, shadows, stopped objects, noise and a detailed background. in the context of tracking, we can also distinguish some interesting problems, like the presence of groups of people and a number of occlusions of object to object and scene to object."
"more advanced tracking algorithms are needed to deal with the problem of merge and splits, occlusions, ambiguities and so on [cit] . most of these tracking algorithms rely on an initial object detection that yields an object's shape, size, position and descriptive features like color or texture. when an analysis module delivers this information using metadata our system allows to provide this information as instances of the vss ontology. as such, if complex tracking algorithms are needed, software can be written that takes the rdf triples as input."
"currently, the vss ontology contains a restricted set of concepts for video surveillance, but it can easily be extended since it is created in owl. as such, in future work, other relevant aspects can be added like the usage of different cameras and viewpoints, means to describe behavior or contextual information like prohibited areas and so on. [cit], respectively. after a visiting scholarship at the university of arizona tucson, he returned to ghent university, where he became a professor of multimedia systems and applications, and head of the multimedia lab. his current research interests include multimedia content delivery, presentation and archiving, coding and description of multimedia data, content adaptation, and interactive mobile multimedia applications."
"thus, relative enhancement criteria (such as washout) and hac and pvc provide complementary information about liver lesions. washout considers the enhancement of both the lesion as well as the surrounding liver, whereas the hac and pvc consider only the enhancement of the lesion itself. it would be interesting to examine hac and pvc relative to the surrounding liver, but these would be technically challenging to measure accurately, because subtracting noisy coefficients would double the variance (assuming uncorrelated noise). we developed an online calculator for liver lesion classification based on relative enhancement, hac, and pvc: http:// www.claripacs.com/calc/liver.html (fig. 7) . note that this classifier should not be applied to cysts, hypovascular metastases, or cholangiocarcinoma, which were not included in the training set."
"the institutional review board approved this retrospective hipaa compliant study; informed patient consent was waived. a diverse set of liver lesions with pretreatment triphasic ct scans was obtained by searching for diagnoses that can be hypervascular. a database of radiology reports (impression section only) was searched for the following terms: focal nodular hyperplasia, transient hepatic attenuation difference, arterioportal shunt, adenoma, hypervascular metastasis, hemangioma, and regenerative nodule. in addition, a database search was performed to identify hccs that were t r e a t e d b y r e s e c t i o n, t r a n s p l a n t, t r a n s a r t e r i a l chemoembolization (tace), or radiofrequency ablation (rfa). the two largest lesions on the pretreatment scan were examined."
"a fourth limitation is that the hac and pvc are only equal to the hepatic artery and portal vein blood volumes if there is rapid blood flow and no leakage of contrast from the vessel. these assumptions are not valid when there is delayed enhancement. delayed enhancement results in calculated blood volumes less than zero, which are not physiologic. accurate blood volumes are difficult to obtain from the limited information in a triphasic scan. however, our estimated blood volumes were useful for classifying liver lesions, even if they do not quantitatively match the actual blood volumes. a more sophisticated model would require more contrast phases and likely a higher radiation dose. however, a previous study showed that even with 60 time points, the enhancement curves only had sufficient information to fit a three-parameter model, and a five-parameter model was underdetermined [cit] . in this paper, we had 3 time points and used a two-parameter model (hvc and pvc). given the limited information in the enhancement curves, liver perfusion models all make a variety of different assumptions and do not quantitatively match experimental measurements [cit] ."
"besides the embedded analysis that occurs on the camera, surveillance systems can have independent analysis modules that analyze the video sequences after compression. consequently, if one wants to analyze the captured images, a decoding step is needed before pixel-based algorithms can be applied. to avoid the decoding step and to reuse the work done during the encoding, the literature holds several efforts to perform the analytics directly upon the compressed video stream. in this case, the compressed video stream is analyzed and the specific coding constructs that are available in the stream are the main information sources. since the compressed video is a more compact representation of the original video stream, analytics working in the compressed domain can be faster than the pixel-domain approaches. moreover, it is not necessary to fully decode the video stream before the analysis can be done, resulting in additional gains in time."
"this approach can already be found in existing video surveillance systems (e.g., the candela project [cit] that uses mpeg-7 [cit] to describe the features). regarding to the used metadata standard many options are available. next, we elaborate on related work using different metadata formats to describe video surveillance related metadata."
"within an intelligent vss, one or more video analytics modules are used that analyze the captured images to retrieve relevant information in an automated manner. when looking at video analytics systems, we distinguish two main approaches. a first one analyzes the original video sequences on a pixel-level. in a second approach, the analysis happens in the compressed domain, meaning that the sequences are first encoded with a video codec and the compressed bit stream is analyzed. in most cases a first form of processing is applied on the camera, including contrast enhancement, noise reduction, etc. in case of a smart camera the processing also includes video analytics, like motion detection. this processing can be used to reduce the amount of data that is sent from the camera. such cameras will, for example, only produce video streams if a certain amount of motion is detected in the scene. since the analysis on the camera can occur on the original captured sequences, we can apply typical pixel-based video analysis methods. a pixel-based moving object detection technique, presented in our previous work [cit], is integrated in our vss. this technique, called esmm is a multi-modal background subtraction system using spatial and temporal information. it detects those pixels that are likely to correspond with moving objects (like people and vehicles) in the sequence. to represent the detection results we have extended the system to output mpeg-7 descriptors."
"lesions were also classified as malignant if their hac and pvc fell within ranges that were similar to other malignant lesions (2d linear classifier). the ranges were chosen to maximize the sum of the sensitivity and specificity [cit], using a hill climbing optimization algorithm."
"hac and pvc can be displayed on grayscale or color images (figs. 3 and 6 ). each type of lesion had a characteristic appearance on color images. notice that the aorta is bright red and the portal vein is bright blue. normal liver is light blue, indicating that the enhancement curve follows the portal vein but is not as intense. hypervascular liver lesions are typically purple, indicating that the shape of the enhancement curve is in between that of the aorta and portal vein. fat and muscle are gray, indicating minimal enhancement. the artifactual color near edges is due to misalignment among the three phases."
"there are several limitations to this study. first, the study population was selected to include a diverse set of liver lesions. the frequency of different types of lesions may be different in a different population, which could change the sensitivity, specificity, and pretest probability. for example, the pretest probability of hcc is higher in a cirrhotic liver (resulting in the recommendation that imaging alone can only be used to diagnose hcc in cirrhotic livers [cit] ). in this paper, we included both cirrhotic and non-cirrhotic livers. second, we only examined lesion characterization, and not lesion detection, which will result in an overestimation of the sensitivity."
"calculated hac and pvc for each pixel in the liver were then displayed on both grayscale and color images ( fig. 3 and appendix). the three color channels in the color image summarize the information from the three post-contrast images. color images were generated using a custom web application written in php, javascript, and c++. seven consecutive triphasic liver scans that showed no focal liver lesions and had clearly visualized hepatic veins were evaluated. for each scan, the hac and pvc of the liver parenchyma were used to calculate the expected enhancement curve of the right hepatic vein:"
"another advantage of the system is that the use of rules allows dynamic adaptation of the behaviour of the system. when the ontology, mappings, or rules are changed, the system's behaviour will change also just by reloading these files. this allows to create different modes of operations for our semvision system. for instance, the rules used above will detect a moving object as soon as one of the analysis modules outputs a bounding box. the tracking rules are consequently used to filter out the objects and determine the actual number of moving objects in the scene. if one of the analysis modules suffers from excessive false positives, this might result in many false alarms. hence, in some cases it might be desirable to restrict the number of false alarms."
"hepatic artery and portal vein coefficients hac and pvc of non-cirrhotic liver, cirrhotic liver, and various liver lesions are shown in fig. 5 and table 2 . there was significant overlap between different liver lesions. however, malignant lesions tended to have perfusion characteristics that fell within the gray rectangle, while benign lesions had a wider range of perfusion characteristics and mostly fell outside the gray rectangle. classification of liver lesions on the basis of perfusion characteristics is described in the next section."
"next to the calculation or conversion of actual values of elements in the ontologies, rules are also needed to relate certain constructs in different ontologies. for example, to denote in the mpeg-7 ontology that a bounding box represents a person, several properties are needed. mpeg-7 is a multimedia metadata standard targeted for different domains. as such a bounding box, or a region in an image can be used to represent different things. if one wants to use mpeg-7 for video surveillance, more specific to denote that an object is detected, the semantics of the bounding box need to be described, resulting in additional properties. in contrast, only one property is needed to describe this in the vss ontology. in owl it is not possible to state that one property is equal to a cascade of other properties, so a rule is needed to convert such information. such a rule first looks for the appearance of a combination of properties according to one ontology and then creates new properties in the vss ontology."
"the appearance of the liver on various phases of imaging is thought to be related to the liver's dual blood supply. the normal liver receives about 70 % of its blood flow from the portal vein and 30 % from the hepatic artery [cit] . hcc receives most of its blood flow from the hepatic artery [cit], which enhances and de-enhances before the portal vein does. washout is presumably related to rapid displacement of highly enhanced blood with less enhanced blood in tissue that is predominantly supplied by the hepatic artery."
"since the proposed tracking rule is very simple, it has some obvious shortcomings. if the bounding boxes of two different objects overlap, the objects will be regarded as being equal. indeed, our system succeeds in tracking every object in the sequence correctly, until frame 866. at that point, the detection of two different objects overlap and our rule wrongly regards them as equal. however, this is a problem that also influences the analysis modules. the compressed-domain detection will only output one bounding box when two different objects approach each other due to the coarse detection. additionally, once the objects overlap in the field of view of the camera, both the compressed and the pixel-based technique output only one bounding box. this is because the analysis techniques do not focus on the accurate tracking of each object, but on delivering fast and accurate detection of moving objects for each frame."
"as discussed before, a moving object can be detected by several analysis modules, possibly from different vendors. it is not feasible that all analysis modules have knowledge on the detected objects in other analysis modules. hence, each module separately creates metadata for the detected object and both cvml and mpeg-7 representations of the object are entered in the rdf triple store. following the rules and mappings, shown in section 3.4, this metadata is automatically mapped upon the vss ontology, creating instances of the vss object class. however, there should only be one instance of this class to represent the detected object."
"hac and pvc were calculated from the pretreatment triphasic ct (arterial, portal venous, and delayed phases). the three phases were manually aligned by translation in the x, y, and z directions. for each phase, regions of interest (rois) were drawn over the liver, aorta (at the level of the celiac artery), and portal vein (near the bifurcation). for heterogeneous lesions, measurements were made in the most arterially enhancing portion of the lesion and in the same location on the other phases. rois were elliptical, with a length of at least 5 mm. rois were also drawn over the background liver. large intrahepatic vessels were avoided when drawing the rois in the liver. the liver enhancement curves were expressed as a linear combination of the hepatic artery and portal venous enhancement curves. specifically, for each pixel in the liver:"
"in traditional surveillance systems, an analysis module usually does both the detection and tracking of moving objects. information on the trajectories of moving objects can be interesting to an end-operator, so, accordingly, some metadata standards have provided constructs to represent these. for example, in cvml the tracking is represented by giving a unique identifier to an object. this way, all objects with the same identifier are conceptually the same, so the bounding boxes in the different frames can be found. in the cvml ontology, we have chosen to represent the identifier as a property that is an owl:inversefunctionalproperty. this is a standard construct in owl, stating that if two instances have the same value for this property, they are considered to be equal. this way, if two instances of the cvml object class have the same identifier, they are automatically set to be equal by the reasoner. so, regardless whether the tracking occurs by the analysis module (in software) or by the reasoning engine (through rules), the result is that only one instance of the vss object class will represent the tracked object."
"generally, when defining an interchange format, a metadata scheme is used that determines the structure of the format. different languages can be used for this interchange format like plain text, binary codes, or a shared database is used in which the metadata is stored [cit] ."
"the color images in fig. 6 summarize the information in the three phases of the scan, potentially allowing for a quicker assessment of multiple time points. each type of liver lesion had a characteristic appearance on the color and grayscale images showing hac and pvc. these color images encode three numbers per pixel (hac, pvc, and average hounsfield units) and thus have a higher information content than prior methods for colorizing multiphasic scans, which encode a single number per pixel [cit] . the role of these color images in lesion detection or characterization should be examined in future studies."
"semantic web technologies allow to alleviate the interoperability issues within one metadata standard. for example, efforts have been undertaken to translate mpeg-7 into an owl ontology and to enable its integration with other ontologies through appropriate frameworks, thus enhancing interoperability [cit] . in the same way it is possible to express each metadata standard that is used for video surveillance as an owl ontology. these ontologies, called metadata ontologies, allow to structure the data and incorporate the semantic meaning of and relations between the different elements of the metadata standard. such metadata ontologies form the first part of our metadata model."
"the way semvision is built gives some advantages compared to traditional approaches. firstly, since we represent the metadata (including the ontologies and mappings) in owl, we can make use of the existing semantic web technologies to perform reasoning or to create rules. some practical examples are given in the next section. secondly, our system allows to combine metadata formatted according to different metadata standards. in section 5.2, a practical use case scenario is given that shows how the semvision system integrates different metadata formats to detect and track moving objects. lastly, in section 5.3 we show how the system can be reconfigured by performing simple adaptations of the ontologies and rules."
"the main contribution of this paper is the introduction of semantic web technologies for the creation of a layered metadata model to augment the capacities of video surveillance systems. the layered metadata model has been created to deal with current interoperability problems induced by the application of different metadata formats in the various modules of current video surveillance systems. an upper layer consists of an ontology specifically created for video surveillance systems and includes technical and analytics metadata. this ontology is linked to a lower layer containing a pool of metadata ontologies, commonly used in surveillance. we introduced the application of semantic web technologies consisting of mapping ontologies and inference rules to integrate the different ontologies in the layered metadata model. additionally, we presented a video surveillance system that integrates this metadata model. to show the advantages of our approach, an object tracking system has been created with rules inherent to the semantic web technologies. lastly, we have shown that the system can deal with the information management of multiple analytics modules each using different metadata standards."
"when trying to match the xml schemas of different standards we face interoperability problems. these problems were already signaled by the w3c multimedia semantics incubator group in which the authors of this paper have actively participated [cit] . although each of the standardized formats introduces interoperability amongst applications that use that standardized metadata scheme, issues occur when using different metadata schemes together. listing 1 shows an xml fragment that describes the event of a detected person using cvml constructs [cit] . the orientation and position are denoted and high level information about the specific action of the person is described. similarly, listing 2 shows an xml description of a detected person using the visual surveillance xml schema (vs7) [cit] . this fragment holds metadata on the captured video sequence and includes temporal and spatial information on a bounding box that represents the detected person."
"by using the same principle, we can introduce a tracking system that finds identical objects in consecutive frames. an example of a very simple tracker can be found in listing 7. this rule states that if two bounding boxes largely overlap in consecutive frames, they denote the same object."
"to summarize, in our implementation we analyze the same video sequence with two distinct algorithms. the algorithms are implemented in c++ and detect moving objects in video surveillance sequences. information on these objects (e.g., bounding boxes for spatial location) are represented using different xml-based metadata standards (mpeg-7 for the pixel-based approach and cvml for the compresseddomain approach) to show the interoperability of our system."
"an additional disadvantage of xml is that it does not allow to explicitly define the semantics of the concepts that are described. traditional metadata standards present an xml schema to define the structure and fields that can be used, and supply a textual description of the meaning of the different concepts. as such, the metadata is machine-readable but the semantics of the metadata fields are not."
"as can be seen, the rule support is a powerful feature, moreover entering a new rule does not require to reboot the system. we would like to note that in many cases"
"as this overview shows, most related work focuses on the usage of one single metadata format within a vss. however, with the growing size and modularity of these vsss, the possibility to include and work with different metadata formats is a prerequisite for the creation of a practically useful vss. a number of approaches exist that focus on the integration of different metadata formats or analysis modules."
"the use of different xml schemes has been noted by different sources. for example, cagle [cit] described this issue for the description of an invoice. he suggests to use some guidelines in the creation of new xml schemas, like the re-use of components of other schemas. additionally, he advocates that a specific xml schema manager needs to be employed for the creation and management of the xml schemas, which might be very costly."
"hac and pvc calculated from triphasic liver ct examinations can be used to classify hypervascular liver lesions. these coefficients improve the specificity for diagnosing malignancy in liver lesions, when combined with traditional relative enhancement criteria (such as washout)."
"since the ontologies and rules are read by the system, these can be seen as configuration files, avoiding to recompile the system. the introduction of the changes above will not affect the normal working of the system. lastly, a new query can be introduced that searches only for objects with high reliability. as a result, if the pixelbased technique generates false positives, while the compressed-domain approach does not, it will not disturb our detection results anymore."
"to solve interoperability issues inherent to the use of several different metadata schemes, the ideal scenario would be to create one commonly accepted (metadata) ontology that encompasses all relevant concepts and that would be used in every module of the vss. however, this is not feasible in practice as can be seen by the plethora of existing metadata standards. different standards are used, whether they are small and simple (cvml) or broad and complex (mpeg-7). conceptually, these metadata formats are on the same level, i.e. they all describe content. consequently, we regard each metadata format as equally important and will handle the ontologies representing them as such."
"the red, green, and blue values for each pixel in the color images (ranging from 0 to 1) were calculated from the hac, pvc, and hounsfield units (averaged over the arterial, portal venous, and delayed phases), using the following pseudocode. the result is the color scale shown in fig. 3 ."
"the second analysis module uses cvml to describe the moving objects. note that the specification of cvml explicitly defines a bounding box to represent a moving object. as such, it does not need additional constructs like the mpeg-7 example to denote that the box corresponds to a detected person. hence, the xml annotation in cvml is much simpler (listing 9)."
"as the previous example suggests, to make metadata practically usable for information exchange between two or more modules, a common machine-readable metadata format is needed. this format describes which metadata can be used to describe the information of interest and how the metadata is structured. when using a common metadata format, software tools for automated manipulation can be created. one popular format for metadata is xml, which allows to structure the information so that it is machine-readable."
"in this section, we present a semvision, our vss that uses the layered semantic metadata model proposed in this section. semvision was developed in context of the isyss project (intelligent systems for security and safety) [cit], which focuses on managing a multitude of surveillance video streams (originating from regular surveillance cameras and mobile cameras on land and airborne vehicles). firstly, we briefly elaborate on the analysis modules that are used in our system. next, we show how the metadata in rdf format can be obtained. lastly, we show how the semantic metadata model is integrated in the system."
"note that, some metadata formats can be in fact regarded as an ontology (defined as a formal representation of a set of concepts and their relationships in a specific domain). however, in this paper we will use the term ontology to refer to an ontology in the context of the semantic web, more specifically, meaning expressed in owl."
"to conclude, different metadata standards are used within video surveillance and with the increasing demand for large scale systems integration becomes more and more important. in this paper we want to provide means to integrate video analysis modules that use xml-based metadata formats to describe their results. in the next section, we first give more details about these interoperability problems with some concrete examples. next, a semantic layered metadata model is presented tailored for use in a vss."
"note that the querying occurs with respect to the vss ontology, and not to the metadata ontologies. this prevents that the sparql queries have to incorporate knowledge about each metadata format that is used. since all metadata ontologies in the system are linked to the vss ontology, we can retrieve information by only querying the latter. as such, when a new analysis module is incorporated in the system, using a different metadata standard, it is enough to provide the mappings to the vss ontology to make it fully integrated with the semantic metadata model."
"video surveillance is proliferating worldwide, and, recently, distributed multi-camera surveillance systems have gained popularity. currently there is a shift towards ipbased video surveillance, which connects the cameras directly to the network and provides more scalability and easier integration. typical surveillance systems start with the detection and segmentation of objects of interest in images captured by each camera. the output of such object detection systems are pixel-wise segmentations of the image in foreground and background regions. additional information that can be extracted from the images are the object sizes, colors, speeds, or other features for distinguishing or classifying these objects. typically, the next step is tracking of the objects and classification. this information forms the input for high-level analysis modules to make intelligent decisions on events and object behaviours. the extracted information (both low and high level) is generally called metadata and it has applications in a broad range of domains within computer science. when using a distributed video surveillance system (vss), a specific format is needed to represent this metadata so that it can be exchanged or stored in a central repository [cit] . however, with the increasing complexity of large-scale distributed surveillance systems it becomes more and more difficult to use the same format in all modules of the system. additionally, when we envision that cameras and analytics modules of different vendors will be combined in video surveillance systems, the problem of interchanging the metadata becomes even more clear [cit] ."
"since the metadata is represented using semantic web technologies, we can take advantage of the standard rule support to deduce that different detected objects are conceptually the same. the rule shown in listing 6 states that if two bounding boxes in a frame largely overlap, they represent the same object. as a result, the instances of the vss object class are now considered to be equal but represented by several bounding boxes."
"in this diverse set of liver lesions, the most suspicious relative enhancement pattern was hypervascularity with washout (table 3), which was seen in hcc, hypervascular metastases, and some benign lesions. the second most suspicious relative enhancement pattern was hypodensity on all phases, which was seen in hcc and regenerative nodules (hepatic cysts were rectangle were classified as malignant. the size and position of the gray rectangle were chosen to maximize the sum of the sensitivity and specificity of the classification not included in this study). using only the most suspicious relative enhancement pattern (hypervascularity with washout) to diagnose malignant lesions resulted in a sensitivity of 72 % and specificity of 81 %. using the top two most suspicious relative enhancement patterns (hypervascularity with washout or hypodensity on all phases) to diagnose malignancy resulted in a sensitivity of 89 % and specificity of 72 % (the improved sensitivity was statistically significant, but the decreased specificity was not)."
"a third limitation is that we only examined hac, pvc, and relative enhancement characteristics. several other imaging features can be used to distinguish hcc from other liver lesions: portal vein invasion, abnormal internal vessels [cit], heterogeneous enhancement [cit], large size [cit], hyperintensity on t2 mri sequences, and a pseudocapsule with delayed enhancement [cit] all favor a diagnosis of hcc. our findings should be confirmed with a prospective study, where the hac and pvc are considered in the context of all of the other imaging and clinical data."
"a traditional xml-based vss would have to create queries that interpret all the xml metadata formats. so for each format, a specific query has to be made that follows the structure of the standard. in our case, only one query is needed since all information is linked together through the use of the semantic metadata model. if an integrated system is used, like the s3 system by ibm [cit], all metadata is available in the same format, namely the proprietary xml-based format that they use. the major disadvantage is that the different analysis modules need to implement a specific interface which introduces additional efforts and costs. the use of one format (in this case xml) does allow to search through the results of the different analysis modules. however, again specific software needs to be written for this purpose, in contrast to semantic web technologies for which tools to reason and query exist. additionally, it has been shown that semantic web technologies can enrich the search functionalities. for instance, in the vss ontology, a vehicle is modeled as a subclass of an object, likewise extended hierarchies can be created modeling different types of vehicles. if an analysis module has the capabilities to classify an objected object, it might for instance detect a yellow van and represent this in some metadata format. when this information is represented as rdf in our system, this can be linked to the appropriate class (and automatically to all super classes), so that a search query for all moving objects will also retrieve the van. when using xml, software needs to be written that explicitly translates a query for moving objects into a query for moving vehicles and a query for moving vans."
"a variety of benign and malignant liver lesions can be characterized using contrast-enhanced ct. the optimal times for imaging the liver are during the late-arterial phase (35 s after contrast injection), when hypervascular liver lesions tend to have the greatest enhancement relative to background liver, the portal venous phase (60-70 s), when hypovascular liver metastases and the portal veins are best visualized, and the delayed (or equilibrium) phase (3-5 minutes), when washout or contrast retention can be best characterized. the non-enhanced phase provides minimal additional diagnostic information for liver lesions [cit], and to reduce the radiation dose, it is no longer part of our institution's multiphasic liver ct protocol."
"the next section lists related work that discusses metadata in surveillance systems. in section 3.1 we discuss the interoperability issues that occur when trying to incorporate existing metadata schemes with each other. accordingly, in section 3.2 the layered approach is presented that builds upon and combines formal representations of existing metadata schemes. section 3.3 elaborates on the ontologies that represent the metadata standards and section 3.4 shows the mappings and rules between the different ontologies. section 4 presents a surveillance system, called semvision, that is built around our model and discusses the used technologies. to show the benefits of our approach, section 5.1 discusses the semantic reasoning by using rules. additionally, a use case scenario is shown in section 5.2 to illustrate our system and, finally, conclusions are drawn in section 6."
"since mpeg-7 is a large (and complex) metadata standard, they proposed a video surveillance specific profile to limit the amount of descriptors that need to be supported. additionally, they created a visual surveillance xml schema (vs7) that uses some of the mpeg-7 descriptors and contains new types."
"since the semantic representations of our vss metadata model and the underlying mpeg-7 and cvml standards are linked together, through the ontology mapping and rules as explained in section 3.2, the system retrieves references to all images that contain a moving object (or more precisely, that contain a bounding box assumed to represent a moving object). using the references, the actual pictures are retrieved and shown to the operator. the web service also retrieves the coordinates of the box that represents the object and draws the box on the shown image for better interpretation. our system provides two views of the detection results. firstly, an object-focused view lists all objects detected by the system. for each object, references to the frames in which it appears are given. likewise, a frame-based view lists for each frame the moving objects that are present."
"a visual comparison is given in fig. 5, which explains the behaviour of the techniques. the scene is having a gradual illumination change which influences mgm severely. the technique by shan can better cope with this but it loses some actual foreground pixels (the two persons walking further away from the camera). the esmm technique deals with the illumination change while preserving the detection of all objects in the scene. likewise, the compressed-domain technique is robust against noise and illumination changes, but the detection is a bit coarse."
"here we show how we can take this uncertainty into account in our system. first, we add a property to the vss ontology that describes how certain the system is that an object is present. this can be done by creating a datatypeproperty called reliab ility as a property of the object class. the value of this property gives the percentual probability that the detected object is in fact an actual object. second, a rule is created that gives objects which are represented by several bounding boxes, high reliability, as shown in listing 11."
"we have chosen the generic xml to rdf conversion presented by van [cit] . this uses an xml document as mapping document that defines specific mapping rules between an xml instance document and the resulting rdf document. a generic x mltordf tool takes this mapping document and the used ontology as input and then automatically transforms corresponding xml documents to rdf instances, as shown in fig. 6 . the xml-based mapping document is built from specific elements which form a mapping language and can be interpreted by the x mltordf tool. this language allows to create a simple mapping of xml nodes to corresponding owl classes or properties. conditional mappings are available in case a mapping not always holds. in that case, a condition can be made of xpath (xml path language) or sparql ask expressions. finally, value processing is included which specifies different ways to infer the value of a resulting owl property. these specific language constructs ease the development of such xml to rdf mappings."
"a possible solution would be to extend our tracking rule to state that two bounding boxes represent the same object, only if the dominant color is very similar. this would mean including a datatypeproperty, for instance called color to the segment class in our vss ontology. next we can define a mapping of the mpeg-7 dominant color descriptor to our new property. the tracking rule can then be extended to check whether the color differences are small."
"more advanced solutions can be envisioned. rules could be created stating that small objects are less reliable, or that one analysis module is more sensitive to noise and should have a lower reliability (for instance, it could be included in the mapping rule that objects detected by the pixel-based module have lower reliability)."
"typical scan parameters were as follows: 120-150 ml of iodinated contrast (iopamidol 300-370 mg iodine/ml) was power injected intravenously at a rate of 4-5 ml/s. the liver was imaged at 30-35 s (arterial phase), 60-70 s (portal venous phase), and 180 s (delayed phase) after the start of contrast injection, using a kvp of 100-120 [cit] . the following scanners were used: siemens definition, siemens definition as+, siemens somatom definition, siemens sensation 64, ge discovery ct750 hd, ge lightspeed 16, and ge lightspeed vct. images were reconstructed using a soft tissue kernel, with a slice thickness between 1 and 5 mm."
"the layered metadata model is shown in fig. 2 and consists of the created vss model and the underlying metadata standards. between the different ontologies there are so called mappings (represented as arrows in the figure) which consist of mapping ontologies and inference rules. these define the relations between classes, properties, and instances of involved ontologies, as will be discussed in section 3.4. in this paper, we restrict the layered metadata model to a cvml and mpeg-7 ontology to demonstrate the functionalities. however, additional ontologies can easily be added by providing the appropriate mappings. the way we organize the different metadata schemes allows reasoning on different levels. the usage of a formal representation of different metadata standards in the lower layer allows to easily search across different formats. the upper ontology allows the application of the semantic knowledge to make intelligent decisions on system level (e.g., make thumbnails of image regions with detected persons if the size is larger than a certain value)."
"the different analysis modules create metadata for each frame that is analyzed. for instance, when a moving object is detected, a bounding box is created and represented in the used metadata standard. in semvision, the same moving object can be detected by different analysis modules, so different metadata representations can exist. the detected bounding boxes can differ in size and position, depending on the used analysis algorithm, but the object that they describe is conceptually the same. if an operator wants to retrieve images with a specific moving object, the detected objects should be regarded as one object. so the information expressed in the different metadata standards need to be linked to each other. hence, we can use the layered semantic metadata model presented in section 3.2 to perform the appropriate mappings. for this purpose, a semantic service is created that uses jena 5 as underlying platform. the vss ontology and the ontologies that represent the metadata formats are imported in the jena platform at start-up of the system. for the reasoning we use pellet 6 and rules are described in swrl and interpreted by the jena platform."
"these examples illustrate the issues of interoperability created when using multiple metadata standards. the same concepts are described but in a totally different format. there are mismatches in the names of the xml elements, the structure, and the semantics. mapping such xml fragments on each other is obviously a cumbersome task. the usage of extensible stylesheet language transformations (xslt) stylesheets [cit], which are specifically created to transform xml instances, cannot always encompass the differences between different metadata standards."
"to combine the vss ontology with the metadata ontologies, we create a layered metadata model that combines the different metadata ontologies. different metadata ontologies (representing the different metadata standards) constitute the lower layer and are linked to the vss ontology, which acts as an upper layer. as such, we create a hierarchical system of two layers. the upper layer contains the vss ontology with concepts suited for video surveillance systems. the lower layer exists of several metadata ontologies which can be used by different modules within a vss."
"additionally, xml was mainly created to structure information, so the xml specification holds constructs to structure the data in a tree-like fashion. to describe semantic aspects, like classes, specific instances of these classes, and relationships, xml is lacking some expressive power. indeed, in many cases a metadata standard consists of an xml schema to denote the structure and the metadata fields that can be used, and a complementary textual description of the actual semantics of the metadata fields. to describe the semantic aspects, ontology languages have been created that hold specific constructs to declare classes, instances, and different kinds of relationships."
"the study group included 78 liver lesions in 64 patients (table 1 ). there were 40 hccs, 10 regenerative nodules, 9 transient hepatic attenuation differences (thads) [cit], 7 hemangiomas, 6 hypervascular metastases, 4 fnhs, and 2 adenomas. the thads were secondary to alterations in blood flow from portal vein occlusion (2 cases), hemangioma (1 case), abscess (1 case), around the falciform ligament (1 case), and unknown (4 cases)."
"hac and pvc provide an estimate of the blood volumes in liver lesions. the model of liver perfusion used to calculate these coefficients assumes that enhancement of the liver parenchyma is due to enhancement of small branches of the hepatic artery and portal vein, which follow the enhancement of the aorta and main portal vein, respectively. this assumption is reasonable, given that the liver transit time (approximately 10 s [cit] ) is less than the contrast bolus length (approximately 30 s). furthermore, the model was validated by using it to predict the enhancement curves of the hepatic veins."
"hypervascularity with washout is a key diagnostic criterion for hcc but had a sensitivity of only 72 % and specificity of 81 % for diagnosing malignancy in a diverse set of liver lesions. using the hac and pvc, in addition to the relative enhancement patterns, computer-aided diagnosis resulted in a sensitivity of 76 % and specificity of 97 % for malignancy."
"genetic algorithm (ga) employs the natural biological evolution theory as the concept to solve constrained or unconstrained optimization problems [cit] . the algorithms are inspired by darwin theory of \"the survivor of the fittest\" [cit] . ga is a method where the first-generation individual left for the competition. for every round of competition, the survivor will become the new-generation individual to continue to be in the competition. the process will keep repeating where the survivor becomes the parent of the new generation to pass over the genetic. the survivor is also known as the fittest individual. the competition continues to leave the desired genetic behind until the best fitness individual is obtained [cit] ."
"it can be seen through fig.6 that by using mpso technique for base loading condition before network reconfiguration, voltage angle of each node of the network have higher degree of phase shift which is being prominently controlled and minimized after reconfiguration of the network by applying suggested technique on the system. as mpso is improving the voltage profile and minimum voltage in the system is enhanced up to 0.94234 per unit. similarly, voltage angle is phasor value of the enhanced voltage it is also improved."
"to calculate the optimum radial effective arrangement that lessens the real power losses of the system by maintaining the essential functional constraints is thee basic target of dnrc issue. the real power losses minimization of a radial network (p loss ) is the key objective problem of dnrc, below its operating constraints, i-e, magnitude of system voltage, maximum rated load on a feeder and network radial arrangement. minimal real power losses which is the basic element can be expressed as:"
"the chb-mli switching configuration is similar to the other topology. when s1 and s4 are on and s2 and s3 are off, voltage v an is equal to v dc due to the current flow sequence. when s1 and s2 are on and s3 and s4 are off, the voltage output is 0. when s1 and s2 are off and s3 and s4 are on, the voltage output is 0. when s1 and s4 are off and s2 and s3 are on, the voltage output is -v dc ."
"the pulse generator operation is illustrated in figure 4 . the top-row black signal in the top figure is the triangle repetitive pulse, while the red signal in the top figure is the modulation wave. the bottom-row red signal in the bottom figures shows the pulse generated throughout the comparison. when the modulation signal is higher than the carrier signal, the output signal comparator would show 1. then, when the modulation signal is less than the carrier signal, the output signal comparator would show 0. the generator switching pulse is then injected to a specific switch. the switching affects the output voltage where different switching sequences allow for different voltage-level configurations with different switching frequencies. hence, the voltage levels are able to be switched according to the spwm signal."
the staircase waveform is analyzed to obtain a general equation. the quarter-wave symmetric waveform can be written in a fourier series form shown in eq. (4):
"the simulation is repeated for three-, five-, and seven-level with equal and unequal dc source to test the topology performance and capability of the both algorithms in resolving such"
"she is proposed to eliminate the unwanted harmonics. harmonic distortion occurs due to the conversion of dc to ac. harmonic distortion is able to be resolved by using harmonic filters such as low-pass filter to reduce the high-frequency harmonics. for medium and high voltage or power applications, the first approach and the number of switching angles are limited by switching loss and usually are used when the available voltage steps are limited."
"she is manipulating the switching angle to reduce the harmonic distortion. in multilevel application, every single voltage level has multiple switching angles. the she method provides the best switching angle to decrease the harmonic frequency. the harmonic elimination is depending on the number of voltage step. in inverter with voltage output of three voltage steps, the possible harmonic elimination would be the third and fifth harmonic component only. the general staircase waveform fourier series given in eq. (10) is derivative of the above equation. the mathematical fourier series formulation is shown as the possible method of eliminating the desire of low harmonic component and can be manipulated as selective harmonic elimination:"
"ga method has good prediction of switching angle for three-level chb-mli. but for five-level, the harmonic contained is higher than pso method with the same level. for ga, the unequal dc source handling capability is there but not as good as compared to pso. the result is obviously shown where pso effectively reduces each selected harmonic."
"in this research, only single-phase chb-mli undergoes the optimization using ohsw. hence, implementation of ohsw in three-phase multilevel inverter topology can be done for further detailed analysis. three-phase system has additional switches and configuration which will be challenged on calculation complexity on the calculation of switching timing."
"particle swarm optimization (pso) originates from the natural behavior of a flock of birds, a school of fish, or a swarm of bee. james kennedy and russell eberhart are the pso founders which use optimized nonlinear functions [cit] in this technique. by applying bioinspired algorithm, constraint or unconstraint optimization can be solved efficiently and faster. pso terminology is based on the current moving speed and direction in progress of searching the best personal and group position and in the end is located to the optimal or almost optimal solution [cit] . pso starts with declaration of parameters. the pso algorithm for the selective harmonic elimination parameter is set in table 2 . the process continues with the random generation of particles by initializing their position and velocity. fitness of position and velocity are evaluated to obtain the best fitness, p best, and global best, g best . the process repeat is continuously updating the particles p best and g best until the optimum result is obtained."
there are several advantages of ga as compared to other optimization techniques. ga is more robust as compared to the conventional artificial intelligence. besides that the ga is also not easy to break the error due to the slight input change or reasonable noise. this method also shows advantages of larger state space search optimization technique. the genetic algorithm parameter is set by the creation function in the optimization tool of matlab.
"both the constraint and fitness m-files are applied in the genetic algorithm optimization tool. in this stage, the fitness function section where the fitness file is created needs to be included. the number of variables in the fitness function needs to be clarified. besides that, the boundary needs to be set for each variable to narrow the prediction. the upper bound should be 0, while the lower bound should not exceed Ï/2. the last process applied is the nonlinear constraint function where the constraint m-file is applied at this point."
"in the same way, the particle best initial position is determined and expressed as:, are the opening value of weight and the closing value of weight, respectively; are the maximum iterations number; iter is the current number of iteration;"
each peak voltage is calculated to obtain the switching angle. the equation showed that the voltage of each harmonic can be manipulated. the method to manipulate to remove the other undesired harmonic voltages is the basis of selective harmonic elimination (she) method. eq. (12) showed the general she method calculation. h represents the number of harmonics step [cit] :
"unequal dc source performs better than equal dc source chb-ml. the problem is found to be related to modulation index step. both gas apply modulation step of 0.05 interval where a better solution possibly lies in the step with smaller interval. besides that, the arrangement of the voltage source is in increasing order which is also one of the factors, which smoothens the staircase waveform."
"selective harmonic elimination is a series of nonlinear equations that can be solved to obtain the switching angle for the inverter. due to the multiple possible set of data, the calculation involved is very complicated. in order to resolve the problem, the research makes comparison between particle swarm optimization algorithm and genetic algorithm."
"the ga algorithm starts with the injection of the initial population. the existing instantaneous individual is evaluated by fitness function to determine the best answer. if the fitness in not reaching the desired level, the individual selection will undergo crossover, which mutates new offspring to be tested on its fitness, and the old generation will be disqualified. the process is repeated until the best fitness or optimum result is achieved."
"eq. (7) assumes that x is equal to Ït to simplify the calculation. then, the resultant equation is eq. (8). eq. (8) shows separate dc source formulation where e 1 (e 2 -e 1 ) and (e k -e kÃ 1 ) represents each h-bridge voltage source. there are two conditions in the simulation, that is, the equal dc source and the unequal dc source. the equation above applies for both conditions. however, for equal dc source, the equation is able to be simplified as show in eq. (9)."
"numerous types of pso in last twenty years are suggested for the improvement of the result quality, merging velocity and its consistency improvement. eberhart and kennedy [cit] suggested the pso discrete version by fluctuating the search space into binary. the anticipated modified pso is also another altered form of binary pso, in which alteration in particle velocity in search space is done which narrowed the particle search towards better quality solution. mathematical arrangement is presented in this segment, for the proposed technique."
the sequence is given of the method proposed in fig. 2 . the three key phases to resolve dnrc issue by mpso are: 1. to frame the set of dimensions.
"besides that, implementation of different bioinspired algorithms in comparison mode will be a worthy research. the latest intelligent algorithm such as bat algorithm and bacterial foraging optimization can be applied for the switching timing calculation. as concluded from the research found, different algorithms can lead to different result accuracies. the latest intelligent algorithm is not yet being widely exposed in power electronic fields."
"the growing distribution power system complexity has prolonged due to stressed surroundings in a radial system. rapid development of industrial and residential heavy current load demands on distribution power system are primary fear for the engineers of planning by transitory of each particular day. distribution power losses in pakistan consuming massive volume nearby (17.4%) [cit] . automation of distribution power system is a suitable way out for the decline of these losses, the positioning of the power devices having reactive adjustment, advance optimization methodologies and distribution arrangement by switching settlement is incorporated [cit] . distribution power system is a promising approach as compare to other well operational approaches because it increases the efficiency of the radial power system, stability and the power profile at the consumer's end. by changing the on/off position of tie switches (normally open) and partitioning switches (normally close) distribution network reconfiguration (dnrc) may be explained through regulating the settled configuration of distribution power network [cit] . successive objectives can be achieved by dnrc:"
"the information obtained from the table show that thd percentage reduces as the number of the voltage level increases. this phenomena show that the increase of the voltage level is able to reduce the thd. this also inherits information that the decrement of thd is exponential to the number of levels. hence, the increase of the voltage level to decrease thd is only limited to a certain number of voltage levels in terms of economical and performance aspect."
"the genetic algorithm application method in matlab is done with a series of m-file code. the calculation to resolve the nonlinear equation is done by applying genetic algorithm. calculation process is separated into two parts, that is, constraint and fitness. constraint file functions as the limitation of the switching angle. the switching angle of the wave should not be more than 180 . the constraint also gives a faster guideline, which ensures that the switching angle of the each switch is in the feasible range. for example, the switching angle for Î¸ 2 is greater than Î¸ 1, Î¸ 3 is greater than Î¸ 2,a n d Ï 2 is greater than Î¸ 3 . this step is to make sure that the switching angle calculated is valid to be applied in the switching order which is"
"ohsw technique has the advantage to reduce harmonic distortion. the basic concept is applying selective harmonic elimination pwm and quarter-wave symmetric to eliminate loworder harmonics [cit] . for the topology of ohsw cascaded h-bridge multilevel inverter to operate, switching angle is first calculated using matlab's main interface to obtain the best switching angle. the result obtained is applied to be the pulse generation block generated in simulink environment. other parameters such as modulation signal and carrier signal are added in the block."
"another set of simulation applies the particle swarm optimization method with same fitness function and modulation step. the result is shown in table 6 . the result shows that she by pso is less effective than ga. for three-level and seven-level chb-mli, ga produces lower thd output and then pso type."
the modulation index (m) is set to the range of 0 to 1. the process takes a few seconds to finish depending on the complexity of the function. the switching angle computed according to the modulation index provide and the fitness value is determined. fitness function section for a three-level inverter for unequal dc source includes harmonic elimination on the third harmonic order by applying eqs. (12) and (13) as mentioned before.
"the number of switching angles represents the harmonic component that is required to be eliminated. the fundamental component is the desired voltage, and the rest needs to be eliminated. hence, the number of harmonics needed to be removed is k-1. from the expression of eq. (10), each peak voltage for the nth harmonics is expressed in eq. (11):"
by the separate dc sources is different in the voltage level where the amplitude or voltage of each dc source can be different. there are two variables which may manipulate the harmonic distortion for the inverter. the first variable is the amplitude of the separate dc source where the voltage level of each dc source can be equal or unequal. the second variable is the switching step where the step can be short or long. both variables will affect the result of the total harmonic distortion of the output voltage.
"profile of voltage is enhanced by switching configuration executed as a result of presented techniques; when nr is done by mpso algorithm yet noteworthy raise is witnessed for base loading conditions. in fig. 5 the influence of switching configuration on voltage altitudes using mpso is displayed graphically. after reconfiguration, the least voltage of the bus is enhanced to 0.94234 per unit. at bus 32 using mpso algorithm for base case. it can be seen in fig.5 that voltage gets reduced from bus 21 to 25. it is happened due to optimal reconfiguration of switches, which shift some load where buses have voltage near to 1 per unit so that other system bus voltages can be increase."
"all sets of modulation index and switching angle were tested in the simulation environment, and the results are recorded in table 3 . the table shows information for modulation index, total harmonic distortion, and the third harmonic of the corresponding modulation index. the result of extremely low third harmonics is highlighted which shows the region achieving figure 6 . three-level switching angle versus modulation index."
"for the modulation index of an h-bridge multilevel inverter, the definition is similar to the modulation index for spwm. the equation is drawn as given in eq. (2) assuming that all the vdc are equal:"
two or more separate dc sources in a full bridge are placed in series to generate a staircase ac output waveform voltage. figure 1 shows a two-level chb-mli topology. chb-mli requires sources are required. the number of sources s is also equal to the number of full-bridge modules.
"the switching angle for each switch is calculated by applying she using genetic algorithm. graph of switching angle versus modulation index is plotted and shown in figure 6 . the switching angle is set as a constraint where the angle theta_2 must be greater than theta_1. the result shows validated data for the switching angle, which only happened to be located in between the range of modulation index 0.05 and 0.85. in this range, the result fulfills the constraint set. hence, the best fit switching angle is within this range. modulation index of 0 gives an invalid result due to highly inaccurate switching angle. from modulation index 0.05 to 0.4, the difference between theta_1 and theta_2 is small which causes the fitness function value to be high. from 0.45 to 0.85, the difference between theta_1 and theta_2 is larger. hence, the fitness value for this region is low. from 0.9 to 1, high fitness value is shown. fitness value is affected in terms of validation of switching angle calculated."
"to further narrow down the range, graph of fitness function versus modulation index is plotted as shown in figure 7 . theoretically, the lower the fitness function value, the better the chance to obtain the optimized switching angle. hence, the range of relevant switching angle is narrowed down to the range of 0.45 to 0.8 of the modulation index."
"the result simulation between spwm and ohsw (ga) method is tabulated in table 4 . chb-mli applied optimized harmonic stepped waveform modulation method. the advantage for this method compared to spwm is the switching frequency. spwm in the first part of the simulation employed 4000 hz carrier frequency which resulted in the pulse to occur at every step of the voltage. ohsw method operates at fundamental frequency. switching frequency is mainly affecting the loss of energy and increases harmonic content. high switching frequency also results in short life span on power electronic switch. hence, oshw is superior to spwm"
"ieee 33 bus system single line diagram is shown in fig.3 . the radial network has 5 normally open tie switches and 32 normally closed sectionalizing switches. the 5 tie switches are 33,34,35,36,37 particularly. the tie switches are indicated by dotted lines and the solid lines represents the sectionalizing switches in fig. 3 ."
"switching strategy aid in manipulate the harmonic profile for the inverter output waveform. square wave output is a conventional type. this type evolves into a quasi-square wave, which gives a better profile as compared to square wave. the current trend is pulse width modulation (pwm), which has been widely applied in current vsi devices [cit] . however, researchers explored other methods on overcoming the cons of pwm where different kinds of add-on methods have been applied in conjunction with pwm such as selective harmonic elimination (she). she consists of a complex nonlinear equation on resolving the best switching timing. hence, various calculation approaches have been tested to optimize the overall performance. the calculation method includes newton-raphson, fourier transform, and even bioinspired algorithm approach such as bee, ant, particle swarm, genetic, bat, and others [cit] . the overview for this chapter is to introduce genetic algorithm and particle swarm optimization in resolving the she which eliminates the low order of undesired harmonics."
"from the matlab calculation for the selective harmonic elimination, the switching angle for each specific voltage-level change is recorded. the recorded data is simulated from 0 to 1 of the modulation index with a step of 0.5. the result obtained is converted into a graphical form for further analysis of the pattern."
"the h-bridge multilevel inverter has an individual voltage supply for each full bridge. hence, the total voltage output for the inverter is given as eq. (1):"
"furthermore, actual experimental work can be done in the future where the real work environment data can be obtained. this method is proposed to be suitably applied in the renewable energy harvest application where unstable sources frequently occur."
"multilevel inverter system can be separated into two sectors which are inverter topology system and switching strategy. inverter topology system consists of the most part include switch, power sources, topology configuration, and filter system. power sources are mostly re such as solar panel and wind turbine. for topology configuration, there are three main types, which have been frequently cited in the literature, that is, diode-clamped multilevel inverter (dc-mli), capacitorclamped multilevel inverter (cc-mli), and cascaded h-bridge multilevel inverter (chb-mli) [cit] . filter is applied to remove harmonics and to smoothen the inverter output quality."
"the topology is implementing a combination of several h-bridges dependent on the number of levels required. three-level cascaded h-bridge multilevel inverter is shown in figure 2 . the separate dc sources gain the possibility of a single-phase topology to be constructed. the numbers of levels involved are three-level, five-level, and seven-level, which are constructed throughout observing the effect on harmonic distortion."
"chb-mli are suitable to be implemented in the photovoltaic cells, battery cells, or fuel cells [cit] . the consideration of the number of levels for chb-mli is different from other. the calculation of chb-mli of the number of voltage levels includes the negative side of each voltage level, while other topologies do not."
", load compensative power which is at bus i., +1, line resistance which is among the bus i and i +1., +1, line reactance which is among the bus i and i +1. among the buses i and i +1 the active power losses is calculated as:"
"the result shows that unequal dc source chb-mli performs better than equal dc source. all three-level, five-level and seven-level show lower thd percentage but the only minor differences. this result shows that oshw modulation method is capable of reducing thd of both equal and unequal dc sources of cascaded h-bridge multilevel inverter. the result of low harmonic component elimination also shows the satisfactory level where the selected harmonics are reduced less than 1%."
are spwm and oshw. the simulation result will be evaluated based on the total harmonic distortion (thd) level of the inverter. spwm is constructed with parameters shown in table 1 .
in terms of performance and durability. table 4 shows the thd comparison of spwm and oshw modulation on chb-mli. the result shows a huge reduction on thd for three-level but for five-and seven-level show minor reduction less than 1%. the result shown for chb-mli with oshw method shows better performance as the selective harmonic elimination is applied. genetic algorithm shows the capability of simplifying the fourier series calculation and gets the most appropriate solution of switching angle.
"over the past decade, numberless of literature has proven multilevel inverter is a practical solution on resolving high switching losses problem exist in conventional inverter for high power application [cit] . research trends nowadays are more focusing on several multilevel inverter topologies for renewable energy source application. multilevel inverter topologies generate multilevel voltage source output which synthesizes the staircase waveform from single or multiple low dc voltage source. the low-input voltage source reduces the stress encounter by the switches with the ability to produce high-output voltage source. cascaded hbridge multilevel inverter (chb-mli) and it modified topologies is highly grab researchers attention due to the flexibility toward renewable energy."
"inverters can be classified into two main types, that is, voltage source inverter (vsi) and current source inverter (csi). each type has its own unique characteristic, which has been listed in the literature [cit] . from the literature, a brief conclusion of vsi which is more popular than csi has been made [cit] . vsi transformer-less inverter popular in renewable energy (re) application due to overall size reduction. the most commonly used inverter is high-power two-level pwm inverter. however, high-power application ideally requires low switching losses."
"where n is the number of voltage source applied in the whole h-bridge multilevel inverter. figure 5 shows the generalized staircase waveform of a five-level multilevel inverter. from the figure 5, it can be observed that the Î¸ 1, Î¸ 2, Î¸ 3 and Î¸ 4 showed the switching angle for the inverter. for each level, the voltage value is changed according to the switching angle time."
"standalone mode application mostly deals with the storage system. hence, another set of simulation is done to test the capability of oshw method with unequal dc source chb-mli. the result of the comparison of equal and unequal dc source chb-mli is tabulated in table 5 ."
"particle swarm optimization (pso) [cit] . to understand the nature behaviour, this technique has self-learning population inspiration which is taken from birds movement in a joint swarm, a group of bees or a fishes flock."
"before reconfiguration, the active power losses are 208.46kw and at bus 18 least voltage is 0.91075 p.u. as fig. 3 shows, when five tie switches are closed; as the result five loops are developed. consequently, the dimension numbers of this network, 33 bus are five. table i shows the particular loops along with dimensions and the search space for particular dimension. while sorting the optimal solution by mpso the \"s1\" switch not included in any loop and hence it is not accounted. as specified in the flow chart fig .3, the common switches of two dimension are included randomly in one dimension. later explaining the search space and identifying the dimensions, (mpso) technique is presented to perceive the radial optimal reconfiguration for the base loading case conditions. to estimate the efficiency of the suggested strategy, the radial optimal reconfiguration under base loading circumstances is also gathered using mpso. parameters used in power distribution system simulation for mpso are given in table 2 . table. 3 shows that the minimization in active power loss is more noticeable in base loading when the nr is achieved by using mpso technique. before optimal switching configuration the power losses are 208.46kw for the base case which is minimized up to 33.36% after switching configuration is done with mpso as contrast to 31.0% [cit] where spso technique is employed, 30.15% [cit] where ga technique is used and 33.08% [cit] where aca technique is applied. it can be seen in the table 3 that aca, spso and proposed method gets the best optimal configuration for minimal losses in the system is 7,9,14,32,37. hence it can be decided that mpso outperformed spso and ga and aca for reduction of the active power losses. it can be seen through fig.4 that initially before reconfiguration, power losses at each branch of the network have high magnitude of impulses specially line two and five and seven it can be seen through fig. 4 that these losses are noticeably minimized after network reconfiguration using mpso technique for the base loading condition. it can be observed that there is slight change in power losses from bus 20 to bus 37 it is due to optimal reconfiguration of switches."
"where v out is the amplitude output voltage, v dc is the amplitude dc source of the h-bridge, and n is the number of dc source in h-bridge. for the condition of unequal voltage source, the v dc value is replaced with the total voltage of all the dc sources in use as shown in eq. (3):"
"sinusoidal pulse width modulation (spwm) is applied with the capability of offering switching signal to the power electronic switch. the operation function is by comparing two principal component signals which are the modulation signal and the carrier signal. modulation signal used is a sine wave signal due to the desired output wave. the carrier wave applied mostly is high-frequency triangle or sawtooth repetition wave. this technique employs comparison between the modulation signal and the carrier signal to obtain a desired fundamental component of the voltage output waveform. figure 3 has shown phase disposition modulation, which uses one of the multicarrier pwm methods where the number of carriers depends on the multilevel inverter. the method is applied (m Ã  1) where m is referring to the number of sources. both carriers are in the same phase, which gives rise to the name of phase disposition modulation."
"mpso is effectively implemented in this paper, for ieee 33 bus system beneath base loading conditions, to optimize power distribution radial system for least active power losses and enhanced profile of voltage. results obtained from simulation are compared with ga, aca and spso algorithm. it can be concluded from the results that mpso outclassed the other compared algorithms with noteworthy minimization of losses and voltage profile improvement in a radial distribution network. further this proposed technique can be tested for the power losses minimization of large radial distribution system. more over mpso technique can also be tested for the other conventional methods to minimize power losses of the system i-e, capacitor placement and generator placement."
"the next part is fitness where the nonlinear equation is listed for calculation. in this part, the change variable is modulation index, where m affects the result of switching angle. fitness function is added in this m-file which generates fitness indices to clarify the best fit modulation index. fitness value (fval) is a measurement of the solution from the fitness function with respect to the original objective and the amount of infeasibility. the fitness function applied is shown in eq. (13):"
"on locking all the tie switches the set of dimensions in a radial network are equivalent to set of loops created. a certain dimension in the search space consists of the loop branches, denoted by this dimension. in any search space the network branches which do not exist in any mesh are not included and therefore also not accounted in the optimization technique. the common branches for the loops and dimensions, at a time must include only in one dimension that can be run randomly. the mpso can be functional to get the optimal result, at the instant the dimensions and the individual dimension of search space is stated."
"the feeder over all real power losses are calculated as: (10) where is the system over all power loss, that is attained by summing up all the line losses of power for all feeder segments. 4"
"interestingly, the two previous strategies can be seen as equivalent when assuming that the discovery of the introduced class of association rules is guided by rule-based constraints and the discovery of patterns from annotated data is guided by itemset/sequence constraints."
"we propose biclustering with constraints using pattern mining (bic2pam) to effectively incorporate fullconstraints (including the set of constraints motivated in previous section). bic2pam's extensions to the existing contributions on pattern-based biclustering [cit] are twofold. first, a precise formalism was defined to represent full-constraints (with identical notation to the one introduced along this work) and new processing procedures were implemented for their parsing and interpretation. under these principles, the desirable properties of biclustering solutions can be defined with sharp usability. bic2pam supports not only the specification of full-constraints (definition 10), but further makes available the possibility to specify native constraints to customize the structure, coherency and quality of biclustering solutions (as described in appendix). second, bic2pam implements different strategies to incorporate distinct types of constraints:"
"higher complexity from mining heterogeneous data (input data plus a large amount of annotations), results show that bic2pam is in fact more efficient than the baseline option. furthermore, the observed match scores suggests that the presence of annotations may play an important role in guiding the recovery of true biclusters."
"ai-isa [cit], genminer [cit] and scatter biclustering [cit] are able to annotate data with functional terms retrieved from repositories with ontologies and use these annotations to guide the search."
"the listed native constraints can be specified in declarative form. as such, bic2pam provides the possibility to affect structural aspects of its outputs with sharp usability."
"f2g [cit] implements a pattern-growth search that does not suffer from efficiency bottlenecks of peer searches since it relies on frequent pattern tree structures (fptrees) that store transaction-ids without duplicates. the fp-tree is efficiently traversed to enumerate all full-patterns. full-patterns are generated by concatenating the pattern suffixes with the full-patterns discovered from conditional fp-trees where suffixes are removed. figure 5 instantiates the behavior of f2g. in this section, we first show the compliance of f2g with principles to handle succinct and convertible constraints [cit] . second, we show its compliance to handle difficult combinations of monotone and anti-monotone constraints [cit] ."
"dogs were initially trained to insert their nose into the odor port breaking the ir beam. dogs were shaped using an automated program to hold their nose in the odor port for 7 s before receiving a treat. once dogs were successful in holding their nose in the port, discrimination training began. dogs were required to initiate a trial by placing their nose in the odor port. the olfactometer then began to make the correct odor mixture which took 3 s. all odorant during this time was ported to the exhaust, and therefore dogs were not receiving any odor at this point. after 3 s, the final upstream valve directed the odorant towards the dog. the dogs were then required to maintain their nose in the port for 0.5 s. if the dog did not maintain its head in the odor port for 0.5 s, the trial was cancelled. if dogs maintained their nose in the port, breaking the ir beam for at least 0.5 s, a go/no go trial was initiated. if a target odorant was present, dogs were required to maintain their head in the odor port for another 3.5 s. if the odor was not present, dogs were required to remove their head for at least 3.5 s. if dogs made a correct response (a 'hit' or 'correct rejection') they received a piece of food from the automated feeder. otherwise, no food was presented and the 15 s inter-trial interval began. across all phases of the study, the ratio of target to non-target trials was held constant at .5."
"following 25 days of training, all dogs underwent testing for generalization. the goal of these trials was to test whether dogs that were target-only trained would generalize to mixture trials at similar rates to mixture trained dogs, and to compare how both groups of dogs generalized to target odors that contained novel components they had never experienced during training. thus, during generalization testing, all groups were transitioned to mixture training. in addition, 25% of the trials were probe trials in which from one to all of the distractor odors in the mixture was an odorant from table 1 that had never previously been used in training. this assessment was conducted to more closely replicate an applied scenario in which dogs trained using a 'target-only' method might be expected to detect mixtures with previously familiar components (which reflect mixture training trials) or may include novel components (probe trials)."
"at its core, pattern-based biclustering relies on the (iterative application of the) full-pattern mining task [cit] . a full-pattern defines a region from the input data space, thus enclosing not only the underlying pattern (itemset, association rule, sequential pattern or graph with frequency and length above certain thresholds), but also its supporting rows and columns."
"compliance with different types of constraints unlike candidate generation methods, pattern growth searches provide further pruning opportunities. pruning principles can be standardly applied on both the original database (fp-tree) and on each projected database (conditional fptree)."
"despite these synergies, two major problems persist. first, there is a lack of understanding on whether domain-driven pattern mining and biclustering can be consistently integrated. in particular, there is not a solid ground on how to map the commonly available background knowledge in the form of constraints to guide the biclustering task. second, pattern-based biclustering algorithms depend on a specific variant of pattern mining, referred as full-pattern mining, which has been scarcely studied in the context of domain-driven pattern mining. in fact, although new full-pattern mining searches have been recently proposed to guarantee the scalability of the biclustering task over large and dense data [cit], there are not yet contributions on how 1 biclustering involves combinatorial optimization to select and group rows and columns and it is known to be a np-hard problem (proven by mapping the problem of finding maximum edge (bi)clique in a bipartite graph into the problem of finding dense biclusters with maximum size [cit] ). the problem complexity increases for non-binary data contexts and when elements are allowed to participate in more than one bicluster (non-exclusive structure) and in no bicluster at all (non-exhaustive structure)."
"this section provides empirical evidence of the soundness of the proposed contributions and of the relevance of using constraints within (pattern-based) biclustering to prune the search space and guarantee biologically significant solutions. to this end, we assessed the performance of bic2pam on synthetic data, gene expression data and biological networks in the presence of domain knowledge. bic2pam was parameterized with default behavior and applied with f2g-bonsai for the discovery of constant biclusters with itemset constraints and with indexspanpg for the discovery of order-preserving biclusters with sequential pattern constraints. the stopping criteria of bic2pam was specified as a minimum of 20 dissimilar biclusters for synthetic data contexts and 50 dissimilar biclusters for real data contexts. bic2pam is implemented in java (jvm v1.6.0-24). the experiments were computed using an intel core i5 2.30ghz with 6gb of ram. table 1 describes the generated data settings, with properties resembling the regularities of gene expression data. constant and order-preserving biclusters with varying quality and coherency strength were generated. noise factors (Â±20 % of the range of inputted values) were imputed and overlaps between biclusters allowed. the selected number of rows and columns per bicluster follows a uniform distribution using the ranges in table 1 in order to guarantee the inclusion of biclusters with dissimilar shapes. reported results are the average of performance views collected from 30 data instances per setting."
"to confirm that the present results were due specifically to the olfactory stimuli, following the last test session for each oxidizer, dogs were given a control test in which the odorants were cleaned from the olfactometer. dogs were then tested under normal operatione just absent any intended odors. dogs showed a precipitous drop in performance, with a mean accuracy of 45% during control sessions, indicating that the olfactory stimuli had been controlling performance."
"the ability to identify a critical oxidizer (such as an) in a complex odor mixture is likely influenced by prior experience and training history, but no direct tests have been done in this critical context [cit] . the aim of this study is to evaluate the effect of form of training on configural olfactory processing and detection of an oxidizer target in odor mixtures. dogs were trained using an automated olfactometer in a go/no-go procedure using a configural or \"mixture-training\" procedure or an element or \"target-only\" training procedure. we compared dogs' subsequent detection of the oxidizer in mixtures with familiar and unfamiliar components."
"six mixed breed dogs participated in the present study. dogs were between 8 months and 3 years of age at the start of the experiments. two of the dogs showed poor motivation for food during training and were subsequently excluded from testing. the remaining subjects were four mixed breed dogs (three females and one male) between 13.5 and 23 kgs. dogs' backgrounds were unknown but all dogs were presumably naÃ¯ve to detection training and adopted and housed in a university training facility for the purposes of the study. dogs received twice daily walks, social enrichment, and training and were adopted at the end of the study. procedures were reviewed and approved by the arizona state university institutional animal care and use committee."
"the homogeneity criteria determine the structure, coherency and quality of biclustering solutions, while the statistical significance of a bicluster determines whether its probability of occurrence deviates from expectations. the homogeneity of a biclustering model is commonly guaranteed through a merit function. following madeira's taxonomy [cit], existing biclustering algorithms can be grouped according to their homogeneity criteria (defined by the underlying merit function) and search paradigm (determining how the merit function is applied). the structure of a biclustering solution is essentially defined by the number, size and positioning of biclusters. flexible structures are characterized by an arbitrary high set of (possibly overlapping) biclusters. the coherency of a bicluster is defined by the observed correlation of values (coherency assumption) and by the allowed deviation from expectations (coherency strength). a bicluster can have coherency of values across its rows, columns or overall elements, where the values typically follow constant, additive, symmetric and order-preserving assumptions [cit] . finally, the quality of a bicluster is defined by the type and amount of accommodated noise. definitions 2 and 3 formalize these concepts, while fig. 2 shows a set of biclusters with different coherencies in a symbolic dataset."
"the lsmeans package was utilized to conduct tukey corrected post-hoc tests. to compare overall accuracy differences when transitioning from baseline (target vs. blank) to respective training conditions, we fit a logistic mixed effect model, in which whether a response was correct/incorrect was predicted by the oxidizer type (an or h 2 0 2 ), the trial type (baseline or training), the group (mixture or target only training) and the interaction between trial type and group (see raw data: \"baseline_raw_data.csv\"). to test for differences in the hit rate or false alarm rate, identical predictors were fit to models in which the response variable was whether the dog alerted for trials where the odorant was present (hit rate) or the odorants was absent (false alarm rate)."
"some of these constraints are said to exhibit nice properties when their input can be effectively pushed deep into the pattern mining task [cit] to prune the search space and therefore achieve efficiency gains. below, we explore different types of constraints according to the selected full-pattern mining task for biclustering: itemset, rule-based and sequential-pattern constraints."
"to assess bic2pam over real data, we selected expression and network datasets with varying properties. four gene expression datasets were considered: dlblc (660 genes, 180 conditions) with human responses to chemotherapy [cit], hughes (6300 genes, 300 conditions) to study nucleosome occupancy [cit], and yeast-cycle (6221 genes, 80 conditions) and gasch (6152 genes, 176 conditions) measuring yeast responses to environmental stimuli [cit] . three biological networks from string v10 database [cit] were additionally considered. these networks capture the gene interactions within human (6314 nodes, 423,335 interactions), escherichia coli (8428 nodes, 3,293,416 interactions) and yeast (19,247 nodes, 8,548,002 interactions) organisms. the scores in these networks are inferred from literature and multiple data sources, revealing the expected strength of correlation between genes."
"to compare the effect of training group on performance during training and generalization, we focused analyses on the last five days of training and the first day of generalization testing (w4,000) trials (see raw data: \"generalization_raw_data.csv\")."
"the previous extensions to pattern-growth searches are not able to effectively comply with monotone constraints when anti-monotone constraints (such as minimum support) are also considered. in fp-bonsai [cit], principles to further explore the monotone properties for pruning the search space are considered without reducing antimonotone pruning opportunities. this method is based on data-reduction operations originally implemented in exante to seize efficiency gains from the properties of monotone constraints. there are two data-reductions: Âµ -reduction, which deletes transactions not satisfying c; and Î±-reduction, which deletes from transactions single items not satisfying c. thanks to the recursive projections of fp-growth, the exante data-reduction methods can be applied on each conditional fp-tree to obtain a compact number of smaller fp-trees (fp-bonsais). the fp-bonsai method can be combined with the previously introduced principles, which are particularly prone to handle succinct and convertible anti-monotone constraints. f2g can be extended to support these reductions on the (conditional) fp-trees by guaranteeing that transactions consistently rise up. the only requirement is to preserve the order of items in the header table [cit] . as such, f2g complies with the fp-bonsai extension (see algorithm 2)."
"pattern mining searches have been adapted to seize efficiency gains from different types of constraints [cit] . these efforts aim to replace naÃ¯ve solutions based on post-filtering to guarantee the satisfaction of constraints. instead, the constraints are pushed as deep as possible within the mining step for an optimal pruning of the search space. the nice properties exhibited by constraints, such as anti-monotone and succinct properties, have been initially seized in the context of frequent itemset mining by apriori methods [cit] to affect the generation of candidates. convertible constraints can hardly be pushed in apriori methods but can be adequately handled by pattern growth methods such as fp-growth [cit] . fica, ficm, and more recently mcfptree [cit], are fp-growth extensions to further explore opportunities from diverse constraints. the inclusion of monotone constraints is more complex. filtering methods, such as exante [cit], are able to combine anti-monotone and monotone pruning based on reduction procedures. empirical evidence shows that these reductions are optimally handled within pattern growth methods by adequately growing and pruning small fp-trees (referred as fp-bonsais) [cit] ."
"we further demonstrated the consistency between domain-driven pattern mining and pattern-based biclustering based on the notion of full-patterns; surveyed the major drawbacks of existing research towards this end; and extended pattern-growth searches with state-of-the-art principles to prune the search space by pushing constraints with nice properties deep into the mining process. in particular, we showed the compliance of f2g searches with principles to effectively prune (conditional) fp-trees, and the compliance of indexspan searches with principles to effectively prune prefix-growth structures. these searches were respectively extended to support pattern-based biclustering with constant and order-preserving assumptions."
"given a matrix a, let d be a transactional database derived from a: either the concatenation of items with their column index (transactions given by itemsets) or the ordering of column indexes according to the values per row (transactions given by sequences). a full-pattern is a tuple (p, ï¿½ p, Ï p, Ï p ), where p is the pattern in d, p â x is its coverage (rows satisfying p), p â y is the set of indexes (columns), and Ï p is the original pattern in a [cit] 11:23 sequence prior to the concatenation or ordering of column indexes)."
results from synthetic and real data show that the incorporation of background knowledge leads to large efficiency gains that turn the biclustering task tractable for large-scale data. we further provide initial evidence of the relevance of the supported types of constraints to discover non-trivial yet meaningful biclusters in expression and network data with heightened biological significance.
related work is surveyed according to: (1) the contributions and limitations of existing attempts to perform biclustering with domain knowledge; (2) the state-of-theart on domain-driven pattern mining; and (3) the existing efforts towards full-pattern mining and their adequacy to accommodate domain knowledge.
"four major directions are identified for future work. first, the extension of the proposed contributions towards classification tasks based on the discriminative properties of biclusters in labeled data contexts. second, an in-depth systematization of constraints with nice properties across biological data domains, including a structured view on their relevance for omic, genomewide and chemical data analysis. third, a broader quantification of the impact of incorporating constraints across these data domains. finally, the extension of the proposed framework for the tasks of biclustering time series data and triclustering multivariate time series data in the presence of temporal constraints."
"dogs were initially trained to respond to vanilla extract diluted to 1% v/v in distilled water. once they reached 75% accuracy or higher, the dilution was decreased gradually to 0.01% v/v. on attaining 75% accuracy on this dilution, dogs were transitioned to an as the target odorant."
"overall when mixed with the continuous line) at above chance levels (at least 50 out of 80 correct trials per day per binomial test) for six consecutive days. the probability of a target vs non-target odorant was 50%. during these baseline sessions, dogs were required to alert to the pure target odorant and reject clean air as the distractor. once dogs showed a stable performance for at least six days, they were changed to experimental training for 25 days. the dogs in the target-only training group were trained to the pure target (at 50% dilution) and to ignore a variety of distractors as non-targets. distractors were generated using five different pure odorants from table 1 and the mixture algorithm described in detail above. dogs in the mixture group were trained with odor mixtures with the target and with odor mixtures of non-target stimuli. these odor mixtures were created from the target odorant and five distractors using the odor mixture algorithm noted above. target trials and non-target stimuli were identical kinds of odor mixtures differing only in whether the mixture included the oxidizer odor or not."
a constraint is traditionally seen as a conjunction of relations (predicate) over a set of variables describing a given dataset [cit] . definitions 9 and 10 revise this notion to guarantee its proper applicability within (patternbased) biclustering tasks.
"experimental results on synthetic and real data show the importance of incorporating background knowledge within pattern-based biclustering to seize large efficiency gains by adequately pruning the search space and to guarantee non-trivial and (biologically) relevant solutions. this paper is structured as follows. first, we provide background on domain-driven pattern mining for pattern-based biclustering. second, key contributions and limitations from related work are surveyed. third, we list meaningful constraints in gene expression data and biological networks, and describe an algorithmic basis (bic2pam) for their incorporation. bic2pam is further extended to attain efficiency gains from constraints with nice properties. fourth, we provide initial empirical evidence of bic2pam's efficiency and ability to unravel non-trivial yet biologically significant biclusters. finally, concluding remarks and major implications are synthesized."
"understandably, constraints addressed at the postprocessing stage are not desirable since they are not able to seize major efficiency gains. nevertheless, bic2pam supports three key types of constraints that could imply additional computational costs, but are addressed with heightened efficiency: (1) maximum percentage of noisy and missing elements per bicluster (based on merging procedures [cit] ), (2) minimum homogeneity of the target biclusters (using extension and reduction procedures with a parameterizable merit function [cit] ) and (3) minimum dissimilarity criteria to guarantee compact outputs."
dogs were trained to use an automated 12 channel dynamic-dilution computercontrolled olfactometer (fig. 1) . the olfactometer controlled presentation of odor mixtures to dogs with proportional valves and electronic mass air flow meters.
"all components that came in contact with odorants, except the check valves, were comprised of teflon (ptfe), glass, or stainless steel. check valves were changed for each odorant to prevent cross-contamination. fig. 2 shows a dog interacting with the olfactometer. at the front was a teflon (ptfe) odor port where the odor was presented. infrared (ir) sensors detected nose entries into the port. an led above the port illuminated whenever the olfactometer was active for a nose poke."
"despite the high number of contributions from domain-driven pattern mining, the ability of patterngrowth searches to effectively incorporate full-constraints with nice properties ( definition 9) was not yet demonstrated."
"the use of domain knowledge to guide biclustering has been increasingly stressed since solutions with good homogeneity and statistical significance may not necessarily be biologically relevant. however, few biclustering algorithms are able to incorporate domain knowledge."
"each channel held a specific odorant in a saturation jar. the odorant was presented by passing clean air through the jar, forcing the odorant headspace into a mixing manifold. each channel was regulated independently using a feedback control mechanism between the mass air flow controller and proportional valve. the feedback control required a maximum of 3 s to obtain accurate air flows for all channels. during this settling time, an upstream valve insured all odorants were ported to the exhaust system. only when the system was ready with accurate dilutions, was the odor presented to the odor port where the dog could sniff it. some a priori restrictions were placed on odor generation and are described in the odor mixtures section below. the odor mixture was always diluted with a final clean air continuous line, so that the odorant comprised nominally 33% of the final presentation."
"next, the concentration for each odorant was selected. the flow rate of the odor line was restricted to a total of 1 l/min and each individual line was permitted to provide between 0.2 to 0.9 l/min of the 1 l/min total. the odor flow rates were randomly allocated with the restriction that they must sum to 1 l/min or less. if the flow rates summed to less than 1 l/min, then the remaining flow rate was allocated to a clean air channel so that the final air flow from the odor line was always 1 l/min. this line met with the continuous line (always 2 l/min) providing a final airflow of 3 l/min to the odor port. concentrations and odor identity were determined in this manner to provide dogs with a wide range of variability in both odor type and concentration which might be expected in a home-made explosive."
"although the incorporation of constraints with nice properties can only be easily supported under aprioribased searches, there is large consensus that patterngrowth searches are better positioned to seize efficiency gains from these constraints than peer apriori-based and vertical searches. as such, f2g-bonsai and indexspanpg, described below, extend respectively the recently proposed f2g (full-frequent itemset miner) and indexspan (full-sequential pattern miner) algorithms to guarantee a more effective pruning of the search space in the presence of constraints. these extensions are integrated in bic2pam. native constraints are effectively incorporated in bic2pam through adequate parameterizations of pattern-based biclustering algorithms (appendix)."
"the led was off during the inter-trial intervals. below the odor port was a plastic tube which delivered food reinforcers for correct responses. at the back of the olfactometer (not shown), an exhaust tube evacuated odorants from the odor port to a standard laboratory fume hood."
"note that similar constraints can be formulated for the analysis of alternative biological data, including: structural genome variations to enable the discovery of highorder single-nucleotide polymorphisms; genome-wide data to find promoters where mutations or appearing binding sites show properties of interest; or medical data to force the inclusion of certain clinical features or to focus on less-trivial disease markers."
"despite the relevance of discovering optimal and flexible biclustering solutions to effectively incorporate knowledge-driven constraints, most of the existing biclustering algorithms are based on greedy or stochastic searches, producing sub-optimal solutions, and place restrictions (such as simplistic forms of coherency, fixed number of biclusters, non-overlapping structures) that prevent the flexibility of the outputs [cit] ."
"there are three major classes of full-pattern mining searches [cit] : (1) aprioritid-based searches, generally suffering from costs of candidate generation for dense datasets and low support thresholds; (2) searches with vertical projections, which show efficiency bottlenecks for data with a high number of transactions since the bitset cardinality becomes large and associated intersection procedures expensive; and (3) recently proposed pattern-growth searches based on the annotation of original pattern-growth structures with transactions' identifiers. in particular, f2g [cit] and indexspan [cit] (default options in bicpam, bip, bicnet and bicspam biclustering algorithms [cit] ) were the first pattern-growth searches for full-pattern mining aiming to surpass memory and time bottlenecks associated with bitset and diffset structures used by aprioritid and vertical-based searches."
"this work motivates the relevance of constraint-guided biclustering for biological data analysis with domain knowledge. to answer this task, we explored the synergies between pattern-based biclustering and domaindriven pattern mining. as a result, bic2pam algorithm was proposed with two major goals: (1) to learn biclustering models in the presence of an arbitrary number of annotations from knowledge repositories and literature, and (2) to effectively incorporate constraints with nice properties derived from user expectations. bic2pam can therefore be applied in the presence of domain knowledge to guarantee a focus on relevant regions and explore potentially high efficiency gains."
"recent attempts to perform biclustering based on enhanced pattern mining searches [cit], termed as pattern-based biclustering, showed the unprecedented possibility to efficiently discover arbitrarily positioned biclusters with parameterizable size, coherency and quality [cit] . in this context, two valuable synergies can be identified between pattern-based biclustering and knowledge incorporation. first, the optimality and flexibility of pattern-based biclustering solutions provide an adequate basis upon which knowledge-driven constraints can be incorporated. pattern-based biclustering tackles the restrictions of peer algorithms, being an adequate candidate to flexibly constrain the desirable properties of the target solution space. second, the effective use of domain knowledge to guide pattern mining searches has been largely studied in the context of domain-driven pattern mining [cit] ."
"olfactory figure-background segregation is the identification of a target odorant against a complex odor background. in natural environments, animals need to be able to identify target odorants, such as food items, even against complex and variable backgrounds [cit] . in nature, target and background odor plumes can be temporally and or spatially segregated thereby simplifying olfactory figurebackground segregation [cit] . a more complex task, however, occurs when a single component needs to be distinguished from a mixture of simultaneously presented odorants [cit] . some researchers have questioned whether identifying the components of such a mixture is possible at all [cit] . mixtures tend to be perceived configurally, such that the mixture produces a unique percept distinct from the constituent elements, and this may vary depending on the chemical similarities of the components in the mixture [cit] ."
"to formalize the task targeted in this work, we introduce below the concept of constraint in the context of biclustering, and further describe different types of constraints according to the selected full-pattern mining task."
"at the end of each testing phase for both an and h 2 o 2, a day of control testing was conducted. during control testing, all the odorants were cleaned from the olfactometer, and the dogs were required to complete two sessions with no odorants presented. the same olfactometer channel and jars that held the target odorant during testing trials was assigned as the target for control trials, although no odorant was intentionally placed. thus, the olfactometer arrangement was identical to the testing procedures, except that no odorants were presented. this control tested for the possibility that dogs were utilizing extraneous cues during testing such as discriminating between subtle 'clicks' of valves opening or other unintentional non-olfactory cues."
"the previous results highlight the relevance of full-pattern growth searches for biclustering (f2g-bonsai and indexspanpg) to adequately prune the search space. figure 12 further motivates the importance of the proposed f2g-bonsai against aprioritid and eclat (f2g is able to surpass efficiency bottlenecks associated with bitset data structures), and the relevance of indexspanpg against prefixspan (indexspan is able to explore further efficiency gains from the item-indexable properties of the biclustering task). results show the relevance of parameterizing bic2pam with the proposed full-pattern growth searches for large data and for hidden biclusters with loose coherency strength (highly dense data)."
"meaningful constraints with succinct, monotone, antimonotone and convertible properties were presented for distinct biological tasks (gene expression analysis and network data analysis) in order to focus the search space on less-trivial yet coherent regions."
"logistic mixed effect models using r [cit] and the lmertest [cit] and lsmeans [cit] packages were run. for all models, subject id was included as a random intercept."
"together, these results suggest that for dogs to show optimal performance in detecting an odor target in highly variable and complex backgrounds, they need to be trained with a variety of odor mixtures with and without the target. this is in contrast to a target only training procedure in which dogs are trained to detect the primary oxidizer (the element/target) and to not respond to 'distractors' or odors without the target. some research suggests that these target-only based procedures may not be optimal. for example, dogs trained to pure potassium chlorate did not generalize to potassium chlorate based explosives and mixtures [cit] . furthermore, dogs trained to an, did not readily generalize to chemically related salts or even different grades of an such as fertilizer grade [cit] ."
"these results indicate two important findings. first, the training method influences whether a target odorant in a complex odor mixture is unrecognizable (configural perception) or detectable (figure-background segregation). second, dogs can perform the required figure-background segregation to differentiate potential"
"recent research has demonstrated that mice can identify a target odorant in mixtures of up to 14 simultaneously presented components [cit] . these authors trained the mice using variable backgrounds that changed from trial to trial. the results of the present experiment confirm that it is the type of training that is critical for successful figurebackground segregation. by comparing a training procedure using odor mixtures to training with an equivalent number of trials to just the target, we demonstrate the importance of the training paradigm on the type of perceptual processing that's observed."
"biological data are characterized by the presence of local patterns, whose discovery has been widely studied and motivated in the context of biclustering [cit] . in particular, the relevance of biclustering has been largely shown in the analysis of gene expression data (to discover transcriptional modules described by subsets of genes correlated in subsets of samples [cit] ) and biological networks (to unravel meaningfully dense regions from weighted adjacency matrices derived from interaction data [cit] ). a key question in the field of biclustering is how to benefit from the increasingly available domain knowledge. initial attempts to incorporate background knowledge from user expectations [cit] and knowledge-based repositories [cit] within biclustering showed its importance to explore efficiency gains and guarantee relevant solutions. however, these attempts only support very specific forms of knowledge and cannot be extended to flexibly constrain the desirable properties of outputted biclusters. furthermore, due to the complexity of the biclustering task 1, most of the existing algorithms: (1) are based on greedy or stochastic approaches, producing sub-optimal solutions; and (2) usually place restrictions on the allowed structure, coherency and quality of biclusters, compromising the flexibility of the outputs [cit] . in this context, these biclustering approaches cannot be extended to incorporate knowledge-driven constraints since their restrictions may a priori contradict the inputted constraints."
"the datasets and bic2pam software are available in http://web.ist.utl.pt/rmch/software/bic2pam/. incorporated within bic2pam by adapting the parameters that control its behavior along its preprocessing, mining, postprocessing steps."
"this section extends pattern-based biclustering algorithms [cit] to accommodate constraints by proposing bic2pam (biclustering with constraints using pattern mining). in what follows, we first provide principles for biclustering annotated biological data. second, meaningful full-constraints with nice properties are listed to guide expression data analysis and network data analysis. the possibility to specify alternative constraints in order to customize the structure, coherency, quality and statistical significance of biclustering solutions according to available knowledge is discussed in appendix. third, we describe a set of principles for the specification, processing and incorporation of constraints within pattern-based biclustering. finally, we adapt the full-pattern mining searches used within bic2pam in order to seize heightened efficiency gains by exploring the properties associated with the inputted constraints."
probe trials were non-differentially reinforced (food was given whether responses were correct or not) to evaluate spontaneous responding. all probe responses were reinforced to maintain the reinforcement rate and sustain motivation and responding during generalization tests which we anticipated would show lower accuracy rates. each dog received six days of generalization training.
"a logistic mixed effect models was fit in which the response (correct/incorrect) was predicted by the oxidizer type (an or h 2 0 2 ), the trial type (training, generalization mixture trial, probe trial), the group (mixture or target only training) and the interaction between trial type and group. identical models for hit rate and false alarm rate were fit with similar parameterization but focused on whether a dog alerted (yes/no) for trials in which the odorant was present (hit rate) or absent (false alarm rate)."
"in this context, constraints need to be satisfied by the antecedent, consequent, or can be alternatively applied during the generation of frequent itemsets, prior to the composition of rules."
"after all dogs completed training and testing with an, they changed to the alternative condition and were trained under baseline conditions to h 2 o 2 such that dogs learned to respond to h 2 o 2 compared to clean air at above chance levels for six consecutive sessions. once reaching this criterion, the respective procedures were repeated with the opposite assignment of groups completing the crossover design ( table 2 )."
"there are several important limitations to the present study worth considering. first, the overall sample size is small. due to the extensive training required for these dogs to successfully respond to an and h202 in mixtures under controlled laboratory conditions, only a few animals could be tested. before generalized recommendations can be made for detection training programs, this study will need replication in a larger sample."
"a variety of odorant mixtures was dynamically created by mixing odor head spaces. the odor mixtures were generated using the following procedure. first, the number of components in the mixture was chosen from a range from 1 to 5, with a probability of one component set at 25% and the probability of all other numbers of components set uniformly at 18.75%. next, the trial was determined to either contain the target or not depending on whether the target was being mixed or not in the respective phase of the experiment. all other odorants of the mixture were then selected from the odorants available with equal probability (6 odorants were available during training and an additional 5 novel odorants were available during generalization testing)."
"please have in mind that the constraints instantiated throughout this section represent a small subset of all possible constraints of interest, thus being mainly introduced for the sake of motivating the relevance of succinct, (anti-)monotone and convertible properties. the specification of constraints of interest is always dependent on the learning goal and the peculiarities of the input data. as such, an exhaustive listing and discussion of relevant constraints for biological data contexts is considered to be out the scope of this work."
"we utilized a cross-over experimental design to evaluate whether mixture training or training to pure targets ('target-only training') led to the greatest generalization to novel odor mixtures. the four dogs were randomly assigned to two groups, a and b (see table 2 ). both groups were first trained and tested using an as a target. after testing with an, the training assignments were crossed between groups and all dogs were trained with h 2 o 2 as the target. training and assessment procedures were identical for both target odorants. throughout training and testing phases dogs received two 40 trial sessions per day (one in the morning and one in the afternoon)."
"understandably, despite the inherent simplicity of incorporating constraints with nice properties in aprioribased searches, there is a critical drawback: the inability to rely on key pattern-growth searches, such as f2g (for the discovery of constant/additive/symmetric/plaid biclusters) and indexspan (for the discovery of orderpreserving biclusters). these pattern-growth searches were previously shown to be able to mine large data with superior efficiency [cit] . adding to this observation, there is a considerable agreement that the underlying structures of pattern-growth searches, such as frequent-pattern trees and prefix-growth trees, provide a more adequate representation of the search space for an improved pruning."
"these contributions were extended for association rule mining [cit] . in particular, nice properties were studied for item constraints [cit], support constraints [cit], bounds interestingness criteria [cit], and constraints on the structure and dissimilarity of rules (respectively referred as schema and opportunistic) [cit] ."
"domain knowledge comes often in the form of annotations associated with specific rows and columns in a matrix (or nodes in a network). these annotations are often retrieved from knowledge repositories, semantic sources and/or literature. annotations can be either directly derived from the properties associated with each row/column/node (e.g. properties of a gene or a sample in gene expression data) or can be implicitly predicted based on the observed values by using feature extraction procedures. for instance, consider the set of functional annotations associated with gene ontology (go) terms [cit] . a go term is associated with an interrelated group of genes associated with a specific biological process. since a gene can participate in multiple biological processes, genes can have an arbitrary number of functional annotations. as such, rows in an expression matrix (or nodes in a biological network) can be annotated with a non-fixed number of labels."
"with regards to properties of the aforementioned constraints: length constraints are anti-monotonic, while super-pattern constraints are monotonic. item constraints, length constraints and super-pattern constraints are all succinct. some aggregate constraints and regular expressions can also show nice properties [cit] ."
"this work addresses these problems. to this end, it extends pattern-based biclustering algorithms using principles from domain-driven pattern mining to seize large efficiency gains in the presence of background knowledge. furthermore, it shows how functional annotations and constraints with succinct, (anti-)monotone and convertible properties can be used to guide the biclustering task. the major contributions are fivefold:"
"regular expressions and aggregate functions are the most common form of constraints to guide frequent itemset mining. in this context, efficiency gains can be seized in the presence of constraints with succinct, (anti-)monotone and convertible properties."
"hmes from innocuous complex odor mixtures. this finding has critical significance for the detection of hmes. dogs trained to alert to only one component of an odor mixture will fail to recognize the oxidizer in an explosive mixture. importantly, if the dog is trained with mixtures containing the oxidizer and similar mixtures not containing the target, dogs can generalize detection of the oxidizer even when mixed in novel odor mixtures. this suggests that training dogs to a critical component of hmes needs to be done using a mixture-training procedure and not direct training to an oxidizer such as an."
"according to these principles, we extended indexspan [cit], an extension of prefixspan to explore efficiency gains from the intrinsic properties of the order-preserving biclustering task. indexspan is compliant with the enumerated principles. the minimalist data structures, fast database projections and early pruning techniques [cit] do not interfere with the underlying prefix-growth behavior, the essential requirement to incorporate prefix-monotone constraints. furthermore, given the fact that indexspan explores item-indexable properties associated with the order-preserving biclustering task, testing constraints is done in an efficient and elegant way (see algorithm 3). this is true with regards to both: (1) the validation of whether an anti-monotonic constraint (or regular expression) cannot be satisfied by a given prefix (in order to stop its growth), and (2) the validation of whether a a monotonic constraint cannot be satisfied by a given (projected) sequence (in order to prune the search)."
"unfortunately, the identification of individual components in mixtures, at least for humans, can be quite challenging [cit] . furthermore, it is not clear how a variety of factors may combine to determine whether odor mixtures are perceived as individual elements (elementally), or configurally [cit] ."
"a large number of studies explored how constraints can be used to guide pattern mining tasks. two major paradigms are available: constraint-programming (cp) [cit] and dedicated searches [cit] . cp allows pattern mining to be declaratively defined according to sets of constraints [cit] . these declarative models can allow for complex mathematical expressions on the set of fullpatterns. nevertheless, due to the poor scalability of cp methods, they have been only used in highly constrained settings, small-to-medium sized data, or to mine approximate patterns [cit] ."
"to reduce contamination between trials, there was always a 15s inter-trial interval. during this interval, an upstream valve to a clean air line was pulsed repeatedly every 20 ms for 15 s to help clear the lines. at the end of every day, all common odor lines and components were washed in hot water, rinsed with ethanol, and placed in an oven overnight between 85 to 100 c. for all odorants, their manufacturer and purity are shown in table 1 . all odorants except an and h 2 o 2 were diluted to 0.01% v/v in mineral oil. an was not diluted and 10 g was placed in the saturation jar. an was not diluted because of its low vapor pressure [cit] and reduced concentration of this odorant in comparison to all others. h 2 o 2 was diluted in deionized water to 10% v/v to reduce the dogs' exposure to h 2 o 2 . a total of 10 ml of the odorant dilution was placed in the saturation jars. odorant dilutions were prepared and replaced weekly throughout all testing."
"different types of constraints were introduced in definition 11. in order to show how these constraints can be specified and instantiated, this section provides examples of meaningful constraints for gene expression and network data analysis."
"in this subtask, given a full-text article, we need to identify goess and associate genes related to these sentences. the task can be defined as a supervised machine learning task by considering goess as positives and all other sentences as negatives. as positives and negatives are from the same pool of articles, the resultant models may be overfitted. we supplemented negatives with unlabeled excerpts from generif (6) records aiming for better models based on our prior experience on distant supervision, i.e. use existing resources to obtain weakly labeled instances for training machine learning classifiers (7, 8) ."
"we were not aware of the need of containing experimental methods for detecting go evidence excerpts and assigning go terms as specified by the annotation guideline. this may explain why the use of section features in subtask a has the most gain in the f1 score. additionally, we sampled only from human generif records with at most two records per gene. the rationale behind it is to avoid overrepresentation of popular studied genes and their homologous genes. it is not clear whether such sampling approach has impact on the performance of the system."
"the capacity of vr-based systems as a facilitation tool for functional recovery by engaging brain circuits, such as motor areas, has been demonstrated [cit] b) . a recent review study has shown that such systems can be effective and motivating for rehabilitation therapies involving repetition and feedback [cit] . it seems that motivation is a key factor for applications based on augmented feedback using vr for rehabilitation of motor skills of patients with neurological disorders [cit] . in particular, there is evidence for the effectiveness of such approaches for the rehabilitation of upper limbs in patients with stroke [cit] ."
"this is an open access article distributed under the terms of the creative commons attribution license (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited."
"step 2. family name inferred. because genes belonging to the same family can appear as plurals in the document, we assemble a dictionary of family names based on the gene mentions provided. for each mention of the family name in a sentence (using direct string matching), all of the members of that family in the gene list are assigned to the sentence."
"the device and the control software offer three configuration modes: single mode, teleoperation mode, and mixed mode. in the single mode, the device works as a joystick and the patient interacts with the virtual environment. the teleoperation mode allows bidirectional person-to-person interaction directly, reciprocally transmitting the forces applied locally to a second, remote unit. two identical devices are therefore needed, in our case, one for the patient and one for the doctor. the most complete mode, however, is the mixed mode (a combination of the single and teleoperation modes), where both patient and doctor work together and that is especially useful for cooperative tasks, e.g., for lifting a virtual object. therefore, when the mixed mode is enabled, the movement of the haptic device by the patient and the eventual collisions with virtual objects generate forces that are transmitted to the remote place, where the doctor feels these forces being applied remotely at the local haptic unit; and vice versa, the doctor may for instance explore the patient's arm mobility remotely by moving his own haptic device, allowing the detection of resistances to movements. the bilateral teleoperation has been implemented according to a classical position-position (pp) scheme. basically, the force-feedback for both the patient and the doctor are generated by a control based on the position error between the end effectors of the remote devices. since the rehabilitation task is focused on the patient, the force-feedback generated"
"regarding the representation of the patient at the hospital facilities, and as robotic technology becomes available, the patient may be embodied in a life-sized teleoperated agent through which the patient would be able to see, hear, feel the hospital environment, and it would also reproduce the patient's movements at the hospital. the representation of the patient by a robot at the hospital will change the concept of remote tactile interaction as we know it today. at the hospital, sensors attached to different body parts of the robot would capture and transmit, when touched by the doctor, the generated tactile information to the patient's body. there, the patient, attached with small actuators in the same body part as the robot [cit], will \"feel\" the contact, even with force-feedback if the patient wears the appropriate equipment (e.g., exoskeleton). further, using a human-shaped robot as the physical representation of the patient, rather than a life-sized virtual avatar or a mechanical haptic device (representing patient's arm), enhances the physical presence of the patient and, consequently, facilitates the natural interaction between the doctor and the patient. indeed, robotic agents for remote embodiment are emerging. recently, we have shown this concept using the same technology during an interview and demonstration at the bbc news [cit] ."
the work was supported by us national science foundation (abi: 0845523) and us national institute of health (r01lm009959). funding for open access charge: us nsf (abi:0845523) and us nih (r01lm009959).
"in this paper, we have presented an innovative set-up for remote interaction and remote rehabilitation that includes the body www.frontiersin.org projection into virtual bodies in a fully immersive environment and physical embodiment at the remote place. this unique system for telerehabilitation is the result of the integration of state of the art technologies developed at different institutions in the fields of vr, haptics, computer science, biomedical research, and neuroscience. this approach systematically differs from non-immersive telerehabilitation systems and should represent a step forward in the field."
"where cv ij r can be formulated using ij Î¸ according to eqs. (9) and (10), and ij t can be formulated using ij r according to eqs. (14) and (15)."
"neurorehabilitation facilitates the recovery of functional skills lost after neurological diseases or accidents. according to the american academy of neurology, \"neurorehabilitation is the process of restoration of function for persons with disorders of the nervous system. this process involves multiple disciplines and the application of strategies aimed at reducing impairments, disabilities and handicaps, and ultimately enhancing quality of life for persons with neurological disorders.\" neurorehabilitation is an emerging specialty in neurology [cit], and the integration of technology at this frontier is of interest from both a medical [cit] and social perspective, and is particularly relevant when the rehabilitation has to be performed at home due to limitations of patient mobility [cit] . therefore, it represents a field with high expectations for the future, where the integration of new technologies may enhance the versatility and effectiveness of the current rehabilitation systems [cit] ."
"the patient connects to the application from home. after donning the vr equipment and physiological monitoring, the patient starts the session, becoming immersed into a virtual or mixedreality representation of the doctor's office. the embodiment of the patient into the virtual character is induced by means of the multisensory correlations and the body is seen by the patients from their first-person point of view, i.e., when they look down toward their body they will see a virtual body instead of their real one. the patient sees, hears, and touches as if he/she were at the doctor's office. the exploration and evaluation of the patient takes place within this framework. we describe next the technical developments and equipment used at patient's home that allow the immersive experience."
"the patient sits in front of a computer to which the hmd and tracking devices are connected. the computer has an nvidia geforce 480 gtx graphics card and uses nvdia physx engine for virtual collision detection. the system was programmed using the xvr system [cit] and the virtual body using the halca library [cit] . the arena software (optitrack, naturalpoint inc.) is used for arms and upper body tracking to control the movements of the avatar. a gendermatched rocketbox (rockebox studios gmbh) avatar with 123 www.frontiersin.org bones and articulated joints is used to represent the patient within the virtual environment."
a new formulation based on constraint-topology transformation is proposed to generate oscillatory differential equations for a general multibody system. vibration displacements of bodies are selected as generalized coordinates. the translational and rotational displacements are integrated in spatial notation. linear transformation of vibration displacements between different points on the same rigid body is derived. absolute joint displacement is introduced to give mathematical definition for ideal joint in a new form. constraint equations written in this way can be solved easily via the proposed linear transformation. the oscillatory differential equations for a general multibody system are derived by matrix generation and quadric transformation in three steps: 1. linearized odes in terms of absolute displacements are firstly derived by using lagrangian method for free multibody system without considering any constraint.
"from a practical viewpoint, as a basic solution for the representation of the patient at the doctor's office, life-sized 3d video is displayed on a 3d display (figure 5 ). this solution requires the doctor to wear minimal equipment, at most some tracking device and/or 3d glasses for stereoscopic vision, allowing other doctors or hospital staff to join the session at any time easily. flexibility is a key word in the proposed system. therefore, an alternative solution, consisting in showing a virtual representation of the patient, is also contemplated [cit] ."
the open-loop constraint matrix â² b corresponding to system shown in fig. 4(d) takes the form 0q 00 00 00 0 00 00 i0 00 00 0 0p p 0p q 0q 00 00 0 b 00 00 00 i0 00 0 0p p 0p q 00 0q 00 0 00 00 00 00 i0 0 0p p p0p p q00 0p q 0q 0 00 00 00 00 00 i
two systems for body tracking are used: a) marker-based body tracking with optitrack infrared cameras (naturalpoint inc.). b) markerless body tracking with kinect (microsoft corporation).
"in this article, we describe our participation systems for the go task. for subtask a, we trained a logistic regression (lr) model to detect goess using the training data supplemented with noisy negatives from an external resource. a greedy approach was applied to associate relevant genes with sentences. for subtask b, we designed two types of systems: (i) search-based systems, which predict go terms based on existing annotations for goess that are of different textual granularities (i.e., full-text articles, abstracts, and sentences) using state-of-the-art information retrieval techniques and (ii) a similarity-based system, which assigns go terms based on the distance between words in sentences and go terms/synonyms."
"retrieval. we build indexes for the abstract collection and the full-text collection, respectively, using the indri (17) search engine. in particular, we use the porter stemmer for stemming words in the documents. we choose the query likelihood language model as our retrieval model. this model scores documents for queries as a function of the probability that query terms would be sampled (independently) from a bag containing all the words in that document. formally, the scoring function is a sum of the logarithms of smoothed probabilities:"
"a grab device, like the one available at the patient's home, is used in mixed teleoperation mode for haptic interaction with the patient. besides the ring task, it may be useful to assess patient's arm mobility or force, as the interconnected devices transfer the forces to each other in all three directions. the device is placed in a position that corresponds as far as possible with that at the patient's home (figure 5) ."
"we extract positive and negative instances (i.e. sentences) from both training and developing sets to train our model. the training set contains 1318 positive and 26 868 negative instances, while the development set gives 558 positive and 14 580 negative sentences."
"transformation of displacements of two points on a same rigid body is fundamental to the dynamics of a multibody system. the transformation can be divided into two steps. firstly, the displacements of spring acting point are formulated by using the displacements of cm on the same body, with respect to the same reference frame. and then the resulting displacements are transformed from body reference frame to spring reference frame. a linear transformation is proposed for vibration displacements based on homogeneous transformation. assume that there are two reference frames, -cx yz and â² â²â² -uxy z . the direction cosine matrix fromcx yz to â² â²â² -uxy z is determined by Î± Î²Î³"
"if the number of transmission mechanisms in a general multibody system is t, the constraint equations for all transmission mechanisms can be integrated as"
"the set-up includes continuous recording and monitoring of the patient's physiology, including electrocardiogram, galvanic skin response (gsr), and electromyographic activity of the affected limb. gsr is useful during exploration to reveal movements that may induce pain or discomfort. additionally, the grip force in both hands is used for detecting force asymmetries and for the followup of the patient's evolution. for that purpose, we have developed a device for measuring hand force built on pressure sensors, and have created a corresponding virtual model. all these data are recorded and streamed in real time to the remote doctor's office (see the data exchange platform). we have developed a standalone application for data monitoring and saving using matlab (mathworks inc.). the physiological data are displayed at doctor's pc screen (figure 5) ."
"generally, there might be several spring-dampers sharing the same stiffness or damping coefficient p in a multibody system. if p is the stiffness coefficient of spring-dampers interconnected between b i and b j, and b j and b k, it can be obtained that"
"feeling a virtual body to be your own allows body transformation and manipulation in a way that it is not possible to do outside of vr [cit] . it has also been shown that illusory body experiences are able to induce similar levels of activity in the brain areas associated with anxiety and interoceptive awareness, as when the person's real hand is threatened, or lateralized autonomic responses such as changes in body temperature [cit] ."
"a quadruped robot and a stewart platform were taken as case studies to verify the effectiveness of the proposed method for both open-loop and closed-loop spatial mechanism systems, respectively. simulations and experiments were further carried out on a wafer stage to justify the presented method. a. quadruped robot the proposed method has been applied in linear vibration analysis of a quadruped robot, which is an open-loop spatial mechanism system. as shown in fig. 9, the body is connected with four legs via revolute joints along z direction. each leg consists of three parts which are connected by two turbine worm gears. the leg mechanism can be modeled as three rigid bodies connected by two revolute joints and torsion springs along x direction. each flexible foot is modeled as a three dimensional linear spring-damper, then the quadruped robot becomes an open-loop spatial mechanism system with 13 bodies and 18 dofs."
"bringing today's computer science, vr, robotics, and neuroscience together does not assure the success of the venture per se [cit] . in order to assure a long-term success in the standardization and integration of emerging technologies in society, they need to go hand-by-hand with a wide, global theoretical approach that considers all factors involved, including economical and ethical. for example, the viability of non-profitable solutions, economically speaking, becomes problematic, independent of the necessity or contribution to the well-being of humanity. saving emissions and costs [cit] and improving efficiency and usability [cit] have become as essential as medical factors. truly effective telemedicine and telepresence technologies hope to reduce the need for travel (co 2 release, time, and energy savings) by augmenting effective communication dramatically in comparison to current solutions [cit] ."
"it can be proved that ii c and ij c are also determined by eqs. (25) and (26) b and j b . matrices c and k contain explicitly damping coefficients and stiffness coefficients, and reveal clearly the topology of spring-dampers. by using the system matrices m, c and k, eqs (18), (21) and (28) can be reformed as"
"both subtasks are evaluated using the standard precision (p), recall (r) and f1-measure (f1) scores (4). however, there are two different criteria for determining a match between a candidate sentence and the ground truth sentence: (i) exact match between sentence boundaries and (ii) partial overlapping. subtask b is also evaluated by p, r and f1 based on two different matching criteria: flat or hierarchical. for the flat p, r and f1, a match occurs when the predicted go term is exactly the same as the gold standard. for hierarchical p, r and f1, a match occurs when the predicted go term has a common ancestor with the ground truth go term. table 2 presents the official evaluation results of subtask a. runs a1 and a3 obtain comparable f1 scores. run a2 has a lower f1 score because of the relatively low performance for recall. note that the performance difference between runs a1 and a2 was purely because of different noisy negatives sampled from generif."
"the results look promising compared with previous challenges. however, there is still much room for improvement. specifically, we plan to explore advanced text modeling methods including deep learning (20) (21) (22) (23) and hierarchical/supervised topic modeling (24) (25) (26) for the task. we can make use of unlabeled text for feature extractions or build deep belief networks for sparse feature learning. with enough goa, we can explore the use of hierarchical/supervised topic modeling for predicting goa given evidence sentences."
"if p is the stiffness coefficient of spring-dampers interconnected between b i and b j, and b k and b l, it can be obtained that"
"apart from being useful to induce virtual body ownership, all body tracking systems are of great value for the medical team to evaluate the evolution of the motor capabilities of the patients. body tracking systems have the additional advantage over visual inspection that they can provide the position in space at any instant (trajectory tracking), therefore allowing an immediate quantification of different movement parameters such as amplitude or speed of movements, and their evolution from session to session."
"taking all these experiences together, there seems to be a general agreement that the field of robotic-assisted and telerehabilitation has yet not matured, as reflected by the considerable list of review papers published during the last years (see citations above) that discuss the future of vr, robotics, or remote rehabilitation. therefore, despite the many benefits that these technologies potentially may provide in the clinic, this appealing technological approach has not so far yielded the expected improvement for rehabilitation therapies [cit] . in our opinion, beside the limitations of the current technology (in particular the still emerging field of haptics), there are several challenges that have been neglected up to now."
"to promote research and tool development for assisting go curation from biomedical literature, the critical assessment of information extraction in biology (biocreative) iv organized gene ontology curation task (go task) [cit] (4) . there are two subtasks: a) identification of go evidence sentences (goess) for relevant genes in full-text articles and b) prediction of go terms for relevant genes in full-text articles. the training set of go task contains 100 full-text journal articles [in bioc format (5) ], while the development and test sets each have 50 articles. task organizers also provided ground truth annotations for the training and development sets to all participants (5). table 1 gives the detailed statistics about genes, gene-related passages and go terms in the go task data."
"for each article, all relevant genes are provided. therefore, we use a greedy approach to associate evidence excerpts with the relevant genes. the approach includes four steps:"
"neurorehabilitation borrows elements from the fields of haptics and robotics. therapies based on telerehabilitation mainly employ haptic devices for monitoring data captured during physical exercises, so that the performance of patients can be evaluated [cit] ) . however, haptic feedback also enriches sensory experience for the participant [cit] by providing forces that produce biomechanical interactions with other devices, virtual objects (simulating the interaction forces produced by the same object in reality) or remotely located people [cit] . these telerehabilitation set-ups can yield comparable benefits compared to those of traditional non-mediated therapies. for example, it has been shown that the effect of robotmediated therapy can have greater effect than the same duration of non-functional exercises [cit] . a different range of robotic integration in rehabilitation are exoskeletons, which adapt their force to the patient's performance and complement it to reach the defined goal, e.g., for gaiting purposes. [cit] have demonstrated in an experiment with chronic stroke patients that an adaptive \"assist-as-needed\" robot controller increases participation from the motor system. however, first clinical trials suggest that the only contribution to clinical practice currently is the provision of intensive, repetitive movements [cit] . moreover, a recent comparative study has shown that the advantages in both functioning recovery among patients with chronic upper-extremity disability related to stroke and cost-effectiveness of robot-assisted therapies are modest [cit] ."
"at the hospital facilities, the doctor monitors the session and patient's physiological data, feels the presence of the patient and interacts with him or her."
"to enable physical interaction between the patient, the virtual environment and the doctor, a ring task has been implemented in the virtual environment as a rehabilitation exercise. the task consists of a virtual ring being passed along a virtual wire, with the aim of avoiding contact between the ring and the wire (figure 4) . the ring position is controlled with the haptic arm device at the patient's side. whenever the ring touches the virtual wire, the force-feedback is enabled so that the patient feels the collision and cannot go through the object. the physical simulation calculates the interaction forces between the virtual ring and the wire, while the position of the ring and the effector of the haptic device of the patient are linked by a virtual coupling with tuneable stiffness and damping."
"a fundamental requirement for remote neurorehabilitation is to enable physical interaction between the medical staff and the patient. in particular we have used bidirectional haptic interaction including force-feedback. the novelty of the presented approach is not the interaction with the computer or virtual environment, but to enable person-to-person interaction between patient and doctor. furthermore, this bidirectional haptic interaction enriches the sensory experience, amplifying the scope of tactile feedback and contributing to the illusory body feelings. next we describe the haptic device for the physical task serving as a rehabilitation exercise, where biomechanical and neuromuscular interactions with the virtual environment and remote persons are enabled."
"where q i is the i th query term, jdj and jcj are the document and collection lengths in words, respectively, tf q i ;d and tf qi;c are the document and collection term frequencies of q i, respectively, and l is the dirichlet smoothing parameter. the indri retrieval engine supports this model by default."
"as pointed out in previous derivation, the mass matrix m of free system contains only mass and inertia parameters of each body. the damping matrix c of free system contains only damping coefficients and position and orientation of dampers. the stiffness matrix k of free system contains only stiffness coefficients and position and orientation of springs. matrices â² b and â²â² b contain information such as position and orientation of all joints. therefore eigenvalue sensitivity about specific design parameter can be obtained. a. eigenvalue sensitivity about mass or inertia parameter if p is the mass or inertia parameter of body b i, one can obtain that"
"beyond the technology itself, there are open questions concerning the display of remote places and persons that may affect participant's experience: should the patient be virtually represented or physically embodied in a robot? should the real physical doctor's office be displayed or may it be replaced with a neutral or emotionally relevant (e.g., peaceful landscape)? in order to answer these questions participant studies should follow. here the versatility of vr facilitates such comparisons for evaluating participant's preferences in each case."
"in recent years, neurorehabilitation has found in telepresence [cit] a convenient and promising ally. telerehabilitation systems allow remote assistance, which may reduce the stress of a visit to the hospital [cit] or the pain in patients with acute or chronic pain . patients with reduced mobility can benefit from the possibility of remote interaction with their doctors and other patients, and also carry out this training from their home, under remote supervision. for doctors, telerehabilitation systems provide online remote monitoring of both the rehabilitation process, the clinical and physiological parameters of the patient, and the personal interaction in a virtual space [cit] ."
"the dynamical properties of the virtual ring, such as the mass and the friction with the wire, and the parameters of the pp bilateral teleoperation, such as the stiffness, can be set according to the capabilities of the patient."
"we use generif as an unlabeled data pool, which contains excerpts from literature about the functional annotation of genes described in entrezgene. in particular, each record contains a taxonomy id, a gene id, a pmid and a generif text excerpt extracted from literature. we randomly obtain 20 000 excerpts from human generif records with at most two records per gene id and the corresponding articles not associated with any go annotations (goa) record based on goa information available in iproclass (9) . we assume those excerpts have a higher chance to be negatives, assuming that if the excerpts are evidence excerpts, the corresponding article has a higher probability to be included in goa. the rationale behind this assumption is that the scope of the functional annotation in generif is broader than that of go. besides the scope of go annotation, generif also includes phenotypic and disease information that are not the subject of go annotation. note that this assumption does not guarantee all excerpts obtained to be true negatives."
"step 3. gene assignment based on proximity. for the remaining predicted positive sentences with no relevant gene mentioned, we assume that prior sentences would contain the gene information. for positive sentence s, we perform direct string matching using the gene list provided and the family name dictionary assembled in step 2 on all prior sentences belonging to the same section of s. gene hits are identified similarly as in steps 1 and 2. we then assign gene hits from the closest one (among all prior sentences with gene hits) to s."
"note that the use of goslim in system b1 aims to reduce the number of candidate go terms for consideration. as subtask b depends on subtask a, it is not clear how well our search-based methods for subtask b can achieve giving the gold standard output from subtask a. owing to the time constraint, we leave this interesting investigation as a potential future investigation."
"considering that the elements in â²â² q or â² q are not necessarily consecutive variables in â² q, they are reordered by introducing a matrix s as"
www.frontiersin.org in the hmd in real time. this option allows the patient to follow those changes occurring at the doctor's office in real time but at the expense of higher transmission bandwidths requirements. currently both systems have been integrated with our system [cit] .
"for tracking the hospital personnel, the same markerless tracking system based on kinect implemented for the patient is used. in those cases where higher movement resolution or reliability may be required, the optitrack system can be used in combination with inverse kinematics, so that only very few markers would be attached to the body [cit] ."
"equation (59) can be regarded as obtained by multiplying eq. (47) with the transposed cutjoint constraint matrix â²â² t b and replacing â² q by â²â²â² â² bq . it indicates that the solution of constraint equations for cut-joints can be directly obtained via quadric transformation upon system matrices for open-loop system, by using the corresponding cut-joint constraint matrix â²â² b . complicated solving for constraints and linearization are unnecessary in this method, and the resulting equations contain explicitly the design parameters. the suggested method can be used to greatly simplify the procedure of vibration calculation. furthermore, the suggested method is particularly suitable for sensitivity analysis and optimization for largescale multibody system. the proposed algorithm has been implemented in matlab, and is named as amva (automatic modeling for vibration analysis). the eigenvalue problem is solved using standard lapack routines. the flowchart of the proposed algorithm is illustrated in fig. 5 ."
"if p is the position and orientation of spring-dampers interconnected between b i and b j, similar to eq. (74), it can be obtained that"
"the above-mentioned sensitivity formulations are based on the topology of the multibody systems. particularly, eigen-sensitivity with respect to design parameters of mass and inertia, coefficients of stiffness and damping, position and orientation of connections are all derived analytically in detail. these results can be directly applied for sensitivity analysis of general mechanical systems and complex structures which are modelled as multibody systems."
"if p is the damping coefficient of spring-dampers interconnected between b i and b j, and b k and b l, it can be obtained that"
"topic features. these features are generated by latent dirichlet allocations (10), which can effectively group similar instances together based on their topics (11) (12) (13) (14) ."
"the gene ontology (go) provides a set of concepts for annotating functional descriptions of genes and proteins in biomedical literature. the resulting annotated databases are useful for large-scale analysis of gene products. however, performing go annotation requires expertise from well-trained human curators. owing to the fast expansion of biomedical data, go annotation becomes extremely labor-intensive and costly. thus, texting mining tools that can assist go annotation and reduce human effort are highly desired (1) (2) (3) ."
"for genes failed to be associated with any predicted positive sentence, we picked sentences containing the corresponding genes with the largest positive probability score (assigned by the lr model) to be the evidence sentences."
"bigram features. for each sentence, we generate a vector of bigrams by concatenating every two neighboring stemmed words in the sentence. we also have two boundary bigrams (sos_lw and rw_eos) where sos indicates 'start of the sentence', eos indicates 'end of the sentence', lw, the leftmost stemmed word and rw, the rightmost stemmed word."
"generally, there might be none or more then one joint in a multibody system. as shown in fig. 4, the topologies of constraints in multibody systems can be classified into five groups: (a) free, (b) scattered, (c) chain, (d) tree, and (e) closed-loop. free multibody system means that there is no constraint in the system. groups (b), (c) and (d) can all be regarded as general open-loop multibody system. since the spring-dampers do not change the topology of constraints in a multibody system, spring-dampers between two nonadjacent bodies are not displayed in the figure."
"1. starting from an initial list that contains top-ranked k goid, select goid one by one down the list until the score difference of current goid with the topmost goid is above threshold h. 2. aggregate goid frequency across all goes associated with a particular gene, and rank goid by frequency. 3. take the top-ranked m goid for each gene."
"considering a general closed-loop multibody system as shown in fig. 4 linearized odes in terms of absolute displacements are derived by using lagrangian method for free multibody system without considering any constraint, as shown in fig. 4(a) ."
the constraint equations for all the rest joints can be formulated similar to eq. (40). the constraint equations for the entire open-loop system can thus be integrated as
"the ring task. the ring task presents the implementation of a possible haptic task that could be used for evaluation of the patient's performance or as an exercise for rehabilitation. it integrates a number of elements (virtual objects, force-feedback, tracking of trajectories, force monitoring) that could be combined in the design of different ad hoc tasks. therefore, it should be taken as a model task for illustration."
"we apply lr to predict labels for each instance. in particular, we impose a constraint on model parameters in a regularized lr to avoid overfitting and to improve the prediction performance on unseen instances. note that lr assigns probability scores to each class. in a task with skewed class distribution, a threshold can be chosen to optimize the performance."
"the motion capture system is used to sample the body movements of the patient (in particular hands/arms). hence, patients see their movements reflected in the co-located avatar representing them from a first-person perspective. this enables visuo-motor synchrony between their own movements and the movements that they see on the avatar [cit] . additionally, synchronous visuo-tactile stimulation may be applied to enhance the feeling of virtual body ownership over the virtual character. we define visuo-tactile stimulation as the tactile stimulation received at the real body of the participant while a synchronous virtual stimulus acts on the virtual body. furthermore, allowing the patient to move within the virtual environment facilitates active tactile interaction with both virtual and remote persons and objects (see haptic interaction). the motion capture of the patients is critical not only for their embodiment within the virtual body representation but also for their representation at the hospital facilities, where they are represented by a virtual human displayed on a screen."
this research was supported by fp7 eu collaborative project beaming (248620). the authors thank arturo fuentes for the edition of the video s1 in supplementary material.
"the eigenvalue sensitivity can be expressed as the formulation is very simple. however, matrices â²â² m, â²â² c and â²â² k generated by using conventional methods are implicit functions of design parameters, such as mass and inertia of bodies, stiffness coefficients and damping coefficients of spring-dampers, position and orientation of spring-dampers and joints, and etc. that is to say, â²â² ij m, â²â² ij c and â²â² ij k are intermediate quantities instead of original design parameters. therefore, the existing sensitivity formula can not be directly used for optimization."
"the security of the physiological data processing and streaming is fundamental. there already exist several specific protocols to protect data in clinical as well as computer games networking environments. in particular, our platform is based on raknet (jenkins software llc), a cross-platform, open-source c++ networking engine for game programming. raknet offers secure connectivity among other security measures, guarantying data privacy. an alternative, based on experience in the simulator or games industries, is to run the session over a virtual private network that can provide security for all services that are running between the both sides."
"2. an open-loop constraint matrix is derived to formulate linearized odes via quadric transformation for open-loop multibody system, which is obtained from closed-loop multibody system by using cut-joint method. 3. a cut-joint constraint matrix corresponding to all cut-joints is finally derived to formulate a minimal set of odes via quadric transformation for closed-loop multibody system. sensitivity of the mass, stiffness and damping matrix about each kind of design parameters are derived based on the proposed algorithm for vibration calculation. the results show that they can be directly obtained by matrix generation and multiplication without derivatives. eigen-sensitivity about design parameters are then carried out. several kinds of mechanical systems are taken as case studies to illustrate the presented method. the correctness of the proposed method has been verified via numerical experiments on multibody system with chain, tree, and closed-loop topology. results show that the vibration calculation and sensitivity analysis have been greatly simplified because complicatedly solving for constraints, linearization and derivatives are unnecessary. therefore the proposed method can be used to greatly improve the computational efficiency for vibration calculation and sensitivity analysis of large-scale multibody system. sensitivity of the dynamic response with respect to the design parameters, and the computational efficiency of the proposed method will be investigated in the future."
"for successful telerehabilitation we argue that the environment should be highly physically and psychologically involving, meaning that the virtual experience is perceived as reality, i.e., ideally providing all sensory streams. the participant should become main actor in this reality rather than remaining as an external (although active) spectator as it is the case in video-games. this paper describes an integrated system for telerehabilitation, which results of the integration of state of the art systems developed within the frame of an interdisciplinary and multicenter research project. the key elements that make this system unique among its peers are: (a) full immersion (mental and physical) into the virtual environment, by means of representation of the patient with a virtual body that is felt as their own; (b) multimodal (auditory, visual, tactile) interaction with remote people (doctors, patients, nurses); and (c) physical presence of the participant at the remote place through embodiment agents (e.g., as a physical robot). the aim of such a setting is then to produce a new kind of experience, where the person can be physically embodied interacting with people remotely. the result of that integration work is a scenario with relevance in clinical and rehabilitation environments. we next present a set-up for neurorehabilitation that integrates these elements."
"through the participation of the go task, we investigated the use of distant supervision for detecting sentences for go annotation assignment and explored the use of information retrieval techniques for finding relevant existing goa and used them for assigning go terms to new articles."
"in the following sections, we will first describe our systems in more detail. then, we will present and discuss the official evaluation results. finally, we draw conclusion and point possible directions for future work."
"step 1. direct matching with dictionary lookup. direct dictionary lookup is done for each predicted positive sentence to detect whether there are relevant genes appearing in the sentence. if so, the corresponding genes found are assigned to that sentence."
"if the number of cut-joints in a general spatial closed-loop multibody system is c, the constraint equations for all cut-joints can be integrated as"
"are displayed or reproduced) in real time. this problem is wellstudied in the simulation and computer games domains [cit] ). the technology is transferable to the telerehabilitation domain. therefore, given the disparity of tracking and data acquisition systems available in the market, the safest way to assure that avatar data are displayed at both sides consistently is to compute all transformations (bones positions and orientations) locally and publish these data in a neutral, avatar-centered coordinate frame. therefore, we have developed a software platform to centralize the management of data, including data transmission and reading to/from the connected clients, in such a way that the sent/read avatar data are exact clones. all sensory data streams (avatars, virtual scene, physiological data, speech, haptic information, etc.) travel from one physical place to another in parallel. the software platform is unique amongst its peers as it is designed to support a wide range of heterogeneous platforms, whilst retaining the ability to record and analyze interactions between participants [cit] ."
"optionally, an additional data glove can be used for tracking finger movements. the glove records flexion strength of fingers and uses this information to bend the avatar's fingers individually, allowing opening/closing the virtual hand, grasping, or pointing to objects in the virtual environment."
"a medical scenario for remote rehabilitation has been implemented. the set-up has been conceived and designed for treating patients with motor deficits (e.g., stroke or parkinson disease) or with neuropathic pain in upper limbs (e.g., complex regional pain syndrome, carpal tunnel syndrome). the objective is first to carry out an evaluation (and later a follow-up) of the neurological state of the patient, and next to guide him/her through the realization of rehabilitation exercises. it is also important to provide the patient the feeling of closeness to the medical team, contact (including physical) and support, even when the patient is at home. this is achieved by mediating the patient's experience through immersive vr system (figure 1 ). once the patient is wearing the vr equipment, they are immersed in a hospital room where the medical personnel is actively represented. at the same time, the patient is \"captured\" from home and \"beamed\" into the hospital facilities, where his/her physical representation physically interacts with the hospital personnel. the specifications and technical details of this set-up for remote neurorehabilitation are described next. for a better understanding, the description of the set-up is divided into three subsections according to the two remote places (patient's home and hospital), and a common platform for data exchange."
"www.intechopen.com normal mode analysis and transfer function analysis were both performed in adams and amva for such a quadruped robot. as shown in fig. 10, natural frequencies and damping ratio solved in two tools are equal to each other. fig. 11 shows that results of transfer function computed in two packages are identical. it indicates that dynamic analysis of openloop spatial mechanism system can also be solved using the proposed method. the proposed method has also been applied in linear vibration analysis of a stewart isolation platform, which is a closed-loop spatial mechanism system with six parallel linear actuators, as shown in fig. 12 . the isolated platform on the top layer is connected with linear actuators via flexible joints. the lower end of each actuator is also connected with the base via flexible joint. based on previous finite element analysis, each flexible joint is modeled as spherical joint together with three-dimensional torsion spring-damper. and each linear actuator is modeled as two rigid bodies connected with a translational joint together with a linear spring-damper along the relative moving direction. therefore the system can be modeled as a closed-loop spatial mechanism system with 14 rigid bodies and 12 dofs. normal mode analysis and transfer function analysis were both performed in adams and amva to acquire vibration isolation performance of such a stewart platform. as shown in fig. 13, natural frequencies and damping ratio solved in two tools are equal to each other. fig. 14 shows that results of transfer function of displacement computed in two packages are identical. fig. 15 shows that results of time response of displacement computed in two packages are identical. it indicates that dynamic analysis of closed-loop spatial mechanism system can also be solved using the proposed method."
"in such a technology-based approach, factors such as the cognitive-emotional and social context should also be taken into consideration for successful participant-acceptance [cit] ). in particular, in-home patients often suffer from solitude and isolation [cit], additionally to their physical problems, what may aggravate their personal situation. therefore, the possible social role of the proposed set-up for patients with motion difficulties needs to be evaluated. a waiting room scenario for the patient to wait in prior to the doctor's appointment, similar to a real-life scenario, is envisaged. the patient, wearing the vr equipment, is virtually beamed into the doctor's waiting room and represented by his/her own controlled avatar. there he/she can move around and interact with other patients also waiting there, while waiting for his doctor's appointment. the assessment of the effectiveness of all these scenarios through participant studies is envisaged."
set the potential energy of the system at equilibrium positions to be zero. then the potential energy of spring ijs k can be formulated as
"presence of relevant genes. because relevant genes of each article have been provided, we also use dictionary lookup to check the presence of relevant genes in the sentence."
"multisensory integration based on visuo-tactile correlations is the most commonly used combination but not the only one. indeed it has been suggested that only the visual input of a virtual body co-located with the real one while seen from a first-person point of view is enough to generate that feeling of virtual body ownership to a great extent, as long as the multisensory contingencies do not break. the strength of the illusion is reinforced when, to the visual co-location, synchronous visuomotor correlations are provided, e.g., with the person controlling the body movements (arms, legs, etc.) of the avatar, who mimics her movements [cit] ."
the position and orientation of connection such as spring-damper and joint affect the dynamics of multibody system too. eigenvalue sensitivity about these geometrical design parameters will be derived in this section.
"in this section, we describe two systems that generated the first two runs of task b. the basic idea is to leverage existing goa to label new articles. in particular, we search for relevant documents (sentences, abstracts or full-text articles) that have existing goa to the target article, and then score and aggregate these existing goa to produce the goa for the target article. figure 1 gives an overview of system b1. we highlight external resources in blue and system modules with gray. next, we describe each part in detail."
"since matrices â²â² m, â²â² c and â²â² k generated by using the proposed method are explicit functions of design parameters, sensitivity analysis about design parameters can be easily carried out."
"the coexistence of several entities in the common state space can lead to inadmissible situations, called collisions. a collision occurs in a particular moment of time and involves two entities. it is not defined using the equality of a e function values in the same time: two entities may collide being in different states (for example, if they are too close to each other). therefore, a collision is defined using the following, domain-dependent function:"
"for instance, an intuitively admissible and consistent heuristic for a problem where each entity state is two-dimensional and represents the location of an entity on a planar surface is a multi-dimensional variation of the straight-line distance (sld) heuristic, i.e.,"
"in the us, the federal communications commission (fcc) ruling [cit] has obviated mandatory spectrum sensing in white space networks. instead, the ruling requires wsds to find spectrum opportunities at their respective locations from a central database. meanwhile, database administrators have been appointed by the fcc, and early services to help identify white spaces have been launched by spectrum bridge [cit] . in finland, the telecom regulator ficora issued a geolocation database service test license for tv white spaces to fairspectrum [cit] ."
this section describes the procedure and actions to run the web-based demo [cit] . the geolocation database and the log-a-tec sensor network are remotely accessed using secured https connections. the demonstration flow follows:
each entity can be manageable or only observable. manageable entities require planning and follow provided plans. observable entities provide information about aims or the current individual plan.
"the services are listed in order, representing the steps that a wsd must take to obtain service from the geolocation database. several timers are also implemented and used by the protocol, during operation:"
"was defined (in section 3) by wojciech turek. individual planning algorithms were adapted to the multi-entity environment and described by sebastian ernst and igor wojnicki (wavefront). the hybrid algorithm was conceived of by igor wojnicki and sebastian ernst, with application-specific contributions by wojciech turek and later described by igor wojnicki. experiments were conducted by igor wojnicki (scenario, complexity reduction) and sebastian ernst (base a-star performance measurement). all authors have read and approved the final manuscript."
"the description of the algorithm is as follows. the problem is defined by selecting the start state, v s, and the goal state, v g . the initial state contains the states of all entities at the beginning, and the goal state is composed of the desired location of each entity."
the solution to the given problem is a set of plans for every manageable entity. each plan consists of state changes feasible for the particular entity. the set of plans cannot cause any collision and have to handle possible failures in plan execution.
"1. remote setup each generator nodes are previously configured to emulate a wireless microphone signal with the characteristics described in section 6. they are represented as wireless microphones on the web interface, as shown in figure 7 . additionally, all seven sensor nodes are previously configured according to section 6. these nodes appears as green flags on the web interface. 2. generator selection the user remotely instructs one of the six generator nodes to broadcast a wireless microphone signal. for demonstration purposes, pmse emulation was set to channel 59. the wireless microphone symbol on the gui starts to blink. 3. sensing process when the user starts the sensing process from the web gui, all sensing nodes are remotely commanded to scan the spectrum in the tv bands. dvb-t channels with tv broadcast figure 9 gui of the demo, presenting the message exchange between a wsd and a geolocation database. http://jwcn.eurasipjournals.com/content/2014/1/210 signals are protected by the geolocation database and therefore not sensed to detect the presence of wireless microphone signals. for demonstration purposes, the frequency range is set to the same range as the generator nodes. however, the frequency span may be programmed to the full dvb-t range (470 to 790 mhz) if the sensor network is to be tested with real wireless microphone systems, located in the clusters area. the sensing threshold is user selectable from the web interface and was set to â93 dbm. when the sensing process is finalized, the network communicates the data to the tvws database in raw format for post-processing and also to the web interface, using the java api implemented for that purpose."
"we assume that a problem in the considered class is composed of a varying number of relatively similar sub-problems, which share the same environment and, therefore, need a common planning method. the sub-problems can be perceived as independent entities, each having different individual goals. the entities coexist in the same state space with time. each entity is in a particular state in every moment of the considered time frame. the individual goals of each entity can be represented as a desired state or a list of states that have to be achieved."
the proposed approach to the problem of controlling the entities when they fail to execute a given plan is to prepare alternate solutions in advance. a robust planning algorithm for the considered problem should assume the possibility of execution failure and create a multi-variant plan that covers exceptional situations. the wavefront algorithm can be used to provide such a multi-variant plan.
"let us assume the existence of a finite set of entities: e. each entity has an assigned target state (or a set of states) it wants to reach. the aim is to change the state of all entities into the desired target state as quickly as possible. in real-life situations, the state of an individual entity can be composed of multiple attributes, for instance representing the location of an entity (e.g., a robot), its speed and perhaps its battery level. for the purpose of this paper, only the components that are defined in a common domain (e.g., the location, which can be occupied by various entities at different times) will be taken into consideration."
"here, the maximum permissible eirp of a wsd is relevant for the outcome as shown in figure 5 . the top left picture shows the number of available channels between 21 and 60 if the maximum eirp of a wsd is assumed to be 0 dbm, whereas in the bottom right the number of available channels is shown for 30 dbm. as expected, the number of free channels drops significantly."
"the contents of the queries and response from the protocol need to be specified. a data model is required which enables the wsd to query the database while including all the relevant information such as geolocation, power characteristics, sensing capabilities, etc., which may be country, spectrum, and regulatory dependent. partially following an ietf proposal [cit], but adapted to the crew-tv project, the implemented geolocation database is able to interpret the data model and respond to the queries using the same data model that is understood by all wsds."
"the state space defines a notion of collision, which can be caused by two entities being in a particular state at the same time. the collision does not mean that the entities are in the same state. the intuition would rather be that the colliding entities are \"too close\", according to a particular metric defined in the space."
a sequence of edges is less ambiguous than a sequence of vertices for the cases where two edges e i and e j may join the same vertices.
"a collision neighborhood can be heuristically defined as a sequence in the plan, for which particular agents get close to each other at a distance smaller than some given d c . the particular d c value should take the properties of the world being modeled into consideration; these include the likelihood of slipping, the uncertainty of agents' behavior, their controllability, observability, autonomy, etc."
"additionally, a distributed sensing algorithm, running on the geolocation database, combines data coming from the sensor nodes (energy detection vs. position) to detect the presence of pmse devices and provide an estimative of its location. the algorithm is based on a logical or operation to combine information from each sensor. if one or more sensors detect an active wireless microphone, the geolocation database engine computes an estimate of the wireless figure 8 web interface of the demo, displaying two sensing nodes with spectrum measurements and an exclusion area in red."
"the problems considered in this paper can be addressed using both of the aforementioned approaches to planning. however, the complexity and time constraints will typically enforce the use of heuristics-based approaches."
"further research is mainly on improving the heuristics used for the identification of collision-prone neighborhoods and the determination of the collision sub-space sizes. furthermore, work is being done to adapt the algorithm to a quasi-continuous environment."
"to avoid interference with primary users, any sensor should ideally scan the overall uhf spectrum in real time, without decision errors, and report the results to the geolocation database. however, due to physical limitations of the outdoor testbed (sensitivity threshold, adverse weather conditions, resource sharing, etc), compromises have to be made when experimenting, compared for example to taking a simulation approach that does not take into account those limitations."
the wavefront algorithm allows one to find the shortest (least expensive) path in a graph from any starting point to a given goal. there are two steps to the algorithm:
"methods for managing entity groups are also receiving attention in contemporary research. many different management methods have been applied to specific problems concerning mobile robots management. some examples of the considered problems are formation control [cit] and cooperative search [cit] . to overcome the problem of real-time requirements, the solutions tend to simplify the planning algorithm or even replace it with behavioral controllers."
"the spirit of cognition in tvws usage lies in the idea that one of the secondary users of the spectrum knows the tvws and assigns one or more channels to a device for usage. to describe the potential given by tvws, the number of free channels for each location is a relevant parameter. we consider uhf channels from 21 to 60, so up to 40 dvb-t channels may theoretically be free. http://jwcn.eurasipjournals.com/content/2014/1/210"
"the performance of different planning methods is always dependent on the problem size. in general, solving a planning problem for a complex problem is considered a time-consuming task. this fact causes serious limitations in the applicability of these methods in certain classes of problems."
"the domain of planning is a very broad and important area of research. well-known solutions to the domain-independent planning problems are based on the notion of a search space, which has to be searched in order to find suitable solutions. the search space can represent a graph of cities and highways (where searching is relatively simple), but also multidimensional continuous spaces representing the locations of several moving mobile robots in time."
"the environment and the entities determine specific rules of changing the state over time. the rules can be perceived as the abilities of particular entities; therefore, they can differ between individual entities."
"one obvious solution to the situation of plan execution error would be to run the planning algorithm once more with the modified initial state of all entities. however, this approach is not possible in the considered problem: the time required for the planning algorithm to finish is too long. to solve the issue of execution unpredictability, a different approach to the planning problem must be involved."
"one of the most important features of the wavefront algorithm is that it provides a gradient map, which is a result of the vertex labeling process, which allows one to find an optimal path from any state to the goal by simply descending the gradient. this fact allows one to react immediately in any possible situation that can occur in the controlled group of entities."
"the field of planning has received enormous attention over the last few decades. the development of methods for domain-independent planning help to find better solutions to many real-life and abstract problems [cit] . it is hardly possible to name all possible applications for this class of methods; therefore, research in this area is definitely justified."
"the class of problems considered in this paper assumes a group of entities coexisting in a common environment. the aim is to continuously plan behaviors for all entities in order to reach the highest possible performance of their operation. the group of entities is not homogeneous in terms of observability and management capabilities. some entities may follow given orders precisely; some may merely provide information about their planned actions. the behavior of the entities is highly dynamic: changes in the observed state occur quickly, which means that there is not enough time to execute a complex planning algorithm between observing the change and having to provide control."
"both types of entities can fail to follow the specified plans. attempts to perform particular transitions between states can fail or result in different transitions with a given probability, which models uncertainty in plan execution."
"a wavefront-based approach seems to be the most suitable for such multi-agent planning with uncertainty, but its time complexity renders it less applicable. on the other hand, the a-star algorithm, having better time complexity, does not provide alternatives in the case that deviations from the plan occur."
"additionally, if the data cannot be collected in real time, sd card storage available in every device should be used, so we must account for the time needed to store and retrieve sensing data from sd cards, approximately 10 s. the control network in the log-a-tec testbed is based on a zigbee network, which occupies only one channel in the 868 mhz frequency band and offers a low transmission rate: on average, 1 kb/s transmission rate can be achieved, and the latency of the network is a few hundred milliseconds. combining all the time contributions, the overall process may take up to 3 min between the sensing request from the geolocation database to the reception and analysis of the sensing information. however, this time may be significantly reduced, down to 1 min, if the process skips non-crucial operations, such as configuration node verification or searching for free memory slots on the sd cards (in the last case, we simply delete previously stored data from the first memory slot and replace it with new sensing data)."
"hardware limitations on vesna nodes imposes a minimum sensing time of 60 ms per channel. moreover, the time needed for a sensor node to effectively detect a wireless microphone cannot be made arbitrarily small, a situation where the sensing algorithm could miss the presence of pmse devices. for sensing measurements, vesna nodes offer two preinstalled configurations:"
"one important aspect of the log-a-tec sensing network is to assess the performance of individual vesna nodes to detect wireless microphone signals. however, some of the performance metrics (probability of detection -p d and probability of false alarm -p fa ) may be calculated only when the signal to be detected, i.e., a wireless microphone, is effectively present during the experiment. however, wireless microphone radio signals are usually intermittent in time and space, and their spectral characteristics are dependent on power level and spectral bandwidth of the acoustic signal (human voice, music instrument, etc.). additionally, the relative distance between generator and sensors, combined with the signal attenuation caused by buildings or trees, plays an important role in the sensing process. thus, we need an effective method to control the signal source and to get useful conclusions on the sensors performance. to overcome this problem, we use a particular set of vesna nodes, equipped with signal generation capabilities for the uhf band. thus, two different vesna nodes are used for sensing experimentation:"
"in each step of the generated plan, an action must be performed, which means that a state has to be changed into another state, at defined costs."
"the core of the log-a-tec testbed consists of a sensor network containing approximately 50 low-cost nodes mounted on public lighting infrastructure, distributed between two clusters: one in logatec city center and the other in the industrial zone. each vesna node is installed with omnidirectional antennas on a light pole at 10 m in height and communicates with a coordinator node using zigbee communication module at 868 mhz. the relative distance between each vesna node ranges from 60 m up to 600 m, with different propagation loss scenarios between them. with ip connectivity, they can be remotely reprogrammed according to the needs of the investigated use case. each node hosts a gps module providing internal geolocation and precise reference timing capability. for storing large sets of data, it also incorporates a mini sd card up to 32 gb of memory [cit] ."
"a java application, located in the same server as the geolocation database, remotely accesses the testbed and controls it in two different modes: figure 7 web interface of the demo, displaying an active wireless microphone and three sensing nodes from the log-a-tec network."
"where g(v â² ) is the already known cost of achieving v â² from v s, and h(v â² ) is the value of the heuristic (estimate cost of getting from v â² to v g )."
"traversing the entire state space while labeling might be not feasible: it is too time-consuming or complex. thus, a hybrid approach is proposed here. the motivation for it is best presented with an example, which is planning for multiple agents, as shown in the next section."
"there have been some efforts carried out regarding state space reduction, e.g., using preferences [cit] . depending on particular cases, the reduction can be substantial; however, introducing another entity or factor of uncertainty can trigger the growth of the state space beyond the capabilities of the reduction process. there is also ongoing research regarding efficient graph processing solutions that take distributed and parallel processing into consideration [cit] ."
"let us assume a state space, containing a finite set of n states. as outlined in section 4.1, collective states will be used for planning purposes. therefore, while the basic idea of a-star remains unchanged, the number of states grows significantly, as each consists of a much larger number of dimensions."
"the proposed algorithm is verified by an example that involves planning for four independent entities in a two-dimensional world. computational complexity is reduced by a factor of 225, compared to the fully-robust plan."
"in order to find the maximum permissible eirp of the wsd, we search within the scenarios of interest (coand adjacent pixels, co-and adjacent channels) to find the pixel-channel combination that imposes the strictest restriction. we have to look at each such combination individually, compute the eirp that it permits, and select the combination that permits the lowest power. this is the eirp that can be allowed for the wsd. to estimate the maximum wsd transmit power in decibel-milliwatt, the contributions have to be put together, with the combinations from table 3 :"
"although [cit] s [cit], it is still being actively researched [cit] . as heuristic search methods are often used as the basis of state-of-the-art planners [cit], methods for constructing heuristic functions are currently one of the most-researched topics in artificial intelligence (ai) planning [cit] ."
"where c((k, p)) is a transition cost assigned to the edge corresponding to an activity. the above process is repeated for each element in c, and it subsequently continues as long as there are any vertices to relabel. this labeling provides a gradient map used in the subsequent step. the second step of the algorithm, which is selecting a path, begins with an arbitrarily-chosen starting vertex s and continues according to descending values of function g at subsequent vertices, being a gradient descent. the path p is given as an ordered set of vertices:"
"we have also implemented two experimental methodologies to use the testbed; one focused on coexistence studies with real wireless microphones (sensing only) and another for demonstration purposes (sensing and signal generation from the testbed). both are valuable methods for experimenters to assess the advantages of combining sensing and geolocation database access, when protecting primary users of the uhf spectrum."
"which returns a set of possible a e functions. the f a e function represents all limitations of the entity e, showing which states can be achieved from a given state in a given time."
"the problem of planning actions for multiple entities coexisting in a common environment, which has been defined in the previous section, can be solved with well-known, general-purpose planning algorithms. however, the algorithms must be adapted to the specific requirements of the problem. this section provides a detailed description of two such algorithms (a-star and wavefront), preceded by the general intuition on the subject of multi-entity planning, as opposed to classical planning for a single entity."
"different planning techniques are suited to different problems. however, most of the existing solutions are not suitable for the class of problems considered in this paper. a very short response time, planning problem complexity and high-quality planning results cannot be achieved by using these state-of-the-art approaches."
"as indicated in table 3, four possible scenarios describe the possible arrangements of tvws transmitter and dvb-t receiving antenna. with realistic assumptions on minimum distance and by applying appropriate propagation models, the propagation loss can be determined. as the distances are usually short, typically less than a few kilometers, simple propagation models, like itu-r p.1546-4 [cit] or extended hata [cit], that do not take into account topology can be used. for distances from 10 m to 10 km, these models are comparable to an even more"
"let s denote a state space. let us assume the existence of time, and let t denote the time. all dimensions of the search space and the time are discrete, which is the assumption of the algorithm presented in this paper."
"the scenario behind the proposed experimental methodology assumes that wireless microphone systems are not registered in a database; therefore, its protection completely relies on sensing. this is a common scenario in many eu countries. this section explains how the loga-tec sensor network is effectively used to detect pmse devices, and when requested, report to the geolocation database."
"the paper presents a brief state-of-the-art in order to properly position the research in the domain of planning and multiple entity control. the following sections provide a formal definition of the problem and possible solutions based on general-purpose planning methods: a-star and wavefront. then, the description of the proposed, hybrid algorithm is presented. finally, a case study is discussed, and the directions of further research are drawn."
"the total number of operations to establish a robust plan is proportional to: comparing to s w, this makes the problem over 225-times smaller. as a result, there is a plan in a multidimensional space providing optimal paths for multiple agents. the plan is multi-variant to some degree. while executing it, if there is any deviation from the plan within a previously-identified collision sub-space, there are means to return to the previously-optimal path without any additional calculations. however, this does not imply that in such a case, the plan is still optimal. if a deviation takes place outside of the collision sub-space, there are no means to compensate; thus, proper identification of such and their sizes is crucial."
"in this paper, we present a successful combination of a tvws geolocation database access with the sensing information from an outdoor infrastructured sensor network. we test the ability of the geolocation database to automatically create protection areas around detected pmse devices using real-time information from the jsi sensing network. we describe the development and implementation of a signaling protocol between master wsds and a geolocation database, using a web-based environment. the protocol allows any wsd to gain access to the services of the geolocation database by communicating over commonly used internet protocols, using a well-defined and secure access method. the communication protocol between the log-a-tec sensor network and the geolocation database, based on an api written in java language, is an essential tool to implement an effective and secure connection to successfully gather sensing data and send it to the geolocation database for post-processing."
"where ch â² is the wsd operation channel, and d is the separation distance between a dvb-t receiver and a wsd. combining propagation loss with other relevant parameters (antenna gain, feeder loss, polarization discrimination) determines the coupling loss between the wsd and the dvb-t receiver."
"1. 'sensing' mode: this mode is used for coexistence studies in tvws. here, the network continuously scans the uhf spectrum and reports the results back to the geolocation database. a single iteration takes approximately 1 min. with similar periodicity, the wsd contacts the geolocation database and update the list of available channels. 2. 'sensing and generation' mode: when using the second mode, one of the generator nodes is activated on a specific frequency, and all sensing nodes are commanded to scan the spectrum simultaneously. with this mode, the network emulates the presence of a wireless microphone in the vicinity of the sensors, at any time. this mode is an essential tool for demonstration purposes, when wireless microphone activity is needed."
"tv white space (tvws) frequencies are becoming a real world test laboratory of spectrum sharing. however, a cognitive white space device (wsd) operation within the ultra high frequency (uhf) bands may be permitted if (and only if ) it does not interfere with incumbent services, such as digital tv broadcast and programme making and special events (pmse) services, e.g., wireless microphone systems [cit] . wsds should either sense the presence of other signals or make use of a geolocation database to determine which spectrum is unused in the vicinity. recent studies have shown that the sensitivity of these wsd receivers needs to be very high in order to detect these spectral opportunities effectively, and indeed this task is difficult to accomplish with the existing mobile technology [cit] ."
"together with the number of considered entities, the number of state space dimensions increases, while the number of acceptable state transitions slightly decreases, assuming that no two entities can occupy the same location. they both influence the number of state transitions. it needs to be pointed out that suppression of state transition due to reduced acceptable state transitions is much weaker than their growth due to the increasing number of entities, resulting in an increase of dimensions. that is why planning complexity remains proportional to the number of state space dimensions."
"assuming that the original state s was represented by k values (dimensions), each collective state s â² consists of nÂ·k components. possible collective states encompass all combinations of individual entities' states, excluding the states for which the collision function evaluatesto one. such an approach creates a single, collective state space that does not contain any forbidden states. any plan created in such a space will be feasible. in the following sections, the collective states and the collective state space will be considered."
"which returns a set of possible modified activity functions with assigned probability. this function can be used for calculating the collision probability for a feasible plan. one way of solving the given problem would be to accept plans with the collision probability below a certain factor; however, this would result in highly inefficient plans. different approaches will be presented in the next section."
"this section describes the api developed to enable a reliable communication channel between the geolocation tvws database and the log-a-tec sensor network. this api is used to collect sensing information from the testbed area, on wireless microphone activity, and later used to compute exclusion areas on the geolocation database. we based the communication on a custom protocol, which is abstracted by a proxy server based on standard https protocol. this section also reports on the operational sequence to obtain sensing data from the log-a-tec sensor network nodes, using a specification and description language (sdl) diagram. http://jwcn.eurasipjournals.com/content/2014/1/210"
"the flowchart diagram presented in figure 2 gives the operation sequence to gather sensing data from a vesna node. each numbered block on the flowchart diagram is associated with the corresponding object/method/class from the api described in table 1: 1. the user selects the cluster (industrial zone or city center) and the nodes from where the sensing campaign will take place, the sensing time for each node, and the periodicity of the sensing process and triggers the communication between the geolocation database web server and the log-a-tec sensor network. the java class httprequest implement https post and get methods for a request-response protocol between the geolocation database (server) and the log-a-tec network (client) during the sensing process."
"tvws devices, according to cept report 24 [cit], are allowed to operate on a 'non-interfering, non-protected basis'. several means were discussed in several international forums [cit] to cope with this non-interfering demand, among them are sensing and geolocation. for the geolocation scenario (with or without sensing), besides the equipment for the wsd to locate its own position, a database is required that provides data on acceptable transmit power for the possible channels at the requested location and time. this section describes how to calculate the data, i.e., the acceptable transmit power for the wsd."
"the labeling provides a guideline to select the shortest path. let us assume a search space given by a graph, which is invariant to multi-dimensional or collective issues mentioned earlier:"
"the considered class of problems is defined in an abstract way. the particular problem definition requires providing information about the space in which the entities exist, about the possible behavior of entities, the method of system control, the performance evaluation function, etc. there are many real-life problems that belong to this class. hardware entities, like mobile robots or cars [cit], require a method for motion coordination, which has to guarantee safety. virtual entities, like computational tasks executed on modern, heterogeneous hpc (high-performance computing) systems, require very efficient methods for resource assignment."
"services not considered in the current implementation, but defined in ietf paws, are the database discovery and master (or slave) wsds enrollment in the database."
"the cd function is a heuristic. it should consider domain-specific properties of the agents, which reflects the likelihood of collision. increasing the cardinality of s requires more calculations, but makes the plan more robust at the same time."
"the main objective of this protocol is to allow a wsd to request spectrum from the geolocation database and retrieve a list of available channel to operate as a secondary user, as shown in figure 1 . the paws protocol is currently an internet-draft (version 14) and still under development. the services are accessed by the master wsd using get and put requests over the internet. operations are only initiated by the wsd, with a response from the geolocation database. this eliminates the necessity of the geolocation database to initiate communications with the wsd. we defined separate requirements for slave wsds and master wsds. the protocol enables a master wsd to complete the following tasks:"
"there have been several experiments conducted, with a variable number of dimensions, regarding different strategies for both collision sub-space identification and its size. the number of dimensions varies from four to eight. the collision sub-spaces are identified based on their distance among entities ranging from one to three. additionally, such a collision space is grown by a factor of zero to three, to increase the robustness of the plan. depending on the particular experiment and comparing to the wavefront, the proposed method offers a speedup of up to 225. the plan in such a case is still robust, attaining the ability to recover if deviations happen. comparing its performance with the a-star results in a speedup of down to 0.24. the above numbers indicate that while the proposed robustness costs computational time, this cost is significantly lower than it would be for the total robustness delivered by the wavefront. this enables the proposed approach to be applicable to real-world cases."
"the number of states, or transitions, if there are any unreachable states, is the core factor of computational complexity, no matter what planning algorithm is used. the proposed hybrid approach is a solution that balances complexity with uncertainty. the proportion is kept by the heuristic function cd described earlier. there are two factors that influence the computational complexity of the proposed algorithm:"
"the presented multi-entity a-star algorithm can be used as an efficient method for finding optimal plans for multiple entities coexisting in a common environment. however, the algorithm is not suitable for addressing the problem of plan execution uncertainty. the class of problems considered in this paper includes different levels of entities' autonomy, predictability and controllability. therefore, deterministic execution of the calculated, optimal plan cannot be assumed."
"in order to estimate the maximum transmission power of tvws devices in the uhf bands, a methodology aspect has to be considered. the methodology used to compute tvws maps, i.e., the maximum acceptable transmit power for a given location and tv channel, follows ecc report 186 [cit] directives, as presented on the diagram of figure 3 ."
"for each spatial unit (x, y), and for each digital video broadcast (dvb-t) channel ch, the wanted signal strengths e(x, y) and location probabilities q(x, y) can be calculated with a dvb-t coverage calculation software. location probability describes the probability that a broadcast reception is possible at a given location:"
"the application protocol utilizes the following protocol stack for communication between the geolocation database and a master wsd: several programming languages, from php and javascript, were used to develop this implementation of the protocol. mysql is the technology used to implement all the requirements for the database: to store geolocation data, information about all master wsds (registration process), and the slave wsds (serial number only). in this context, the main objective of php is to access a mysql database, where the tvws geolocated data is stored. javascript language is used to control the user web interface and the google map api."
"from the input data (q(x, y) and e(x, y)), the total nuisance field u(x, y) can be determined. the nuisance field describes the acceptable interference level of the wsd at the location of a broadcast reception antenna, comprising of noise (n) plus a minimum signal to noise ratio (snr) and the unwanted signal contributions (n u dvb-t ) from other dvb-t transmitters:"
"the collision sub-space size r c should take physical aspects of the agents into consideration. these include the maximum and minimum speed and acceleration, mass, etc."
"planning for multiple agents is not different from planning for a single agent, except that the number of dimensions in the search space increases. there are also some side effects that need to be taken into consideration. the one considered here regards possible collisions among agents, due to uncertainty. these might happen if the agents are allowed to deviate from the plan; there might also be different reasons for this situation, such as limited controllability, predictability and observability."
"the main contribution of this paper is an abstract description of the planning problem for autonomous entities and a proposed method for solving it in a discrete search space. a case study is provided together with detailed solution analysis, which proves correctness and efficiency."
"more neighborhoods make the solution more robust, and this takes more uncertainty into account. with larger collision sub-spaces, more significant deviations can be covered. however, the increase of both the number of neighborhoods and their size makes it necessary to perform more computations to synthesize the plan."
"power of the wsd. for a wsd with maximum eirp of 30 dbm, limited by the overload threshold on the dvb-t receiver, a radius of 30 km is adequate [cit] . the pixels outside the circle are assumed to be unaffected from the wsd. when we consider adjacent channel interference, the radius will be significantly smaller. given that the difference in the protection ratios can be up to 73 db, as indicated in table 2, the radius should be 10 to 15 times smaller, and it was set to 1.5 km."
"establishing a robust plan for multiple entities might easily lead to combinatorial explosion: it renders the planning process infeasible, especially when time constraints have to be met. the paper proposes a solution to this problem by providing a hybrid planing algorithm, based on a combination of the the a-star and wavefront algorithms. a-star is used to establish a single, optimal plan in a multidimensional space, encompassing the states of all entities. neighborhoods of states in which a need for robustness is anticipated are identified. at each such neighborhood, a sub-space, called the collision sub-space, is defined. each such sub-space is covered by the wavefront algorithm to calculate all possible plans to return to the original path provided by a-star. both the identification of the neighborhoods and the size of the collision sub-space are provided based on heuristics. as a result, a partially-robust plan is established."
"this definition (using unique edge identifiers) allows the situation where we have more than one edge between two vertices, which can be important for some planning domains."
"microphone location through triangulation and creates an exclusion area around it; furthermore, the corresponding dvb-t channel is removed from the list of available channels for that area [cit], until the next sensing campaign does not detect wm activity."
"we compute tvws availability and populate a geolocation database in slovenia. preliminary results of the available tvws channels investigation in the logatec area were presented, and will be used as a case study scenario in the performance evaluation of the tvws allocation techniques in future trials."
"which returns one when two different entities cannot be in particular states at the same time. then, we can define a function for detecting inadmissible situations in activity functions:"
"a good indication of the complexity of a-star-based search complexity is the number of states that have to be considered before the goal is reached, i.e., the number of heuristic function computations for candidate states. for the given example, assuming a von neumann neighborhood and a straight-line distance heuristic, the number of these operations is:"
