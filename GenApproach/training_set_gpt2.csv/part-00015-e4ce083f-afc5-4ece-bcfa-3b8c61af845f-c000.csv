text
assistive technologies should be supported; there should be a standard where you log in once and access many different websites; students developed some packages outside of moodle and uploaded using scorm.
"a student in any field has many ways of learning and being taught. in order to improve these methods, it is important to understand students\" levels of metacognitive knowledge and skills. metacognition, briefly defined as the awareness and understanding of one\"s thought processes, is fundamental in developing effective reading comprehension and problem solving skills and strategies. these skills show how well students are able to solve complex tasks such as reading comprehension. there is agreement among researchers that variability in reader characteristics can be used to partially explain individual differences in reading comprehension performance [cit] . the process of reading is greatly influenced by the beliefs, attitudes, and values that readers possess. we know, for instance, that how students feel and know about their own cognitive and metacognitive abilities and skills affects whether they succeed or fail in school."
allows quizzes only for self-assessment purposes with no grading; allows you to randomise your test questions and answers; sends an e-mail notification to the instructor upon completion of test; importing questions is a useful feature; online tests opens and closes at a specific time; you can give back feedback immediately or delay feedback.
"course content delivery and management educators reported that this facility allowed the use of multi-media content namely audio, video, podcast, and animation to scaffold the learning process. in addition, this facility allowed the use of links to external sources. educators at both institutions agreed on the usefulness of the content creation and delivery/management subthemes, which confirmed the need for the content authoring, delivery and management tools and features described in table 6 . the 'course content delivery and management' subtheme was clearly found to more useful than the 'content creation' subtheme at both institutions. some of the data extracts pertaining to the usefulness of this subtheme were:"
"online marking tools allow lecturers and teaching assistants to assess student work online [cit] . functions include assessing paragraph questions, assessing and loading marked assignments through the assignment drop box, and assignment feedback through annotations [cit] . online grade book"
communicating in mathematics using forums is cumbersome; need for separate channel of communication or forum for co-teachers on a course or module; should resemble a face book [sic] type of environment.
copying courses and rolling them over is supported; supports hiding of courses/documents being developed; educators need to structure or organise their courses using a pre-defined format such as weekly or topics.
"to make a decision tree, the amount of information needed to classify the dataset is calculated. then, the amount of information needed to classify the dataset after a split using each attribute is calculated. the information gain is defined as the difference between information needed to classify before the split and after the split. the attribute with highest information gain is used for the split at the root node. the process is then repeated with the remaining attributes until all are processed and the tree is grown."
"of a given rule has more than one clause, fuzzy operators are applied to obtain one number that represents the result of the antecedent for that rule. it is possible that one or more rules may fire at the same time. outputs of all rules are then aggregated. during aggregation fuzzy sets that represent the output of each rule are combined into a single fuzzy set. fuzzy rules are fired in parallel, this is one of the important aspects of a fls. in a fls, the order in which rules are fired does not affect the output. the defuzzifier maps output fuzzy sets into a crisp number. given a fuzzy set that encompasses a range of output values, the defuzzifier returns one number, thereby moving from a fuzzy set to a crisp number. several methods for defuzzification are used in practice. they include: the centroid, maximum, mean of maxima, height, and modified height defuzzifier. the most popular defuzzification method is the cendroid defuzzification method. it calculates and returns the center gravity of the aggregated fuzzy set."
"studies have indicated that despite increasing adoption rates, the full potential of a vls for online teaching and learning has not been achieved, with lecturers using a limited set of functions [cit] . vls usage amongst academics in higher education is influenced in some measure by a number of factors, which includes system functionality [cit], and non-functional characteristics [cit] . the term non-functional refers to desired characteristics that a system must possess that may not be related to a specific function such as security or usability. the application of the technology acceptance model (tam) in previous studies has shown that information system usage is preceded by behavioural intention to use, and the determinants of behavioural intention are perceived usefulness and perceived ease of use [cit] . while previous studies confirm the influence of perceived usefulness on adoption and usage of information systems, this construct has been applied to systems or to information technology applications as a whole and rarely to individual function/feature sets [cit] . hence, there is limited research on educators' perceptions on the usefulness and importance of vls functionality and non-functional characteristics, which has implications for system usage behaviour. this article attempts to address this gap by focussing on two key research questions:"
"the literature shows there is limited research on educators' perceptions of vls quality characteristics that are deployed in higher education. hence the aim of this article is to understand educators' perceptions of vls functionality and non-functional quality characteristics using the iso 9126 external software quality model, and respectively measuring these characteristics using the constructs of perceived usefulness and perceived importance."
"once the system has been implemented, the test data can be run through the fis to validate the data. by using the command line in matlab, it is possible to test each value and the system returns the resulting class. by comparing the predicted classes to the actual classes, an accuracy rating can be determined. overall accuracy was calculated by taking the number of correct classifications divided by the total number of samples."
"next, we measured the times to transfer data between the cloud storage and the computing nodes (table 3) . we noticed that uploading data to the compute machines is faster on the private infrastructure, due to the higher, unshared bandwidth available. regarding the two types of vms tested in azure we see a gain in performance and stability for extralarge vms for downloads only. we argue that this is due to the exclusive access to the network interface and the additional memory available that allows larger network buffers to manage incoming data and."
"the second step consists of moving the data to be processed from the users' local storage into the cloud. to do this, data is first transfered to some cloud accessible storage (e.g., the cloud storage service if available), which is a one time operation. then, for each run of the application, data is transfered from this storage to the computing nodes. once the processing is finished, the data follows the reverse path. the cloud storage services were the azureblobs in azure and the cumulus repository in nimbus."
"grade book organises, records, and publishes marks; allows students to view/monitor their progress; allows tracking of students grades in a course over time; generates statistics per question; allows overriding marks for blog posts; assigns group assignment mark to all group members; has graphing ability to depict results."
allows group assignments; peer review of assignments at postgraduate levels is useful; controls deadlines for assignments; return assignments with comments to students; gives confirmation of online submissions; has ability to turn the assignment back for further work; has digital box for assignment submission; uploading assignment question file(s) is a nice-to-have feature; useful to have an online repository of all student assignment submissions with comments and final grades; allow single or multiple files assignment submission.
"online assignment the online assignment sub-functions were welcomed by educators at both institutions. peer reviews of assignments were reported to be of particular importance to educators involved in postgraduate programmes where fellow students were required to provide constructive criticism of their peers' work. in addition, educators assigned grades to peer reviewing of assignments so that students took this activity seriously. some of the data extracts pertaining to the usefulness of this subtheme were:"
"after predicted rating of current user ub on item ii is gained according to equation (4), computational formula of positive credibility between user ub and user ua is:"
"we chose nimbus as a representative private cloud since it was designed to address the needs of the scientific community in multiple contexts, ranging from high-energy physics to bioinformatics. nimbus is an open-source infrastructureas-a-service cloud platform, which enables clients to lease a set of virtualized computational resources and to configure customized environments. the architecture of a nimbus cloud is based on several modular components, on top of which new modules are added to enable easy cluster configuration, interfaces to other iaas clouds or building paas clouds. the workspace service provides an implementation of a compute cloud allowing users to lease computational resources by deploying vms. a complementary tool, cumulus, provides an implementation of a quota-based storage cloud, allowing multiples implementations and playing the role of the front end to the nimbus storage repository for vm images. nimbus is an interesting option for scientists since it provides functionality well beyond homemade scripts: it offers a structure and apis for launching, configuring, and monitoring applications."
"course authorisation tools are used to assign specific access privileges to course content and tools based on specific user roles, namely students, instructors, teaching assistants (wcet, n.d.)."
"standards standards compliance implies that the system conforms to \"standards, conventions or regulations in laws and similar prescriptions\" [cit] . scorm is a \"set of technical specifications that enables sharable, durable, and reusable web-based learning content\" [cit] . the openid standard allows to access many websites without having to create new passwords for each website (sakimura, bradley, jones, de medeiros, & jay, n.d.) ."
(1) standard cosine similarity; user's rating is abstracted as vector in n dimensional item space and cosine values of vectors are used to measure similarity between corresponding users. cosine similarity between user a and user b is:
used to communicate new deadlines or new arrangements and there is a record of that; you have got 24/7 access so you can post and i see the post via my e-mail.
"online grade book the grade book was met with mixed reactions. while many of the educators interviewed at both institutions acknowledged the usefulness and value of having a grade book in a vls, it was the least used component of blackboard and moodle. a possible reason for this finding is that many educators are familiar/competent with the use of a spreadsheet tool for recording marks/grades, performing statistical analyses, and calculating class and final marks. educators who have not been trained to use the grade book tool may therefore be less confident to use an unfamiliar tool for performing the same functions. some of the data extracts pertaining to the usefulness of this subtheme were:"
"access tools directly or access via links; seamless integration of tools rather than being separate entities; open architecture is needed; students should be able to choose a text only website, integration between the vls and the university's website, the library, the publishers, online modules."
"we used c4.5 induction tree, a variation of the id3 induction tree, to analyze the datasets. both the id3 and c4.5 induction trees were proposed by quinlan [cit] . id3 and c4.5 are very similar methods, but have a few differences. for example, the c4.5 algorithm allows the usage of both continuous and discrete attributes, whereas the id3 algorithm has difficulty dealing with continuous data since it is more intensive to find a proper split on this kind of attribute [cit] . tree classifiers use supervised learning methods to organize data results into a hierarchical tree, with each node correlating to a different attribute. the possible values of each attribute become the branches that lead to child nodes. each node acts as a separate decision, and leads to a class at bottom of the tree, or the leaves. these trees act as multi-stage classifiers and are more efficient than single-stage classifiers since decisions are made at multiple levels and reduce the computational load [cit] . by selecting a leaf node and traversing up the tree recording attributes and decision values until the root is reached, the rule can be created by listing those conditions. the id3 induction tree algorithm has proven to be effective when working with large datasets that have a large number of features where it is inefficient for human experts to process. these rules are also clear and easy to understand to the average user. induction trees also have low rates of error when classifying data with noise as long as the noise rate is not extremely high. when dealing with errors in a single attribute or multiple attributes, the tree is still able to find enough information to branch on, even if the error rate of the data are high [cit] . while simple decision trees for small datasets can be created quickly by a user, large datasets with many attributes would make user creation less than ideal. induction trees can handle large datasets with multiple attributes easily with little computational power needed to produce a simple decision tree."
"blogging responses on the usefulness of this tool varied as only a select few educators used this tool in their courses. the educators that used blogging found it to be useful for student participation in discussions, and contributing to the learning of a topic or subject area. educators that had no prior experience of using the tool agreed that it was potentially useful. some of the data extracts pertaining to the usefulness of this subtheme were:"
"electronic whiteboard this tool was rarely used and reported to be less useful than the other communication tools. an educator at university a was cited as having attempted to use the tool but could not get it to work. this tool was used to a limited extent at university b and reported to be useful for drawings and annotations for distance learners. another view expressed was that it would be more useful for lessons rather than lectures. some of the data extracts pertaining to the usefulness of this subtheme were: presentation on whiteboards are used; we annotate on the presentation with extra drawings, extra explanations."
tracking student participation in online activities; track students contributions to discussions supported; moodle gives you activity reports; graphs that moodle generates that tells you the hours they spend on moodle per week; when they logged in and logged out.
"course management was regarded as useful by interviewees at university a and university b. educators found the use of authorised courses, course backups, password protected courses and selective release of course content to be particularly useful. some of the data extracts pertaining to the usefulness of this subtheme were: supports changes, archives and back up of courses; records marks; supports online surveys; authorised courses are very useful; all our courses have passwords on it; can selectively release course content."
"should give students more privileges, their own classroom; lecturers should be given rights to reset password automatically; limit access to registered users; most of our courses have guest access."
"the 'usefulness of student involvement and productivity tools' theme covered a number of subthemes, which are described below. educators at both institutions agreed on the 'usefulness of student involvement and productivity tools' theme, which validates the need for the corresponding tools and features of described in table 5 in the appendix."
"fuzzy inference systems employ rules. however, unlike conventional expert systems, a fuzzy rule localizes a region of space along the function surface instead of isolating a point on the function surface. for a given input more than one rule may fire, in a fls multiple regions are combined in the output space to produce a composite region. a general schematic of a fls is shown in figure 2 [cit] ."
"we have selected several metrics, structured according to the execution phases of scientific applications. a first step consists of deploying the customized environment and fetching the initial data. in a second phase, the application is executed, so we are interested in the computation performance and the efficiency of the network transfers. we then give some considerations about the estimated costs."
"the educators believed that student profiles were useful for students, as it allowed them to upload their pictures, edit their information, and conduct online discussions. some of the data extracts pertaining to the usefulness of this subtheme were:"
"quality in use is the user's view of the quality of software, and is measured in terms of using the software, rather than properties of the software itself. quality in use is the combined effect of the software quality characteristics for the user [cit] .the iso 9126 iso/iec 9126 is an international standard, which categorises quality as six characteristics, namely functionality, reliability, usability, efficiency, maintainability and portability. the characteristics of maintainability and portability were excluded as they require the expertise of trained it professionals. the functionality characteristic represents the functions that satisfy stated or implied needs in the system [cit] . the reliability quality characteristic refers to the capability of the software to retain its performance level under specified conditions [cit] . efficiency refers to the capability of the software to provide required performance in relation to the amount of resources used, under specified conditions [cit] . usability refers to capability of the software to be understood, learnt, used and liked by users, under specified conditions [cit] . security is included as a non-functional characteristic, which refers to the ability to prevent unauthorised access, whether accidental or deliberate, to programs and data [cit] . this article focuses on educators' perceptions on the quality characteristics of a class of online education technologies that gonella and pantò (2008) reported as being widely adopted in universities."
"in k nearest neighbor, positive credibility can be gained through the difference value between rating of nearest neighbors on certain item and current user's predicted rating on the item. set iab as sharing rating item set between user ub and user ub and ua is one of the nearest neighbors of user ub,"
"this dataset consisted of all the marsi data, having 30 questions as attributes and 1636 records. this dataset determined an aggregated level of reading strategies for the reader by considering all reading strategies to classify them into three categories: low, medium, or high. after running this dataset through the c4.5 induction tree, nine rules were selected, three from each class of high, medium, and low, that represented the strongest rules that applied to the largest sections of the data. to find the strongest rules, the rules were ranked by number of samples classified for each class, and the three highest rules that classified the most data were selected. these rules for the marsi overall dataset can be seen in figure 6 . these rules were derived from the tree produced from the c4.5 algorithm. the subset of the top layers of the tree is shown in figure 7 . from the tree, we can see that the attribute \"analyse23\", corresponding to the 23rd question in the instrument, is the attribute that is the most influential on overall reading strategies. using the c4.5 tree produced, the data can be run through the classifier again to determine accuracy. after running a confusion matrix on the marsi overall dataset, it was found to correctly classify the data 83.33% of the time. those rules were then taken and input into a fuzzy inference system. after processing all the data through the fis, it was found that the fis classified data correctly 72.55% of the time. a tool known as a confusion matrix can show induvial accuracies for each classification. the confusion matrix for the data after being run through the fis is shown in table 1 ."
"the 'usefulness of student tracking and progress tools' theme comprised two subthemes, which are described below. educators at both institutions agreed on the 'usefulness of online tracking of student participation and progress tools' theme, which confirmed the need for the corresponding tools and features described in table 8 in the appendix."
"intel (r) core (tm)2 duo cpue7400 with dominant frequency of 2.8ghz, memory of 2g and hard disk of 320g. operating system is windows professional sp3 realized by visual c++ language. adopt public data set movielens adopting common measurement and recommended algorithm accuracy as simulation object to select public partial data sets, which contains more than 100000 true score records from 943 users for 1682 films."
system should allow for peer comments and not necessarily for grading purposes as students are not knowledgeable enough to grade other students work; should be able to set custom scales for grading; set up marking criteria on online marking forms where final grades are awarded.
"student participation some educators required tracking of student participation in threaded forums and chats in an online environment. it was suggested that a vls should provide activity reports, usage statistics and frequency of student participation in various forums to assist the lecturer in assigning final student participation marks. some of the data extracts pertaining to the usefulness of this subtheme were:"
"the 'usefulness of communication tools' theme for the two universities covered a number of subthemes, which are described below. educators at both institutions agreed on the 'usefulness of communication tools' theme, which confirmed the need for the corresponding communication tools and features described in table 4 in the appendix."
"while commercially available public clouds could be used for scientific computing, they were not designed with such applications in mind. several studies presented in the previous section showed that many public cloud platforms have slow network connections between the vms, which often becomes a bottleneck for running some scientific applications. also, the multitenancy model of the cloud implies that applications from different users can share the same physical machine and the network infrastructure, leading to performance variability. in this paper, we choose a platform-as-a-service public cloud, microsoft's azure. it allows scientists to install applications, customize the computing environment and it provides a comprehensive api for building applications: role based workers, message queues, tables, blobs etc. the computing service of azure is composed of two types of nodes: web and worker roles, typically used by scientific applications as a master-slave / replicated workers model. the provided resources are described in table 1 : there is a one to one relationship between the virtual cores and the physical cores and an extra large vm acquires the whole physical machine. the storage service of azure is composed of 3 systems: blobs, tables and queues. this work will concentrate on evaluating the azure blobs as this is the default mechanism to store large sets of scientific data."
(3) pearson correlation coefficient; average rating value of current item shall be deducted from item set iab of sharing rating between user a and user b to amend calculation of similarity. similarity calculation of pearson correlation coefficient is as follows:
"cloud computing has recently emerged as a viable alternative to the acquisition and management of hardware or software: it allows resources to be dynamically provisioned on a fine-grained, elastic basis, in a scalable way. as this model relieves users from the resource management burden, more and more applications are being ported on clouds. with multiple cloud providers and technologies emerging, developers are faced with an increasing difficulty to evaluate and compare the various offerings and to decide which model or platform to use for their applications. considering cloud service delivery, the options are threefold. private clouds are built using in-house resources from a local cluster or data center: this option favors a high degree of control on resources, but requires a certain management overhead. in contrast, public clouds are not restricted within an organization: multiple users typically share resources and environments for application development and deployment. these services are proposed by commercial providers using a payper-use model. hybrid clouds combine private and public cloud infrastructures, possibly from several providers."
wikis this tool was used by a select few educators to promote collaborative or group work in their courses. the findings indicated that wikis had not taken off. possible reasons for this are that undergraduate levels of study have large student numbers or that educators have not adopted a constructionist approach. one of the views expressed was that it could be useful for higher levels of study. some of the data extracts pertaining to the usefulness of this subtheme were:
"difference between predicted rating of user ua on item ii and actual rating of user ub on item ii shall be used to weaken the credibility value. where,  is the corrected parameter which prevents denominator to be 0. for judgment of popular resources, the thesis adopts the following strategies. set h is popular resources set at present, for any user ii:"
"credibility shall be introduced into recommendation system and its specific definition is: a kind of subjective recognition degree of users to be recommended on authenticity and reliability of k nearest neighbor and reverse k nearest neighbor selected by system. it is a kind of \"user-user\" and \"user-group\" subjective credibility relation. introduction of credibility is a kind of punitive correction on similar users actually and it aims to weaken influence of unrelated users on recommend results in k nearest neighbor under data sparsity condition. in the thesis, position credibility and negative credibility are introduced to correct k nearest neighbor and reverse k nearest neighbor respectively."
"in the third phase of the study, surveys were used as a follow-up technique to interviews with a broader sample of the target population and served to confirm the results of the qualitative study. a total of 107 completed questionnaires were returned from both universities. a five point likert scale was used in the design of the survey questions ranging from strongly disagree (1), disagree (2), neither agree or disagree (3), agree (4) and strongly agree (5). survey questions measured educators' perceptions of the usefulness of the vls functionality for online teaching and the importance of vls non-functional characteristics as depicted in figure 2 . the cronbach's alpha technique was used to measure the reliability or internal consistency of the vls functionality and non-functional characteristic groups, which were all above 0.7, thereby permitting analysis. descriptive statistics, namely the mean and standard deviation was computed for each of the groups. a quantitative analysis of the perceived usefulness of vls functionality groups (namely communication, student tracking, course administration, course content, assessment, and student productivity and involvement) and the perceived importance of non-functional characteristic groups (namely usability, reliability, efficiency, security, flexibility and standards) revealed that the mean scores were all significantly above a neutral score of 3. hence, there was significant agreement amongst the educators on the quality characteristics of blackboard and moodle formally adopted at the two universities."
"our next experiments examine the tcp throughput as a measure of path capacity because tcp is the dominant traffic type of cloud applications. path capacity impacts data transfer throughput and congestion events can lead to errors or delayed responses. we have done several thousands transfers of 512mb of data, with one minute delay between them, within a period of a week and we grouped them into 24 time intervals. in figure 3 we present the throughput of the tcp communication and its variation in time. the public cloud delivers an almost double performance although with a higher variability. nimbus is penalized by the network virtualization approach implemented for deploying cloud clusters. however, it should be taken into account that when it comes to private infrastructures, users might have the option of choosing a better interconnectivity (10 gigabit ethernet, infiniband), which is not available in public clouds like azure. naturally, not sharing the network card significantly increases the throughput of the extralarge instances."
"support for a variety of data formats such as word and excel documents; use of videos in a pdf; post links to external resources; the use of multi-media content namely audio, video, podcast, and animation to scaffold the learning process; support embedding of code from providers of multimedia content; play pdf slide shows; use video and or podcasts to explain mathematical concepts."
"indeed, the development of metacognitive beliefs about reading and the understanding of the parameters and complexities involved in reading tend to develop whenever and wherever students receive instruction in reading."
"the public clouds provide compute and storage resources on demand, using a pay-as-you-go model. they offer wide availability and economies of scale that only very large data center operators can achieve. several commercial offerings exist from providers like microsoft, amazon or google. building on their elasticity, scientists have started to migrate applications in the public clouds. these allow users to customize the environment and replicate experiments through the use of vm images, while proving an efficient collaboration tool that enables the global sharing of information."
"online real-time chat this tool was less used at both institutions. educators did not perceive this tool to be highly useful as both institutions offer face to face contact with the students. they believed that this tool was useful for supporting student communication, particularly in cases of part-time programme offerings. data extracts pertaining to the usefulness of this subtheme were:"
in this section we evaluate the azure and nimbus clouds with respect to the previous metrics. we then provide a cost-estimation of running scientific applications in a private cloud and compare it with the costs of commercial clouds.
"the qualitative findings and analysis of educator perceptions on the usefulness of the following suite of vls tools, namely communication, student productivity and student involvement, course administration and management, student tracking and content tool features is presented in this section. a summary of themes and subthemes were coded from all the interview transcripts, covering the perceived usefulness of vls tool functions for online teaching."
two functions of introduced parameters are: (1) modify weight value proportion of nearest neighbor and reverse nearest neighbor to adapt to different demand to apply or not to apply data set; (2) prevent final evaluation score from exceeding upper limit of initial score value. two sub-algorithms in equation (11) can be described as influence of product of the sum of score of item not yet scored by k (reverse) nearest neighbor and average score deviation of the user multiplying by positive (negative) credibility relative to the current user on average score of item not yet scored.
"(1) influence of user which is both k nearest neighbor and reverse k nearest neighbor on current user shall be greater than that of k nearest neighbor user or reverse k nearest neighbor user due to asymmetry of k reverse nearest neighbor. therefore, credibility enhancement model is used to amplify its influence on current user. namely for any user ua,"
students should not be able to view other students marks or change marks; the system should not crash through malicious activity; access should be restricted to registered students; only registered students should take tests; moodle is fairly secure.
"all the educators interviewed at both universities reported that they were familiar with and comfortable using office applications such as word, excel and powerpoint presentation with some reporting that they were competent with programming, and other specialised application software."
"above recommended algorithms use influences of nearest neighbor on current users to gain interested items of current users without consideration of influence relation between reverse nearest neighbor and current users and the influence of user's credibility on preference acquisition expressed by user's behavior. therefore, for cold start and data sparsity problems, it is difficult for above algorithms to get high quality recommendation results."
"in figure 1 we show the evolution of the running times for the neuro-imaging application over several time intervals, during different hours and days within a week. for each time interval we tracked several thousands samples, one minute apart, by running the application on a fixed dataset stored locally. as it can be seen, the nodes from nimbus deliver the best performance. this is due to the fact that public clouds typically rely on commodity hardware, which is less efficient for scientific computation than the high end hardware used in private infrastructures like grid5000. the standard deviation of all samples is 0,242 for nimbus, 0,523 for extralarge and 1,207 for small, confirming the stability of nimbus. for better understanding the variability due to multitenancy in azure we propose the analysis in figure 2 . we assume that there exists a \"default\" variability that is due to the os and to the virtualization mechanisms. since the extralarge vm occupies the entire physical machine and all computations are done locally, we consider that the standard deviation measured for the extralarge vms approximates this default variability. hence, in figure 2 we represent the ration between the standard deviation measured on the small instances with respect to the reference one. the goal of this analysis is to see the fairness of sharing the physical nodes among vms belonging to different users. a value of the ratio close to 1 (in red) would mean that either the neighboring applications do not affect our performances, or that there are no neighboring applications, as we can see for the 16 th time interval. a high value for the ratio would indicate that the performance of the instance is affected by external factors, as is the case for the 5 th and 21 th interval. the reason for observing both small and big values is that the samples (2880) were done in a time span of approximately 1 week, one minute apart, a time span that proved sufficiently large to capture various situations. the interferences that affect the instances are caused by the local disk i/o or by memory accesses (we don't include the cpu since there is a 1 to 1 relation between virtual cpus and physical cores). such an analysis could be used as a hint for sched-"
the system can automatically mark multiple-choice questions; allows assessment feedback with comments and grades; exporting is there; i often use the grading forms; i find the criteria and indicators quite useful; set up marking criteria on online marking forms; i like the integration of turnitin into moodle.
"this finding supports the result of a previous study where educators reported creating a journal topic to give students a place for their own writing, which can be kept private between the student and the lecturer or shared with the class [cit] ."
"content authoring uses content creation tools for educational content [cit] . functions include integrating a wysiwyg (what you see is what you get) tool, creating and organising linear learning sequences, organising and reusing learning objects and content [cit] . functions include storyboarding capabilities, course creation templates, course homepage wizards, syllabus, organiser pages, and content modules [cit] ."
"new educational technology services and features are driven by advances in information technology and a growing market. virtual learning systems have been formally adopted by higher education institutions to stay abreast of the latest educational technologies, to be competitive in the higher education domain and to afford their stakeholders new innovative ways of teaching and learning. a vls refers to a class of software that is known by a number of names such as course management systems (cmss), learning management systems (lmss), virtual learning environments (vles), online learning platforms, and e-learning systems. examples of a vls in use are blackboard, atutor, and moodle (wcet, n.d.) ."
"a mixed methods research design was used for the broader study combining qualitative and quantitative approaches. this paper reports primarily on the qualitative research approach used to conduct the study [cit] . the justification for a qualitative approach was to obtain rich and deep insights into the perceptions of educators with regards to vls quality characteristics, and suggestions for potential improvements."
"the 'usefulness of course content tools' theme comprised two subthemes, namely user management and course design and management, which are described below. educators at both institutions agreed on the 'usefulness of course content tools' theme, which confirmed the need for the corresponding tools and features described in table 9 in the appendix."
"where, n is the number of the existing score numbers of target user in test set. rmse is more sensitive to large errors. rmae (u) equals to the root square error of the sum of squares of deviation of true score values in predicted value of target user u and test set. rmae of the whole recommended algorithm is the average of rmae of all users. details are as follows:"
"this can be done by investigating two main components. on one hand, the computation power is influenced by the cpu and memory usage. on the other hand, the data transfers between the vm nodes are affected by the underlying network and the existing traffic."
"the global reading strategies dataset (glob) is a subset of the marsi overall dataset consisting of only thirteen of the thirty questions. after processing this dataset similarly to the first and using the same instruments to analyze the data, this dataset showed an 84.62% accuracy when using the 9 rules found to reclassify the data. after taking those rules and using them in building the glob fis, classification of the data resulted in a 70.29% accuracy."
"in all their flavors, clouds prove to be a cheap and customizable alternative to supercomputers and, moreover, they have been proven much more reliable than grids. because of these advantages, cloud computing is gaining increasing attention for scientific applications, which typically consist of a very large number of tasks that process massive datasets to extract meaningful information. however, most current commercial clouds have been built to support web and small database workloads, which are very different from typical scientific computing workloads. moreover, the use of virtualization and resource time sharing may introduce substantial performance penalties for data-intensive or computationintensive scientific applications. under these circumstances, an important step in the process of choosing a specific cloud model is to extensively characterize the performance of the provided services and resources."
"data transfers. it is interesting to observe the impact of the network performance, considering that in private settings user applications typically run in isolation, while on public clouds they share the network infrastructure. we use a synthetic benchmark in which we transfer large sets of random data between the cloud computing instances. since tcp is the default underlying protocol for data transfers we examine its performance in both settings. we also noticed that many scientific applications rely on rpc to implement their functionalities, and therefore evaluated the overhead introduced by this communication paradigm in clouds. again, we take into account the sustained network performance during long time intervals, since variability can impact the overall performance of applications [cit] ."
this tool should resemble web 2.0 type of functionality; it needs to be less structured and more spontaneous; blackboard does not send a notification to students when a notice is posted to them; i would like an activity report on how many times they commented on a post data extracts supporting the need for an improved design in moodle were:
"(2) users are very likely to gain popular resources in other e-commerce platforms or through other approaches. therefore, probability for popular resources to appear in recommendation list shall be reduced so as to reduce matthew effect of recommended results. weakening relationship is as follows:"
"before actually executing an application in a cloud, one has to acquire the nodes, set the environment and prepare the data to be fed into the application. one time consuming step consists of copying the data from the local storage (exterior to the cloud) to the cloud storage and from there to the computation nodes (and vice-versa for the final results). our first metric measures these times to move data to and from the cloud. such a metric is relevant for data intensive applications like the ones that have to index huge collections of documents or process massive data sets."
"the 'usefulness of course administration tools' theme for both the universities comprised two subthemes, namely user management and course design and management, which are described below."
"the support reading strategies dataset (sup) is a subset of the marsi overall dataset consisting of the remaining eight questions. as with the other datasets, the marsi sup dataset was processed through the c4.5 induction tree to determine rules and an accuracy of 100% was observed. after taking those rules and using them to build the sup fis, the accuracy of the system was shown to be 68.89%."
"where, iri is quantity of users having behavior records on item ii; ir is average quantity of users having behavior records on other items; t is corrected parameter for different application environment."
"the minimum length of usage of vls moodle by interviewees at university b in their teaching practice was a year. some of the interviewees had attended introductory workshops on moodle run by the academic computing department, while others had learnt to use the vls through selfexperimentation. university b had a dedicated academic computing department providing adequate support for moodle users."
"educators reported mixed experiences with online discussion forums. some educators at university b found that students were less inhibited and more frank in online discussions than they were in face-to-face discussions whilst others reported that students' uptake of online discussion forums was poor and generally one-way communication from lecturer to students where no mark was awarded for online participation. the latter finding on online discussion forums supports the view of wyles (n.d.) who argues that the transmission model of information is common in a vls because it was easier than the interactivity implicit in a conversation. another viewpoint was that online discussion forums were very cumbersome and involved much reading and writing and was time-consuming. [cit] supports this finding that a vls enhances communication and coordination, but creates more work. educators reported that there were scheduled classes where face-to-face discussion took place, as both universities were residential, and therefore there was a lesser need for online discussion. discussion forums were not used for the mathematics discipline as typing of mathematical equations was reported to be cumbersome. some of the additional features that educators recommended for blackboard were the need for threaded discussion forums to be modelled around social media systems; tracking of student participation in threaded forums and chats, and notification mechanisms for discussion tools. data extracts supporting the need for an improved design in blackboard were:"
student can use course calendar with deadlines to manage his/her own learning; one central calendar where you can put all the key dates so its reminding them whats coming up.
"the main argument of the paper is that iso9126 quality model [cit] can be applied to assess the quality in use of a vls by studying the perception of end users on the usefulness and importance of system quality characteristics. it was towards this goal that the first four characteristics of the iso 9126 external software quality model were selected, namely functionality, reliability, usability and efficiency, and corresponding sub-characteristics were defined to fit the context of a vls. these four external software quality characteristics were selected as they represent the main software quality in use characteristics, which are easily observable by educators whilst interacting with the software [cit] . the functionality characteristic of iso 9126 external software quality model was applied to the vls tool functionality. in the iso 9126 model, security was considered as a sub-characteristic under the functionality characteristic in order to measure the security of individual functions within a product. however, for the purpose of this study, security is classified under the category of non-functional characteristics together with reliability, usability and efficiency in order to assess the software product characteristics that are not related to a specific function [cit] . formal topic-related discussions as well as informal interactions [cit] . blog a class blog or blog is a type of discussion topic to which users can post web logs (inc., n.d.) ."
"the findings on the announcement tool is supported by unal, z. and unal, a. (2011), who reported that the announcements (in blackboard) or the news (in moodle) is mostly used to help educators keep students updated on course information."
"we make a subtler evaluation, by considering besides performance, other factors that impact scientific applications: ease of management, cost and performance stability. we evaluate whether the azure commercial cloud is a feasible and cost effective alternative for offloading computation and storage resources from an in house maintained nimbus cloud, for scientific applications. to this end, we characterize both platforms using several metrics: data access performance, computation speed, variability and cost models. we rely on synthetic benchmarks to assess the network efficiency when using several communication protocols and models (e.g. tcp, rpc). for evaluating the computation power we use a real life genetic and neuro-imaging application. in addition, we make an evaluation of the transfer time of data from users' local storage to the cloud storage and from there to the computation instances, and vice versa, as this is an inherent step in any data-intensive application."
the number of interviewees who participated in this study per university is summarised in table 3 . all participants interviewed were educators who were teaching at both undergraduate and postgraduate levels of study from the two selected universities.
"calendars permit teachers to do course planning and enter submission dates for assignments (wcet, n.d.) . online journal/notes a journal is a type of discussion topic to which users can post either private or public entries (wcet, n.d.) ."
"with the development of e-commence, service providers must clearly grasp users' needs and preferences and provide users with services to their satisfaction in the premise that product quality is guaranteed. therefore, excellent recommendation systems are very significant in e-commence application and also become one of the researchers' focuses of attention [cit] . basic thought of collaborative filtering algorithm is to judge whether the evaluation is of value to target user through evaluation of other users having similar interest preference with target user on target item and then to decide whether to recommend target item to target user further. nearest neighboring collaborative filtering recommendation is the most successful recommendation technology at present and the algorithm recommends items to target user based on rating data of nearest neighbor having similar rating. as nearest neighbor's rating on item is similar to that of target user, target user's rating on unrated item can be predicted through nearest neighbor's rating on the item with weighting method. as collaborative filtering algorithm is usually influenced by data sparsity and cold start, a collaborative filtering recommendation algorithm based on improvement nearest neighbor is proposed. firstly, the current user's k nearest neighbor and reverse k nearest neighbor are obtained on the basis of the similarity algorithm, which are used to compute positive and negative credibility values respectively based on their predicted ratings and the current user ratings. then modifications of constraint are made for the users who are both the k nearest neighbor and the reverse k nearest neighbor and the hot resources. finally, the collaborative filtering recommendation algorithm based on weight fusion is derived and a comparative experiment of simulation is conducted on movielens. the result shows that the algorithm in the thesis decreases the mean absolute error value while improving the accuracy of recommendation [cit] ."
"thematic analysis, which is a commonly used method of analysis in qualitative research was used to reduce texts to codes that represented themes or concepts [cit] . tagging was used to analyse qualitative data with the aid of the nvivo tool. the process followed for thematic analysis was generating initial codes based on the categories from the two research questions, namely perceptions of vls functionality and perceptions of non-functional characteristics as summarised in tables 1 and 2 . additional themes and subthemes relating to individual toolsets and non-functional quality characteristics were identified and refined during the coding process."
all the interviewees from both universities agreed that usability is an important characteristic of a vls. some educators at university a commented that blackboard was user friendly whilst others commented that blackboard usability needed improvement. data extracts confirming the importance of this characteristics were:
"creating a fis in software consist of three primary steps: entering inputs, outputs, and rules. the inputs are the attributes of the dataset and are represented graphically in the fis by mapping each one with a set of membership functions. for each possible value of each specific input, a membership function showing the degree of membership to a set of values is created and mapped. this fuzzification of values allows us to define how specific inputs relate to the memberships of each of the values. after each input value has been mapped, a graph similar to figure 3 is produced. next, the outputs are mapped in a similar fashion, only using the final possible classes for each membership function as seen in figure 4 . lastly, the rulebase must be entered into the system. these rules are the ones obtained from the c4.5 algorithm when it was applied to the dataset. the preprocessing that was applied to the dataset earlier now proves useful when entering rules in to the fis, where the rules must be entered using a verbose representation."
"strengthening and weakening relationship between positive credibility and negative credibility shall be determined according to the corresponding rating difference. as some particular users and items shall be taken into consideration during calculation of credibility, in order to correct credibility among users, the thesis introduces credibility constraint model. the model is divided into two parts:"
"in this research, the c4.5 induction tree algorithm was applied to four datasets in order to obtain rules that were easily understandable and helped to show patterns in conditions that led to a classification. these rules were then tested for accuracy using a fuzzy inference system that was built for each dataset. the method was applied to the marsi datasetsoverall, global, problem-solving, and support. rules for the systems were selected manually from a list, choosing rules that had the greatest strength. it was shown that rule generation has varying efficiency depending on the dataset used, however, the rules generated still classified the majority of the data in all datasets with the lowest accuracy being 68.89%. www.ijacsa.thesai.org looking at the marsi datasets, it can be seen from table 2 that there are far more classifications of \"high\" than \"medium\", and even fewer \"low\" classifications. this relationship is most likely due to the effect of the students participating in the instrument inflating their own selfassessments. it has been found that people often overstate their abilities and see themselves as above average when they actually score low in areas where they rate themselves [cit] . this effect can also alter the results if the student is unskilled in the area they are evaluating themselves on. this can possibly explain how the data for the marsi datasets are slightly skewed, having few rules generated for the \"low\" classes. rules selected for the \"low\" classes often contained far fewer samples than the other classes, but since they had the highest sample size in their class, they were selected and used in the system. even with the inflation of self-assessment effect providing some inaccuracy in the data, the marsi datasets were still able to produce accurate rules, with some datasets having higher accuracy than others. our study has shown that induction trees provide an efficient tool for extracting reliable rules from datasets such as marsi. however, in future research, we plan to apply other methods such as neural networks, support vector machines (svm), and k-nearest neighbor algorithm for classification and rule extraction. the usage of such methods will enable us to compare methods with respect to efficiency of rule extraction, classification accuracy, and potentially attainment of a best possible rule set. in addition, it is possible to rank attributes by their information content and use a subset of those with the highest information content to improve efficiency and possibly accuracy. the current dataset used for purposes of this study was limited to only thirty attributes and did not have demographic attributes such as age, gender, ethnicity, and student reading ability. demographic variables such as these, and others, could have been factored into the research to show how they contribute, individually or collectively, to students\" awareness and use of metacognitive strategies when reading."
"able to upload images to posts; automatic notification of new posts; keep a record of the conversation; free fold in the conversation, and conversation central where you can upload and tag files"
"uploading and downloading of assignments should not be time consuming; minimum number of steps to perform tasks; shouldn't freeze when too many people are logged in; it was a problem when we were just using our server and if eighty people logged on it just froze, because of the latexcommands in between and so on; i want my computer systems to be slick and responsive; it is very efficient."
"for neighbor selection, k neighbor strategy is selected generally, namely the first kth users having the most similarity with current user shall be selected as neighbors. for an active user a, a neighbor set"
"a fuzzy inference system (fis) essentially defines a nonlinear mapping of the input data vector into a scalar output using fuzzy rules. the mapping process involves input/output membership functions, fuzzy logic operators, fuzzy if-then rules, aggregation of output sets, and defuzzification. a fis with multiple outputs can be considered as a collection of independent multi-input/single output systems. a general model of a fuzzy logic system (fls) is shown in figure 1 [cit] . the fls maps crisp inputs into crisp outputs. it can be seen from figure 1 that the fuzzy logic system contains four components: the fuzzifier, inference engine, rule base, and defuzzifier. the rule base contains linguistic rules that are provided by experts. it is also possible to extract rules from numeric data. once the rules have been established the fls can be viewed as a system that maps an input vector to an output vector. the fuzzifier maps input numbers into corresponding fuzzy memberships. this is required in order to activate rules that are in terms of linguistic variables. the fuzzifier takes input values and determines the degree to which they belong to each of the fuzzy sets via membership functions. the inference engine defines mapping from input fuzzy sets into output fuzzy sets. it determines the degree to which the antecedent part is satisfied for each rule. if the antecedent part fig. 1 ."
"the problem solving strategies dataset (prob) is a subset of the marsi overall dataset consisting of eight of the thirty questions. applying the same process to the data showed an accuracy of 62.5% after applying the c4.5 algorithm, and an 86.76% accuracy with the prob fis built with the extracted rules."
difference between actual rating of user ua on item ii and predicted rating of user ub on item ii is amplified to enhance credibility value.
"for our evaluation we used 40 nodes in the azure cloud, half of them with small instances and the others with extralarge instances (occupying the entire physical machine). we chose these two types of instances in order to better understand the multitenancy model of azure. the interconnection network between the machines is gigabit ethernet. we also created a nimbus cloud on top of the grid5000 infrastructure and executed the synthetic benchmarks and the abrain application inside the vms deployed in the cloud. we used 40 nodes belonging to the griffon cluster, as it is equipped with a large amount of memory and has 8 cores per node (similar to the extralarge machines used in azure). these nodes provide an efficient support for virtualization and each of them can host several vms while being used as workspace control agents. to install and deploy nimbus, we used a set of ruby scripts that take advantage of the grid5000 api to deploy one's own customized operating system image. intracluster communication is done through a gigabit ethernet network as well."
"this knowledge can be used by managers/directors of e-learning or educational technology departments to bridge the gap between features deemed useful and actual system usage via interventions such as training programmes and instructional design/development support. managers/directors of e-learning or educational technology departments can use the knowledge of non-functional characteristics deemed important to ensure that the implementation of a vls in an organisation makes provision for characteristics such as security, reliability and performance when using a vls. furthermore, the constructs of perceived usefulness and perceived usability have been found to be determinants of usage, which in turn has implications for wider acceptance and usage amongst non-adopters of vls in education. finally, the findings and results of this research can be used by system designers to identify additional vls tool features/functions required and non-compliant system quality characteristics and address these in future upgrades of systems. an improved quality in vls design that supports the work of educators is likely to lead to an increase in vls functionality usage. [cit] . blog a blog usually is organised as a chronological series of postings. people contribute to blogs, but there is usually only one central author for each [cit] ."
"open-source cloud frameworks such as eucalyptus, opennebula and nimbus [cit] are designed to allow an organization to set up a private group of machines as their own cloud. private clouds are typically deployed behind a firewall where access is controlled and the target users are known. hence, the primary distinction between private and public clouds may be more of a relative distinction concerning the ownership of resources and the ability to enforce security policies, etc., versus delegating those responsibilities to a second party. for scientific applications, users can create arbitrary size clusters of cloud servers. private clouds provide an integrated set of tools that deliver the power and versatility of infrastructure clouds to scientists. their implementation specifically targets features of interest to the scientific community such as batch schedulers, best-effort allocations, increased security through credentials, etc."
tracks student progress; keep track of student submissions; tracking students marks is useful as they are able to see all their marks and how they are doing.
"a total of ten educators were interviewed from various disciplines at university a, namely, three from fine arts, one from jewellery design, one from hotel and catering management, one from radiography, two from management studies and two from engineering. the interviewees comprised five females and five males. a total of sixteen educators were interviewed from several disciplines at university b, namely, one from tele-health, one from it ed, two from nursing, two from information systems and technology, one from pharmacology, one from education, one from internet studies, one from computer science, two from genetics, one from engineering, one from mathematics, one from law, and one from dietetics and human nutrition. seven of the interviewees were males and nine were females. all of the interviewees at university a had been using the blackboard vls in their teaching practice for over a year. in addition, educators employed at university a had completed the pioneers online training programme, which is a short certificated course. certificated courses were run by the e-learning unit within the centre for excellence in learning and teaching (celt) at university a, which provided training in blackboard together with instructional design and teaching philosophies. some of the educators had also completed the intermediate and advanced training courses in blackboard. university a had a dedicated technician to support blackboard users."
"the theme and subthemes relating to vls non-functional characteristics deemed important by educators at the two universities were coded from all the interview transcripts. interview findings confirm the importance attached to the following non-functional characteristics of blackboard and moodle by educators at university a and university b: usability, security, reliability and efficiency. in addition educators confirmed the importance of flexibility, interoperability, extensibility, and standards. it should be noted while educators have confirmed the importance of vls non-functional characteristics, several recommendations for improvement were suggested in the light of challenges experienced. [cit] who reported that reliability, system's functionality, interactivity, and response time were found to be significant on e-learning acceptance and use. the findings of the present research confirmed the theory on required system non-functional characteristics as summarised in table 2 ."
"user management the user management function was performed largely by an administrator at university a and was, therefore, regarded as less useful than university b. some properties of the user management function were reported useful and additional properties were recommended that would enhance usage. a positive comment from university b was that the system automatically uploaded registered students into the vls and allowed guest access. some of the data extracts pertaining to this subtheme were:"
the online marking and grading sub-functions were welcomed by educators at both institutions. some of the data extracts pertaining to the usefulness of this subtheme were:
online journal the online student journal was reported to allow students to reflect on their learning process. some of the data extracts pertaining to the usefulness of this subtheme were:
"content delivery allows for the dissemination of course material content to students. functions include uploading, storing, viewing of course material, importing and exporting content, an e-reserves folder and glossary [cit] ."
"in order to compare the costs of running applications on the two platforms, we estimate the price per hour of computation in a private infrastructure and compare it to the price listings of azure. the cost of an owned infrastructure has several components: hardware infrastructure, human resources and electrical power. it is hard to obtain exact values of these for a private infrastructure like grid5000. however, we derive an estimate based on official values of the hardware and personel costs in conjunction with the history of utilization [cit] . the grid5000 report [cit] estimated the total cost (without electricity) of the platform for 3 years at 15 million euros. the total number of cores is 7469 with an average utilization of 85% per year. hence, the cost of one hour of computation for a core can be computed as follows:"
you cannot go into blackboard and change anything like from mcq to short answers it doesn't do that because it keeps a statistic of the test over time; doesn't allow one to upload more than one image for each online test question.
"different algorithm experiment results are shown in figure 5 . from comparison figure 5, compared to reference algorithm, the algorithm recommended in the thesis is higher in accuracy so as to obtain better recommended quality. comparison results show algorithm of the thesis has fused influence of credibility between reverse k nearest neighbor and users and modify the credibility to obtain more ideal recommended result."
"as for the problem the current collaborative filtering algorithm is largely affected by data sparseness degree, a recommended collaborative filtering algorithm with improved nearest neighbor is proposed. simulation experiment result shows algorithm in the thesis has improved recommended accuracy and recommended quality to better solve defects in traditional collaborative filtering algorithm with nearest neighbor, thus significantly improving efficiency of algorithm."
"computation speed. to assess the performance differences between azure and nimbus, we used a-brain, a reference real-life application, that joins neuro-imaging and genetic analysis [cit] . the application compares brain regions of mri images with genes for finding significant links between the two. this application is representative for a large class of scientific workloads that split an initial large domain into subdomains, perform the computation (e.g., matrix operations) and combine the final results (e.g., image processing, weather simulations, etc). a-brain reads data from the local storage into memory, stressing the caching mechanisms of the machine, performs floating point operations on large matrixes and then writes the results back to the local storage. we track readings from the cpu, memory usage and virtual disk i/o to have a clear picture on how the application performs on azure and nimbus. we further take into account how the multitenancy model influences the computations and investigate if there is a way around it."
"scientists adopt different deployment models based on their particular requirements. this casts the differences between private and public clouds into a clear perspective. the adopted model can be sometimes more complex, as a user owning a private cloud can acquire public cloud resources to form a hybrid cloud; two or more private clouds could also interact for common goals and thereby form a federated cloud. in this section we give a brief overview of public and private clouds and their impact on scientific applications."
internal email the built-in email facility was found to be very useful in communicating with students and maintaining a record of that communication. some of the data extracts pertaining to the usefulness of this subtheme were:
"real-time chat is a conversation that takes place over the internet and involves an exchange of messages between participants at virtually the same time (wcet, n.d.). electronic whiteboard whiteboard tools include an electronic whiteboard used in a virtual classroom (also known as smart board) (wcet, n.d.) . announcements"
might be more useful for higher levels of study; supports collaborative work. [cit] that wikis are one of the moodle modules used by students to create collaborative learning communities on topics of their interest.
these findings confirm the usefulness of corresponding online assessment functionality described in table 7 in the appendix. some of the challenges reported with this blackboard tool were:
"the research makes a theoretical contribution to the field of vls usage in the form of a proposed model representing the vls quality in use characteristics. this model extends the discussion of quality characteristics derived from the iso quality model by identifying vls functions and nonfunctional characteristics and measuring educators' perceptions about the usefulness and importance of the quality characteristics of blackboard and moodle. the results demonstrate the validity of the iso 9126 external quality model for assessing vls quality in use characteristics. the research also demonstrates that the constructs of perceived usefulness and perceived importance, generally linked to technology usage can be used to measure educators' beliefs on software quality characteristics. the perceived usefulness and perceived importance constructs are related to implied user needs and user satisfaction, which are linked to vls usage in higher education."
"course authorisation tools are used to assign specific access privileges to course content and tools based on specific user roles, namely students, instructors, teaching assistants [cit] . functions include access based on user roles, assigning of different roles to instructors or students in courses and granting users' rights or privileges [cit] . registration integration registration integration tools are used to register and de-register students from an online course [cit] . functions include manual addition of students to courses, and importing class lists [cit] ."
"online calendar the course calendar, which is both a planning and communication tool, was reported to be useful as it had an updated record of course events such as test and examination dates and submission deadlines for projects and/or assignments. some of the data extracts pertaining to the usefulness of this subtheme were:"
"computing the costs of resources in public clouds is straightforward using the lists of pricing for the charged services (hours of computation, storage capacity in time, transfer of data etc.). however, when it comes to computing the costs in private clouds things are more complex. from the scientist's perspective, the cost is zero, as he usually has free access to the local cluster. to make a fair comparison with the public clouds, in this paper we consider the infrastructure owner's perspective. the cost of the hardware (servers, racks, network switches etc.), the energy cost (consumed power and cooling) and the human costs for the management make up a total cost from which one can derive a \"virtual\" price per hour, taking into account some reference period and the utilization level of the platform."
"the constructs used to measure vls quality characteristics were perceived usefulness and perceived importance. [cit], the construct perceived usefulness, which is adopted from the tam model is a determinant of behavioural intention and usage behaviour. the construct perceived ease of use from the tam model is encompassed in the usability quality characteristic. the construct perceived importance has been found to influence ethical judgement (attitude) in behavioural intention studies [cit], as well as measure information system success factors [cit] . the system quality characteristics, factors, and usage are depicted in figure 1 together with their relationships. users' perceptions of the usefulness of the functions of vls was measured to gain an understanding of whether implied needs were met in the context of vls usage in education, while users' perceptions of importance was measured to get an understanding of the importance attached to non-functional attributes of a system in the context of vls usage as these characteristics have an influence on user satisfaction with a software product. these allied concepts provide insights into whether the functions are useful to the educator with regards to technology support for teaching, learning and administrative tasks and whether the system as a whole satisfies quality in use characteristics of reliability, security, usability and efficiency."
"in the second phase of the research, a qualitative approach was adopted where data was collected through structured in-depth interviews. interviews were used as the main method of data collection to confirm categories/themes and sub-themes relating to the two main research questions namely perceived usefulness of vls tool functionality, and perceived importance of non-functional software characteristics. the interview questions concerned: vls functions and services needed or deemed useful for online teaching; types of tool support found useful when performing online assessments and tracking student progress with a vls; vls facilities needed or regarded useful for student involvement and productivity in an online course; quality characteristics deemed important to be integrated into a vls to support the online courses. a purposive sampling technique was used to select potential participants for the semi-structured in-depth interviews from the two universities. at the time of the study, the interviewees were using a vls, and had a minimum of one year's experience using a vls."
"(2) constrained cosine similarity; in order to amend rating dimension differences of different users and reduce influence of such difference on acquisition of user preference, user's average rating value on all other items is deducted in constrained cosine similarity algorithm to relieve rating difference. constrained cosine similarity is:"
"as most users, scientists prefer the ownership of virtual resources, since this reduces the uncertainty concerning access when needed. private clouds like nimbus incurred less variability and showed a better computing performance due to the underlying hardware. arguably, they are the first option for data intensive applications since they incur lower data staging times, due to proximity, and have good support for rpc. on the other hand, public clouds like azure have decreased costs and deployment times and support efficient tcp data transfers. we believe that this work represents a significant step towards enabling an informed cloud model selection for the emerging scientific hpc applications."
"this section presents a summary of studies conducted on the educators' perceptions of a vls in use. [cit] evaluated educators' perceptions of blackboard version 6.1 tools using the iso 9126 model characteristics of functionality, reliability, usability and efficiency with the objective of detecting design flaws. [cit] study tested for specific sub-characteristics of e-learning systems as originally defined by the iso9126 model. the current study, however, focussed on integrating the concepts of perceived usefulness and perceived importance that are linked to system usage with iso 9126 quality in use characteristics. [cit] into mediators of lecturers' perceptions on lms at universities in the western cape in south africa. the key findings reported were six mediators that influence c/lms usage, namely perceived usefulness, perceived ease of use, accessibility, functional expectations, support and use mediation. [cit] study are related to the current study. the construct of perceived usefulness is applied to vls functionality and the construct of perceived ease of use is included within the broader category of usability, which is measured for perceived importance."
"where, pbi is predicted rating of user ub on item ii; sai is actual rating of user ua on item ii and sui ui s is actual rating of other k nearest neighbor user uu on item ii. the ratio between the difference value between predicted rating pbi and rating of user a on item ii and rating difference value of other users in k nearest neighbor of user ub shall be deemed as positive credibility value between user ub and user ua."
"with the inputs, outputs, and rules programmed into the fis, data can now be passed into the system to observe the - behavior of the rules. from here, the fuzzifier component of the fis evaluates each input into the system and finds the firing strength of each rule. the firing strength of a rule is a measure of how accurately the inputs match the conditions of the rule. each input is matched against all rules and receives a numerical output for each rule whose conditions are all satisfied. the output of each rule is mapped to the membership functions of the output variable. this creates a shape in the membership function that shows the degree of membership as seen in figure 5 . several rules could fire for any given input. if this happens, the output of all the rules are aggregated into one result. this aggregation combines all the shapes into a larger shape for the final result which can be seen in the bottom-right of figure 5 . to obtain a single crisp value for this range of values covered by the shape, the center of gravity is calculated. in this fis, the centroid method was used, calculating the center of the area under the curve of the shape. once the center of gravity of the aggregated shape is calculated, the range value at that point becomes the crisp value for the input. this crisp value is then plotted on the set of output membership functions to defuzzify it and obtain a final value which is used to determine a class for the input data. afterwards, the next input can be processed."
the educators at university b expressed the view that the user interface of moodle needed improvement. data extracts confirming the importance of the usability characteristic were:
"in k reverse nearest neighbor, calculation of negative credibility is contrary to that of positive credibility. the difference value between current user's rating on certain item and predicted rating of reverse k nearest neighbor on the item shall be the basis for calculation of negative credibility. calculation formula is:"
"this section covers the qualitative results of data collected from university a and university b pertaining to educator perceptions on quality characteristics of the vls formally adopted for use at these institutions. the qualitative findings presented cover the demographic profile of interviewees, the perceived usefulness of blackboard and moodle functionality, as well as the perceived importance attached to their non-functional characteristics."
"we applied c4.5 induction trees to extract rules from a dataset consisting of results from an instrument distributed to undergraduate college students to assess their metacognitive awareness and use of reading strategies. these rules were then tested for accuracy using a fuzzy inference system. the purpose was to extract students\" metacognitive knowledge about reading using a metacognitive awareness strategies inventory. specifically, we wanted to identify relationships between categories to better understand how different reading strategies relate to each other and affect overall reading skills. this information will be valuable in helping students understand their metacognitive awareness and creating teaching and learning programs designed to improve on these skills."
"in this study, we used a set of reading strategies data using the metacognitive awareness of reading strategies inventory [cit], which was administered to a group of college freshmen and sophomores enrolled in a community college in the south central us. the original dataset had 1811 records, which were pruned down to 1636 to accommodate missing or incomplete data. it is worth noting that the sample size is sufficient in light of the main objective of the study, which is to extract students\" metacognitive knowledge about reading using a metacognitive awareness of reading strategies inventory. the size of the data set used is also consistent with similar data sets used in prior studies exploring students\" metacognitive knowledge about reading [cit] . the instrument consists of thirty questions designed to assess students\" level of awareness or perceived use of reading strategies by classifying the questions into different categories. the questions assess what kind of reading strategies a student uses while reading conventional academic texts. depending on the responses, ranging from \"never or almost never\" to \"always or almost always\", the students can be placed in three different reading categories and in a combined overall category. these categories cover three broad areas of strategies including (a) global reading strategies (glob), which can be thought of as reading strategies used when preparing to read text (e.g., setting purpose for reading, previewing text content, predicting what the text is about); (b) problem-solving strategies (prob) which are typically used during reading when problems develop in understanding textual information (e.g., checking one\"s understanding upon encountering conflicting information, re-reading for better understanding); and (c) support reading strategies (sup), which scaffold or support the process of reading and text understanding (e.g., use of reference materials like dictionaries and other support systems). these three categories of reading strategies contribute to a calculation of a student\"s overall reading strategy score. using the scores of these categories, a student can be classified into a \"low\", \"medium\", or \"high\" category with respect to their levels of reading strategy awareness and perceived use of reading strategies when reading academic texts."
"where is the observation vector, is the number of classes, and is the probability that belongs to a given class. the information gain is calculated by subtracting the difference in entropy from the total amount of information contained in the data using (2)."
"students use it to chat among themselves; we conducted our session via the live chat and kept a record of our session. [cit] maintained that the online real-time chat is a collaborative learning tool, which is supported in this study. [cit], who argued the online chat was one of the web components of a blended model. the argument presented by educators was that there was no real need for this tool as they had face to face contact with their students in scheduled classes."
"in this paper, we evaluate the performance of public and private cloud platforms for scientific computing workloads. we assess the benefits and the drawbacks of the two models, the constraints they impose to the targeted applications and the costs they incur. motivating this work is a typical researcher's dilemma when choosing the appropriate platform for large scale scientific experimentation. one option would be to deploy an open-source cloud on the local cluster in order to be able to fully customize the environment where the applications are running, since usually users don't have complete control on the physical nodes in the cluster. faced with the burden of managing the open-source cloud, the alternative is represented by the public commercial clouds. several previous studies have compared supercomputers and public clouds showing an increasing improvement in the scalability and performance of cloud-based hpc systems [cit] . however, linpack numbers do not always translate into application performance, which depends on the composition and pattern of computation, communication and i/o within each individual parallel application."
"educators reported on the usefulness of creating and administering objective-type online assessments. objective-type questions in the form of multiple-choice questions, true or false, matching and filling in the blank spaces, etc., were popular choices for online testing as they can be automatically scored. the availability of immediate feedback for online quizzes was reported to be the most helpful feature. some of the data extracts pertaining to the usefulness of this subtheme were:"
"in the first phase of the study, a literature review was conducted of vls tool functionality, nonfunctional quality characteristics, studies on educators' perceptions of vls in use and potential quality and usage models. the literature on vls tool functionality and non-functional quality characteristics were used to frame the study and design the research instruments."
need indicators to see how many people have read your blog i quite like the idea of blogging; it should automatically track students blogs; blogging supports communication.
"in figure 5 we illustrate the throughput for the rpc communication. due to the magnitude of the standard deviation for the small machines, we chose not to represent it. unlike for tcp, nimbus delivers up to 39% better performance for such type of communication. this confirms the existence of some software control on the http traffic in azure, for fairness reasons. as http is used to access the azure storage service, unrestricted data transfers from vms to azureblobs could temporary slow down neighboring applications."
user interface needs improvement; the functionality in moodle is not readily discernible; navigation can be more intuitive; the design should be similar to familiar applications like facebook.
"interviewees at both universities regarded reliability as an important characteristic, supported by the following data extracts: reliable (twenty four hours seven days a week) availability, lecturers work in off-peak hours setting quizzes; reliability is a very important issue as you do not expect the system to be continually down; report downtime on a weekly basis"
"this research was conducted at two higher education institutions in south africa, which are hereafter referred to as university a and university b. at the time the study was conducted, blackboard version 6 was the vls deployed at university a, and moodle version 1.5.2 was adopted at university b."
this finding was supported in the literature where students reported that they would use the online calendar for scheduling tasks and setting up alerts for upcoming assignments [cit] .
"an announcement tool allows lecturers to create and send text messages to class members (inc., n.d.) . wiki \"a wiki is a collection of collaboratively authored web pages\" [cit] ."
"online grade book keeps track of student marks and graded online activities, with the added capability of assigning course grades [cit] . functions include entries for new assessments, adding grades of offline assessments, customising grading scales, assigning weights to assessments, test item analysis, manually editing grades, sorting grades, searching grade book, downloading and uploading grade books in common formats such as excel [cit] . online assignment submission assignment-specific digital drop boxes allow student submission of assignments using a drop box. functions include date-stamped assignments, assignment drop boxes, multiple file submissions, tracking submissions, group assignments, deadlines, resubmission of assignments, and grading forms with marking criteria and weighting [cit] student tracking helps to track aggregate individual course material usage by students, and perform additional analysis and reporting [cit] . functions include tracking frequency and the date student accessed individual course components, monitoring what students have read and posted, and identifying students at risk [cit]"
"where is the number of distinct values of attribute, and shows the weight value of the split of the tree. once the information gain for each attribute is known, the attribute with the highest information gain becomes the root node in the tree and is the first split. next, the process is repeated again for the remaining nodes and another node of the tree is created and split upon using the highest available information gain. these nodes branch out using the different conditions of the previous nodes to begin narrowing down the data with each split on information gain creating nodes that are children of their parent node. this process continues until one of a few conditions is met. if all of the records in the list at this point belong to the same class, a leaf node is created that states the class. if there is no information gain on any of the attributes or a new class is encountered for the first time, the algorithm creates a node higher up the tree to represent the expected value of the class at that point. once all attributes have been processed, the tree is built and rules can be extracted."
"in this paper we systematically compared the performance and cost of two public and private clouds (azure and nimbus) along several dimensions that matter to scientific users. we observe dramatic performance and cost variations across platforms in their virtual instances, storage services, and network transfers."
"create content viz. lessons, quizzes, tests etc.; support hyper linking where you can link to a page or bulk of pages; you can create lessons and link to the content of a particular topic, and to a quiz to test their knowledge of the topic; create definition of terms or concepts in a course glossary, which can be linked to learning activities and tests; attach meta data on learning materials; create learning portfolios and home pages is important."
"course content creation educators reported that this facility was useful in that it allowed them to create lessons, quizzes, tests, learning materials, learning portfolios, definition of terms in the glossary, and create content using wikis. some of the data extracts pertaining to the usefulness of this subtheme were:"
"the compute and storage nodes are usually distinct, as the cloud storage is a stand-alone system providing persistency to applications. the storage is accessed using http based protocols and consists of binary large objects with sizes in the orders of gbs, that are usually structured in a higher level namespace (buckets, containers). the main issues for the scientific applications arise from the impact of the data transfer protocols on the throughput, size limitations on the data objects, the coarse-grain access and the little support for heavy concurrency (e.g. no or little versioning support)."
"online threaded discussion forum posting in threaded forum discussions was the main tool reported to be used for learning, discussion and debate in the online environment. some of the data extracts pertaining to the usefulness of this subtheme were:"
grade book could be more effective/flexible with regards to in-depth feedback; offline assessment grades have to be manually entered into moodle grade book; it won't import from excel. it does export very nicely.
"these findings confirm the need for identification and authentication of users [cit], as well as ensuring the security of assessments [cit] who argued that there should be restricted access to data based on permission levels. the finding on security supports the need for security measures such as passwords and encryption [cit] and ensuring that information is secure, accessible and accurate [cit] )."
the notice board or news forum was also reported to be very useful to make announcements and convey messages to students. it was also mentioned that student can receive immediate notification of announcements when they log on to the system. some of the data extracts pertaining to the usefulness of this subtheme were:
educators reported that they and the students can see the marks obtained for assessments and the progress made. educators can view students' submissions. some of the data extracts pertaining to the usefulness of this subtheme were:
extensible so if i see third party tools that i want to integrate with the system i would be able to; with moodle it's open-source there is thousands [sic] of plug-ins.
"the literature review on vls tool functionality and non-functional characteristics provided the reference point for eliciting educator perceptions on the vls quality characteristics using the broad framework of the iso9126 software quality model. vls tools deemed useful from the qualitative findings were communication, student productivity and involvement, course administration, assessment, student tracking and course content. the vls non-functional characteristics deemed important from qualitative findings were usability, security, reliability, efficiency, flexibility, interoperability, extensibility and standards. the qualitative findings also revealed challenges associated with vls functions and characteristics. the findings of the study were integrated into a proposed model of vls quality in use characteristics depicted in figure 2 with relationships to perceived usefulness and perceived importance constructs, which is linked to system usage. it is important to note that the present study targeted educators in higher education who were computer-literate and had the experience of using a vls in their teaching practice. another important factor worth mentioning was that a technical and training support system was available to keep the adopted vls functional at both universities. in addition, the influence of demographic variables such as age, computer literacy, academic rank and discipline expertise of non-adoptees on the perceived usefulness of vls tool functions was not covered in this study. these relationships can be explored in future studies."
"in order to evaluate the accuracy of extracted rules, a fuzzy inference system (fis) was built using the extracted rules and the data samples were reclassified using the fis. a fis is a system that applies fuzzy logic to map inputs to outputs, functioning similar to an artificial neural network [cit] . fuzzy inference systems attempt to build models that can be used to predict new data. these systems allow us to model the behaviors of complex systems using rules made up of basic logic statements and then use those rules to simulate the effects on new data. in this project, the software matlab and the fuzzy logic toolbox is used to build a fis."
"function include a built-in email service [cit] . online journal/notes a journal is a type of discussion topic to which users can post either private or public entries (wyles, n.d.) ."
"the test data is processed by the proposed algorithm to recover the missing calibration parameters (table i) . in all experiments, a 2-camera active set, and the dynamicscene assumption are utilized. the results are presented in figures 5 and 6 . figure 4 illustrates the variation of the foreground area, which is computed by projecting the foreground object to the principal camera (section iv-b). as seen in the graphs, the estimated quantities have only an insignificant amount of jitter, and their initial and final values do not appear to be in conflict with figure 3 . the initial high-uncertainty of the pose estimate in figure 6 reflects the initial uncertainty of the ukf, which rapidly diminishes as more measurements arrive."
we have demonstrated how to adapt an ldpc code for rate compatibility. the capability to adapt to different error rates while minimizing the amount of published information is an important feature for qkd key reconciliation. the present protocol alows to reach efficiencies close to one while limiting the information leakage and having the important practical advantage of low interactivity.
"the relative orientation and focal length estimation problem is often encountered in the context of image stitching and calibration of pan-tilt-zoom cameras. in both cases, a feature-based solution essentially involves the estimation of a homography from image correspondences [cit] . the 3d counterpart, orientation and focal length with respect to a scene model, given the camera center, requires two 3d-2d correspondences. the method, presented below, follows a similar strategy to the 3-point pose estimation algorithm [cit] : locate the scene points in the camera reference frame (defined by the camera center, the image plane, and the principal axis vector), and then recover the 3d homography that maps them to the world reference frame. in addition to the camera center, we make the common assumption that all intrinsics except for the focal length are known, and the image points are normalized accordingly. figure 1 illustrates the geometry of the problem."
"authorship analysis has been widely used in resolving authorship attribution of literary and conventional writing. with increasing cybercrime arising in internet, web information authorship attribution began to draw researchers' attention."
some technical means have been used to analyze the writing features to arrive at the purpose of attributing a text's authorship. mathematical methods and intelligent algorithms were adopted. the techniques vary with different periods. the following is summary of three common authorship analysis approaches.
"is the frequency of term t in document d, n is the total number of documents, t n is the number of documents that contain term t."
"for a qualitative study of the performance of the algorithm, 3 tasks are chosen, on the basis of their sensitivity to calibration errors: dense 3d reconstruction, scene augmentation and stereoscopic rendering. the scene models ( figure 7 ) obtained via the dense 3d reconstruction algorithm are utilized by the other applications. in order to render the floor and the walls, the room is modeled as a cube."
"in order to determine a k-element active witness set, the equations 5 or 6 are evaluated for all k-element subsets of the witness set. the evaluation is not carried out exactly, over the entire volume, but only at the vertices of a regular lattice covering the scene, usually about several hundred points (depending on the size of the scene). alternatively, an existing scene model can be used, if it provides reasonably uniform coverage. ω ij should contain only the vertices which can be triangulated accurately (e.g., with a low covariance), so that narrow-baseline witness camera pairs are excluded."
"a hybrid multi-camera setup, involving a moving (principal), and a set of static (witness) cameras is frequently employed in film production and broadcasting, as such a setup saves editing time and facilitates post-production. in the latter case, the plausibility of many special effects depends on whether accurate camera calibration information is available. moreover, the prominence of this setup, and the importance of calibration, is on the rise due to the popularity of 3d films, and new applications, such as 3d-tv. for static cameras, the necessary level of calibration accuracy can be attained via manual calibration techniques [cit] . however, the fact that a principal camera may have variable calibration parameters (i.e., that it can move and zoom) leaves throughthe-lens calibration as the only viable approach, despite the challenges posed by dynamic scene content. the algorithm we propose aims to address this problem, i.e., recovers the intrinsic and the extrinsic parameters of a camera in general or nodal motion, and viewing a scene with dynamic elements, given a set of cameras with known calibration."
reconciliation protocol able to adapt to different channel parameters is presented and its asymptotic behavior discussed. in section iii the results of a practical implementation of the protocol are shown. in particular we have analyzed the rate compared to the optimal value and the reconciliation efficiency.
"machine learning techniques including decision trees, neural networks, and support vector machines(svm) are the most common analytical approaches used for authorship attribution in recent years. the distinctive advantage of the svm is its ability to process many highdimensional applications such as text classification and authorship attribution. [cit] have drawn the conclusion that svm significantly outperform neural networks and decision trees in authorship analysis. in our study, support vector machines were used for learning the authors' writing features, and authorship attribution model was gained."
"the most widely used protocol for information reconciliation in qkd is cascade [cit], because of its simplicity and good efficiency. cascade is a highly interactive protocol that runs for a certain number of passes. in each pass, alice and bob both perform the same permutation on their respective strings, divide them in blocks of the same size and exchange the parities of the blocks. whenever there is a mismatch they perform a dichotomic search to find an error, finding one usually means discovering more errors left in previous passes."
"the presented results demonstrate the instantaneous exciting current space phasor approach establishes a cost effectiveness online diagnosis technique, that provide fast and reliable detection of low level faults on the transformer windings."
"the precondition of authorship attribution is that the web information of suspected authors can be obtained. we assume that there is enough web information of suspected authors. by analyzing the known author's web information, the author's writing style is gained. then the author of unidentified information can be attributed. so the first step of authorship attribution is to collect web information of suspected authors as much as possible."
two dimensional presentation of three phase variables as instantaneous current space phasor (icsp) can be used to examine their balancing situation. the transformer icsp as a function of instantaneous line currents for a system with positive sequence is defined by:
"where l u denotes the distance of u to the camera center, which can be computed from the known world coordinates of the scene point and the camera center. v c, v, d v, and l v are defined similarly for the image point v."
"the frequency of certain words reflects author's preference or habit for usage of some specific words. in our study, the frequency of certain words was expressed as lexical features."
"this paper proposes a method for calibrating a moving camera in the presence of a dynamic object, given a witness set. this is a setup commonly encountered in film production, where accurate calibration is valuable for the post-production process. the algorithm first builds a reference structure, with respect to which the calibration is measured. the calibration measurements are then smoothed by a ukf. the algorithm can handle general and, via a novel p2p solver, nodal motion both with known and unknown intrinsics. moreover, it can dynamically determine an active witness set to focus the processing on more promising portions of the available data. this capability makes it possible to robustly deal with dynamic scenes, by refreshing the scene model at each frame. the performance of the algorithm is demonstrated both quantitatively, and through several applications (dense 3d reconstruction, scene augmentation and steroscopic rendering)."
"in our experiments with boujou, with the default parameters, we observed that as long as the checkerboard pattern is visible, the performance remains satisfactory. however, when the camera is zooming, a certain amount of manual intervention becomes necessary to mitigate the jitter, and abrupt jumps. moreover, when the foreground ratio is above 20%, the calibration estimates detoriorate considerably."
"although (1) is similar to the expression related to the positive sequence component in the fortesque transform, but in order to calculate the icsp the instantaneous values containing harmonic components is inserted in (1) instead of phasor values related to a given frequency."
"various internet service such as e-mail, bbs, blog, microblog has been widely applied to people's daily life. while internet provides convenient to people, a lot of problems appear at the same time. some illegal web information, such as antisocial information, fraud information, pornographic information, terroristic threatening information, gambling information, appears by means of e-mail, bbs or blogs. the internet provides criminals new criminous space and means. illegal web information affects social stabilization and national security seriously. some measures should be taken urgently. now, installing filtering software to filter the information containing sensitive words is the main method to prevent these phenomena. however, this passive defensive method cannot stop these phenomena, because criminals can make use of some substitute words to break through the defense of the filtering software. punishing the criminals by means of law can strike these crimes effectively. many states have made interrelated laws. however, due to lacking effective evidence, many cases cannot be brought to the court. if web information's authorship is attributed by technical means, criminal's evidence for computer forensic can be collected. this will provide an important application value and practical significance to law enforcement, social safety and stabilization, internet environments' purification."
"stylometry is the study of the unique linguistic styles and writing behaviors of individuals in order to determine the authorship. it is an interdisciplinary study of statistics and computer science etc. the research of stylometry is based on the premise of two assumptions. the first assumption is that all authors have distinctive writing habits, which can be captured from a number of quantitative features such as certain vocabulary usage, sentence complexity, and phraseology. the second assumption is that these habits are unconscious. even if some authors make a conscious effort to disguise one's writing habits, the effect is not obvious. stylometry focuses on defining authors' subconscious writing features and determining statistical methods to measure these features so that the similarity between two or more pieces of text can be analyzed."
"(2) writing in the internet doesn't obey the rules of punctuation. interrogation marks, exclamatory marks, and suspension points are used frequently. authors input a succession of exclamatory mark when they approve others viewpoint and input several suspension points when they do not understand others viewpoint."
"from table v., we can see that accuracy of lexical features on literature, bbs and blog dataset were 77.02%, 56.26% and 80.51% respectively. accuracy of structural features was 94.97%, 62.86% and 84.35% respectively."
"finally, the simulated transformer has been analyzed by selecting the transient mode for solution type and 100 µs for time step (equal to 10 khz sampling frequency). the rated values of simulated transformer have been presented in table 1 ."
"according to the faraday's law, the input flux into a turn depends on the electro motive force (emf) on its terminals for certain frequency and hence, the fundamental magneto motive force (mmf) of the transformer winding is fixed by the fixed emf of voltage supply. as a result, when an inter-turn fault happens on the primary or the secondary winding, a great circulating current flows in the shorted turns to generate an opposite mmf to the fundamental mmf of winding and change the input flux into the shorted turns proportional to the decreased voltage across them. so, for the faults containing a minor fraction of the coil, the circulating current will be larger to be able to oppose the fundamental mmf of the coil as a result of high ratio of transformation between the whole winding and affected turns [cit] . when a turn-to-turn fault is located on the each one of transformer windings, since the fundamental mmf in the affected turns has been reduced, in order to preserve the fundamental mmf of whole winding, a higher instantaneous exciting currents space phasor approach current flows in the primary winding as shown in fig 3 and fig 4. it should be noted that the fault occurrence on the primary winding has a cumulative effect in decrement of the fundamental mmf and consequently, increment of the primary current to compensate it. because there are lower turns in the primary side to generate the same mmf that demanded to preserve the fundamental flux of the transformer in this case. furthermore, it can be mentioned that the induced voltages in the secondary side and so the output currents do not change because of the same fundamental mmf before and after the fault occurrence in the primary winding. while in the case of inter-turn short-circuit on the secondary side it is observed that the secondary terminal values is reduced as a result of the lessened effective number of the turns on the secondary winding [cit] ."
"the covariance of a calibration measurement is estimated via the unscented transformation (ut) [cit], as justified by the highly nonlinear nature of the pnp solvers. the operation involves applying deterministic offsets to the minimal correspondence set (as determined by its covariance), passing the new sets through the appropriate pnp solver, and computing the sample statistics of the resulting calibration set (with due attention to quaternions [cit] ."
"the crimes utilizing internet increase rapidly. for the purpose of providing evidences for the court, cwaap, an authorship attributing platform for chinese web information was developed. in this paper, the framework of the system was provided. two types of features including lexical features and structural features were extracted. to test the effect of cwaap, three datasets were collected. five experiments were designed and performed. experimental results proved that the two features extraction methods were effective. the number of words in samples used for authorship attribution exceeded 200 at least. by ig feature selection methods, 800 lexical features could express the authors' writing style. there was a small difference between the authors' topics. all the parts of speech reserved were perfect. the accuracy exceeded 80% by experimenting on the blog datasets. the experimental results suggest that the platform is effective and feasible to apply for cybercrime forensic."
"been previously subject to consideration [cit] . it is a process known as key distillation, that requires a discussion carried over an authenticated classical channel. it is interactive in the sense that it needs communications through the channel. since it can also be listened by an eavesdropper, it is important to minimize the amount of information that have to be transmitted in the reconciliation process."
"(3) glossary: in english language, words are composed of 26 letters. sentences consist of several words. in chinese language, there are above 90,000 chinese characters totally. just the commonly used chinese characters amount to above 7,000. there are many more words than the characters, because words are composed of several characters."
"in this work, we develop the idea of using ldpc codes optimized for the binary symmetric channel. we take these codes as an starting point and develop a rate compatible information reconciliation protocol with an efficiency close to optimal. in particular, the proposed protocol builds codes that minimize the exchanged information for error probabilities between 1% and 10% 1, the expected values in real implementations of qkd systems."
"in order to simulate a turn-to-turn fault on the transformer winding, the defective transformer is modeled as shown in fig 1. as seen from the figure, when a turn-to-turn short-circuit occurs on the transformer, the affected winding is divided in two subwindings \"a\" and \"b\", that are associated to the healthy and faulty part, respectively [cit] ."
"in this understanding, the study seeks to examine strategies for the web-based cooperative learning in wireless network environment and compare its effectiveness with those of the extant cooperative learning method. units and materials appropriate for cooperative learning are selected from each subject. the selected materials are taught in three different ways -offline cooperative learning method, web-based cooperative learning and wireless network web-based cooperative learning. then the students' interest, satisfaction and achievement are evaluated."
"(2) computational approaches with the development of computer technology, sensitive classification techniques rather than simple count statistics have been applied to authorship attribution. [cit] analyzed the frequency of words. the pearson product-moment method correlated each word with all others. principal component analysis methods were used to transform the original variables to a set of new uncorrelated variables. [cit] described how traditional and non-traditional methods were used to identify seventeen previously unknown articles that were believed to be written by stephen crane. 3000 word samples of text were analyzed for frequencies of 50 common words. principal component analysis was used as the method of discrimination."
"in the above equation, 1, 2 and 3 are the exciting currents of phase a, b and c, respectively. the exciting currents equations of the transformer can be obtained based on the ampere-turns balance principle and they will different in various connections of the transformer. the exciting currents equations for the yzn5 connection (notation as per fig 1) will be:"
"the rate of parts of speech can reflect the preference for word class usage. for example, some authors always use exclamation, however some authors hardly ever. the usage of parts of speech can reflect the authors' degree of education. chinese has 12 categories parts of speech in common use which are listed in table ⅲ. the weight of parts of speech is the ratio of number of the parts of speech in the document to total number of parts of speech in the document."
"to test the effectiveness of cwaap, three datasets including literature, bbs, and blog were collected, and several experiments were made. the detail information of the three datasets was showed in table ⅳ."
"the web text should be inputted by keyboard. at the same time, authors always ignore the difference of chinese and english punctuation, which can treated as writing habits to extract. table ⅱ is the punctuation features. the weight of punctuation features is the ratio of the number of a particular punctuation in the document to the total number of punctuations in document."
where h is the binary entropy function and f the efficiency (e.g. tab. i). then she can derive the optimal values for s and p:
"this paper described a new technique that detect the inter-turn short-circuits on the transformer windings by using instantaneous exciting current space phasor. the proposed method is based on analyze the exciting currents and hence, it is stable for the case of load imbalance and external faults condition. the occurrence and progression of turn-to-turn faults on the transformer windings had been simulated based on fem and evolution of the iecsp characteristics with the fault severity had been presented for simulated transformer. it was observed that the propagation of inter-turn fault appears in the deformation of iecsp locus and its stretching toward the faulty phase. as an effect, the oscillation of iecsp modulus with frequency of 2f and also create impulse in the angular speed waveform of iecsp are symptoms of existence or propagation of winding faults. these characteristics had been used as fault detection tools and some indexes had been extracted from them. the relative increment of these indexes due to growth of the fault had been presented and according to it, the maximum of angular speed had been assessed as the optimum index."
"in web texts, authors always ignore some punctuations or use incorrect punctuations. the authors can write freely on the premise of expressing the author's meaning. so the structure of web texts is loose. furthermore, the authors have a preference for part of speech usage which can reflect the authors' degree of education. so we extracted three aspects of structural features, namely, punctuations features, structural characteristics, and part of speech features. table ⅰ shows the structural features."
"computation of calibration estimates: if the calibration measurements are computed independently from each other, they inevitably exhibit some jitter. a sequential state estimator can alleviate this problem in two ways: filtering out the noise, and providing an initial estimate to the guided matcher. in order to avoid linearization errors, we choose ukf [cit] . in the most general case, a constant velocity model for position, orientation, focal length and lens distortion is employed. concretely,"
"with the popularization of internet, cyber-language begins to spread, which has struck the criterion of the traditional languages. cyber-language is free in use and not restricted with grammar. the style of cyber-language has the following characteristics."
"what features set can represent web information authors' writing style is the next step. in the feature extraction step, the extensive stylometric features including lexical features, structural features were extracted. the writing features were represented by the vector space model (vsm). thus one web information document was denoted as a dot in the high dimensional space."
"to test ig feature selection method, the number of lexical features from 100 [cit] was tested on blog dataset. 5-fold cross-validation was used to validate the experimental results. table vii and figure 3 were the experimental results."
"under normal condition, the exciting currents are slightly unbalanced and non-sinusoidal due to the asymmetry and nonlinearity of the transformer core. consequently, the iecsp locus will be an asymmetric hexagon in healthy transformer. the occurrence of the inter-turn short-circuits on each side of the transformer causes an increment in amplitude of the exciting current corresponding to the affected phase and leads to a deformed iecsp locus. in order to evolution of the iecsp modulus oscillations, an index can be defined as the ratio between the maximum and minimum value of the iecsp modulus. consequently, the extremums ratio (er) is expressed by:"
"the advanced methods such as wavelet transforms [cit], s transform [cit] and hilbert transform have been already applied for turn-to-turn fault detection in the transformer windings. although these approaches detect the minute faults, but they are based on the signal processing and have complicated computation. hence, these methods require large number of processors and instruments and so implementation of them is difficult [cit] ."
"stylometry is the application of the study of linguistic style, usually to written languages. stylometry is the theoretical basis of authorship attribution which attributes authorship of unidentified writing on the basis of stylistic similarities between the authors' known works and the unidentified piece. researchers have focused on academic and literary applications ranging from the questions of the authorship of shakespeare's works to forensic linguistics. the research language of authorship attribution has been mainly english, arabic, and japanese etc. however, there were little related authorship attribution researches on the chinese language. the language characteristics of the chinese language are very different from other languages such as english and indoeuropean languages, where the feature extraction methods for authorship attribution are different. in this paper, an authorship attribution platform for chinese web information, cwaap, was introduced and described. based on the language characteristics of chinese web information, various authors' writing features including lexical features and structural features which could express the authors' writing habits were extracted. support vector machines (svm) were used for learning the writing features."
"(2) the second experiment the former study on authorship attribution needs 1000 words in one sample at least, which can express author's writing style better. however, the number of words in web information is small. two or three words in bbs or e-mail texts are common. how many words in one document can be used to attribute authorship reliably? we made experiments on the literature dataset. three authors' samples were experimented. every author had 30 samples. the number of words in samples was 50, 100, 200, 500, and 1000. 5-fold cross-validation was used to validate the experimental results. the experimental results that were measured as table vi. from table vi, we could see that the accuracy increased with the increase of words in samples. that was because the more words in samples, the writing style could be expressed better. the experimental results showed that the accuracy did not have distinct change when the number of words exceeded 200. conclusion could be draw that words in samples reached 200 could be used to attribute web information's authorship."
the literature dataset was collected from one online books library. the blog dataset came from the website http://blog.sina.com.cn/. we gained the bbs dataset from one web forum. every dataset's author was different from others.
"in this section, the above-mentioned fault detection tools are studied and discussed, comprehensively. in the entire of fault cases presented in this section, the fault resistance has been adjusted so that limit the current in the defective part of the winding to 1.25 of rated current in the secondary side."
lexical features could be extracted by tf-idf techniques which had been used in the research of text classification. the weight of lexical features was calculated as formula 1.
international the simulation is performed using comsol simulation tool to calculate the real effective refractive indices over different wavelengths of interest. the birefringence and confinement losses of each of the structure were formulated and analyzed.
"our approach constructs a scene model from a witness set, computes the calibration measurements at each frame via guided matching and ransac, and removes the measurement jitter through an unscented kalman filter (ukf). it has the following capabilities and novel features:"
the remainder of the paper is organized as follows. section 2 presents a general review of stylometry and previous related work. section 3 describes the framework of cwaap. section 4 is our feature selection and extraction methods. section 5 provides our experimental methodology and analyses the experimental results. section 6 draws the conclusions of the paper.
"in order to recover the only unknown, f, we observe that the cosine of the angle m (figure 1 ) can be computed either by applying the law of cosines on the triangle uvc, or as the dot product of d u and d v . this defines a quadratic equation in f 2, i.e.,"
", (3) where a is the distance between the scene points. upon solving equation 3, u and v can be recovered via equation 2. the coordinates of 2 scene points in the camera and the world reference frame are sufficient to recover the rotation relating the reference frames [cit] ."
"different from the english language, chinese does not have clear natural word segmentation markers. the lexical features are main writing features to extract. the precision of word segmentation relates to the effect of feature extraction. now a lot of chinese word segmentation software packages are available for use. however, the latest appearing words such as newbie are difficult to segment correctly. in cwaap, word segmentation software named segtag developed by professor xiaodong shi at xiamen university was used for word segmentation and part of speech tagging. an additional dictionary was used to supply the new appearing words. in the case of incorrect word segmentation, the platform provides adjustment functions manually."
"after the fault occurrence, the angular speed of iecsp will be changed, as it is shown in fig 8. the angle of iecsp is also presented in this figure for different fault severities in phase c of the secondary winding. in a sound transformer, the iecsp rotates with the angular speed close to constant and its angle changes from 0 to180 degree and vice versa, linearly. while in presence of the inter-turn fault, the angle of iecsp will be changed in a nonlinear manner, so that it is often equal to the affected phase angle (120 and 60 degree) and its angular speed will contain several impulses corresponding to 0 and 180 degree. obviously, nonlinearity of the iecsp angle and maximum of the angular speed are increased as the fault intensifies."
"presence of the unbalanced loads and external fault cases. so, the icsp of the terminal currents is not suitable way for the fault diagnosis. to conquer the aforesaid difficulty, a modified detection method according to the analysis of the instantaneous exciting current space phasor (iecsp) locus is suggested. the three phase exciting currents of the transformer are balanced in the sound situation and also they are independent from condition in out of the transformer such as load condition and external faults. so, the proposed technique will be robust in varies load condition and external fault cases. the occurrence of winding faults leads to create an unbalanced system of the exciting currents that can be used as one of the diagnostic tools. additionally, the exciting currents are more sensitive to the fault expansion and hence, this technique enhances the fault diagnosis sensitivity as compared to the former diagnosis approach. the iecsp technique will be employed for fault detection in the further section."
"intuitively, the active witness set should share a large fov with the principal camera. however, covisibility of a feature does not necessarily imply that it can be matched. a better criterion is the matchable volume (mv), the volume that contains scene features that can be potentially related to the image features observed by the principal camera. a feature in ω ij, the mv of the cameras c i and c j, satisfies the following conditions:"
"the waveforms of the terminal currents as well as circulating current in the shorted turns that have achieved by fea simulation are shown in fig 3 and fig 4. the transformer has been supplied from the hv side by the rated nameplate voltage and the rated resistive load has been connected to the lv side. a turn-to-turn fault involving 3% of the turns has been located on the primary and secondary windings of the simulated transformer, respectively. also, the fault impedance value has been adjusted near to zero to create a metal-to-metal contact."
". in the dynamic scene case, it is not feasible to use the entire witness set at each time instant. assuming a k-camera subset is requested, the camera selection procedure uses the current estimate of the calibration, and a sparse 3d lattice covering the scene, to rank all k-element subsets of the witness set. the details of the algorithm are discussed in section iii-b"
"photonic-crystal fiber (pcf); a new class of optical fiber has been very popular in the field of fiber-optic communications because of its confinement characteristics not possible in conventional optical fibers. the distinctive properties of pcfs are the high design flexibility [cit] . it is possible to obtain pcfs with diametrically opposite properties by changing the geometric characteristics of the air-holes in the fiber crosssection, that is, their dimension or position. the main difference between pcf and conventional fiber is the photonic crystal fibers have an air-silica cross section, whereas standard optical fibers have all glass cross-sections [cit] . according to their mechanism for confinement photonic crystal fibers possess two modes of operation-index guiding and photonic band gap fibers-in index guiding pcf's, where light is confined in a higher refractive index region, the light is guided by total internal reflection between the solid core and cladding region. instead, when the core has a refractive index lower than that of the cladding region, as in hollow-core fibers, it is necessary the presence of the photonic band gap (pbg) [cit] . a pcf has a holey cladding region surrounding a solid core [cit] . the birefringence and dispersive properties of polarization maintaining photonic crystal fiber were analysed using finite element method. to maintain the linier polarization high level of birefringence is required by reducing polarization mode dispersion. by breaking the circular symmetry and implementing asymmetric defect structures such as dissimilar air hole diameter, varying the number of circular and elliptical air holes, high birefringent pcf can be designed. the number of air hole rings around the central core need to be increased for making the structure more confined at a specific wavelength."
"this research seeks to overcome the limitations of the extant web-based cooperative learning by introducing a wireless network environment. accordingly, new cooperative learning strategies were designed herein. the proposed strategies are consisted of three steps of problem situation recognition and goal setting, problem solving and evaluation and reflection. wls is a system to assist the web-based cooperative learning online. it monitors the progress of students during the cooperative learning process in real time to provide mentoring about study goal setting, problem solving and study achievement. the figure 1 below shows the overall structure of the wls system. the system is operated through the google document tool and naver blog and supports the following functions; first, it produces statistics based on the level test results and final test results to assess the levels of student performance and provides proper feedback. students send their pre/post-intervention tests to teachers and teachers refer to student-specific and test question-specific statistics to identify those with poor performance and let them receive teachers' and colleagues' mentoring. second, in the problem-solving stage, teachers monitor information production process and advice about the produced information to lead students' problem-solving efforts in the right direction. students, in the information production process, receive real-time feedback from teachers through the 'sharing' function and 'share view' function in the google document tool."
"any extra information limits the performance of the qkd implementation. in theory one could minimize the information leakage using a highly interactive protocol, but in practical applications this would lead to a prohibitively large communication overhead through the network, limiting also the effective keyrate."
"the calibration algorithm, summarized in figure 2, requires a set of witness cameras as input. the set could be manually calibrated, or, could be the output of an automatic calibration algorithm, such as bundler [cit] . in the case of multiple principal cameras, e.g., a nodal and a free camera tracking the same object, the calibration algorithm can be run iteratively, employing the estimated calibration sequences as \"dynamic witness cameras\" for the remaining principal cameras. the initialization involves the construction of a sparse scene model from all camera pairs [cit], and the estimation of the initial calibration measurement, by solving the appropriate pnp problem. the resulting scene model is a collection of 3d features, each of which is composed of a 3d coordinate, the covariance of the coordinate, and a sift descriptor [cit] for each image in which the feature is observed. in the main loop, the algorithm goes through the following steps:"
"indices n e and n o for extraordinary and ordinary emerging rays respectively. if an un-polarized beam of light enters a material with a nonzero acute angle to the optical axis, the perpendicularly polarized component will refract at an angle as per the standard law of refraction and its complementary component at a non-standard angle determined by the difference between the two effective refractive indices known as the birefringence magnitude."
"in the following discussion, the terms foreground and background refer to the dynamic, and the static scene elements, respectively. the checkerboard pattern in the background does not help the algorithm: due to the repetitive nature of regular patterns, the matcher considers such features as ambiguous, and does not include them in the final correspondence list."
"step 4: bob can reproduce alice's estimation of the optimal rate r, the positions of the p punctured bits, and the positions and values of the s shortened bits, and then he creates the corresponding string"
"cooperative learning practice offline is limited in terms of time, space and information usage, taking longer time to produce a result. for these reasons, offline cooperative learning can be only limitedly utilized in actual classes. web-based cooperative learning [cit] was developed to address such problems. through webbased cooperative learning practices, learning environment was expanded."
"in this research, web-based cooperative learning strategies were designed and applied in a wireless network environment with a view to address the shortfalls of the existing web-based cooperative learning practice. as a result, the following conclusions are made; first, wireless network environment helped ease the limitations in students' learning environment by reflecting their demand in real time so that the students could work as a main player of learning with full interest and proactive participation. second, the introduction of web could address the problem of inefficient interaction among participants under the previous web-based cooperative learning because web enabled the students to share, process and re-produce their data. third, as existing education curriculum was presented in relation to everyday issues, the suggested method could help improve the students' problem-solving ability as aimed by the cooperative learning practice."
"a. birefringence a ray of light, during its passage through any anisotropic materials results in the splitting or decomposition of the ray into two. commonly materials with uniaxial anisotropy--have an axis of symmetry with no equivalent axis in the plane perpendicular to it--exhibits this optical phenomenon. this axis of symmetry is termed as optical axis of a particular material. light rays with linear polarizations in parallel and perpendicular direction will show unequal effective refractive"
"as the particular pictograph in the world, chinese language is highly uniform and canonical. compared with english and other european languages, chinese has the following characteristics."
"the students in this research were found to have 13.6% interest level in offline class; 40.4%, web-based class; and 46%, wireless web. this finding implies the effect of learning environment. the fact that they use a computer seems to work as a study motivation to show no big difference between web-based and wireless web activities. but as the wireless web was based on personal notebooks and reflected own thoughts or opinions in real time, it gained a higher score from the students."
"the themes selected above are separated by different units but, for this research experiment, they were performed in similar period. each was taught in the form of class then the students were surveyed for their interest in and satisfaction with the class. their final test results were referred to view their study achievement. the following figure is the graph of the students' class interest, satisfaction and subjectspecific achievement."
"the icsp locus is obtained by drawing in terms of . in normal situation, the terminal currents are balanced and the icsp locus is a circle in the center of the coordinates plate. when an inter-turn fault is occurred on the each one of the transformer windings, the primary currents will be unbalanced and hence, the icsp locus will be converted to an elliptic that its major axis orientation indicates the faulty phase and its ellipticity increases by the fault expansion. but, the terminal currents of the transformer are also unbalanced in"
"the above researches are for english, japanese, and arabic documents' authorship analysis. however, techniques of authorship analysis used for feature extraction are dependent on languages, and in fact differ dramatically from one language to another. for example, chinese does not have word boundaries explicitly in texts. in fact, word segmentation itself is a difficult problem in the chinese-like languages. so feature extraction methods for chinese documents are different from other languages such as english and other indo-european languages. figure 1 presents the framework of cwaap (chinese web information authorship attribution platform). according to the process of chinese web information authorship attribution, there are six steps, namely information collection, information pre-processing, chinese word segmentation, feature selection and extraction, authorship training, and authorship attribution."
"stylometry is the basis of authorship analysis. authorship analysis can be divided into three distinct problems, namely, authorship attribution, authorship characterization, and plagiarism detection. the aim of authorship attribution is to determine the author of a piece of text by comparing the similarity of writing style between the author's known works and unknown ones. authorship characterization attempts to formulate author's sociolinguistic profile by making inferences about gender, educational, and cultural background on the basis of writing style. the purpose of plagiarism detection is to calculate the similarity of two or more pieces of text and to determine if a piece of text has been plagiarized."
"(1) the words are typed into the computer's screen by keyboard. so to save typing time, users do not obey the rules of the usual writing. elliptical sentences and incomplete sentences are common in web information. furthermore, the writing is free in the internet. a lot of blank line and blank spaces are inputted at will. the sentences are brief. sentences usually consisting of two or three words are common."
"the mv between the principal camera and an active witness set depends on the scene type. in the static case, if a point is in the mv of a principal camera and any member of the set, it is in the mv of the entire set. therefore, given an index set i, representing a subset of witness cameras, and the principal camera c p,"
each code was used within a range of error rates and the reconciliation efficiency was maximized only in the region close to the code's threshold.
"number of distinct punctuations/total number of punctuations number of distinct words/total number of words mean sentence length mean paragraph length number of digital characters/total number of words number of lowercase letters/total number of words number of uppercase letters/total number of words number of space/total number of words number of blank lines/total number of lines number of indents/total number of words figure 2 . the main interface of cwaap cwaap was developed in visual c++ development environment. the operating system was windows xp. other software tools including segtag(chinese word segmentation software package), libsvm-2.9(support vector machine software package) were used. the system was composed of four modules, which were web information's content extraction, web information's features extraction, web information's authorship training, and web information's authorship attribution. figure 2 shows the main interface of cwaap."
"if all the words were treated as lexical features, the number of features can reach thousands of the dimensions. but some features are useless, which can waste storage space and result in system degradation. in our study, information gain (ig) feature selection method was adopted to select effective features. the information gain of lexical features was calculated as formula 2."
"web-based cooperative learning, still, could be limited if performed only within classrooms or computer rooms because problem-solving ability is acquired in the process of resolving diverse daily issues [cit] . thus, in order to maximize the educational effectiveness of the web paradigm, wireless network environment is all the more necessary. classroom boundaries are blurred. learners come to practice in diverse time and space. they should be allowed to collect, process, share information *corresponding author collected from corners of their school or during their days and then re-process and reproduce such data in real time."
"in order to assessment to the optimum indicator for the proposed method, the relative increment of the indicators corresponding to growth of the low level faults are calculated and presented in fig 9. the relative increment is defined as the ratio between the absolute increment and the reference value corresponding to the healthy condition. it should be mentioned that the mean value of iecsp modulus under healthy operation is considered as the reference value for the iecsp modulus indicator."
"there are many categories of web information, such as e-mail, bbs, and blog. the object of authorship attribution is the text of web information. disorderly information such as photos, sound, and advertising information should be removed. so it is necessary to preprocess the web information and leave the useful texts of web information to be analyzed."
"accuracy of combination of lexical and structural features was 95.62%, 70.99% and 89.06% respectively. accuracy of structural features on all dataset was higher than lexical features, which proves that structural features were one effective feature. accuracy of combination of lexical and structural features was higher than structural features, which shows that the combination of lexical and structural features was more effective than lexical features or structural features singly. the accuracy exceeded 80% by experimenting on blog datasets. the accuracy of bbs dataset was low, which might be caused by too few words in bbs document."
"footprint, handwriting, signalment have been used to obtain evidence for courts. but the evidence of criminals via internet is difficult to collect, because internet is a free and open place and the messages on the internet are spread anonymously. criminals being hidden in any online corner can commit a crime. though internet services such as e-mail or bbs require users to fill out their personal information when registering, criminals always forge their real information or log on anonymously. so registering information, ip address, and e-mail's header information cannot provide convincing evidence for the court. however the text's content and structure can be obtained from web information, the same as their handwriting. authors of web information have their inherent writing habits. the writing habits cannot be changed easily (although the criminals will always try), which embody a writing style such as usage of certain words, the length of sentences and paragraphs, and the format of the text."
"the frequency of certain word-usage, the length of sentences etc can be used to attribute authorship. the former researchers have focused on what the features could represent the writing style of authors. however, no fixed features set were agreed on. the following is several types of features."
"alice creates now a string she then sends s(x + ), the syndrome of x +, to bob as well as the estimated crossover probability p * ."
"the table ⅶ and figure 3 show that the overall trend of results was ascending in the rough, though waves occurred in the course. the accuracy of 100 lexical features was low, which proved that 100 lexical features could not express authors' lexical writing style adequately. there was not distinct change in experimental results when the number of lexical features reached 800. too few lexical features could not express the author's writing style adequately. too many lexical features might result in storage space wasting and system performance degradation, and improve little on the results. table viii showed that there were a small difference between the same authors' topic and different authors' topic. that was because that authors' topic was embodied in noun or verb. some other parts of speech could express authors' writing style well."
"the above process performs quite successfully for all cases, except for p4p. in our experiments, we observed that, even when the intrinsics are constant, the successive focal length measurements computed by the p4p solver may exhibit some variation, introducing a significant jitter to the position measurements. we identified the likely cause as the focal length-depth ambiguity, which is not trivial to resolve just by the reprojection error. in order to alleviate this problem, we employed a multiple-hypothesis approach, and at each frame, in addition to the p4p solution, we computed a competing p3p solution, by using the current estimate of the intrinsics. the p4p solution is preferred only if it is considerably superior to the p3p in terms of the number of inliers. the increase in the computational cost is affordable, as the p3p solver is about 6 times faster than p4p."
"to investigate the transformer behavior in different situations, a two winding three phase transformer has been simulated in maxwell 14.0 software which is based on finite element analysis. the finite element analysis is a quick and efficient way in simulation of modern engineering systems that solves a problem by separating the problem field into several elements and then implementing physical rules to each minor element. under healthy condition, each one of the transformer coils has been drawn as two whole regions and the nominal ampere-turn has been assigned to each region. for faulty transformer, the defective part of winding has been modeled as separated geometric region and desired value of ampere-turn has been allotted to it. then, the dirichlet boundary condition has been applied to the boundary surrounding the transformer. in order to modeling the nonlinear nature of transformer core, the magnetization curve of core material has been assigned to it. afterwards, the circuit domain related to geometric domain has been designed in ansoft maxwell circuit"
"(1) the first experiment to test whether the two types of features extracted in our study are effective, the first experiment was made. different features and features combination on different dataset were tested. 1000 lexical features were selected by the ig features selection method in formula 2. 5-fold cross-validation was used to validate the experimental results. the experimental results are showed in table v."
"for text classification, the useless empty words are removed. the substantives such as nouns, verbs, and adjectives are accepted. however, conjunctions, prepositions, and adverbs are useful for attributing authorship. the fifth experiment was concerned about whether all the parts of speech should be reserved to attribute authorship. 1000 features of the blog dataset selected by the ig method were extracted as features. the results were validated by 5-fold cross-validation. from tableix, we could see that nouns, verbs, adverbs, conjunctions, adjectives and pronouns tested solely were better. the accuracy of preposition, quantity, auxiliary, modal particle was lower. however, the accuracy that every part of speech tested solely did not exceed the accuracy that all part of speeches reserved. except for the above six types of part of speech, the accuracy of the rest part of speech was 64.64%, which showed that the rest part of speech had discrimination ability. the fifth experiment showed that the results of all the part of speech reserved were perfect. some empty words had discrimination ability for attributing authorship, which should be reserved, differing from text classification."
"in order to simulate the inter-turn fault, the geometric domain should be modified as well as circuit domain. in the equivalent circuit a time-controlled switch has been used in series with the fault resistance to create the winding fault in desired moment."
"editor environment. in the circuit domain, each separated region of coils in geometric domain has been modeled by its equivalent inductance calculated by software and the resistance corresponding to it."
"transformers are one of the most important and expensive devices in electrical systems that are critical links between the generation stations and consumers. variety of unusual conditions and faults can be affected the transformers. unplanned repairs such as fix or replacement of the faulty transformer are very costly and time consuming. one of the most sensitive parts of the transformer is the insulation system that can be exposed by electrical, mechanical and thermal stresses and moisture. degradation of insulation system causes a breakdown in the insulation and leads to development the inter-turn faults. internal turn-to-turn faults are the most difficult types of faults to detect within the transformers. if turn-to-turn fault has not been rapidly"
"where f is the focal length. the corresponding scene point, u, and the direction vector of the projection ray, d u, in the camera reference frame, are"
"a linear kernel function was used as the kernel function of support vector machine. since there were only a small amount of data to produce a model of authorship attribution, the experiments results were measured by kfold cross-validation to provide a more meaningful results. accuracy was used to evaluate the experimental results. five experiments were performed. the first experiment is to test the validity of two types of features. the second experiment is to test the effect of document size on experimental results. the third experiment is to test the effect of number of lexical features on experimental results. the effect of author's topics was tested in the fourth experiment. different parts of speech were tested in the fifth experiment."
"detected, it can develop into more critical and costly to repair faults such as phase-to-phase or phase-to-ground faults. therefore, quick detection of turn-to-turn faults is essential in order to protect the entire of electrical system and reduce the damage and repair cost. in this way, development of online techniques for condition monitoring and in order to diagnose of inter-turn faults is very important for improvement of the system reliability."
it is in this scenario where modern forward error correction (fec) is an interesting solution. the idea is to make use of fec's inherent advantage of requiring a single channel use to reconcile the two sets.
"once alice has estimated p *, she knows the theoretical rate for a punctured and shortened code able to correct the string. now she must decide what is the optimal rate corresponding to the efficiency of the code she is using:"
"instantaneous exciting currents space phasor approach in this figure, the fault has been illustrated on the primary winding of phase a by connecting the fault resistance ( ℎ ) across the shorted turns. the severity of fault depends on the number of shorted turns as well as the circulating current flowing of them that is limited by the fault resistance."
"in order to evaluate the effectiveness of the active witness set selection procedure, we repeated the previous experiment with a 6-camera active witness set. since the order statistics are similar, we conclude that the pair of cameras included in the active set are the ones that provide sufficient information for an accurate calibration, and hence the selection procedure achieves its objective."
"3d scene augmentation: scene augmentation involves planting virtual objects in a real scene, and is of high importance to post-production. any calibration errors are manifested either as a drift in the location of the virtual object, or as an incorrect occlusion. figure 8 depicts several examples, in which the scene, as seen by the principal camera, is augmented by a virtual advertisement. the images do not exhibit any symptoms of poor calibration."
"stereoscopic 3d rendering: this application utilizes the scene geometry, and the principal camera calibration, to generate a stereoscopic sequence from a monocular input. this is achieved by synthesizing novel views of the scene for two virtual viewpoints located on either side of the principal camera. a poor calibration leads to visible artefacts such as texture mapping onto an incorrect depth layer. the images in figure 8 appear very realistic, and are free of such problems."
"standard puncturing and shortening need an a priori knowledge about the channel in order to adapt the rate. the bit error rate (ber) in the case of qkd protocols is an a priori unknown value, hence it is important to be able to construct codes that can adapt to the varying ber values that might appear during a qkd transmission. in order to cope with this, we propose an inverse puncturing and shortening protocol, that is performed after the distribution of the correlated variables."
"u is the probability mass function at the symbols during iteration l, and p u0 is the initial message density distribution, which in our case is:"
"the lack of ground truth calibration information prevents a direct assessment of the accuracy of the estimates. instead, we use an indirect measure: distribution of the reprojection error. assuming that each pixel coordinate in the image measurement set is corrupted by an independent noise process with a standard deviation of 1 pixel, in the absence of any calibration errors, the reprojection error would be distributed as χ 2 with 2 degrees of freedom (ideal in table ii ). in order to assess the performance of the algorithm, we employ cross-validation over the scene model, i.e., partition the scene model into a \"training\" and a \"test\" set by randomly discarding half of the scene points, run the algorithm on the former, and calculate the distribution of the reprojection error on the latter. the results, presented in table ii, show that the error can be mostly attributed to the image measurement noise, and the calibration is reasonably accurate. however, it should be mentioned that 1-pixel standard deviation assumption is somewhat pessimistic-in table ii, juggler seems to be slightly superior to ideal, the perfect calibration case."
"the following are several typical authorship attribution studies. [cit] came to the conclusion that the 12 disputed articles were written by madison. another wellknown study is the attribution of disputed shakespeare works. [cit] compared the writing style of shakespeare's work \"earl of oxford\". the writing style included unusual diction, frequency of certain words, choice of rhymes, and habits of hyphenation. \"and quite flows the don\" [cit] . sholokhov was accused of plagiarizing from kryukov. [cit] draw the conclusion that sholokhov was the true author of \"and quiet flows the don\" by comparing the statistical features of sholokhov and kryukov. the features included the length of sentences, part of the speech, sentence structure etc. \"dream of the red chamber\" is a masterpiece of chinese literature and is generally acknowledged to be the pinnacle of classical chinese novels. for a long time, the first 80 chapters written by cao xueqin and the 40 additional chapters written by gao e were recognized universally. professor chen bingzao at university of wisconsin researched on the authorship of \"dream of the red chamber\" for the first time. computers were used to calculate and analyze the frequency of words occurring in the masterpiece. he came to the conclusion that all the 120 chapters were written by cao xueqin."
"although the modern calibration algorithms offer mature tools, they are not without their limitations: msfm is vulnerable to dominant planes and insufficient camera motion [cit] . slam algorithms aim for real-time operation, and are superior to msfm in terms of accuracy only in the case of small processing budget [cit] . moreover, all of the methods above operate under the static scene assumption, and their performance is susceptible to large dynamic objects."
"looking at table i we can see the effect of the protocol on the efficiency of the reconciliation. when close enough to r 0 it is close to one, and for small enough δ values it remains close to one for the whole set of rates, which is not the case for the higher δ values as expected by the thresholds found in b maximum bit error rate corrected."
"direct memory access (dma) is a feature of modern computers that allows certain devices to access system memory independently of cpu. in order to process a device request r, a device might read/write data using dma."
"to accelerate post-silicon validation, high-quality tests should be ready before a silicon device becomes available [cit] in order to save time spent on preparing, debugging and fixing tests in the post-silicon stage after the device is available. test coverage is an important metric for evaluating the quality and readiness of post-silicon validation tests. precise coverage results are needed for engineers to judge whether existing test suites can achieve sufficient coverage on the device."
"we further evaluated time and memory usages for the offline replay process. as shown in table i, time and memory usages of the offline replay are modest. it only takes a few minutes to process tens of thousands events. figure 6 shows partial register coverage results for e1000. each register is identified using the register offset, such as 0x0 and 0x8. the figure shows that how many times and how much percentage top ten registers are accessed. for instance, the most accessed register is register 0x8 (status register), which is accessed 21927 times. the system software reads this register very frequently to query the device state. with the same test suite, we instrumented drivers to capture run-time data on two silicon devices: e1000 and tigon3, and computed the coverage on the corresponding virtual devices. we compare the results with these results shown in section v-c. the coverage results are very similar for both e1000 and tigon3 in terms of code and register coverage. one major difference is reflected on transaction coverage. due to different speeds of physical machine and virtual platform, several transactions are affected. for example, while transmitting network packets, silicon devices can transmit more packets than virtual devices in the transmit transaction since the speed of silicon devices is much higher than virtual devices. we conclude such differences in coverage are acceptable."
"[blws07] have pointed out that the probabilistic concepts underlying the uncertainty cone can be easily misinterpreted. for instance, instead of reading the cone as the 66% likelihood region through which the storm center will pass, it is very easily misread as indicating an increasing storm size. indeed, the nhc has begun placing a notice to this effect at the top of their most recent displays. in addition, the cone is a binary representation, possibly leading one to a false sense of security outside of the cone, or an exaggerated sense of certainty inside the cone."
"going from a set of spatially-distributed points to a continuous representation of geospatial uncertainty requires the ability to derive a continuous scalar field over the spatial region covered by the data samples. in our work, we use simplicial depth to associate a scalar value with each of the data samples. we then build a continuous scalar field over this spatial region using radial basis function interpolation."
"our approach to creating time-specific visualizations from predicted storm path ensembles begins by sampling the paths at specific times. instead of an ensemble of paths, this gives us ensembles of locations fixed in time, from which we can construct visualizations. how to do this in a compelling way is the primary question addressed by this paper. as already demonstrated in figure 3, due to spatial undersampling, a simple scatter plot of predicted locations tends to create a confusing display as the prediction time increases. one possible improvement would be to portray the underlying spatial density distribution implied by the hurricane predictions as a \"heat map\". however, our early attempts to produce heat maps, by laying down a spatial grid and counting data points, led to displays that were too coarse where data points were tightly clustered, and too incoherent were they were widely spread."
"we have implemented this approach in device coverage analyzer (dca), a coverage analysis tool using virtual prototypes. we have applied our approach to evaluate a suite of common tests with virtual prototypes of five network adapters. our approach was able to reliably estimate that this suite achieves high functional coverage on all five silicon devices."
"we implement our approach on the qemu virtual platform. the event capture mechanism is implemented as a qemu module which can be used for hooking qemu virtual devices. device interface functions are invoked by the qe-mu framework. for instance, a driver issues a read register request, the qemu invokes the corresponding read register function defined in the virtual device. our module hooks all the interface functions when the virtual device registers these functions to qemu. in this way, the module captures the device events when there is an interface register request, an environment input or a dma access. this module provides capability to hook different virtual devices without modifying virtual devices. for capturing events on silicon devices in physical machines, we modified device drivers to achieve it."
"a possible criticism of this approach is that the filtered result may not be faithful to the data. the combination of the two filters is applied only to the orientation angle of the ellipse, not to the radii of the major and minor axes. the nonlinear filter is not strongly sensitive to small angle changes, and is thus only removing large orientation flips in the sequence of visualizations. the smoothing filter is only removing small perturbations from the data, thus removing jitter. these have a negligible effect on the overall orientation of the ellipse angle, as can be clearly seen by comparing the curves in figure 7 ."
"we found that dynamically adjusting the kernel radius, used in eq. 1, solves the rbf interpolation problems caused by a highly nonuniform data density. we do this by selecting the kernel spread parameter based on the prediction density distribution -dense regions are interpolated with a narrow spread, while sparse regions are interpolated with wide spread."
"in our visualization design, having a strong feel for the uncertainty in a prediction is of paramount importance. therefore, we elected to present the storm position as three overlapping confidence intervals: 33%, 66% and 99%. these intervals are unembellished except that each is of a different color, and each is of a different transparency. the 33% region is most opaque, the 66% region less opaque, and the 99% region is highly transparent."
"liu [ly90] developed the notion of simplicial depth, which is a powerful non-parametric approach for describing robust statistical summaries of an ensemble of samples. simplicial depth defines the centrality of an individual point within an ensemble of points, and may be used to compute a center outward ordering of the data. a sample point with larger simplicial depth is considered to be closer to the center of the ensemble, and thus more representative of the set of points. a sample point with smaller simplicial depth is considered to be less representative. once the simplicial depth of each point in an ensemble has been determined, the points can be sorted based on their depth, with the indices of the sorted samples providing the structure of a cumulative distribution. we divide these indices by the number of samples to produce a normalized ranking of the points."
"π, the normalized difference between the ellipse orientation angle in time step i and the previous time step i − 1. we then compute two weights"
"the gis and the mobile device communities have adopted the convention of presenting a geolocation containing uncertainty by a pale (i.e. transparent) blue dot, with radius conforming to some (e.g. 95%) confidence interval. often this blue dot is augmented by a marker indicating the center of the dot, an outline, and sometimes by a transparency fade indicating the probability distribution. a recent series of experiments [bhmg14] provides strong evidence that the pale blue dot, without border or center marking, and (contrary to intuition) without a transparency fade, provides visual cues most helpful in aiding experimental subjects to make correct spatial judgements incorporating uncertainty."
"our color choices started with the color coding common in emergency systems, e.g. the u.s. homeland security advisory system, which employ red, orange and yellow to present the top three levels of warning. however, yellow is a poor choice for our application, since highly transparent yellow over a white background is almost invisible. thus, in our design we use red to indicate the region of highest risk, orange to indicate the medium risk region, and maroon to indicate the cautionary region. given a depth interval and an associated color, the opacity is given by"
978-3-9815370-2-4/date14/©2014 edaa the remainder of this paper is structured as follows. section 2 provides the background. section 3 presents the design of our approach. section 4 presents the implementation. section 5 elaborates on the five case studies we have conducted and discusses the experimental results. section 6 reviews related work. section 7 concludes and discusses future work.
"computing test coverage requires appropriate coverage metrics. in our approach, we use virtual prototype coverage to estimate silicon device functional coverage. a virtual prototype is not only a software program, but also models the characteristics of the silicon device. therefore we have employed two kinds of coverage metrics: we have adopted the typical software coverage metrics and developed two hardware-specific coverage metrics: register coverage and transaction coverage."
"before the first silicon prototype is ready, it is very challenging to quantify coverage of post-silicon validation tests since we do not have a silicon device to run these tests on. even if a silicon prototype is ready, the black box nature of the silicon prototype only supports limited observability and traceability that makes post-silicon validation difficult. recently virtual prototypes are increasingly used in hardware/software co-development to enable driver development and validation at an early stage even before silicon prototypes become available [cit] . an example is how intel used virtual prototypes to enable software development for their 40g ethernet adapter (e40g) before the silicon prototype became available [cit] . a virtual prototype for the e40g was created and used to test and validate the e40g driver being developed. bugs were found in the driver using the e40g virtual device, even before the real e40g device became available. as shown in figure 1, virtual prototypes and silicon devices are running respectively in virtual platforms and physical machines. virtual prototypes can provide the same transactionlevel functionalities as silicon devices to support driver development and validation. virtual prototypes have major potential to play a crucial role in estimating silicon device functional coverage of post-silicon validation tests. the white box nature of virtual prototypes brings complete observability and traceability that evades silicon devices. it is possible to have thorough test coverage evaluation over virtual prototypes. this paper presents an online-capture offline-replay approach to coverage evaluation of post-silicon validation tests with virtual prototypes. we first capture necessary run-time data, including the initial device state and device requests from a concrete execution of the virtual prototype within a virtual platform under a given test. we then compute the test coverage by efficiently replaying captured data offline on the virtual prototype itself. to evaluate the coverage, we have adopted four typical software coverage metrics and developed two hardware-specific coverage metrics: register and transaction coverage. to ensure fidelity of coverage estimation on the silicon device, we further extend our approach to compute coverage after the silicon device becomes ready and check conformance with coverage estimate on the virtual prototype."
"devices and, therefore, virtual devices are transactional in nature: they receive interface register requests and environment inputs, and process them concurrently without interference. thus, an interesting and useful metric is transaction coverage. for a virtual device (which is a c program), given a state s and a device request r, a program path of the virtual device is executed and the device is transitioned into a new state. each distinct program path of the virtual device represents a distinct device transaction. when computing coverage, the impact of a test case on the virtual device in term of what transactions it hits and how often they are hit are recorded. the impact of a test suite can be recorded the same way. the coverage statistics can be visualized using pie or bar charts in term of what and how many requests were made, what and how many transactions were hit, and what percentages they account for among all requests. moreover, the details of a transaction is recorded, such as registers accessed and interrupt status."
"the cone represents the probable track of the center of a tropical cyclone, and is formed by enclosing the area swept out by a set of circles (not shown) along the forecast track (at 12, 24, 36 hours, etc). the size of each circle is set so that two-thirds of historical official forecast errors over a 5-year sample fall within the circle."
"a hardware register stores bits of information in such a way that systems can write to or read out from it all the bits simultaneously. high-level software can determine the state of the device by reading registers, and control and operate the device by writing registers. it is critical for engineers to know what registers have been accessed so they can check whether the device is accessed correctly according to the specification. virtual devices provide complete observability, therefore we can capture accesses on both interface and internal registers. actually in our approach, we capture all register accesses and deliver different kinds of register coverage reports according to user configuration."
"while the large angular jumps observed in the original curve have been successfully removed, there are still small perturbations that interfere with frame-to-frame visual coherency. to filter out these bumps, we utilize a gaussian filter, with kernel width 5, centered on the current time. this gives filtered results like those shown in the bottom curve in figure 7, and provides smooth frame-to-frame transitions."
"a technique, often used with radial basis functions, is that of matrix regularization. briefly, what is done is to add a small value to each element of the diagonal of the matrix φ. this allows the solution to closely approximate the data at the sample points, rather than forcing a strict interpolation [alp14] . in all of our rbf work reported here, we are using a regularization constant of 10 −4 ."
"following our early, unsuccessful experiments using data density, we turned to the concept of simplicial depth. simplicial depth is a measure of the centrality of data elements within a data set, giving a clean measurement associated directly with a data sample, and not dependent upon the local sampling density. once simplicial depth is assigned to each sample, interpolation methods can be used to create a continuous simplicial depth scalar field from the available samples. we approached this task in two steps."
"one impediment to developing such an interactive application is the speed of the current algorithm. while most stages of the computation can be easily accelerated to interactive rates, the solution for rbf interpolation weights involves solving an n x n linear system in the number of sample points. this is prohibitively slow for a typical system of 1000 or more samples. our plans include investigating fast algorithms for getting a good approximate solution to this system, with special attention to choosing a subset of the samples that minimizes approximation error."
"thus, the width of the cone is an estimate of the uncertainty in the prediction, based on the nhc's own performance in the recent past."
"the primary contribution of this paper is to demonstrate an approach to generating and smoothly interpolating robust statistics from path ensembles, including outlying paths, to produce time-specific visualizations that inherently include uncertainty. as a demonstration piece, we outline the development of a visualization encoding three levels of positional storm-strike risk, for a specific point in time. an example of this visualization is shown in figure 1c . beyond strike position, the methods of the paper should be applicable to the visualization of other predicted variables such as storm speed, wind strength, storm size, and flood risk. the approaches used that will be of interest to the visualization community include:"
"in order to compute test coverage on virtual devices, we need to collect necessary run-time data from the virtual platform. a naïve idea is to capture all necessary run-time data including execution information of virtual devices directly from the virtual platform. however, such approach has three disadvantages. first, we need to instrument virtual devices to capture execution information of virtual devices. second, capturing detailed execution information introduces heavy overhead into the virtual platform. third, we need to decide what kinds of information should be captured before run-time execution of the virtual platform. it is hard to guarantee that captured information is sufficient. once a new metric is added, it is possible that we have to modify the capture mechanism and then rerun the virtual platform to capture more data."
"one common approach to post-silicon coverage evaluation is to use in-silicon coverage monitors [cit] . however, adding coverage monitors to the silicon is costly in terms of timing, power, and area [cit] . in order not to introduce too much overhead, developers can only add a small number of coverage monitors in the design. consequently, the effectiveness of coverage evaluation highly relies on what kinds of device signals are captured by in-line coverage monitors. moreover, such approach of using coverage monitors can take effect only after silicon devices are ready. another approach to coverage evaluation of test cases before silicon devices are available is rtl emulation. however, emulating hardware design has some limitations as we discussed in iii-a. our approach takes the obvious advantages of virtual devices: complete observability and traceability, and is applicable without silicon devices. we utilize test coverage over virtual devices to estimate silicon device functional coverage."
"in previous work [cit], we have developed an approach to post-silicon conformance checking of a silicon device with its virtual device. the conformance between the silicon and virtual devices is defined over their interface states. the request sequence issued to the device is first captured on the silicon device, and then replayed on the virtual device to check if the interface states of the silicon and virtual devices are consistent."
"we associate with each data point i a location v i, a weight w i, and a kernel function φ i . then the rbf interpolation at a given point x, is"
"we have presented an approach to early coverage evaluation of post-silicon validation tests with virtual prototypes, which fully leverages the observability and traceability of virtual prototypes. we have applied our approach to evaluate a suite of common tests on virtual prototypes of five network adapters. we have also established high confidence in fidelity of coverage evaluation by further conducting coverage evaluation and conformance checking on silicon devices."
"approaches to gleaning statistical information from ensembles fall into two main categories: parametric and nonparametric. parametric methods require an a priori assumption of the model describing the data distribution and focus on estimating the parameters (e.g. mean and variance for a gaussian distribution) best matching the data. nonparametric methods attempt to describe the data distribution without any assumption of a model. since we have no basis on which to assume a given model, non-parametric methods seem to be the most attractive choice for our work."
"before a silicon device is ready, post-silicon validation tests can be evaluated using rtl emulation. however, emulating hardware design has certain limitations. first, rtl emulators can be very expensive. second, rtl emulation is often slow. third, it requires a complete working rtl design [cit] to evaluate post-silicon validation tests. recently virtual devices and virtual platforms have been used for driver development and validation before a silicon device is ready. virtual devices are software components. compared to their hardware counterparts, it is easier to achieve observability and traceability on virtual devices. this makes virtual devices amenable to coverage evaluation of post-silicon validation tests."
"a test case/suite issues a sequence of requests to a device. simultaneously, the device may receive environment inputs and read dma data. given a test case/suite, all device events are captured. the replay engine replays all captured events and generates the code coverage, the register coverage and the transaction coverage for the test case/suite."
"the first step is to compute the simplicial depth values for all sample points. the sample points are the predicted locations from a path ensemble, generated to correspond with a storm advisory. we then compute the simplicial depth of each sample point, using the fast algorithm of rousseeuw and ruts [rr96], and sort the sample points in ascending order by their depth. if n is the number of samples, a point's sorted array index, divided by n − 1, is its normalized rank. the set of points contained within a ranking interval can be visualized by drawing its convex hull. figure 4a is a scatter plot of the rainbow mapped simplicial depth values, and the red line is the convex hull of the [0%, 67%] rank interval. although there are good reasons not to use a rainbow color map for displaying levels [bt07], we began by following the nhc's convention for drawing heat maps, moving to a better designed system later in the study."
"post-silicon validation has become a bottleneck in system development cycle and is a significant, growing part of overall validation cost [cit] . to speed-up post-silicon validation, some tasks should be conducted early in the pre-silicon stage, e.g., development and evaluation of post-silicon validation tests."
"post-silicon validation has become a critical problem in the product development cycle, driven by increasing design complexity, higher level of integration and decreasing timeto-market. according to recent industry reports, validation accounts for a large portion of overall product cost. post-silicon validation consumes an increasing share of the overall product development time [cit] . this demands innovative approaches to speeding up post-silicon validation and reducing its cost."
"given an event, a user can check what transaction is explored, what registers are accessed and whether any interrupt is fired. moreover, the user can debug the execution trace step by step using the replay engine."
"as an extension of their ensemble path visualization, cox and house [ch13] began to explore the idea of interactive visualization of path ensembles at fixed points in time. rather than rendering complete paths, showing a prediction over an entire forecast period, they implemented a time slider to fix a time within the prediction and rendered the corresponding point on each path in the ensemble. examples of their visualization for hurricane katrina, with the time set at 21 and 45 hours from the start of the advisory, compared with the corresponding uncertainty cone, shown in outline. while this method highlights the uncertainty in the forecast in both space and time, the naive rendering of predicted positions has clear drawbacks. near the start time of the advisory, data points are tightly clustered, resulting in many overlapping points, making it impossible to take advantage of the point-based display to visually encode other variables using glyphs. at later times in the advisory, there is more spread in the points, but the visual clutter of the display, and the overemphasis of outliers makes the position distribution difficult to estimate visually. in the work being reported here, our goal is to build a coherent display of the distribution of the data, at a specific point in time, starting with similar time-specific point ensembles."
"we construct our replay engine using the symbolic execution engine klee [cit] . we modify klee in three aspects. first, we implement some special function handler for loading events and dma data. second, we capture execution trace during execution of virtual devices. third, we realize our own module for coverage generation."
"[mwk14] built on their ideas to derive statistical characteristics from ensembles of multivariate curves extracted from flow fields, allowing them to draw curve boxplots. as one potential application, they demonstrated how ensembles of hurricane forecast tracks can be summarized using their method. since these methods apply to paths they are not directly applicable to ensembles of points."
"besides the text documents with each advisory, the nhc produces several visualizations to assist in interpreting the information in the advisory. the most well-known of these is officially named the track forecast cone, but is most often referred to as the uncertainty cone or cone of uncertainty. an example is shown in figure 1a . according to the nhc website [noa14a],"
"while this study was intended to support future work on visualization of time-specific predictions from timeparameterized path ensembles, it also stands alone as a project to develop a new visualization tool for evaluating hurricane risk. the next natural step in this side of the work will be to conduct a study comparing how users perform on time and place specific risk evaluation tasks using this visualization versus other proposed alternatives, including the uncertainty cone itself, and a scattered point approach displaying color-coded path samples."
"minimum enclosing ellipses have the property that they preserve the aspect ratio of a region along two orthogonal axes. this orthogonality corresponds to the two sources of uncertainty in the prediction: hurricane bearing, and speed. although their effects are not entirely independent, speed uncertainty tends to manifest in elongation of the risk region along the predicted path, while bearing uncertainty tends to broaden the region orthogonal to the path. figure 6 shows three snapshots of a hurricane isaac advisory, with the risk regions presented in this way. to determine the center, lengths of minor and major axes, and the rotation angle of the ellipses, we use an image moments-based algorithm proposed by [rvc02] ."
"in a step towards providing more time and locationspecific information, the nhc recently began augmenting an advisory with an ensemble of potential paths generated using monte carlo methods that follow an advisory's path prediction, while accounting for its uncertainty (personal communication: [cit] ). figure 1b is an example of such an ensemble, containing 1000 paths, for the same advisory as the uncertainty cone. since the paths in an ensemble are sampled in time, they can carry with them time-based predicted storm characteristics such as storm size and wind speed, and since the paths are projected geospatially, they can be used to produce time and position based visuals. for example, the nhc uses them to produce \"heat maps\" of wind speed probabilities across the region predicted to be affected by the hurricane [noa14c] . while such heat maps can be used to provide useful information, because they are spatially sampled on a grid they are coarse grained and subject to artifacts due to undersampling."
"if f i is a scalar value known at each data sample, and we impose the condition that f (x) interpolates the available data, for each data sample i we have the linear combination"
"the us national hurricane center (nhc) begins posting advisories when a tropical storm, in either the atlantic or eastern pacific region, develops into a cyclone, meaning an \"organized system of clouds and thunderstorms that originates over tropical or subtropical waters and has a closed low-level circulation.\" [noa14d] advisories take the form of several text documents, including the forecast advisory, which, along with other information, includes the storm center's predicted latitude and longitude, wind intensity, and storm size for 12, 24, 36, 48, and 72 hours from the time of the advisory. advisories are issued every six hours at 04:00, 10:00, 16:00, and 22:00 us eastern standard time. they are downloadable from [noa14b], and easily parsed to extract prediction information."
"another challenge is whether coverage estimation on virtual devices can really reflect functional silicon device coverage. although both virtual devices and silicon devices are developed according to the same specification, whether they conform to each other is still a major concern."
"in our approach, we use coverage evaluation of virtual prototypes to estimate functional coverage on silicon devices. in order to make our approach practical and reliable, we need to address the following two key challenges: 1) accuracy: in our approach, we capture run-time data from the concrete execution of virtual devices within a virtual platform. events (e v ) issued to virtual devices within a virtual platform can be different from events (e s ) issued to silicon devices within a physical machine for the same tests. the concern is whether the coverage (c v ) computed on (e v ) is a good approximation of the coverage (c s ) computed on (e s )."
"a criticism of our approach is that, as time progresses into a prediction, the sizes of the risk regions increase, leading to the very strong perception that the storm itself is increasing in size. this problem is also inherent in the nhc uncertainty cone, and indeed in any geospatial display that attempts to track dispersion in a prediction using a summary display. this is not a soluble problem, as long as spatial extent is being used as an uncertainty measure. as yet unpublished studies, underway in our research group, are producing strong evidence that ensemble displays do not induce this same perceptual anomaly. therefore, our research plan is to build on the work reported here, resampling of the simplicial depth field in a well-distributed way to produce a set of exemplar storm positions that can be displayed as points, but without the visual clutter and confusion of the early work of cox and house shown in figure 3 ."
"we have applied dca to qemu-based virtual devices for five popular network adapters: intel e1000, broadcom tigon3, intel eepro100, amd rtl8139 and realtek pcnet. while our tool currently focuses on qemu-based virtual devices, the principles also apply to other virtual prototypes. the experiments were performed on a desktop with an 8-core intel(r) xeon(r) x3470 cpu, 8 gb of ram, 250gb and 7200rpm ide disk drive and running the ubuntu linux os with 64-bit kernel version 3.0.61."
"one of our eventual goals is to develop an interactive ap- plication that embeds this approach to storm position visualization, allowing the user to \"scrub\" through time. interpolation in time is easily achieved via any one of a number of interpolation methods across the known data points in a path in the ensemble. these are every hour in the nhc ensembles, and every three hours in the method by cox and house. in our current work we are using simple linear interpolation, after determining that using a higher order method produced visually indiscernible results."
"coverage evaluation in the post-silicon stage often requires instrumenting the device driver and comes too late. coverage evaluation on virtual prototypes can be available much earlier; therefore, it can guide improvement of post-silicon tests. from conformance checking results and coverage report comparison, it is clear the more conforming the virtual and silicon devices are, the more accurate the coverage evaluation on the virtual device. even if there exist inconsistencies, conforming checking facilitates quick correction of coverage estimate in the postsilicon stage by conveniently detecting these inconsistencies."
"code coverage is a typical measure used in software testing. virtual devices are software models. we can apply all code coverage metrics to virtual devices. we select four common coverage metrics: function coverage, statement coverage, block coverage and branch coverage."
"in algorithm 2, we first reset the virtual device to get the initial device state s. we assume that the internal states between the silicon device and its virtual device are the same after resetting devices. even if both internal states are not exactly the same, a few differences should not cause a large number of functional differences according to device specifications. we take the captured device state ss ik and e k+1 as inputs to replay one event. the virtual device is executed with s and e k+1 to compute the execution information and the state s after processing e k+1 . then conformance checking is conducted between the computed interface state s i on the virtual device and the captured interface state ss i(k+1) on the silicon device to detect inconsistencies. after replaying one event, we keep the internal state and load next interface state captured to compose the device state. after replaying all events, we can get coverage reports and inconsistency report."
"the second step is to interpolate across the evaluated simplicial depth values in order to provide a smoothly varying continuous representation. one interpolation method is to splat each point into the map, which produces results like those in figure 4b, where a transparency is applied to each splat proportional to its depth. splatting leaves many uncolored regions, and depends on sampling to a spatial grid. to overcome these problems, radial basis function (rbf) interpolation can be used, which produces a depth value anywhere in space, and can be used to produce very smooth visualizations, as illustrated in figure 4c . while this is a big improvement over splatting, with the central region filled smoothly, the outer region is highly serrated. this is because we were using a constant rbf kernel spread parameter, with data samples that are very unevenly spread."
"therefore, we developed an online-capture offline-replay approach to capture minimum necessary data at run-time, and then replay the run-time data on the virtual device itself offline to collect necessary execution information. a device can be treated as a state transition system. as shown in figure 3, given a device state s k−1 and a device event e k, the device will transit to a new device state s k . therefore, with the initial state s 0 and the whole event sequence seq, we can infer all states and reproduce all state transitions. in other words, capturing s 0 and seq from the concrete execution of a virtual device within the virtual platform should introduce the lowest overhead and deliver the most effective data."
"we utilize the coverage evaluation and conformance checking results in three aspects to assure the coverage estimation accuracy. first, we compare c s and c v to detect differences. if we can verify that there is no difference or few differences between c v and c s, we can better trust that c v can be a good approximation of c s . second, the number of inconsistencies provides basic measurement how many differences there are between the silicon device and the virtual prototype. after analyzing the inconsistencies, we further evaluate whether these inconsistencies cause different device behaviors. if there are few inconsistencies found and there is no significant effect on the device, it can increase our confidence on coverage estimation. third, it is easy to fix the detected inconsistencies on the virtual device so that the fixed virtual device conforms with the silicon device. then we compute coverage again on the fixed virtual device using the same test cases. by comparing the coverage report on the fixed virtual device with that on the silicon device, we further verify that the differences in coverage caused by the inconsistencies are removed."
"a problem with the ellipse representation became apparent while producing an animation to simulate scrubbing through time. when the ellipses become nearly circular, the choice of minor and major axis is not stable, leading to rapid 90"
"we have presented a visualization technique to provide exploration of time-specific predictions from an ensemble of potential hurricane paths. these paths are sampled in time, to create a set of points for each time period, which are assigned a scalar value associated with their simplicial depth. we then create a scalar field over the region covered by the samples using radial basis function interpolation. using this field, we determine risk regions based on simplicial depth, and render them using best-fit elliptical approximations. the approach has been shown to be robust across a number of storm predictions, and across two different monte carlo path ensemble generation approaches."
"to generate coverage reports, we first analyze virtual devices statically to get program information, such as the position of branches and the number of functions, and then generate all kinds of coverage reports based on the execution traces computed by the replay engine. our approach provides flexibility to generate reports on two different levels:"
"since the gaussian kernel has infinite support, the solution matrix tends to be densely filled. we use this kernel because of the very broad spread of our data points, but kernels of finite support might be used to advantage in speeding computation by exploiting matrix sparsity."
"this work provides a simple geospatially located visualization, incorporating uncertainty, and keyed to a particular point in time. our intent is that this will form the basis for future research leading to a set of interactive tools for exploring a hurricane prediction in both time and space. given the structure for spatial interpolation that we have developed, it should be possible to interpolate storm parameters other than strike risk, such as storm speed, bearing, wind speed, size, and flood risk. this has the potential to enable development of an integrated hurricane prediction visualization application to be used in the field by emergency managers."
"while the images shown in figure 5 are close to what we envisioned, the colored depth intervals are somewhat irregular shapes, unlike the standard blue dots. the irregularity is induced by the monte carlo ensemble generation process and does not carry any useful information. since the irregular risk regions are already nearly elliptical, we decided to replace them by minimum enclosing ellipses, rather than circular dots."
"virtual prototypes are fast, fully functional software models of hardware systems, which enable unmodified execution of software code. virtual prototypes are running within virtual platforms such as synopsys virtualizer [cit] and qemu [cit] . to better understand the concept of virtual prototype, we illustrate it with a qemu virtual device for the intel e1000 gigabit network adapter. as shown in figure 2, the e1000 virtual device has the following components: 1) the device state, e1000state, which keeps track of the state of the e1000 device and the device configuration; 2) the interface register functions such as write_reg which are invoked by qemu to access interface registers and trigger transaction functions; 3) the device transaction functions such as start_xmit which are invoked by the interface register functions to realize the functionality; 4) the environment functions such as receive which are invoked by qemu to pass environment inputs such as a packet received to the virtual device. both transaction functions and environment functions may access dma data by invoking dma functions pci_dma_read or pci_dma_write, and may fire interrupts by calling interrupt function set_irq."
"this gives us a spread parameter that adapts to the density of sample points in the neighborhood of each sample point, and provides smooth interpolation across all of the samples. example visualizations using this interpolation approach are shown in figure 5, with the nhc uncertainty cone shown in blue for reference. these examples also move away from the rainbow color map, using a color encoding meant to clearly show three nested risk regions."
"the replay process is independent of the virtual platform/physical machine. once runtime data is captured, users can replay the event sequence and reproduce the execution at any time. based on different user requirements, users can generate different coverage reports from the replay process with different metrics."
"we applied conformance checking to detect inconsistencies between e1000 and tigon3 and their corresponding virtual devices. there are 13 inconsistencies discovered between the two network adapters and their virtual devices under the given tests: 7 in intel e1000 and 6 in broadcom bcm5751. we modified 21 lines of code in virtual devices to fix all 13 inconsistencies. then we rerun coverage tools on fixed virtual devices to generate new coverage reports. after comparing the new reports with the post-silicon coverage reports, we found no differences except the known transaction differences."
(1) we will research on how to define new coverage metrics which can be used for better evaluating hardware coverage. (2) we will investigate how to utilize the coverage results to measure the validation completeness and guide test generation.
a device request is denoted as r which is received by the device from either the system software or the environment. the request r is either r ir or r ei .
"while this visualization gives an overall view of the path of the hurricane and its associated uncertainty, it does not facilitate important time and location-specific queries such as \"what is the likelihood that the storm will hit my area, with hurricane strength winds, by 8:00 a.m. on friday?\" emergency managers, responsible for planning in advance of an oncoming hurricane, are anxious to have such time and location-specific information readily available (personal communication: [cit] ). in addition, moving away from path-based predictions to time and location-specific predictions would facilitate the superposition of multiple storm variables, such as wind speed and storm size, on the display."
"equation 1, together with the above method of finding a set of weights, forms the basis of our method for turning an ensemble of predicted storm centers into the smooth continuous function of simplicial depth."
"experiments are performed to analyze upp-sne's sensitivity to three parameters: (1) m: the dimension of learned node representations; (2) γ: the number of random walks from each node; and (3) t: the window size for collecting context nodes. in turns, we fix any two of the three parameters and study the effect of the third one on the classification accuracy. figure 2 shows the accuracy of multi-class node classification on google+, hamilton, and rochester, when the training ratio is set as 5%. we can see that the performance of upp-sne is stable with respect to different values of the parameters."
"to evaluate our method on node clustering tasks, we apply kmeans to the learned representations of nodes and use accuracy and nmi [cit] ] to assess the quality of the clustering results. we repeat the clustering process 20 times to reduce the sensitivity of k-means to the initial randomly selected centroids, and report the averaged results. figure 3 reports the clustering results on google+ and ego-facebook. as we can see, upp-sne consistently yields the best clustering results. on google+, compared with the second best results, upp-sne achieves 200% improvement over line-2 on nmi, and 4% improvement over lane on accuracy, demonstrating the superior performance of our method. on ego-facebook, upp-sne significantly outperforms tadw, yielding 14% and 11% performance gains on accuracy and nmi. this again confirms upp-sne's better capability in handling and incorporating noisy user profiles to learn social network representations."
"the federation concept applies to a wide set of environments: isp interconnection, peer-to-peer systems, the grid, and cloud computing, are all cases in which independent administrative domains can share their local resources and services for their mutual benefit. numerous studies in the literature try to capture the most important dimensions of these resource exchange economies."
"authorization: the system shall require strong authorization while accessing data, as it manages several user's information simultaneously. third-party sensor authorization is also needed because it affects the system's accuracy and privacy. therefore, it is important to ensure the device's good operation due to system criticality."
"virtualization technology has augmented the potential for sharing computing and network resources. bandwidth, storage, cpu, and services belonging to different organizations could in principle become part of a global commodity infrastructure that can be used for a wide variety of network services and applications. cloud computing, distributed experimental facilities, and the grid are examples of virtualization-enabled syspermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. [cit], november 30 -december 3 2010, philadelphia, usa. [cit] acm 1-4503-0448-1/10/11 ...$ tems that can augment their value to users by pooling resource contributions from multiple organizations."
"in our work so far we have only scratched the surface of this problem. our simple models and examples demonstrate that simple contribution-or consumptionbased valuation schemes do not properly reflect the value of a player in the federation. we also showed that a shapley value-based approach leads to a much more fair valuation of resources. the approach leads to quantification of intuitive notions of the value and bargaining power of the players depending on the demand of the users. diversity was one simple demand parameter that yielded shapley values that departed significantly from proportional valuations. we believe our approach extends easily to other forms of demand patterns and can lead to economically sound, incentive-compatible profit sharing schemes that can have practical use in the ongoing federation efforts around the planetlab facility. they can also be used as a basis to study the global federation game both for the commercial and the p2p cases, and other future internet scenarios."
"to achieve this objective we made a first step toward assessing the importance of various important system parameters to the relative contribution of the members of a federation. our numerical analysis provides a deeper understanding of how different profit sharing schemes perform under various utility functions, expected minimum requirements of users in terms of diversity, and volumes of demand."
"then the facility's contribution to the geographic diversity of a federation could be expressed through the number of distinct locations at which it provides resources,"
"the acceptance of cyber-physical systems in society depends on trust from users that must be earned. this trust can only be gained by providing adequate security goals to users. security goals aim to protect the system from threats and vulnerabilities and reduce risk factors. we aim to extend our understanding of security goals for cpss. for instance, in the case of sensor data oriented systems with multiple sensor nodes generating data, security issues are crucial to see if the data generated is coming from a trustworthy source. this shows that authentication, availability, integrity, and confidentiality are very important security goals in cpss. the following are important security goals: 3.1.1. authentication nodes (sensors) should be identified and authenticated before adding them to the network [cit] . authentication in a cps is considered difficult to achieve as, in some cases, it requires heterogeneous network authentication. the lack of an authentication process can lead to exposure of the network/information to an unauthorized user."
"role-based access control: different authorized users have various roles in the system. these roles define the access limit to a diverse data set, hardware, or screens present in the system. the system maintains the access control list to define the assigned task of reachability in the resources. this can be facilitated by login id, profile detail, port, network channel, and important addresses."
"this paper examines the case of the planetlab experimental facility [cit], which has recently been structured to operate as a federation of regional planetlab testbeds (planetlab in north america, planetlab europe, etc.) that each provide their respective users with full access to the global system [cit] . this federated system provides a geographically distributed set of computing and network resources. on top of planetlab, users can deploy a network overlay to run a peer-to-peer service or a content distribution network, experiment with novel internet architectures in realistic conditions, perform location-sensitive measurements, etc."
"in our context, the number of different requested locations is the most important dimension of the resources assigned to an experiment. the utility of a networking experiment is typically zero if it cannot be run in a sufficiently large number of locations. so it is reasonable to assume a minimum threshold level of locations required by a user."
"the sensor and actuator of car parking lots are situated in a physical environment. therefore, such an environment needs to be protected against tampering and unauthorized access. sensor communication uses different channels to receive data from the physical environment. we have used an ieee 802.15.4 based communication protocol in a smart car parking system to secure communication."
"in table 1, we summarize major characteristics of information sources available for network embedding. our analysis and empirical study confirm that user profiles are largely different from node content features, and therefore existing rich node content based network embedding methods are ineffective in handling user profiles for representation learning."
the internet connection should be secured by a password. internet must be protected through a passcode based on car id or user id to ensure use by a dedicated user.
"the significance of addressing security is widely accepted from the very beginning of system development [cit] . generally, the security of software is not considered at the very beginning of a sdlc; it is only incorporated in the later stages of software development [cit] . as a consequence, there are increased risks of security threats that are introduced in various stages of software development [cit] . therefore, integrating security requirements right at the beginning not only ensures secure software, but also saves precious time and reduces the effort of reworking by software development team. hence, it is evident that to support the process of security requirements, we need a security requirements framework for cpss."
"the purpose of security requirements engineering (sre) framework is to identify security requirements. meanwhile, there is no comprehensive security requirements engineering framework available for cpss, since the nature of cpss is quite different to classical software systems because of the cps characteristics of heterogeneity and adaptability. therefore, we propose a security requirements engineering framework that provides ways to determine security requirements throughout the requirements engineering (re) phase, which consists of a number of activities to elicit and finalize the security requirements for cpss. these activities identify security to avoid potential consequences of attacks for a cps. the purpose of this framework is to develop early security concepts in the requirements engineering phase. this quest leads to re methodologies so that security concerns can be addressed during the early stages of software development. the proposed framework is a systematic approach to incorporate security goals, threats, and risk assessment that are critical to the cps. we have a set of 8 main activities, and one important technique called misuse case as shown in figure 4 . a misuse case is operated like a use case but it is inverse, i.e., a function should not permit the system to operate in a normal condition [cit] . this technique is common across all the processes of main activities. moreover, this cps framework offers complete guidance to practitioners and researchers to determine security requirements. the framework recognizes the activities that are essential for requirement analysts to follow in order to identify the security requirements for cps."
"in this activity, we analyzed threats, assets, and the expected risks that can affect them. we have identified the impact of each risk on the asset, and categorized them into 4 values. in the following section, we have provided the few important risk analyses. table 2 shows the impact range from 0 to 4. four is the highest value and 0 the lowest. the large value of risk cost shows the high impact of risk. therefore, important attention needs to be paid to the high risk cost."
"radio frequency (rf) jamming aims to paralyze communication from the physical environment. this may interfere with the interaction of sensors to plc or any gateways. usually, the radio frequency jamming occurs with radio signals to detach the tag through electromagnetic waves or high level traffic of signals [cit] ."
"data sniffing: over the channel or from the server, a sniffer can get user data over the transfer channel of the user app or database server. this could happen if encryption is absent. 2."
"federation of computing and network resources provides two benefits, generally speaking, to users: increased capacity and increased diversity. increased capacity reduces the completion time of computation-intensive tasks and enables faster communications. diversity provides value through the complementary characteristics of resources, such as their geographic location, technology, attached users or services, reliability against natural disasters through redundancy, etc."
"more sophisticated federation policies will soon be needed for planetlab to succeed in becoming a hierarchical federated system that offers both increased resource capacity and diversity to its users. currently, the main focus is on federation at the top level of this hierarchy (plc-ple-plj, etc.). we will treat this as a single layer by making the simplifying assumption that each authority has full control over the amount of resources that it brings to the federation. in future work, we will study the interdependencies between local and global federation policies."
"the focus of this paper is precisely on how to decide on the values s i and not on how to maximize the total profit. thus the above simplifications for profit maximization do not change the qualitative results that we obtain. we could equally well use any predefined price function that depends on external factors (e.g., competing services provided by cloud computing providers)."
5 when experiments are easy to multiplex or when all facilities are required to cooperate for any experiment to be meaningful. the latter is the case for the internet (where full connectivity is the minimum requirement).
"notice that sharing p efficiently is an issue that already arises in the planetlab context, as subscription fees are paid by industrial users of the system, such as google and hp. the default policy at present is for each top-level authority (plc, ple, eventually plj) to retain the totality of the fees that it brings in."
google+ 1 is an ego-network of a google+ user and nodes in this network represent the user's friends. there are 1206 nodes and 66918 links in this network. we use people's gender as class label. each node is described by a 940-dimensional bag-of-words vector constructed from treestructured user profiles [cit] .
"business goals, quality attributes are combined to develop the required security decision. this is achieved to identify security goals. security goals mainly refer to confidentiality, integrity availability, and authentication."
this paper is organized as follows. sec. 2 formulates our economic model by defining the main actors in federation and its value. sec. 3 presents the high-level federation game and the different theoretical optimization problems that need to be solved in order to compute the optimal policy based on the model's assumptions. sec. 4 presents some interesting insights using different numerical examples since the general problem is not analytically tractable. sec. 5 discusses related work and places our contributions in context. we summarize these insights and discuss the possible future directions of our work in sec. 6.
"security requirements engineering is an essential aspect of cyber-physical systems, but there is a lack of methodology defined to develop a secure software system. though many methodologies and frameworks have been proposed for software, there is still a need to improve them [cit] . many researchers address the requirements engineering best practices and highlight the importance of system functionality, but a small amount of attention has been given to what the system should not do [cit] ."
"notice that capacity-hungry services (e.g., computationally intensive jobs) have lower holding times as the amount of resources made available to them increases. on the other hand, diversity-hungry network experiments require as many network nodes as possible but their holding time remains a wholly independent variable. this property of holding time can significantly affect the level of statistical multiplexing achieved under different federation scenarios. in our model, we take this into account by assuming that holding time is part of the request and independent of the available capacity in the system. this assumption captures an important differentiating factor of networking experimental facilities compared to grids and internet services."
"in this type of attack, the attacker could harm physical devices such as hardware, sensors, cameras, terminals etc. such attack could threaten human lives and this must be prevented at any cost. if these systems were attacked from outside, this could cause great harm. natural disasters can also lead to a loss of human lives, and the sensors or actuators would be unusable, and the financial damages would be significantly higher [cit] ."
"the core question is how to share the extra value that federation brings to its participants. the answer will come in the form of a federation policy whose parameters depend on the special characteristics of the system: the types of resources it offers, the locations of the customers, the levels and type of demand, etc. if properly conceived, this policy will provide incentives to potential members that encourage them to share their resources."
"sensors and actuators are devices that communicate with the external environment. the sensor generates data regarding the object with which they are in contact. these data are received through an application program interface (api), and pass to a programming logic controller (plc) and supervisory control and data acquisition (scada). some of the higher-level sensors provide support for the cloud of centralized data for data broadcasting. these sensors use machine to machine (m2m) protocols for communications. since different sensors depict different mediums, we have proposed to identify all such factors during sensor analysis."
"confidentiality is also a subject of major importance within sensor networks. networks could be used, in the worst case, to spy on individuals [cit] . an example would be the long-term monitoring of persons or vehicles on a routine basis. another aspect that affects the security of sensor networks are attacks on these networks communication between physical environment and gateway to controller/server, as shown in figure 2 . in the simplest form, the attacker sends a high-energy signal to the sensor to prevent communication within the physical layer to the network layer. this can lead to serious consequences, especially in the case of security-critical cps. military applications are also threatened by such attacks. a possibility to combat these attacks lies in the nature of the networks themselves. if a part of the network has been compromised, this part can be demarcated and the communication can be routed around it [cit] . therefore, it is of utmost importance to address the security of all components (i.e., physical layer, network layer and application layer) of cpss."
"in a compromised-key attack, the attacker gains access to a key for the system, and thus, can modify data. the attacker can also access other areas of the system. this is done without the actual users of the system being aware of it [cit] ."
"secure tropos is a goal-oriented methodology for eliciting security requirements [cit] . the modelling concept of secure tropos focuses on security constraints, security dependencies, secure goals, secure tasks, and secure resources. an actor represents an entity with strategic interests and intentions. an informational entity is referred to as a secure resource. actors can be linked by dependencies, indicating that one actor depends on the other to attain some goal, execute some tasks, or deliver a resource. international standard common critical [cit] provides support for evaluating security requirements for information systems."
"a denial-of-service attack floods the system with data; thus, the normal operation of the system cannot be continued [cit] . for instance, this can lead to users being unable to retrieve their mobile bank data. however, more critical scenarios are also conceivable."
"it is recommended to use only authenticated endpoint hardware. this activity involves the identification of supporting hardware that may include a sensor, machine, router, reader, point-of-sale terminal, server and smart devices."
"the reason is that when demand is low, certain value generators, such as diversity, should be treated as nonrivalrous goods. understanding in more depth how the outcome of our global game is affected by the inclusion of actual demand in policy design will shed more light on the debate between markets versus rule-based schemes for resource management in p2p systems [cit] ."
"recently, a series of algorithms have been proposed for network representation learning (nrl), such as deepwalk [cit], line [cit], grarep [cit], and node2vec [cit] . these approaches have been shown to be effective in a variety of network analytic tasks, ranging from node classification [cit], anomaly detection [cit], community detection, to link prediction [lü [cit] . however, most of them have considered network structure only, e.g., the links between nodes, but ignored other user-generated content (e.g., text, user profiles) that could potentially benefit network representation learning and subsequent analytic tasks."
"clearly, the existence of the core for a given game is of significant importance and it has been studied extensively in the literature (see [cit] for a recent survey of applications of this concept to communication networks). the most important properties of the game that determine the existence of the core are superadditivity and convexity. both depend significantly on the utility function assumed."
"in the p2p scenario it is through resource allocation schemes that the total value will be shared. there are different ways for the system designer to differentiate the quality perceived by users of different facilities based on contribution [cit] . in the following, we will constrain ourselves to those that affect the core parameters of our model: capacity (r) and diversity (l)."
"cpss are in the initial stages of development, and therefore, face many challenges. security is one of the challenges of cpss. different studies show that cyber threats have increased in the cps environment, and there is a need to do more research to systematically handle security requirements [cit] . recently, many incidents of cps attacks have been reported in the literature. [cit], stuxnet was the first cyber-attack on a cps. it targeted a siemens control system 'supervisory control and data acquisition (scada)' through malware to control and destroy iran's nuclear program [cit] . as a consequence, more than 50% of the iranian nuclear infrastructure was attacked. certainly, this incident creates an alarm for cyber threats [cit], an australian man was found guilty when he attacked the maroochy waste management systems and released one million liters of impure sewage into rivers and local parks [cit], a hacker infiltrated a us water filtration plant with malware that changed the levels of chemicals being used to treat tap water, and thousands of homes were affected in illinois, usa [cit] . other famous cyber-attacks are duqu and flame, which were used to gain unauthorized access."
"planetlab is the largest geographically-distributed experimental facility yet constructed for computer networking. it is at its base a federated system since its infrastructure is built from the individual contributions of participating universities and research institutions, the sites, which are each required to contribute at least two servers, or nodes, connected to the internet. when an institution fulfills this requirement it obtains the right to deploy up to ten slices across the facility. a slice consists of one virtual machine on each of a set of nodes. each slice benefits from a short-term fair allocation policy (i.e., competing slices acquire a fair share of the available resources at each node and in each time slot)."
"the purpose of this activity is to evaluate a risk-related impact. the process elicits the possible risk that might occur in the security requirements. assets and threats of the system are also identified in this step. the impact of the risk on the asset is organized into groups, ranking the impact from 0 to 4 on the scale of low to high. the ranking impact is then used to calculate the impact factor of each risk. the impact cost sums the value of identified risk cost. ranking and expert opinion are applied on impact factor of each risk. the highest value of risk cost should be specified high priority while eliciting security requirements. the impact factor of each risk is calculated by the following formula:"
"these varieties of attempts make difficult the choice for security analysts to select the appropriate security requirements engineering methodology according to their needs and expectations. all of these methodologies provide support for software security, but all of them work in isolation. a model that supports a goal perspective does not consider the activity of software development. similarly, a model based on the information system does not consider other systems. furthermore, all of these models are supporting software services or component and their relationships. however, very little attention has been given to hardware, communicators, receivers, or system security issues. therefore, it is very important to explore security methodologies, especially in the domain of cps."
"a simple and intuitive way to express this type of utility would be with a simple step function, which could be linear, concave, or convex after a certain threshold. this is the utility function that we will consider in our examples below:"
"which demand is low differs significantly fromπ i . an alternative to the shapley value and the simpler proportional fair schemes presented above, is the nucleolus function, which is a more complex mechanism [cit] . simply put, the nucleolus function achieves a max-min fair allocation of profit among all possible coalitions. it achieves this by recursively maximizing the excess valuefor simple fairness schemes such as the equity approach which always divides the profit equally among all the members of the grand coalition. thus, in the following we will focus on comparing shapley value with the two above types of proportionally fair profit sharing. in sec. 5 we discuss how our work is related to marketbased approaches (e.g., [cit] )."
"indeed, several very recently developed algorithms [cit] have attempted to utilize node content information, such as textual features of each node in citation networks, for effective network representation learning. these existing works have confirmed that node content indeed provides crucial information to learn better network representations. however, as we will soon demonstrate in section 5, these methods are mainly designed to consider consistent node content, but fail to work for user profiles in social networks. this is mainly attributed to two reasons. first, today's online social networks rely on users to manually input profile attributes, so attributes in profiles could be very it is thus very difficult to find useful information from user profiles that could complement network structure towards learning a joint vector representation of social networks."
"security is considered to be the core aspect for the strength of the software. several types of research have proposed different methodologies, models, and frameworks to support security requirements elicitation and maintenance. the most famous security requirements frameworks are square, srep coras, ms sdl, umlsec, and secure tropos. therefore, in this paper, we are providing a review of only major security requirements frameworks that have significant contributions in the literature. a detailed literature review has been conducted to summarize the evidence, and the main contributions of each paper have been examined. system quality requirements engineering (square) is a security requirements engineering framework and comprehensive methodology to determine security requirements for software [cit] . the framework provides a means to elicit, categorize, and prioritize security requirements for a system. the objective of the framework is to build security concepts in the early stages of the software development lifecycle, and integrate effective security requirements engineering into system development processes. the framework consists of nine activities, and has been implemented in the development of software to elicit security requirements [cit] ."
"in this paper we will ignore resource provision costs, assuming that facilities have invested in their infrastructure prior to federation. in practice many of these costs are subsidized by research funds (e.g., [cit] ) and in general are similar for different institutions. so, our solutions for dividing the value will not be significantly affected by the actual costs involved."
"as reflected on tables 3 and 4, on hamilton and rochester, tadw performs even worse than the only network structure preserving nrl algorithms. it implies that, on the two facebook networks, structural relationships are more indicative if noisy user profile information is not properly exploited. this observation also proves that the linear mapping used by tadw cannot effectively filter noisy information from user profiles, thereby degrading its performance."
"much of the value of planetlab comes from its geographic diversity. its approximately 1,000 nodes are widely situated, allowing experimenters to deploy overlays on the internet at a global scale. this value is attested to by, for example, one particularly high profile user of planetlab: google pays an annual fee and uses planetlab to check the accessibility and performance of its services from different geographic locations."
"in order to elicit security requirements for a smart car parking system, the security goals, assets, and threats are analyzed, together with the security risks. all security goals, assets, threats, and risks of the system are subjected to detailed analysis by the stakeholders. on this basis, we have determined 40 security requirements for a smart car parking system, as shown in table 3 . table 3 . security requirements."
"it is estimated that the software development budget to fix security flaws is almost 75% of the total cost after handover of the product to the customer. this is an enormous amount of spending that builds untrustworthiness amongst customers [cit] . figure 1 states the number of yearly spending on information technology (it) security, which shows an escalation of approximately 8% annually [cit] ."
ego-facebook 1 is an ego-network of a facebook user. this network consists of 755 nodes and 60050 links. people's education type is used as class label. each node is described by a 477-dimensional vector [cit] .
"this means that while in the commercial scenario it is in the interest of the facilities to maximize the total utility of the customers (and thus the profit), the requirement for incentive compatibility might force a coalition to a suboptimal solution in terms of total utility generated. our optimization problem in this case takes the following form:"
"the unified modelling langauge for security (umlsec) is a uml-based modelling method to integrate security features and analysis in the design phase [cit] . umlsec is used to evaluate uml specifications for security threats and vulnerabilities. the extension of uml is provided through the standard uml extension mechanism by the addition of few elements. security requirements that evolve from modelling the systems design with umlsec cover the main security goals, specifically confidentiality, integrity, and availability. an approach based on goal perceptive combines umlsec and secure troops for delivering secure system [cit] . this approach considers both technical and social aspects. they perceive the support from umlsec for model translation to ensure the proper application of security requirements at a design level. the authors in the paper [cit] have proposed a goal-based security model to handle software product line issues. it does not provide the support for physical security aspects."
"using this model we demonstrate that simple proportional sharing mechanisms can lead to solutions that are considerably different from the optimal ones dictated by coalitional game theory. our objective is to apply in practice this and other insights gained from our numerical analysis in the design of realistic policies for the emerging planetlab federation, as discussed in sec. 4 ."
"more specifically, the shapley value mechanism creates powerful incentives for resource provision around the threshold points when diversity is important. this is a potential weakness of using the shapley value since it could cause instability in the system. however, as already mentioned, we do expect the shapley value to be used more as an input to the complicated process of policy design rather than an absolute policy parameter calculation tool. for example, one could compute off-line the valuesφ i using some more detailed versions of the examples provided above based on the system characteristics in terms of expected demand. then these values could be used as generic weights for sharing profits and resource allocation decisions instead of the proportionally fair ones."
"work on isp interconnection has focused largely on the economics of peering and transit agreements (e.g., [cit] ). there are also more theoretical studies on incentive mechanisms for optimal routing (e.g., [cit] ), and the effects of interconnection policies on the internet graph (e.g., [cit] )."
"we can also see that, lane suffers from the same problem in exploiting user profiles to learn social network representations. lane learns latent user profile and network representations separately, and project them to get the final representations. because the learning of latent user profile representations and latent network representations is uncorrelated, the interplay between user profile and network structure is not well captured, resulting in unsatisfactory performance."
"the utility function expresses the satisfaction of a user acquiring a certain amount of resources to run her experiment. it is typical to assume that it is a concave function of the acquired resources, which is often a critical assumption for the analysis to be tractable."
"in this section, we review two lines of nrl algorithms, namely network structure preserving nrl methods that are based on network structure only, and content augmented nrl methods that combine node content with network structure to enhance network representation learning."
"more specifically, the smaller the t k s, the more chances for the game to be super-additive and convex. note that this is the typical case in which grid federation would be meaningful [cit] . in our scenario, it is also the the importance of diversity that significantly affects the convexity of the game: as l grows, the more small coalitions are of zero value and thus the comparative value of the grand coalition increases, turning the core nonempty. on the other hand, when d 1 the core always exists."
the data being transferred within the network is inaccessible to an unauthorized user. the risks associated with confidentiality involve exposure of network information to an unauthorized user [cit] .
"certain resource attributes are easier to provide than others. for example, many organizations have just a few locations and cannot easily increase the geographical diversity that they provide. other attributes depend on long-term investments such as the number of nodes and their resource capabilities. finally, there are attributes such as reliability that depend on investing ongoing effort. in general, the cost function of a facility i depends with different intensity on all three"
"a simpler notion of contribution is the one considering only the amount of resources shared by different facilities either in terms of availability or consumption. proportional sharing schemes based on this measure of contribution have been recently studied in the context of scientific grids (see, for example, [cit] )."
"integrity: data is strongly related to the accuracy of the system. therefore, its integrity is important. any unwanted update can cause extreme issues, such as two users claiming one parking space, which can cause an issue over vehicle and user ownership, or even accidents by providing improper free lot information."
"gridecon is a spot market based on double auctions, which trades virtual machines for a specific duration. in terms of profit sharing, it has similarities with the parallel synchronized markets proposed for bandwidthon-demand services [cit] . in all these cases, profit between independent organizations is shared implicitly through the market ignoring the possible complementarities in the valuation of the users. 7 finally, there are numerous approaches that build on various marketbased resource allocation mechanisms [cit] and typically trade in cpu cycles."
"both scenarios apply to planetlab, which currently has external industrial customers participating by paying a fixed annual fee and internal users, mostly researchers, affiliated with the contributing research institutions and universities."
"plc and ple each run almost identical instances of the myplc operations server software and they each follow the same local provision and allocation policies. this makes the implementation of the current federation policy (a simple peering agreement) rather straightforward. but the planetlab federation is rapidly becoming complex: plc and ple are being joined by emerging regional authorities such as planetlab japan (plj), and other testbeds (e.g., g-lab, emanicslab, and vini) are joining the federation through the regional authorities."
"the cps framework has the following 8 activities (a1 to a8). these activities are selected based on their importance vis a vis security requirements engineering [cit], and the characteristics of cyber-physical systems [cit] ."
"we call the process of aggregating administratively isolated resources into a common pool, federation. federation enables more efficient utilization of resources through statistical multiplexing. in many cases it is also the only way for certain demanding services to be deployed at a global scale. the internet is an archetypal example of such a global service: it functions only thanks to federation agreements between pairs or groups of the more than 30,000 autonomous systems."
"the proposed framework is best worked out by the security advisors, the team of requirements analysts, and other stakeholders who are involved in the project. table 1 illustrates the way in which the proposed framework is applied to the cps, which explains the input, technique, output and the name of the activity. the framework is in the form of a workflow process of activities that needs to be followed. the output from each activity represents its completion. the framework proposed an agile methodology to select the required activity. the analyst needs to review each activity and propose changes if required."
"since many cyber-physical systems depend on sensor networks, their security is an important factor to consider, as a malicious attack could harm or damage the physical part of the system. figure 2 shows the security issues on the physical, network, and application layers. the application areas of sensor networks are very wide. they range from monitoring machines in production to military applications. the data are processed in networks, which makes their security critical. this is also due to the fact that sensor networks have new security requirements which are not matched by the security techniques of traditional networks [cit] . one reason for this is that the sensors are partly located in open, accessible areas. this make them more vulnerable, and they become potential attacks. this is an important aspect of security of sensor networks that should be considered for every component of network [cit] . if this is not done, unprotected components are vulnerable to attacks."
"risk assessment does not only identify the risks associated with an individual requirement, but also identifies the risks associated with the interdependencies of various requirements of system development. lack of risk assessment in implementing security requirements of cps can lead to unexpected and undesired behavior of the system, as the developers might miss critical requirements resulting from the interdependency of these security requirements. therefore, we also aim to posit a roadmap on the risk assessment of security requirements in the cps framework. the risks in cpss are based on the requirements of the software and other components of the cps. furthermore, we have analyzed and identified the major security goals and threats of a cyber-physical system. this analysis is based on few matrices that were developed during the implementation of a smart car parking system. the identified security goals and threats of cps are shown in figure 3 ."
"availability ensures that the information is available all the time to the authorized user when required. the risk level has to be increased when there is a denial of service (dos) attack or service distractions as a result of a hardware failure, systems updates, or power failure [cit] ."
"the purpose of security requirements engineering (sre) framework is to identify security requirements. meanwhile, there is no comprehensive security requirements engineering framework available for cpss, since the nature of cpss is quite different to classical software systems because of the cps characteristics of heterogeneity and adaptability. therefore, we propose a security requirements engineering framework that provides ways to determine security requirements throughout the requirements engineering (re) phase, which consists of a number of activities to elicit and finalize the security requirements for cpss. these activities identify security to avoid potential consequences of attacks for a cps. the purpose of this framework is to develop early security concepts in the requirements engineering phase. this quest leads to re methodologies so that security concerns can be addressed during the early stages of software development. the proposed framework is a systematic approach to incorporate security goals, threats, and risk assessment that are critical to the cps. we have a set of 8 main activities, and one important technique called misuse case as shown in figure 4 . a misuse case is operated like a use case but it is inverse, i.e., a function should not permit the system to operate in a normal condition [cit] . this technique is common across all the processes of main activities. moreover, this cps framework offers complete guidance to practitioners and researchers to determine security requirements. the framework recognizes the activities that are essential for requirement analysts to follow in order to identify the security requirements for cps. an adversary can threaten the failure of communication protocols, which may lead to the failure of network communication and physical hardware [cit] ."
"federation is meaningful only if this extra value minus the cost of federation and local resource provision is greater than the value acquired when participants act alone. in general, the assumed utility and cost functions, and the characteristics of demand are the main factors that will affect the estimation of the value of federation."
"security requirements engineering process (srep) is an asset and risk-driven framework [cit] . authors of this framework combined common criteria (cc) with software development life cycle and the requirement repository. they have proposed 9 activities which can be processed in an iterative or an incremental manner. they have suggested that the framework is enhanced with the support of a repository for reusable security requirements. these requirements provided information regarding threats and assets. in another paper [cit], the authors have applied the proposed framework on information system (is) and suggested computer-based tool development for the support of frameworks."
"hamilton and rochester 2 are two of the collection of 100 us university facebook networks [cit] . the two networks contain 2118 nodes, 87486 edges, and 4145 nodes, 145305 edges respectively. each node's profile is described by 7 user attributes: student/faculty status flag, gender, major, second major/minor, dorm/house, year, and high school. we select student/faculty status flag as class label and construct a 144-dimensional, and a 235-dimensional binary feature vector for hamilton and rochester, respectively."
"to develop a system with a security focus, several security requirement frameworks have been proposed. among them, some of the famous ones are square, srep, secure tropos, clasp, coras, and umlsec [cit] . the benefits of these models are limited to the software perspective, and at some point, to supporting the computer hardware. unfortunately, none focuses on the new trend that is cyber-physical systems. cpss get their support from the sensor network, along with other, traditional software. this interaction involves a different form of data processing from and to the outer world [cit] . moreover, cpss require a dedicated communicational channel for secure interaction. the diversity of cyber-physical systems forces the developer to reflect upon details of the security aspect of sensors, receivers, data processors, and communicators, as well as the general software security aspect."
"availability ensures that the information is available all the time to the authorized user when required. the risk level has to be increased when there is a denial of service (dos) attack or service distractions as a result of a hardware failure, systems updates, or power failure [cit] ."
"note that there is also a fixed federation cost c f related to the administrative, technical, and legal aspects. but this cost will only affect the final decision about the benefit for forming a federation, which again in practice will depend on often external factors and will not influence the properties of the different profit sharing policies."
"since many cyber-physical systems depend on sensor networks, their security is an important factor to consider, as a malicious attack could harm or damage the physical part of the system. figure 2 shows the security issues on the physical, network, and application layers. the application areas of sensor networks are very wide. they range from monitoring machines in production to military applications. the data are processed in networks, which makes their security critical. this is also due to the fact that sensor networks have new security requirements which are not matched by the security techniques of traditional networks [cit] . one reason for this is that the sensors are partly located in open, accessible areas. this make them more vulnerable, and they become potential attacks. this is an important aspect of security of sensor networks that should be considered for every component of network [cit] . if this is not done, unprotected components are vulnerable to attacks."
"the purpose of this activity is to identify a secure network communication protocol. wireless sensor network devices need to be properly authenticated in the network domain. it is important to deploy standard security protocols like transport layer security (tls), datagram transport layer security (dtls), internet key exchange/internet protocol security (ike/ipsec) and host identity protocol-diet exchange (hip-dex). this performs the communication protocol secure for wireless sensor networks."
the first decision that needs to be made by a federation is the way resources will be allocated to different users when demand exceeds the available capacity.
"second, the problems defined above are not analytically tractable in the general case. however, it is easy to compute the shapley value for specific realistic scenarios. this is especially so in our case since planetlab federation has a hierarchical nature and the number of providers at each level of the hierarchy is not expected to be large. so, even though our numerical examples are simple, they are meaningful and the insights they provide could be used for the design of practical policies, like the ones required for the top level federation between plc, ple, and plj. for example,φ i s can be computed off-line and used as heuristic evaluators of the individual contributions of facilities, given the mixture of expected users."
"software security engineering proposes many tools, techniques, methods, and best practices to develop a secure system [cit] . there is a lack of understanding of software security that should be clarified and managed in the early phase of the software development life cycle (sdlc) [cit] . therefore, researchers have not been successful in developing a secure software system when applying software engineering best practices [cit] ."
"looking at these threats and their consequences, it becomes clear that security for cyber-physical systems has a central role to play in the development of these systems. if this aspect is not taken into account, attackers are free to access data and abuse systems as they wish. in fact, dangers would arise which are hardly imaginable. as a result, the value offered by cyber-physical systems would be almost completely lost."
"there are many interesting directions in which to extend the analysis and gain valuable insights. for instance, we can develop more sophisticated demand models, use a loss networks formulation, and compute the shapley value in a manner similar to paschalidis and liu [cit] . such an approach can also lead to a more careful analysis of the tight coupling between demand and the value of a resource, as opposed to a simple proportional valuation based on resource contribution or consumption. integrating complex topologies, hierarchical federations, the formal comparison with marketbased approaches for sharing value inside a federation, and the effect of competition in the form of cloud facilities like amazon ec2 are also avenues for further research."
"while planetlab is our main target, our proposal can be extended to other federation scenarios for network connectivity, content distribution networks, and cloud computing. in fact, our model serves as a generalization of these scenarios that can help in comparing the properties of the coalitional game theory approach in different cases and understanding in more depth the factors that should influence the design of federation policies at large."
"assets for cpss are grouped into 3 categories: software layer, network layer and physical layer. after analysis of the car parking system entities, we have listed 8 assets for the smart car parking system."
"as already stressed, the utility acquired by a user of an experimental facility depends not only on the total amount of resources acquired (as often assumed in the literature on grids [cit] ) but also on the number of different locations and/or technologies providing these resources. this means that the value that a researcher/customer acquires through an experiment k will depend on the number of geographically distributed nodes that will have enough capacity to host this experiment upon its arrival. so, an experiment can be characterized by three different demand attributes:"
"confidentiality is also a subject of major importance within sensor networks. networks could be used, in the worst case, to spy on individuals [cit] . an example would be the long-term monitoring of persons or vehicles on a routine basis. another aspect that affects the security of sensor networks are attacks on these networks communication between physical environment and gateway to controller/server, as shown in figure 2 . in the simplest form, the attacker sends a high-energy signal to the sensor to prevent communication within the physical layer to the network layer. this can lead to serious consequences, especially in the case of security-critical cps. military applications are also threatened by such attacks. a possibility to combat these attacks lies in the nature of the networks themselves. if a part of the network has been compromised, this part can be demarcated and the communication can be routed around it [cit] . therefore, it is of utmost importance to address the security of all components (i.e., physical layer, network layer and application layer) of cpss."
an unauthorized access of data is a real threat that should be handled at the beginning of sdlc. there are many possibilities that an attacker can easily access user information. this information can be injected through network communication or sensor nodes [cit] .
"for upp-sne, we set the number of random walks per node γ as 40, the walk length l as 40, the window size t as 10, and the number of iterations as 40. all baselines use default parameters as reported. the dimension of learned node representations m is set as 128 for all the algorithms."
"we also conduct experiments to investigate the convergence property of solving the optimization problem in eq. (8). we vary the number of iterations from 1 to 60 and plot the corresponding value of the objective function on google+, hamilton, and rochester, as shown in figure 1 . we can see that, our algorithm can achieve fast convergence within only 10 iterations on the three networks."
"security is considered to be the core aspect for the strength of the software. several types of research have proposed different methodologies, models, and frameworks to support security requirements elicitation and maintenance. the most famous security requirements frameworks are square, srep coras, ms sdl, umlsec, and secure tropos. therefore, in this paper, we are providing a review of only major security requirements frameworks that have significant contributions in the literature. a detailed literature review has been conducted to summarize the evidence, and the main contributions of each paper have been examined. system quality requirements engineering (square) is a security requirements engineering framework and comprehensive methodology to determine security requirements for software [cit] . the framework provides a means to elicit, categorize, and prioritize security requirements for a system. the objective of the framework is to build security concepts in the early stages of the software development lifecycle, and integrate effective security requirements engineering into system development processes. the framework consists of nine activities, and has been implemented in the development of software to elicit security requirements [cit] ."
"during the iteration, the most time-consuming steps are line 6 and line 9, i.e., the calculation of ∂o ∂µs and ∂o ∂νs for each s. the time complexity of line 6 and line 9 is o(nnz(n) · m ·d · k) and o(nnz(n) · m · k), respectively, where nnz(n) is the number of non-zero entries of n(v i, v j ) andd is the averaged number of non-zero entries of node profile feature x i . as the sparsity of matrix n(v i, v j ) and profile feature x i is utilized, the calculation is efficient."
"so, in this case resource allocation decisions are more complex since the objective is not only to maximize the total value generated in the system but at the same time to share it in an incentive-compatible way, depending on the contributions of the different facilities."
"assets for cpss are grouped into 3 categories: software layer, network layer and physical layer. after analysis of the car parking system entities, we have listed 8 assets for the smart car parking system."
"in this work, first we compared the most commonly-used security requirements engineering frameworks. the best papers were selected after a thorough search in the literature. the criteria of comparison were based on particular parameters, and these parameters were decided on their significance of security requirements engineering and characteristics on cpss. table 4 shows the comparison of different security requirements engineering frameworks. this comparison helps us to determine the strengths and weaknesses of each framework. our findings from this comparison survey indicate that none of the frameworks performs all the required activities for a secure software system. this may result in the development of unsecured cyber-physical systems. furthermore, this comparison helps us to identify the shortcomings in sre frameworks which have been rectified in our proposed security requirements engineering framework for cps."
"for deepwalk, the representation of node v i, φ(v i ), is learned from scratch, which is independent of node content features. however, as confirmed by existing works (e.g., [cit], node content can provide crucial information to enhance network representation learning. thus, we propose to construct node representations via a non-linear mapping from node profile features ϕ(x i ):"
"to this end, it is important to note that in systems with low demand, evaluating contribution based on consumption, instead of resource availability, might be suboptimal in terms of incentives for resource provision. 6 see our myslice project at https://trac.myslice.info/."
"we are living in the era of digitization where software, system hardware, and sensors are working together over networks. this combination describes the concept of cyber-physical systems (cps) [cit] . in this situation, the urge to maintain security is of prime importance. secure system development depends on an extensive focus on the process of requirements engineering towards security. software engineering gets its developmental supports from tools and techniques, and models that guide them to manage quality development support [cit] . these techniques provide information on how services are provided. however, when developing a secure system, one has to consider the vulnerabilities of the system as well."
"looking at these threats and their consequences, it becomes clear that security for cyber-physical systems has a central role to play in the development of these systems. if this aspect is not taken into account, attackers are free to access data and abuse systems as they wish. in fact, dangers would arise which are hardly imaginable. as a result, the value offered by cyber-physical systems would be almost completely lost."
"an individual facility's contribution to a federation has many characteristics, among them: capacity (cpu, physical memory, hard disk, bandwidth), geographical or network position of nodes (location or as number), scale (the number of nodes), technology, availability (the portion of time that it can be used), reliability (how long it remains available without interruption), and average load (congestion level). their effect on the total value of a federation depends on the types of experiments and services that are to be run on the federated infrastructure and the corresponding utility and cost functions. if we wish to have a tractable model, we must simplify by sacrificing some details as described below."
"the existence of the core ensures the feasibility for forming the grand coalition but not all different solutions in the core are equally desirable since they could be based on different notions of fairness (e.g., equity, max-min, proportional) and different ways to evaluate the individual contribution of a member of the coalition. in our case, facilities that offer resources in locations with low overlapping will contribute more to the total value generated. how can one quantify the relative contribution of different facilities in a testbed federation?"
"the acceptance of cyber-physical systems in society depends on trust from users that must be earned. this trust can only be gained by providing adequate security goals to users. security goals aim to protect the system from threats and vulnerabilities and reduce risk factors. we aim to extend our understanding of security goals for cpss. for instance, in the case of sensor data oriented systems with multiple sensor nodes generating data, security issues are crucial to see if the data generated is coming from a trustworthy source. this shows that authentication, availability, integrity, and confidentiality are very important security goals in cpss. the following are important security goals: 3.1.1. authentication nodes (sensors) should be identified and authenticated before adding them to the network [cit] . authentication in a cps is considered difficult to achieve as, in some cases, it requires heterogeneous network authentication. the lack of an authentication process can lead to exposure of the network/information to an unauthorized user."
integrity denotes that the information is accurate and trustworthy to the users. integrity is disrupted when an information is modified in an unauthorized manner. this ensures that the data cannot be modified in any manner [cit] .
"assets could be anything that has value for the organization i.e., people, money, software, hardware, sensors etc. therefore, the purpose of this activity is to determine all the assets of the cps components. this activity also involves the environmental and organization asset evaluation. these assets usually involve human resources, data resources, network resources, and sensors and physical components."
"under this model, some locations can host resources from multiple facilities. we can capture this by introducing the probability of overlap o ij between the sets l i and l j . for simplicity, we could assume that these probabilities are independent but more realistic overlapping models can be easily developed if needed."
"1. confidentiality: confidentiality is the major element of security goals. the user and vehicle data shall not disclose personal information to an unauthorized person. 2. integrity: data is strongly related to the accuracy of the system. therefore, its integrity is important. any unwanted update can cause extreme issues, such as two users claiming one parking space, which can cause an issue over vehicle and user ownership, or even accidents by providing improper free lot information. 3. authorization: the system shall require strong authorization while accessing data, as it manages several user's information simultaneously. third-party sensor authorization is also needed because it affects the system's accuracy and privacy. therefore, it is important to ensure the device's good operation due to system criticality. 4. role-based access control: different authorized users have various roles in the system. these roles define the access limit to a diverse data set, hardware, or screens present in the system. the system maintains the access control list to define the assigned task of reachability in the resources. this can be facilitated by login id, profile detail, port, network channel, and important addresses. 5. robustness: the system should have a backup or alternate servers, sensors, and electricity sources to provide a timely response if any failure occurs. 6. availability of data: this means that requested data shall be available when it is needed. therefore, it is important that the system should be responsive all the time or at a specified time. 7. contractual integrity: third-party hardware, software, and network providers should properly follow a written contractual definition."
"planetlab is unlike grid systems, which have been the focus of existing literature in this area [cit] . whereas the value from federating grids comes principally from shared computational resources, planetlab federation provides value in the form of the multiple locations at which shared resources reside. our federation model extends recent work on coalitional game theory for communication networks [cit] by capturing this diversity dimension."
"data conversion is not working: the digital to analogue conversion after vehicle \"id\" is processed could face issues. this could happen because its hardware system or software is malfunctioning. 6."
"the purpose of this activity is to identify the threats for cyber-physical systems. threats are then categorized with the help of a misuse case. we have classified threats into 3 categories; software layer, network layer, and physical layer."
"until recently, all nodes were controlled by the planetlab operations center at princeton university, which acted as the sole manager of the system's complex trust and security issues. today, the european part of planetlab, planetlab europe (ple), is part of the onelab experimental facility [cit] and planetlab central (plc) and ple are federated through the direct exchange of user credentials and node descriptions."
"on the demand side, the economic models most relevant to our work are bellagio [cit] and gridecon [cit] . bellagio is a combinatorial auction system designed for allocating planetlab resources on the basis of a virtual currency. while combinatorial auctions provide a general approach to meeting diverse requirements, they lead to complex solutions and they do not provide the means to share profits among a set of independent providers whose resources are part of a certain bid."
"in this paper, we proposed a user profile preserving social network embedding method. we argued that, although a handful of methods exist to leverage node content for network representation learning, node content and user profile are largely different; while the former is topic-centric, the latter is noisy, inconsistent, highly incomplete, and uninformative. as a result, existing node content augmented network representation learning methods are ineffective to leverage node profile information. accordingly, we proposed to use a nonlinear mapping to augment network structure and node profile information, in order to jointly learn an informative representation. experiments on node classification and clustering tasks demonstrate the superior performance of the proposed method in learning effective social network representations."
"the shapley value, originally proposed by lloyd shapley [cit], provides an elegant means to evaluate the individual contribution of the members of a coalition. it expresses the 'importance' or 'uniqueness' of each member of a coalition with regard to the value of its output. applied to the case of isp interconnection, for example, the share of each provider depends on the number of similar providers, which could be those providing the same content or those covering the same region [cit] . the less overlapping, the more valuable one's contribution."
"availability of data: this means that requested data shall be available when it is needed. therefore, it is important that the system should be responsive all the time or at a specified time."
"security requirements are a significant part of cyber-physical systems, but there are a lack of processes to develop secure systems. many security requirements methodologies have been proposed, but these are limited only to software, and none supports cyber-physical systems. in this paper, our main contribution is to provide a comprehensive security requirements engineering framework for cyber-physical systems that can offer complete guidelines for practitioners and researchers to determine security requirements. the novelty of such an implementation at this scale has not been significantly reported in the literature. the proposed cps framework identifies security requirements throughout the requirement engineering phase. this not only helps to build a secure cps, but also to avoid potential consequences in cyber-attacks. furthermore, our proposed security requirements engineering framework is compared with other existing software security frameworks. the encouraging result shows that none of the software security frameworks implements all the essential activities for the development of secure cps. to evaluate the cps framework, we have applied our approach to a smart car parking system, from which we obtained promising results. we have identified 40 security requirements with the help of proposed cps framework; these major security requirements have contributed to developing a secure smart car parking system. the findings of this research will be of great benefit to practitioners and researchers."
"to overcome the above-mentioned difficulties, we propose a new algorithm called user profile preserving social network embedding (upp-sne), which incorporates user profile information with network structure to jointly learn a vector representation of social networks. the theme of the upp-sne algorithm is to learn a joint embedding representation by performing a non-linear mapping on user profiles guided by network structure. in this feature reconstruction process, network structure helps filter out noisy information from user profiles and embed them into a consistent subspace, into which topology structure is seamlessly encoded to jointly learn an informative embedding representation. the interplay between user profile information and network structure enables them to complement with each other towards learning a joint network representation that preserves both network proximity and user profile affinity. the effectiveness of the upp-sne algorithm is validated on four real-world social networks for the tasks of node classification and clustering. experimental results show that, upp-sne's representation yields superior performance to the state-of-the-art baselines."
a way to generalize and formalize our initial analysis toward characterizing the equilibrium of the global federation game is a very interesting research avenue with significant potential impact on the theory and practice of coalitional game theory for communication networks. we believe that our model and first observations constitute a good starting point.
"the performance gains of the upp-sne algorithm over the structure preserving nrl baselines indicate that, with user profile information being properly exploited, better social network representations can be learned. this confirms the usefulness of user profiles in learning social network representations and the capacity of upp-sne in effectively capturing useful information from user profiles to complement network structure for learning better user representations."
"in this paper, we have extended the framework proposed in our previous paper [cit] . here, we are providing details of the implementation of our proposed security requirements engineering framework for cpss, its activity, and supporting techniques. the proposed framework aims to serve as a complete guide for a number of activities to analyze and identify threats and to determine security requirements of cpss by taking different aspects of cps into account. the novelty of such implementation at this scale has not been significantly reported in literature. the findings of this research will be of great benefit to practitioners and researchers who play an important role in the development of security requirements engineering (sre) for cpss. the increased demand for security in organizations justifies the need of a security requirements engineering framework. therefore, organizations that apply the proposed framework derived from the research results can train their requirement analysts and software developers, and this research will help them to explore security requirements in the early phases of software development."
the central server is not responding: central server does not respond to user or app server because of a malicious action. this could lead to data loss.
"using the same configuration, but assuming that demand exceeds capacity, fig. 9 depicts how the individual profit of facility 1 changes as it increases the number of locations that it covers. this figures shows, as expected, that facilities will have different incentives to upgrade their contribution to the federation depending on the type of demand and on the profit sharing mechanism employed."
"for the case of planetlab, we have extensively analyzed user behavior based on available measurement data [cit] . this analysis suffered from the opacity of users' utility functions. we have now initiated changes to the planetlab interface 6 to allow users to explicitly select resources on the basis of their properties (geographic location, reliability, etc.) this will allow us to construct more realistic utility functions in the future."
"the huge growth of online social networks, e.g., facebook, twitter, google talk, wechat, etc., has revolutionized a new way for people to connect, express themselves, and share content with others in today's cyber society. users in online social networks are connected with each other to form a social graph (e.g., the friendship graph). one of the most critical problems in social network analysis is the automatic classification of users into meaningful groups based on their social graphs, which has many useful practical applications such as user search, targeted advertising and recommendation systems. therefore, it is essential to accurately learn useful information from social networks. one promising strategy is to learn a vector representation of a social network: each network node is represented as a low-dimensional vector such that the information conveyed by the original social network can be effectively captured. as a result, existing machine learning methods can be directly applied in the lowdimensional vector space to perform network analytic tasks such as node classification, network clustering, etc."
in this paper we don't fully address this challenging question but define one of its most important elements: the estimation of the relational importance of the individual contributions of the federation members. we present numerical results that help us understand the dynamics of the game and the properties of the different solution concepts.
"this paper formulates a simple but expressive economic model that captures an interesting characteristic of the evolving federation ecosystem: the value of diversity. based on this model, we compute the shapley value [cit] of the individual members of a federation facing a certain type and volume of demand. our study leads us to the following conclusions:"
"based on this utility function the contribution l i affects the value per experiment generated by a federated infrastructure while the contribution r i affects the number of different experiments that can be hosted at each location. so, using a rather simple model we manage to capture both dimensions of federation: capacity and diversity."
"the comprehensive, lightweight application security process (clasp) is a framework that connects roles, resources, and their associations [cit] . on the basis of relational information, the authors have provided a list of security services. the authors suggest that for each service, there is a need to establish specific measurable acceptable realistic and time-bound (smart) requirements. to illustrate their approach, they have performed a walkthrough on a service-based application for contact storage. however, it lacks the results comparison of cross-project analyses. on the other hand, the paper [cit] provided a comparison of microsoft secured development life cycle (ms sdl) and clasp. they have analyzed the activities of both processes, specifically from a security perspective. they have identified that both models have an advantage which is specific for different levels. even among the two models, neither provided evidence to support communication and physical aspect of security issues."
"there are several different ways to attack cyber-physical systems. the first would be eavesdropping, which is used to intercept the exchanged data [cit] . in a military context, this is of particularly high relevance as tactical information can be intercepted by the enemy. also in the case of industrial espionage, this can lead to serious damage to the organization."
"faced with an agreed profit sharing or resource allocation policy, facilities decide the quantity of resources they will bring to the federation based on the trade-off between the extra value/profit generated and their internal costs. fig. 3 illustrates the different stages of the federation game and their interdependencies. the fact that more sophisticated schemes like the shapley value do not have a closed form makes it very challenging to analytically study the evolution and possible equilibria of this game in the general case."
"the proposed cps framework has been applied on a smart car parking system. therefore, a functional prototype was developed for the demonstration of a smart car parking system, consisting of a physical and a software implementation. figure 5 shows the functional prototype; each parking lot is connected to a raspberry pi 3, which collects the evaluated sensor data from the arduino and sends it to artik cloud. the system maintains the information of the vehicle entering the parking area. the system uses the sensor to determine the identity of the vehicle. the user of the system gets support through a mobile application. only registered users are able to use the system. in the following section, we have applied 8 activities of the proposed framework and identified its effectiveness."
"each asset is then assessed relative to functional requirements, and its negative uses are identified. we have identified 20 major threats. however, because of brevity, we are providing 9 of them, along with their definitions."
"we first verify the effectiveness of the upp-sne algorithm for multi-class node classification. to make fair comparisons, we vary the training ratio from 1% to 10% by an increment of 1%. for each training ratio, we randomly split the training set and test set for 10 times and report the averaged accuracy. tables 2-4 report the classification accuracy of all the algorithms on google+, hamilton and rochester, where the best results are bold-faced. we can see that, in most cases, upp-sne outperforms both network structure preserving baselines and content augmented baselines. on google+, upp-sne's representations achieve the best accuracy on all training ratios except for 8%, 9% and 10%. on hamilton, upp-sne performs best on 9 out of 10 training ratios. on rochester, upp-sne beats all baselines on all training ratios."
"security requirements engineering process (srep) is an asset and risk-driven framework [cit] . authors of this framework combined common criteria (cc) with software development life cycle and the requirement repository. they have proposed 9 activities which can be processed in an iterative or an incremental manner. they have suggested that the framework is enhanced with the support of a repository for reusable security requirements. these requirements provided information regarding threats and assets. in another paper [cit], the authors have applied the proposed framework on information system (is) and suggested computer-based tool development for the support of frameworks."
integrity denotes that the information is accurate and trustworthy to the users. integrity is disrupted when an information is modified in an unauthorized manner. this ensures that the data cannot be modified in any manner [cit] .
"man-in-the-middle attacks are sending incorrect information. if it is not recognized as false, it influences the function of the system [cit] . in the case of a system which controls the operation of train switches, such attack can lead to malfunctions or collisions of trains."
"regarding the clustering of machine-generated messages, existing methods that are based on keyword extraction have been proven to be quite effective. they tend to split messages into keywords and other (payload) fields using n-grams [cit] and/or delimiters [cit] . a standard clustering technique is then performed by comparing keywords to find similar messages. in the following, we examine some representative works in keyword-based message clustering."
"step 3, we further refine the keywords by taking into account their positional variations; finally in step 4, the messages are clustered based on the identified features-positional keywords. we present these steps in turn in the following subsections."
using (3) we can obtain an upper bound on the normalized system throughput φ ub when a large field size is used. the expression for φ ub is given by eqn. 4 at the top of next page.
"the css aims to improve detection sensitivity, especially when working under low signal to noise ratio (such as the snr level proposed by 802.22 working group, which is -22 db [cit] ). in the following subsections, we will study three different combining rules for css: hard combining rule (or and and rule), soft combining rule (equal gain combining rule) and quantized combining rule (two-bit quantized combining rule). for each combining rule, we will express the cr network probability of false alarm q f in terms of the required overall probability of detection d q and n under cpup scenario. we will also formulate the cr network probability of detection q d in terms of the required overall probability of false alarm f q and n under csusu scenario."
"where e k denotes the statistic from the k th cr user. it was proved that egc e has a chi-square distribution with n*k degree of freedom. according to the central limit theorem, egc e can be approximated as a gaussian distribution if the product nk is large enough. in this case, the overall detection probability and false alarm probability for cr users' network can be written as follows"
"in this step, we use the positional keywords in set k as features for message clustering. hence, we can map messages into a feature vector space and apply a clustering algorithm, such as k-mediods, on the vectors to cluster messages."
"similarly, under csusu and for a fixed false alarm probability f q and optimized values of β f1, β f2 and l, we can use equation (22) to search ρ numerically. then we find p f1, p f2 and p f3 based on ρ, β f1, β f2 given in (22). after that we compute the detection probability p di in each region based on the following expression:"
"in this step (see algorithm 1), we introduce a new concept and technique, positional n-gram. a traditional n-gram is a subsequence of n elements contained in a given sequence of at least n elements. hence, n denotes the number of consecutive elements that are joined together. for example, the first message ''id:1,op:b'' in figure 2 to identify frequent positional n-grams, we use the following definition to count the number of messages that contain a given positional n-gram."
"in this paper, we have proposed a novel approach, p-gram, to effectively cluster machine-generated messages. a new concept and technique, positional n-gram, is developed to identify message keywords. it addresses the keyword mis-identification problems suffered by existing message clustering methods. in particular, p-gram considers the positions at which message keywords appear, so as to distinguish the multiple occurrences of the same keywords in a message, filter out noise from keywords, and separate short keywords from those that contain the short keywords as sub-strings. the position-based density analysis of positional keywords further delineate keywords from the same words' occurrences in payload fields. furthermore, we have presented theorems that prove the advantages of our proposed approach. we have demonstrated the benefits of the proposed approach by applying it to a range of machine-generated message datasets collected from real-world systems, including both textual and binary messages. the experimental results have shown the superior performance of our approach over existing state-ofthe-art methods."
"several ss techniques are proposed and the most popular sensing techniques are: energy detection (ed), cyclostationary feature detection, and matched filter detection [cit] . ed is a well known detection method mainly because of its simplicity and it doesn't require any prior knowledge of the pu signal compared to other techniques."
"p-gram effectively addresses the aforementioned keyword mis-identification issues faced by existing methods. first, p-gram can identify the repetitive occurrences of keywords in messages by analyzing the probability density of keywords at different positions (or position windows). second, it can effectively filter out from the candidate keywords noise that is part of message payload. third, by using the proposed independent frequency, p-gram can effectively distinguish short keywords from those longer ones that contain the short keywords as sub-strings, whereas existing methods cannot because they solely rely on the generic frequency of strings."
"note that, due to the variable length of payloads, the exact position of message keywords may vary in a certain ''window'' (range). therefore, we should consider all the occurrences of a keyword at different positions of a window as one positional keyword that has the aggregate frequency at these different positions. for keywords with multiple occurrences in a message, their positions may vary in multiple windows."
"where v t is the volume of all of t's possible positions in x, and f x (t) is the total independent frequency of t's positional n-grams in the window"
"our technique, p-gram, takes advantages of the template structure of machine-generated messages. from the above analysis, we see that existing keyword extraction based methods for message clustering faced with the keyword mis-identification issues, in particular with keyword repetition, noise (false keywords) from payloads, and the omission of short keywords covered by other longer keywords. compared to these methods, p-gram successfully addresses these issues by considering the position of keywords in messages. experiments on various textual and binary messages demonstrate the effectiveness of the proposed p-gram for clustering machine-generated messages."
"in this rule, each user decides on the presence or absence of the primary user and sends a one bit decision to the data fusion center. the main advantage of this method is the easiness and the fact that it needs limited bandwidth for the reporting channel. in this subsection, considering both or and and combining rules, we analyze the performance of combining rules for css under cpup and csusu scenarios. the and rule decides that a signal is present if all sus have detected a signal. the cr network probability of false alarm q f, and the cr network probability of detection q d using the and rule can be formulated as follows [cit] :"
"we use two standard evaluation metrics, precision and recall to quantitatively evaluate and compare the effectiveness of p-gram for message clustering. the following formulas give the definitions of precision and recall:"
"our goal is to design a method that, given a set of machine-generated messages, separates messages into type-specific clusters as determined by their keywords. previous clustering methods often suffer from keyword mis-identification issues. taking the messages in figure 2 as an example, ''ou'' can be identified as a keyword in existing methods, as it has a high frequency of occurrence. however, it will only be considered as having appeared in a message, ignoring its multiple occurrences in a message. another example would be ''cn'', which can be identified as a keyword by existing methods. however, the short keyword ''c'' will be ignored, because ''c'' is a sub-string of ''cn'' and it will be merged into ''cn'' by existing methods. in addition, the ''cn'' from payload ''haycne'' is often wrongly treated as a keyword in the message by existing methods. in this paper, we seek to address these issues with a new keyword identification method and achieve better clustering accuracy for machine-generated messages."
"in this section, we have performed matlab simulations to evaluate the optimization problem (30). it should be noted that all selected simulation parameters are based on the ieee 802.22 wran. the frame duration (t) is set to 100 ms and the bandwidth channel of the pu is fixed to be 6mhz. the signal to noise ratio snr is put to -18 db for all k cr users."
"in summary, p-gram achieves better clustering performance by addressing the keyword mis-identification problems faced by existing methods. therefore, similar to considering semantic information in natural language processing (nlp), for machine language processing (mlp), we need to consider the structural features of machine-generated messages."
"given a slot with a collision of size k, the receiver tries to decode k independent linear combinations in gf (2) of the colliding signals. one possible way to proceed, although not necessarily the optimal one, can be the following. for each slot with a collision of size k, the decoder tries to decode single messages. if less than k messages are decoded correctly, the decoder tries to decode the sum in gf (2) of pairs of messages (there are"
"each message shows a number of fields. manually, we can identify the keywords, such as ''id:'', '', op:b'', volume 7, 2019 figure 2. an example list of 6 messages following the fictional lightweight directory access protocol (ldap). there are two types of messages covered in this example: op:b and op:s. the op:s messages present 5 keywords: ''id'', ''cn'', ''ou'', ''ou'' and ''c''. note that, (i) ''ou'' is a repetitive keyword with 2 occurrences; (ii) the keyword ''c'' is a sub-string of the keyword ''cn''; (iii) the payload haycne in message 4 has the same sub-string of the keyword ''cn''."
"while these results verify the functioning of the joint decoding approach, for a practical random access scheme the assumption of fast fading is not realistic and we therefore apply block fading for the following throughput evaluations."
"note that matrix a is rank deficient if coefficients are chosen in gf (2) (i.e., all coefficients shown in the matrix above are equal to 1), while it can be full rank in some extended galois field. for instance, in gf (4) the matrix is full rank. if, by choosing the coefficients at random, the above matrix is obtained it would be possible to decode the whole frame. the probability of obtaining a full rank matrix increases with the field size. finally, we note that in the example the average number of packets decoded per slot is 2."
"the independent t i,n itself is also a potential keyword. therefore, s returns a set of potential keywords, which can be either longest positional n-grams or short frequent independent positional n-grams."
"in this paper, we focus on the clustering of machinegenerated messages. many methods from bioinformatics and social media information extraction can be applied to machine-generated messages. however, due to the specific nature of machine-generated messages (using message formats/templates), directly applying those methods cannot achieve satisfactory results [cit] . hence, in the following analysis we only focus on the methods for clustering machine-generated messages."
"in the present paper we propose a random multiple access scheme for symbol-synchronous slotted aloha systems named seek and decode (s&d) in which the transmitters pre-encode their information messages multiplying them by a random coefficient in an extended galois field while the receiver tries to decode any linear combination in gf (2) from the set of colliding bursts within each slot. the receiver employs a hybrid between a joint multiuser decoder and a plnc decoder. once the whole frame has been processed at the physical layer, the receiver uses the whole set of linear combinations available to retrieve all messages transmitted within the frame by using matrix manipulation techniques over the same extended galois field used in the pre-coding stage."
"in this section, we introduce our clustering method, p-gram, for machine-generated messages. it considers the positions of various words in messages in determining whether or not they are keywords, achieving greater accuracy in identifying keywords and message clusters. as shown in figure 3, p-gram has four major steps: in step 1, we identify frequent position-specific words (or positional n-grams) in the messages; in step 2, we extract the position-specific keywords (or positional keywords) from the message words of step 1; in"
"in our experiments, p-gram involves two parameters: the threshold ρ and the minimum length n of positional n-grams. here, we analyzes the sensitivity of p-gram to these two parameters. we have chosen two datasets, sosp logs and text ldap protocol messages, for sensitivity analysis, as they show uneven distribution of messages over their types (see table 1 ) and hence have a greater impact on ρ and n."
"autoreengine utilizes the variation of keywords' positions to filter out noises from potential keywords. overall, it shows a better performance than the previous two methods in terms of precision (see figure 5 ). however, due to its message clustering strategy (i.e., messages that have the similar keyword sequence are clustered in one group) without using the prior knowledge of the number of clusters, it often generates more clusters than the ground truth clusters. hence, as we can see in figure 6 that, autoreengine shows lower recall values than prodecoder on some datasets, including ldap, twitter rest, soap, asgard, and sosp. meanwhile, as we observed in figures 5 and 6 that, vanilla n-gram shows the worst performance in terms of precision on most datasets. and it also shows poor performance in recall on many datasets. however, on some databases, such as binary ldap and twitter rest, autoreengine shows even worse recall than vanilla n-gram."
"windows, where x max and x min are the maximum and minimum positions in x . for each window, say [x, x + δ t ), we can calculate the probability density of t in the window as follows,"
"the or rule decides that a signal is present if any of the cr users detect a signal. hence, the cr users' network probabilities using the or rule can be expressed as follows:"
"there are many studies that focus on the clustering of texts/messages. for example, in the area of bioinformatics, many advanced methods [cit] have been proposed for the clustering of unstructured biomedical texts so as to construct the domain knowledge graph, assemble dna sequences, etc. in the area of social media information extraction, many effective methods [cit] have been developed for the clustering of (generally short) user posts so as to discover trending events, identify rumors/false information, etc."
"the proposed p-gram consists of four main steps. given a set of machine-generated messages, in step 1, we introduce our new concept, positional n-gram, and identify frequent positional n-grams with different n lengths in messages. in step 2, we extract the longest common positional n-grams and the shorter but significant positional n-grams as position-specific keywords (or positional keywords) from the frequent positional n-grams of the previous step. to reduce the impact of variable-length payloads on keyword position, in step 3, we analyze the variation of the positions of each keyword, calculate the position window(s) for each keyword, and merge the positional keywords within their windows. in"
"where e j is the shannon entropy for the characters at the j-th position, q ji is the ratio of the i-th character in the character set of position j, and r is the total number of characters in the character set. the less diversity of characters at a position, the lower entropy that position has. then, we calculate the variability of a feature by adding up the entropy of the positions covered by the feature. hence, for a feature t i,n, its entropy is:"
cognitive radio (cr) has been proposed to solve the conflicts between spectrum scarcity and spectrum underutilization [cit] . cr users are allowed to utilize the spectrum resources when the pus (primary users) do not use the spectrum without causing harmful interference to pus services. one of the issues in cr system is how the cr users detect the pus whether they are present or absent. this task is known as spectrum sensing (ss) [cit] .
"the objective is to determine the optimal sensing time (t) such that the cr users' network throughput is maximized. in the case of css, this objective can be formulated as follows:"
"in the present section we derive an upper bound to the system throughput, defined as the average number of decoded messages per time slot, for the proposed scheme. the throughput depends on the repetition strategy chosen. for mathematical tractability we assume a general scheme in which each active node transmits in each slot with probability p, fixed for all nodes."
"as some strings from payloads may be extracted as features in set k (due to their high occurrence), we wish to assign a greater weight to the structural features (such as operation type) of the message and a lower weight to noise (extracted from payload). to do so, we make use of the observation that structure features are more stable than payload data. we use entropy as a measurement of variability, and use it as the basis to calculate a weighting for each byte position. the shannon index entropy is adopted to measure the variability [cit],"
"the performance of the modified needleman-wunsch (mnw) is close to p-gram. however, p-gram is better overall. in particular, for the ims messages, p-gram achieves 100% precision, but mnw has only 92% precision (see figure 5 ). for the rest messages, p-gram achieves around 93% precision and recall, but mnw has only around 65%. this is because mnw uses the needleman-wunsch distance to measure the similarity between messages. it could involve substantial noise from payloads when extracting message keywords. in contrast, p-gram effectively filters out noise by using the position information. therefore, p-gram achieves better performance than mnw."
"based on the work in this paper, we plan to investigate general techniques to automatically extract accurate message formats from message traces. then, we will further discover the control and data dependencies that exist in message traces. these control and data models will provide critical support for system security inspection and application behavior analysis."
the use of an extended galois field in the pre-coding stage increases system diversity. we derive an upper bound on the throughput at the system level and present numerical results for the number of innovative messages decoded within a slot in a block fading channel. fer curves for the fast fading channel are also presented.
"in this article, the performance of css has been investigated under two operational scenarios, namely, cpup and csusu using different combining rules (or, and, egc and the quantized two-bit). it is shown via simulations that the egc soft combining rule outperforms the hard and the two-bit quantized combining rules and the quantized two-bit combining rule exhibits better performance detection than the hard combining rule. further, the relationship between cr users' throughput and sensing time has been studied for both scenarios and under different combining rules. the simulation results showed that under cpup, there is an optimal sensing time for which the cr users' network throughput is maximized. the optimal sensing time and the corresponding maximized value of the cr users' throughput depend on the combining rule used. the highest value of the throughput can be obtained by the egc soft combining rule. the two-bit quantized combining rule which has been derived in this paper could be an appropriate combining rule to realize a tradeoff between performances (in terms of detection and throughput) and overhead (in terms of complexity and reporting channel bandwidth)."
"machine-generated messages follow a template structure and are constructed according to message templates/formats. given a set of messages, there may be of different types, where each message type follows a particular format. a message consists of fixed fields (keywords, that are repeated in messages of the same type) and variable fields (containing payload data), as shown in figure 1 . in the context of this paper, the concept of a keyword is similar to but broader than its usual meaning in machine languages. that is, a keyword is a maximum consecutive sequence of bytes or characters that is reserved by and has a special meaning in a machine language, which is either compulsory (occurs in all messages of a given type) or optional (occurs in only some messages of a given type). in general, the type of a message is determined by the keywords it contains. figure 2 gives an example list of messages from interacting with a system. it follows a fictional directory service protocol similar to the widely used lightweight directory access protocol (ldap), but is simplified to make our running example easier to follow."
"due to many factors such as multipath fading, shadowing and the noise uncertainty problem [cit], the detection performance in ss can be degraded. to combat these impacts, cooperative spectrum sensing (css) schemes have been proposed [cit] ."
"vanilla n-gram is the baseline of our work. it has been widely used in statistical nlp. in contrast to positional n-gram, the vanilla n-gram ignores the positions of n-grams. all the other strategies of vanilla n-gram are the same as p-gram."
"to evaluate the effectiveness of the proposed approach, we compare p-gram with existing state-of-the-art message clustering methods, including prodecoder [cit], auoreengine [cit], modified needleman-wunsch [cit], and a baseline algorithm (an nlp-style ''vanilla'' n-gram approach). eight message data sets generated from real-world systems and protocols are used for our evaluation, including both textual and binary messages. the experimental results show that p-gram achieves more accurate clustering than any of the other methods."
"where s(n) are the samples of the transmitted signal from the pu, which are assumed to be a random process with variance ı s 2, ) (n w k is the receiver noise for the k th cr user, which is assumed to be an i.i.d. random process with zero mean and variance ı w 2 and h k is the complex gain of the channel between the pu and the k th cr user. h 0 and h 1 represent whether the signal is absent or present respectively."
"autoreengine [cit] adapts the apriori algorithm [cit] to identify keywords. then, the variation of keywords' positions are calculated. those with variations lower than a given threshold are kept as keywords, and others are filtered out as noise. finally, keywords are sorted into vectors, and messages are clustered by the groups of keyword vectors. we set the threshold as 0.07 for keyword identification and keyword group extraction. the same threshold is used for the proposed p-gram in the keyword identification step."
"we see from figure 5 that, compared to other methods, vanilla n-gram shows the worst performance in precision on all datasets except on twitter rest. this is due to the mis-identified keywords by vanilla n-gram, which only considers the frequency of n-grams but fails to utilize the structural feature of machine-generated messages. this easily introduces noise into keywords. hence, directly applying nlp approaches such as vanilla n-gram on machine-generated messages fails to achieve good performance for mlp. as the twitter rest datasets involves many natural language texts, vanilla n-gram shows a slightly better performance than some other methods. similar performance of vanilla n-gram in terms of recall can be observed in figure 6 . this, from another angle, justifies the effectiveness of our p-gram, which utilizes the metadata of machine-generated messages-the position of message keywords."
"we proposed a new random multiple access scheme for symbol-synchronous slotted aloha random access systems. each node transmits several replicas of the same message within a frame after a pre-multiplication by a (pseudo)-randomly chosen coefficient in gf (2 n ). the receiver tries to decode as many linear combination in gf (2) of signals colliding in each slot as possible and then tries to recover all the messages transmitted within a frame treating the linear combinations decoded in the whole frame as a single system of equations in gf (2 n ). we presented analytical results for the throughput at system level and simulation results for the decoding process at the physical level. as future work we plan to optimize the multiple access scheme taking into account the decoder performance, which is a function of the collision size and the specific linear combination within a collision, with the aim of maximizing the system throughput and minimizing the packet error rate. at the physical level we plan to consider higher order modulations and different decoding approaches."
"in this paper, we propose a new machine language processing (mlp) approach, called p-gram, specifically designed for the clustering of machine-generated messages. p-gram leverages the positions of keywords in the template structure of machine-generated messages to extract message keywords more accurately. it is based on the observation that message keywords appear at relatively fixed positions in machine-generated messages."
"the rest of the paper is organized as follows: section ii introduces the structural features of machine-generated messages and our goals. we provide the technical details of p-gram in section iii and present the implementation details and experimental results of p-gram in section iv. related work is discussed in section v. finally, section vi presents some concluding remarks and proposes future work."
"the cr users' network might operate at the pu's licensed band if the fusion center decides that the channel is idle, this occurs in two cases:"
"step 4, we use the merged keywords as features for message clustering. to numerically weigh each feature in messages, we exploit the position information of keywords and derive a ''positional weighting'' for each feature using a variability weighting technique, namely, entropy analysis. the features' positional weightings are then used by a general centroid clustering method, k-medoids, to group the messages into clusters reflecting message types."
"to evaluate the effectiveness of the proposed p-gram, we have applied it to eight datasets of machine-generated messages from eight real-life systems. they include 3 system log files (asgard [cit], logparser sosp and proxifier [cit] ), 2 binary protocol traces (ldap [cit] and ims [cit] ), and 3 text protocol traces (ldap, bank soap [cit] and twitter rest [cit] ). all the datasets can be found in https://github.com/jiaojiaoswin/datasets. table 1 lists the basic statistics of these datasets. note that, the message clusters of these datasets are imbalanced (see the gini indexes). a lower gini index means the message clusters are more equally distributed. ims and soap present relatively low gini indexes, while ldap, sosp and proxifier present very high gini indexes. the ratio of the smallest cluster in each dataset is also presented."
"in this paper, the sensing performance of a cr and a cr network is evaluated under two different operational modes, cpup and csusu transmission modes. the cpup mode guarantees a minimum level of interference to the pu (we fix the probability of detection at the required level) and try to find a tradeoff between the probability of false alarm and the sensing time at a particular snr. the csusu scenario is taken from the cr's perspective; by keeping fixed the usability of unoccupied bands at a certain level (we fix the probability of false alarm at lower values) and try to find the tradeoff between the probability of detection and the sensing time at a particular snr."
"for this study, we consider a tdm based system in which each frame consists of one sensing slot of duration (t) plus one data transmission slot of (t-t), with t is the total frame duration."
"each message is transmitted more than once within a frame, i.e., several replicas of the same message (bursts) are transmitted. assume that node i has a message u i to deliver to r during a given frame, i.e., node t i is an active terminal. before each transmission, node i pre-encodes u i as depicted in fig. 1 . the message to be transmitted is divided into sub-blocks. fig. 1 . pre-coding, channel coding and modulation scheme at the transmitter side. pre-coding consists in dividing the message into blocks of n bits (indicated as u r i in the figure) and multiply each of these blocks by the same coefficient randomly chosen in gf (2 n ). the sub index j indicates the slot within a frame in which the replica of message u i is transmitted. a different coefficient α ij is used for each replica."
"to filter out noise positions of t in x, we set a probability density threshold (t) for t. particularly, we set (t) as half of t's average probability density over all windows:"
"to assign a high weight to stable keywords and a low weight to dynamic noise, we invert the entropy for each feature by applying a scaling function of the form given in the following weight equation"
", and t i,n is called a sub-gram of t i,n (denoted as t i,n t i,n ). based on the above definitions, we can obtain the following theorem."
"from figures 5 and 6, we see that prodecoder outperforms the vanilla n-gram in terms of precision and recall over all datasets except ims and twitter rest. prodecoder adopts the topic-model approach from nlp to extract ''topics'' (i.e., keywords) from messages. the ''topic terms'' are those highly related 4-grams [cit] . prodecoder utilizes the hidden features (i.e., the co-occurrence of 4-grams) of mlp."
"the rest of the paper is organised as follows. section 2 provides a background discussion on geometry optimisation using cfd, and in section 3 we present the necessary background regarding geometry representation. the problems in this test suite are detailed in sections 4 and 5. finally, we draw conclusions in section 6 with the base geometry performance and relevant computation time."
"in order to convert the classes and properties representing data semantics into the sequence of nucleotides, we propose the property representation and type design suited for dna implementation. for example, this paper annotates three rsis e1eb7, d87c9 and b8ef1 with three properties: city (ct), imagequal (qa) and cloud_cover (cc). the first image's property values are guang zhou (gz), excellent (e), and 0, respectively. the other two's values are hong kong (hk), good (g), 0, and hk, g, 16. considering the linear structure of dna strands, we arrange these properties and their values in sequence as shown in figure 3 . the label of a vertex is denoted as two-tuples (property name, property value). the edge denotes the connection between the vertices in the directed graph. to simplify the graphic structure, two new vertices labeled as ''start'' and ''end'' are added to the directed graph and the vertices are integrated into one if they have the same property and property values. as shown in figure 4, there are directed paths representing the annotation results of the rsis between initial and terminal vertex in property network."
"(1) the im loads absorbed much more reactive power after the fault. consequently, voltage instability occurs following a severe fault. (2) motor speed recovery is closely related to voltage recovery. if the speed cannot be recovered after clearing the fault, the motors will absorb much of the dynamic var. (3) the slip characteristics depend on the mechanical and electrical torques. when the mechanical and electrical torque do not equal each other, the slip moves to 1 as the equilibrium point is exceeded."
"indeed, semantic fusion problem have been shown to be an np-complete problem [cit], which means that it is unlikely to find an algorithm working in polynomial time. the semantic fusion on image properties of modest size requires an altogether impractical amount of time on conventional electronic computer [cit] . however, we use a finite sequence of ligation reaction and screening operations described above to solve the semantic fusion problem. a fusion starts with an initial tube and ends with one final tube. the fusion time depends solely on the total time of ligation reaction and five screening steps instead of the number of semantic properties and ontology complexity. then the massive parallelism of dna renders exponential time complexity in semantic fusion to linear time."
"(1) the im loads absorbed much more reactive power after the fault. consequently, voltage instability occurs following a severe fault. (2) motor speed recovery is closely related to voltage recovery. if the speed cannot be recovered after clearing the fault, the motors will absorb much of the dynamic var. (3) the slip characteristics depend on the mechanical and electrical torques. when the mechanical and electrical torque do not equal each other, the slip moves to 1 as the equilibrium point is exceeded."
"in this work we describe a python-based software framework for the automated optimisation of a suite of computationally expensive design problems. these include two single objective, and one multi-objective cases to act as benchmark test problems for the cfd and optimisation communities. an appealing aspect of this code is its flexibility, allowing the user to test their optimisation methodology under a number of dimensions for the decision space, and the application of these methods to real-world engineering problems. the schematics and contour diagrams shown in this paper were generated using utilities in the proposed framework, and paraview -a visualisation utility -was used to visualise the flowfield data. in summary, table 1 below shows the cost function values and typical simulation runtimes for the base case of each test problem, which may be used as a yardstick to compare the fitness of optimal designs found using different optimisation algorithms."
the post-fault transient voltage dip or overshoot should not exceed 25% at the load buses or 30% at the generator buses; it should not exceed 20% for more than 20 cycles at the load buses.
"as mentioned earlier, statcom regulates the voltage at its pcc by controlling the amount of reactive power that is injected or absorbed from the power system through its voltage-source converter. when the system voltage is high, statcom absorbs reactive power (inductive mode), while the device generates and injects reactive power into the system when the system voltage is low (capacitive mode). figure 3 represents the voltage-current (v-i) characteristics of svc and statcom."
"product line engineering (ple) is a paradigm for reuse-based complex systems development that is well installed in the industry. among the proven benefits are reduced time to market, better asset reuse, and improved software quality [cit] . to be successful, ple must efficiently manage the variability -the capacity of product line's artifacts to vary -present in the products that form a product line (pl). several modeling approaches have been proposed to represent the artifacts of a pl, their properties and relationships. all these notations can be used to describe in a single product line model (plm) all the legal combinations of features (qualities, artifacts, etc) [cit] . in this context, being able to reason about the plm is an important success factor in the ple strategy. reasoning on plms is achieved by querying the models in order to verify, analyze or configure them [cit] . for instance, plms can be verified to guarantee that they do not have undesirable properties affecting the correctness of the products they help develop. several approaches are available in the literature to support automatic reasoning on plms. several approaches consist in transforming the plms into a constraint program that can be executed by a solver. for example, satisfiability (sat) solvers are used to analyze plms specified as boolean constraints. others use sat or constraint over finite domains solvers to find the number of solutions that can be configured on a plm. interestingly, it is actually well know that for this task binary decision diagram (bdd) solvers are more efficient. thus, authors seem to undermine the efficiency of certain reasoning operations to prioritize others. one reason might be that the transformation is guided by the solver to be used and not by nature of the plms or the efficiency/limitations of using one solver or another one."
"before the semantic information is converted into dna, an encoding model is required. although diverse coding strategies for dna sequences have been developed and some have been demonstrated [cit], no standard model exists. church gm [cit] first proposed a simple, universal strategy. in church's work, arbitrary digital information can be converted into bitstreams by utilizing the ascii code. these bits are then encoded onto the oligonucleotide library. unlike conventional approaches, church encodes one bit per base in order to meet the appropriate gccontent and introduces a 19-nt oligonucleotide to represent the data's address space."
"where q i is the reactive power at load bus and y is the branch parameter. with this information, the most efficient load bus for restoring the voltage can be selected. the reactive power reserves of the var compensation resources are determined at the point of convergence of the power flow equations. thus, if the remaining flows can be shifted to other branches using reactive power compensation, the power system might be restored. in the unsolvable case, the yv analysis can trace the power flow solution depends on the variations of transmission line parameter via continuation when a power flow solution does not exist after the branch outage [cit] . the parameterization of the branch can be depicted as a π-equivalent circuit, as shown in figure 7 ."
"coordinating council (wecc) [cit] . in the wecc, criteria are presented as a procedure available for determining voltage stability limit under a fixed load condition. to consider uncertainty, the required margins for performance levels are specified; in this paper, the minimum required margins on the wecc voltage stability criteria are used. in addition, a procedure for evaluating the secure operating limits of voltage stability is established. figure 4 shows the pv curves for margin evaluation at each performance level. as shown in figure 4, based on the wecc criteria, two cases of voltage stability analysis are considered. one can notice that the maximum operating point on the p axis must have a mw margin equal to or greater than the values measured from the nose point of the pv curve for each performance level. in this study, it was fixed that the minimum power margin for performance level a, which is the contingency, must be greater than 5%. for performance level b, the minimum margin should be greater than 2.5% [cit] . it is to be noted that the performance level should not cause voltage collapse within the limits of the study."
"a key factor that results in fidvr in power systems is the nature of the im loads in the transmission and distribution systems. the performance characteristics of these loads are changing due to technological advances. a large three-phase induction motor (tpim) and small single-phase induction motor (spim) are connected directly to the electrical network. these types of im loads are expected to be a major component of the aggregate load due to economic reasons. figure 2 shows the structure of the load model used in this study. generally, im loads operate in a constant torque mode depending on the voltage of the connected nodes [cit] . as soon as the voltage magnitude is reduced, im loads attempt to slow down. if a low voltage persists for a certain period of time, the im loads might be stalled or stopped. however, when the voltage begins to recover, the im load restarts, draws large reactive power and current from the grid, resulting in a delayed voltage recovery. another typical load that causes load recovery is the heating load based on thermostat-controlled resistance. thermostatically controlled loads are self-restoring in nature. if these electric heating loads constitute a substantial share of the load, especially in winter, the system might be significantly affected. in the case of a heating load with a thermostat, the load might increase even if tap blocking is performed. these characteristics might be the main reason for voltage instability [cit] ."
"a hydropower plant converts the gravitational potential energy of water from an upstream reservoir into electrical energy, by means of a turbine coupled to a generator. the flow leaving the turbine loses its velocity in the draft (exhaust) tube, where kinetic energy of the flow is transformed into pressure. this energy conversion has a significant impact on the efficiency and power of the turbines, thus, the draft tube design is of great interest to the industry. the two most common draft tubes considered in the literature are the sharp-heeled and underground designs; [cit] s. elbow-draft tubes are widely used for vertical kaplan and francis turbines, due to their low height, lesser excavation cost, and greater potential for pressure recovery. this type of draft tube consists of three parts: a cylindrical cone, an elbow, and an end diffuser. the draft tube geometry considered for the second case of this test suite is a 1:11 scaled model from the hölleforsen [cit] . a schematic of this draft tube geometry is shown in figure 4 (top-left)."
"load characteristics exert a significant influence on power system stability. however, load modeling is complicated because a typical load bus, represented in stability studies, is composed of a large number of devices. in addition, the composition changes depending on many factors, including time, weather conditions, and state of the economy. even if the load composition is known exactly, it would be difficult to represent each individual component, as there are millions of such components in the total load supplied by a power system. therefore, load representation in system studies is based on a considerable amount of simplification [cit] . this subsection describes the fundamental concepts related to the development of the load model used in this study. for the purpose of this study, a more detailed dynamic load representation of the korean power system was generated. detailed information on the actual load compositions was obtained from the national statistical information database [cit] . the recent change in the peak load in winter is shown in figure 1 . in figure 1, one can notice an increase in seasonal demand due to an increase in winter loads; further, the annual variation in power demand is continuously increasing. this increase in electricity consumption is attributed to the rise in gas prices. an increase in severe cold conditions due to climate change and changes in lifestyle are contributing to the increase in residential power equipment. in addition, due to advancements in the service industry, the demand for load in the commercial class is rapidly increasing. in particular, in the case of winter peak load, the proportion of commercial load exceeded that of the industrial load. in figure 1, one can notice an increase in seasonal demand due to an increase in winter loads; further, the annual variation in power demand is continuously increasing. this increase in electricity consumption is attributed to the rise in gas prices. an increase in severe cold conditions due to climate change and changes in lifestyle are contributing to the increase in residential power equipment. in addition, due to advancements in the service industry, the demand for load in the commercial class is rapidly increasing. in particular, in the case of winter peak load, the proportion of commercial load exceeded that of the industrial load."
"as large quantities of im loads exert a significant influence on the stability of the power systems, the requirements for reactive power support have become important. reactive power plays a major role in the transmission of power systems. during im reacceleration after a fault, when high current is drawn, a high reactive power support is required for a few seconds to provide a fast response [cit] . facts technologies are available in different forms, such as svc, statcom, thyristor-controlled series capacitor (tcsc), and unified power flow controllers (upfc). among them, svc and statcom are categorized as dynamic var compensation resources that rapidly inject or absorb vars to support power system voltage immediately after system disturbances. the statcom technology increases voltage stability by providing dynamic control and compensation of the system voltage. the advantage of statcom is that its compensating current is not dependent on the system voltage level at the point of common coupling (pcc) when the voltage drops; it operates at full capacity. utilizing the statcom switching control allows for faster control response when compared to svc and improved performance in power systems. in addition, statcom is more resilient to changes in power system conditions, such as variations in harmonic levels, and requires smaller outdoor equipment when compared to svc [cit] ."
"when a severe contingency occurs, it might be difficult to analyze systems using general power flow equations as the solutions are not valid after a contingency. therefore, a branch-parameter continuation power flow method is used to the restore power flow solvability in unsolvable contingencies based on a continuous power flow. the branch parameter y is multiplied by the series and shunt admittances of the branch. the bcpf can trace the power flow solution, in which the state of the branch moves from a pre-contingency state to a post-contingency state by changing the y parameter from zero to one [cit] . this method represents the yv analysis described in this investigation. figure 6 shows two cases of yv curves with contingencies. from figure 5, it can be inferred that the reactive power margin can be calculated using the qv curve, which defines the amount of reactive power that can be consumed by the bus. when a fault occurs, the reactive power margin of the bus should be maintained at a level sufficient to meet the criteria. therefore, it is possible to calculate the reactive power margin on each bus for serious faults, so that the bus or area with a low reactive power margin can be determined as the reactive power vulnerable site. to this end, a vqvi margin vulnerability index can be used to identify vulnerable areas. vqvi considers the changes in the reactive power margin before and after the contingency and can be expressed as follows."
"author contributions: y.l. conceived and designed the research, conducted static and dynamic simulations, and wrote the paper. h.s. improved the theoretical part and supervised the research."
"at present, remote sensing data are dramatically increasing in volume. for example, the u.s. national climatic data center holds the world's largest archive of weather data and has archived 3 pb (petabyte) satellite imagery [cit] . the extreme compactness of dna is incredible. because the mean molecular weight of a nucleotide is 330 g/mol [cit] . we compare favorably contemporaneous storage technologies in table 3 [cit] . dna storage has obviously the potential of storing data 100 times more compactly than other technologies."
"this model starts with the input tube t, containing the result of the ligation reaction. all separate operations select the oligonucleotides and thus require the amplification of the resulting tubes by the pcr (polymerase chain reaction)."
"this paper presents a new framework on the reactive power compensation strategy designed based on voltage stability analysis. the framework adopts the conventional analysis of active and reactive power margins to decide the critical contingencies and the areas of reactive power deficiency. this paper adopts not only steady-state analysis but also dynamic time-domain simulations for the adequate amount of dynamic reactive power support. there also might be a limitation of only covering solvable cases using the conventional analysis, but this paper adopts the sensitivity analysis for unsolvable cases using the branch parameterized continuation power flow. for adequate compensation strategies, vulnerable buses as compensation locations must be carefully chosen in terms of voltage stability. the case study results demonstrated that the compensation level decided by the proposed method could mitigate the fidvr caused by im loads. in addition, the proposed method might be applied for determining the optimal number and placement of facts devices to resolve the fidvr."
"there is few published work in the literature about the application of dna-based approach to semantic fusion. tsuboi proposed a pattern matching algorithm based on stickiness of dna molecules [cit] . semantic network technology is used to solve information recognition problem. however, the fusion of semantic relationship is not involved. this restricts the analysis and reasoning capacity of the processing system. moreover, the encoding scheme in this algorithm is not suitable for arbitrary digital information and the different data objects have to be encoded by different oligonucleotides. however, an exhaustive representation is considered unrealistic. church proposed a novel strategy to store digit information in dna [cit] . in church's work, all data blocks can be programmed into a bitstream and then encoded onto thousands of oligonucleotides. but the sequential conversion code (perl) faces the challenge from big data. xu provided a new dna computing model for graph vertex coloring problem [cit], which can effectively reduce the solution space by seminested polymerase chain reaction. all these approaches described above lack support for semantic reasoning and little attention has been given to big data, which have become the key problems of knowledge sharing and semantic representation in the web environment."
"the flowfield through the base design of the draft tube is shown in figure 4 (bottom-left). an unusual characteristic of this test case is that a swirl-flow is imposed at the inflow, which simulates the exit flow from the turbine. this makes the flowfield complex, and, unlike the other test cases in this suite, inherently three-dimensional. as discussed earlier, the main purpose of the draft tube is to recover the kinetic energy leaving the turbine by increasing the pressure energy. a performance indicator of this is given by the pressure recovery factor,"
"load buses with large participation factor values have more influence in contributing to the voltage instability. consequently, these buses can be regarded as best candidate location for determining the injection to enhance system voltage stability."
"many real-world engineering design optimisation problems are computationally expensive. for instance, optimising the shape of an aircraft wing may require evaluating the performance of candidate designs in flight using computational fluid dynamics (cfd). a high-fidelity simulation may take hours to converge, imposing a practical limit to how many designs may be considered during optimisation."
"where and represent the injection of the active and reactive power, respectively. meanwhile, is chosen to remove the ill-conditioned problem from the singularity. further, is the number of voltage-controlled buses. as shown in equation (5), ⁄, ⁄, and are augmented from the original power flow jacobian matrix. using these characteristics, the bcpf can determine whether the cause of the divergence problem is unsolvable. the following is a sensitivity analysis procedure for applying the bcpf application to obtain a solution."
"using the above results, a transient stability analysis was performed to verify the stvs. the objective of the dynamic simulation is to establish appropriate dynamic var supports to satisfy the voltage criteria. facts device placement determined based on the qv analysis is shown in table 2 . ten candidate buses were chosen to install the facts devices, which were installed in the order of ranking to observe whether the voltage was recovered or not. as the typical size of a facts device installed in the transmission level is 300 mvar, the dynamic var support source capacity limit is set at ±300 mvar. the post-fault voltage trajectories of the buses are shown in figure 13 . from figure 11, one can notice that the transient voltage criteria are satisfied when three facts devices are installed. if only one or two facts devices are installed, the fidvr event caused by an insufficient reactive power support cannot be resolved. therefore, it was decided that the facts devices should be installed at 1400, 1300, and 1700 to mitigate the fidvr phenomenon."
"in the next step, we employed qv analysis to determine the control actions necessary to overcome violations of the voltage criteria. the purpose of this analysis is to determine the region of reactive deficiency in terms of reactive power for the severe contingencies determined through pv analysis. additionally, qv analysis is used to identify the required amount of reactive power compensation. figure 5 represents the qv curve for determining the reactive power margin."
"design performance in a fluid environment cannot usually be evaluated analytically. it is therefore necessary to resort to a numerical approximation using cfd. cfd undoubtedly represents the more computationally costly end of engineering simulation, requiring fast processing speed and making serious demands on memory, multi-processor intercommunication speeds (for parallelisation) and even graphical visualisation. cfd requires the solution of a set of partial differential equations (pdes) which describe the physics of fluid flow (principally the navier-stokes equations, obtained independently by m. navier and g. stokes in 1822). this is typically achieved using the finite volume method, in which the fluid continuum is discretised into a grid and the pdes are solved algebraically for each cell. there are many software packages available to perform these calculations. over the last few years the open-source c++ code openfoam [cit] has emerged as one of the most popular cfd codes in the community, partly boosted by the financial issues of using a commercial code with a 'per core' license cost."
"generally, the standard procedure to create a geometry is to use a computer aided design (cad) software. however, it is difficult to automatically alter designs using cad. consequently, we resort to various parametric representations for parts of the original geometry created in cad; varying the parameters of the representation allows the generation of new geometries. the new geometries may then be considered as candidate designs in the optimisation. below we briefly describe the representation methods used in this paper. to keep computation times manageable we formulate the test problems in terms of two-dimensional geometries (although one is a fully three-dimensional flow)."
"from figure 8, the post-fault transient voltage performance is expected to meet criteria, namely: facts is an effective technology to mitigate transient voltage dip by providing dynamic var support for reactive power compensation. this study focuses on identifying the most effective locations and determining the capacity of facts so that the transient voltage dip criteria are satisfied for mitigating fidvr. to accomplish this task, analysis of the transient voltage recovery of the korean power system was conducted according to the above described standards. figure 9 shows a flowchart of the employed voltage stability analysis and assessment method. as shown in figure 8, the post-fault transient voltage performance is expected to meet the following criteria."
"all properties and property values are converted to binary strings based on ascii encoding. each character corresponds to an 8-bit binary code. for example, the property cty has the binary code 011000110111010001111001. conversion code in file s4 can then convert these bits to a or g for 0 and t or c for 1. bases are chosen randomly according to the result of function rand(). considering the big dataset, we add a 32-bit address starting from 00000000000000000000000000000000. for example, the properties and property values of an rsi e1eb7 in figure 3 is represented by the string startctygz qa e cc00 end, where the symbol represents a whitespace character, start and end are the labels of the new vertices added in figure 4 . this property string has an ascii code 001000000111001101110100011000010111001001110100 01100011011101000111100101000111010110100010000001110 00101100001001000000100010100100000011000110110001100 11000000110000001000000010000000100000011001010110111 001100100. it is then encoded to two 200 nt oligonucleotides by the conversion code given in file s4. each encodes a 128-bit data block (128 nt). before synthesized, the sequence is augmented to include the bases representing data type and data unit. for example, an oligonucleotide accggattgtccgcaggccttggcatagacctacgttaca is the result of encoding the property ctygz in the vertex (cty,gz). considering the data type is string and data unit is undefined, we add tcga and tgca to the original oligonucleotide according to table 1 . thus, the final oligonucleotide of the vertex (cty,gz) is accggattgtccgcaggccttggctcgatgcaatagacctacgttaca, as shown in table 2 ."
"it should be noted that from practical perspective only a few iterations of subdivision usually results in a visually smooth curve that may be exported in stl format. we set the iteration limit to five in this paper. in figure 1, we provide an illustration of a catmull-clark subdivision curve."
"the results of the stability analysis identified the areas vulnerable to fidvr and the candidate buses for most effective actions have been proved; the participation factors evaluated could provide the valuable information on the adequate facts application against fidvr. in addition, from the bus and branch participations, remedial actions to improve the overall system voltage stability could be decided. the above-mentioned results can be summarized in table 4 . from these results, one can notice that the number and placement of facts devices to quickly recover voltage depends on the performance level. in addition, dynamic voltage recovery times for two facts implementation cases are obtained for the comparison purposes. in table 4, it is indicated that when the system voltage is restored up to 0.8 p.u. within 2 s, and that fidvr can be properly mitigated. for case 1, the voltage was recovered within 2 s when three facts were applied. for case 2, the voltage was recovered within 2 s when four facts were equipped, but the transient voltage dip criteria were not satisfied, and hence fdivr cannot be adequately alleviated. it can be seen that a five facts application is required to mitigate the fidvr."
"from figure 8, the post-fault transient voltage performance is expected to meet criteria, namely: facts is an effective technology to mitigate transient voltage dip by providing dynamic var support for reactive power compensation. this study focuses on identifying the most effective locations and determining the capacity of facts so that the transient voltage dip criteria are satisfied for mitigating fidvr. to accomplish this task, analysis of the transient voltage recovery of the korean power system was conducted according to the above described standards. figure 9 shows a flowchart of the employed voltage stability analysis and assessment method."
"flow [cit] . under these circumstances, when a serious fault occurs in interface lines, a region with a high proportion of im loads might significantly increase the var demand during voltage recovery. however, external reactive power cannot be sufficiently delivered to the im load-concentrated region through long-distance transmission lines. as a result, the system might experience stvs problems. unless immediate countermeasures are applied, this leads to a fidvr phenomenon. figure 10 shows a bus voltage profile without a countermeasure. here, the current under voltage load shedding (uvls) scheme operated in the korean power system is reviewed. the uvls scheme is a demand-side solution that accomplishes the stabilization of the system by shedding the pre-defined loads in the case of a severe contingency as an event-based special protection scheme (sps). however, the current uvls system has a time delay between the activation and the actual load interruption. when the load shedding is delayed, it might not alleviate the fidvr phenomenon. in addition, the uvls system is forced to shed the pre-defined loads as the last resort, possibly resulting in an inconvenience for the end-use consumer. therefore, in this paper, a scheme is proposed to utilize the dynamic reactive power resources to mitigate the fidvr phenomenon. the comparison results of the bus voltage recovery for an existing scheme and the proposed scheme are shown in figure 11 . after the fault occurs, the voltages of the load buses drop immediately. without a countermeasure scheme, the bus voltage cannot be recovered to its normal value after fault clearance. from figure 10, the following observations can be made:"
"in the unsolvable case, the yv analysis can trace the power flow solution depends on the variations of transmission line parameter via continuation when a power flow solution does not exist after the branch outage [cit] . the parameterization of the branch can be depicted as a π-equivalent circuit, as shown in figure 7 ."
"the results of the stability analysis identified the areas vulnerable to fidvr and the candidate buses for most effective actions have been proved; the participation factors evaluated could provide the valuable information on the adequate facts application against fidvr. in addition, from the bus and branch participations, remedial actions to improve the overall system voltage stability could be decided. the above-mentioned results can be summarized in table 4 . from these results, one can notice that the number and placement of facts devices to quickly recover voltage depends on the performance level. in addition, dynamic voltage recovery times for two facts implementation cases are obtained for the comparison purposes. in table 4, it is indicated that when the system voltage is restored up to 0.8 p.u. within 2 s, and that fidvr can be properly mitigated. for case 1, the voltage was recovered within 2 s when three facts were applied. for case 2, the voltage was recovered within 2 s when four facts were equipped, but the transient voltage dip criteria were not satisfied, and hence fdivr cannot be adequately alleviated. it can be seen that a five facts application is required to mitigate the fidvr. it can be seen in figure 14 that the time domain simulations are run by applying a fault. to address the delayed voltage recovery caused by im stalling under post-fault voltage trajectories, the voltage criteria are met using five facts devices. when four or less facts devices are installed, the fidvr phenomenon cannot be mitigated due to insufficient var support. therefore, it was concluded that the facts devices should be installed at 2700, 4500, 2500, 4700, and 4650 to mitigate the fidvr phenomenon."
"to overcome these limitations, we propose to represent the semantics of plms as abstract constraints with a unique notation that encompass other constraint languages (e.g., over booleans, integers, reals, trees, lists, etc.). as figure 1 sows it, once a plm is specified as abstract constraints, it can be compiled with the platform in any constraint language depending on the analysis to achieve and on solver to use for the analysis."
"note that authors often develop custom codes for their problems, and they are generally reluctant to release code, primarily because many problems are proprietary in nature. therefore, despite numerous example problems, it is often difficult to acquire the simulators for exact comparison of methods. moreover, there is no complete test suite of benchmark problems similar to inexpensive test suites such as the dtlz test problems [cit] . the optimisation community is actively investigating benchmark computationally expensive problems. however, most previous attempts, for example [cit], use pseudo-expensive problems, i.e. inexpensive functions are used with delays to mimic expensive problems."
"the proposed reactive power compensation strategy has also been applied to no.2 on the contingency list in performance level b. as mentioned earlier, contingency no.2 represents the most severe outage. for performance level b, corresponding to the bus section representing contingency, the minimum margin should be greater than 2.5%. this means that a 2.5% margin on the pv analysis is required for the worst-case performance level b disturbance cases. the 2.5% is based on the load level measured at the voltage collapse point. therefore, the proposed method based on bcpf was applied to verify the unsolvable case. to identify effective buses for reactive power compensation, a sensitivity analysis was then performed at the critical point of the yv curve using the participation factor calculated for the load buses. the results of sensitivity analysis are shown in table 3 . table 3, one can observe that the eastern metropolitan area consists of vulnerable sites. in the korean power system, when the most severe 765 kv transmission line fault occurs, power flow takes a roundabout way to the nearby lines. when a large amount of power is concentrated in the power transmission line, voltage instability occurs. in addition, the dynamic nature of im loads is the same as imposing a heavy demand for fast-responding reactive power resources under low-voltage situations. this leads to secondary effects, such as fidvr, which further exacerbates the problem. the transient voltage dip and slow voltage recovery issues might be addressed by fast-responding from figure 11, one can notice that the transient voltage criteria are satisfied when three facts devices are installed. if only one or two facts devices are installed, the fidvr event caused by an insufficient reactive power support cannot be resolved. therefore, it was decided that the facts devices should be installed at 1400, 1300, and 1700 to mitigate the fidvr phenomenon."
"semantic fusion is a process that is ubiquitous in nature. in this paper, a novel dna-based semantic fusion model is proposed. the model combines organically parallel strategy with dna encoding, which makes semantic conversion more efficient and storage density higher. furthermore, we describe the abstract representation of semantic fusion and thus show that the fusion time of semantic properties in remote sensing images depends solely on the biochemical reactions and operations instead of the ontology. however, there are still many issues to be considered. foremost issue is error. dna molecules are fragile and they break easily. the errors of separate operations with dna strands can make a really dramatic difference. thus, steps towards coping with errors should be taken in. in future work, we also implement the ligation reaction and screening procedures based on biochemical techniques and clarify details in another paper."
the hpc-jnu cluster system (http://hpc.jnu.edu.cn/) has 20 computational nodes. each node is connected via the infiniband network. table 4 shows the specifications of the hpc-jnu cluster system. figure s1 and figure s2 show the photographs of the computational nodes and the storage node. file s1 code for remote sensing data ontology (see also http://cs.jnu.edu.cn/sun/ontology). computer code in the rdf schema language is used to generate the remote sensing data ontology in figure 1 . the rdf/owl api is required.
"in order to do that, our first concern is to define a notation that consists in a constraints system allowing represent product lines. according to saraswat [cit], a constraint system can be defined as a tuple where d is a set of first-order formulas closed under conjunction and existential quantification, is an entailment relation between a finite set of formulas (taken from d) and a single formula and must be generic (that is:"
"-we focus on real-world problems of designing apparatus using cfd to evaluate the performances of a geometry in a fluid environment. as such these are functions that are truly computationally expensive to evaluate, and optimisation has real implications for engineers. -all problems use open source software suitable for popular platforms and machines, and thus enable comparison between different methods without requiring substantial hardware. -this flexible suite offers the opportunity to create different instances of a problem with a configurable number of dimensions for the decision space. in practice increasing the number of dimensions increases the difficulty of the associated problem. -we provide the base geometry performance and the computation time so that they may be used as a yardstick for comparison. -some decision vectors may result in an unphysical geometry, and consequently a cfd simulation will fail. we therefore constrained the problems to only evaluate feasible geometries. we provide a callable function encapsulating the constraint checks to inform the users whether a decision vector is feasible. as such users may treat these constraints as black-box functions. -some design optimisation scenarios are naturally phrased as single objective problems, while others are inherently multi-objective. adhering to such genuine objectivity of design optimisation, we present three distinct design problems: two single-objective and one multi-objective."
"head losses within a flow are an undesirable characteristic for engineering design. to quantify this, the mechanical energy loss factor, ζ, describes the energy that is converted to a form that cannot be used during the operation of an energy producing, consuming, or conducting system (i.e. due to frictional losses, or dissipation due to turbulence). in one mathematical form, ζ is defined as the total pressure difference between the inflow and outflow of the apparatus (relative to the kinetic energy at the inflow), i.e."
"remote sensing data ontology ontology, as a formal representation of both implicit and explicit domain knowledge, can help to deal with heterogeneous representations of data and their interrelationships. there exist several forms of ontology with different semantic richness. as a specification developed by world wide web consortium, the resource description framework (rdf) [cit] can present semantic information of web resources. rdf schema [cit] provides a type system for rdf and defines classes and properties that may be used to describe classes, properties and other data resources. it can also be used to build a lightweight ontology by describing rdf vocabularies. figure 1 illustrates the remote sensing data ontology by using rdf schema language. the computer code of the ontology is provided in file s1. all terms in the ontology vocabulary are divided into five groups (namely, identification information, data quality information, spatial data organization information, instrument information, and location information) to represent the content, quality, condition, and other characteristics of data. to enable the extensibility of the ontology, we evaluated the suitability of several existing geospatial metadata standards, including the content standard for digital geospatial metadata: extension for remote sensing metadata [cit] 5 [cit] and iso/ [cit] 9 [cit] . the extension defines the metadata elements published by the u.s. federal geographic data committee and documents digital remote sensing datasets in the us. [cit] 5 [cit] 5 are from the extension standard. iso/ [cit] 9 [cit] 5. these two iso standards are very simple but not suitable for ontology modeling. considering the fact that the conceptual model in the extension does not provide enough semantic description of geographic data, we construct a hierarchical structure of the ontology. the relationships among specific classes are encoded into the ontology structure. the rdf schema properties rdfs:range and rdfs:domain describe the relationships between specific properties and classes, and a lot of image data relationships have been described using the domain properties from the extension standard."
"where is the reactive power at load bus and is the branch parameter. with this information, the most efficient load bus for restoring the voltage can be selected. the reactive power reserves of the var compensation resources are determined at the point of convergence of the power flow equations. thus, if the remaining flows can be shifted to other branches using reactive power compensation, the power system might be restored."
"generally, im loads operate in a constant torque mode depending on the voltage of the connected nodes [cit] . as soon as the voltage magnitude is reduced, im loads attempt to slow down. if a low voltage persists for a certain period of time, the im loads might be stalled or stopped. however, when the voltage begins to recover, the im load restarts, draws large reactive power and current from the grid, resulting in a delayed voltage recovery. another typical load that causes load recovery is the heating load based on thermostat-controlled resistance. thermostatically controlled loads are self-restoring in nature. if these electric heating loads constitute a substantial share of the load, especially in winter, the system might be significantly affected. in the case of a heating load with a thermostat, the load might increase even if tap blocking is performed. these characteristics might be the main reason for voltage instability [cit] ."
the post-fault transient voltage dip or overshoot should not exceed 25% at the load buses or 30% at the generator buses; it should not exceed 20% for more than 20 cycles at the load buses.
"the real rsis must be first preprocessed with semantic annotation technique, where semantic tags defined in the ontology are assigned to the phrases in the descriptive metadata of the rsis. this facilitates the fusion and reasoning based on image semantics. rdf instance of an rsi is shown in figure 2, where the metadata of rsi 103001001e1eb700 are annotated with the properties such as imagequal (image quality), cloud_cover and spatresv"
"here, the current under voltage load shedding (uvls) scheme operated in the korean power system is reviewed. the uvls scheme is a demand-side solution that accomplishes the stabilization of the system by shedding the pre-defined loads in the case of a severe contingency as an event-based special protection scheme (sps). however, the current uvls system has a time delay between the activation and the actual load interruption. when the load shedding is delayed, it might not alleviate the fidvr phenomenon. in addition, the uvls system is forced to shed the pre-defined loads as the last resort, possibly resulting in an inconvenience for the end-use consumer. therefore, in this paper, a scheme is proposed to utilize the dynamic reactive power resources to mitigate the fidvr phenomenon. the comparison results of the bus voltage recovery for an existing scheme and the proposed scheme are shown in figure 11 . figure 11 indicates that the fdivr phenomenon occurs even when the existing scheme is applied. this is attributed to the reactive power consumption of the motor load being increased dramatically as the voltage level falls to the limit value in the case of delaying the load shedding caused by the communication delay. on the other hand, it can be seen that the voltage is recovered when the proposed scheme is applied. it is noted that facts could have a positive effect on delayed voltage recovery owing to the induction motor power consumption characteristics. this paper presents the analysis associated with the design and application of the dynamic reactive power control strategy in case studies based on the practical system. it is desirable that facts devices need be installed in metropolitan areas with adequate amount of reactive compensation, because the facts system has a small footprint and requires less outdoor equipment. the placement of the most important buses, in terms of voltage stability, are determined through the proposed methodology. figure 11 indicates that the fdivr phenomenon occurs even when the existing scheme is applied. this is attributed to the reactive power consumption of the motor load being increased dramatically as the voltage level falls to the limit value in the case of delaying the load shedding caused by the communication delay. on the other hand, it can be seen that the voltage is recovered when the proposed scheme is applied. it is noted that facts could have a positive effect on delayed voltage recovery owing to the induction motor power consumption characteristics. this paper presents the analysis associated with the design and application of the dynamic reactive power control strategy in case studies based on the practical system. it is desirable that facts devices need be installed in metropolitan areas with adequate amount of reactive compensation, because the facts system has a small footprint and requires less outdoor equipment. the placement of the most important buses, in terms of voltage stability, are determined through the proposed methodology. furthermore, the system load condition corresponding to peak winter is applied in the case studies. as for the im load, the above-mentioned (in section 2) load model is adopted. in the simulation, ims are used to represent inductive loads. for fidvr events, it is assumed that im loads have a composition proportion of 30% in the dynamic analysis based on the statistical system [cit] . in this simulation, two case studies, solvable and unsolvable, are considered. the results of the two cases are described in detail in the following subsections."
"where variable(x) means that x is a variable in a non-specified domain. now, our next issue is to identify a proper form for the components that allows transforming constraints specified with the generic notation into some kind of constraints in a particular domain, and the other way round. in order to achieve this, we are developing a series of transducers. the difficulty in developping these is that they must be monotonic and continuous in the orderinginformation . because of the first-order structure of the constraints, we require that the transducers be generic in all the variables. to be generic in a variable v, means that if the transducer can produce the information d on input c, then it can also produce the information"
"fidvr is a phenomenon caused by the dynamic behavior of constant-torque im loads. when a motor stalls following a severe fault, the reactive power requirements might be 5 to 8 times higher than normal. the unexpected increase in the reactive power requirement may prevent voltage recovery in a local area power system. furthermore, when a fault occurs during peak load conditions, a highly inductive power system load may not be able to maintain the voltage at the accepted levels [cit] ."
"this section describes the results of voltage stability analysis in the korean power system. steady-state and dynamic studies were performed based on the proposed methodology. the korean power system has the following distinct features: (i) concentration of load demand in the metropolitan region; (ii) major generating plants located in nonmetropolitan regions; and (iii) environmental regulations and land for constructing new facilities, such as transmission lines. due to these geographical characteristics, six interface lines are operated to supply power to the metropolitan area from non-metropolitan areas. system operators in the korean power system are always concerned with the lack of a reactive power reserve in the metropolitan area for an interface flow [cit] . under these circumstances, when a serious fault occurs in interface lines, a region with a high proportion of im loads might significantly increase the var demand during voltage recovery. however, external reactive power cannot be sufficiently delivered to the im load-concentrated region through long-distance transmission lines. as a result, the system might experience stvs problems. unless immediate countermeasures are applied, this leads to a fidvr phenomenon. figure 10 shows a bus voltage profile without a countermeasure."
"in the next step, we employed qv analysis to determine the control actions necessary to overcome violations of the voltage criteria. the purpose of this analysis is to determine the region of reactive deficiency in terms of reactive power for the severe contingencies determined through pv analysis. additionally, qv analysis is used to identify the required amount of reactive power compensation. figure 5 represents the qv curve for determining the reactive power margin. from figure 5, it can be inferred that the reactive power margin can be calculated using the qv curve, which defines the amount of reactive power that can be consumed by the bus. when a fault occurs, the reactive power margin of the bus should be maintained at a level sufficient to meet the criteria. therefore, it is possible to calculate the reactive power margin on each bus for serious faults, so that the bus or area with a low reactive power margin can be determined as the reactive power vulnerable site. to this end, a vqvi margin vulnerability index can be used to identify vulnerable areas. vqvi considers the changes in the reactive power margin before and after the contingency and can be expressed as follows."
file s2 code for id 103001001e1eb700 instance (see also http://cs.jnu.edu.cn/sun/ontology). computer code in the rdf language is ontology annotation file of remote sensing data (catalog id 103001001e1eb700) instance in figure 2
"finally, this study attempted to determine the most severe contingency and voltage-vulnerable areas through static analysis and detailed time simulations for verifying the reduction in fidvr. usually, transient load characteristics are more sensitive to voltage than steady-state load characteristics. to address fidvr issues, in this study, the power system dynamic performance is evaluated against typical transient voltage criteria according to the wecc and nerc planning standards. the criteria imposed on the transient voltage dip are summarized in the following sections and will be used to illustrate the proposed reactive power planning approach. step 2: the bcpf is applied to select the unsolvable contingencies in the list."
"moreover, a property value may sometimes appear to be simple, but may actually be more complex. for example, the unit information of the spatial resolution for satellite imagery is meter, but in some cases such information is not explicitly given and omitted in contexts where it can be assumed that anyone accessing the property value will understand the unit information being used. however, this assumption is generally unsafe in the wider context of the imagery. one might give a resolution value in kilometer or degree, whilst others might assume that is in meter. in general, a comprehensive consideration should be given to the explicit representation of unit information."
"using the above results, a transient stability analysis was performed to verify the stvs. the objective of the dynamic simulation is to establish appropriate dynamic var supports to satisfy the voltage criteria. facts device placement determined based on the qv analysis is shown in table 2 . ten candidate buses were chosen to install the facts devices, which were installed in the order of ranking to observe whether the voltage was recovered or not. as the typical size of a facts device installed in the transmission level is 300 mvar, the dynamic var support source capacity limit is set at ±300 mvar. the post-fault voltage trajectories of the buses are shown in figure 13 . as shown in figure 12, the metropolitan region is largely divided into eastern, western, and northern areas. in addition, generators are aggregated in the western area. in substation 1300, active and reactive powers are supplied from 3150 c/c before faults (i.e., outages between 1300 and 3300, occur. when the fault occurs, the 1300 substation would be at the end of a load. for this reason, the 1300 substation might have problems of insufficient reactive power supply. thus, this area needs reactive power sources because plenty of power is supplied through the interface line in the eastern and southern areas. thus, it is concluded that facts devices should be installed at 1400, 1300, 1700, 2500, and 3300 to counter the fidvr phenomenon."
"nowadays, ontology has gained more and more acceptance as one of semantic technologies to solve the problem of heterogeneous knowledge sharing [cit] . many research efforts have been devoted to ontology modeling over the past decade [cit], and quite a few running systems based on manual ontologies have been developed [cit] . however, data is accumulating at an astounding rate with increasing computing power. many activities, for instance encoding an organism's dna [cit], collecting satellite data [cit], and conducting scientific experiments at the large hadron collider [cit], can create a staggering amount of data. the growth of these big data outstrips the capacities of current ontology engineering practices and tools. in bioinformatics, the semantic integration of big data has been identified as a new frontier [cit] . the same trend can also be observed in other scientific domains. for example, with a vast amount of geographical data becoming available from satellites, especially the recent opening of the landsat archive [cit], there comes an increasing demand for automatic semantic processing of remote sensing images (rsis) in a reasonable amount of time. up to now, reasoning from big data is challenging. as the winner of the semantic web challenge, williams provided the experimental results showing that reasoning over the billion triple dataset required 3712 processors from ibm ls21 blade servers and the computation time was 1314 seconds per processor [cit] . although this dataset contains 898,966,813 triples and the size of the combined dataset is around 17 gb, the amount of data obtained from satellite devices and open sources on the internet per day is much higher and beyond the capabilities of analyst to process the data with the help of ontology [cit] . novel tools and approaches are needed to address this problem that has arisen during the current period of rapid data and knowledge growth. now dna computing has become an active research area [cit] . dna-based parallel computing takes advantage of many different dna molecules to solve the np-complete problems in polynomial or even linear time, while exponentially increasing time is required in silicon-based computer. in this paper, a dna model is introduced for semantic fusion of the rsis. it utilizes dna computing and ontology technologies to enable the complete representation of the rsi's knowledge in linear time regardless of the amount of data obtained."
"often multiple geometric variables may be spatially related, but the nature of this relationship may not be known a priori. rather than representing these independently, it may be useful to encode their relationship with a parametrised function such that altering the parameters changes all geometric variables simultaneously. an additional benefit is that a small number of parameters may then represent a large number of variables, and consequently reduce the search space size. in this paper, we used chebyshev polynomials for encoding spatial relationships for one dimensional variables [cit] ."
"modern power systems are becoming increasingly complex and dynamic owing to an increased use of induction motor (im) loads. the increasing usage of these loads with low inertia characteristics make power systems more vulnerable to short-term voltage stability (stvs). such problems might be aggravated by the growing use of single-phase heating, ventilation, and air conditioner (hvac) units with high-efficiency and low-inertia motors. when a fault occurs on the load side, where there is a high proportion of im loads, a large power vacancy is induced and can cause a widespread voltage to drop. additionally, the demand for reactive power increases significantly at large im loads. under these conditions, the local dynamic volts-ampere-reactive (var) reserves are insufficient and reactive power support cannot be delivered; therefore, the power system may experience fault induced delayed voltage recovery (fidvr) or voltage instability [cit] ."
"finally, this study attempted to determine the most severe contingency and voltage-vulnerable areas through static analysis and detailed time simulations for verifying the reduction in fidvr. usually, transient load characteristics are more sensitive to voltage than steady-state load characteristics. to address fidvr issues, in this study, the power system dynamic performance is evaluated against typical transient voltage criteria according to the wecc and nerc planning standards. the criteria imposed on the transient voltage dip are summarized in the following sections and will be used to illustrate the proposed reactive power planning approach. as shown in figure 8, the post-fault transient voltage performance is expected to meet the following criteria."
"since the volume of electronic data expands rapidly, it is important to choose the optimal computer architecture for converting big data set. conversion solutions range from clusterbased computing [cit] to cloud-based computing [cit] . considering the cost-effective way to achieve a supercomputer performance, we use the cluster computing. all the conversion experiments in this paper were carried out in the hpc-jnu cluster system. the description about the hpc-jnu is provided in the materials and methods section. the sequential and parallel codes in c language are provided in file s3 and file s4 respectively. [cit] dataset (http://km.aifb. kit.edu/projects/ [cit] /rest/). this dataset is encoded in nquads format [cit] and includes three data files that range in size from 409.99 mb to 2.69 gb. figure 5 shows the conversion results of 4.34 gb source dataset in the hpc-jnu cluster system. as an explanatory scripting language, the perl language has poor io disk performance. the result of the parallel method shows the best performance although the user of the cluster system has a maximum limit of 80 cores."
"first of all, in order to assess the vulnerable areas, a contingency list was derived using pv analysis. in the wecc criteria, the voltage stability analysis included in the pv and qv analysis. through the using concept, this paper reports a study of voltage stability limit in the metropolitan region and the neighboring region in the korean power system [cit] . in the pv analysis associated with this study, the minimum active power margin for performance level a shall be greater than 5% under the contingency. the credible contingency sets to be applied in this simulation are all double circuit outages of 345 kv and 765 kv lines. among them, the results of the six worst contingencies are shown in table 1 . from table 1, it can be seen that the voltage stability limit of % margin corresponding to no. 2 in the contingency list corresponds to performance level b; therefore, it might be regarded as the most serious failure. the remaining situations correspond to performance level a; the % margin of contingency no. 3 was the lowest. therefore, no. 3 was considered as a solvable case and simulated. further, reactive power reserve calculation for no. 3 was performed based on the vqvi margin vulnerability index. the results are shown in table 2 . these results are related to the capability of maintaining voltage stability on individual load buses and they can be used to identify vulnerable areas in the system. from the data in table 2, one can infer that the northern metropolitan area includes vulnerable sites. in no. 3 outage, the critical contingencies that influence the stability of the northern area are the route lines between 1300 and 3300. figure 12 shows the shows the diagram of the metropolitan region. route lines between 1300 and 3300. figure 12 shows the shows the diagram of the metropolitan region. as shown in figure 12, the metropolitan region is largely divided into eastern, western, and northern areas. in addition, generators are aggregated in the western area. in substation 1300, active and reactive powers are supplied from 3150 c/c before faults (i.e., outages between 1300 and 3300, occur. when the fault occurs, the 1300 substation would be at the end of a load. for this reason, the 1300 substation might have problems of insufficient reactive power supply. thus, this area needs reactive power sources because plenty of power is supplied through the interface line in the eastern and southern areas. thus, it is concluded that facts devices should be installed at 1400, 1300,1700, 2500, and 3300 to counter the fidvr phenomenon."
"1. input(t) 2. t r pre-separate(t, v start ) 3. t r post-separate(t, v end ) 4. t r length-separate(t, 240) 5. t r sub-separate(t, tgcatgca) 6. detect(t)."
"at a fixed location t j may be varied by changing the relevant parameters, but the basis function representation ensures that changes to a single coefficient yields correlated changes to all the variables."
"load buses with large participation factor values have more influence in contributing to the voltage instability. consequently, these buses can be regarded as best candidate location for determining the injection to enhance system voltage stability."
"for this final test case, we have constructed a simple heat exchanger with three rows of tubes. the design variables include altering the diameter of the tubes, the position of the tubes in the streamwise direction, and the number of tubes per row. to alter these variables, the decision vector contains the parameters of chebyshev polynomials (for number of tubes in a row, and radii of the tubes), and monotonic beta functions (for the position of the tubes) as described in sections 3.2 and 3.3. the cost functions describing the heat transfer and pressure drop across the heat exchanger are defined as follows :"
"as large quantities of im loads exert a significant influence on the stability of the power systems, the requirements for reactive power support have become important. reactive power plays a major role in the transmission of power systems. during im reacceleration after a fault, when high current is drawn, a high reactive power support is required for a few seconds to provide a fast response [cit] . facts technologies are available in different forms, such as svc, statcom, thyristor-controlled series capacitor (tcsc), and unified power flow controllers (upfc). among them, svc and statcom are categorized as dynamic var compensation resources that rapidly inject or absorb vars to support power system voltage immediately after system disturbances. the statcom technology increases voltage stability by providing dynamic control and compensation of the system voltage. the advantage of statcom is that its compensating current is not dependent on the system in figure 2, the impedance and load tap changer settings were based on typical values in the available data. for the im loads, two different motor models were used. one of the motor loads is a spim, which has a low inertia and responds instantaneously to voltage changes. another motor load is a tpim; they are commonly used in industry to drive processes. the zip load represents static loads (e.g., constant impedance, constant current, and constant power loads) [cit] . the areas of particular concern in this study (metropolitan area of the korean peninsula) are primarily residential and commercial. the primary sources for power demand consist of the tpim (commercial class) and spim (residential class). in order to better represent the power consumption characteristics of im loads, it is necessary to take advantage of the power consumption characteristics under the most severe peak conditions, in terms of voltage stability."
"flow separation, recirculation, and reattachment are common phenomena observed in many engineering applications, and are usually undesirable features within a product's design. based on the experimental set up by pitz and daily [cit], this first case features a so-called 'backward-facing step', which serves as a simple prototype for simulating the above flow phenomena. in this geometry, the flow separates at the edge of the step, creating a recirculation zone, the flow then reattaches at some distance beyond the step. the flow structure for the base case can be seen in figure 3 (bottom-left) . traditionally, this case has featured as a benchmark case for testing the accuracy of cfd methodologies and thus has been the focus of much experimental and computational investigation. furthermore, this has also been used as a test case for adjoint (gradient descent) methods of optimisation (see for example [cit] )."
"as the hereditary basis of every living organism, dna has an ability to store and process information. this information is determined by the sequence of four distinct bases (a, c, g, t). an oligonucleotide is a short, single-stranded dna molecule, and the complementary base pairing enables hybridization into a doublestranded polymer. these features of dna have inspired the idea of dna computing [cit] . dna computing, known also under the name of molecular computing, has great advantages of in vivo computing and in vitro computing, such as massive parallelism, extraordinary information density and exceptional energy efficiency. in contrast to traditional silicon-based technology, dna computing has the natural potential of semantic fusion and reasoning for big data."
"a key factor that results in fidvr in power systems is the nature of the im loads in the transmission and distribution systems. the performance characteristics of these loads are changing due to technological advances. a large three-phase induction motor (tpim) and small single-phase induction motor (spim) are connected directly to the electrical network. these types of im loads are expected to be a major component of the aggregate load due to economic reasons. figure 2 shows the structure of the load model used in this study. in figure 2, the impedance and load tap changer settings were based on typical values in the available data. for the im loads, two different motor models were used. one of the motor loads is a spim, which has a low inertia and responds instantaneously to voltage changes. another motor load is a tpim; they are commonly used in industry to drive processes. the zip load represents static loads (e.g., constant impedance, constant current, and constant power loads) [cit] . the areas of particular concern in this study (metropolitan area of the korean peninsula) are primarily residential and commercial. the primary sources for power demand consist of the tpim (commercial class) and spim (residential class). in order to better represent the power consumption characteristics of im loads, it is necessary to take advantage of the power consumption characteristics under the most severe peak conditions, in terms of voltage stability."
"the proposed reactive power compensation strategy has also been applied to no.2 on the contingency list in performance level b. as mentioned earlier, contingency no.2 represents the most severe outage. for performance level b, corresponding to the bus section representing contingency, the minimum margin should be greater than 2.5%. this means that a 2.5% margin on the pv analysis is required for the worst-case performance level b disturbance cases. the 2.5% is based on the load level measured at the voltage collapse point. therefore, the proposed method based on bcpf was applied to verify the unsolvable case. to identify effective buses for reactive power compensation, a sensitivity analysis was then performed at the critical point of the yv curve using the participation factor calculated for the load buses. the results of sensitivity analysis are shown in table 3 . from table 3, one can observe that the eastern metropolitan area consists of vulnerable sites. in the korean power system, when the most severe 765 kv transmission line fault occurs, power flow takes a roundabout way to the nearby lines. when a large amount of power is concentrated in the power transmission line, voltage instability occurs. in addition, the dynamic nature of im loads is the same as imposing a heavy demand for fast-responding reactive power resources under low-voltage situations. this leads to secondary effects, such as fidvr, which further exacerbates the problem. the transient voltage dip and slow voltage recovery issues might be addressed by fast-responding dynamic var resources. candidate buses are considered for selecting the locations of reactive power compensation in order to finalize the space and geographical location to install facts devices. for dynamic assessments, the above-mentioned performance criteria should be followed. figure 14 shows the voltage responses of buses under contingency conditions after facts installation. dynamic var resources. candidate buses are considered for selecting the locations of reactive power compensation in order to finalize the space and geographical location to install facts devices. for dynamic assessments, the above-mentioned performance criteria should be followed. figure 14 shows the voltage responses of buses under contingency conditions after facts installation. it can be seen in figure 14 that the time domain simulations are run by applying a fault. to address the delayed voltage recovery caused by im stalling under post-fault voltage trajectories, the voltage criteria are met using five facts devices. when four or less facts devices are installed, the fidvr phenomenon cannot be mitigated due to insufficient var support. therefore, it was concluded that the facts devices should be installed at 2700, 4500, 2500, 4700, and 4650 to mitigate the fidvr phenomenon."
"this section describes the results of voltage stability analysis in the korean power system. steady-state and dynamic studies were performed based on the proposed methodology. the korean power system has the following distinct features: (i) concentration of load demand in the metropolitan region; (ii) major generating plants located in nonmetropolitan regions; and (iii) environmental regulations and land for constructing new facilities, such as transmission lines. due to these geographical characteristics, six interface lines are operated to supply power to the metropolitan area from non-metropolitan areas. system operators in the korean power system are always concerned with the lack of a reactive power reserve in the metropolitan area for an interface flow [cit] . under these circumstances, when a serious fault occurs in interface lines, a region with a high proportion of im loads might significantly increase the var demand during voltage recovery. however, external reactive power cannot be sufficiently delivered to the im load-concentrated region through long-distance transmission lines. as a result, the system might experience stvs problems. unless immediate countermeasures are applied, this leads to a fidvr phenomenon. figure 10 shows a bus voltage profile without a countermeasure."
"output: read out the property strings (if any). as shown in figure 8, we can obtain the result property string by using the semantic fusion method based dna. it is consistent with the semantic properties in figure 6b . abstract representation of semantic fusion"
"after the fault occurs, the voltages of the load buses drop immediately. without a countermeasure scheme, the bus voltage cannot be recovered to its normal value after fault clearance. from figure 10, the following observations can be made:"
"coordinating council (wecc) [cit] . in the wecc, criteria are presented as a procedure available for determining voltage stability limit under a fixed load condition. to consider uncertainty, the required margins for performance levels are specified; in this paper, the minimum required margins on the wecc voltage stability criteria are used. in addition, a procedure for evaluating the secure operating limits of voltage stability is established. figure 4 shows the pv curves for margin evaluation at each performance level. as shown in figure 4, based on the wecc criteria, two cases of voltage stability analysis are considered. one can notice that the maximum operating point on the p axis must have a mw margin equal to or greater than the values measured from the nose point of the pv curve for each performance level. in this study, it was fixed that the minimum power margin for performance level a, which is the contingency, must be greater than 5%. for performance level b, the minimum margin should be greater than 2.5% [cit] . it is to be noted that the performance level should not cause voltage collapse within the limits of the study."
"in the next step, we employed qv analysis to determine the control actions necessary to overcome violations of the voltage criteria. the purpose of this analysis is to determine the region of reactive deficiency in terms of reactive power for the severe contingencies determined through pv analysis. additionally, qv analysis is used to identify the required amount of reactive power compensation. figure 5 represents the qv curve for determining the reactive power margin. as shown in figure 4, based on the wecc criteria, two cases of voltage stability analysis are considered. one can notice that the maximum operating point on the p axis must have a mw margin equal to or greater than the values measured from the nose point of the pv curve for each performance level. in this study, it was fixed that the minimum power margin for performance level a, which is the contingency, must be greater than 5%. for performance level b, the minimum margin should be greater than 2.5% [cit] . it is to be noted that the performance level should not cause voltage collapse within the limits of the study."
"a cross-flow tube-bundle heat exchanger has a wide range of applications in many fields, such as the chemical, food and nuclear industries, and hvac (heating, ventilation and air conditioning) sectors, to name a few. generally, this type of heat exchanger contains many rows of tubes oriented in a direction perpendicular to the flow, as shown in figure 5 . the tubes may be arranged in many configurations in order to obtain the greatest heat transfer between the two media. the transfer of heat between the tubes and the main flow occurs through the tube walls and will be maximised by increasing the surface area of contact between the flow and the walls. a detrimental effect of this may be that the static pressure across the tube configuration increases, requiring greater energy to push the flow through the heat exchanger. overall, this potentially results in a conflicting pair of objectives for heat exchanger design."
"where p t and q t represent the injection of the active and reactive power, respectively. meanwhile, e k is chosen to remove the ill-conditioned problem from the singularity. further, npv is the number of voltage-controlled buses. as shown in equation (5), ∂p t /∂y, ∂q t /∂y, and e k are augmented from the original power flow jacobian matrix. using these characteristics, the bcpf can determine whether the cause of the divergence problem is unsolvable. the following is a sensitivity analysis procedure for applying the bcpf application to obtain a solution."
"this paper presents a new framework on the reactive power compensation strategy designed based on voltage stability analysis. the framework adopts the conventional analysis of active and reactive power margins to decide the critical contingencies and the areas of reactive power deficiency. this paper adopts not only steady-state analysis but also dynamic time-domain simulations for the adequate amount of dynamic reactive power support. there also might be a limitation of only covering solvable cases using the conventional analysis, but this paper adopts the sensitivity analysis for unsolvable cases using the branch parameterized continuation power flow. for adequate compensation strategies, vulnerable buses as compensation locations must be carefully chosen in terms of voltage stability. the case study results demonstrated that the compensation level decided by the proposed method could mitigate the fidvr caused by im loads. in addition, the proposed method might be applied for determining the optimal number and placement of facts devices to resolve the fidvr. by the performance assessment, the alleviating impacts of dynamic var figure 15 shows that the facts control is reasonably performed to help maintain proper voltage regulation in metropolitan areas, especially for the critical contingency. as indicated by the above results, the information of bus participations can help determine the placements of facts, and the branch participations can be used to identify candidate lines for series compensations. this has been demonstrated in actual system studies, as in the discussion."
"where *, *, and *, are the admittances of the removed branch. as the continuation parameter increases, the system gradually closes to the power system with branch outages from precontingency systems. when is zero, the original power flow equations are obtained. when reaches one, the new power flow equations that represent the network with the branch totally removed is obtained. this method comprises a predictor and a corrector, and the bcpf uses an augmented jacobian matrix. the augmented jacobian used in the bcpf can be expressed as"
"addressing these issues, the aim of this paper is to present a test problem suite 1 for computationally expensive problems with the following features :"
"in case 1, when the fault occurs, it is seen that reactive power cannot be supplied to the northern metropolitan area from 3150 cc. for this reason, it is effective to inject facts directly into 1400. facts should also be installed on buses 1300 and 1700 that supply power to the metropolitan region. in case 2, it is seen that facts injection directly on buses 4700 and 4650 is effective. in addition, it could be confirmed that it is effective to inject facts at 2700, 4500, and 2500 to increase the supply of reactive power in the eastern area. it is seen that the system voltages can be successfully maintained against this voltage instability scenario by five facts placed by the proposed methodology. figure 15 shows the facts locations for improved voltage stability in a metropolitan region for the two cases."
"by the performance assessment, the alleviating impacts of dynamic var compensation on the fidvr phenomenon can be observed. consequently, the dynamic support of reactive power could enhance the stvs. the contributions of this paper can be summarized as follows:"
"-an accurate mathematical model is proposed for simulating the phenomenon occurred in the pemfc stacks; -studying the impact of changing the operating conditions such as the cell temperature and the pressures of reactants (hydrogen and oxygen) at the inlet channels on the performance of fuel cells; -ten chaotic functions are applied to develop chho techniques and the best technique is determined based on a comprehensive comparison among them; -four different commercial pemfc stacks are used to validate the proposed chho; the rest of this work is subdivided as follows. a simple mathematical formulation of pemfc as well as the objective function are introduced in section ii. the proposed chho techniques and conventional hho techniques are reported in section iii. the obtained results as well as the performance under various operating scenarios are presented and analyzed in section iv. finally, the conclusions are presented in section v."
the energy mentioned in (3) is the electrical energy produced due to the flow of electrons of hydrogen gas from the anode electrode to the cathode electrode through an external load.
"with the continuous increase in the electric energy demand and the shortage in the reserves of fossil fuels, the need for a clean source of energy becomes necessary not only for small power applications but also in large industrial applications. besides the most popular renewable energy sources (solar and wind), fuel cells are used in many applications and developed very fast to be a good competitor with these energy resources. in the past decades, fuel cells have attracted the attention of many researchers and manufacturers. fuel cells directly convert the chemical energy obtained from the the associate editor coordinating the review of this manuscript and approving it for publication was fabio massaro . chemical reactions between hydrogen and oxygen or natural air in the presence of a catalyst into electrical energy [cit] . based on the electrolyte type, there are many types of fuel cells. among these cells, the proton exchange membrane fuel cell (pemfc) is the most common type [cit] . pemfc received high attention from researchers thanks to its highpower density under low temperature of operation and its fast response against electrodynamic processes [cit] ."
"the institutional review board at northwestern university approved all procedures and the protocol for this study. seven healthy subjects were recruited for this study and bipolar fine-wire emg electrodes were inserted into muscles of their right forearm. the muscle compartments associated with the index and middle fingers were targeted in extensor digitorum communis (edc1, edc2), flexor digitorum profundus (fdp1, fdp2), and flexor digitorum superficialis (fds1, fds2). the subjects were instructed to make appropriate test contractions during electrode insertion and the emg signal was played through a speaker to locate the desired muscle and compartment. a constant current stimulator (digitimer ltd. model ds7a, hertfordshire, england) was used to verify electrode placement after insertion was complete."
"the challenge to the wimax receiver chain designer is the wide dynamic range of received signal levels due to a highly variable transmission path. the wimax receiver's ability to effectively detect signals from a variable transmission path is critical to ensure system efficiency and data accuracy. because of wimax's unique requirements, using a low noise amplifier (lna) at the rf front end of a wimax receiver is the best way to reduce the noise. a lna is a simpler, space saving and more efficient solution which allows the receiver chain to have variable gain, low current consumption and excellent linearity."
both the raw emg data and filtered rms data were recorded during the experiment. the mean noise from each channel was subtracted from the filtered rms data and the middle two seconds of each trial was used for analysis. the first and last half seconds were removed from each trial to reduce transient activity and filtering artifacts. an ensemble average was calculated for each of the six channels using the 10 trials of each target muscle compartment.
"subjects wore a ball splint once the electrodes locations were verified. the splint served to standardize hand posture for all subjects, allowed for isometric contractions, and minimized confounding motions from the wrist and digits. a maximum voluntary contraction (mvc) was recorded for each muscle compartment as it was contracted to its fullest. each channel was normalized to its mvc signal in order to establish a reference for the activity level of each muscle compartment. the labview program displayed six vertical bars that represented the normalized real-time rms signal for each electrode. the top and bottom of each bar corresponded to the rms value for mvc and no activity of each muscle, respectively."
"a cross-correlation analysis was performed on the filtered emg signals to confirm that measured signals were accurate representations of individual muscle compartment activity and not crosstalk between electrodes. for this analysis, finewire intramuscular emg signals were recorded from edc1and edc2, which provided two sites in adjacent compartments of a single muscle, as well as a site in a neighboring muscle, fdp1. figure 2 shows example activity detected in both fdp1 and edc1 (top and middle signals, respectively) but minimal activity in edc2 (bottom signal). cross-correlation values less than 0.3 suggest that the signals contain little crosstalk [cit] . the maximum crosscorrelation found between any pair of the tested signals was less than 0.1."
"in this case, the values of hydrogen and oxygen pressure at the input channels are kept constants at the values reported in the datasheet of 250w pemfc stack. fig. 7a-d show the polarization characteristics of the 250w fc stack and bcs-500w pemfc at 323k, 343k, and 363k. the impact of reactants pressure (h 2 and o 2 ) at the inputs of fuel cell stack on the performance of fc stack is demonstrated while the cell temperature is maintained constant at the value given in the datasheet of the manufacturer. the i-v, i-p polar-ization curves of sr-12 500w pefc and temasek 1kw stack at pressures of 1/0.2075bar, 2.5/1.5bar and 5/3bar while maintaining the stack temperature at 343.15k are shown in figs. 7 and 8, the reader can notice that the pressure ratio (p h 2 /p o2 ) at the supply inlet as well as the cell temperature play a significant role in the operation of pemfc stack. when the input pressures of reactants and/or the cell temperature are increased, the output terminal voltage and the stack output power will be raised."
"the conventional hho and proposed chho techniques have programmed by matlab [cit] ) using personal intel core i3-m370 cpu@2.40ghz with 4.00mb ram laptop. the optimal estimated parameters obtained by the proposed chho techniques have been validated using measured data of a commercial pemfc stack provided in literature. the proposed chho with ten different chaotic functions introduced in the previous section have been validated via several operating scenarios. in this study, the following control variables have been adopted: the maximum number of iterations equals 500 iterations and the number of search agents equals 30 search agents. in order to overcome the randomness of proposed optimization technique and examine the goodness of different chaotic functions, 50 independent executions have been carried out under each chaotic function as well as the conventional hho and the best solution is taken as the minimum value of fitness function over the 50 runs."
"in equation above, the term and is an added input impedance introduced by the source inductor, and the added resistive and reactive component both help improve the performance of the lna. normally, ls should be a small inductor optimized according to the zin. based on the analysis above, small microstrip lines can be placed in the source based to act as the added input impedance. to further improve the lna, via-hole (via) are introduced on both of the source leads to work as a small inductor. to further improve the lna and feedback amplifier, a via-hole (via) are added in the designs. the via are added after the microstrip lines in the source base of the transistor and the via hole diameter, d value are tuned to optimize the gain and noise figure."
"muscle activity was measured using a delsys bagnoli-16 system (delsys inc., boston, ma) connected to a pc running labview and sampling at 3,000 hz (national instruments inc., dallas, tx). a labview program was written to filter the emg signals (4 th order butterworth filters to band-pass 30-450 hz and notch 59-61 hz), compute the rms using a 200 msec window, and record the raw and processed data."
"the i-v and i-p polarization characteristics of the bcs-500w fc, the sr-12 500w pemfc, and the temasek 1kw pemfc stacks based on the estimated parameters obtained by chho4 method are shown in fig. 6a-f . from this figure, it can be observed a good matching between the estimated and measured values."
"subjects were instructed to activate a single muscle compartment (instructed muscle) to 20% mvc without activating other muscle compartments. the user was presented with a green 'target zone' in each bar. the target activation zone for the instructed muscle compartment was centered at 20%±2.5% mvc and target zones for the noninstructed compartments spanned from 0-5% mvc. this tested the subjects' ability to sustain a controlled level of activity in one compartment while maintaining the others in a non-active state ( figure 1 ). subjects were given as much time as they desired to practice each task before data was collected, with most subjects feeling comfortable with their performance after 5-10 minutes. each trial lasted three seconds and commenced after the subject's targeted muscle activity was in the 202.5% mvc green zone. the order of instructed target muscles was randomized for each subject. there were 10 trials for each of the six muscle compartments for a total of 60 trials per subject. several trails were also recorded at rest for background noise measurements. fig. 1 . experiment display. six vertical bars represent real-time normalized activity for each muscle compartment. the height of each bar depicts 100% mvc for that muscle compartment and the bottom of the bar is 0% activity. target zones are shown in green for each channel. one compartment at a time was tested in its ability to sustain isolated activity, and subjects attempted to activate to 20% mvc."
"recently, the performance of different metaheuristic optimization techniques, namely, gwo, ga, butterfly optimization algorithm (boa), harmony search (hs), krill-herd algorithm (kha), artificial bee colony (abc) and firefly algorithm (fa), has been improved based on theory of chaotic [cit] . replacing the random initial variables of optimization technique with the chaotic variables is the main principle operation of chaotic theory [cit] ."
"the middle finger extensor compartment, edc2, was unable to activate without significant co-activity from edc1 ( figure 4, second column from left, top row). however, when edc1 was the target muscle, subjects were able to activate it without co-activity from other muscles, specifically edc2 (figure 4, left most column, second row). this means that, on average, subjects could extend their index finger alone but when they tried to extend their middle finger they also extended the index finger. for this reason edc2 is not recommended as a control site for direct myocontrol of a prosthetic hand."
"from this project, the best design for lna is the single stage lna with feedback amplifier. in the future, the designs will be fabricated and measured. the fabricated results will be compared to the simulation results for analysis. the fabrication results are important for this project commercialized value for future used."
"each graph shows the median and interquartile ranges of normalized activity of the six muscle compartments across all trials for the target muscle (shaded). each bar represents the activity for a muscle compartment normalized to its mvc value. fig. 3 . examples of a well-isolated (edc1) and not well-isolated (fdp2) target muscle activations, a and b respectively. the target muscle compartment (shaded) maintained 20% target activity level and even though subjects attempted to minimize all other muscle activity, varying amounts of co-activations were measured. normalized median activity for each muscle compartment is shown and error bars represent the inter-quartile range."
"where, e o.c represents the open-circuit voltage of cell, v act is the activation overpotential per cell, v ohm is ohmic resistive voltage drop per cell due to the resistance of electrons conduction through the externally connected load and the resistance which the protons face through their motion in the electrolyte membrane as well v con is the concentration overpotential per cell. [cit], have adopted a fuel cell's electrochemical model. when the number of cells n cells of identical fuel cells are connected in series to get a high value of voltage, the total output voltage of stack calculated as:"
"to design a lna, the first step is to do a raw device testing. the testing is made to the heart of the lna which is the transistor. the transistor will give the lna a high gain and a low noise figure. the test is made to check the transistor stability and gain by calculation. the calculations are done only for the center frequency"
"moreover, in this subsection, various cases of stack temperature and reactants' pressure at inlet channels of the pemfc stack are studied to validate the accuracy of proposed chho4 technique. based on the estimated parameters in the previous subsection, the polarization characteristics of 250w pemfc stack under different temperatures are demonstrated."
"according to nyquist theory, the noise from any impedance is determined by its resistive component [cit] and an ideal lossless element will not impact the nfmin if it is applied as the feedback network [cit] . in figure 1, the zin can be shown as:"
"it was interesting to note which muscle compartments co- activated with edc2 and fdp2 and not just that there was co-activity. extensor digitorum inserts just distal to the metacarpophalangeal (mcp) joint in the fingers and primarily causes extension in that joint. extending your fingers against load is not a common task, and it is even less common to extend the middle finger by itself. the index finger is used more frequently by itself so it stands to reason that we observed isolated activations of edc1 but not of edc2. also of note was the large amount of co-activity from edc1 during attempted isolated activation of edc2."
"wimax can be used for wireless networking in much the same way as the more common wi-fi protocol. wimax is a second-generation protocol that allows for more efficient bandwidth use, interference avoidance, and is intended to allow higher data rates over longer distances."
"the cross-correlation analysis performed on the data validated that the emg measurements of each electrode were representative of individual muscle compartments and were not corrupted by signal crosstalk. this was an important distinction to make because electrodes were in adjacent muscle compartments separated by only a few centimeters. due to the results of this analysis, any simultaneous activity measured during these experiments are believed to be neurological co-activations of muscle compartments and not signal crosstalk between electrodes."
"where, β is a constant value set to 1.5, µ, and υ are random values between [cit] . finally, the updating position of hawks is calculated as:"
"a few elements such as capacitors and resistor are added to the design which has various functions to help improving the lna. without these elements, the lna will not function correctly and have a very high noise figure and very low gains. the values and function of each component are as follows:"
"where, t fc denotes the operating absolute temperature of cell in kelvin; p h 2 and p o2 are the partial pressures of hydrogen and oxygen at the input channels of the fuel cell stack (atm), respectively. when the inputs to the pemfc stack are hydrogen and natural air during its operation, the partial pressure of oxygen p o2 can be calculated as follows [cit] :"
"when the inputs of fuel cell stack are hydrogen and pure oxygen, the partial pressure of oxygen. p o2 will be calculated as follows [cit] :"
"wimax is expected to offer initially up to about 40 mbps capacity per wireless channel for both fixed and portable applications, wimax can support hundreds of businesses with t-1 speed connectivity and thousands of residences with dsl speed connectivity depending on the particular technical configuration chosen. wimax are build to support voice and video as well as internet data."
"a lna is design for wimax application. wimax has two type of frequency spectrum, the licensed and unlicensed band. for licensed band, the frequencies that are used are 2.3 ghz, 2.5 ghz and 3.5 ghz and for unlicensed band the frequency used is 5.5 ghz. this project will design a lna for frequency range 3.3 ghz to 3.8 ghz. the reason this frequency is used because wimax in most of the world use frequency band 3.5 ghz."
"for input and output matching of the component, microstrip stub element matching is used. the length and distance of the stub matching are found by calculation. the gain and noise figure are affected when the stub matching element is inserted in the design. the calculations done are using the center frequency which is 3.5 ghz. the distance and length of the stubs are tuned to optimize the devices to get satisfactory gain and noise figure."
both the design met the lna specifications that have been made before the design started. the design of single stage lna with feedback amplifier technique is chosen in this project lna design. the design has the highest gain and lowest noise figure compare to single stage lna with balance amplifier. the design has a nominal noise figure of 1.02db and gain of 12db.
"where, n cells represents the number of cells connected in series, and v cell is the output voltage of each single fuel cell, which calculated using (4)."
"where, x is a vector of unknown parameters, n represents the number of measured data, i denotes an iteration counter, v meas defines the experimentally measured voltage of pemfc and v cal represents the estimated pemfc voltage, according to the following inequality operating constraints:"
"all six muscle compartments tested were able to achieve and hold the target activation level of 20  2.5% mvc. however, co-activity was frequently observed from the nontargeted muscle compartments. the relative amounts of coactivity varied greatly depending on the subject and the tested muscle compartment. figure 3 shows an example of a well-isolated activation of the target muscle, edc1, ( figure 3a ) and an example of a target muscle that was not capable of isolated activity, fdp2 ( figure 3b )."
"a single stage lna are able to design by adding an input and output matching and dc bias in the transistor. to further improve the design, two techniques of broadband amplifier are applied in the design. from two of the design, the feedback amplifier design gives the best performance with gain at 12 db and noise figure at 1.02 db. the gain and noise figure that are achieve in the design are following the design specification which is gain more than 10 db and noise figure lower than 2 db."
"in this paper, novel chho techniques based on ten chaotic functions are proposed and applied for extracting the optimal values of the unknown parameters of different pemfc stacks. however, the main contribution of this paper could be summarized in the following points:"
"the rf input matching always plays a key role in an lna design. it is not only a way to achieve a low nf; it is also the way to obtain higher gain and better input return loss. for the lna, a stub element matching is used in both the input and output of the lna. to design the stub matching, the length of the stub, l and the distance of the stub from the load, d need to be found. for this project, to design a single stage lna, a passive dc biasing will be applied at the transistor atf-54143. it will be accomplished by the use of voltage divider consisting of r1 and r2. the voltage for the divider is derived from the drain voltage which provides a form of voltage feedback through the use of r3 to help keep drain current constant. the values of r1, r2 and r3 can be found as follows:"
"from the mentioned previous equations, it is clearly noticed that the fundamental operation of pemfc basically depends on seven unknown variable parameters. to match well between the model outputs and the experimentally measured data of pemfc, this paper aims to solve this optimization problem by developing chho technique. however, the sum of squared errors (sse) between the experimentally measured voltage of pemfc and the calculated stack output voltage is defined as the objective function (of) [cit] ."
"the relative mean activity (rma) was calculated for each muscle in each trial. this was done by dividing the mean activity of each muscle by the mean activity of the target muscle for that trial. figure 4 shows the rma of the six muscle compartments (vertical axis) for a given target muscle compartment (horizontal axis). the diagonal displays the rma of a target muscle with respect to itself, giving a value of 100% (dark red). if a muscle activated completely in isolation then the off-diagonal, non-targeted muscles would be at 0% rma (blue). edc2 and fdp2 showed noticeable co-activity when they were the target muscles. when edc2 was the target muscle (second column from left) there was 85% relative co-activity from edc1. when fdp2 was the target muscle (fourth column from left) there was roughly 50% relative co-activity seen in edc1, fdp1, and fds2."
the transistor that is used in designing the lna is atf-54143 from avago technology. the transistor is chosen because it is recommended by avago technology to be used in designing a lna for wimax application and it satisfies the lna specification which is the maximum noise is 2 db and minimum gain is 10 db. the noise figure and gain are preferably to be lower and higher respectively.
"to arrive at a balance between noise figure, gain and linearity, the device drain source current (ids) was chosen to be 60 ma with a 3 v drain-to-source voltage (vds); the gate-to-source voltage was 0.59 v (vgs). the s-parameter for frequency 3. from rollet's condition, the value of k is found to be greater than 1. the transistor is in unconditionally stable so there is no need to draw the stability circle for the transistor."
"accordingly, it is recommended to complete the simulation results based on the 4 th chaotic function. the convergence characteristics of proposed chho techniques as well as the conventional hho are shown in fig. 3 . the current-voltage (i-v) and current-power (i-p) polarization curves obtained by chho based on the 4 th chaotic function compared with the experimentally measured data of 250w pemfc stack are shown in fig. 4a-b . from this figure, it can be observed that the computed polarization curves give a good agreement with the measured ones."
"to complete the single stage lna, dc biasing for the lna are done using a passive dc bias which use a voltage divider, r1 and r2. then the design is added with capacitors and resistor which functions and values are discussed. after dc bias is implemented in the design, the lna is degraded in aspect of its gain and noise figure. this is normal because the added element in dc bias affected the gain and noise figure. the single stage lna satisfies the specification of this project which has nominal gain and noise figure of 12 db and 1.02 db respectively."
"the results are carried out based on the measured data of the polarization curve of four different commercial fuel cell stacks, namely 250 w pemfc stack, bcs-500w pemfc, sr-12 500w pemfc, and temasek 1kw pemfc which have the specifications provided in table 2 . the measured data of these fuel cell stacks [cit], are used as the input data for the optimization algorithm. table 3 presents the search limits of the unknown parameters of pemfc stacks."
"wimax could potentially be deployed in a variety of spectrum bands: 2.3 ghz, 2.5 ghz, and 3.5 ghz for licensed band and 5.8 ghz for unlicensed band. telecommunication companies are unlikely to use the unlicensed spectrum widely other than for backhaul, since they do not own and control the spectrum."
"where, ρ m denotes the specific membrane resistance for the flow of electron ( cm), l is the membrane thickness (cm). the empirical formula for ρ m can be expressed as follows [cit] :"
"to validate the robustness of proposed chho for the parameters identification of pemfc stacks, a statistical analysis of the minimum values of fitness function (sse) over 50 individual runs is presented. this analysis is demonstrated to give a clear assessment of ten chaotic functions and select the most accurate one. the comparisons between the chho based on different chaotic functions as well as the conventional hho are carried out with respect to many metrics, mainly the best and worst values of sse, mean value of sse, median, standard deviation (sd), relative error (re), root mean square error (rmse), mean absolute error (mae) and efficiency. the mathematical expressions of these metrics are presented in (33) to (37):"
"signal amplification is a fundamental function in all wireless communication systems. amplifiers in the receiving chain that are closest to the antenna receive a weak electric signal. simultaneously, strong interfering signals may be present. hence, these low noise amplifiers mainly determine the system noise figure and intermodulation behavior of the overall receiver [cit] ."
"where, p a represents the input channel pressure at the anode electrode (atm), rh a denotes the relative humidity of water vapor in the side of the anode."
"flexor digitorum profundus tendons run across all the joints in the fingers and inserts on the palmar side of the distal phalanx. when subjects attempted to activate this muscle in the middle finger we observed co-activity in the other flexor muscle for that finger (fds2) as well as an extensor and flexor (edc1 and fdp1) in the index finger. this implies that while both flexor muscles are working to essentially curl the middle finger, the index finger has \"stiffened\" by co-activating both a flexor and an extensor."
"in this paper, an efficient optimization algorithm, called chho, has been proposed and applied for estimating the optimal effective parameters of different proton exchange membrane fuel cell stack models. the proposed chho algorithms are based on the conventional harris hawks optimization hho and ten chaotic functions. the best chho algorithm has been applied for finding the optimal parameters of four different commercial pemfc stacks under a wide range of operating scenarios. the mathematical model of pemfc has been precisely described and defined. this model has been used for optimizing and estimating parameters of pemfc models. the 250w pem fuel cell stack, three various commercial fuel cell stacks, namely, bcs 500w, sr-12 500w, and temasek 1kw have been used to evaluate the effectiveness of chho algorithm. the optimal estimated data obtained by the proposed chho shown a good agreement with the experimental data of different commercial pem fuel cell stacks. the results obtained by chho have been compared with different recent metaheuristic optimization algorithms. in all case studies, the obtained results from the proposed chho shown high accuracy in estimating the optimal parameters of pem fuel cell stacks. moreover, a statistical measurement has been performed to prove the superiority and accurateness of the chho in solving the studied optimization problem. during the simulation process, the most obtained values of objective function of chho are in the range of minimum value of sse, and there are a few numbers of obtained values in the range of maximum value of sse. the proposed chho can be considered as an efficient optimization technique for solving the problem of pemfc parameter's estimation. in the future work, the proposed technique could be applied for solving other optimization problems."
"where, ξ 1, ξ 2, ξ 3, ξ 4 are semi-empirical coefficients; c o2 represents the oxygen concentration at the cathode in (mol.cm −3 ), which can be expressed as below [cit] :"
"termed as the new era in wireless.communications, worldwide interoperability for microwave access or generally known as wimax is the answer to anytime, anywhere access to information, offering reliable internet connectivity all around the world."
"where, x (t) and x(t+1) are the hawk's positions at the current iteration t and the next iteration t+1, respectively. x rab (t) is the rabbit position, x rand (t) is a random hawks position, r 1, r 2, r 3, and r 4 are randomly generated number between [cit], lb and ub are the lower and upper limits of control variables, q is a random number between [cit], which used to switch between the two exploration strategies, and x m (t) is the average position of hawks which calculated as:"
"implantable wireless emg sensors [cit] have also been developed and are capable of providing emg signal measurements from up to 16 muscle sites to a prosthetic device. however, there needs to be a better understanding of the independence of the extrinsic finger muscle compartments before they can be fully utilized as myoelectric control sites for a prosthetic hand."
"harris hawks use two strategies to detect the prey which is usually a rabbit. the first strategy assumes that the hawks allocate close to the family members and the prey. in the second strategy, the hawks place on random trees. these two strategies can be modeled mathematically using (19):"
"for direct myocontrol, the relative amount of activity of non-targeted muscles with respect to the target muscle activity is more important than the absolute amount of activation in each muscle. this is because signal thresholds and differential measurements are commonly used by prosthetists to compensate for co-activity when setting up direct myocontrol devices."
"in designing a lna, if a good input return loss is desired, the noise figure will be high; if a good noise figure is desired, the vswr will be high. the best way to resolve these opposing requirements is to obtain as low as possible noise figure and good return loss is by using a feedback network."
"where, r m denotes the resistance of membrane surface in ohm ( ), r c denotes the equivalent resistance of the connection which the protons face during passing through the membrane. the membrane resistance can be expressed as follows:"
"for more validation, the values of output voltage of pemfc stack are compared with the corresponding measured one. the deviation between the estimated and measured data is evaluated by individual absolute error (iae) and relative error (re) at each point of 15 data set. iae and re are mathematically calculated as given in (38) output terminal voltage of 250w pemfc stack are listed in table 6 . in this subsection, the proposed chho4 is applied for extracting the unknown parameters of the bcs-500w fc, the sr-12 500w fc, and the temasek 1kw pemfc stacks. the statistical analysis based on 50 individual runs for the volume 8, 2020 three fuel cell stacks using chho4 and the conventional hho are reported in table 7 . the convergence characteristics of proposed chho4 and hho for these fuel cell stacks are shown in fig. 5a-c obtained by chho4, hho and other optimization algorithms."
"for persons surviving a trans-radial amputation, extrinsic hand muscles that reside in the forearm provide potential emg sites for controlling their prosthetic device. surface emg electrodes are commonly used to detect the activity of the underlying musculature [cit], but because the muscles are small, close together, and sometimes deep within the forearm, these electrodes detect the summation of simultaneous activity from several muscles. the three extrinsic muscles of the fingers each have compartments with a tendon connecting each compartment to a finger. these compartments could provide additional control sites for a prosthetic hand if people are able to control individual compartments."
"as stated above, the activations do not need to be completely isolated from one another for the application of using these muscles as potential control inputs for direct myocontrol prostheses. rather, there needs to be a consistent difference between the activations of the muscle of interest and the other muscles being used as control sites. the index and middle finger compartments of the three extrinsic hand muscles tests could all serve as control sites for myoelectric devices, with the exception of the extensor of the middle finger, edc2. each digit would have two command inputs for flexion and both digits would share a common input for extension. this potentially allows each finger to have two degrees of freedom in flexion, which would allow for more functionality and dexterity in a prosthetic hand."
the voltage drop resulting from the activation process v act around the anode and cathode electrodes can be mathematically calculated according to the following expression [cit] :
"targeted activation of the middle finger compartment of fdp (figure 4, fdp2 column) resulted in co-activity from the middle finger compartment of fds and index finger compartments of edc and fdp (fds2, edc1, and fdp1 rows, respectively). the relative mean activity in these muscles was roughly 50%, meaning that their activity was roughly half of that of the target muscle, fdp2. all other muscle compartments were able to activate with minimal coactivity from the other compartments (light and dark blue squares). if fdp2 were to be a control site for direct myocontrol, thresholds or other customizations would need to be used in order to accommodate for the coactivity from edc1, fdp1, and fds2."
"due to the importance of pemfc and its advances in industrial applications and reduction of cost, there is a need to build an accurate model for a good understanding of the dynamic processes and phenomenon occurred in the fuel cell without the need for complicated experiments to save effort and time [cit] . in the last years, many papers tried to model the operation of pemfc, and many models have been provided in literature [cit] . one of the essential characteristics of the fuel cell is its polarization curve, which explains the relation between the cell current and the output voltage (i-v curve) because the operating points and other auxiliary devices such as air conditioning and controllers mainly related to these curves."
"the lna design is test with two technique of broadband amplifier design which is a feedback amplifier and balance amplifier. from these two designs, the best technique will be chosen which gave the most satisfactory result for its gain and noise figure. the feedback amplifier has the best performance because it has superior value for its gain and noise figure compare to balance amplifier."
"there are three type of gain that the transistor have. the gains are power gain, g p, available gain, g a, and transducer gain, g t . the transducer power gain;"
"this subsection introduces the mathematical formulation of hho based on the hunting approach of harris hawks [cit] . as any population-based optimization technique, hho has been formulated using exploration and exploitation phases."
"where, x (t) indicates the distance between the rabbit location and the hawks' position, j is the random jump of the rabbit during the escaping, r 5 is a random number between [cit] ."
"every electrode location was verified with electrical stimulation during electrode placement and at the conclusion of the experiment. if an electrode was found to have moved outside of its muscle compartment during the experiment, the data associated with that electrode was not used in our analysis. this occurred in only four out of 42 electrodes."
"where, sse i is the value of objective function obtained at the end of each run. sse min represents the minimum best value of sse overall. sse represents the mean value of sse over the 50 launches of optimization process. the statistical results of proposed chho based on ten chaotic functions and hho have been listed in table 4 . from this table, it can be observed that the smaller values of mae and rmse proved a well matching between the estimated and the measured parameters. the values of minimum sse obtained by using 4 th chaotic function is the best within all functions as well as the conventional hho algorithm. the optimized parameters of pemfc model using ten chaotic functions are provided in table 5 . the results obtained from the proposed chho are compared with those obtained from hho and other recent optimization techniques reported in literature (see table 5 ). from this table, it is clearly noticed that the chho based on the 4 th chaotic function (chho4) gives the minimum value of objective function (0.674734884) within all ten chaotic functions and other optimization techniques included in the comparison."
"in the section, the proposed chho techniques based on chaos theory are presented. chaos maps have been used to predict unpredictable actions by formulating a set of chaotic equations [cit] . for optimization techniques, chaotic maps are used to improve their convergence characteristics by applying the chaotic equation instead of utilizing random parameters. in this work, ten chaotic maps presented in table 1 are applied for the conventional hho to update the exploration parameter q instead of using random probability as follows:"
"regardless of which finger muscles were contracting, this experiment necessitated the use of intramuscular fine-wire electrodes in order to measure those activations. the individual muscle compartments are very small, close together, and reside deep within the forearm. traditional surface emg electrodes are not able to separate the activity from individual muscle compartments from that of surrounding musculature. fine-wire electrodes have a much small detection volume and are therefore ideally suited for this study. however, their small size makes them more sensitive to electrode movement during the experiment. this sensitivity coupled with some discomfort due to the invasiveness of the electrodes caused some recording complications."
"links are the web's vital bridges. since the beginning, hyperlinks have been a crucial element of the world wide web, and continue to be a substantial success factor. this is certainly true for the human web, and even more for rest and the semantic web, where meaning is created and defined by linking to specific uris [cit] . the possibility to uniquely identify and link to a resource is so crucial that it should apply to all intelligent services. fielding indeed famously described hypertext as \"the simultaneous presentation of information and controls\" [cit] ."
"again, reasoning can play an important part here. by incorporating ontological knowledge into the reasoning process, a bridge between different ways of expressing information can be built. as a result, service discovery can work even in cases where the vocabulary of the description is different from the vocabulary of the data. furthermore, reasoning also allows different clients to obtain the information they need to construct the request that satisfies their goals."
"on the level of infrastructure, a strong link with http can be identified. restdesc is based on the fundamental properties of the web and especially its resource-oriented nature, the ideas of which formed the basis of the current http . standard [cit] . the hypermedia constraint inherent to rest is satisfied by extensively making use of link types, which directly map to rdf predicates. the essence of restdesc is indeed to describe the relationships among resources and the concrete http requests instantiating the functionality of those relationships."
"dereference link types -since we use rdf predicates as the hypermedia link types, the preferred option is to request the description of each link by dereferencing its uri. for instance, the description of the ex:comments relation can be found directly at http://example.org/image#comments. using content negotation, agents can indicate whether they are interested in the image ontology or the restdesc description. this method thus functions in a link-centric way."
"linked open services (los, [cit] ) expose functionality on the web using linked data technologies, namely http, rdf, and sparql. input and output parameters are described with sparql graph patterns embedded inside rdf string literals to achieve quantification, which rdf does not support natively."
"in this article, we proposed an effective mtlbo algorithm to solve multi-objective disassembly sequence planning problem. as many non-dominated solutions as possible should be reviewed by a stakeholder to select in order make better decisions. to this end, we investigated a set of experimental simulations. the simulations showed that our mtlbo algorithm achieved good convergence and spread for the non-dominated solutions it found. in addition, the algorithm performed better than nsga ii to solve the multi-objective disassembly problem. we also proposed a service-oriented disassembly sequence planning framework to help the stakeholders involved with handling e-waste to more effectively collaborate on disassembly sequence planning via the internet. our approach extends the e-business model to include treatment of e-waste at the end of its useful service life. it addresses key questions on how to take advantage of web services and e-business technologies to support more sustainable business practices."
"from an evaluation perspective, all proposed algorithms were evaluated on popular data sets. regarding the baseline methods used for comparison, it can be observed that most evaluations aimed to prove that using or incorporating tag information into traditional cf will improve the quality of recommendation and, thus, the focus has been on whether the proposed approach will outperform traditional cf recommendation approaches or not. as far as the evaluation methodology is concerned, there are predominant offline evaluations where the classification accuracy of the recommended top-n items is examined. none of the discussed research has used more than one evaluation methodology."
"effective disassembly sequence planning methods can improve disassembly efficiency, reduce disassembly cost and environmental impact, and improve the recovery value of ewaste. to make disassembly planning more effective, it is necessary to obtain detailed information on e-waste at the outset of the analysis work. however, it is likely to not be effective to get this kind of information support through current e-waste-related informationsharing mechanisms. also, the disassembly model should be built in a way that ensures the representation of the disassembly process constraints. and finally, there needs to be an optimization algorithm that performs effectively when it is used to solve the disassembly sequence planning problem."
"our three-layer service-oriented disassembly sequence planning framework is shown in figure 9 . the resources layer considers distributed information that is provided by different stakeholders, and the software and algorithms used for disassembly modeling and optimization. the resources are provided as cloud services in the core service layer. the service management layer is used for the description and publication of cloud services, as well as their registration and match-making for their use in the marketplace."
o ne of the core issues in technology enhanced learning (tel) is the personalization of the learning experience. there is a shared belief among tel researchers that tel models require a move away from one-size-fits-all models toward a learner-centric model that puts the learner at the center and gives her the control over the learning experience.
"compactness and simplicity -authors and consumers are not faced with long and verbose descriptions that are difficult to understand at sight. restdesc descriptions provide at a glance the essence of a service: its functionality, the precise task the service performs. this high level of compactness is possible because restdesc only describes what is strictly necessary, namely the functional hypermedia relation between resources and the http request that obtains that result. details, such as parameter types, reside where they belong: in ontologies. the example in listing 1 shows for instance that the thumbnail and height properties reuse the dbpedia ontology, which provides their characteristics."
"the traditional method to solve a multi-objective optimization problem is to weight the relative degree of importance of each objective and then transform them into a singleobjective optimization problem. however, this method has a drawback for the multiobjective disassembly sequence planning problem. since disassembly involves different stakeholders with different legislative and economic considerations, one single-objective solution cannot meet the needs of different stakeholders. furthermore, it makes sense to determine as many non-dominated solutions as possible for a stakeholder to select in order to support better decisions."
this algorithm is very similar to algorithm 15. the only difference is that the association rules are learned from the item-tag binary data set l.
"in order to make the restdesc description paradigm work in real-world applications, we need a method to automatically discover descriptions. after all, we aim to make generic agents perform operations with specific services. we envision several possible methods, listed below. listing 3: restdesc allows state-changing operations, such as adding comments to an image."
"this algorithm is an adapted version of algorithm 5, using lsa for dimensionality reduction. due to the power law distribution of tags, it is possible that we cannot identify similar users when calculating the cosine similarity on the entire user-tag data set. therefore, in the first step of this algorithm, we perform the k-nearest neighborhood discovery on the lower dimensional space a 0, the result of the application of lsa on the original user-tag frequency data set a. if the like-minded users are determined on the basis of collaboratively shared latent topics, it can be possible that the cts contains more appropriate tags for the target user u."
"another important decision is our choice for notation3, whose simplicity and powerful logic foundation reflect on restdesc. this decision was prompted by the observation that, in order to make a statement about generic instead of specific resources, support for quantification is necessary, yet currently not provided by rdf. the same observation was implicitly made by owl for services (owl-s, [cit] ) and linked open services (los, [cit] ), which both have to resort to richer expression languages inside rdf string literals."
"disassembly sequence planning for e-waste always involves globally-distributed participants in the lifecycle of this kind of equipment, such as manufacturers, distributers, retailers, disassemblers, recyclers, remanufacturers, management authorities and so on. to make an effective disassembly plan, all the participants should work together. however, information"
"v. experimental results to evaluate our relinoc architecture, we used noxim [cit] which is a cycle accurate noc simulator implemented in systemc. the switch model in this simulator has a 2-stage pipeline and therefore has 2 cycles minimum latency. we extended it to support both virtual and physical channels. we also extended it for our reliability techniques."
"we also calculated the worst-case latency overhead in all fault configurations having fully-connected networks. experimentally, we found the worst-case latency overhead in bitcomplement traffic. our experiments showed that even in the worst-case, the latency overhead on qos packets was 30% for up-to 25 faults. however, in the worst-case configuration, the normal packets payed much more latency overhead in presence of many faults compared to the qos packets. the latency overhead in normal packets of bit-complement traffic for 15 faults in the worst-case configuration was 5x."
"this approach is based on the idea that a user can have a latent preference for the tags of his similar users [cit] . the algorithm is divided in two steps. in the first step, we extract the tags from the k-nearest neighbors of the target user u. then, we compute the predicted value of user u preference for a tag t as the sum of the tag frequency of the k-nearest neighbors of user u for tag t weighted by the cosine similarity between the target user u and his k-nearest neighbors. the w most highly ordered tags are then put in a candidate tag set. in the second step, by applying probabilistic classification, we can recommend to the target user u those items that have the highest probability to be annotated by the tags from cts. for the classification task, we train weka's naive bayes classifier [cit] using the itemtag binary data set q, assuming that each item tag vector represents one class of items. in other words, the occurrence of given tags can be characteristic for one class of items."
"additionally, we needed to prove that the reasoner can create an execution plan, as indicated in subsection 3.4. the descriptions of listings 1 and 4 should be sufficient for the reasoner to, given the starting situation (a local image), create a plan to achieve a goal (obtaining a thumbnail). we verified that it correctly generates the details of the two required requests: uploading the image and retrieving the thumbnail. this experiment and its result details are also available online. furthermore, from previous experience with reasoner-based composition, we are confident that this approach is scalable to a level that comfortably enables practical use [cit] ."
"experiment results show that our 2-channel switch architecture can decrease the latency of both qos and normal traffic by 30 to 50 percent compared to the vc-based switch in an 8x8 noc with synthetic traffic. in addition, relinoc can tolerate up-to 50 faults inside the 8x8 mesh with 10 and 40% overhead on control and data packets latencies, respectively, in real traffic. moreover, it provides almost 1.5 to 3 times better network physical connectivity compared to a vc-based architecture. synthesis results show that relinoc switch has only 13% area overhead with respect to the baseline 2-channel switch."
"despite the advantages of cf recommendation, there are several shortcomings. these include, for example, the cold start, sparsity, scalability, and gray sheeps problems [cit] . moreover, as traditional cf recommender systems consider only the rating information, this results in the loss of flexibility [cit] ."
"as a concrete instance, 1 suppose an agent has an image located at http://example.org/images/37. when it requests this url, the server returns a representation of this image, accompanied by several hyperlinks (e.g., via link headers [cit] ). the description in listing 1 tells the agent that, in order to obtain a thumbnail of 80 pixels in height, it should find a hyperlink with relation smallthumbnail. this hyperlink is amongst the returned links, and points to /images/37/thumb. following the instructions in the description, the agent thus performs a get request to /images/37/thumb, in response to which the server generates the desired thumbnail and sends it back."
"it has been observed that in ples, learners rarely share the same or similar learning resources due to the fact that they follow their individual interests and preferences. thus, recommendation in ple should rely on the activities and the metadata (e.g., tags) generated by the learners [cit] . in this paper, we consider using the tagging information and incorporating it into traditional collaborative filtering (cf) recommendation methods to provide recommendation of learning items in a ple. we focus on tag-based collaborative filtering recommendation methods and provide a thorough offline and user evaluation of different related techniques that can provide a basis for their further application and research in tel environments."
"multi-objective evolutionary algorithms have been widely studied and applied to solve multi-objective optimization problems, such as the non-dominated sorting genetic algorithm ii (nsga ii) [cit], the multi-objective evolutionary algorithm based on decomposition [cit], and the multi-objective differential evolution algorithm [cit] . this article proposes a modified teaching-learning-based optimization (mtl-bo) algorithm to solve the multi-objective disassembly sequence planning problem. five major components have been designed and incorporated into the algorithm in order to make it applicable for specifying complex disassembly precedence constraints."
"this paper demonstrates and validates a framework of egovernment adoption generally, findings indicate that website design, perceived public value, trust in government, trust in the internet, perceived ease of use, perceived usefulness, attitude toward using e-government, behaviour intention, egovernment actual use are valid measures of e-government adoption success. apart from h5 (pu on bi), h7, h8, h9, h10, h11 and h12 (demographic factors), the other hypothesised relationships between variables were moderately or significantly supported. however since overall goodness of fit of model is high including all constructs, it is proposed that different relationships for the demographic constructs within the model could be investigated in future, not specifically connected via perceived usefulness and perceived ease of use."
"this section starts with a brief review on the teaching-learning-based optimization (tl-bo) algorithm and multi-objective optimization theory, and then presents the proposed mtlbo algorithm in detail and compares it with the tlbo algorithm."
"as the size of the power system data and number of power system clients are increases, compressed data occupies more bandwidth then uncompressed data. hence the rtt also differs more. from the above graph, it is very much evident that data compression algorithm performs better than the conventional distributed rmi model for load flow monitoring. an effective rmi-based distributed model has been developed to compress the power system data, send the compressed data across the network, decompress it and retrieve the data and calculate the round-trip time taken for the process in a single server -single client environment. it has been tried out to overcome the overheads associated with data congestion through this model. although, data communications models are well established, this paper emphasizes a unique methodology based on rmi to serve a large number of clients in a distributed environment which enables less congestion for data flow. the time taken to send compressed data will be less, the networks will not be jammed and the whole process of data transfer will be more efficient. a practical implementation of this approach suggested in this paper was assessed based on rmi based load flow monitoring but it finds major online power system applications such as contingency analysis, economic load dispatch, unit commitment etc. accordingly the proposed model can be implemented for power system network spread over geographically."
"adding the power of restdesc service descriptions to web intents creates an automated and more scalable solution. web intents is extensible by design: neither the list of actions nor the list of media types are fixed. nonetheless, service developers cannot simply define new actions, because existing web intents clients would not understand how to use them. this is where restdesc functional descriptions play an important role, since they can provide semantics for new intent actions that can be understood by existing clients, even if the described functionality was not available when that client was built. because the goal of restdesc is precisely to support generic clients in their decision making process, its combination with web intents is a natural step to make new interaction patterns possible."
"linked data services (lids, [cit] ) define interface conventions that are compatible with linked data principles [cit] and are supported by a lightweight formal model. this enables automatic creation of lids interfaces and integration of links to lids in existing data sets."
"often more interesting to agents are state-changing operations using unsafe methods such as put or post. they can also be described in the restdesc skeleton. for example, suppose we want to describe that comments can be added to images, as in listing 3. first, we add a hyperlink from each image resource to its comments resource 1 . for example, the image http://example.org/images/37 may have its comments at /images/37/comments. an additional precondition is that we need to have a comment. we then describe the request needed to add this comment to that image 2 . finally, we explain that the comment is attached to the image as a result 3 ."
"the offline evaluation aimed to compare different tag-based cf recommendation algorithms and to select those that can offer the best recommendation accuracy based on three evaluation metrics: precision, recall, and f1 score. however, as mentioned earlier, these metrics do not measure the user satisfaction with the recommended items. the quality of user experience often does not correlate with high recommendation accuracy measured by these metrics [cit] . thus, we conducted a user evaluation on the seven selected recommendation approaches, as discussed in the previous section. we evaluated the recommendation module in plem with students and teaching assistants in the \"advanced learning technologies\" [cit] . the participants were asked to collect, manage, and tag learning items related to the topics of elearning and web technologies. a screenshot of the plem recommendation user interface is shown in fig. 4 ."
"vocabulary reuse -we do not force description authors to learn and use a new vocabulary. restdesc adapts to the application domain of the author, instead of the other way around. listing 1 uses a service-specific vocabulary (ex), dbpedia [cit] vocabularies (dbpedia and dbpedia-owl), and the http vocabulary (http, [cit] ). we do not need additional vocabularies to describe the service, because of the resource-orientedness of our approach: the resources in the description are the resources of the service. variables, an n feature, instantiate the vocabulary for concrete resources. hypermedia link types are simply predicates in the application's ontology (ex), since they indeed express a connection between two resources."
"service repository -as a last resort-for instance, when describing an external a service without access to its serverrepositories can provide descriptions for various services. the idea is not to have one central repository but multiple repositories (possibly connected), which a client can use as a starting point. for example, a client can ask to find services for images, and the repository could return listings 1 and 3. this can happen in link-and resource-centric ways. hypermedia links between the repository and the service can be made, allowing hypermedia-driven discovery."
"unlike the k-means clustering approach that assigns a user strictly to one cluster, in the expectation maximization approach [cit], a user can belong to several clusters with a certain probability. we consider a probability threshold of 80 percent when assigning a user u to a cluster m. therefore, for the target user u, we obtain first if he was clustered to any cluster with probability above 80 percent. if this is not the case, we cannot provide any recommendation. otherwise, we get his neighbor users from the cluster he was assigned to and generate the items unfamiliar to user u as his candidate item set. in the last step, we recommend the top-n items for the target user u. similar to k-means, the input parameters for the cluster learning phase are the usertag frequency matrix a, the number of cluster k, the maximum number of iterations, and the seed value."
"the restful web is resource-oriented. most programmers tend to be familiar with method-based thinking. as a consequence, there has been a tradition of web service techniques that bend http [cit] to act as a tunneling protocol for action messages (e.g., soap [cit] ). however, this does not align with the fundamental principles of resource-oriented architectures [cit] and rules out the hypermedia mechanism. designing web services for intelligent agents requires a different mindset, by thinking in resources instead of in actions. the semantic web is also based on those same resources (hence, the resource description framework or rdf [cit] ), another reason the web for agents should focus on resources."
"we deliberately chose to describe services that employ the resource-oriented and hypermedia principles underpinning the fundamentals of rest. commonly referred to as rest services or restful apis, they contrast with services that use remote procedure calls, the so-called rpc-style. unfortunately, several services that label themselves as \"rest\" lack a hypertext-driven architecture [cit], typically by falling back to uri construction rules defined in advance instead of at runtime, which makes them plain http interfaces [cit] . however, the rest community is working hard to turn the tide."
"one of the key questions is of course whether our proposed approach is able to realize the functional descriptions that generic intelligent agents need. therefore, we investigated how current state-of-the-art reasoners perform on restdesc descriptions, based on functionality. the main questions are whether restdesc descriptions are sufficient for a generic reasoner without plugins, and whether the reasoner returns sufficient information for a generic client."
"service-oriented disassembly sequence planning offers a useful extended e-business model for the end of the service life involving the kinds of electronic and electrical products that we have studied in this research. we introduced the use of cloud services for the disassembly context, and why it will fundamentally transform the business model that is in use today in the disassembly industry. the hallmarks of our proposed approach involve how the characteristics of customization and distribution can be leveraged for better performance."
"this means that, when the preconditions for the action are fulfilled, executing the request will lead to the postconditions. for instance, having an image and requesting a get operation on its thumbnail uri implies that we will receive a thumbnail image. the complexity in equation 1 lies in the fact that the implication is fulfilled only when the request for the action is successfully carried out. to obtain simple expressions that can be reasoned upon easily, we must express equation 1 in a"
"attitude is the construct that gathers most attentiveness and is used most extensively for forecasting users' \"likelihood to adopt a new technology\" [cit] . practically, users today have many technology innovations options, for which they might have developed a favourable or unfavourable attitude about, regardless of whether they have actually used the product in question."
"it also maintains subroutines and programs for predefining access requirements and for accessing the physical database. the database management function can be viewed as the centre energy control system. in this relational structure, power system data is logically arranged as a collection of two dimensional arrays. each of these data arrays is called a data type. a data type represents a distinctive group of data such as a set of meter readings, a complete description of transformers, generator data etc. the commonality of data requirements between power system planning and operation functions is shown in fig. 3 which comprises the logical database design diagram. modelling of on-line power systems is an area of ongoing interest in the transmission management and control systems community in a distributed environment. continuing development is driven by two forces. the traditional tasks of model maintenance and management must be achieved with fewer resources. at the same time, information exchanges between the models and coordination have become a priority. the later force arises from the disaggregation of subsystems in a power system and the introduction of new power systems. the traditional architecture faces the problems of versioning and version control, migration of models between different schema, the transformation of models for different purposes or any change in power system logic and merging old power systems with an existing power system model from different sources. these tasks are handled by semi-manual methods or heavily customized software."
"discovering functionality-for example, that a site offers a news feed for feed readers-is possible due to the fact that a set of preconditions is fulfilled. in this news feed example, the preconditions are the existence of the link to a resource (e.g., feed.xml), the presence of a certain link relationship (e.g., alternate), and the availability of a specific media type (e.g., application/atom+xml). web browsers either already have those preconditions hardcoded, or can be supplemented with browser extensions to offer services for those preconditions. the postcondition in this case is that the user will be subscribed to the feed in her preferred news reader."
"generally, men are encouraged to be \"assertive and competitive\", and women to be \"nurturing and cooperative\"; these different gender roles are often conveyed through socialisation [cit] . gender could be a factor in defining a person's way of assessing a technology; e.g., usefulness or ease of use. some empirical evidence proposes that perceived usefulness is more important for men than for women [cit] . the social roles and prospects for women seem rigid in the arabian culture [cit] ."
"website design was proved to be a significant predictor to the users' beliefs (perceived usefulness and perceived ease of use. citizens' adoption of e-government increases if egovernment websites are provided with sufficient, attractive and well-organised design and content. government agencies in egypt should make sure that their websites are accessible to various users. the government in egypt therefore should guarantee that their websites are offered with reliable and attractive screen layout. also, there should be continuous updating of the website links, so as not to direct to deleted or re-directed pages and to guarantee that people are comfortable in interacting with the government via the web-based channel."
"in this way, the disassembly precedence in the solutions is preserved. for instance, table 1 gives an example with 5 elements: a new solution ([ 2 3 4 5 1 ]) is generated from a teacher ([ 2 3 1 4 5 ]) and a learner ([ 4 3 5 1 2 ]). suppose that operation 3 has precedence over operation 5. then, the precedence will be preserved in the new solution when our approach is used."
"the user-based hierarchical clustering algorithm creates a hierarchical decomposition (a tree of clusters, dendrogram) [cit] of the given set of users. the closeness of two users is defined by the cosine distance. once the hierarchy is learned, we specify the number of clusters k we want to have. then, we find all the users within the cluster the target user u has been assigned to. finally, the top-n items are computed and recommended to the user u."
"given the item-tag frequency data set m, we build hierarchical groups of items. for each item i of the target user u, we get the assigned cluster m. if user u has not seen a neighbor item from cluster m, this item is added to the candidate item set. from the candidate item set, we then generate the top-n items and recommend them to the user u."
"perceived ease of use was established as a variable that affects the adoption process due to its various impacts on key variables such as perceived usefulness and attitude (h4, h2 tested previously). as long as as citizens recognise that using e-government services is effortless (easy to use) their attitude and perceived usefulness become increased. it is the government's responsibility to explain e-services based on users' needs and knowledge, so as to render them uncomplicated to use for the wide-range of citizens in relation to internet experience."
"we note that, vc-based switches are better when links are off-chip, as they trade-off increased logic complexity for better utilization of physical channels. when physical links are expensive, e.g. for off-chip networks, this trade-off makes sense. also, vc-based switches are better when the number of vc becomes very large and they are used rarely, but this is not our case, as we have only two classes of traffic and high utilization for both."
"smaller, yet more powerful, computers are linked together to share memory space, software and information. a silent shift to distributed computing is happening. today, computer systems are being implemented as clusters of servers sharing the workload and supporting each other in various environments."
"this section summarizes the tools and technologies used for implementing our approach. the plem system has been developed using java programming language, spring in the back end, and google web toolkit for the front-end interface. the tag normalization and stemming task takes advantage of the apache lucene's snowballanalyzer [cit] . for executing the data mining tasks, we leveraged the weka api. 2 we use weka v3:7, development version, as well as two separate packages: latentsemanticanalysis 1.0.1 and opticsanddbscan 1.0.1. since weka does not support the cosine similarity measure, we extended the euclideandistance class of weka and implemented a subclass cosinedistance to compute a cosine distance metric: 1 à cosðu; vþ, where cosðu; vþ is the cosine similarity between two users/items u and v. building a recommendation module with weka requires the following four basic steps [cit] :"
"according to davis [cit], customer's intention to adopt and make use of a certain technology depends not only on the attitude but also on how the customers believe that using egovernment would provide all required information -i.e. usefulness. [cit] established that behavioural intention was largely driven by perceived usefulness."
"although recommender systems are increasingly applied in tel, there is relatively little research on recommender systems in tel that rely on tagging information. in tel systems, tags are often used to annotate learning resources [cit] . in a recent survey of tel recommender systems, [cit] discuss few examples of systems that apply tag and rating data for recommendation to overcome the cold-start problem of the recommender system. these approaches, however, mainly use classic cf recommendations based on the rating information. in another study, [cit] explore the extent to which implicit feedback of learners (e.g., tags) can be used to augment explicit feedback ratings to improve recommender performance in the tel domain. as a potential solution for the data sparsity problem, the authors apply a standard cf recommendation on several data sets (e.g., mace and mendeley data sets) that include tags that are provided by users on learning resources [cit] . in this experiment, the tagging information is just used to rank items to the user in order of decreasing relevance. in sum, state-of-the-art tagbased recommenders in tel combine rating and tagging information. to our knowledge, there are no tel recommenders that rely solely on tagging information."
"the bit of link-1 in isr is '1': this means link-1 or the output channel of previous router is faulty. therefore, no data will arrive at channel-1 because of icf signal which is already sent to the previous router. however, in this case if buffer-1 and muxbuf-1 are non-faulty, they can be used by channel-2. here, control logic generates appropriate signals so that channel-2 is connected to both buffer-1 and buffer-2. in this case buffer-1 is used for qos traffic and buffer-2 for both qos and normal traffic."
"the main assumption behind the k-nearest neighbor approach is that users with similar tagging behavior will share the same learning interests. the more their tags overlap, the more interests these users will have in common. consequently, we can recommend the items of the similar users to the active user, given that he has not saved those items yet. knn is an instance-based learning approach. this approach is also called lazy learning, because its training phase is simply storing all instances from the data set. given a particular point of the data set, the task of the algorithm is to find the k closest points, the nearest neighbors, from the training data [cit] . the distance between two instances is determined by the cosine distance. for the knn discovery, performed by weka's linearnnsearch, we input the user-tag frequency data set a and the number of k similar users we want to have. in case that the search reveals similar users, we get their items, remove those known by the target user u. each candidate item receives one vote from each neighbor who has tagged it. finally, the top-n item list for target user u is generated from the candidate item list by sorting the received votes."
"the computation of p distance,i requires sorting the population according to objective i (f i ) in the descending order. thereafter, if f i (p ) is the smallest or largest, p distance,i is assigned an infinite value. otherwise, it is assigned a value equals to the absolute normalized difference in values of f i of the two adjacent solutions."
"there is a large number of recommender systems that have been deployed in tel settings [cit] . however, relatively little significant work around the evaluation of recommender systems has been undertaken. until today, the evaluation of recommender systems gives emphasis to rather technical measures (e.g., accuracy, coverage, performance in terms of execution time) although the importance of including user-related evaluation methods (e.g., effectiveness, efficiency, satisfaction) has been highlighted [cit] . moreover, an implementation of different recommendation algorithms within a single recommender system to compare them against each other is missing in the tel recommenders literature."
"tag-based srs leverage the tagging behavior of the user to generate useful recommendations. recognizing the importance of tags, a large number of srs harness the tags of collaborative tagging systems. alag [cit] recapitulates why user-generated tags are useful: they use terms that are familiar to the user; they bring out the concepts related to the item; they can capture a semantic value associated with an item even though not mentioned directly in the content; and they can offer useful collaborative information about the user and the item. in tel context, several researchers stressed the importance of tags in tel environments. [cit], for instance, investigate the importance of tags in \"tag ecologies\" and show that tags can enrich and add value to multilingual controlled vocabularies. the authors further conclude that tags can become a useful source of metadata for learning repository owners, and help them better understand users' needs and demands. in another study in a context of european learning resources exchange, vuorikari and ochoa [cit] state that tags could be used to facilitate the discovery of educational resources across country and language borders. hsu and chen [cit] define three categories of usergenerated tags:"
"as keyword annotations seem to be a comprehensive way to relate a user to items, incorporating them in recommender systems is a promising step for boosting the performance and quality of recommendation. from a recommendation perspective, the above presented tag categorization indicates that mining folksonomies [cit] to discover user interests can be useful. making use of factual tags can also help in organizing the items in topic clusters and suggesting relevant content based on the user preferences. on the other hand, using the second and third type of tags can increase the diversity of the suggestions. moreover, tags can enrich the rs with additional information and, hence, improve the quality of recommendations with respect to relevance, coverage and diversity, as well as the user experience. additionally, using tags to represent the user's preferences, we can even introduce recommendation for systems where rating information is not available. furthermore, tag-based profiles can result in better user similarity calculation and in identifying more neighbors, thus achieving a more precise and complete recommendation [cit] . another benefit of tags is the fact that they can capture the changing user preferences over time and this can easily be updated in his profile by simply adding the new tags. hence, the personalization of recommendation will adapt according to the changing preferences."
"in other application areas (e.g., e-commerce, artificial intelligence), the evaluation of several algorithms developed to use the tagging information for recommendation has shown that tag-based recommendation can improve the performance and quality of the recommendation in comparison to the traditional recommender systems [cit] . in the next sections, we discuss important state-ofthe-art tag-based cf recommendation approaches that could be applied in a tel context."
"1. q1-ability to recommend. the system is able to provide recommendation for me. (y / n) 2. q2-accuracy. in my opinion, the system is able to recommend to me 1-3 / 4-6 / 7-10 interesting or relevant learning items. 3. q3-novelty. the learning items recommended to me are novel and still interesting. 4. q4-diversity. the learning items recommended to me are diverse (not all of them are similar to each other)."
perceived ease of use is another main impact factor of attitude toward use in the tam model. this internal belief links to an individual's evaluation of the mental effort required to use a system [cit] . enhancements in perceived ease of use may add to better performance. peou has been shown to influence behaviour through two causal ways: (1) an indirect effect on behaviour via att and (2) an indirect effect on behaviour via pu.
"different stakeholders related to e-waste (e.g., remanufacturers, recyclers, and regulators) have different legislative and economic considerations when making disassembly planning arrangements. they have to balance multiple objectives. in order to formulate the multiobjective disassembly sequence planning problem, this paper introduces three disassembly indices to evaluate a disassembly sequence, namely index of diminished toxicity, index of potential recovery value and index of potential recovery weight. accordingly, we formulated a model with the objective of maximizing these disassembly indices."
"following, we evaluated the ability of the proposed recommendation algorithms to provide novel and attractive items to the users (q3-novelty). the user-based lsa knn with cts generation and nbc and both the user-based and the item-based clustering methods (rec 5 and 6) had the worst performance, while the user-based apriori and the user-based knn with cts generation and nbc received the highest scores."
"in figure 4 the relationships between each two of the objectives. the three objectives contradict one another. choosing a solution from among the acquired non-dominated solution set depends on the decision-makers preferences for the objectives. therefore, the optimization results can help in supporting decisions."
"iv. power system data representation effective power system operation requires power system engineers and operators to analyze vast amount of information. in systems containing thousands of buses, the key challenge is to present the power system data in a form such that the user can assess the state of the system in an intuitive and quick manner [cit] . this is particularly"
"ulrik schroeder is a professor of computer science at the rwth aachen university. he heads the learning technologies research group. he is also the head of the center for innovative learning technology (cil) and the director of the school laboratory for computer science (infosphere) at rwth aachen university. his research interests include assessment and intelligent feedback, mobile learning, gender mainstreaming in education, and computer science teachers education."
all bits in isrs of e1 and e2 are '0': this means all the input components at east channels (e1 and e2) of the current router and all the output components at west channels (w1 and w2) of its related neighbour are non-faulty. in this case the router works in its normal operation.
"to verify this, we have fed the descriptions of listing 1 and listing 4 into a reasoner, together with respectively the starting situation (a local image) and the post-upload situation (a hyperlinked resource). we then examined the result and verified that in both cases, the necessary details to construct the http request were correctly present. this experiment and its result details are publicly available at http://notes.restdesc.org/2011/images/."
"oriented request-find-provide business model for disassembly planning. for instance, for e-waste such as we have discussed, the disassembler will need to request help from information support services. when they are found, the appropriate information providers will provide the services. after that, the disassembler will need to request additional disassembly modelling and optimization services to try to optimize disassembly sequence planning."
the bit of muxbuf-1 in isr is '1': this means muxbuf-1 is faulty. this case is similar to the previous one. no data will arrive at channel-1 and channel-2 is utilized for both traffics. the same mechanism is applied on channel-2 when muxbuf-2 is faulty.
"the main control logic is encapsulated at the server side through java servlets, which represent different modules of the application. using modular design enables modifications and enhanced features to be added easily to the system to properly respond to specific user requirements. the back end layer is the database that stores and manipulates the power system data at the backend. the connectivity to the database from the middle layer is through the jdbc drivers, this allows the power system application logic to be written with little dependency on the type of the database used. in addition to the databases, the information service and legacy systems also comprise all the components necessary for undertaking the various types of power system simulations. once the user fills the necessary power system data in the html form and clicks the submit button, which posts the request to a java servlet. the servlet reads the input data and performs the power system computations, and the same time uses jdbc to communicate with a database to obtain the power system data; the dynamic response is then generated and given back to the client for display, depending on the user inputs. with the advent of the above web-based power system simulator, load flow simulation, short circuit fault analysis and harmonic penetration analysis have been carried out successfully [cit] . this simulator has a serious drawback of not having common format for interchanging power system data between tiers."
"web intents build on a long tradition of annotating links with plain words and media types. for instance, a priori meaningless web links can be given a defined meaning using hyperlink relations from the list of iana link relations [cit] and by providing a media type. a common example is annotating the link to a news feed using the alternate link relation attribute value and the atom media type:"
"semantic markup for web services (owl-s, [cit] ) is a wellknown service ontology. owl-s requires an additional description for the grounding, commonly wsdl, which results in owl-s inheriting wsdl's issues like verbosity and perceived complexity. the lack of semantic description of input and output parameters in wsdl is addressed by semantic annotations for wsdl (sawsdl, [cit] ). however, no functional parameter relation is established."
"disassembly sequence planning plays an important role in end-of-life treatment of ewaste. effective disassembly planning methods can improve recovery rates and reduce environmental impacts of e-waste. the disassembly sequence planning problem has been proven to be np-hard, and it has been widely studied in previous research. different meta-heuristic optimization methods have been proposed and implemented to solve the problem [cit] ."
"case 2 studies an lcd tv. figure 7 gives its exploded view and the top-level structure of its bill of materials. in an lcd tv, there are some valuable materials, such as plastics, iron (fe), copper (cu) and aluminum (al). there are also some hazardous materials, such"
"the most important realization, however, is that restdesc is not a technology for the future, but for today. starting from within web browsers-for instance, with web intentsrestdesc can deliver services on demand, precisely because it captures and exposes functionality. after all, the web for agents will not introduce a disruptive change, but rather be the result of an evolution that has already started."
"so the reader should see that disassembly sequence planning services can be invoked by taking advantage of the cloud, with the services delivered by different providers. an interoperable, service-oriented system for this can be realized by implementing our proposed framework. many valuable and effective optimization algorithms have been proposed to solve different kinds of disassembly planning problems in prior research. they can be developed for our proposed system and be invoked by disassemblers. more importantly, our mtlbo algorithm seems like a good candidate for this."
"with the help of weka's dbscan [cit] we build clusters of users that are highly dense in the user-tag space a. for this purpose we specify \" as the neighborhood radius of an instance and minpoints as the minimum number of points to be found in this radius to assign the instance to the present cluster. as usual, the metric for measuring the distance between two points is the cosine distance function. given the constructed clusters, for a target user u we check if he is assigned to a cluster. if this is the case and there are other users in the cluster, we get the items of these other members as candidates and remove the ones already seen by user u. the top-n items are then generated and recommended."
"nowadays, clients have to be programmed against a specific api and vocabulary, because they cannot decide autonomously what services and data they need. to remedy this problem, rest advocates the principle of hypermedia as the engine of application state (hateoas, [cit] ), which demands that a server supplies the possible next steps alongside each resource. that way, an agent does not need to know in advance how to use an api; instead, it can just \"follow its nose\" at runtime through these supplied hypermedia controls. but how can an automated agent understand what it means to follow such a hypermedia link? the goal of our approach is therefore to provide a machine-processable description of the functionality of hypermedia links, since functionality is the key differentiating characteristic between web services and consequently the crucial factor for automated decisions. restdesc will thus be a method to capture functionality, integrating rest infrastructure, services, and linked data-the three essential elements."
"however, as just noted, not all possible use cases can be foreseen. how can restdesc then anticipate on every possible api that will be described with it? and how will an agent, when it sees a restdesc pre-and postcondition, be able to know for what cases this api could be useful? the answer is provided by linked data. an ever-growing amount of structured content is being published on the web [cit] . recent web-scale structured data efforts such as facebook's open graph protocol [cit] or google, yahoo!, and microsoft's schema.org specification [cit] are intensifying this trend even more. these initiatives help make content-such as prices, locations, or movies-machine-accessible, closing the circle to interpretation of conditions:"
"http options -the http specification provides an options method, representing \"a request for information about the communication options available on the request/response chain identified by the request-uri \" [cit] . while the specification does not yet determine the response body, servers could use it to return restdesc descriptions via content negotiation. for example, an http options request to http://example.org/images/ could return the descriptions in listings 1 and 3. this method is resource-centric."
"we replicate the internal components of each port as well as the links and make 2-channel ports. replicating links removes the need of vc decoders at input ports. at the output ports we need only one level of multiplexing because of link replication. for each buffer at the input port we have a bit which shows the type of traffic inside the buffer, and the arbiters decide based on that bit and give the priority to the buffers which contain qos traffic."
"the item-based nearest neighborhood cf method aims to overcome the difficulty of finding similar users. the user's tagging behavior reflects the diverse and highly individual interests. items, on the other hand, tend to be annotated according to the concepts they represent. it is more probable that one can find items sharing the same topic. for the neighborhood computation, the item-tag frequency m data set is given as input to weka. for each target user u, it computes the k neighbor items that are most similar to each of the user's u items. each new candidate item receives one vote. if an item is already in the candidate set, we increment its voting with 1. given all candidate items, the top-n recommendation list is built."
"the selection mechanism of nsga ii is based on a fast non-dominated sorting approach and a crowding distance sorting approach, which guard the selection process against a uniformly spread pareto-optimal frontier. these two sorting approaches are implemented in this research."
"the operational and commercial needs of the power industry required information systems to not only perform traditional functions but also support many of the new functions specially to meet the needs of competition with deregulation. the rapid development of the internet and distributed object computing has opened the door for feasible and cost effective solutions. the choice of distributed technologies such as rmi, ejb, corba and .net remoting offers unique and powerful features such as zero client installation, on demand access and platform independence [cit] for the design of the on-line power system analysis architecture. with the evolution and wide spread deployment of the www accelerated by the rapid adoption of the browsers, web-based applications have been developed [cit] for a variety of applications in power system simulations. most of these applications use web techniques for information access and exchange. for example, internet-based client-server concepts are used to monitor transmission substations [cit] an internet-based energy trading system has been introduced which allows buyers and sellers of energy to freely engage in trading activities over a wide range of energy source products and geographical regions [cit] . with the help of high performance cpu and fast network communication more sophisticated applications have also been built on the web. internet is widely used in a distributed environment to share data and resources for parallel computing power system applications. an innovative application was developed using java [cit] for the interactive learning of the power system stability on the web. the remote power flow monitoring methodologies in a simulation environment were introduced [cit] and an educational tool for on-line power system analysis had been hosted in the internet. a webbased system had been implemented to support electronic commerce solutions for deregulated electricity markets by re-engineering the legacy energy management system (ems) software [cit] . a web-based electricity market simulator had been introduced, which can be used as decision making tool for market participants [cit] . in most of these cases java technology is used for the implementation. a prototype had been developed [cit] for distributed virtual reality system aimed at the training of operators working in power utility switching or distribution stations. it allows the user to exercise operations that typically consist of changing the topology of distribution networks by opening and closing the transmission lines, isolating equipments such as circuit breakers and transformers in order to perform maintenance or repair work, providing appropriate compensation or redistributing the load. job training simulators were developed for load dispatch, which is based on client-server model with user interfaces. an interconnected power systems laboratory had been setup that provides the students with a hands-on learning experience about the attributes and implications involved in the management and control of a small electric power system. it is desired to teach students how to analyze reallife, three-phase power system networks and show them the modern tools that are available to energy management system operators. one of the unique features of the ems emulators is that it incorporates the emerging technologies of client-server and industry standard networking. using the advanced technologies along with readily available hardware and modern software programming techniques, a customized graphics intensive laboratory environment had been created. this environment enhances the student's perception of electric power systems and their performance by graphically modeling the active control elements of the power system. most of the research works were being carried out in the development of training and educational tools for on-line power system analysis using the modern and current trends of information technology models are essential to the operation and control of modern power systems. the on-line operations models are typically more comprehensive than those used for power system planning. an on-line power system model has relatively complex set of information, involving between 100 and 1000 different classes of information. moreover, these models are closely tied in a distributed environment and loosely tied to a range of other information not related to power system computation throughout the utility. creation, maintenance and verification of models are significant activities for most power system engineers. the desegregation of utility functions and the introduction of new power systems have increased the burden of model maintenance in a distributed environment. this introduces new problems of model information exchange, interoperability among the power systems and exacerbates legacy issue problems such as version control and verification. in this context, it is worth revisiting the traditional modeling approaches to find improvements. to solve the problems associated with legacy power system applications, a data conversion model has been developed which represents the power system data into an xml form which is a common solution required for making the power system applications interoperable in a distributed environment."
"the recommended learning items take my tag preferences into consideration. 6. q6-perceived usefulness. i feel supported in finding learning items that i like with the help of the recommendation. 7. q7-attitude. overall, i am satisfied with the recommendation."
"the interconnect architecture in a chip multiprocessor becomes a single point of failure as it connects all other components of the system together. a faulty processing element may be shut down entirely, but the interconnect architecture must be able to tolerate partial failure and operate with performance or latency overhead [cit] . networks-on-chip provide opportunities to address this issue, as redundant paths exist from point to point, potentially allowing for reconfiguration around failed components. a network on chip (noc) is a high performance and scalable communication mechanism, for transferring data among the cores in a multi processor soc [cit] . the reliability of noc designs is threatened by transistor wearout in aggressively scaled technology nodes. wear-out mechanisms such as oxide breakdown and electromigration become more prominent in these nodes as oxides and wires are thinned to the physical limits. these breakdown mechanisms occur over time, so traditional post burn-in testing will not capture them. nocs provide inherent structural redundancy and interesting opportunities for fault diagnosis and reconfiguration."
"another important aspect for us is how the participants perceive the support that the recommendation algorithms offer in the context of learning q6-perceived usefulness. with a significant lead of 3.8 score, the user-based apriori seems to be the best recommendation approach that helps in the discovery of quality items for learning purposes. second best, scores the item-based lsa knn cf. all other algorithms shared similar perceived usefulness."
"lastly, we analyzed the overall satisfaction of the user with the recommendation provided by all seven algorithms q7-attitude. not surprisingly, the leader in terms of accuracy, novelty, and perceived usefulness; i.e., the userbased apriori algorithm, received the best score, followed by the item-based lsa knn cf that performed best in regards to diversity and consideration of users' tag preferences. the user-based k-means clustering and the item-based knn cf algorithms received ranks 3, and, respectively, 4. both classification approaches and the item-based k-means clustering received the same score of 3.0."
"perceived usefulness and perceived ease of use could be impacted by external variables considered in the original tam model. this study extends the original tam by integrating the original tam constructs with the new constructs: trust in government, trust in the internet, website design, perceived public value, and demographic variables (age, gender, education level) to test how this new adjusted model of factors influences citizen adoption of e-government services. hence the following new hypotheses are tested in this study:"
"amos 21 and pasw 18 were used to test the goodness of fit of the model. the overall model fit was assessed using seven indices: chi-square (x2), degree of freedom/chi-square (x2/df), comparative fit index (cfi), normed fit index (nfi), goodness-of-fit index (gfi), adjusted goodness-of-fit index (agfi), root mean square error of approximation (rmsea)."
"learning phase operator is used to generate the offspring population modified from the transitional population. all the solutions in q g are considered as the learners, who learn from each other and try to improve their results. each solution x o in o g is generated according to the corresponding learner (x), another randomly chosen learner (x a ) from q g, and a new randomly generated learner x r . the pseudo-code of learning phase operator is shown next."
index of potential recovery value f v (x). the potential recovery value means the value of reused components or the materials recycled form components. the reusable com-ponent can be all recovered while the recyclable component can be recovered at a rate depending on the material types and recycling conditions. this index can be computed as follows:
"our reliability mechanism is based on knowing precisely the location of faults in the switch. built-in-self-test (bist), a well-known approach to diagnose faults, has been extensively addressed as a post-silicon technique for fault detection during the noc lifetime [cit] . to utilize our recovery mechanisms, the network requires to go periodically into selftest in regular intervals and update isrs and osrs."
"in this algorithm, we use the association rule mining approach [cit] . we assume that each user represents a transaction while the tags he uses are the items of this transaction. we apply the apriori algorithm [cit] on the user-tag binary data set b after defining a minimum support, a minimum confidence, and the number of rules that should be learned. when all rules are found, for each user u we perform the following steps: for each rule whose head contains tags of the user u, we get the tags from the body of the rule that are new for u. we query the database for all items that are annotated by these tags and add them to the candidate item list. the target user u is then recommended the top-n items in the candidate item list."
"in recent years, the concept of personal learning environment (ple) has been widely discussed among tel researchers, as a natural and learner-centric model that supports the self-directed learning process by surrounding the learner with the environment that matches her needs best. ple-driven tel approaches have been proposed as an alternative to traditional learning management system (lms)-driven tel initiatives. while an lms adopts a knowledge-push model and is concerned with exposing learners to content and expecting that learning will happen, then a ple takes a knowledge-pull model. learners can create their very own environments where they can pull knowledge that meets their particular needs from a wide range of high-value knowledge sources [cit] . one concern with a ple-driven knowledge-pull approach to tel is information overload. it, thus, becomes crucial to examine some mechanisms that would help learners to cope with the information overload problem. this is where recommender systems (rss) can play a crucial role to foster self-directed learning."
"we focus on tag-based cf recommendation that rely solely on the tagging information. our approach differs from the state-of-the-art approaches in both the baseline methods used for comparison and the evaluation methodology. we will investigate, compare, and contrast a wide range of tagbased cf recommendation methods, memory as well as model based. and, we will apply two different evaluation methodologies, namely an offline evaluation by measuring the accuracy quality of the recommendation and a user evaluation to check whether the user satisfaction correlates with the measured high accuracy of the recommendation. based on both evaluation methods, we aim at highlighting the best tag-based cf algorithms for personalized recommendation supporting learners in discovering new quality learning items in their ples."
"the example makes clear that we depart from traditional approaches. by design, restdesc integrates with existing semantic web tools and practices. the important differences with other description methods are listed below."
"how does a client decide what service to use? clients start from the current situation and work towards a certain goal. for example, suppose we have a local image:"
"most technology adoption researchers have utilised behaviour intention to expect technology adoption. ajzen [cit] suggests behavioural intention has a direct effect on adoption. measurement of behavioural intention comprises of the intention, and predicted use of e-government services."
"the evaluation results show that it is inappropriate to rely solely on offline evaluation to gauge the effectiveness of tag-based recommender systems. we believe that user evaluation methods are more suitable when assessing the recommendation quality of tag-based collaborative filtering algorithms in ples. the perceived usefulness of recommended items depends on the learners' subjective opinion and background knowledge. statistically measured accuracy does not correlate with the individual understanding of relevant and interesting items. most probably, this is due to the fact that tags are instruments of natural language and hold semantics that cannot be captured completely by recommendation algorithms."
"first, using one or more of the methods in subsection 3.3, the agent discovers what functionality it has at its disposal. it retrieves several descriptions, including listings 1, 3, and 4, which it can obtain by issuing an options request to the uri http://example.org/. using a reasoner, the agent can find out that listing 1 describes a way to obtain a thumbnail, on condition that the image has a smallthumbnail link. the reasoner also devises that such a link can be obtained by uploading the image, as described by listing 4."
"without understanding what factors influence the public to utilise e-government services, a government will be incapable of devising a strategy to expand up-take or e-government services [cit] . however, understanding of citizen adoption of egovernment services is currently lacking due to the following:"
"older users might have inadequate experience using computers and the internet, hence it is possible that they might have self-efficacy worries regarding learning how to use services offered online. in a similar manner, older users tend to have self-referent opinions about perceived changes concerning their performance competencies due to aging."
the set of all the pareto optimal solutions is called the pareto-optimal set (s * ) and the set of all the pareto optimal objective vectors is called the pareto-optimal frontier (f r * ) stated as follows:
"in the following sections, we present the evaluation details of the different tag-based cf recommendation algorithms outlined above. the data set we have used in our experiments is the plem data with 83 users, 773 items, and 1,005 tags in their stemmed version. the constructed user-tag matrix is a 83 â 1;005 matrix with a sparsity level of 0.9823, and the item-tag matrix is a 773 â 1;005 matrix with a sparsity level of 0.9968. the sparsity level is defined as the relation 1 à nonzero entries total entries [cit] . we apply two different evaluation methodologies, namely an offline evaluation and a user evaluation."
"recommender systems can provide a potential solution to overcome the problem of information overload. generally, recommender systems aggregate data about user's behavior or preferences to draw conclusions for recommendation of items she most likely might be interested in. technically, recommender systems are classified into the following classes, based on how recommendations are made [cit] :"
"while n offers an integrated solution for quantification with a strong logic basis and a minimal extension to rdf, some clients might not be fully compatible with it. our answer here is twofold: first, the clients themselves do not need to consume n, as the reasoner is the component carrying out the n-related transformations and providing the client with instantiated representations in plain rdf format. secondly, powerful n consumers such as cwm [cit] and eye [cit] exist for several years now, the latter being compatible with a broad spectrum of other web logic languages, thanks to its interoperability with the wc rule interchange format (rif [cit] )."
"on the other hand, nocs significantly affect performance, latency, power and area of the chip. noc latency reduction is essential for soc performance as it is introduced to every communication stream within the soc. latency may become vital in the case of socs with critical timing demands (realtime socs). priority-based noc architectures are a well-known approach to provide quality of service (qos) in networks and to reduce the latency of certain packet classes whose priorities is high [cit] . virtual channels (vcs) over a physical channel have statically assigned priorities [cit] : high priority vcs are used for qos traffic classes and the low priority vcs are used for normal traffic classes."
"for test 1, the acquired non-dominated solutions can be plotted in three dimensions with the axes denoting the three objective function values, as illustrated in figure 4 (a). for"
"the second step is to build user or item tag profiles. to build the profiles, we investigate the frequency of tag applications of a particular tag."
"each solution x q in q g is generated according to the corresponding learner x and a teacher x t . a precedence preservative operator is applied to maintain the precedence relationships in the feasible solutions when they are being modified. the pseudo-code of teaching phase operator is outlined as follows, with m defined as the number of disassembly operations (length of a solution)."
"different considerations have led stakeholders to pursue different objectives for disassembly planning. for instance, according to the waste electrical and electronic equipment directive directive, e-waste regulators need to check whether an end-of-life treatment operator is able to recover (reuse and recycle) at least 75% of the weight, and remove all of the hazardous materials. the components containing hazardous materials need to be removed from the equipment for further processing. apart from fulfilling these fundamental environmental objectives, remanufacturers want to improve the economic efficiency by prioritizing the valued components during disassembly. we develop a multi-objective disassembly sequence planning model according to these considerations by introducing three disassembly indices as shown in figure 1 . the multi-objective disassembly sequence planning model involves different e-waste stakeholders distributed across different areas. it is a complex business process that needs to be optimized, and the enabling technologies to support e-business engineering need to be implemented to handle it through the internet."
"an mtlbo algorithm is proposed to solve the multi-objective disassembly sequence planning problem. to make it applicable for the problem with complex disassembly precedence constraints, a feasible solution generator was designed to generate feasible solutions, and teaching phase and learning phase operators were designed to improve the solutions by applying the precedence preservation cross-over operation method. to sort the population for selection, a fast non-dominated sorter and a crowding distance sorter were designed according to nsga ii."
"to see the effect of faults on the network, we injected faults on the entire 8x8 noc. our fault model is based on the area and covers seven different components (rc, muxbuff, muxrc, buffer, arbiter, out-mux, link) of each channel in each router. we injected faults on the network based on the area portion of each component in the network. those components that have more portions of the total noc's area have more probability to receive faults."
"perceived public value (h19, h20) which is considered the user's overall evaluation of the usefulness of a product based on the user's opinion of what is received, and what is given, had a substantial effect on users' beliefs (perceived usefulness, and perceived ease of use). higher level of perceived public value, would predict a higher level of users' beliefs about using e-government system. government must concentrate on how to increase the value for the public from using egovernment services in order to attract more people to use their online services; also e-government strategy must pay more attention to the requirements and expectations of users in developing e-government services."
"the initial step of our user-based lsa knn cf algorithm is to find the latent topics in the data set, based on the cooccurrence of tags in the original user-tag frequency data set a. for this purpose, we utilize weka's latentsemanticanalysis. the low-rank approximation of the full data requires a rank r parameter that specifies the proportion of total singular values to be used. once we have established the lsa attributes of the lower dimensional semantic space a 0, the nearest neighborhood search is performed on this data set. the further steps of candidate items discovery and the generation of the top-n items follow the same logic as in algorithm 1."
"similarly, [cit] decompose the tensor into user-tag and item-tag matrices and use them to find latent topics among users/items and to cluster these according to their neighborhood. the outcome is a predicted score based on the ratings of similar users, as well as the similarity to other items."
"every two connected routers in relinoc send the information of their corresponding isr and osr to each other by dedicated signals. each router receives two signals from its neighbour per each channel. one shows the status of the neighbour's input channel and another one is for the output channel. we call these signals input-channel-faulty (icf) and output-channel-faulty (ocf) respectively. icf and ocf signals can affect on osr and isr registers. if router a receives an active ocf signal from router b for a specific channel, this means that router b never sends data to router a on that channel. this is equal to having faults on the input link of that channel in router a and, therefore, router a sets the \"link\" faultiness bit in the corresponding isr to '1'. if router a receives an active icf signal from router b on a specific channel, it means that router a should not send data on the corresponding output channel. this is equal to having faults on the output link of the related channel in router a and, therefore, router a sets the corresponding bit in its osr to '1'."
"in addition to the synthetic traces, we also performed simulation to see the effect of faults on 5 real traffic traces from parsec benchmarks, a suite of next-generation sharedmemory programs for cmps [cit] . the traces used are for a 64-node shared memory cmp arranged as a 8x8 mesh. each processor node has a private l1 cache of 32kb and 1mb l2 cache (64mb shared distributed l2 for the entire system). there are 4 memory controllers at the corners. to obtain the traces, we used virtutech simics [cit] with the gems toolset [cit], augmented with garnet [cit], simulating a 64-core noc. like the previous experiments we chose those fault configurations that give us fully-connected networks, injected faults and ran the simulation for each benchmark trace. each trace contains two types of packets: data and control. we considered control and data as qos and normal packets respectively. figure 5 shows the relation between latency and number of faults for each benchmark and for each packet type. as can be seen, in real traffics relinoc can recover up-to 50 faults with a very low penalty on latency in both traffic types. in all the benchmarks and for 50 injected faults, the maximum latency penalty on control and data packets is 10% and 40% respectively."
"the data generation and preprocessing task is performed in three main steps. first, we generate the list of attributes based on the user tags in the system. the second step is generating the user profiles, or, respectively, the item profiles. the third step builds upon the data from the previous two steps to create the training data set for weka."
"the semantic web has arrived but its envisioned main consumers, the intelligent agents [cit], are still missing. to find causes for this important void on the web for agents, we should examine the essentials of which this web consists:"
"deregulation is, and will continue to have, a tremendous impact on power system analysis and operations. in many regions deregulation has resulted in  manuscript received august 28, 2013; revised december 1, 2013. the creation of much larger markets under the control of an independent system operator. this will result in even more buses and other devices to monitor and control. simultaneously, the entry of many new players into the market and the increase in power transfers will result in even more data to manage. finally, system operators will come under increased scrutiny since their decisions, such as whether to curtail particular transactions; can have a tremendous financial impact on market participants."
"social recommender systems address the issues of traditional recommendation by taking advantage of the social data about a user, i.e., his tagging behavior, relationships, membership in communities, likes, comments, votes, bookmarks, and so on. this data implicitly represents preference about certain items or additional contextual data for rich media. augmenting the traditional collaborative or contentbased recommendation with this information can lead to significant improvement."
"behavioural intention was found to have a significant positive effect on individuals\" actual use of e-government portals (h20). tam originally hypothesises that, actual systems' use is directly determined by behavioural intention to re-use."
"equation 2 states that, given the preconditions and the fact that the request has been performed, we obtain the postconditions. however, this equation does not always hold in practice: any actual request could fail to deliver an adequate response for several reasons, invalidating the above implication in the general case. therefore, a better interpretation of equation 1 is:"
"since n possesses the full power and expressivity of firstorder logic [cit], we can write equation 3 in the n language, which is the essence of restdesc. listing 1 shows indeed an example of this: the precondition (having a smallthumbnail link) means that a request exists (an http get to the smallthumbnail link), which entails a postcondition (receiving an image with height 80 pixels in the response body). the general skeleton is displayed in listing 2."
"the users were asked to evaluate to what extent the provided recommendation incorporate their tag preference (q5-context compatibility). the item-based lsa knn cf, the user-based apriori, and the user-based clustering achieved the best acceptance. the user-based lsa knn with cts generation and nbc algorithm is the only algorithm below the average with a score of 2.8."
"tra is a well-accepted and widely studied intention model that was utilised successfully to clarify behaviour across a vast variety of settings [cit] . according to tra, an individual's behaviour is best predicted by his/her behavioural intention, in turn determined by the person's attitudes and subjective norm. tra scope disregards a broad range of behaviours, for example impulsiveness, spontaneity, cravings, or even mindlessness because performance may be involuntary or because the actor is not intentionally engaging into these behaviours. in this research study related to e-government, users require certain skills, experiences and base knowledge of the of internet technologies to enable having obvious behavioural intention to accept e-government services, hence this model was not found suitable as an adoption model for egovernment."
"according to studies, individuals with less academic education report inadequate knowledge as one of the major reasons why they don't choose to use the internet [cit] . figure 1 below shows the proposed model with the hypotheses: this study examined the different factors and their hypotheses indicated previously. majority of the questionnaire items testing the hypotheses were customized from previous studies [cit] . the five-point likert scale was used to measure responses to the questions. since english is not the first language of the respondents, who don't have english fluency, the questionnaire was translated into arabic. for validation, reverse translation was done, translating the questionnaire from english to arabic first and then from arabic to english."
"the second model-based approach applies topic clustering of items as a solution to the synonymy problem of tags (i.e., two different tags have a similar meaning). the idea bases on the assumption that co-occurring tags will be interpreted as belonging to the same topic. therefore, [cit] perform topic clustering of items by using expectation maximization (em) clustering method. then, each item is represented by a topic domain vector, where each entry gives the probability that the item k belongs to the topic j. the probability is calculated based on all tags annotating the item k. the algorithm proved that it can recommend items even for new users with few bookmarks. the proposed approach was evaluated by comparing it to three other algorithms: 1) based only the topic domain, 2) using the most popular tags of the currently viewed item, and 3) matching the topic domain vector of the viewed item to the bookmarks of all users with commonly shared items. the proposed algorithm performed best by achieving a precision of 0.594."
"thirdly, the dependency on semantic web technologies means that restdesc inherits some of the limitations of rdf. for instance, rdf does not provide a native way to state that a resource does not exist. this is a consequence of the open world assumption: the absence of a certain triple does not necessarily indicate its inexistence. therefore, there is currently no direct way to explain a delete request. even if there was, it still would carry a danger of contradiction in the first-order logic model, because a resource must exist before you can delete it-but if you delete it, it does not exist anymore. however, we believe that workarounds are possible, such as introducing a \"deleted\" flag, or the notion that future requests will result in a 410 gone status code [cit] . but most importantly, the semantics of delete are already fully defined by http itself, so there is most likely no need to redefine them in restdesc in the first place. still, we have to be well aware of those limiting model properties."
"tam hypothesises that the up taking of a new information system can be expected based on users' behaviour intention (bi), attitude towards use (att), and two other factors: perceived usefulness (pu) and perceived ease of use (peou). davis [cit] defined perceived usefulness as \"the degree to which a person believes using a system provides all the required information\"; perceived ease of use as \"the degree to which a person believes that using a system would be free of mental effort\"; attitude as \"individual's positive or negative feelings (evaluative affect) about performing the target behaviour\"; behaviour intention as strength of one's intention to do a certain behaviour (for innovation), and actual system usage."
"hence, more empirical studies are required in the area of egovernment adoption to help governments to improve their comprehension of the factors, which affect citizen adoption of e-government services. in addition, a thorough examination of the e-government adoption studies reveal that a bigger percentage of published literature was conducted in developed countries. hence, scarce material is available about the factors influencing e-government adoption in developing countries."
"to overcome the limitations of traditional cf recommendation, a significant amount of research has been done recently in trying to generate recommendations by harnessing the interactions in social media as they can reveal further user preferences. formally, these approaches introduced a new type of rs, namely social recommender systems (srs)."
"further experiments will be necessary to indicate which of the suggested methods will work best in practice. additionally, all of the above methods can be used in conjunction without interference, giving agents as many options for discovery as they need. in any case, these discovery mechanisms indicate that restdesc can fulfill the needs of generic clients without bindings to specific apis."
"suppose there are m learning subjects offered to n learners. in each generation g, the grade that learner i gets in subject j is denoted as"
"subsequently, the client can execute this request and upload the image to the server as instructed. in response, the client retrieves a representation of this image, containing indeed the links indicated by its description in listing 4. for example, the actual image may link to comments and thumbnails as follows:"
"a first issue is the responsibility for creating restdesc descriptions. this is indeed a manifestation of the chicken-andegg problem that many new semantic web developments face: widespread support makes a technology used, but support only evolves in case of widespread use. we aim to counter this argument by making the threshold to restdesc as low as possible by its innate simplicity and consiceness, adoption of existing technologies, and availability of community support. additionally, we plan to bootstrap the process by providing descriptions for popular web apis."
"it is important to note that the data set used in our study was generated within the plem environment. the methods of the study can, however, be applied in other (tel) environments, where users manage and tag items. from a technical perspective, this should be possible because the recommendation methods that we implemented in this experiment just require as input a user-tag matrix and an item-tag matrix that can easily be built from any available tag-based data set."
"web intents [cit], a project started by google developer advocate paul kinlan, is driven by the observation that users expect web applications to work together seamlessly. kinlan states that, for example, a photo gallery web application should be aware of a user's preferred third party photo editing web application, rather than enforcing the specific one that the gallery happens to be integrated with. web intents therefore proposes client-side service discovery and inter-application communication. initially, services register their intent, i.e., their ability to perform a certain action on a certain media type (listing 5). thereafter, applications can request to initiate such an action (listing 6). as a result, the web intents client then starts the desired service for the user. the developer teams behind the firefox and chrome browsers have committed to provide native web intents implementations. meanwhile, full compatibility with all major browsers is provided by a public javascript library."
"this fig. 6 indicates the power system data compression process by which the input data is compressed, stored or sent through a network to another location, where it is decompressed and the output data is obtained. the proposed compression and decompression model has been implemented in rmi based online load flow monitoring in a distributed environment. these power system clients are interconnected with a load flow server is shown in fig. 7 . a client computer basically does the distributed power system monitoring through an applet for every specific period of time and frequently exchanges data with the server. in the client program, the data is read from the input file, stored in a double array and converted to binary form. the data is then compressed and the compression is carried out by xoring the value with the most common predicted value and compressing with leading zeros. the compressed data received from the client by the load flow server and it decompresses it. the server does the load flow computation and then distributes the results in a compressed form. chronologically the server process should be started first so that it can take the initiative to set up a connection link. it then starts waiting till it receives a connection request from the client. a client can register itself with the remote object (server object) just by invoking the registration procedure on the server object, when it needs a service from it. the remote object obtains the necessary data from the registered client objects and responds back to them respectively with the results. this total process can be automated by making the server get the input data for every specific period of time. transaction of data between clients and server takes place several times and so the possibilities of the occurrence of errors may be high. hence it must be handled properly. when a remote client object registers with remote load flow server object, the server uses the remote client reference to invoke its method to obtain the system data from that client and then provides the service through its methods. both client and server objects are considered as remote objects and hence inter-remote object communication is achieved. the server object uses a single thread of control to distribute the load flow results simultaneously to the clients registered with it. subsequently, the following steps are to be carried out."
"as e-government websites develop to be the major medium for online communication between government and citizens, designing user-centred websites will be a necessity for governments [cit] . bertot & jaeger [cit] claim that accessibility is one of the vital requirements in creating beneficial usercentred e-government services. studies reveal the significance of well-presented material on government websites in confirming citizens' satisfaction with the offered services [cit] . without suitable assessment of web-based e-government services, some e-government advantages, e.g. speedy access to government services and cost decrease cannot be guaranteed."
"a simple answer, however, does not imply a simple solution. we therefore do not claim to have found a definite entrance to the agent-enabled web. what we do claim is to have identified functionality as the central concept and a way to closely connect the three aforementioned elements. we define functionality as the means that enables agents to use the http rest infrastructure (a) to learn what a specific service (b) can do with linked data (c)."
"utilizing the properties of the proposed 2-channel router, relinoc switch is introduced to ensure fault-tolerant operation of the noc. in this section, we explore various possible failure modes within an noc router, and propose detailed recovery schemes with minimum area cost. relinoc router architecture possesses some inherent fault-tolerance due to its replicated design. this additional operational granularity may be utilized to allow replacement of a faulty component by another one, thus allowing operation of the router with latency overhead instead of a complete breakdown. five major components of the router i.e. routing computation units (rc), arbiters, buffers, output muxes, and links, are susceptible to permanent faults. we propose relinoc switch to overcome faults in all these components. the proposed switch architecture is shown in figure 1 . note that, only input channels of west port and output channels of east port are shown in the figure. four types of components are added to the baseline 2-channel switch for fault tolerance purposes: 1) two muxes per each input channel 2) 5-bit input status register per each channel 3) one control logic for both channels 4) 10-bit output status register for the entire switch."
"the internet is basically a system allowing computers to communicate with each other. with computers diminishing in both size and price there is a tremendous growth in the number of computers used. as they became wide spread, new ways to harness their potential are constantly being developed."
the above compression/decompression algorithm has been implemented in windows xp-based hp workstations connected in an ethernet lan. the major factor that influences the performance of the proposed model is the round trip time (rtt) that includes the convergence time. the round trip time measures the time needed from the point when the power system client initiates a method invocation to the point when the client receives the results. the round trip time is measured for all the power system clients that invoked the load flow method simultaneously without any delay. the performance analysis of the proposed data compression model has been carried out with compression and without compression of various power system data. the variations of round trip time with respect to the number of buses are shown in fig. 8 . from the above graph it is evident that as data size of compressed and uncompressed data for small power systems is almost similar and hence their rtts are also similar.
"the crucial step in srs is to decide which social data to exploit and how to use it to achieve better quality of recommendation. regardless of which social information is used, srs share few common characteristics:"
"a three-tier architectural model is proposed for power system applications which contains a service tier, a data store tier and a man machine interface (mmi) tier as shown in fig. 1 . to overcome the limitations of two-tier architecture, a middle logical tier was added between the presentation tier and the data tier. this is where the power system application logic resides and performs a number of different operations. the middle tier can be physically located on a separate machine but this is not always necessary. in a power system application where the amount of clients is limited, the power system application server can be deployed on the client machines or on the database server, thus reducing the number of interface connections required and making the system faster. as the number of power systems clients grow, the middle tier can be moved onto a separate machine which addresses the desired scalability benefits."
"the standard interaction patterns carry well-defined semantics in web intents. however, many of today's popular services [cit] offer different interactions that those patterns do not cover. some services offer simple operations on currently uncovered media types-for instance, services that act on location data, such as a weather forecast service or a map service. other services offer more complex operations on already covered media types-for instance, a service that recognizes people in an image. centralized api catalogs like programmableweb.com let people categorize and document services, much like in the early days of world wide web search engines. still, in order to discover those services, manual intervention is necessary, because their actions or media types are currently outside the web intents scope."
"to compare the performance of mtlbo algorithm with that of nsga ii, we conducted an additional test to assess it relative to test 2, as follows:"
"from a recommendation point of view, there are several directions for improvement. first, we can enhance the way the user profiles are built. for example, we can ignore tags that occur rarely. second, we can improve also the item profiles. this can be done by extracting tags for a given item from third-party services like del.icio.us. furthermore, the problem of polysemy and synonymy of tags can be reduced by leveraging a dictionary like wordnet, wikipedia, or freebase to compute whether two tags are semantically similar."
"where section 2 introduced restdesc through an example, this section aims to provide a rigorous description of what restdesc descriptions are and how to create them. basically, the functional description provided by restdesc describes the result of an action a on a resource r. more formally, it explains: given a set of preconditions pre a on this resource r, what request request a is necessary to obtain a set of postconditions post a for that action a:"
service-oriented technologies promise a way to create the basis for agility so that companies can deliver flexible business processes [cit] . they are mission-critical supporting technologies for e-business applications. the disassembly and recovery of ewaste involve complex business processes across the lifecycle of their production and use.
"attitude had a significant positive effect on intention to use e-government services (h3). the findings demonstrated that the intention of citizens' to utilise e-government services is mostly affected by their attitude towards using these services and less so by perceived usefulness. however, prior research has suggested that the inclusion of attitude is not meaningful but this paper suggests otherwise, due to the voluntary nature of citizens\" adoption of e-government services. voluntary users shaped their intentions to utilise the systems mainly based on their attitude toward using the system. this result confirms the importance of attitude in the tam model, as this variable was removed from other versions of the tam model."
"we conducted simulations to test the performance of the mtlbo algorithm to solve the multi-objective disassembly sequence planning problem. all tested algorithms were coded with c++, and carried out on a pc with a 2 ghz intel core2 duo cpu t5750 and 2 gigabytes of memory."
"the results of test 2, 6 and 7 are illustrated in figure 6, showing that the performance is improved when g max is increased. we found 27 non-dominated solutions in test 6, 31 in test 2, and 33 in test 7. the convergence to pareto-optimal solutions improved as g max increased from 100 to 500 to 1000. test 7 had a better spread of solutions than tests 2 and 6 did. thus, the reader should recognize that it seems like a larger g max values support better convergence and solution spread."
"the aim of the study is to experiment with different tag-based cf recommendation algorithms, memory based as well as model based, in a ple context. in the case of memory-based tag-based cf we use the k-nearest neighbor method (knn). for the model-developing algorithm, we focus on the following approaches: dimensionality reduction, probabilistic classification, clustering, and association rule mining. further, we consider for most algorithms to perform user-based and item-based cf. for the user and item similarity computation, we utilize the cosine similarity metric."
"in the present study, trust in e-government comprised of two dimensions, namely trust in government (h13, h14) and trust in internet technology (h15, h16). all these hypotheses were supported. high levels of trust in government, and trust in internet would directly predict a higher level of users' belief in using e-government system, which means that citizens would not anticipate gaining any usefulness from egovernment services if thy do not see them as trustworthy. the government in egypt shows inadequate collaboration between its entities to create laws and regulations related to ict usage, standardizing system use, and sharing information (according to interview results explained in the consequent chapter). these issues should be considered by the government in egypt in order to increase citizens' trust and have them more enthusiastic to use government websites."
"so information support services, disassembly modeling, and optimization services need to be integrated together when they are used in computer networks, such as the internet. we proposed a service-oriented disassembly sequence planning framework to provide a comprehensive and standardized service-based environment for distributed information sharing between the producer and the disassembler. our framework also provides a service-oriented environment for disassembly modeling and optimization."
"the objectives in a multi-objective optimization problem are contradictory, namely, when one of the objective functions is improved, the others will be affected and get worse. hence, it is impossible to achieve the best results for all of the objective functions. the best trade-offs among the multiple objectives can be defined in terms of pareto optimality."
"further, we were interested in the testers' experience specifically with tag-based recommendation: 67 percent agreed to be familiar with it, while 27 percent disagreed. lastly, the majority of users consider it useful to have recommendations in learning environments based on the behavior of like-minded learners (see fig. 8 )."
"the proposed mtlbo algorithm is different from the tlbo algorithm. first, the generation methods for the initial population are different. in the mtlbo algorithm, the initial population is generated using the feasible solution generator, hence each individual is a feasible solution, while in the tlbo algorithm, the initial population is generated randomly without considering the individuals feasibility. second, the realization methods for the evolutionary mechanism are different. in theby mtlbo algorithm, learners are modified using both the teaching and learning phase operators, applying the precedencepreserving cross-over operation. so the disassembly precedence constraints will be retained in the modified solutions. in contrast, with the tlbo algorithm, the learners are modified using multidimensional matrix computation. third, the fast non-dominated and crowding distance sorting approach are implemented in mtlbo, making it suitable for the multiobjective disassembly problem, while tlbo is used to solve single-objective optimization problems."
"the problem is further compounded if site monitoring involves a number of recording units. as the demand for more data storage space and data transfer continues to grow at an unprecedented pace, the need for real time data compression systems becomes more prominent. the essential criteria for compression are that the storage occupancy of the source data is sufficiently reduced such that the apparent integrity of the data is not adversely affected by the reduction method. by identifying redundancy in a data set it is possible to reduce effective size. in this paper, the compression of the power system model has been proposed which obtained from real online power system applications. performance comparisons of the proposed method for network operation and control are also presented."
"in a ple-driven approach to learning, there is a crucial need for recommendation methods to help learners find quality knowledge nodes (i.e., information and people) that can populate their ples. in this paper, we investigated the application of tag-based cf recommendation methods to recommend learning items in ples. we implemented and experimented with 16 different tag-based cf algorithms, memory based as well as model based. we conducted an extensive offline and user evaluation to contrast and compare the different algorithms in terms of accuracy and user satisfaction. the results of the evaluation carried out confirm that the quality of user experience does not correlate with high-recommendation accuracy measured by statistical methods. [cit] . he is an assistant professor of computer science in the learning technologies group (informatik 9) at the rwth aachen university, germany. his research focuses on web information systems, technology-enhanced learning, and knowledge management."
"a. start load flow server b. the server should invoke its own registry service. c. register a load flow client and load a server's stub dynamically from the common location. d. load flow server uses the load flow client's reference to receive the compressed load flow data from the power system client periodically. e. the server un-compresses the power system data and computes the load flow and returns it to the client. f. the client obtains the result through a load flow stub. g. for every specific period of time, the server automatically receives system compressed data from the client, thereby providing automatic load flow requirement evaluation."
"based on the item-tag frequency data set m, we build k item clusters, using weka's simplekmeans clusterer. for each item i of the target user u, we obtain the neighbor items from the cluster m the item i was assigned to as recommendation candidates. from the list of candidate items, we generate the top-n items to be suggested to the user u."
"the results of the evaluation in this experiment revealed that the item-based k-means clustering was the best performing algorithm in the offline evaluation whereas the userbased apriori algorithm was ranked first in the user evaluation. the major finding in this study is that the offline evaluation of recommender systems does not always correlate with their user evaluation. however, due to the small sample size (83 users, 773 items, and 1,005 tags), it is not possible at this stage of research to generalize that the item-based k-means clustering and user-based apriori are always the best performing algorithms in tag-based cf recommendation tasks. to draw similar conclusions, it is necessary to apply the recommendation techniques used in this study on other (educational) data sets, such as delicious, mace, remashed, mendeley, and pslc datashop [cit] ."
"the bit of rc-1 in isr is '1': this means rc-1 is faulty. clearly, since rc and muxrc are driven only by the header flit, their utilizations are relatively low compared to the flitby-flit operation of per-flit components like buffers. thus, rc of one channel can be shared during its unloaded periods with another channel. however, this needs a more complex control logic since two packets may arrive at the same time. to avoid this, we do not share rcs between channels and, therefore, consider channel-1 as faulty. the same mechanism is applied on channel-2 when rc-2 is faulty."
"the bit of buffer-1 in isr is '1': this means buffer-1 is faulty and can not be used any more. in this case we consider channel-1 as faulty. as an icf signal is already sent to the previous router, no data will arrive at channel-1 and channel-2 is utilized for both types of traffics. thus, control logic does not change control signals for muxes. in this case rc-1 also remains idle. the same scheme is applied when buffer-2 is faulty. note that, even in these cases the qos traffic has priority over normal traffic because of the priority bit."
index of potential recovery weight f w (x). the potential recovery value means the value of reused components or the materials recycled form components. the reusable component can be all recovered while the recyclable component can be recovered at a rate depending on the material types and recycling conditions. this index can be computed as follows:
"directly applying the evolution mechanism from the tlbo algorithm is not suitable to solve the disassembly sequence planning problem, however. in this problem, the decision space grows exponentially in the number of disassembly operations, while the disassembly precedence constraints are complex. these characteristics will cause there to be few feasible solutions for a population if a random solution generation method is embedded in tlbo algorithm. furthermore, the disassembly precedence constraints cannot be honored when using an arithmetic operation method during the evolutionary iterations needed to achieve an optimal solution. this will lead to only a few feasible solutions in the offspring. hence, the evolutionary mechanism of tlbo algorithm should be modified to suit the disassembly sequence planning better."
"an important benefit of restdesc is that common n reasoners, such as cwm [cit] or eye [cit], can perform the above deduction in a completely automated way, without requiring any plugin. indeed, because restdesc descriptions are n rules by design, they seamlessly integrate with current semantic web tools. reasoners are able to directly instantiate restdesc descriptions for a concrete case. for example, the reasoner can combine listing 4 with the starting situation above to obtain the concrete request, filling in the variables with the actual values. the result is in plain rdf format, without quantifications, and contains all the details the client needs to issue this request. note that the reasoner does not need access to all data in each step. instead, it runs initially with the starting context, the goal, and the available descriptions, to generate an execution plan. in further steps, only the actual context and this plan are necessary."
"a. web-based power system simulator-design and implementation the web-based power system simulation architecture had been developed [cit] using three-tier client/server architecture as shown in fig. 2 . the first step in designing a web-based power system simulation environment is to define its functions. the power system simulation functions are creating and editing of the power system networks and the execution of various simulation analyses. the support for each function will span across multiple tiers with components responsible for specific roles. the motivation behind this design is to minimize software coding efforts by reusing many of the existing simulation routines. the power system application logic is separated from the gui and the backend database, so that changing any one of the layers can be done without the need to change other layers. the top layer is the presentation layer, where the client machine is located. data is presented by a thin client solution, using a web browser and the html standard file format. the parameters of the application are controlled by the client side using html forms. the user sets the parameters and then activates the process on the server side via the network."
"\"trust in government\" can be defined as the public's evaluation of government according to their perceptions of political authorities, institutions' and agencies reliability and competency to offer services in accordance with citizens' expectations [cit] ."
"based on the values in isr, control logics generates appropriate signals for muxbuff and muxrc of each channel. the information in osr is used by rc logic to specify the appropriate output channel for each packet. depending on the status of isr and osr, several different cases exist for each channel and each direction of the router. in the following paragraphs we outline these cases for the east direction and its two channels (e1 and e2) and describe our fault recovery schemes for each situation. the mechanisms for other directions are exactly the same as those of east."
"1. attributes generation creates the list of attributes that characterize the data set. in plem, these are the user tags used for the annotation of a given item. 2. training set generation builds the data set used for learning the data patterns. 3. learning the predictive model specifies the classifier or clusterer to be used for building the respective model. 4. recommendation for the active user by using the learned predictive model performs the recommendation generation. our implementation of all recommendation algorithms follows the above-described steps."
"the crowding distance (p distance ) of p is calculated as the sum of its individual distance values (p distance,i ) corresponding to each objective i:"
"©2014 engineering and technology publishing make this implementation transparent to the power system client which is located at geographically different location. the main advantage of the two-tier model is better reuse, where the power system application logic is placed solely on the server and it can be initiated from many power system clients. the client/server model has practical drawbacks that often outweigh its economic advantages. developing client/server programs is three times more expensive than traditional power system applications. it is more difficult to program a distributed application that has a richer set of error and failure modes than a centralized one. another problem is that many applications do not easily decompose into pieces that can run on separate machines connected by a relatively slow (compared to memory speeds) communication medium. any upgrade that requires replacement of the client machines is cost prohibitive."
"as shown in figure 1, two muxes are added to each channel, one at the entry of buffer (muxbuff) and another one at entry of rc (muxrc). the muxes are added to enable each channel to access both buffers and routing computation units. there is a 5-bit register for each input channel storing the faultiness status of five different components in that channel including: link, muxbuff, muxrc, rc, and buffer. we call this register isr (input status register). one simple control logic reads the isr and generates appropriate control signals to muxbuff and muxrc based on the faultiness of each component. finally, there is a 10-bit register for the entire switch tracing the faultiness of all output channels in that switch. we consider one output channel as a faulty channel if there is a fault in at least one of the following components of that channel: arbiter, out-multiplexer, link, and the input channel of the corresponding next router. we call this register osr (output status register). fig. 1 . relinoc switch. each direction has two channels which are used for qos purposes in normal operation and for reliability purposes in case of faults."
"leaving the example and more generally speaking, it is obvious that no user agent (including, but not limited to web browsers) can be prepared for the functionality of every web api and can have foreseen a mechanism to deal with it. when we think of various apis-such as currency conversion, weather forecasts, or movie rental-we see that they all have well-defined pre-and postconditions, where the execution of a particular api request is the state transfer from pre to post. since restdesc is based on the formal description of pre-and postconditions, it is an adequate candidate to capture these interactions."
"perceived usefulness is revealed as the most important factor affecting citizens\" attitude (h1) (directly) to adopt egovernment services. this gives evidence that, comparable to other technology focused services, the observed benefits resulting from using them are the main reasons for citizens to adopt e-government services. if the e-government system is seen to be useful, the users will have positive attitudes and strong intentions for using the system. managers responsible for providing the online public services should consider that in order for potential users to adopt a service, they have to perceive its usefulness first."
"to see the effectiveness of relinoc router with respect to the vc-based router, we measured the average latency on 8x8 mesh networks of both routers. we applied different synthetic traffic patterns including unified random, butterfly, transpose, and bit complement on both networks. we injected two classes of traffic to the network including qos and normal, and measured the latency of both traffic types as a function of packet injection rate of both traffic classes. the results for unified random traffic are shown in figure 2 . in this 3-dimensional plot, the x and y axes represent the packet injection rate (pir) of normal and qos traffic respectively. the z axis shows the latency. the surfaces in figure 2 are the latency of qos and normal traffic. as seen, in both routers the pir of normal traffic does not have effect on latency of qos traffic. this is because of the arbitration policy in the router which gives high priority to the qos traffic. however as can be seen, the latency of normal traffic in relinoc is much less than that of noc with 2-vc switch. as shown in the plots, increasing pir of qos traffic has much less impact on latency of both traffic types in relinoc compared to that of noc with 2-vc switch. as seen, the latency of both qos and normal traffic has been reduced by 30 to 50 percent."
"conventional power system applications are, for the most part, self contained monolithic programs that have limited access to one another's procedures and data. they are usually cumbersome to build and expensive to maintain because simple functional changes require the entire program to be rewritten, recompiled and retested. by contrast, the client/server structure provides the scalability and robustness required to support mission critical power system applications such as load flow monitoring, economic load dispatch and on-line dynamic security analysis. this clean, modern architecture which is server based and database driven is ideally suited for web-based power system analysis. an important aspect of implementing a client/server power system application is how to distribute the different parts of the power system application between the client and the server and how to"
"the first question (q1-ability to recommend) investigates if the proposed recommendation algorithms are able to make any suggestions for the target users. all techniques have a success rate above 80 percent. the algorithms that fail in providing suggestions to all users are the user-based lsa knn with cts generation and nbc and the user-based k-means clustering. all other approaches have successfully recommended items to all evaluators. further, we have observed that the number of tags in the user profile does not relate to the fact whether a user will receive a recommendation or not. the system could recommend items to participants that have used less than 10 tags."
"true when trying to analyze the relationships between actual network power flows, the scheduled power flows and the capacity of the transmission system. the lack of a reliable, simple and universally deployed power system data exchange format has long limited the effective communication between heterogeneous on-line power system operations."
"former studies report that male students generally appear more comfortable learning and trialling with it than female students; e.g., [cit] observed that women have more regard for service aspects more than men. hence, female users could appreciate the ease of use of computer technology more than male users."
"however, most social tagging systems do not have explicit rating data and they cannot build the traditional user-item rating matrix. to overcome the lack of rating information, most researchers (e.g., [cit] build an implicit binary user-item matrix, where each entry indicates if a user has bookmarked or annotated an item. in the next sections, we discuss the related work in the area of purely tag-based cf recommendation according to the main categories in cf, namely memory based and model based."
"the teaching-phase operator procedure generates a new solution by choosing and setting the elements one by one from the left to the right. first, the probability is set randomly."
"the only constraints imposed by restdesc are 1) for agents to understand the http vocabulary in rdf-a reasonable condition, since one of their main tasks is to deal with http requests and responses-and 2) for developers to be able to design and work with descriptions in restdesc. since restdesc does not invent a new language but adopts n, the design should not pose large problems for developers familiar with the semantic web. to support description creators, we provide an informational website http://restdesc.org/ with an interactive online group for questions and advice. also, developers unfamiliar with reasoning techniques can use the reasoner as a black box, for example through a web service."
"tests 2 and 5 were run 30 times independently. the mtlbo algorithm found 31.17 nondominated solutions on average with a standard deviation of 0.75, while nsga ii found only 27.10 on average with a standard deviation of 2.14. one of the test results is illustrated in figure 5 . it shows that the mtlbo algorithm found a better spread of non-dominated solutions and had better convergence to the pareto-optimal solutions than nsga-ii did. two additional tests were also conducted to assess the comparative effects of different settings for g max on the performance of the mtlbo algorithm:"
"clean, inexpensive, transportable electricity is the indispensable commodity of modern civilization. but the electric supply network is probably the largest and most complex man made infrastructure in the world. therefore many analyses are needed before any operations can be performed on the actual network. there are many possible phenomena in power systems, and they all need the existing power system simulation designs are very similar to those designed for the mainframe computers of many years ago, such that different simulation analyses are usually lumped together in a single package and are normally designed for a specific platform. however, the operational and commercial needs of the power industry require information systems to not only perform traditional functions, but also support many of the new functions, especially to meet the needs of competition with deregulation."
"all the power system client sees is an abstract operation request which takes input and output parameters. since web browsers are available for almost all platforms, using them as gui eliminates the need for designing different application interfaces across different platforms, and also allows to adequately presenting the application to the user without too much coding effort. the middle layer is the application server, which implements the power system application logic to process data requests. an http web server is running on the machine to receive http request from power system clients, and send http response back to the clients."
where c i denotes the set of components disassembled by operation x i . higher f h (x) is achieved when the components with higher toxicity level are disassembled earlier.
"to overcome the problems of synonymy and polysemy of tags as well as the high dimensionality and sparsity of the data set in the memory-based cf approaches, dimensionality reduction techniques can be applied. we used latent semantic analysis (lsa), a text mining technique that transforms the set of terms (e.g., words, tags) in a linear combination of these terms that represents a concept. the main assumption is that co-occurring terms belong to the same latent topic and they are similar in their meaning. in lsa, a low-rank approximation is performed on the original data. it is based on the mathematical technique singular value decomposition (svd). svd is a matrix factorization approach. the resulting matrix is a lower dimensional space where the attributes represent the concepts learned, while the original similarity between the instances is preserved [cit] ."
"however, in regards to the percentage of accurately suggested items (q2-accuracy) the number of applied tags do play a role. most users with less than 25 applied tags receive less relevant items. the larger the tag set of one user, the more relevant items are recommended. from fig. 9, we can observe that the outstanding approaches are the user-based apriori and user-based knn with cts generation and nbc. the proposed alternative for the last algorithm using lsa (rec 4), as well as the user-based k-means clustering suggest the least interesting items to the user. another observation we made is that the itembased k-means clustering algorithm can always offer at least one to three relevant items."
"in this study, we implemented 16 different tag-based cf recommendation algorithms: two memory based and 14 model based using different classification, clustering, association, or dimensionality reduction models. all data mining methods that require similarity computation in term of distance between two instances of the data set use the cosine distance function. fig. 1 gives an overview of all implemented recommendation algorithms. in total, there are five categories of algorithms: k-nearest neighbor, latent semantic analysis for dimensionality reduction, naive bayes classification, clustering, and apriori association rule mining. all approaches, except the classification ones, are implemented both in their user-based and itembased alternatives. in the following sections, we outline each proposed algorithm. for reasons of space, we do not give the implementation details of the algorithms in terms of pseudocode."
"since we do not recover faults in fault-status registers (isr and osr), the recovery techniques in our architecture rely on having robust design for these registers. error correcting codes (eccs) [cit] are good robust mechanisms for registers. various ecc mechanisms have been integrated within vlsi chips with large internal memories and cache units [cit] as well as nocs [cit] . also, tmr can be used for the reliability of icf and ocf signals."
and the objective function value f (x g i ) is the result of learner i considering all the subject 7 grades. the learner with the best result is considered as the teacher and presented as
"to see the effect of faults on the latency and to verify our recovery mechanism in relinoc, from previous experiments we selected those fault configurations that have fully-connected networks. then we injected faults and ran the simulation for different synthetic traffics. we measured both average and worst-case latency among all configurations. figure 4 shows the relation between average latency of both qos and normal packets and number of faults for 2 synthetic traffics: random and butterfly. as can be seen, when the number of faults increases, the latency of both qos and normal packets increases. however, in both benchmarks the qos traffic pays less latency penalty compare to that of the normal traffic. experiments showed that our recovery mechanism in relinoc could handle all the fully-connected networks for up-to 50 faults, and all packets reached the destination with a reasonable latency overhead."
"this tpb model, like tra, lacks the users' necessity for certain skills, experiences and base knowledge of the internet technologies to have evident behavioural intention on the acceptance of e-government, hence this model was not found suitable as an adoption model for e-government."
"in test 1, all the three mentioned objectives are chosen. in tests 2-4, two of them were chosen for each. the aim of solving a multi-objective disassembly sequence planning problem is to find a set of non-dominated solutions for the decision-makers to select from. now, see figure 4 ."
"for a specified \" as the neighborhood radius of an instance and the minimum number of points to be found in this radius, weka's dbscan builds clusters of items that are highly dense in the item-tag space m. for each item i of the target user u, we do the following: we estimate the cluster assignment for item i if any. unless there are no other items in this dense cluster m, we get the neighbor items of i inside m and put them into the candidate item set, given they are unknown to user u. the top-n items in the candidate item set are then recommended to the user u."
"the need for a centralized means of managing power system data has become evident as the role of computers in power system operation and planning continue to evolve. two decades before, a database management system had been developed to support a major electric utility's operating and planning information requirements. its structure is of the relational type, extended to suit the records relationship commonly found in power system applications."
"tlbo algorithm has been successfully applied to solve many optimization problems [cit] . its merits include that it does not require any parameters to be tuned, except population size and iteration times."
"effective power system operation requires power system engineers and operators to analyze vast amounts of information. in systems containing thousands of buses, a key challenge is to present this data in a form such that the user can assess the state of the system in an intuitive and quick manner. this is particularly true when trying to analyze relationships between actual network power flows, the scheduled power flows, and the capacity of the transmission system. with restructuring and the move towards having a single entity, such as an independent system operator or pool, operate a much larger system, this need has become more acute."
"the main aim of the user evaluation was to gauge the user satisfaction with the recommendation results. in the following, we focus on the perceived quality and usefulness of the best performing seven recommendation algorithms from our offline evaluation (see table 1 ). we analyse the results per question basis for all approaches. a summary of the average scores per question and per algorithm are given in fig. 9, based on the 5-point likert scale where 1 indicates \"strongly disagree\" and 5 \"strongly agree.\""
"in case 1, 10 disassembly operations (o 1, o 2,,o 10 ) are needed to disassemble an e-waste to remove the hazardous or valuable components. the properties of these components (c 1, c 2,,c 8 ) are listed in table 2 . the disassembly precedence constraints are represented by disassembly preference rules: to demonstrate the effectiveness and robustness of the mtlbo algorithm, four tests were implemented:"
"therefore, we see our choice for rest as an important benefit, because resource-orientation is a key principle of both the web of services and the semantic web. the rest community's efforts convince many services to move towards a restful architecture [cit] . furthermore, when a server offers such a resource-and hypermedia-based api, the concept \"service\" is gradually fading, since clients then see nothing but resources and the relationships between them. the service exists only behind the scenes, but its functionality manifests itself in the resources-which is why we coined restdesc as a functional description format."
"flows about e-waste in the lifecycle have not yet been effectively established. it is usually not effective to get information and decision support for disassembly planning from the participants [cit] . in this research, we propose a service-oriented framework to support business integration for disassembly planning. effective and optimized disassembly planning can be done via the internet by integrating the related distributed services provided by the participants."
"inconsistent with the proposition of the original tam, this study found that pu did not directly affect citizens' intention to use e-government services (h5), but it directly affects their attitude, which means that attitude construct fully mediates the effect of perceived usefulness on behavioural intention. the more useful the e-government website, the more the person feels good about the idea of using the e-government services, which will increase the intention to use this service."
"simplicity is preferred over complexity. the simplicity of the http rest architecture lies in its uniform interface, harnessing an extreme variety in resources. only a handful of actions provide for the majority of web interactions. similarly, the semantic web is based on a simple, threefold model, which accommodates for all of its data. we feel it is necessary to continue on this same course of simplicity to maximize the efficiency of our description method."
"all of the above indicates that the long-envisioned intelligent agents suddenly become within reach, using the tools and technology that are already available on the web today. in the following section, we discuss the chances, risks, and limitations of our approach."
"second, we propose relinoc switch which is based on the mentioned 2-channel switch architecture. in the relinoc switch, we take advantage of the redundant components in switches and use them as replacements in presence of faults. our architecture can tolerate several faults in the network."
"the east-channel-1 bit in osr is '1' and east-channel-2 bit is '0': this means one of the components (arbiter, link, multiplexer) at the output channel e1 is faulty or the input channel of the next router which is connected to e1 is faulty (icf is '1') . as all the rcs take the osr bits into account, in this case no traffic will go through e1, and e2 is used by both qos and normal traffic."
"analogous to the previous approach, we define its itembased nearest neighbor cf counterpart based on the item's dimensionality reduced space. we transform the original item-tag frequency data set m into the lower dimensional m 0 by the latent topics emerging from the annotations of learning items. once lsa is performed, the k most similar items to the target user u items are selected. given the ordered set of all candidate items, the top-n recommendation list is built, as described in algorithm 2."
the general restdesc skeleton is very flexible. more conventional paradigm. a straightforward conversion to first-order logic would be to see the request as a part of the precondition:
"the reasoner then similarly instantiates listing 1 with the current situation, which now includes the hyperlinks above. this deduction then indicates that the agent needs to issue a get request to the /images/37/thumb resource in order to obtain the thumbnail. when the client performs this final request, the initial goal is reached and execution terminates."
"the theory of planned behaviour (tpb) is offered as an addition to the theory of reasoned action (tra, connected to voluntary behaviour), due to the restrictions of tra in accommodating for behaviours over which users exhibit deficient control. the tpb presented a third independent determinant of intention, perceived behaviour control (pbc)."
"data conversion model is which converts any form of power system data in to a well formed representation of power system data which minimizes the overheads associated with the data transformation between different power systems and it provides global format for power system data. conversion of power system data in to an xml form enables the independency of the data used by the power system application. the proposed data conversion model is a standards-based integration platform designed to significantly reduce the engineering efforts required to integrate power system data in the distributed environment as shown in fig. 4 . legacy power system applications represent power system data as a set of flat tags in a flat file or in a database. in order to access the data, the power system application server must know the tag name for each data value that access. each power system has typically developed their own proprietary tag naming conventions to add some context to the arcane tag names which externally provides meaning to the particular data element. the tag name conventions are unique to individual power system in a network. the power system application that needs this data must either be programmed to understand the proprietary tags or must use separate data transformation tools that transform the data for each power system application. figure 4 . bringing together power system data, api and xml for solving legacy issues as the power system changes over time due to new data points being added, the data moving from power system client to the power system server has to be updated. the data transformation mapping must be continuously maintained to reflect the changes in the underlying power system application server programming. with a common model-driven approach, applications exchange data in the context of a data conversion model that hides the details of how data is stored internally in individual power system client. the use of data conversion model allows individual power system no need to maintain proprietary arcane tags naming conventions or no need to send the data in one particular order. the proposed data conversion model makes the legacy power system data customizable to the existing power system application. fig. 5 shows the hierarchy of power system data access in xml form by the legacy power system clients. the legacy power systems data dealt in per unit form and which is of floating point format and recent power system data is in xml form. data compression and decompression model has been developed to compress this huge amount of data efficiently and send the data over a network in the minimum time span. data compression techniques can be both with loss and lossless. since most of the power system data representation in the floating point number format, it would be better to adopt lossless data compression techniques. lossless data compression methods are widely used to reduce data overhead in storage systems. many data compression algorithms and hardware implementation architectures have been proposed in the past decade. huffman coding, arithmetic coding, lempelziv (lz), and lempel-ziv-welch (lzw) are some of the lossless compression algorithms commonly in use today. lzw is a dictionary based data compression that compresses content character by character; these characters are combined to form a string. a special code is assigned to new strings and strings are added to the dictionary. thereafter, when the string is repeated it can be referred to with that code. the lzw algorithm requires sequential construction of a dictionary and involves extensive comparison of input data with the dictionary content during compression."
"several researchers have stressed the importance of recommender systems in ples and tel environments in general. [cit], for instance, state that recommender systems offer a promising approach to facilitate both learning and teaching tasks, by identifying suitable learning resources from a potentially overwhelming variety of choices. [cit] note that direct measures like ratings and tags given by users allow identify paths in a learning network, which are faster to complete or more attractive than others. buder and schwind [cit] explore the potentials of recommender systems for learning from a psychological point of view. the authors note that commercial recommender systems need adaptations to facilitate learning and discuss potential adaptations both with regard to learners as recipients of recommendation and learners as producers of data by contributing annotations, tags or rating. the authors further stress that recommender systems provide (and require) user control, thus facilitating selfdirected learning, as it is the case in ples."
"in this paper, we introduced the functional description format restdesc. the obvious question is: why would this method offer what several previous formats could not? and why would restdesc help fulfill the long-standing promise of generic intelligent agents?"
"web service or web api description has been a topic of intense research research for at least a decade. there are many approaches to service description with different underlying service models. earlier service description technologies mainly focus on technical aspects like input and output parameters, data types, and exceptions. prominent examples include the web services description language (wsdl, [cit] ) and to some extent also the web application description language (wadl, [cit] ). however, recently, a trend towards more functionality-oriented formats is evolving."
"in summary, our proposed mtlbo algorithm seems to have worked quite well, and achieved better convergence and non-dominated solution spread than nsga ii did in solving the multi-objective disassembly sequence planning problem."
"it has been predicted that future designs will consist of hundreds of billions of transistors, with upwards of 10% of them being defective due to wearout and process variation. consequently, we must soon learn how to design reliable systems from unreliable components, managing both design complexity and process uncertainty [cit] ."
"the reason we decide to focus on rest services-in the original meaning intended by fielding and others-is that they provide a beautiful integration on every level of the web. after all, http was designed with rest principles in mind [cit], offering a high degree of scalability. it therefore comes as no surprise that the resource-oriented nature of rest aligns perfectly with the resources on the semantic web, where the resource description framework (rdf) is the dominant model. this alignment is not a coincidenceit is a feature, which we purposely embrace. also, even if a service does not fully follow rest principles, restdesc can still describe it, albeit with more verbosity (e.g., using string concatenation to construct uris)."
"finally, linked data and vocabularies form an important part of restdesc, again because existing work is fully reused. a major strength of restdesc is that it functions with the vocabulary of the application domain. this gives service authors the freedom to select the vocabularies that explain their functionality with the greatest expressiveness. thanks to the links between different vocabularies and data, a wide interoperability is possible. this paper aims to be an important step towards simple functional descriptions. several important research challenges are still ahead. for example, integration with authentication mechanisms and other real-world applicational concerns will have to be fluently incorporated, especially in pay-per-use scenarios where a lower number of requests is preferred. a study showed that more than 80% of all web apis on the web service catalog programmableweb.com require some form of authentication [cit] . in general, the handling of exceptional situations in a restful context, resulting in changes to the initial execution plan, should be investigated. also, developers must have a clear understanding of the benefits of restdesc for their services. this is why we will work on implementing agents that use the power of restdesc to accomplish tasks for human needs."
"using weka's simplekmeans clusterer [cit], we build groups of users based on the user-tag frequency matrix a. the parameters for the clustering process are: the number of clusters k, the maximum number of iterations, and the seed. the seed is used for random number generation, and this number is used for the initial assignment of instances to clusters. we can calculate the recommendation for each user, using the obtained assignments of users to clusters. as long as a user u is not the only member of a cluster m, we get the items of his neighbors in m and recommend those he has not seen yet."
"e-waste has become one of the major and challenging waste streams in terms of quantity and toxicity. informal treatments result in environmental pollution and secondary resource waste. the waste electrical and electronic equipment directive (2002/96/ec) [cit] in europe 1 . it aims to prevent the generation of e-waste and promote its recovery to reduce disposal. many other equivalent directives have been developed and implemented in the world. with legislative pressure for environmental protection, electronics companies have come to recognize that they must take on more responsibility. it is crucial for them to balance economic and environmental objectives, for example, to maximize recovery value and minimize environmental impact."
"for services, restdesc uncovers their key differentiating feature, namely functionality, exposing this in a logical way that integrates semantics and the rest architectural modalities. this enables services to involve in new and different interactions in an automated way, making compositions based on functionality instead of input and output parameters."
"equation 3 states that when the preconditions are fulfilled, there exists a request that fulfills the postconditions. of course, it is exactly this request that an agent will try to perform when it wants to achieve the postconditions."
"the above description can be interpreted in three parts as: if you have an image with a smallthumbnail hyperlink 1 then you can make an http get request to that link 2 to receive a thumbnail of the image with height 80 pixels. 3 immediately, the powerful nature of restdesc becomes apparent: this description unites infrastructure (http), services (thumbnail generation), and data (vocabulary reuse)."
"perceived value is a subjective assessment between all material / content that is received, and all that is surrendered during the procedure of obtaining / using a consumption related object (product, store, service) [cit] . more attention has been paid recently to perceived value as a stable construct to predict buying behaviour. customers' value perceptions have been found to influence customer behavioural intentions. it increases their willingness to buy a product or use a service, and decreases their intentions to trying to find an alternative [cit] ."
"restdesc differs in that it finds a solution inside the boundaries of currently available web technology, instead of introducing new techniques that require new tools and infrastructure. let us look back to restdesc from our three initial perspectives: infrastructure (a), services (b), and data (c)."
"the mtlbo algorithm is illustrated in figure 3 . in the selection phase, parent population p g and offspring population o g in gth generation are combined together to form"
"we will focus further only on the collaborative filtering recommendation techniques as they are the main interest for this paper. the existing collaborative rs can be classified in two categories: memory-based and model-based cf techniques. memory-based cf algorithms make rating predictions or recommendations based on the user's past ratings. for this purpose, the whole user-item rating database is used to predict the unknown rating of particular item. in contrast to memory-based cf methods, modelbased cf algorithms use the collection of ratings to learn a model, which is then used to make rating predictions. furthermore, the existing research distinguishes between user-based and item-based cf approaches, depending on whether the similarity is computed between users or items."
"the top i frontiers are selected for p g+1, and f r i+1 should be sorted using crowding distance sorter, after which, the top best solutions in f r i+1 are selected to fill the vacant positions of p g+1 . in the evolution phase, transitional population q g+1 is generated from p g+1 using teaching phase operator, and then o g+1 is generated from q g+1 using learning phase operator."
we conducted an offline evaluation to gauge the performance of all the proposed algorithms. this process required additional experimental tuning of the parameters of all used data mining techniques.
"our framework also links the relevant stakeholders so they can do more effective disassembly sequence planning together by collaborating via the internet. to disassemble e-waste, the relevant services can be invoked to create a disassembly sequence plan."
"secondly, the main benefit of restdesc-its solid integration with the semantic web-is also its main dependency. current research questions on several topics, such as ontological alignment, are therefore also relevant for restdesc. just like the semantic web, restdesc depends on the availability of machine-readable information and the interlinking of that information, which similarly can be a benefit (e.g., in terms of scalability and independence) or a burden (e.g., in case of insufficient availability or absence)."
"the choice to use a new technology is directly linked to the amount of knowledge a person has about how to use that technology suitably, and complex technologies, like egovernment, demand additional knowledge [cit] . early adopters of new technologies could have higher educational levels, mirroring their capability to comprehend \"how-to\" knowledge quicker than those with less education [cit] ."
"some future works include: (1) developing a more practical mathematical model to solve disassembly problems by considering more disassembly issues to optimize, such as disassembly cost, environmental impact, energy consumption, etc.; (2) improving the performance of the optimization algorithms to solve the problems we focus on; and (3) developing a more detailed service-oriented disassembly sequence planning system to transform the business model of the disassembly industry. the latter, in our view, needs to be based on e-business engineering approaches and technologies that will support process performance improvement."
"trust in the internet can be identified as institution-based trust [cit], which is \"belief that needed structural conditions are present (e.g. in internet) to enhance the probability of achieving a successful outcome in an endeavour like e-commerce\" [cit] . prior research has extensively recognised trust in the internet as an important predictor of adopting e-government services."
"the third step is to generate the input data set for weka. in total, our chosen algorithms use five different data sets. we list shortly the generation procedure for all of them below:"
"second, if the probability is less than p t, the leftmost element of the teacher will be chosen and pushed back to the new solution; otherwise the left-most element of the learner will be used. third, the chosen element will be erased for both the teacher and the learner. then, these three steps are repeated until the new solution contains all the disassembly operations."
"evidently, the web for agents will not arrive tomorrow, yet. this does not mean that restdesc cannot be used today. people access web applications through browsers, which are increasingly adopting characteristics of intelligent agents, either through native support for interactive features, or by an extension mechanism that allows third-party developers to enrich people's browsing experience. therefore, browsers present interesting restdesc use cases."
"in the preceding sections, we have presented the details of seven representative tag-based cf recommendation algorithms. three of them [cit] incorporate also rating information and the other four [cit] rely solely on tagging information. all algorithms decompose the original three-dimensional folksonomy into twodimensional matrices. typically for the cf recommendation, the most often used data mining technique is nearest neighborhood formation. as a similarity metric, the majority of approaches utilize the cosine distance."
"the em clustering on the item-tag frequency data set m assigns one item i to a cluster m with a certain probability. we iterate over all items of the target user u. if an item i is clustered successfully with a probability more than 70 percent to a cluster m containing other items, we add to the candidate item set of the user u those items that are new for him. the top-n items are then recommended to the user u."
"in conclusion, according to the evaluators, the best recommendation is achieved by association rule mining of users' tags, as well as by item-based cf on a lower dimensional semantic space. independent of the specific approach, the good overall satisfaction with all techniques proves that tag-based collaborative filtering has high potential for recommendation tasks in ples."
"information and communication technology (ict) is currently thought of as a key factor in developing any society. with the majority of countries globally having incorporated new technologies with links to the internet and the world wide web, currently many governments are attempting to employ these technologies to develop the method by which they provide services to citizens. e-government is defined as the use of information and communication technologies (ict), especially internet and world-wide-web, to improve efficiency, cost and quality of the government information and services provided to stakeholders: citizens, businesses, employees and other government agencies [cit] . the acceptance and success of e-government relies on the willingness of citizens to implement these innovative technologies. hence, many governments around the world currently suffer from the low level of adopting e-government services by citizens, particularly in the arab world [cit] ."
"a high level of pu leads to more positive attitude (att) [cit] . pu has been constantly found as a direct determinant of intention to use (bi), and also affects a user's bi indirectly as a direct determinant of att [cit] ."
"similarly, the hypermedia relation types (being rdf predicates) can belong to proprietary or public vocabularies [cit] . for instance, smallthumbnail represents a 80 pixels high image only in a specific application context. other applications could have similar or different definitions. furthermore, it is precisely the meaning of these hyperlinks that is fully defined by the restdesc description: in this concrete case, listing 1 describes the meaning of the applicationspecific relation smallthumbnail."
"many services running on different platforms are converging to use the internet as the common medium of interaction between humans, between human and machine, as well as between machines themselves. traditional internet functions of sharing news, entertainment, educational and documentary information are gradually being replaced with commercial, banking and financial transactions, as well as other every day services such as telephone and television broadcasts over the internet. industrial services including process monitoring, real-time control and automation are also being developed for the internet."
"technology reuse -restdesc does not require new models or paradigms. instead, it adopts existing semantic web technologies and is therefore compatible with existing tools. common n reasoners can interpret restdesc descriptions and instantiate them for concrete situations (subsection 3.4). once instantiated, the descriptions become plain rdf, ensuring compatibility with clients that do not wish to use n. importantly, n reasoners are by design able to perform goal-driven service compositions by combining multiple restdesc descriptions, without requiring additional plugins."
the bit of muxrc-1 in isr is '1': this means muxrc-1 is faulty. this case is similar to the previous one and we consider channel-1 as faulty. the same mechanism is applied on channel-2 when muxrc-2 is faulty.
"the rapid development of the internet and distributed computing, have opened the door for feasible and costeffective solutions. the internet, being platform independent, allows users to continue using the platforms with which they are most familiar. in addition, the heterogeneous operating environment provided by the internet helps to achieve continuous development of distributed applications by integrating different kinds of computer hardware and software systems."
"questionnaires were sent to 15 evaluators. altogether, five females and 10 males from different nationalities tested the recommender system. the evaluators were students and teaching assistants from computer science at the age range 24-34 years. the spectrum of profiles based on gender, age, nationality is summarized in fig. 6 . most of the evaluators (67 percent) have had prior experience with recommender systems. however, in regards to trust only (47 percent) of the respondents feel confident with such systems. fig. 7 summarizes these results."
"in terms of diversity (q4-diversity), the item-based lsa knn cf and the association rule mining approach achieved the best scores. the item-based k-means clustering is the algorithm that offered minimal diversity in recommendation. this is not surprising as clustering aims to group similar items, thus diversity within a cluster cannot be achieved."
"this study was conducted based on a data set generated within the plem 1 environment. plem is a personal learning environment that supports learners in creating a personalized space, where they can easily aggregate, manage, tag, and share learning items. a learning item in plem can be a link to a learning service, learning expert, a learning community, or a learning resource collection [cit] . plem is an online service that has been used by, for example, students at rwth aachen university and technical university sofia as well as secondary school pupils at the european school mol in belgium [cit] . the plem data set currently include 83 users, 773 items, and 1,059 tags. we enhanced plem with a recommendation engine to recommend learning items based on the tagging behavior of the learner."
it is based on pareto-optimality and has two parts beyond the stlbo algorithm. they are the fast non-dominated sorter and the crowding distance sorter. both were modified from those in the stlbo algorithm.
"hendrik thü [cit] . he is working toward the phd degree with the learning technologies research group (informatik 9) of the rwth aachen university, germany, where he is a research assistant. he focuses on mobile learning in different context situations and on the generation of user profiles according to their usage of media."
"now that its needs have been identified, we present a resource-oriented and hyperlink-based method that describes web services in an elegant way. restdesc is entirely created using existing technologies and mechanisms, while its novelty lies in the creative combination of the latter for functional service description. restdesc expresses descriptions in notation3 (n, [cit] ), a semantic web language put forward by tim berners-lee. n builds upon rdf, adding straightforward concepts such as variables and graphs."
"in a population, the set of all the solutions, whose objective vectors are mutual nondominated, is called a non-dominated frontier in nsga ii. a population (p op) can be separated into different non-dominated frontiers (f r 1, f r 2, · · ·):"
"first, we compared the network connectivity of relinoc with that of a network comprising 2-vc switches in presence of different number of faults. in this experiment, we injected different number of faults on the network on different random components by giving higher probability to larger components. then, we checked statically if the network is fully connected or not. we consider a network as fully-connected if there is at least one physical path from each source to each destination regardless of the routing algorithm. for any number of faults, we injected that amount of faults on the network with 1000 different random seeds, and calculated the average number of fully-connected networks out of 1000 possible fault distributions. we did this experiment for two types of networks: i) 8x8 relinoc and ii) 8x8 noc with 2-vc switch. figure 3 shows the plot of network connectivity of both cases in terms of number of faults. the y axis in this chart is the probability of having a fully-connected network as a function of \"number of faults\". as seen, relinoc has much better connectivity in presence of faults. for example, in presence of 20 faults, relinoc has 90% probability of fully-connected network, while that of noc with vc-based switches is 70%. for 40 faults, relinoc shows 2 times better physical connectivity compare to the vc-based architecture."
"a priority-based switch architecture which can provide reliability in presence of several faults inside the noc has been proposed. first, a 2-channel switch architecture which completely removes the control logic overhead of vcs was introduced. then, we proposed relinoc architecture which utilizes the inherently redundant components inside the 2-channel switch as replacements for the faulty elements."
"the registration procedure is done only once, when the device joins the home system for the first time. thanks to the initial http connection, the network organizer collects a certain amount of information on the device and the attached user account like operating system, ip address, login, password, etc (figure 3 ). the collected information will be used by the network organizer first to build export/import folders on its side devoted to the new comer, and secondly, to generate a script (bash/shell) dedicated to the operating system of the user's device. the script is proposed to the user's device for upload and execution via the web browser. when executed, the script builds on the user's device appropriate folders (import and export). at this point in time, everything has been prepared so that the new user/device can join (connect) the home system."
"today, users enjoy creating and collecting a rapidly going up amount of digital content like digital photos, music and videos. new internet storage services increase this trend. users expect to share their content with their family, friends or even wider communities making their content available not only over their own devices but also over other devices handled by third parties. unfortunately, these devices have different capabilities and interfaces. consequently, many users are discouraged by the complex challenge of accessing, moving, organizing, and formatting their large digital content libraries. indeed, many protocols and technologies can be used to solve the home network content access and sharing issues: the popular computer's network storage protocols (nfs, smb, ftp, http), the established upnp/dlna residential networking protocol and the ietf web dav (web based distributed authoring and versioning) http extension protocol. anyway, if these different protocols are effective in their own domains, none of them covers all the home network requirements."
"despite the fact that the majority of users place a high value on their own content, many users do not backup their files. in our test bed, we offer to each user a secured storage space located on the gateway, secured because regularly and transparently (for the user) backed up in the cloud (dropbox) . the proposed gateway centric architecture implementing the content access and the protocol translation at the file system level gives interesting opportunities to isps to propose backup applications based on well established and widespread file system and data backup tools & process. in addition clever choices could be done to ensure complementary high bandwidth local backup onto the local nas and high secure storage services onto the cloud. hierarchical storage management strategies could be designed to automatically move content between these in home and outside storage spaces quite easily."
"in this section, a neural organization structure for processing object motion based on retina mechanism will be described at first. then the spatio-temporal properties of each type of neurons in the motion pathway will be proposed and analyzed in succession."
"thus, when a user identified a content using the dam tools, and expects to render it on his tv, the dam resolves the content path (from the distributed file system view path into the upnp media server path) and uses the local upnp control point to invoke the upnp media renderer of the tv with the relevant content path. this example demonstrates how the gateway, ensuring communications with devices using their respective native protocols and translating locally these protocols, leverages significantly users' home network experience."
"in such devices, the network organizer creates dedicated folders, basically one devoted to shared content exported by the device and one devoted to import content shared by other devices. at the same time, the shared content of each device appears in the gateway as a part of its own folders tree."
"alternatively, we have implemented a web based digital asset management (dam) application on the gateway, requiring only the support of a web browser on an end device. the term dam is used here to refer to all the different tasks and applications used to ingest, to index, to catalogue and retrieve digital assets (content) such as documents, digital photos, music and videos based on metadata embedded in the digital assets or at least file names ( figure 5 ). whole home network content can be indexed by the dam application and the results (i.e. the name of the content, the location of the content, and eventually a preview or a jacket) stored in a database on the gateway. users can browse this database from their pcs, their smartphones, or any device equipped with a web browser, the dam being in charge of building the web pages adapted to the device screen size and resolution. the support of query services by the dam, using key word search is also straight forward and enables more powerful content retrieval end user experience. lastly, to play content, users will transparently run the local media players or take advantage of the ones supported by the dam itself."
"furthermore, the dam may be enriched (possibly under the control of the isp) to offer additional services (not implemented in our test bed) like content format adaptation (i.e. transcoding) and content duplicate detection. depending on the gateway resource availability, transcoding and duplicate detection may be launched in background to avoid gateway overloading."
"the connection procedure makes effective the mounts prepared by the registration procedure. so the content of exported folder of the user's device is now available into the tree folders of the gateway, as it is part of the local gateway file system. this content is exported from the gateway to the rest of the network. the content is now visible by every connected device. and, in the other direction, the content shared by the other devices of the home network appears as a mounted folder in the tree structures of the local file system of the device. note that, for the sake of security, this content may appear as read only, and only selected devices appear. security aspects may be further investigated. however we consider this issue being out of scope of this paper."
"and the weight distribution in bipolar receptive field can be denoted as: (6) where and represent the amplitudes of two gaussian weight components, whose standard deviations are and respectively, in . signal converge into bipolar cell is:"
"first, as new technologies or opportunities appear, the standard needs to evolve, establishing different releases and creating backward compatibility issues. for instance, upnp/dlna initialy solved the problem of sharing av content in the home and is currently being extended in order to also support commercial content delivery in the home."
"a last important part of home network devices consists in the personal computers themselves. as file system level network protocol, they natively support smb (for microsoft windows systems) and nfs (for unix like systems)."
"this paper is organized as following. model for processing object motion based on retina mechanism is firstly proposed in section 2. in section 3, experimental results and corresponding analysis is implemented. finally, conclusions on the proposed algorithm are drawn out in section 4."
"in this paper, we propose an innovative solution based on a gateway-centric architecture where this central equipment is in charge of translating the native content sharing protocol of any connected device."
"center-surround antagonistic receptive field (csarf) organization is the basic synaptic circuit for spatial information processing in the visual system. bcs are the first neurons along the visual pathway that exhibit csarf organization. in both the receptive field center and the antagonistic surround region, the bipolar cell's temporal filter followed a biphasic time course. the surround response was delayed and inverted in sign relative to the center."
"when suitable parameters are given, the receptive field of a horizontal cell can be modeled [cit] as: the response dynamics of a horizontal cell   y x, to its stimulus is described as following equation [cit] :"
"in the opposite direction, a upnp fuse (file system in userspace) module like djmount [cit] will map under the vfs all the available upnp/dlna media servers on the network and their attached offered content. local applications running on the gateway can therefore see this content (present on remote upnp/dlna media servers) as if they were locally available from the local file system interface. it can also be exported towards remote client using other network file system specific export functions (nfs, smb) as being described later."
a upnp/dlna media server running on the gateway on top of the vfs is in position to offer to upnp/dlna clients any visible user's space connected to the vfs. therefore the simple installation of a upnp media server allows to expose the files being available at the gateway level towards any upnp/dlna player and/or controller. we experimented the ps3 media server [cit] and the minidlna server [cit] .
"ganglion cells dsgcs signal the direction of image motion across their receptive fields by firing action potentials in a 'preferred' direction, other than in the opposite 'null' direction. a key circuit module of retinal dsgcs is a spatially asymmetric inhibitory input from starburst amacrine cells [cit] ."
"to evaluate the effect of the proposed retina model on motion detection, quantitative analysis and comparasion with other method (here it is optical flow based algorithm) are both accomplished. for achieving this, the criterion on performance evaluation of motion detection algorithms will be firstly given in this section. obviously, for a method of object motion detection, accuracy, universality and timeliness are the three most important factors that should be used to construct a formula for measuring system performance on motion detection. the following expression is one of this kind of representations: table ii shows the mean square error (mse) of motion velocity matrices detected by using different methods (the proposed retina model vs. optical flow)."
upnp/dlna natively includes a discovery protocol so that there is no specific action to carry out when upnp/dlna devices enter or leave the home network. the upnp/dlna installation on the vfs is performed by the network organizer.
the new coming user shall first contact the network organizer via its web browser. his first operation is to register himself in the home network.
"nowadays, an increasing number of web servers offer storage services to internet users, supporting a wide range of applications from raw data storage (dropbox, carbonite, etc) to photo (picasa, flickr, facebook, etc) or video sharing (youtube, dailymotion, etc). most of these servers provide fuse compliant software that can be installed at the vfs level so as to ease cloud storage integrations. when they do not provide the code themselves, they generally provide an open interface so that third parties can develop applications, easing the integration of those cloud storage services with other applications. oftenly the open source community develops a fuse compliant software."
"to solve the problem of interoperability in heterogeneous home networks, we have proposed a gateway centric architecture where the content access unification is realized at the file system level, allowing each device to access the entire home network using its native protocol. the unification is automated thanks to a software module called network organizer running on the gateway. our solution does not require any software installation on end-devices, relying on the gateway as the unique device for software installation or upgrade. our architecture therefore enables flexible migration scenarios, smooth integration of new features, protocols or technologies as they appear. regarding devices where the manufacturer policy prevents direct access to the file system of the device (typically iphone), access unification has to be envisioned at the application level. further studies are required on this topic, as well as on security aspects which have not been addressed in this paper."
"the connection procedure may be done just after the registration and has to be done each time the user/device joins the home network, for instance at each power on of the user's device."
"in this paper, a bioinspired neural model for detecting object motion based on retina computational machanism is proposed based on synthesizing those representative works on modeling retina and incorporating some latest findings on retina mechanisms. to understand more in-depth the retina machanism of motion detection, the spatio-temporal properties of each type of neurons in the retina motion pathway is also mathematically analyzed. for evaluating the model performance on motion detection, a set of experiments and quantitative analysis of the experimental results are accomplished. the experimental results show that this proposed model can be used to detect object motion effectively."
"the preferred direction of the cells and the strength of the directional tuning of the dsgcs can be calculated from responses to stimuli in each of stimulus directions evenly spanning 360°. with directional tuning data, the response r of a dsgc to stimulus from the direction"
"we experimented in our prototype the integration of picasa and youtube by adding gdatafs to the vfs [cit] . we also added a raw data storage service by integrating a dropbox software to the vfs [cit] . such software installations need to be done only once and could be facilitated via the network organizer. indeed, during the software installation user credentials (login/password) need to be provided, which could be done with the assistance of the network organizer (dynamic provisioning, etc). we have realized this assistance in our test bed for the dropbox module installation."
"neural mechanism experimental data [cit] and theoretical analysis [cit] show that directional selectivity is the result of local, postsynaptic, nonlinear interactions in the ds(directionally selective) cell. the light responses of cones and bipolar cells are not directionally selective [cit] . the direction-selective circuit in the retina relies highly upon the selective wiring of synaptic contacts between sac(starburst amacrine cell) distal dendritic tips and the dendrites of the ds gcs(ganglion cells). as a presynaptic interneuron, the sac is asymmetrically connected to ds cells and delivers direct inhibition to dsgcs. the sacs pointing in the null direction deliver inhibition and those pointing in the preferred direction do not. they project inhibition laterally ahead of a stimulus moving in the null direction [cit] . in addition, starburst inhibition is itself directionally selective: it is stronger for movement in the null direction [cit] . excitation in response to movement in null direction is reduced by an inhibitory signal acting at a site that is presynaptic to the ds cell. thereby excitation is reduced by enhanced inhibition given stimulus of motion in the null direction, and thus the effect of directional selectivity is realized. therefore, signaling between the cones, bipolar cell, sac, and dsgc constitutes a neural network that generates the ds light responses of the dsgc [cit] . the retina motion pathway, in a form of laminar organization, is shown as fig.1 ."
"amacrine cells are served to integrate, modulate and interpose a temporal domain to the visual message presented to the ganglion cell. experiments [cit] show that sac are wired to suppress the visual response of dsgcs through presynaptic inhibition of a bipolar terminal."
"these findings and works sketch the structural scheme of motion processing channel in retina. however, mathematical analysis of the models and their applications to motion processing are insufficient without exception. fortunately, numerous neurophysiological findings [cit] on retina information processing pathways have been revealed. these new findings provide great source of ideas for our modeling the retina more reasonably."
"the disconnection procedure shall take place when the user/device leaves the network. it consists in unmounting the user's exported folders in the gateway. the user's device will not be part of the network anymore. the other devices still connected to the home network will not be able to access to the shared content of the disconnected device, which now appears as \"disconnected\" to them. the disconnection procedure can be initiated by the device itself (as part of the shutdown/standy procedure). but this cannot be the only way, as devices may be disconnected from the network by simple cable removal. consequently the network organizer has to monitor devices presence, for instance by regularly pinging operations."
"the ability to detect motion in the visual scene is a fundamental computation in the visual system that is firstly performed in the retina [cit] . as for complete models of retina motion extracting channel, boahen etc. proposed[2~3] a retinomorphic system, which uses four different kinds of neurons for firing specific spikes in response to concrete stimuli and representing the input data. f. barranco [cit] designed and implemented an event-driven processing scheme based on artificial retinas for the detection of spatiotemporal features. stephen a. baccus [cit] etc. investigated the circuit basis for detecting object motion by recording intracellularly from all classes of retinal interneurons while simultaneously recording the spiking output of many ganglion cells."
"the input of the cell; parameter  is the extended length of excitement field, h  time constant of the horizontal cell, and  the feedback factor."
"the structure of the paper is as follows: after a review of the related work from which we drew our thinking in general, we detail the motivations of our approach in section 3. then we introduce our solution and describe how we can operate with major content access and sharing protocols, in section 4. in section 5 we describe how the distributed file system view can be leveraged by various applications. section 6 outlines potential impact for contributing storage applications, while section 7 concludes this paper."
the weight distribution in receptive field of sacs is described as: where centre and suur respectively represent the centre and surround region in receptive field of sacs.
"however, the implementation of the module, its configuration and the creation of the mounting points require computer skills that not everybody has. the challenge is to make possible that any user overcomes this obstacle. to this purpose, we have developped a piece of software, called \"network organizer\", which realizes an automation of the home network structuration and, in particular, of the vfs adaption modules installation. this piece of software runs on the gateway. it may be provided by the isp."
bipolar cells are the only ones with receptive field properties that match the requirements for the essential part of motion computation of nonlinear spatial summation over small subunits in the ganglion cell receptive field center.
"the first common benefit of the distributed file system view comes from the file system interface itself: any legacy application is accessing data via a local file system interface, as if it would manipulate local content. typically, pc users can continue to use their file manager application (i.e. windows explorer or linux file browser) for browsing their files (figure 4), or their popular media players (e.g. windows or vlc media player) to play their content. the advantage is that users have now access to all home network distributed content thanks to the specific folders created in their pc to import content shared by all connected devices; it could be content shared by another pc, a local nas or content stored in cloud areas which accounts are mounted in the gateway file system. dlna devices (typically tv sets or game consoles) can access shared content via the dlna media server (as described in section 3.1) once it has scanned properly configured folders (indeed any content shared by any connected device appears as a folder of the gateway local file system, including the cloud accounts)."
the network protocols (smb/nfs) expect the devices going through a process when entering (connecting) or leaving (disconnecting) the system. this raises the issue of sorting out these complex manipulations without any additional software installation on every users' devices. the network organizer is in charge of managing the registration and the connection or disconnection of users' devices. let's describe these different procedures more in detail.
this section illustrates how the distributed file system view can be leveraged by various sets of applications. we here describe such applications that we have deployed on our test bed.
"some dlna media players are proposing support in this direction (e.g. artist classification, album identification); the main issue is that these services concern exclusively dlna compliant devices."
"taking benefit of vfs capabilities, we propose, in addition, to represent the shared content of every connected device as a dedicated folder in the tree structure of the gateway. each dedicated folder is the mounting point of the corresponding device in its native network protocol. (figure 2 ), for each of which vfs offers an adaptation module."
"for these reasons, we choose to explore an alternative solution, not imposing any additional software installation on the devices of the home network. consequently we propose to use their native communication protocols and, to solve the interoperability problem, we propose to connect each device to a central equipment, to which we devote the role of protocols translator. software updates are now necessary only on this central equipment. this equipment should be connected and powered on most of the time. it may be under the control of a service provider so as to be upgraded when appropriate, as is typical for a home gateway. additionally we propose to implement this translation at the file system level. this choice, driven by content management storage concerns, ensures to keep compatibility with all applications manipulating stored content."
"for the moment, thanks to the vfs abstraction layer, users do not need to care about the physical location of the accessed content on the distributed network; they just need to remember the name and path. the system is confortable for a user if the number of files stored is not so high. but when a huge number of files is available in the home network, it becomes impossible for a human-being to remember all the content names and paths."
"where and represent the center and surround region of center-surround antagonistic receptive field, and represent respectively the output signals from pcs and hcs located at in csarf. the response dynamics of retina bcs to stimulus is described as following equation [cit] : (8) where and represent the membrane potential of a bipolar cell and its input signal respectively, and is a time constant."
"as can be seen from the above table, the two methods show similar motion sensitivities as well as good accuracy on the direction of 0°, 90°, 180°and 315°. and generally, the proposed retina model shows better performance on motion detection than the traditional optical flow based algorithms. or it has a lower value of mse than that with traditional optical flow based algorithms."
"our solution consists in a gateway centric architecture where we realize the content access and the protocol translation at the file system level ( figure 1 ). to this purpose, we take benefit of the virtual file system (vfs) layer from linux operating system. the vfs was developed in order to ease the addition of new file systems into the linux kernel. it is frequently used in unix like operating systems to facilitate the integration and the use of several file system types [cit] ."
"horizontal cells accept the input of the photoreceptor cells [cit], acting as a low-pass filter in the transmission of information from responses of receptors [cit] ."
"second, the approach requires a device to either natively support the protocol or to provide ways to be upgraded (at least to allow further sofware installation), which is generally very problematic in many consumer electronics (ce) devices as tv sets, mobile phones, etc. in the upnp/dlna case for example most tv sets now claim being compliant with upnp/dlna, but they generally cannot be upgraded. in order to benefit from the next release of the standard (as commercial content delivery support), users will have to change their tv set."
"new users' home network experience is possible with the proposed distributed home file system view. a tablet, or a smartphone, can advantageously replace the outdated tv remote control as well as the keyboard or the mouse of any pc. users will just take advantage of the web browser, the virtual keyboard and the touch screen offered by these devices to query the dam database via key words and auto completion support. as an alternative to install a dedicated upnp control point application on the tablet, we propose to run the upnp control point software on the gateway, and to make it available to the user via the dam interface. this also allows preserving the user interface experience of the system even when rendering content on external tvs."
"a human-in-the-loop experiment was performed under different requeue conditions, which showed that the use of requeues increases the overall probability of detection and the operator confidence but could decrease the fraction of targets found. furthermore, the additional use of requeues increased operator workload as measured by utilization, although this was shown to be within acceptable standards based on previous research in human supervisory control."
"a survey on text mining in social networksare declarative languages and generally express the logic of computation based on either first-order logic or description logic. for instance, the w3c organization introduced standardized ontology web language that supports interpretability of language by providing additional vocabulary with formal semantics [cit] . common logic and semantic application design language [cit] are the popular ontology-based languages commonly used for semantic evaluation of data sets available in social networking websites."
"this effort makes three novel contributions in presenting a choice model for a search task with requeues. first, we develop a retrial queue model (reqm) for visual search tasks that includes the possibility of requeuing difficult images and pose reqm as a variation of a retrial queue with feedback [cit] . we next develop a des with reqm (des-reqm) that embeds operator models derived from previous experimental data of a simulated multi-uav mission. we then present results in the predicted performance of multi-uav visual search tasks using des-reqm. we build on previous work [cit] by discussing the results of a human-in-the-loop experiment that confirm the predictions made by des-reqm, as well as presenting experimental observations of operator behavior in requeuing and relooking tasks. we conclude with a discussion on the implications for requeues and relooks with an actual operator in the loop."
"the single-pass algorithm is the simplest form of partitional clustering [cit] . the algorithm starts with empty clusters and randomly selects a document as a new cluster with only one member [cit] . single-pass algorithm calculates a similarity coefficient by considering a second object. if the calculated similarity coefficient is greater than the specified threshold value, then the object will be added to the existing cluster, otherwise a new cluster will be created for the object. the birch (balanced iterative reducing and clustering using hierarchies) algorithm is an example of the single-pass clustering algorithm [cit] . the algorithm uses hierarchical data structure called cf (clustering feature) tree for partitioning the data sets [cit] . nearest neighbour clustering is iterative and similar to the hierarchical single-link method [cit] ."
"we used experimental data obtained from a multi-uav simulator including visual search tasks to determine the relationship between detection probability and search time in search tasks performed by single operators in multi-uav simulated missions [cit] . the search tasks contained imagery obtained from google maps, and the participants were instructed to maximize the number of targets found over the course of the mission. in multiple target searches, this previous work observed that subject probability of detection degraded with time since difficult searches required additional cognitive effort from the operators [cit] and operators had to switch between planning and searching [cit] . we used these data to determine that the empirical mission probabilities of detection decreased with increased search time and hypothesized that the increased likelihood of mistakes arises because people are forced to make a choice on a search task in order to move on to waiting search tasks. fig. 2 shows the probability of detection modeled using a logistic regression for a visual search task obtained from previous experimental data in multi-uav simulated missions [cit] and is consistent with previous vigilance literature [cit] . the logistic model will be explained further in section iii, but given the decrease in operator accuracy with time, it appears that there could be a benefit to requeuing the current search task (and possibly abandoning it in the absence of new sources of information). while one of the key benefits of a requeue is that it frees the operator to pursue other searches, particularly since the queuing model assumes that tasks are continually arriving, an additional benefit of a requeue is that a search task could be investigated at some later time in the mission via a \"relook.\" note that, when search tasks are requeued, they are simply reinserted in the queue, and there is no explicit provision for how a requeued task is searched again (e.g., first-come firstserved (fcfs) policy). for example, an operator may choose to take another look at a requeued task after having explored other tasks or never search the task again for the remainder of the mission."
"a n important aspect of ongoing and envisaged unmanned aerial vehicle (uav) missions is the visual search task, in which operators are responsible for finding a target in an image or a video feed. due, in part, to advances in networked sensors, military analysts are becoming increasingly overwhelmed with the volume of incoming uav imagery (both full motion video and static images) [cit] . given the future department of defense vision of one operator supervising multiple uavs, the amount of incoming imagery to be analyzed in real time will grow [cit] . moreover, with recently announced widearea airborne sensors such as gorgon stare and argus which can generate up to 64 images per single uav camera concurrently, there is an urgent need to develop efficient approaches for human analysis of uav-generated imagery [cit] ."
we are interested in a mission objective that maximizes the total number of targets found (n f ) out of the total number of possible targets in the environment (n t )
(1) assign all the points to the nearest centroid and (2) calculate the centroids for a newly updated group [cit] . the iterative process continues until the cluster centroid becomes stabilized and remains constant [cit] .
"text mining can be a solution of above-mentioned problems. owing to the increasing number of readily available electronic information (digital libraries, electronic mail, and blogs), text mining is gaining more importance. text mining is a knowledge discovery process used to extract interesting and non-trivial patterns from natural language [cit] . the technique comprises of multidisciplinary fields, such as information retrieval, text analysis, natural language processing (nlp), information classification, and database technology. [cit], the authors defined text mining as an extension of data mining technique. the data mining techniques are mainly used for the extraction of logical patterns from structured database. text mining techniques become more complex as compared with data mining owing to unstructured and fuzzy nature of natural language text [cit] ."
"an agglomerative method uses a bottom-up approach by successively combining closest pairs of clusters together until the entire objects form one large cluster [cit] . the closest cluster can be determined by calculating the distance between the objects of n-dimensional space. agglomerative algorithms are generally classified on the basis of inter-cluster similarity measurements. the most popular inter-cluster similarity measures are single-link, complete-link, and average-link [cit] . several algorithms are proposed based on the above-mentioned approach, such as slink, clink, and voortices use single-link, complete-link, and average-link, respectively. the ward algorithm uses both the agglomerative as well as divisive approach as illustrated in figure 3 . the only difference between the aforementioned algorithms is the method of computing the similarity between the clusters."
"social networking websites such as facebook are rich in texts that enable user to create various text contents in the form of comments, wall posts, social media, and blogs. owing to ubiquitous use of social networks in recent years, an enormous amount of data are available via the web. application of text mining techniques on social networking websites can reveal significant results related to person-to-person interaction behaviours. moreover, text mining techniques in conjunction with social networks can be used for finding general opinion about any specific subject, human thinking patterns, and group identification [cit] . recently, researchers used decision trees (dts) and hierarchical clustering (text mining techniques) for group recommendation in facebook where user can join the group based on similar patterns in user profiles [cit] ."
"meaningful sentences are composed of logical connections to meaningful words [cit] . a logical construction of words is generally provided by machine readable dictionaries, such as wordnet. in semantic-based clustering, the structured patterns are extracted from an unstructured natural language. moreover, the approach emphasizes meaningful analysis of contents for information retrieval."
"2) requeuing decision: if the operator is not willing to make a detection decision, then the operator can choose to requeue the task. however, the requeuing policy describing how the operator decides to requeue tasks may depend on a number of factors, including the total amount of time spent searching for a target, the total number of remaining tasks, and target arrival rates. as a first approximation, reqm assumes that an operator will requeue the target with some probability p, which is the probability that the search time exceeds some critical search time t rl (additional details on how t rl is chosen are provided in section iv)."
"the k-mean algorithm is widely used because of the straightforward parallelization [cit] . moreover, k-mean algorithm is insensitive to data ordering and works conveniently only with numerical attributes. however, the optimum value of k needs to be defined in advance [cit] ."
"abstract-unmanned aerial vehicles (uavs) provide unprecedented access to imagery of possible ground targets of interest in real time. the availability of this imagery is expected to increase with envisaged future missions of one operator controlling multiple uavs. this research investigates decision models that can be used to develop assistive decision support for uav operators involved in these complex search missions. previous human-in-the-loop experiments have shown that operator detection probabilities may decay with increased search time. providing the operators with the ability to requeue difficult images with the option of relooking at targets later was hypothesized to help operators improve their search accuracy. however, it was not well understood how mission performance could be impacted by operators performing requeues with multiple uavs. this work extends a queuing model of the human operator by developing a retrial queue model (reqm) that mathematically describes the use of relooks. we use reqm to generate performance predictions through discrete event simulation. we validate these predictions through a human-in-the-loop experiment that evaluates the impact of requeuing on a simulated multiple-uav mission. our results suggest that, while requeuing can improve detection accuracy and decrease mean search times, operators may need additional decision support to use relooks effectively."
"given the complex interactions between the human and the automated sensors in these uav missions, models of the human operator are necessary in order to develop more appropriate decision support systems (dsss) that account for operator decision-making inefficiencies, such as increased wait times for vehicle selection and loss of situation awareness [cit] . mathematical models for human operators interacting with multiple uavs have been developed using a queuing framework [cit], where external tasks are generated from an underlying stochastic process and the human supervisor, modeled as a server, services the stream of tasks. while analysis of realistic multi-uav missions is analytically intractable, discrete event simulation (des) of operator queuing models has been used to generate accurate performance prediction of experimental results [cit] . operator models have also been developed for human information aggregation using two-alternative-choice (2-ac) models [cit] and visual search formulations [cit] ."
"the search images were obtained from google maps, and aerial views of different scenes were presented to the operator. each scene contained a target that needed to be found by the operator, and hence, errors were only in the class of missed detections. different images were presented to the operator. fig. 7 shows two examples of search tasks, where fig. 7(a) shows a search task with the objective of finding the fighter jet (located in the lower right corner of the image). fig. 7(b) shows an example of a search task requiring the detection of a helicopter landing pad in a cluttered environment. the tasks were randomly placed in the mission, and pilot tests were used to populate the image database to ensure sufficient diversity."
"an important feature of an operator queuing model is that a submodel is needed to understand how humans accumulate information and ultimately make detection decisions in search tasks (the \"operator\" block in fig. 1 ). one common formulation uses a 2-ac framework [cit] . 2-ac models originate from hypothesis testing models [cit] and characterize information accumulation as a stochastic diffusion process. it can be shown [cit] that sufficient statistics of the diffusion model can be summarized by two random variables that can be measured empirically: the probability of choosing one of the alternatives p and the mean response timet . for the visual search task in this paper, p is the probability of detection."
"reqm is an initial attempt to represent how a repeated visual search task with a task requeuing option can be properly formalized using retrial queues. however, analysis of this model is difficult without human-in-the-loop experimental data, as it is unclear how frequently subjects decide to requeue tasks, and previous work in retrial queuing theory does not provide insight into these choices for human operators."
mltc comprises of quantitative approaches to automate nlp that uses machine learning algorithms. preferred supervised learning techniques for text classification are described in the subsequent text.
"partitional clusters are also known as non-hierarchical clusters [cit] . to determine the relationship between objects, partitional clustering uses a feature called vector matrix. features of every object are compared and objects comprised of similar patterns are placed in a cluster [cit] . the partitional clustering can be further categorized as iterative partitional clustering, where the algorithm repeats itself until a member object of the cluster stabilizes and becomes constant throughout the iterations. however, the number of clusters should be defined in advance [cit] . different forms of the iterative partitional cluster-based approaches are described as follows."
this survey attempts to provide a thorough understanding of different text mining techniques as well as the application of these techniques in the social networking websites. the survey investigates the recent advancement in the field of text analysis and provides a comprehensive overview of all the exiting text mining techniques that can be used for the extraction of logical patterns from the unstructured and grammatically incorrect textual data. this survey will definitely provide new ways for researchers to proceed and develop novel classification or clustering techniques that will be useful for analysis of text in social networks.
"hierarchical clustering organizes the group of documents into a tree-like structure (dendrogram) where parent/child relationships can be viewed as a topic/subtopic relationship. hierarchical clustering can be performed either by using (a) agglomerative or (b) divisive methods, which are detailed in the subsequent text [cit] )."
"while it is straightforward to implement a requeue option in a multi-uav simulator, the effect of providing requeue and relook capabilities must be investigated experimentally since there is a potential for undesirable effects such as increased operator workload. furthermore, these capabilities could be operationally important in minimizing collateral damage and reducing errors and have been studied in the context of optimal stopping [cit] and inspection problems [cit] . while these works showed promising results in assessing the informational value of an additional look, these studies are limited in two main ways. first, previous work related to the speed-accuracy tradeoff shows that operator accuracy may degrade with time, due to either vigilance effects attributed to fatigue [cit] or task difficulty [cit] . furthermore, complex missions such as those involving multivideo visual search tasks or missions that require both planning and searching tasks can increase operator workload [cit] ."
"to overcome the challenges, researchers need to apply different text mining techniques in social networks that can filter out relevant information from the large text corpora. however, determining whether to use clustering or classification approach for text analysis in social networks is still a challenging task that totally depends on the data set and the nature of the problem being investigated. in future, text mining tools can also be used as intelligent agent that can mine user's personal profiles from social networks and forward relevant information to the users without requiring an explicit request."
"one of the interesting results from this experiment was that detection probability increases as subjects are provided with the capability to requeue. however, relooks actually increase the probability of making an error. this seemingly counterintuitive result arises from the total number of targets that were searched in the nr mode compared to the rwc and rwoc conditions. in fact, subjects in the nr mode made an average of 32.6 total searches, while 31.2 and 30.8 were made in the rwc and rwoc modes. however, in the rwc mode, only 25.8 and 22.0 total decisions were made, and of these decisions, the respective error probabilities were 19.2% and 19.1% for rwc and rwoc. in contrast, the nr mode had an error rate of 27.8%, suggesting that the benefit of the relook was to allow people to skip difficult targets. nonetheless, it appears that people felt implicit pressure to make a decision for repeated targets. thus, this research suggests that, when there is not new information in an additional glance, allowing operators to requeue (i.e., skip) a target, but not relook at it, may be a more effective strategy when tasks are arriving stochastically. this result has broader implications for the value of information of an additional look, as well as teaming of operators, where it may be advantageous to requeue skipped targets for other teammates to avoid the increased likelihood of a mistake."
"these observations open up an interesting area of work that should seek to understand the value of information of an image, and under what circumstances, an operator may require additional imagery to reach a conclusion. another important conclusion of this paper is that it has shown that supplying the operators with one additional choice of requeuing, rather than constraining them to a forced choice context, improves accuracy and confidence. this has important ramifications, not only for the external validity of the 2-ac models but also for practical considerations in actual missions, in designing dsss that can provide additional flexibility to stressed operators."
"we abstract the operator choice model into a detection probability and search time distributions. for the detection probability, we derived a logistic regression model from previous experimental data [cit] . the operator is assumed to make correct detections with probability"
"document clustering includes specific techniques and algorithms based on unsupervised document management [cit] . in clustering the numbers, properties, and memberships of the classes are not known in advance. documents can be grouped together based on a specific category, such as medical, financial, and legal. in scientific literature [cit], different clustering techniques are comprised of different strategies for identifying similar groups in the data. the clustering techniques can be divided into three broad categories: (a) hierarchical clustering, (b) partitional clustering, and (c) semantic-based clustering that are detailed in the subsequent text."
"in modeling the operator planning policy, we make the assumption in des-reqm that operators allocate uavs to targets according to a policy that routes uavs to the targets that are nearest geographically. while the current research is investigating the role of different routing strategies [cit], we will assume this greedy approach."
in the k-mean approach the data set is divided into k clusters [cit] . each cluster can be represented by the mean of points termed as the centroid. the algorithm performs in a two-step iterative process:
"social networking websites create new ways for engaging people belonging to different communities [cit] . social networks allow users to communicate with people exhibiting different moral and social values. the websites provide a very powerful medium for communication among individuals that leads to mutual learning and sharing of valuable knowledge [cit] . the most popular social networking websites are facebook, linkedin, and myspace where people can communicate with each other by joining different communities and discussion groups. social networking can solve coordination problems among people that may arise because of geographical distance [cit] b) and can increase the effectiveness of social campaigns [cit], 2009b [cit] ) by disseminating the required information anywhere and anytime. however, in social networking websites, people generally use unstructured or semi-structured language for communication. in everyday life conversation, people do not care about the spellings and accurate grammatical construction of a sentence that may lead to different types of ambiguities, such as lexical, syntactic, and semantic [cit] . therefore, extracting logical patterns with accurate information from such unstructured form is a critical task to perform."
"case-based reasoning comprises of three basic steps: (1) classification of a new case by retrieving appropriate cases from data sets, (2) modification of the extracted case, and (3) transformation of an existing case [cit] . textual case-based reasoning (tcbr) primarily deals with textual knowledge sources in making decisions. [cit] for organizing semantically related textual data into a group. [cit] stated better results of knowledge discovery in the sophia-tcbr system. however, in the tcbr approach, extracting similar cases and representing knowledge without losing key concepts with low knowledge engineering overhead are still challenging issues for researchers [cit] ."
"for the past few years there has been a lot of research in the area of text mining. in the scientific literature [cit], various text mining techniques are suggested to discover textual patterns from online sources. [cit], the authors restrict the analysis to techniques that are specifically associated with text document classification. brucher stated various clustering-based approaches for document retrieval and compared different clustering techniques for logical pattern extraction from unstructured text, but most of the techniques presented in the papers are not recent [cit], the authors proposed a new model for textual categorization to capture the relations between words by using wordnet ontology [cit] . the proposed approach maps the words comprised of same concepts into one dimension and present better efficiency for text classification. [cit], the authors indicated a best practice in information extraction process based on semantic reasoning capabilities and highlighted various advantages in terms of intelligent information extraction. the author explained the suggested methods, such as query expansion and extraction for semantic-based document retrieval, but did not mention any results associated with the experiments. [cit], the author introduced general text mining framework to extract relevant abstract from large text data of research papers. however, the proposed approach neglected the semantic relations between words in sentences."
"c-mean is a variation of k-mean that exhibits a fuzzy clustering concept that generates a given number of clusters with fuzzy boundaries and allows overlapping of clusters [cit] ). in overlapping clusters process, the boundaries of clusters are not clearly specified. therefore, each object belongs to more than one cluster. fuzzy c-mean [cit], and fuzzy c-medoids [cit] algorithms are widely used examples of c-mean algorithm [cit], as illustrated in figure 3."
"subjective assessment of the different requeue models is important, since the operator must ultimately accept or reject the recommendations set forth by such automated dsss. in developing dsss for complex mission planning involving the visual search task, two key subjective assessments are confidence and workload."
"when operators were ready to make an assessment of the target, they right clicked the image on their best estimate of the target location, and a small menu appeared. one menu item was \"submit,\" which, when selected, was evaluated as correct or incorrect by the software. if the participant mistakenly clicked the screen, a \"cancel\" menu allowed the participant to return to the search."
"this paper has developed a choice model for an operator performing visual search tasks generated from multiple unmanned vehicles. using previous experimental data that demonstrated that human search accuracy could decay with time, we have developed a novel retrial queuing model of an operator that provides the operator with an additional choice by allowing the operator to requeue challenging targets."
"an additional consideration would be to quantify what kind of additional imagery information would be desired by an operator to increase the likelihood of detection in the event of a relook. finally, a tighter coupling between the role of requeuing and the mission parameters needs to be made. for example, it will be beneficial to understand precisely what the role of vehicle routing is for the purposes of aiding the relook tasks (e.g., with different path planners), as well as the number and heterogeneity of uavs."
the result of the analysis shows that svm and ann performed well in several comparisons. the main purpose of the comparison of hybrid approach is to highlight the applicability of different classification algorithms and complement their limitations [cit] .
"the text document is represented as a vector space model. in a vector space model, each dimension represents a separate term as a single word, keyword, or a phrase. document matrix can be represented with n documents and m terms where any non-zero entry in the matrix indicates the presence of a term in the document [cit] . feature vectors represents document feature. two basic methods have been proposed to calculate feature vectors: (a) term frequency (tf) and (b) inverse document frequency (idf) [cit] ."
"in addition, even if we understood how operators requeue targets, it would be difficult to analyze reqm in closed form since real models for retrial queues may deviate from some of the common assumptions necessary for analytical tractability. in reqm, for example, requeuing invalidates the assumption of independent arrivals. furthermore, choi and park [cit] assume that a task in the orbit queue can only be serviced if the nominal queue is empty. this is not a suitable representation for the multiple-uav relook problem, since a target can be requeued regardless of the remaining outstanding visual search tasks. queuing theory is also concerned with queue stability, in the sense that the number of tasks does not grow unbounded over time, which may not be a valid assumption when human performance is considered."
"text mining using cloud computing: another challenge of the current era is to implement text mining techniques in cloud-based infrastructure that allow people to access technology-enabled and scalable services via internet [cit] . however, in cloud computing, user may have difficulty in the process of storing and retrieving the document [cit] . automatic document archiving can be performed using the text mining techniques. moreover, text processing and text aggregation in cloud would be the issues for the researchers."
"text in social networks: in social networks, textual data may be large, noisy, and dynamic. moreover, interpreting emoticons (smile, sad) for expressing any specific concept or emotion is still a challenging issue for researchers. privacy and trust in online communication is also a major issue. application of ethical values, such as integrity, veracity, in online communication is the only effective way to build trust online."
"different words with similar meanings in a natural language are termed as synonymy. synonymy can be addressed by refining the query or document using the relevance feedback method. in the relevance feedback method, the user provides feedback that indicates relevant material regarding the specific domain area. the user asks a simple query and the system generates initial results in response to the query. the user marks the retrieved results as either relevant or irrelevant. based on the user-marked results the algorithm may perform better. the relevance feedback method is an iterative process and plays a vital role by providing relevant material that tracks user information needs [cit] . rocchio algorithm is an implementation of the relevance feedback method and is mainly used for document refinement. [cit] . the user must have sufficient knowledge to indicate relevance feedback [cit] . moreover, the relevance feedback algorithm may not work efficiently when the user spells a word in a different way. various spelling correction techniques can be used at the cost of computation and response time, such as hashing-based and context-sensitive spelling correction techniques [cit] ."
"to investigate further the existence of a benefit to relooking at targets later in the mission, we show the mean search times for correct detections and probability of errors associated with the different numbers of looks in fig. 12 . note that the mean search time for correct detection slightly decreased from 17 to 15 s with an increased number of relooks. this trend was also visible for targets that were missed. however, the overall probability of error increased as a function of the number of looks: from 22.7% in the first look to 41.5% in the second look, 63.2% in the third look, and only 85.7% in the last look."
"support vector machine (svm) algorithm is used to analyze data in classification analysis. in contrast to other classification methods, svm algorithm uses both negative and positive training data sets to construct a hyper plane that separates the positive and negative data. the document that is closest to decision surface is called support vector."
"artificial neural networks (ann) are parallel distributed processing systems specifically inspired by the biological neural systems [cit] .the network comprises of a large number of highly interconnected processing elements (neurons) working together to solve any specific problem. owing to their tremendous ability to extract meaningful information from a huge set of data, neurons have been configured for specific application areas, such as pattern recognition, fe, and noise reduction. in the neural network, connection between two neurons determines the influence of one neuron on another, while the weight on the connection determines the strength of the influence between the two neurons [cit] ."
"during the text gathering process, the text may be loosely organized and can be interpreted as irrational text integration or missing information. if the text has not been scanned carefully to identify the problems (as reported in section 1), then text mining might lead to the 'garbage in garbage out' phenomena [cit] . unstructured text may lead to poor text analysis that affects the accuracy of an output [cit] . the pre-processing phase organizes documents into a fixed number of pre-defined categories. pre-processing guarantees successful implementation of text analysis, but may consume considerable processing time [cit] . there are two basic methods of text pre-processing: (a) feature extraction (fe) and (b) feature selection (fs), which are detailed in the subsequent sections."
"electronic textual documents are extensively available owing to the emergence of the web. many technologies are developed for the extraction of information from huge collections of textual data using different text mining techniques. however, information extraction becomes more challenging when the textual information is not structured according to the grammatical convention. people do not care about the spellings and accurate grammatical construction of a sentence while communicating with each other using different social networking websites (facebook, linkedin, myspace). extracting logical patterns with accurate information from such unstructured form is a critical task to perform."
"in this experiment, a single operator was responsible for the coordinated search of an area using six homogeneous uavs. the objective of this experiment was to maximize the fraction found, which was explained to the participants as the total number of targets found out of the total number of possible targets in the environment (3). this experiment had two treatments: 1) a relook mode (a \"within-subject\" treatment) and 2) a timer condition to induce artificial time pressure (a \"between-subject\" treatment)."
"the keyword spotting technique is based on keywords specifically used for the description of certain emotions in the text [cit] . for instance, in english language verb, noun, and adjective can be used as the keywords for emotion detection. however, the basic disadvantage of keyword spotting technique is the dependency on the presence of obvious affective words in the text. for instance, the emotion 'sadness' cannot be derived from the sentence 'i lost my money', as the sentence does not specifically mention the word 'sad'."
k-nearest neighbour algorithm is a form of instant-based learning. the algorithm categorizes similar objects based on the closest feature space in the training set. the closest feature space may be determined by measuring the angle between the two feature vectors or by calculating the euclidean distance between the vectors. [cit] .
"there are two basic categories of learning methods used in neural networks: (a) supervised learning and (b) unsupervised learning. in supervised learning, the ann gets trained with the help of a set of inputs and required output patterns provided by an external expert or an intelligent system. different types of supervised learning anns include: (a) back propagation and (b) modified back propagation neural networks [cit] . major application areas of supervised learning are pattern recognition and text classification [cit] . in unsupervised learning (clustering), the neural network tends to perform clustering by adjusting the weights based on similar inputs and distributing the task among interconnected processing elements [cit] ."
"future work will include developing \"optimal\" relook policies, understanding that, in practicality, generating satisficing parameters is more realistic since optimality may be difficult to quantify in dynamic uncertain command and control settings [cit] . these policies will also be evaluated in tasks where the target is absent, leading to a richer set of operator models. moreover, additional work is needed to more fully understand the information processing ramifications of relooks as it is not clear whether the success of the relook mechanism is due to scene complexity, a possible attention filtering bias, or that operators had more confidence knowing that they had such a tool available. such understanding could possibly lead to identification of images in advance that could cause operator difficulty, possibly allowing them to be inserted in the queue at a more opportune time."
"operators did not have the ability to relook. operators were required to commit to the location of a target before returning to the uav planning task. 2) relook with consent (rwc): operators had the option of requeuing at any time, but after t rl seconds, a flashing message was displayed on the search screen to suggest to the operator to requeue. 3) relook without consent (rwoc): operators had the option of initiating a requeue at any time, but after t rl seconds, the target was automatically requeued. the second treatment involved the use of the timer and was inserted to provide operator feedback on how much time had been spent searching. previous work has shown that time pressure can cause different operator strategies, so the two experimental conditions were with and without a timer [cit] ."
"in scientific literature [cit], different fs techniques such as (a) latent semantic indexing (lsi) and (b) random mapping (rm) are discussed. lsi tends to improve the lexical matching by adopting a semantic approach, as in the case of semantic analysis, while rm creates a map through the contents of a large document set. any selected region in a map can further be used for the extraction of new documents on similar topics. a pre-processed document can be represented as in figure 1 . [cit] present the two most commonly used text mining techniques for text analysis in social networking: (a) text mining using classification (supervised) and (b) text mining using clustering (unsupervised)."
supervised learning or classification is the process of learning a set of rules from a set of examples in a training set. text classification is a mining method that classifies each text to a certain category [cit] . classification can be further divided into two categories: (a) machine learning-based text classification (mltc) and (b) ontology-based text classification [cit] and is illustrated in figure 2 .
"hierarchical clustering is very useful because of the structural hierarchical format. however, the approach may suffer from a poor performance adjustment once the merge or split operations are performed that generally leads to lower clustering accuracy [cit] . moreover, the clustering approach is not reversible and the derived results can be influenced by noise."
"using the results from the des-reqm simulations, an experiment was conducted with the objective of investigating the performance of a retrial queue when users had an available requeue option. the experiment was performed in reschu [cit], a simulation specifically tailored to investigate human-inthe-loop interaction with multiple uavs. a typical reschu interface is shown in fig. 6(a) . a single operator is tasked with handling n uavs in an environment where targets are nonmoving but appear at random intervals. when a uav (shown as a blue bell shape) reaches a location of interest (shown as a red diamond), a visual search task is initiated by the operator in the top left panel of the interface, with a magnified version of the search panel shown in fig. 6(b) . note that, in the visual search task panel, the operator can zoom in and out the display while panning the image. in addition, the operator can query the system with the \"query\" button to find out how many residual search tasks still need to be processed in the mission."
"a genetic algorithm (ga) is a heuristic search that simulates the natural environment of biological and genetic evolution [cit] . multiple solutions of a problem are presented in the form of a genome. the algorithm creates multiple solutions and applies genetic operators to determine the best offspring. gas are widely used to solve optimization problems. therefore, researchers are trying to use the utility of gas in social networking websites [cit] ."
"the pos tagging process is commonly used to add contextually related grammatical knowledge of a single word in a sentence. if the lexical class of the word is known, then performing linguistic analysis becomes much easier [cit] . various approaches are mentioned in the scientific literature for implementing pos tagging based on dictionaries [cit] . the most promising approaches used are rule-based ma and stochastic model, such as hidden markov model (hmm). in a rule-based approach, the text is decomposed into tokens that can be further used for analysis. moreover, hmm is a stochastic tagging technique mainly used to discover the most similar pos tagging from sequence of input tokens [cit] . parsing is a technique used for examining the grammatical structure of a sentence. the sentence is represented in a tree-like structure, termed as parse tree, that is mainly used for analysis of correct grammatical order of a sentence. a parse tree can be constructed by using a top-down or bottom-up approach [cit] ."
the basic purpose of fs is to eliminate irrelevant and redundant information from the target text. fs selects important features by scoring the words. the importance of the word in the document is represented by the assigned score [cit] .
"basic components of ontology include (a) classes, (b) attributes, (c) relations, (d) function terms, and (e) rules [cit] . ontology needs to be specified formally [cit] . formal relation can be represented as (a) classes and (b) instances [cit]"
"online information usually resides in digital libraries in the form of online books, conference, and journal papers. in digital libraries, searching techniques are based on a traditional keyword matching approach that may not satisfy requirements of users owing to lack of semantic reasoning capabilities. xu recommended an ontology-based digital library system that analyzed the query with respect to semantic meanings and revealed better results when compared with traditional keyword-based searching approach [cit] . however, semantic analysis is computationally expensive and challenging for researchers, especially for large text corpora such as text data in social networking websites [cit] ."
"to fulfil the needs of a distributed knowledge society, available natural communication tools must understand the meaning of a sentence [cit] . keyword spotting technique is used to determine the useful contents from the textual message [cit] . the keyword spotting technique is completely based on the wordnet-affect, which is a semantic lexicon commonly used for the categorization of words that express similar emotions [cit] . another example is a survey on text mining in social networkssentiwordnet that generally uses wordnet synonyms for measuring the emotions on the basis of two scales, such as positive emotions (happiness) and negative emotions (hate) [cit] . ling analyzed the sentence syntactically and identified the basic emotions by analyzing words with respect to context and structure patterns [cit] ."
"the field of text mining is gaining popularity among researchers because of enormous amount of text available via web in the form of blogs, comments, communities, digital libraries, and chat rooms. ann can be used for the logical management of text available on web. jo proposed a new neural network architecture for text categorization with document presentation called neural text categorizer (ntc) [cit] . ntc comprises of three layers: (a) input layer, (b) output layer, and (c) learning layer. input layer is directly connected with output layer, whereas learning layers determine the weights between input and output layer. the proposed approach can also be used for organizing the text in social networks [cit] ."
"the rest of the survey is organized as follows. section 2 presents different pre-processing techniques. section 3 describes and different classification-based algorithms for text mining in social networks. in section 4, the clustering techniques used for text mining are described. section 5 presents current challenges and future directions. finally, section 6 concludes this survey."
"researchers have proposed several algorithms for computing semantic similarities between text, such as resnik and lin algorithms [cit] are proposed to measure the semantic similarity of text in a specific taxonomy. [cit] introduced a novel approach to automate the ontology construction process based on data clustering and pattern tree mining. the study comprises of two phases: (1) document clustering phase creates a group of related documents using k-mean clustering technique and (2) ontology construction phase creates inter-concept relation from the clustered documents, whereas inter-concept relation is termed as similar concept relationship. the author implemented the proposed approach on weather news collected form e-paper and revealed remarkable results by extracting the regions with high temperature."
"relationships, attributes, and classes in ontology can be structured hierarchically as taxonomies [cit] . the process of constructing lexical ontology by analyzing unstructured text is termed as ontology refinement. dt is a method to semantically describe the concepts and the similarities between the concepts [cit] . different algorithms of dt are used for classification in many application areas, such as financial analysis, astronomy, molecular biology, and text mining. as text classification depends on a large number of relevant features, an insufficient number of relevant features in a dt may lead to poor performance in text classification [cit] ."
"the process of fe can be further categorized as: (a) morphological analysis (ma), (b) syntactical analysis (sa), and (c) semantic analysis. ma deals with individual words represented in a text document and mainly consists of tokenization, remove-stop-word, and stemming-word [cit] . in tokenization the document is treated as a sequence of word strings and splits word by removing punctuations [cit] . in remove-stop-word phase, stop words, such as 'the', 'a', and 'or' are removed. remove-stop-word phase improves the effectiveness and efficiency of text processing because the number of words in the document are reduced [cit] . stemming-word is the linguistic normalization technique generally used to reduce a word to the root form, such as the word 'honesty' can be reduced to root form of 'honest' or the word 'walking' can be reduced to the root form of 'walk'. different stemming algorithms are available in the literature, such as brute-force, suffix-stripping, affix-removal, successor variety, and n-gram [cit] ."
"different classification algorithms have been used for text classification and analysis. however, literature [cit] b; [cit] ) shows that the combination of different classification algorithms (hybrid approach) provides better results and increased text categorization performance instead of applying a single pure method. the result of applying hybrid approach to large text corpora heavily depends on the test data sets. therefore, there is no guarantee that a high level of accuracy acquired by one test set will also be obtained in another test set. moreover, for better performance of the hybrid approach, several parameters need to be defined or initialized in advance. table 1 provides an overview of different hybrid approaches used for text classification that can be further used for the text analysis in social networking. however, selecting the classification approach for text analysis in social networks totally depends on the data set and nature of the problem being investigated [cit] ."
"instance-based learning algorithms (also known as lazy algorithms) are based on the comparison between new problem instances and instances already stored during training [cit] ). on arrival of a new instance, sets of related instances are retrieved from the memory and further processed so the new instance can be classified accordingly. algorithms exhibiting instance-based learning approaches are described in the subsequent text."
"while it will be the topic of future work to investigate whether the analytical methods from retrial queuing theory may be applicable, a method for admitting less restrictive assumptions can be addressed by using des. first, where analytical methods are not available for analyzing a queue in closed form, des can help provide insight of the queue transient properties. second, des can be used for tuning the appropriate set of parameters to be used for human-in-the-loop experiments, such as determining appropriate task arrival rates. the ultimate goal of the des environment in this effort is to provide a high-fidelity simulation of the experiment, and this section discusses a des model of reqm, des-reqm, which is composed of three main parts: an environmental module, a routing policy module, and a requeue policy module."
"most of the scientific literature [cit] focuses on specific techniques of text mining for information extraction from text documents. however, a thorough discussion is lacking on the actual analysis of different text mining approaches. most of the surveys emphasize on the application of different text mining techniques on unstructured data but do not specifically target the datasets in social networking websites. moreover, the existing research papers cover the text mining techniques without mentioning the pre-processing phase [cit] that is an important phase for the simplification of text mining process. in contrast, this survey attempts to address all the above-mentioned deficiencies by providing a focused study on the application of all (classification and clustering) text mining techniques in social networks where data is unstructured."
"statistical techniques for document representation (as described in section 3.1) are not sufficient because the statistical approach neglects the semantic relations between words [cit] . consequently, the learning algorithm cannot identify the conceptual patterns in the text [cit] . ontology can be the solution of the problems by introducing explicit specification of conceptualization based on concepts, descriptions, and the semantic relationships between the concepts [cit] . ontology represents semantics of information and is categorized as: (a) domain ontology consists of concepts and relationship of the concepts about a particular domain area, such as biological ontology or industrial ontology and (b) ontology instance related with automatic generation of web pages [cit] ."
"in this experiment, the probability of detection and the mean search time improved with the presence of the requeue option, whether mandated or not. the cost of such requeues meant somewhat increased objective workload (but no increase in subjective workload), and the more the participants that accessed the relook feature, the fewer the targets that they were likely to find. such results highlight the cost-benefit issues surrounding any new decision support tool in that it can often provide benefit, but there are also possible negative consequences if such a tool is invoked too often."
"probabilistic clustering is an iterative method that calculates and assigns probabilities for the membership of an object [cit] . based on the probability measurements, an object can be a part of any specific cluster. probabilistic clustering technique is popular because of the ability to handle records of a complex structure in a flexible manner. as probabilistic clustering has clear probabilistic foundations, finding out the most suitable number of clusters becomes relatively easy [cit] . examples of probabilistic clustering are the exception maximizing algorithm and multiple cause mixture model. however, these approaches are computationally expensive [cit] ."
"the choice model is the underlying mechanism under which the operator can make a detection decision (e.g., whether there is a target in the image or not) or decide that a task needs to be requeued."
"in difficult search environments, operators searching imagery in multi-uav environments may desire more choices than determining if a target is present or absent in an image. more specifically, operators may seek additional information in order to find the target, possibly through another visit later on in the mission, or they may choose to ignore a task because there is not sufficient information to make a confident assessment. we hypothesized that, instead of a two-choice model, operators would be better served by having a third option of reevaluating a search task at a later time by requeuing the image and taking another glance via a relook. throughout this paper, we make a distinction between the choice of requeuing, which is the abandonment of the current search task, and a relook, which is an additional glance at a previously searched image."
"to interpret a logical meaning from a sentence, a grammatically correct sentence is required [cit] . sa provides knowledge about the grammatical structure of a language that is often termed as syntax. for instance, the english language comprises of noun, verb, adverb, punctuation, and other parts of speech. the sa technique comprises of: (a) part-of-speech tagging (pos tagging) and (b) parsing."
"this section presents simulation results of the performance using the previously developed operator choice models analyzing one hundred 10-min-long simulated uav missions. in this setting, we analyzed the detection probability (p d ) and the fraction found [j f, given by (3)]."
"extensive work in visual search has also emphasized the use of probabilistic models that relate mean decision time to the mean time to search [cit] . recent work has also moved beyond the mean decision times and analyzed the role of parameter identification for parameterization of the search time distributions [cit] . while the characterization of the search is also important from a cognitive science perspective, the work in this paper does not address the low-level details of how the search is accomplished but rather seeks to understand and quantify the effect of sequential searches within the context of supervisory control."
"the k-medoid algorithm selects the object closest to the centre of the cluster to represent the cluster [cit] . in the algorithm, the k object is selected randomly. based on the selected object, distance is computed. the nearest object with respect to k will form a cluster. remaining objects take the place of k recursively until the quality of the cluster is improved [cit] . the k-medoid algorithm has many improved versions, such as pam (partitioning around medoid), clara (clustering large applications), and clarans (clustering large applications based upon randomized search). k-medoid algorithms work well for small data sets, but give compromised results for large data sets [cit] ."
"increased error with relooks of the same image has profound ramifications for supervisory control of uav missions, because it suggests that operators may be willing to make a mistake to avoid repeating the same searches. furthermore, it also suggests that the perceived benefit of the requeuing methodology is the freedom to keep exploring other tasks, rather than being forced to make a choice on a difficult image. (recall that targets that were requeued by the operators were enclosed by an orange circle so that they could be clearly observed by the operator.)"
"the integrated system design shown in fig. 4 .a is implemented for our experimental setup. for classification purposes, support vector machine or svm [cit] with rbf kernel is used. the gamma and nu values of rbf kernel are set empirically for the experiment. among many implementations of svm present in the literature, libsvm [cit], an open source svm tool is used here. as discussed earlier in the present work, experiments are performed over 3 different datasets: (a) bangla basic character set (b) bangla compound character set and (c) randomly mixed set of both bangla basic and compound characters. finally, a comparative analysis is done to measure the performance of our proposed method, based on the metrics of recognition accuracy of the classifier and minimal number of discriminative regions used. experimental results are shown in table 1, fig 7.a and fig 7. b. table 1 shows the recognition accuracies achieved by all the methods. figure 7 .a and 7.b provide a comparison of the minimum number of most discriminative regions used for identifying the character. from the data collected, it can be observed that the proposed method achieves a significant 2.3% increment in recognition accuracy with 43.75% less number of discriminating regions for bangla basic characters, 0.6% increment in accuracy with 12.5% less number of discriminating regions for bangla compound characters and finally 1.2% increment in recognition accuracy with 37.5% decrease in number of discriminating regions is observed for dataset of mixed bangla basic and compound characters. [cit] . so it has the least number of region rejections and least increment in recognition accuracy. table 2 it is clear that the proposed method performs faster classification of test samples than its contemporaries. this may be attributed to better representation of a character by using lesser number of regions and using only the most informative regions. fig. 8 shows some of the correctly classified and misclassified characters by the proposed system. an efficient system is proposed here for recognizing handwritten characters by identifying the regions of the character image which contain most of its discriminating features. an enhanced harmony search is used here for identifying the most informative regions. the proposed method is evaluated for three datasets. evaluation is based on two factors, recognition accuracy and minimum number of local regions sufficient to identify the character correctly. from results of the experiments, the present work showed noticeable reduction in the number of most discriminating regions as well as significant increment of the recognition accuracy. to the best of our knowledge, this is the first work that uses the power of harmony search for sampling local regions to recognize handwritten characters. the results have shown great promise in this approach. therefore it opens up a new frontier for more successful handwritten character recognition systems. also it presents with future scope for researchers to improve its performance by using different feature-set or employing a more powerful variant of harmony search method present in the literature."
"the proposed method has been tested on the datasets of bangla basic [cit], bangla compound [cit] and randomly mixed dataset of both bangla basic and compound characters. it is worthy to mention here that bangla alphabet contains 50 basic characters; out of these 11 are vowels and 39 characters are consonants. apart from these it is also enriched with more than 334 compound characters [cit] . the datasets used in the experiment are developed at cmater lab, jadavpur university, kolkata."
this method uses the framework of hs algorithm with some enhancements to further improve its performance. a roulette wheel selection method is used for both initial population generation and memory selection. this gives some form of control over the quality of harmony selected during the improvisation of the algorithm. iterative decrement of harmony memory size guarantees that minimal numbers of most discriminating regions are being considered. heuristics of the proposed method reduces the time complexity of the algorithm to o(c + 4
"one of the most common approaches of feature extraction in handwritten character recognition is to segment the sample image into several regions, extract the local features after pinpointing the minimum number of regions which are most informative in discriminating the character from others [cit] . to obtain the local feature-set, different techniques are present in the literature [cit], but none of them ensures optimal success rate [cit] . heuristics methods are applied to search the optimal, most informative regions out of all possible local regions. several nature inspired metaheuristics algorithms such as genetic algorithm [cit], artificial bee colony [cit], bacterial foraging [cit] etc. are used very recently. from this perspective, a new region sampling method has been introduced here for recognition of handwritten bangla characters. the proposed methodology has been tested on the databases of bangla basic, compound and a mixed dataset of both basic and compound characters."
"the objectives of the present work are three-fold: (a) designing a region sampling strategy to select the regions of the image containing the most discriminating features describing the character, (b) evaluating the performance of the proposed method on a dataset containing bangla basic characters, bangla compound characters and a randomly mixed dataset containing both bangla basic and compound characters, (c) performing a comparative analysis of the proposed method with the basic hs algorithm for all the datasets. fig. 4 and fig. 5 show the block diagram of the region selection strategy of the proposed method."
"optical character recognition (ocr) for handwritten characters is an active area of research for researchers all around the globe [cit] . motivation behind this is its large scope of applications; but the success of commercially available ocr could not be extended to handwritten characters as various writing styles make it quite difficult to identify the discriminating features of the characters itself. in spite of the huge popularity [cit] of bangla script, ocr of complete bangla alphabet of handwritten characters has not received much attention from researchers until very recently. due to numerous writing styles, huge and complex alphabet and presence of abundant compound characters, bangla script poses a challenge to the researchers."
the algorithm is terminated when the experiment has successfully produced 25 generations for each possible size of non-trivial local region subset. hence maximum number of iterations (ni) of the algorithm is 25.
"systematically, using different types of visual (animations or static pictures) and verbal (written or spoken explanatory text) learning materials. although there are a number of studies in the literature describing difficulties in adapting learning environments to people's cognitive styles (e.g., kirschner & van merriënboer, 2013), we hope to shed more light on the learning processes related to visual/verbal cognitive style and different treatment factors. considering the growing importance of personalized/adaptive education (cf. [cit], our study might contribute to a deeper understanding of learning processes, which in turn might improve educational practice."
"lastly, in order to clarify the effect of visual cognitive style with written text but not with spoken text, it would be interesting to design a study that includes a general resource-consuming task in the learning situation. in this respect, a secondary task that engages executive control resources or working memory, could help investigate possible explanations of this effect."
"the logical scheme of the full fault-tolerant flight controller is reported in fig. 1 . the picture shows four main elements, the autopilot (a/p), the actuators health monitor, the stability and controllability augmentation system (scas), based on the damf, and the control allocation module. the last two elements represent the fault-tolerant control system (ftcs) that is the object of this chapter. although adaptive control exhibits great reconfiguration capabilities, in case of in-flight faults, abrupt and dramatic changes in control effectiveness and/or plant dynamics may occur, such that the adaptive controller may not be able to recover the vehicle. therefore, an adaptive controller could take advantage of a control allocation module to ensure the generation of the demanded moments by the optimal control system, both in healthy and faulty conditions of the actuation system. the remaining two elements are not the focus of this work and they are developed with classic techniques. in details, the a/p is designed by means of the classic sequential loop closures, implementing the typical guidance modes for the aircraft (see table. 1). also the health monitoring of actuators is a very trivial system based on the comparison between the input and the output of each actuators. in the numerical validation it is supposed to use a monitoring system with the capability to detect an actuator fault within 10 seconds, and to pass the binary information healthy/faulty to the control allocation system. in the following two sections the elements of the ftcs are briefly recalled."
but how is it processed in working memory? the interaction found in our research might suggest that the change of channels from visual to auditory occurs in the later stages of processing information and that visual cognitive style is at least related to proceeding information conveyed in written form.
"the title compound, [pb 2 (c 2 o 4 )(no 3 ) 2 (c 10 h 8 n 2 ) 2 (h 2 o) 2 ], was synthesized hydrothermally. the binuclear complex molecule is centrosymmetric, the inversion centre being located at the mid-point of the oxalate c-c bond. the pb ii ion is heptacoordinated by the o atom of one water molecule, two oxalate o atoms, two nitrate o atoms and two 2,2 0 -bipyridine n atoms, forming an irregular coordination environemnt. intermolecular o-há á áo hydrogen bonds between water molecules and oxalate and nitrate ions result in the formation of layers parallel to (010). -interactions between pyridine rings in adjacent layers, with centroidcentroid distances of 3.584 (2) å, stabilize the structural setup."
"this manoeuvre consists in the interception of the localizer beam, parallel to initial flight path, but opposite in versus. so, in the early stage of the manoeuvre, a right turn is performed, and then the capture and the tracking of the localizer beam are carried out. the fault, instead, consists in a runaway of both upper and lower rudder surfaces, so giving a strong yawing moment opposite to the desired turn. the initial flight condition data are summarized in table 4 . in this failure case, a classical technique is totally inadequate to face such a failure, so leading the aircraft to crash into the ground. instead, the damf shows to be robust enough to deal with this failure condition and it makes the aircraft to accomplish the manoeuvre, even though with reduced performance. the control allocation technique, instead, shows a sensible improvement of the robustness (see fig. 4 ), if compared to the damf technique. the awareness of the fault (detected 10 sec after it actually occurs) allows the control laws to fully exploit all the efficient effectors, thus accomplishing the manoeuvre smoothly. it is worth noting that in this case the damf without ca is robust enough to accomplish the manoeuvre, even though with degraded performances."
"with q is a positive definite weighting matrix. by calculating the time derivative of the lyapunov candidate function and by casting it to get null, the following conditions can be found, that represent the adaptation rules for the control laws parameters. moreover by taking into account the equations 10, 11 and 13 it is possible to demonstrate the non-positiveness of lyapunov candidate function derivative:"
"additionally, we might conclude that the modality effect can be considered as a treatment factor that enhances learning with static pictures and animations and leads to comparably good learning outcomes regardless of cognitive style. by way of this reasoning, highly developed visual style can be considered as a compensator when learning with static pictures and written text (cf. [cit] highly developed visual cognitive style can also be an \"obstacle\""
the spoken text conditions were independent from the magnitude of the visual cognitive style and delivered comparable results for all participants regardless of the multimedia environment (static pictures or animations).
"interpreting pictures (particularly animated pictures) and reading onscreen text at the same time can be too demanding for the learner's visual channel (in which, text is processed at least initially) and may lead to cognitive overload [cit] . in contrast, when learning simultaneously from pictures and narration, learners can use the visual channel for processing pictures and the auditory (verbal) channel for processing text. doing so may result in better learning outcomes because working memory resources are used more efficiently, which prevents cognitive overload [cit], 2002 . however, there are some evidence that the modality effect is not necessarily based on freeing working memory resources, or on the reduction of cognitive load in the visual channel, but rather on the lack of the necessity to split attention between text and pictures, as learners listen to the narration and view the pictures at the same time [cit] ."
"in this condition, while in straight and levelled flight, the aircraft experiences a stabilizer runaway to maximum defection that generates a pitching down moment. the initial flight condition data are summarized in fig. 3 (a) shows the great improvement achieved thanks to the adoption of the control allocation. note that the classic technique, for this failure condition, shows adequate robustness. this is caused by its structure. in fact, the longitudinal control channel (pi for pitch-angle above proportional pitch-rate sas) affects only the elevators, while the stabilizer is supposed to be operated by the pilot separately. in this way, the stabilizer runway results to be a strong, but manageable disturbance. instead, the damf tries to recover the attitude lavishing stronger control effort on the faulty stabilizer, the most effective surface, with bad results. the awareness of the fault on the stabilizer gives the chance to the ca technique to compensate by moving the control effort from this surface to the elevators, thus achieving the same results of the classical technique. as it is also evident in the time plots of fig. 3 (b) when the failure is detected and isolated (here it is supposed to be done in 10 sec after the failure occurs), the aircraft recovers a more adequate attitude to carry out properly the manoeuvre."
"additionally, it is important to underline that conducting research on differences between visual and verbal working memory processes is challenging because of the difficulties in controlling or assessing the used strategy. [cit], 47 students were examined with repetitive transcranial magnetic stimulation (rtms) to examine the use of visual processing strategies in the dsb (the digit span backwards task). the results prove the critical importance of the visual cortex for visualizers compared to verbalizers."
"one limitation of our study is the unbalanced sample that contains more females than males. however, any reference concerning gender differences and imaging ability must take into account the type of imagery and the measurement instrument used (campos, pérez-fabello, & gómez [cit] ). as we have used questionnaires instead of performance tests, we could assume that no considerable gender differences influenced our results (cf. [cit] . in any case, such an unbalanced sample might be considered highly ecologically valid, because more women than men study biology at universities in germany. nevertheless, future research conducted on a balanced sample would be desirable."
"a mixture of oxalic acid (0.0634 g, 0.5 mmol), 2,2′-bipyridine (0.0781 g, 0.5mmol), pb(no 3 ) 2 (0.3312 g, 1mmol), naoh (0.0400 g, 1mmol), water (10 ml) and ethanol (5 ml) was placed in a parr teflon-lined stainless steel vessel (25 cm"
"furthermore, the results indicate that the preferred cognitive strategy determines the processing modality to greater extent than the presentation modality. both studies cited above show that visual and verbal proceeding is a complex process, in which not only the modality of the stimulus but also the preferred cognitive strategy play important roles."
"but why is a more developed visual style beneficial when learning from static pictures and written text, yet seems to be a problem when learning from animations and written text? one possible explanation is the expertise reversal effect [cit] . in the written text condition, an animation seems to be an obstacle rather than an aid for people with a more pronounced visual style. is this the case, because the animation, especially a transitory animation, provides a \"ready-made\" product, which inhibits higher developed visualizers to act in their preferred way, in the form of organizing visual information in the way most suitable for them? when confronted with the animation with written text, do less developed visualizers rely simply on one of the two visual sources: either on the written text alone or on the animation alone? [cit], which suggested that animations can provide too much help for high prior knowledge learners."
"in this chapter a fault-tolerant fcs architecture has been proposed. it exploits the main features of two different techniques, the adaptive control and the control allocation. the contemporaneous usage of these two techniques, the former for the robustness, and the latter for the explicit actuators failure treatment, has shown significant improvements in terms of fault-tolerance if compared to a simple classical controller and to the only adaptive www.intechopen.com controller. the ability of the damf to on-line re-compute the control gains guarantees both robustness and performance, as shown in the proposed test cases. however, the contemporary usage of a control allocation scheme allowed improving significantly the fault-tolerance capabilities, at the only expense of requiring some limited information about the vehicle actuators' health. therefore the proposed fault-tolerant scheme appears to be very promising to deal with drastic off-nominal conditions as the ones induced by severe actuators failure and damages thus improving the overall adaptive capabilities of a reconfigurable flight control system."
"as mentioned in the introduction a control allocation algorithm can be very useful for control reconfiguration purposes due to its ability of managing actuator redundancy, so to redistribute control effort after a failure event. moreover, it may be a great support for an optimal control strategy, such as the damf that works well in the case of limited faults (i.e. the plant dynamics do not change dramatically) and in any case it does not take into account the limited range and limited rate of the control variables."
"where the matrix product cb is the high frequency gain of the healthy system (no faults). in the event of one or more faults, system defined in equation 1 becomes:"
"in order to destroy the weeds in his garden, a gardener used the dcmu herbicide. this compound prevents electron transfer to plastoquinone. consequences are the following:"
"after some manipulations [cit], here left out for the sake of brevity, it is now possible to write the real expression of the error dynamics taking into account a parameters variation:"
"our study did not yield significant results regarding the verbal cognitive style, which, on one hand, seems to confirm the twodimensional structure of visual-verbal cognitive style but, on the other hand, also raises new questions-especially, if the verbal cognitive style plays a role while learning from text-picture combinations. if so, what type of role and under which circumstances? the problem might also stem from the sample. a clear (or highly developed) verbal style is rather rare (cf. [cit] ), hence, it is difficult to find a sufficient sample of highly developed verbalizers. overall, future studies should be performed with previously selected, gender-balanced groups of highly developed verbalizers and less developed verbalizers in order to shed more light on their learning behavior."
"thus, the question is not as simple as whether pictures or animations result in better learning outcomes, but rather under what circumstances. [cit] defined principles for learning with multimedia that concern ways of constructing multimedia environments to help learners overcome cognitive overload. among these, the modality principle plays an important role and states that presenting computerbased pictorial material with spoken explanatory text (instead of written text) makes the learning material easier to comprehend."
"second, the modality of the explanatory text has an impact on learning outcome. this suggests that the written mode should be especially applied with caution, because it is negatively related to learning outcome of higher developed visualizers. one possible method to overcome this shortcoming could be to offer students a set of multimedia learning environments to choose from. another possibility is to design an adaptive, personalized learning environment."
"a further challenge regarding visual-verbal cognitive style is an inconsistency regarding its structure. in this regard, some researchers claim it is a one-scale dimension with two ends [cit], whereas others interpret two different scales (e.g., [cit] ), or two scales with the visual scale subdivided into an object and spatial subscale (e.g., [cit] . in this respect, our study aims to shed further clarity on this issue, because in the case of a one-dimensional structure of visual-verbal cognitive style, we could expect that verbal cognitive style would play a comparable but inverse role in multimedia learning in comparison with visual cognitive style. put differently, if the one-dimension structure is true, people with high scores on the verbal cognitive style scale would learn similarly to less developed visualizers."
"a notable challenge that is faced when designing a study on the visual-verbal dimension is the great inconsistency in the literature as to whether the visual-verbal dimension should be treated as a cognitive style (e.g., [cit], as a learning preference (e.g., [cit] ), or as a learning style (e.g., [cit] . [cit] on 14 measures resulted in the identification of four separate factors: cognitive style, learning preferences, spatial ability, and general achievement. three of these factors referred to the visual-verbal dimension of information processing, namely, cognitive style, learning preferences, and spatial ability. [cit] defined spatial ability as a type of cognitive ability, learning preferences as a tendency to choose pictures or texts when learning, and cognitive style as a way of thinking-either more in words or in pictures. [cit] definition of a cognitive style as an individual manner of organizing and processing information, the current study refers to the visual-verbal dimension as a cognitive style. we aimed to compare learners with verbal or visual cognitive style when learning with different types of visualizations."
"where a e is a stable and properly chosen matrix, and φ represents a bounded forcing function, it is possible to write the following identities: equations 7 allow to write the expressions of the optimal terms g 0 *, c 0 *, v * and k 0 to obtain a perfect model inversion that guarantees the asymptotical stability of the closed loop system and the asymptotical null error. in order to evaluate the left hand terms (the gains of the controller), equations 8 require matrix b m to be invertible and cb matrix to be pseudo-invertible. while the former is a design parameter, the latter, called high frequency gain, is a structural characteristic of the plant. anyway, modern aircrafts have typically a sufficient redundancy order for the control surfaces, thus ensuring not to lose rank order even in the case of single and often double actuators failure. concerning the c matrix, no sensor failure cases are addressed in this chapter, anyway the device redundancy or several techniques, available in literature (f.i. kalman filtering), may ensure a full state feedback, even though each signal may lose accuracy in case of sensor failure. it should be anyway noted that the control parameters of equation 8 do not take into account the system parameters variation. however, the system parameters uncertainties can be modelled by a proper variation of the matrices in equation 1. finally, a set of adaptation rules is necessary to react to the system parameters variation and uncertainty, lyapunov theory furnishes a very efficient solution. first of all, let us define the differences between the actual adaptive parameters and the optimal ones:"
"the scas module is made of two nested sub-modules, taking advantage of the dynamics separation principle, being the angular rate dynamics sufficiently faster than those of the attitude ones. this two modules architecture also leads to a relevant reduction of the overall complexity (in terms of states number) of the adaptive algorithm. the detailed structure of each multi input multi output (mimo) controller is reported in fig. 2 . the design of both inner and outer loops consists in tuning some parameters. first of all, the matrices a m and b m, representing the dynamics of the reference model, must be selected with the limitation that the former must trivially be chosen with negative eigenvalues and the latter must be chosen invertible. these two matrices actually define the control system performance requirements. for both attitude and rates regulators, a couple of very simple reference models made of two diagonal systems (1 st order and decoupled systems) have been chosen. the desired error dynamics are chosen through the matrix a e by which, it is also possible to modify the system capability to reject noise and disturbances. the matrix q, used in the equation 12 for the calculation of p, has the meaning of a weighting matrix. by fine tuning this matrix, it is possible to give more or less relevance to the tracking requirement of one or more output variables with respect to the others. finally, the three parameters γ 1, γ 2 and γ 3 (evaluated by means of a trial and error procedure) are used to regulate the adaptive capability. as a reminder, in table 2 all the design parameters are reported."
"there are various implications of our research for learning and instructional design. first, cognitive styles seemingly do exist and have an impact on learning. this finding can be especially interesting in relation to the use of ai in learning. personalized educational platforms can make learning environments more flexible when answering learners' needs and characteristics (e.g., [cit] ) ."
"then, the students completed the two questionnaires regarding visual-verbal cognitive style (idq, vvq). after answering three prior knowledge questions, they watched the displayed learning environment twice (20 min). participants were randomly assigned to one of four conditions (versions). at the end of the study, participants figure 1 exemplary snapshot of a static picture with written text. the text says: \"plastoquinone receives two electrons from electron acceptor in photosystem ii. additionally, it uptakes two protons from the stroma. as a result, plastoquinone turns into dihydro-plastoquinone.\" [colour figure can be viewed at wileyonlinelibrary.com] answered 20 post-test questions. the whole study was computerbased and lasted for about 1 hr."
"as can be derived from figure 2, a higher visual cognitive style is coupled with better learning outcomes with static pictures accompanied by written text. in contrast, higher visual cognitive style is associated with poorer learning outcomes with animations accompanied by open question. max. 8 points."
"according to tabbers, verbal explanations were actually processed through the verbal channel, because interpreting written text only involves visual resources at the initial stage. considering the necessity to split attention between written text and pictures, there is not enough time to process both modes of information, hence, the visual information can only be processed superficially. higher developed visualizers, as kind of experts in using and processing visual information, might be able to handle these difficulties better than less developed visualizers. for less developed visualizers, the necessity to deal with two visual information sources, even if only initially, could have been too demanding."
"overall, participants showed better learning outcomes when learning with spoken text than when learning with written text, which is in line with the modality principle [cit] . in this regard, interpreting a transient learning environment accompanied by a spoken narration does not require splitting attention between written text and pictures."
"the core module of the whole flight control system is the scas that is in charge of guaranteeing vehicle attitude control and stability. as already said, the proposed algorithm for this module has been designed using a direct adaptive model-following method [cit], having the advantage of strong robustness against model parameter uncertainty, and a good capability of reacting to system parameters' variation. moreover, the model following strategy lets the designer to define in a clear and simple way the reference dynamics for the system, thus making this control strategy very attractive among other available robust control techniques. in the following some recalls about the damf are given."
"likewise, the question when and how this switch from one channel to another occurs is far from being clear. [cit], using repetitive transcranial magnetic stimulation (rtms) and fmri paradigm, propose the conversion hypothesis, namely, that people scoring high on the verbal cognitive style questionnaires have a tendency to code even nonverbal information into the verbal domain. hence, according to this hypothesis, the way of representing information is not determined by the way how the information was conveyed but more by the learners' characteristics. why shouldn't the same be true for participants with visual cognitive style?"
"the manoeuvre, here considered, is the same described in the previous subsection, but the failure scenario consists in the loss of the vertical tail [cit] . the initial flight condition data are summarized in table 4 . this is both a structural and actuation failure, in fact, the loss of the rudders strongly affects the lateral-directional aerodynamics and stability, compromising the possibility to damp the rotations about the roll and yaw axes. in this case (see fig. 5 ), the classical technique is not able to reach lateral stability. instead, no significant differences are evidenced between the two versions of the adaptive fcs (with and without ca). in fact, the information about the efficiency of the differential thrust is already available to the damf, due to the linear model of the bare aircraft. thus, as the tracking errors increase, the core control laws raise the control effort for both the rudders (failed) and the differential thrust. the latter is efficient enough to ensure the manoeuvrability."
"the products of the primary reactions of photosynthesis are: (1) oxygen, atp, nadph+h+ (2) glucose and water (3) carbon dioxide and oxygen (4) nadp+ and adp"
"with the centre of symmetry at the mid-point of the c-c oxalate bond ( fig. 1 ). the pb ii ion is hepta-coordinated in an irregular fashion by one water molecule, two nitrate oxygen atoms, two oxalate oxygen atoms, and two nitrogen atoms from 2,2′-bipyridine. the supramolecular assembly in the title compound is completed by o-h···o hydrogen bonds between the coordinating water molecules and oxalate and nitrate o atoms (table 1), resulting in the formation of layers parallel to (010) (fig. 2) . the structure is further extended by π-π stacking interactions between 2,2′-bipyridine molecules of adjacent layers. they overlap with a centroid-to-centroid distance of 3.584 (2) å."
"also of high interest are questions regarding a hindering effect of animations with written text in the group of highly developed visualizers, and the absence of such an effect in the animation and spoken text condition. does this serve as an example of an expertise reversal effect [cit] ? is it related to creating mental images by visualizers [cit] ? further studies might provide further insight about these questions."
"in response to the research question whether cognitive style interacts not only with the type of visualization but also with the type of modality, the triple interaction effect shown in our study suggests that the visual cognitive style and its influence plays an important moderating role when learning with animations or static pictures with written text."
"as above described control allocation algorithm has the aim of redistribute the control effort among the \"healthy\" surfaces to achieve the moments needed to keep the system along reference trajectory. in view of these considerations we argue that control allocation can be very useful, when used in conjunction with a direct adaptive control in those critical failure scenarios which can be hardly handled by the only use of the adaptive controller. nevertheless, in order to be effective for reconfiguration purposes, control allocation needs a fault detection (fd) system, which gives information about the health of the surfaces' actuators. this aspect could make unfeasible the use of a ca scheme. anyway, in the following sections it will be shown that, in order to obtain a satisfactory performance of the ca module, only limited failure information are needed. in fact, also a very simple monitoring algorithm, based on the actuator model and on the surface actual position, can be sufficient to establish whether an actuator is failed or not. the results show that the use of a ca scheme allows significant improvements of the control system performances also in the event of very critical failures and it only needs limited information about actuators' health. these features make the proposed control architecture very appealing for reconfiguration purposes."
"as the analyses of the verbal cognitive style dimension did not provide us with significant results regarding an interaction of verbal cognitive style and learning environment, we also could not state any assumption concerning a compensatory effect for people with a high verbal cognitive style. nevertheless, the lack of any significant effects on verbal cognitive style has provided additional evidence for understanding visual and verbal cognitive styles as two independent dimensions. as our study was predominantly focused on visual processing, verbal cognitive style did not play a significant role in the research. we can only assume that animations are easier to comprehend for learners with less developed visual cognitive style than for learners with highly developed visual cognitive style when the text modality is in the written form."
"where y m is the desired output for the plant, r is the given demand, a m and b m represent the reference linear system. the control laws structure is defined as follows:"
"the vertical tail separates from the aircraft. only the most meaningful conditions are here reported and discussed. to better demonstrate the improvement of fault-tolerance achieved by adopting the adaptive control in conjunction with the control allocation, comparison is made between three versions of the fcs, the first is a baseline scas developed with classic control techniques. the two remaining fcs are based on the adaptive scas with and without the ca respectively. as above said, only limited fd information are supposed to be provided, that is, the information about whether an actuator is failed or not but the current position of the failed actuator will be considered as unknown. the ca parameters have been set to:"
"if the additional information provided by the explanatory text, regardless if written or spoken, is actually verbal, it is still unclear how it is processed. as reading needs initial visual resources, the question arises for how long these visual resources are needed? and does visual cognitive style play a role even at this stage? we can assume that written text is processed visually at the sensory memory stage."
"another interesting contribution to this discussion is made by tabbers, martens, and van merriënboer (2001) who compared system-paced (when the speed of the presentation is defined by the system) and self-paced (when the learner has at least some influence over the speed of presentation) multimedia instructions with either written or spoken explanatory text. the results showed that the spoken modality yielded better learning outcomes than the written modality in the system-paced condition, whereas the self-paced condition yielded the opposite result-the written text condition was more beneficial for learners than the spoken text condition [cit] found a modality effect on the mental effort scale, as learning with spoken narration resulted in lower cognitive load than learning with written text. as our study applies only system-paced learning materials, the inclusion of another possible factor, in the form of visual-verbal cognitive style, might shed a new light on these findings."
"the explaining text/narration was identical in all versions. the learning environment was piloted before the experiment with a group of 30 biology students. this pilot study helped us to determine that the learning material should be demonstrated twice in order to obtain better learning outcomes and to choose 20 well-differentiating post-test questions. the learning environment was noninteractive by designparticipants did not have opportunity to control (e.g., stop or fast-forward/rewind) the visualized learning environment."
"where g 0, c 0 and v are proper terms generated by the adaptation rules, instead k 0 is a feedforward gain matrix off-line computed. it is now possible to calculate the error function as follows:"
"four different versions of a computer-based learning environment were developed-two versions with animations (with written or spoken explaining text) and two versions with static pictures (with written or spoken explaining text). the topic was primary reactions in photosynthesis (see figure 1 ). each version of the learning environment lasted 10 min and provided the same information. in the staticpictures versions, motions and movements were depicted by arrows."
"where i c is the input image, b c is the global background light, and denotes the size of an image patch. udcp also produces inaccurate estimation in some regions of the medium transmission. fig. 2 shows several inaccurate results of the medium transmission estimated by udcp."
most of underwater image restoration methods [cit] directly employ the image formation model of outdoor haze [cit] . such an image formation model can be described as:
"with greenish tone and low contrast, such as images ''divers'' and ''fish''. the poor robustness of iatp method potentially reduces its practical applications. for udcp method, [cit] . (e) result of udcp [cit] . (f) result of odm [cit] . (g) our result. [cit] . (e) result of udcp [cit] . (f) result of odm [cit] . (g) our result."
"in this paper, we have presented an underwater image restoration method based on joint prior using a new underwater image formation model. by jointing underwater image priors and considering the optical properties of underwater imaging, the medium transmissions of three color channels of an underwater image are estimated, respectively. in this way, the estimated medium transmissions are more accurate and robust than those of traditional methods, which leads to the improved contrast and brightness of our restored results. based on the assumption that the global background light is the same with the colors of light source, the color casts of underwater image can be effectively removed based on a new underwater image formation model. experimental results demonstrate the advantage of the proposed method when compared with several existing methods."
"where t r (x) is the refined medium transmission and nrer is the normalized residual energy ratio. thus, the medium 58638 volume 6, 2018 transmissions of three color channels t c (x) can be estimated as:"
"additional information methods mainly include the multiple images captured by polarization filters, stereo images, rough depth of the scene or specialized hardware devices [cit] ."
"contributions.-in this article, we introduce a condition for alignment (theorem 5) and a condition for nonalignment (theorem 3) of two arbitrary binary input symmetric channels. applied to several examples of interest, we show that these conditions are sometimes close in the sense that it can be conclusively determined if there is an alignment of the polarized sets or not. the proof of the alignment bounds is based on the uncertainty principle of quantum mechanics."
"once in the quantum setting, we may consider the description of w in terms of the stinespring dilation (see [9, chap. 8] ). let c and d be additional quantum systems isomorphic to b and define the states"
"however, the proposed method also shows some limitations when it is used to restore the underwater images with non-uniformly lighting. fig. 13 shows two failure cases of our method. in fig. 13, the visibility and contrast are improved by our method. meanwhile, the non-uniformly lighting is magnified, which results in poor visual quality of the restored results. for future work, we plan to add a nonuniformly lighting detection and removal algorithm in our method."
"it introduces over-compensated regions, such as the background regions in images ''rock'' and ''coral'' because of the over-estimation of the medium transmission. odm method can effectively remove the effects of haze in the underwater 58640 volume 6, 2018 [cit] . (e) results of iatp [cit] . (f) results of udcp [cit] . (g) results of odm [cit] . (h) our results."
proof: the level 0 statement follows directly from remark 1. remark 1 can be applied at every step of the polarization tree which proves the assertion.
"where e(x) c d is the direct light and e(x) c bs is the back scattering light. here, following previous methods [cit], it also neglects the effects of forward scattering light. the direct light e(x) c d is further defined as:"
"generally, underwater images are degraded because light is mainly absorbed and scattered by three water constituent particles: micro phytoplankton, colored dissolved organic matter and non-algal particles [cit] . when the light propagates in an underwater scenario, the light received by a camera is mainly composed of three kinds of light: direct light, forward scattering light and back scattering light. the received light by a camera suffers from color deviation due to the wavelength dependent light absorption. in general, the red light first disappears with the distance from objects, followed by the orange light, yellow light, purple light, yellow-green light, green light and blue light. this is the main reason why most underwater images are dominated by the bluish or greenish tone. therefore, to improve the visual quality of underwater image, a method which can remove the effects of back scattering light and wavelength dependent light absorption is needed."
"as shown in fig. 10(b), compared with color-checker image taken in the air, some colors decay due to the wavelength dependent light attenuation and back scattering light. as for fig. 10(c)-(f), dcp, cap, iatp, and udcp methods can not well restore the colors of underwater color-checker images. as for fig. 10(g) -(h), odm and our methods change the colors of underwater color-checker images because the image formation models in this two methods take the selective attenuation of underwater light into consideration. in contrast to the results of odm method, the colors of our results subjectively look more close to those of the color-checker image taken in the air."
"where a is the global background light that may be regarded as the light from infinity when assuming homogeneous lighting along the line of sight. by considering the global background light from infinity b having the same colors with the light source l, the final underwater image formation model can be defined as:"
"we found that the use of a single prior is relatively effective but insufficient. to improve the robustness of the proposed framework, we attempt to joint the above-mentioned iatp and udcp for medium transmission estimation. the main idea behind the use of the joint prior is that we experientially found the salient regions of the medium transmission estimated by iatp or udcp are relatively accurate. however, it is hard to prove this findings mathematically. we leave this work in the future. besides, both iatp and udcp assume that the red light disappear firstly. similar assumption can accelerate the fusion of these priors. in fact, we can directly use other effective and accurate underwater image restoration prior; however, there is no accurate and robust enough prior available for challenging scenes. to our best knowledge, the iatp and udcp are relatively most effective priors. to joint these two priors, a saliency-guided multi-scale fusion scheme driven by the intrinsic properties of input medium transmission is employed, which highlights the salient regions in the restored results. in other word, the salient regions in the restored results are relatively accurate. compared with other joint schemes such as single scale fusion and choosing the maximum between inputs, the multiscale fusion scheme can reduce the introduction of undesirable halos and noise [cit] . therefore, we use the saliency of input medium transmissions to determine which pixel is advantaged to appear in the final medium transmission. the final medium transmission t f (x) can be obtained by summing the fused contribution of all inputs, and can be expressed as:"
"to solve this problem, a variety of methods have been proposed in recent years [cit] . existing methods can be organized into one of four broad categories: single underwater image enhancement method, single underwater image restoration method, deep learning-based method, and additional information method."
"although iatp is relatively effective for many underwater scenes, it produces inaccurate estimation in some regions of the medium transmission for some underwater images captured under challenging scenes. fig. 1 shows several inaccurate results of the medium transmission estimated by iatp."
"in fig. 4, we just show the refined medium transmissions obtained from joint prior because it is hard to distinguish the medium transmissions of three color channels in color image form. as shown in fig. 4, the refined medium transmissions indicate that our method can estimate the medium transmission (depth) of scene in a relatively accurate manner. moreover, our restored results are characterized by natural colors and increased visibility."
"despite these recent efforts, the effectiveness and robustness of the existing methods need to be further addressed. in this paper, we restore the degraded underwater image based on a new underwater image formation model. unlike previous methods which assume that the atmospheric light is obtained from the brightest region, we assume that the atmospheric light is the same as the colors of the light source. then, the global background light is estimated via an effective color constancy method. to robustly predict the medium transmission of scene, we joint two underwater image priors by saliency-guided multi-scale fusion technique. further, the medium transmissions of three color channels (rgb) are achieved based on the optical properties of underwater imaging. finally, with the estimated global background light and the predicted medium transmissions, the restored underwater image can be obtained according to a new underwater image formation model. extensively qualitative and quantitative comparisons against several existing methods are performed. experiments demonstrate that the proposed method not only can restore the degraded underwater image to the relatively genuine colors and natural appearance, but also can increase contrast and visibility. besides, the proposed method is comparable to and even outperforms several existing methods in terms of the underwater image quality metrics."
"in most of underwater image restoration methods, the authors assume that three color channels of an underwater image have the identical medium transmission. however, different channels of an underwater image should have different medium transmissions due to the different attenuation coefficient β. in the new underwater image formation model, the medium transmission is defined as:"
"understanding the structure (and the relation) of the polarized sets d(w) and d(v) is important in several respects. first, this is directly linked to the universality of polar codes, if one fixed code can be used for reliable communication over each member of a given class of channels w. universal codes are important in different coding scenarios, for instance when the statistics of the actual channel are not known precisely. second, several different channels are simultaneously involved in network coding tasks such as wiretap or broadcast channels, and alignment is helpful in designing efficient polar coding schemes. third, knowledge of the structure and relation of polarized sets can be helpful in other aspects of polar coding, e.g. in the construction of polar codes (see [4, chapter 5] )."
"where m represents the total number of the patches in the image, q i, q c and q s are three comparison functions. the higher pcqi values denotes that the image has better contrast. table 1 shows the comparative values in terms of the uciqe and pcqi. the values in bold represent the best results."
"the quantitative results summarized in table 1 show that our method outperforms the compared methods in terms of the average values of uciqe and pcqi. the highest average values of uciqe and pcqi indicate that our method has better visual quality and contrast improvement when compared to other methods. the result of udcp for image ''coral'' ranks first best in terms of pcqi, but there are obvious artifacts in the background. the highest score for image ''coral'' may be from the magnified artifacts. as can be seen in images ''rock'' and ''coral'', there are artifacts and color casts in the background regions of the results of iatp, but these two images rank first best in terms of uciqe. obviously, our results without artifacts and color casts look more visually pleasing. more restored results by our method can be seen in figs. 11 and 12 ."
"in figs. 5-9, we present the qualitative comparisons. observing the qualitative comparison results, dcp and cap methods have few effects on the raw underwater images because the priors obtained from outdoor hazy images are not suitable for underwater scenes. iatp method can improve the visual quality of images ''rock'' and ''coral'', but it shows limitations when it is used to process the underwater images volume 6, 2018 [cit] . (e) result of udcp [cit] . (f) result of odm [cit] . (g) our result. [cit] . (e) result of udcp [cit] . (f) result of odm [cit] . (g) our result."
remark 4 (criterion for alignment cannot get worse for higher levels). suppose the sufficient conditions at level 1 in theorem 5 are satisfied. then using the inequality
"the aim of underwater image restoration and enhancement is to improve the visual quality of images captured under different underwater scenes. in recent years, this research area has attracted increased attention since improving the visibility, contrast, and colors of underwater images is of significance for many computer applications [cit] . nevertheless, enhancing and restoring underwater image from a single image is still challenging due to the complicated underwater environment."
"where d(x) is the iatp, i c (x) is the input image and is the size of an image patch. according to iatp, the medium transmission of an underwater image t(x) can be estimated by:"
"we first introduce a new underwater image formation model. then, the estimation of the colors of light source is presented. next, we propose a joint prior, which can be used for the medium transmission prediction. according to the medium transmission of scene and the optical properties of underwater imaging, we predict the medium transmissions of three color channels of an underwater image, respectively. last, we introduce how to restore the degraded underwater image with the obtained model parameters."
"where l is the colors of light source, m (x) is the surface reflectance which represents the restored underwater image without attenuation and color casts, and c is the camera sensitivity parameter. here, we consider the camera sensitivity parameter c as constant 1. thus, j (x) can be expressed as:"
the action of the channel can be expressed in terms of the dilation as mapping any quantum state ρ to we can define the quantum counterpart to w as
"we derived two analytical conditions that can be used to determine the alignment of polarized sets between different dmcs. the condition of theorem 3 that recognizes situations where there is no alignment (not even essentially) uses a simple counting argument. the condition of theorem 5, which identifies scenarios where there is an alignment of the polarized sets, is based on the uncertainty principle of quantum mechanics."
"if one knows the depth of scene d(x), the medium transmissions of three color channels can be estimated using eq. (14) and eq. (15) . before, we have obtained the refined medium transmission estimated by joint prior. so, the depth of scene d(x) can be calculated as:"
"however, the structure of d(w) and r(w) is poorly understood. in particular, the dependency on w is difficult to analyze in general. for v a binary-input output-symmetric"
"known l c and t c (x), m c (x) can be obtained from i c (x). therefore, this paper focuses on the estimation of the colors of light source l c and the medium transmission t c (x)."
"since aligned polarized sets imply that the corresponding polar codes are universal with sc decoding, our conditions can be used to determine if for a given set of dmcs polar codes are universal or not. we also discuss how the alignment bounds derived in this paper can be used to determine if quantum polar codes [cit] require entanglement assistance or not."
"in order to prove the sufficient conditions for alignment of the polarized sets given in theorem 5, we need the concept of a quantum counterpart of a dmc. the quantum counterpart is useful because its information tranmission capabilities are directly related to those of the original channel by uncertainty relations. such counterpart channels were defined generally in [12, sec. iia] and we give a slightly different presentation here."
"images. moreover, the results of odm method look natural. however, few details and colors of raw underwater images are veiled by odm method. observing our results, they retain vivid colors of the scenes and show the improved contrast and brightness based on the accurate estimation of the medium transmissions and the colors of light source. compared with the results of other methods, our results with few color casts are more visually pleasing. to further compare the color correction performance of different methods, we present the compared results on two underwater color-checker images in fig. 10 ."
"auditory features included the envelope of the stimulus audio, as well as moment-to-moment changes in the presence of human-generated speech. the envelope of the stimulus audio was used to track overall changes in audio intensity, regardless of the content of the audio, and implemented in matlab. following previously published methods, the envelope of audio intensity was calculated by computing power modulations across 25 frequency bands (center frequencies: 200 hz − 5 khz; width: 200 hz; sampling rate: 50 ms) 36 . within each band, the logarithm of the power time course was taken, and then all frequency bands were averaged, resulting in a single time course representing the audio envelope of the stimulus audio. the presence of human speech (excluding human-generated non-speech sounds) was manually coded by three raters, using binary judgments on 1-second bins of audio. discordant judgments were subsequently re-evaluated to reach a consensus."
"to experiment with high traffic condition, a number of services were competing with the one requiring qos. two computers were connected to the ftt-se bus to simulate a large number of random (uncorrelated) interacting systems. the timing characteristics of the messages between the computers were studied to simulate a configurable -and potentially large -number of interacting parties."
"for our use case, we allow a short latency period with n lat frames after each keyword segment. that is, if the system fires within the n lat -frame window right after a keyword segment, we still consider the firing as being aligned with the corresponding keyword. this latency window does not introduce significant delay in perception, and it could mitigate the possible issues of inaccurate keyword alignment boundaries in evaluation."
"in the present study, we used passive movie viewing, a naturalistic sensory stimulation paradigm, to evaluate the feasibility of measuring synchronized, movie-evoked cortical responses in healthy adult participants using hd-dot. this synchronization, as indexed by the voxelwise correlation coefficient between oxy-hemoglobin responses measured across repeated viewings, was most prominent in auditory and visual cortex, highlighting that passive movie viewing is an effective tool for engaging distributed, multi-modal cortical regions 20 . further, the spatial maps of correlation coefficients generated within participants and between participants both demonstrated elevated correlations during repeated stimulus viewings, underscoring that naturalistic stimuli reliably drive cortical activity despite the task's highly unconstrained conditions. the magnitude of the correlation coefficients was greatly diminished when participants viewed different, non-overlapping movie segments."
"the data used in our simulation are collected from real-world experiments carried out using terrier on the same server described earlier. in this case, we use the clueweb09 (cat. a) collection, whose web pages are organized into five different folders (b, a2, a3, a4, and a5) that we use as a form of document partitioning to perform distributed search. in practice, we consider clueweb09 (cat. a) to be composed by five partitions (the folders), each containing ∼50 millions web pages. we use terrier to index each of these partitions independently, in the same way described earlier."
"pesos overcomes these limitations by using the posting as the unit of work associated to a query since the query processing time correlates with the number of postings to evaluate [cit] . consequently, pesos uses two classes of qeps: given a query, estimates how many postings are evaluated; and given the number of postings and a core frequency, estimates the query processing time."
"hierarchical feature contrasts. within the set of features used for functional mapping, individual features differed in complexity. for instance, the audio envelope, a low-level feature, indexed non-specific changes in stimulus audio intensity. changes in audio intensity during movie viewing may be driven by factors such as environmental sounds, music, or human produced speech. processing human produced speech is a more complex auditory task with both auditory and linguistic components, and was indexed by a dedicated, higher-level language feature 54 . consequently, the set of auditory features used in this analysis was both hierarchical and potentially overlapping. to evaluate the relationship between these hierarchical auditory features, a paired t-test was www.nature.com/scientificreports www.nature.com/scientificreports/ computed between the correlation maps for the audio envelope and speech features (fig. 7a), resulting in a map of regions that preferentially respond to speech relative to other sounds indexed by the envelope feature. relative to the correlation map for the speech feature alone (fig. 6), the contrasted map in fig. 7 evaluates a region's selectivity for one feature over another and provided more detailed mapping of regions (e.g. left prefrontal cortex, or broca's area) involved in naturalistic speech processing."
"a testbed was implemented, to demonstrate qos over an arrowhead local cloud. figure 3 depicts the deployment view of the local cloud. the testbed makes use of a ftt-se network, which can provide hard real-time to communication flows. the testbed comprises the arrowhead environment, consisting in the core systems, among them orchestrator, qosmanager and qosmonitor, and the ftt-se environment, which comprises an entrypoint to the network, service consumers and producers, and the master of the ftt-se."
"internally, the qosmanager is articulated into three major components: qossetup, where the core logic is implemented, the qosdriver, and the qosverifier. the qossetup component manages all the core operations such as interaction with other systems. qosdriver acts as an adapter to interact with non-arrowhead-compliant devices and network actives, to configure them according to the request by the orchestrator. qosverifier verifies the feasibility of qos parameters on a specific set of network technologies."
"given these results, we can conclude that pesos helps reducing the cpu energy consumption of a distributed wse (rq1). in our simulations, the cpus in a day consume 254.02 mwh using perf, while with pesos the consumption reduces to 179.26 mwh (-29%). the same cpus consume 218.40 mwh with pegasus, meaning that pesos consumes 18% less energy than pegasus (rq3). however, such energy savings comes at the cost of negligible latency violations when workload is scarce (rq2), while pegasus never violates the target slo (rq3)."
"both synchronization and feature-based mapping strategies have been successfully incorporated in neuroimaging research using other modalities. inter-and intra-subject synchronization during naturalistic viewing was first demonstrated using fmri and has been shown in subsequent studies investigating the reproducibility of movie-evoked cortical responses 20, 21 . similarly, feature-based decomposition of naturalistic stimuli, using both manual and automated decoding approaches, has been incorporated in imaging work in both humans and non-human primates, highlighting that naturalistic tasks are suitable for mapping brain activity in a manner comparable to more constrained stimuli commonly utilized in functional mapping experiments 19, 39 . the present work is extension of these analytic tools to optical neuroimaging modalities, leveraging the relatively high resolution and broad coverage of the superficial cortex that hd-dot offers compared to sparse fnirs. stimuli such as the good, the bad, and the ugly, are narrative movies produced for entertainment; consequently, the \"tasks\" embedded in processing a feature film are complex, rich, and concurrent. prior work using fnirs has also used video stimuli, although generally with the goal of understanding a targeted and constrained information processing task. for example, fnirs experiments investigating the development of specialized cortical responses to social stimuli have successfully leveraged the richness of video stimuli with human actors 56, 57 . further, these www.nature.com/scientificreports www.nature.com/scientificreports/ responses have been shown to be sensitive to altered developmental trajectories 58 . depending on the study, these targeted videos can be optimized for the specific task of interest. on the other hand, because social interactions are inherently rich, multi-modal experiments, video stimuli are an effective tool for recapitulating this richness in a repeatable and controlled manner. by replacing a video stimulus tailored for assessing a specific domain with a feature film, as done in the present work, multiple sensory and cognitive processing domains can be assessed concurrently using a single, integrated movie stimulus. outside of the laboratory, information is rarely encountered in a single sensory domain under rigidly controlled stimulus presentation parameters, underscoring the ecological relevance associated with free viewing tasks as implemented in this work."
"by means of the qosmonitor system, the arrowhead framework becomes capable of performing real-time monitoring of the performance of services, system and devices hosting arrowhead compliant systems. the qosmonitor main functionality is to monitor violations of slas between service producers and consumers, and to inform other systems regarding qos faults. the qosmonitor makes use of modules running over devices, or indirectly by accessing logs of network actives or other devices, to monitor the behaviour of devices and network actives over time. violation of qos requirements and its status is disseminated using the event handler system. additionally, some dynamic and adaptable qos algorithms require the knowledge of the status of the local cloud during run-time in order to adapt to changing conditions. as an example one of the arrowhead pilots is capable of reducing its sampling rate and consequently the consumed bandwidth in order to support more devices in an ieee 802.15.4 network. this can be achieved by monitoring the network status using the qosmonitor system, and informing the interested parties, using the event handler."
"the qosmanager must have access to a global view of the local cloud including network topology and capabilities of each device. depending on the scenario, these data will be provided by the orchestrator when requesting qos verification, or retrieved by the qosmanager by contacting the service registry, system registry, device registry. the qosmanager has also access to its own qos store, which is a soa database that holds information regarding the resource reservations active in the local cloud. the data in the qos store are kept aligned with the qos configurations deployed onto network actives and devices. should the system of systems host more than one qosmanager system, all of them will refer to the same qos store to gain a consistent vision."
"pesos adapts yds to multi-core shard servers by using such qeps. it initially replaces query processing volumes with the number of predicted postings to be evaluated, relaying this information to yds. then, it translates the cores' speeds returned by yds into valid core frequencies, by predicting query processing times."
"we evaluated the performance of pesos, an energy-saving strategy designed for wses' single servers, when deployed on a distributed infrastructure. we compared pesos to pegasus, an industrylevel baseline. both strategies adapt the cpus core frequencies to the incoming query traffic exploiting dvfs technologies. we simulated a distributed wse deployed on a thousand servers, using the clueweb09 document collection and the msn query log. the open-sourced simulation has been finely tuned to reproduce realworld measurements both in terms of tail latencies and energy consumption (∼1% deviations). our results showed that pesos reduces the cpu energy consumption of a distributed wse by up to 18% w.r.t. pegasus, at the cost of negligible latency violations (less than 2% on average) during low workloads periods."
"hd-dot instrumentation. the large field-of-view hd-dot instrument used in this experiment ( fig. 1) has been described in detail in prior work using this instrument 35 . in brief, this custom-built continuous wave instrument consists of 96 led sources illuminating the head at two wavelengths (750 nm and 850 nm), and 92 avalanche photo diode detectors (hamamatsu c5460-01), coupled to the head using 4.2 m long fiber-optic bundles (ceramoptec, 2.5-mm diameter bundles of 50 µm fibers). the weight of the 188 fibers was managed using an extruded aluminum frame and series of collinear rings surrounding the participant, ensuring that participants do not bear any of the fiber weight."
"the producer and consumer have also a wireless interface to perform tcp/ip communications with the entrypoint and, through it, with the arrowhead framework. the service consumer sends an orchestration request with a sla to the orchestrator through the entrypoint. the answer contains a user field with the parameters that the service consumer must use to request a data stream on the ftt-se. after that, it will contact the service producer to start consuming the service."
"pegasus (power and energy gains automatically saved from underutilized systems) is a technique that aims at improving the energy efficiency of large scale systems such as wses [cit] . experimentally shown, pegasus reduces by up to 20% the power consumption of a google search production cluster, while keeping its latencies within an acceptable service level objective (slo). differently, the pesos (predictive energy saving online scheduling) algorithm is designed to reduce the cpu energy consumption of single servers [cit] . experimentally evaluated, pesos can reduce the cpu energy consumption of a query processing server by almost 50%, while response times are kept below a desired time threshold."
"sometimes a projection layer is added on top of the lstm output, to reduce model complexity [cit] . a typical lstm component with projection layer is shown in figure 2 . for the sake of clarity, a single lstm block is shown here."
"a projection layer is added to the lstm output. that is, w rm linearly maps m t to a lower dimensional representation r t, which is the recurrent signal. the network output y t is computed based on the projection layer output r t as well."
"the paper described how qos can be applied to arrowheadcompliant local clouds, both in terms of architecture, of algorithms to verify and configure qos, and of implementaton on a testbed based on ftt-se technology. experimental results were provided regarding the set up of ftt-se streams, and on the different performance of qos-enabled and best effort service fruition. future works will extend the work done to other technologies, starting with ieee 802.15.4, both in terms of qosdriver and of qosalgorithm. later on, our study will regard the implementation of arrowhead-based qos communication on heterogeneous networks. finally, more stringent feasibility tests and more general problem formalization for the qos problem will be studied."
"distributed search. given a document collection, a posting list is associated to each term appearing in the collection, containing the list of the documents in which the term occurs. the set of the posting lists for all the terms is called the inverted index. a wse must manage huge amounts of documents and process billions of queries per day with low latencies. hence, wses partition into smaller shards the inverted index used to match user queries. in fact, query processing times depend on the lengths of the posting lists, namely on the number of postings traversed, decompressed, and scored [cit] . the posting lists of an index shard are shorter than the corresponding ones in the original inverted index, resulting in reduced processing times."
"we present our work of training a small-footprint lstm to spot the keyword 'alexa' in far-field conditions. two loss functions are employed for lstm training: one is crossentropy loss, and the other is max-pooling loss proposed in this paper. a smoothed posterior thresholding approach is used for evaluation. keyword spotting performance is measured using miss rate and false accept rate. we show that lstm performs better than dnn in general. the best lstm system, which is trained using max-pooling loss with crossentropy loss pre-training, reduces the auc number by 67.6% in the low miss rate range. for future work, we plan to add weighting to max-pooling loss based lstm training, i.e., scale the back-propagated loss for the selected keyword frames. it is of interest to see if lstm performance can be further improved by varying model structures, e.g., adding additional feed-forward layers on top of the lstm component. we also plan to benchmark max-pooling loss performance against other segmental level loss functions, e.g., geometric mean of framewise keyword posteriors within each keyword segment, ctc etc, for our keyword spotting experiments."
"the service producer, in these experiments, is limited to one service only. when contacted by the service consumer, the producer answers with video data through the stream specified, and configured, by the service consumer. the ftt-se network is based on a 100 mbit/s 802.3 switch. the goal of the service consumer is to receive a bandwidth of 250 kb/s and a maximum delay of 40 ms per packet, which are the qos parameters requested through a sla when qos is enabled."
"using optical neuroimaging tools to study the brain during naturalistic viewing conditions has broad applicability for experimental questions demanding rich, engaging stimuli alongside wearable and ergonomic imaging tools 17, 66 . developmental cognitive neuroscience has benefitted from the broad applicability of optical neuroimaging tools for imaging the developing brain 16, [cit] . paired with optical neuroimaging, the movie-based imaging paradigm described in this paper provides engaging and ecologically relevant study designs for understanding information processing across the lifespan, highlighting the richness of this paradigm for interrogating \"real-life\" brain function."
"the idea of max-pooling loss is shown in figure 3, where filled frames are aligned with the keywords, and empty frames are for background. given an input sequence of frames, within each keyword segment, only the frame which has the maximum posterior for corresponding keyword target is kept, while all other frames within the same keyword segment are discarded. all background frames are kept."
"keyword spotting has been an active research area for decades. different approaches have been proposed to detect the words of interest in speech utterances. as one solution, a general large vocabulary continuous speech recognition (lvcsr) system is applied to decode the audio signal, and keyword searching is conducted in the resulting lattices or confusion networks [cit] . these methods require relatively high computational resources for the lvcsr decoding, and also introduce latency."
"for visual features generated by image statistics (luminance and flow), voxels in visual cortex have the highest correlation coefficients between the ∆hbo 2 time-series in these regions and the feature time-series. conversely, the set of visual feature time-courses for higher-level visual features revealed patterns of elevated correlation coefficients across broader constellations of regions. for instance, the map of face processing during naturalistic viewing, generated by computing the voxelwise correlation between the ∆hbo 2 time-series and the visually presented faces time series, not only involved extrastriate visual regions, but also auditory and speech processing regions, underscoring that features were not present in isolation during the naturalistic viewing task 53 . similarly, the correlation coefficient between voxelwise ∆hbo 2 and the time-course of visually presented bodies was elevated in voxels in the visual cortex and voxels in the inferior regions surrounding the central sulcus."
"applications have different qos requirements in terms of latency, security, robustness or bandwidth, just to name a few qualities. nowadays, most automation applications are supported on closed systems with limited capabilities to evolve. the trend on applying industrial iot (iiot) technologies and specifically soas to these systems require changes on the philosophy applied to their development [cit] ."
"for lstm training with different loss functions, we use a single layer of unidirectional lstm with 64 memory blocks and a projection layer of dimension 32. this serves the purpose of low cpu and memory, as well as low latency. for input context, we consider 10 frames on the left and 10 frames on the right. note that we still use 10 frames as left context for lstm input, though the lstm learns past frames' information by definition. by doing this our dnn and lstm training setup are aligned better for comparison, and past information is further imposed for lstm training. our lstm has ∼ 118k parameters. for random initialization, the lstm parameters are initialized with a uniform distribution u [−0.2, 0.2] for weights, and constant 0.1 for bias. the initial learning rates are chosen to be 0.00001, 0.00005 and 0.00005 for the cases of cross-entropy loss, max-pooling loss with randomly initialized model, and max-pooling loss initialized with a cross-entropy pre-trained model."
"to be able to support the plethora of network technologies that are currently in the market, the qosmanager makes use of a qosalgorithm module, which performs calculations to verify that qos requirements are feasible, and determine the system parameters that are capable of fulfilling the qos requirements, taking into account the current status of the local cloud. these algorithms can be based on different mathematical models of the system of systems, and the qosmanager can comprise multiple algorithms for the same technologies, for example to perform comparison of their efficiency."
"the goal of the scheduliing algorithm is to assign booleans to the scheduling variables s ijl, which assume value 1 if and only if step q ij involves a data transfer in the superframe l of the related network."
"while pesos results are significant, their validity is limited, as pesos has only been tested on a single server configuration. instead, the de facto standard for wses is to rely on a distributed architecture, as pegasus correctly assumes. in this work we fill the gap between pesos and pegasus by evaluating the performance of pesos on a distributed wse, and comparing pesos and pegasus in terms of energy consumption and their success in meeting response time requirements. to this end, we simulate the behavior of pesos when deployed on thousands of servers, conducting experiments on the clueweb09 (cat. a) [cit] query log. results show that pesos can reduce the cpu energy consumption of a distributed wse by up to 18% with respect to pegasus, while providing query response times which are in line with user expectations."
"the 1-of-k coding is usually used for target vector z t . that is, if the tth frame vector x t is aligned with class k, the kdimensional vector z t has value 1 for the kth element, with all other elements being 0. let k † t denote the aligned class for the tth frame. the cross-entropy loss for the tth frame can be formulated as l"
"the qossetup and the monitor are two core services devoted to supporting qos in arrowhead local clouds. the first is provided by the qosmanager system, and it is consumed by systems to verify that qos requirements are feasible in a local cloud, and to actually request the configuration of network actives and devices to grant given qos, this latter including performing reservation on resources such as network bandwidth and device processing time. the monitor service, produced by the qosmonitor system, is used to instruct the system to collect data from network actives and devices regarding the performance of a service, and compare it with required qos. should the local cloud not meet the configured qos, the qosmonitor sends a message to interested parties regarding the qos fault through the event handler service [cit] ."
"these preliminary experiments allow us to map each cpu configuration to its power consumption and to use this information in the simulator. our simulated cpu consumes 0.8 watts when idle, and up to 34.2 watts when all its cores are busy processing queries at 3.5 ghz."
"in recent years, there are keyword spotting systems built on dnn or convolutional neural network (cnn) directly, with no hmm involved in the system [cit] . during decoding time, framewise keyword posteriors are smoothed. the system is triggered when smoothed keyword posteriors exceed a pre-defined threshold. the trade off between balancing false rejects and false accepts can be performed by tuning the threshold. context information is taken care of by stacking frames as input. some keyword spotting systems are built on recurrent neural network (rnn) directly. particularly, bidirectional lstm is used to search for keywords in audio streams when latency is not a hard constraint [cit] ."
"let us consider a simplified scenario with just a ftt-se network and one 802. but are not on the ftt-se network. let us consider that each node in the x set wants to execute a client/server protocol with the server nodes, which compose set z."
". in a typical participant, this system configuration yielded over 1,200 source-detector measurements (per wavelength), which were then converted into voxelated movies of brain hemodynamics as specified below."
"during decoding time, the system is triggered when the keyword posterior smoothed by averaging the output of a sliding window is above a threshold. considering the practical use case, our keyword spotting system is designed to lock out for some time after each detection, to avoid unnecessary false accepts and reduce decoding computational cost."
we use the evaluation approach described in section 2.3 on our test dataset. the performance of the dnn and lstm models are shown in figure 5 .
"different from feed-forward dnn networks, rnns contain cyclic connections which can be used to model sequential data . this makes rnns a natural fit to model temporal information within continuous speech frames. however, traditional rnn structures suffer from the vanishing gradient problem, which prevents them from effectively modeling long context in the data. to overcome this, lstms contain memory blocks [cit] . each block contains one or more memory cells, as well as input, output and forget gates. these three gates control the information flow within the associated memory block."
"in figure 1 we observe that both pegasus and pesos are closer than perf to the 500 ms slo. perf exhibits a tail latency of ∼350 ms at the cost of a large energy consumption. while pesos tail latencies are closer than pegasus' ones to the target slo, we also observe that pesos leads to small latency violations in the early and late hours. this happens because the wse receives fewer queries during nighttime than during daytime; therefore pesos tends to select small core frequencies. this results in previously unobserved latency violations [cit], but which emerge at scale due to the variability of response times across different shard servers [cit] . nevertheless, such limited violations (on average 509 ms) do not negatively affect the user experience [cit] ."
"cortical responses measured within single individuals reveal movie-driven responses with high correlation coefficients in regions related to stimulus processing. to assess whether this effect extended beyond individual viewers to disparate pairs of viewers, the synchronization analysis was repeated across all possible pairs of the ten viewers. the voxelwise correlation coefficient between the ∆hbo 2 time-series in each participant was calculated for each pair of viewers, which revealed a synchronization topography comparable to the intra-subject analysis (fig. 3) . in other words, not only does the naturalistic stimulus reliably drive cortical responses within an individual, it also reliably drives cortical responses across individuals."
"the qosmanager needs as much knowledge as it can acquire, since it must compute qos feasibility based on structure and condition of the whole local cloud. the orchestrator already acquires that knowledge from the registries when orchestrating services. thus, the orchestrator can feed the information to the qosmanager when querying it."
"as shown in figure 1, log mel filter-bank energies (lfbes) are used as input acoustic features for our keyword spotting system. we extract 20 dimensional lfbes over 25ms frames with a 10ms frame shift. the lstm model is used to process input lfbes. our system has two targets in the output layer: non-keyword and keyword. the output of the keyword spotting system is passed to an evaluation module for decision making."
"to stay on the safe side, if two messages have the same priority, for example if they pertain to different steps q ij1 and q ij2 of the same application a i, the time for sending both messages is considered as the time when both q ij1 and q ij2 are completed."
"the qosmonitor provides two services, the monitor and the log service, used by the qosmanager to configure what must be monitored in the local cloud, and by systems to report performance data, respectively. the qosmonitor periodically compares communication performance between one service producer and one service consumer, against qos contracts accepted by the qosmanager system, and then informs interested parties of any qos violation using the event handler [cit] ."
"arrowhead services are considered either application services (when implementing a use case), or core services (that provide support actions such as service discovery, security, service orchestration, and protocol translation). to ease the development of new applications, the core services are included into the common arrowhead framework [cit] . the arrowhead framework is intended to be either deployed at the industrial site, or accessed securely, for example through a vpn."
"to answer rq2 and rq3, we initially discuss the latencies of our simulated wse in its various configurations. then, we analyze the cpu energy consumption of pegasus and pesos w.r.t. perf to answer rq1 and rq3."
"let us consider that a set of a periodic applications a 1, ..., a a are executed over a heterogeneous network. each application is characterized by a period t i, a deadline d i and a number of bits that must be transmitted c i, i.e.:"
"the remaining part of this paper is organized as follows: section 2 describes our lstm based keyword spotting system, which includes the lstm model, training loss functions and performance evaluation details. experimental setup and results are included in section 3. section 4 is for conclusion and future work."
"fibers were affixed to the scalp using a custom-built imaging cap, which positions optodes such that first-through fourth-nearest neighbor separations are 1.3, 3.0, 3.9, and 4.7 cm, respectively. using previously published temporal, frequency, and spatial encoding patterns, the hd-dot system achieves an overall framerate of 10 hz"
"to evaluate the performance of pesos and pegasus at a realistic scale, we simulate a distributed wse 1 . in doing so, we want to investigate the following research questions (rqs):"
"we index the clueweb09 (cat. b) collection using the terrier ir platform [cit] . we index the collection removing stopwords and applying the porter stemmer. the resulting inverted index stores document identifiers and term frequencies, compressed with eliasfano [cit] . the inverted index is kept in main memory by a dedicated server equipped with 32 gb ram and the i7-4770k processor. this setting is then used to perform the following preliminary experiments. for each possible cpu configuration, we launch a number of instances of the search platform equal to the number of active cores in the configuration. each search instance is pinned to one of the cores, which operates at the frequency indicated by the cpu configuration. [cit] log against the aforementioned inverted index, to retrieve the top 1,000 documents using bm25 and maxscore. we use mammut [cit] to measure the energy being consumed by the cpu to derive its power consumption."
"simulating the latency. we simulate the wse tail rather than mean latency as tail latency is considered a better performance indicator [cit] . in particular, for comparison purposes we chose the 95th percentile latency to mirror prior efforts [cit] ."
"the baseline feed-forward dnn has four hidden layers, with 128 nodes per hidden layer. sigmoid function is used as activation. a stack of 20 frames on the left and 10 frames on the right are used to form an input feature vector. note that the right context cannot be too large, since it introduces latency. there are in total ∼ 129k parameters with the dnn model. layerwise pre-training is used for the dnn. initial learning rate for dnn training is 0.0005, and batch size is 256."
"if the observed synchronization between voxelwise responses is related to processing repetitions of the same stimulus, then the magnitude of the correlation should be diminished when the analysis is repeated with ∆hbo 2 timeseries obtained during disparate viewing conditions (i.e. different movie clips). indeed, when participants view non-overlapping movie segments, the correlation coefficients are diminished both within a single region (fig. 2c) and across the entire hd-dot field-of-view (fig. 2e) . the dramatic reduction in the correlation coefficients is also evident in the distributions of correlation values observed during both matched and mis-matched viewing conditions (fig. 2f) . voxelwise responses in individual viewers show the greatest reliability, or synchronization, during repetitions of the same stimulus (fig. 2d) ."
"the orchestration service is used to assemble complex services, which may be comprised of several individual services. to this aim, services, systems and devices in an arrowhead local cloud have to be registered, and through the registries (serviceregistry, systemregistry and deviceregistry systems) the orchestrator can access a global view of the local cloud. orchestrated services can be \"pulled\" by service consumers, or can be \"pushed\" by the orchestrator itself when it detects changes in the local cloud that create the need for a reconfiguration."
"small-footprint keyword spotting systems have been increasingly attracting attention. voice assistant systems such as alexa on amazon echo deploy a keyword spotting system on device, and only stream audio to the cloud for lvcsr *work conducted while the author was at amazon.com when the keyword is detected on device. for such applications, accurate on-device keyword spotting running with low cpu and memory is critical [cit] . it needs to run with high recall to make devices easy to use, while having low false accepts to mitigate privacy concerns. latency has to be low as well. a traditional approach employs hidden markov model (hmm) to model both keyword and background [cit] . the background includes non-keyword speech, or nonspeech noise etc. this background model is also named filler model in some literatures. it could involve loops over simple speech/non-speech phones, or for more complicated cases, normal phone set or confusing word set. viterbi decoding is used to search the best path in the decoding graph. the keyword spotting decision can be made based on the likelihood comparison of keyword and background models. gaussian mixture model (gmm) was commonly used in the past to model the observed acoustic features. with dnn becoming mainstream for acoustic modeling, this approach can be extended to include discriminative information by incorporating a hybrid dnn-hmm decoding framework [cit] ."
"(1) does pesos help reducing the cpu energy consumption of a distributed wse? (2) does pesos provide acceptable latencies in a distributed wse? (3) how does pesos compare to pegasus in term of both latencies and energy consumption? to answer rq1 and rq3, we simulate the energy consumption (measured in megawatt hours, mwh) of a wse's cpus, while we simulate its tail latency (computed 95th percentile of response times distribution) to answer rq2 and rq3. to better understand the benefits of pegasus and pesos for wses, we will also compare their performance with a wses which always operates its cpus' cores at maximum frequency for the sake of low latencies. we refer to this configuration as perf."
"solve a modified version of the minimum-energy scheduling problem (mesp) [cit] . in mesp, a set of jobs must be scheduled on a cpu to meet their deadlines and minimize energy consumption of the cpu. jobs can be preempted and their processing volumes are known, while the cpu speed can vary continuously and is unbounded."
"the qosmonitor's architecture is divided into three major components: the monitor, the protocol, and the databasemanager. the monitor component implements the core logic, for example to manage the periodic access to logs on network actives. protocol provides a library of interfaces for specific communication protocols. the databasemanager is responsible for all database-related operations, and is able to support interaction with both sql and nosql databases and the arrowhead historian service."
"movie viewing tasks also afford practical advantages for special populations of interest. for instance, toddlers and school age children may find measurements of task-evoked brain activity relying on highly constrained and isolated stimuli to be boring, repetitive, or predictable 17 . indeed, work using fmri indicates that naturalistic viewing tasks in toddlers and school age children reduces head motion, a substantial source of artifact 24, 25 . further, the extent to which a child shows synchronized brain responses during naturalistic viewing correlates with behavioral assessments of mathematical and linguistic ability 59 . future work using naturalistic viewing tasks in conjunction with optical neuroimaging can leverage the practical advantages and scientific value of these paradigms alongside comfortable and wearable instrumentation, such as hd-dot, that is particularly well suited for pediatric imaging 60 . one limitation of the naturalistic viewing task, as implemented in this work, is the lack of measured behavioral responses. behavioral responses can track participant comprehension and attentiveness throughout the task, two variables that have been previously shown to modulate cortical responses measured during naturalistic viewing 27, 28, 61 . possible behavioral responses include comprehension assessments following the experiment 27 and recording eye position during the experiment 62 . indeed, while eye position during viewing of a professionally produced movie is generally reproducible across subjects, gaze position has been reported to vary in special populations, including participants with autism spectrum disorder 63 . in the present work, the importance of eye position during naturalistic viewing was assessed during a separate experiment during which a subset of participants viewed an additional repetition of the stimulus while maintaining central fixation during the entire viewing session. the correlation magnitude between voxelwise responses in visual cortex within viewing conditions indicated synchronized brain responses across participants; however, cortical responses from mismatched viewing conditions did not show the synchronization effect (fig. 4) . in this experiment, participants confirmed their ability to comply with the fixation instructions by self-report. in future work, eye tracking can confirm compliance with experimenter-imposed gaze conditions and provide better characterization of eye position during free viewing."
"to verify qos requirements, a conservative approach is to build the critical instant [cit] for each network n k, and to calculate the time tij for completing each step q ij, i.e. sending the relevant message on the related network. for each network n k, the usual equations to compute the worst case communication time are used."
"we are interested in a small-footprint keyword spotting system that runs on low cpu and memory utilization, with low latency. this low latency constraint makes bidirectional lstm not a proper fit in principle. instead, we focus on training a unidirection lstm model using two different loss functions: cross-entropy loss and max-pooling based loss [cit] ."
"we consider that some devices and most network actives are not arrowhead-compliant yet. the qosmanager is equipped with a module called qosdriver, or qosdrv, that provides a uniform interface for the configuration of qos parameters on network actives and devices. the qosdriver acts as an adapter between the non-soa protocols of the network actives and devices, such as snmp, nagios [cit] and openflow [cit] ."
"qos is required in many industrial applications, for example on distributed control loops. this work identifies 4 classes, related to the qos dimensions on which most industrial applications focus. delay implies the execution of communication and computation within a deadline, both on the time elapsed for a message delivery, and end-to-end delay of a service invocation. this class of qos objectives spans over both hard real-time and soft real-time constraints. bandwidth refers to guarantees for sufficient communication and computational resources, concretized as constraints on the minimum bandwidth for data produced / transmitted in a time unit, and on the number of service requests supported in a time unit. resources limits protects the sos against services, since it prevents resource choking by limiting the resources that a system or service consumes. communication semantics is a class used to request assurance of receiving the message at least once, of not receiving duplicated messages, and of receiving messages in the same order they were produced."
"the qossetup and monitor services are core services, and are produced by qosmanager and qosmonitor systems. they are described in their specific subsections since they are the main implementation results of this work."
"the entrypoint is connected to producers and consumers by means of a wireless interface and traditional tcp/ip communication. during the mediation between the environments, the entrypoint changes the messages protocol from the tcp stream to arrowhead-compliant service-oriented https, or the other way around."
"participants. participants in this experiment were healthy young adults, recruited from the washington university community. all participants gave written informed consent to participate in the experiment, which was approved by and carried out in accordance to the human research protection office at washington university school of medicine. participants, all right-handed native english speakers, self-reported no history of neurological or psychiatric illness. in total, 12 participants were enrolled in the naturalistic viewing experiment (aged 23.5-29.4 years; 6 female). of the 12 initial participants, 10 are included in the analyses reported below, as two participants were excluded due to falling asleep during one of the two imaging sessions."
"an additional limitation of the present experiments is the utilization of a single movie clip. importantly, not all movies are equally suitable for mapping particular features of interest. for instance, an animated movie with animal characters (e.g. finding nemo) would likely be poorly suited for mapping cortical responses to visually presented hands. further, a boring or difficult to understand movie (e.g. waiting for godot) may result in diminished synchronization resulting from poor attentiveness or comprehension 61 . future work employing naturalistic viewing paradigms can assess the efficacy of differing stimuli in performing functional brain mapping within a www.nature.com/scientificreports www.nature.com/scientificreports/ given domain of interest, as well as expand the set of features used for a given movie clip. in addition to the low and high-level sensory features used in this work, movie stimuli contain rich social, emotional, and narrative content that engage higher-order brain functions 54, 55 . sex-related differences have been reported in social and emotional processing in imaging and behavioral studies using non-naturalistic designs 64, 65 . while the sample in this study was neither sufficiently powered nor balanced to assess potential sex-related effects, naturalistic designs such as passive movie viewing offer a convergent experimental strategy to further explore these sex-related differences in social and emotional processing."
"the core services of arrowhead takes care of the maintenance of the local cloud itself and of non-functional requirements of use cases, and are included into, and shipped in the form of, the arrowhead framework [cit] . even in the most minimal local cloud, the core services take care of 978-1-5090-5788-7/17/$31.00 [cit] ieee registration and discovery of services, systems and devices (servicediscovery service, or sd), security (authentication service, or aa), and orchestration of complex services (orchestration service, or o). figure 1 shows an example featuring just the connection between application services (depicted in yellow). the application systems are also consumers of the core services."
"this section provides a formal definition of the problem of verifying qos feasiblity in heterogeneous networks, and proceeds on proposing a preliminary algorithm for its soluton. further on, a concrete example based on ftt-se and 802.15.4 networks is described."
"the setup time is the time elapsed between the request for a qos-enabled orchestrated service by the service consumer, and the beginning of the service fruition. this time comprises the communication, through the entrypoint, with the orchestrator, the time needed by orchestrator and qosmanager to verify and set up the local cloud, and the time needed by the service consumer to request a ftt-se stream from the master. figure 4 shows the cumulative distribution function (cdf) of the elapsed time, measured over 1000 qos-enabled service orchestrations. when qos was enabled, on the other hand, even with much more aggressive secondary service consumers, both the delay and the bandwidth qos parameters were always respected."
"feature-based analysis. the feature extraction procedure, applied across visual and auditory modalities, resulted in a set of seven features (fig. 5) . in general, features were not strongly correlated with each other, with the exception of the two features derived from image statistics, luminance and optical flow (fig. 5b) . correlation maps generated from the time-series for each feature category were in agreement with known functional neuroanatomy (fig. 6) 35,53, and were evaluated using a t-statistic to identify voxels with correlation coefficients that deviate from a null distribution in which there is no observed correlation between signals (see methods). for instance, in the auditory domain, voxels in the bilateral superior temporal gyrus (stg) had the highest correlation coefficients to the audio envelope feature time-series, while the correlation between voxelwise ∆hbo 2 and the speech feature revealed a left-lateralized response in the stg and left prefrontal cortex."
"the following broadcast domains are involved: one 802.15.4 wireless domain, the ethernet lines of the nodes in x, and the ethernet lines of the nodes in y . each application involves a packet that gets from a node in set x to a node in set y, then to a server z, then back to the node in y, and finally back to the node in x. since each relayer can receive messages from multiple nodes, the bottlenecks on the ftt-se lines are on the side of the relayers in y, and the lines to the broadcast domains corresponding to the nodes in x can be disregarded."
"to satisfy such performance requirements, wses adopt a distributed architecture, i.e., they are deployed on clusters of thousands of multi-core servers. this architecture ensures that most users will quickly receive their results, keeping them satisfied. however, such many servers consume a significant amount of energy, mostly accountable to the power consumption of their cpus [cit] . the resulting electricity expenditure can hinder the profitability of permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. wses, as they can consume tens of megawatts of electric power. therefore, energy efficiency is an important aspect for the economic successfulness of wses. we too focus on energy efficiency and evaluate two energy management approaches using real world data. our experiments demonstrate considerable gains using a recently proposed research energy management scheduler."
"we consider two cases for max-pooling loss based lstm training: one starts with a randomly initialized model, and the other uses a cross-entropy loss pre-trained model. with a randomly initialized model, max-pooling loss based lstm fig. 3 . idea of max-pooling loss training may not learn well in the first few epochs with rather random keyword firing. the idea is to take the advantages of both cross-entropy and max-pooling loss training. with a cross-entropy trained lstm as the initial model to start max-pooling training, it already learns some basic knowledge about target keywords. this could provide a better initialization point, and faster convergence to a better local optimum."
"the set of elements that can be configured by the qosmanager comprises devices' traffic smoothing filters on the output of service producers or consumers, parameters like traffic priority and delivery guarantees of message oriented middleware with qos capabilities, like dds [cit], amqp / mqtt [cit] or xmpp [cit] . network actives such as switches, routers or gateways can also be configured in order to control the bandwidth of specific message streams. the qosmanager is also equipped with a qosdriver module that mediates any non-arrowhead interaction used to configure the network actives and devices."
"the architecture of the arrowhead framework comprises three services that must be part of any arrowhead local cloud. as discussed in subsection ii-a, they take care of service, system and device registration (servicediscovery service), security (authentication service) and service orchestration (orchestration service). this latter service is provided by the orchestrator system, which takes care of building orchestrated services and providing them to service consumers as per \"pull\" or \"push\" interaction (section iii). the orchestrator uses the qosmanager as a plugin to verify and configure required qos for the orchestrated services."
"the first item states that we calculate the cross-entropy loss for input frames not aligned to any keyword. the second item shows how we do max-pooling for keyword aligned frames. in more details, for the frames of the pth segment (index range l p ), they are aligned to keyword k † p . we only back propagate for a single frame (index l † p ) whose posterior for target k † p is the largest among all frames within current segment l p, and discard all other frames within current segment."
"hierarchical features were not limited to features within a single modality. during naturalistic viewing of the stimulus, visually presented human faces co-occurred with auditorily presented human speech. consequently, the correlation map for the visually presented face feature may serve as a surrogate feature for mapping cortical responses to social information, specifically human speech 55 . however, other ambient sounds may also co-occur with visually presented faces, which are indexed by the auditory envelope. to test this hypothesis, the paired t-test described above was repeated for the face and auditory envelope features (fig. 7b) ."
"the hartes [cit] technology is quite similar with the ftt-se one, and its main advance is that the master node is incorporated into the switch."
"the qos classes and parameters are applied in environments that vary in terms of device capabilities (techniques that were developed for internet nodes are applied to resource constrained iiot devices), kind of networks (both traditional contention-based networks, real-time capable networks, and heterogeneous networks have to be supported, the latter referring to automation systems that span across multiple networks with different technologies), scalability (local cloud can be limited to a few computers, or span over a large complex of factories) and security (resource constrained devices and traditional computational nodes have to adapt security measures to their computational capabilities)."
"general energy studies are not new, examples including [cit] recent studies show however that users negatively react to large response time, but they can hardly notice response times that are faster than their expectations [cit] . therefore, wses can trade-off performance (i.e., longer response times) for lower energy consumptions when this does not affect the user experience. such trade-offs are feasible by varying the frequency and voltage of cpu cores in wses' servers via dynamic frequency and voltage scaling (dvfs) technologies [cit] . thanks to dvfs, wses can save energy by answering queries no faster than necessary. state-of-the-art implementations of this principle are pegasus and pesos."
"we propose to train the lstm for keyword spotting using a max-pooling based loss function. given that the lstm has the ability to model long context information, we hypothesize that there is no need to teach the lstm to fire every frame within the keyword segment. instead, we want to teach the lstm to fire at its highest confidence time. the lstm should fire near the end of keyword segment in general, where it has seen enough context to make a decision. a simple way is to back-propagate loss only from the last frame or last several frames for updating the weights. but our initial experiments indicate that the lstm does not learn much from this scheme. hence we employ a max-pooling based loss function to let the lstm pick the most informative keyword frames to teach itself. this also helps mitigate issues potentially caused by inaccurate frame alignment around keyword segment boundaries. max-pooling loss can be viewed as a transition from frame-level loss to segment-level loss for keyword spotting model training. alternative segment-level loss functions include different statistics of frame-level keyword posteriors within a keyword segment, e.g., the geometric mean etc. there have been literatures on training lstms using connectionist temporal classification (ctc) [cit] for keyword spotting tasks as well. in addition, architectures that combine lstms and cnns have been applied to different tasks [cit] . typically lstm is added on top of cnn layers, where cnn layers with pooling are used to extracted features as lstm input, and lstm output is used for prediction."
"the interaction between orchestrator and qosmanager happens by means of the qossetup service, which exposes two functionalities, the verification of qos requests with the support of specific communication protocol algorithms, whose preliminary design was described in section iii, and the configuration of all the necessary network actives and devices to guarantee the selected qos with the support of specific communication protocol drivers."
"in some cases, the qosmanager might be capable of configuring the device running the service producer and consumer in order to have response time guarantees for coding/decoding the request and providing a reply. thus, the qosmanager must be aware of the applications and threads running on the devices and it must be able to configure these devices through a specific interface. more complex situations occur when services are composed by set of services running on different devices. assuming that the application requires a specific response time, then, in both cases response time calculation tools, like holistic analysis [cit] have to be applied in order to integrate communications with task scheduling."
"feature-based analysis. the movie stimulus was decomposed into both visual and auditory features in order to more precisely relate features of the stimulus to observed ∆hbo 2 responses. visual features included features based on image statistics calculated based on individual movie frames (luminance, flow). luminance was indexed by the mean pixel intensity for a single frame, after converting the full-color image to a grayscale image 39 . motion was parametrized by calculating optical flow, using the lucas-kanade algorithm for solving the optical flow constraint equation:"
"optical neuroimaging techniques enable functional brain imaging in naturalistic settings unavailable to imaging modalities with highly constrained imaging environments such as functional magnetic resonance imaging (fmri) 1, 2 . for instance, functional near-infrared spectroscopy (fnirs) enables functional brain imaging of social interactions or unconstrained movements [cit] . naturalistic imaging paradigms more closely recapitulate real-life conditions than experiments relying on tightly controlled stimuli, such as assessing speech perception with single sentence presentations, or mapping retinotopic organization of visual cortex using flashing checkerboard patterns [cit] . further, naturalistic paradigms are highly engaging, contain multi-modal content, and may be particularly well suited for populations (e.g. young children) unable make overt behavioral responses or perform a repetitive or predictable task [cit] . in addition to social interactions and natural movements, naturalistic imaging paradigms have included free viewing of movies and television shows [cit] . naturalistic viewing paradigms employing movies or television shows enable repeatability and control over stimulus presentation, like experiments incorporating simplified and distilled stimuli, but preserve the richness and greater ecological validity associated with more unconstrained naturalistic paradigms."
"the master schedules the traffic in elementary cycles (ec), which are divided into 3 parts, one for the trigger message (tm), one for synchronous messages, and one for asynchronous messages. the tm is always sent by the master at the beginning of the ec, and it contains scheduling information for the communication activities. synchronous messages are periodic and are sent over time slots that are reserved in advance. asynchronous messages are associated with priority values, the master takes care of scheduling messages taking into account the respective priorities, and the tm specifies which messages are allocated to each ec."
"inter-and intra-subject synchronization. to assess the extent to which an individual exhibited synchronized responses across repeated presentations (intra-subject synchronization), as well as the extent to which an individual synchronized with others in the sample (inter-subject synchronization) we performed a correlation analysis. for each voxel, we calculated the correlation coefficient between the voxel's ∆hbo 2 timeseries for two separate movie presentations 20 . repeating this procedure across all voxels in the field-of-view produces a spatial map of synchronization across the cortex."
"a condition that is sufficient (but not necessarily stringent) for the correct assignment of the scheduling variables s ijl is that the sum of the transmission times on each step q ij of application a i is under the deadline d i of the application, thus"
"the service oriented architecture (soa) has been used by several to implement internet of things (iot) automation [cit] . the arrowhead project is a large european effort that aimed at normalizing by means of soa design the interaction between iot applications. the effort targeted many application domains comprising industrial production, smart buildings, electromobility, and energy production. services are exposed and consumed by (software) systems, which are executed on devices, which are physical or virtual platforms providing computational resources. the devices are grouped into local automation clouds, which are self-contained, geographically co-located, independent from one another, and mostly protected from external access through security measures."
"conversely, pesos can save ∼10% of cpu energy consumption w.r.t. perf early and late in the day. however, this comes at the cost of small latency violations as shown in figure 1 . during the rest of the day, pesos remarkably reduces the cpu energy consumption by ∼30% w.r.t. perf, while keeping the wse's tail latency just below 500 ms. in fact, query workload is intense during midday, and pesos correctly selects large core frequencies in such situation [cit] ."
"to verify the requested qos, it is sufficient to sum up all the t ij for each application a i, and compare them to the deadline d i . if the total communication time is smaller than the deadling d i, the qos request is feasible, and the computed priorities are implementing it."
"since a wse can receive thousands of queries per second, a single index replica may not be sufficient to deal with such arrival rates. therefore, wses are usually deployed on clusters of servers which host multiple index replicas. by distributing and replicating the inverted index on multiple servers, a wse can process large volumes of incoming queries with low latencies. energy management. the described distributed and replicated search architectures consume megawatts of electricity, mostly accountable to their cpus [cit] . yet, relatively few focus on reducing the cpu energy consumption of wses without degrading their query latencies. here we focus on pegasus and pesos as both exploit the cpu energy minimization based on dvfs technology."
"while high synchronization was observed both within and between participants over the hd-dot field of view, this analysis approach did not relate features contained within the movie to specific cortical areas. in order to leverage the reliable cortical responses to the stimulus and relate them to naturalistic information processing, a feature decomposition strategy was employed to parameterize the movie stimulus 19, 39, 54 . in the initial feature set, seven visual and auditory features of varying complexity were extracted from the stimulus. these features were subsequently used to functionally map cortical regions related to feature-specific processing; highlighting that, despite the richness and concurrent multi-modal stimulation associated with naturalistic tasks, tracking the intensity of individual features encountered during naturalistic viewing provides an effective strategy for parameterizing and mapping the complex movie stimulus."
"to support the qossetup service, the qosmanager must keep track of the network devices configuration and the qos reservations of computational resources. in particular, the qosmanager accesses two stores: the config store, which extends the information received by the orchestrator with data regarding network topologies, capabilities of the network actives and devices, configuration of both network actives and systems; the qosstore, which keeps track of resource reservations over the network actives and systems."
"whenever a new stream is established between a producer and consumer, the interface updates the qos constraints that are being monitored, and a dedicated graph is created for each qos constraint. the application reports messages regarding qos faults, as well as any other events regarding data inconsistency or packet failure. these messages correspond also to the publication by the qosmonitor of events through the event handler to interested parties."
"any arrowhead-compliant local cloud is service-oriented and structured as a system of systems (sos). services are orchestrated either in a reactive (orchestration pull) or proactive (orchestration push) way by the orchestrator system. the qos-related services are provided by the qosmanager and the qosmonitor system, which are devoted to qos verification and configuration, and qos online monitoring, respectively."
"finally, the first firing spike within each keyword segment plus latency window is considered as a valid detection. any other firing spikes within the same keyword segment plus latency window, or outside any keyword segment plus latency window, are counted as false accepts. two metrics are used to measure the system performance: miss rate, which is one minus recall, and false accept rate, which is a normalized value of false accepts. figure 4 illustrates the idea of our evaluation approach. as examples, there are two input audio streams. the keyword segment length varies depending on the way the keyword is spoken. each keyword segment is followed by a fixed length latency window. the keyword segments are labeled by blocks with vertical line fill, while the follow-on latency windows are labeled by blocks with horizontal line fill. there is a system lock out period by design after each firing spike. for the first audio, there are two false accepts (fas) with system firing in the region outside any keyword segment plus latency window. the true accepts (tas) happen as the first detection in each keyword segment plus latency window. true accepts could happen either in the keyword segment, or in the following latency window. for the second audio, false accepts happen as additional firing spikes within the same keyword segment plus latency window which already has a true accept."
"a second set of visual features included manually coded features: visually presented faces, bodies, and hands. for manually coded features, three human raters viewed the stimulus in 1-second bins and made a binary www"
"a limitation of spatially mapping the correlation coefficient between brain responses measured across repeated viewings is that this style of analysis is agnostic to specific components of the stimulus, such as speech or visual motion, that are relevant to mapping cortical information processing. in contrast, feature extraction tools provide a powerful technique for parameterizing individual movie features and subsequently identifying regions related to processing those features during naturalistic viewing 19, 39, 40 . accordingly, the second analysis developed in this paper is an approach that maps feature-specific cortical responses during naturalistic viewing. like cortical responses mapped with reductive, non-naturalistic stimuli, these feature maps relate measured brain responses to task-related information processing demands."
the qosmonitor provides a web-based graphical interface (see the video describing the testbed [cit] ) that enables the online visualization of the performance of service fruition in a ftt-se network.
"the qosmanager acts as a plugin for the orchestrator. when a service consumer asks for an orchestrated service, it can set up qos requirements. the orchestrator computes alternative orchestrated services and verifies them through the qosmanager until one of them appears to support required qos. finally, the orchestrator requests the qosmanager to perform the reservations to grant the qos, and returns the orchestrated service to the system consuming the service."
"naturalistic viewing tasks have been extensively studied using other brain imaging modalities, including fmri 20, eeg 22 and meg 23 . work using fmri has established both practical and neuroscientific advantages of naturalistic viewing experiments. from a practical perspective, participants passively viewing a movie during brain imaging, particularly children, tend to move less relative to other passive tasks, such as resting-state paradigms, thereby reducing the pernicious effects of image artifacts related to head motion [cit] . in the cognitive neuroscience literature, naturalistic viewing tasks have been shown to reliably provide synchronized cortical responses across participants, show sensitivity to subsequent memory of the movie content, and modulate across typical and . though some optical studies have utilized naturalistic settings such as real-life interactions, the methodological and scientific appeal of repeatable and tunable narrative movie viewing paradigms, in general, have yet to be fully leveraged using optical neuroimaging 3 . naturalistic viewing simultaneously and reliably engages multiple cortical processing systems, including those related to processing the movie's auditory/visual content and narrative structure 19, 20 . these systems are spatially distributed across the cortex, underscoring the need for a large field-of-view to capture the multi-modality responses. furthermore, the complexity of information contained within the stimulus demands high spatial resolution, in order to map features within a modality (e.g. visual categories) to cortical structures related to processing those features. as with other whole-brain paradigms, such as resting state functional connectivity, imaging systems with higher space bandwidth product (~fov/resolution) provide more powerful readouts of movie-evoked responses. therefore, in comparison to traditional sparse fnirs systems, optical neuroimaging techniques such as high-density diffuse optical tomography (hd-dot), which utilize a densely arranged array of measurements across a broad field-of-view, are better suited for mapping movie-evoked responses [cit] . the central goal of the present work is to evaluate the functional mapping performance of naturalistic movie viewing combined with a large field-of-view hd-dot system in healthy young adults. cortical synchronization, as indexed by the correlation coefficient between the brain responses to repeated movie viewings, has been demonstrated using other imaging modalities including eeg 22, ecog 36, meg 23, and fmri 20, 37, 38 . cortical maps of the correlation strength between runs during naturalistic viewing highlight the broad constellation of regions reliably involved in stimulus processing. further, if hd-dot is sensitive to complex, multi-modal cortical responses associated with naturalistic viewing, we hypothesize that highly reproducible, synchronized, cortical responses will be measurable across regions related to both sensory (auditory/visual) and higher-order cognitive (e.g. linguistic) processing."
"we consider a posteriors smoothing based evaluation scheme. to detect the keyword, given input audio, the system computes smoothed posteriors based on a sliding context window containing n ctx frames. when the smoothed posterior for the keyword exceeds a pre-defined threshold, this is considered as a firing spike. the system is designed to shut down for the following n lck frames. this lockout period of length n lck is for the purpose of reducing unnecessarily duplicated detections during the same keyword segment, as well as reducing decoding computational cost."
"the qosmanager and qosmonitor systems present different needs to be able to function properly, which shape the set of systems and devices they interact with (refer to figure 2 )."
"the experiments involved the testbed described in the previous section, and a number of secondary service consumers and producers that generated traffic to congestionate the ftt-se network. the experiments aimed at measuring the time the consumer needs to start using a qos-enable orchestrated service, and the effect of qos guarantees."
the testbed considered that all except servicediscoveryrelated systems were installed locally. the servicediscovery service was reached through a virtual private network over internet. a preliminary video describing the testbed is available on [cit] .
"the qosmonitor captures information regarding communication between systems using two strategies. several qosms modules are installed over the devices in the local cloud, and they collect information regarding the performance of service fruition, and provide them to the qosmonitor through the log service. moreover, the qosmonitor can use its protocol component to access network active performance logs through traditional (non-soa) protocols."
"to keep track of both active slas, and of the actions that must be made to retrieve performance data, the qosmonitor owns a monitorstore. this database can be also used to store log data (performance, events, etc)."
"the entrypoint acts as a bridge between the arrowhead and the ftt-se environments. rest-based arrowhead communication uses the ip address of devices, but ftt-se nodes use mac addresses, and the entrypoint masquerades the identity of a system in the ftt-se network by providing it with an arrowhead-compliant address."
"here w * matrices label the connection weights. e.g., w ix, w ir and w ic represent the weight matrices from the input x, recurrent feedback r and cell c respectively. note that the peephole connections w ic, w f c and w oc are diagonal matrices. the b * terms represent the bias vectors for different components of the model. e.g., b i is the bias for input gate activation."
"let us consider for simplicity that each step q ij is executed over a network n k that uses tdma, act over different physical broadcast domains, is divided into superframes of duration m k, and capacity z k bits. moreover, each network has associated a function that provides the network resources consumed to send messages, i.e. sending payload of size x on network n k will consume a total of f k (x) of the capacity of the network. the function allows to take overheads into account. a boolean p ijk is equal to 1 if step q ij is executed over the network n k, else it is 0."
"high performance query processing is fundamental for the success and the profitability of web search engines (wses) [cit] . indeed, wses manage an ever growing collection of web documents and receive billions of queries per day but, at the same time, their users are impatient and expect results for their queries in sub-second times (e.g., 500 ms) [cit] ."
"after partitioning, index shards are assigned to different shard servers. when a query is sent to the wse, it is first received by a query broker which dispatches the query to every shard server. each server computes the query results on its shard independently from the others. these partial results are sent back to the broker, which aggregates them [cit] . the aggregated results are the same that would be provided by a single inverted index; since the computation is now distributed across several servers, query processing times are reduced. the query broker collects and aggregates the partial results from the shards, and the final results are sent to the issuing user. the set of shard servers holding all the index shards is an index replica."
"acting as a support system for the orchestrator system, the qosmanager provides services to verify qos parameters, and configure the systems and network actives of the local cloud. qos parameters are specified by means of service level agreements (slas). as most arrowhead-compliant services, the orchestration can accept messages encoded in xml, json, and other formats. when a service consumer requests a qos-enabled service, it has to include the sla into the orchestration request message to the orchestrator. an example of a qos-related sla encoded in json is as follows:"
"different fieldbus technologies have been invented or modified to provide real-time communication or other kinds of qos guarantees [cit], comprising profibus, profinet and canopen. they make use of either tdma communication, paired with token-based rotation between the bus masters, or dominance-based prioritized communication."
"the results support our hypothesis that peer review scores have some validity and the blend of commercial software, closed but free software (google services), and open source software python and crontab has proved reliable and effective. in looking forward to future courses, our plan is to continue with the infrastructure we have put in place for the delivery of this material for radiological reporting and with the peer review platform."
"a total of 82 students provided written peer review assessments and 80 received peer review data for their work. the difference is explained by two students who did not submit reports for the cases but did provide peer review for others. all took the summative mcq examination where the mean score was 80.6 % (sd 7.6 %), median 81.1 % (passing grade was 60 %)."
"thanks to this approximation, eq. (7) can now be further simplified by recalling that the weight functions sum up to one at every pixel, resulting in its final form:"
"in general, the required inversion of the inner term might lie outside the lbs subspace. however, if our weights are sufficiently contentaware, localized and smooth, we can project the inverse back into that subspace. to do so, we make the following approximation by replacing ∆t with ∆t k in the inner sum:"
"computing optimized weights over the obtained 2-manifold in feature space will provide us with the desired properties, including content-awareness. for instance, in the case of the l*a*b* feature space above, strong image edges will become steep creases in the 5d 2-manifold, making the travel distance on the manifold across the edge long, such that the weight function will have an abrupt derivative across the edge."
"recent advances in deep learning demonstrate promising performance in various fields, including protein structure prediction, natural image analysis, and natural language processing. deep learning methods often need large amounts of training samples (usually more than thousands) to effectively extract hidden patterns in the data and achieve better results. in the field of biomedicine, biomedical data often have few training samples (tens to hundreds) due to the highcost, labor-intensive, and time-consuming characteristics of biochemical experiments. however, the problem of small samples usually makes it difficult to screen robust disease biomarkers and results in poor reproducibility of prediction results among different patients. this disadvantage limits the application of deep learning methods in biomedical data. additionally, it is difficult to design a suitable loss function to learn the potential distribution of the training data. nevertheless, the architecture of the generative adversarial network (gan) can address this bottleneck problem [cit] . the generator in a gan can generate samples with similar distributions to that of samples in the training set. through the game between the unsupervised generator and the supervised discriminator, data that fit the distributions of training samples are generated by the generative model. with the decrease of the loss function, the performance of the generator and discriminator has been significantly improved [cit] . therefore, the architecture of a gan can be used to solve the bottleneck problem of small samples in the field of bio-medicine, improving the prediction accuracy and robustness of clinically useful biomarkers."
"ideally we would optimize over all possible maps, but this is far from tractable. instead, our method works within the subspace of warp functions spanned by the linear combination of a small number of affine transformations:"
"many other distribution methods could be used instead, such as blue noise or dithering techniques; however, our simple method has the advantage of choosing exactly m locations, as well as its computation efficiency and simple implementation."
"the technicalities of running the course were not trivial. while the university software formed the initial point of contact for the student, extensive use of free and open source software which generated and served course web pages and images provided flexibility and control. google forms were readily adapted for use in the collection and display of peer review data. every course has special software requirements and radiology courses are no exception. radiology is well suited to e-learning technologies [cit] . experience in this course suggests that a willingness and ability on the part of both teachers and students to use a wide spectrum of software is invaluable. ideally, enthusiasm, expertise, and ideas can come from a number of quarters, some not directly involved in the course. for this course, the provision by university department for blended learning of workarounds to overcome a lack of flexibility in the in-house teaching software and by the department of information technology of a virtual linux machine with a dedicated public ip address to host the web server and python scripts was invaluable."
"the impression of the teaching staff, although anecdotal and not recorded formally in the study, was nonetheless clearly positive. the 6-day case review periods had the feeling of momentum and automation; the students appeared busy and engaged and they participated fully in the afternoon sessions when cases were reviewed."
"to make the effectiveness of the proposed method more convincing, we compared the results of gan-daemlp with that of the state-of-the-art methods by using a t-test. the p-value of aucs is 4.08e-06 and that of auprs is 1.18e-06, indicating the prominent advantage of the proposed method. to address this difficulty, it is better to devise a robust differentially expressed gene set by intersecting the top ranking 1000 genes in the ranking lists obtained by the eight approaches, which could be considered as the most related to the disease. finally, traip, bsgnt2, ugt8a, ppp3ca, pmepa1, rgs4, ppp3r1, chn1, and st8sia3 were selected. we drew a clustered heatmap of the nine feature genes using the expression under all samples, see fig. 7 . we observed that samples of poly q20 were clustered using the nine feature genes, and the nine feature genes were in turn clustered into a single category."
"with the advent of cloud computing and data centres offering virtual desktop solutions to end users, thin-client solutions are becoming an increasingly popular mechanism for accessing and interacting with remote content and services in an easy-to-maintain and cost effective manner [cit] . in such environments, thin clients can be characterized as simple user interaction terminals, with the requirements for computational power, storage, and maintenance moved to the cloud [cit] . additional benefits include a reduction in the number of underutilized desktops hence leading to energy efficiency, wider access to data regardless of end user devices, and improved data security [cit] . today numerous solutions exist on the market supporting remote desktop connections (rdc), including those provided by microsoft, oracle, and citrix."
"1. does the report separate findings from interpretation from comments? 2. does the report identify significant technical/ radiographic issues? 3. are the radiographic findings accurate and complete? 4. are the conclusions based on the radiological findings? 5. are the comments appropriate? do they communicate the degree of certainty of the conclusions, place them in clinical context and suggest appropriate further actions? 5. graphical display of peer review data: students peer review assessment data accumulates as the course progresses and is displayed using google chart software running on google servers. this is accessible to all. the chart is updated in real time as assessments are received. student codes and scores are displayed. student names are not displayed."
"the development of machine learning algorithms in artificial intelligence applications effectively promotes mining and analyzing of the omics data, accelerating the identification of disease-related biomarkers and pathways, as well as pushing the discovery of regularity and specificity under complex disease phenotypes. machine learning methods can be classified into two categories. one is the generative model, such as the gaussian mixture model [cit], hidden markov model [cit], restricted boltzmann machine [cit], deep belief network [cit], and auto-encoder [cit], etc. the other category is the discriminative model, such as the linear regression model [cit], linear discriminant analysis [cit], support vector machine [cit], and multilayer perception (mlp) [cit], etc. the generative model learns the potential probability distribution and characterization of data in latent space, evaluates the joint probability distribution from the data, and solves the conditional probability as the prediction model [cit] . the discriminative model learns the decision function or conditional probability distribution and extracts various abstract features from input data. compared to the generative model, the discriminative model has been developing rapidly in recent years. moreover, discriminative models, such as alexnet [cit], vgg-net [cit], resnet [cit], inception network [cit], etc., often achieve a higher prediction accuracy on large data sets."
"following training and validation of the machine learning algorithm, we apply the algorithm to real rdc traces to study actual user behavior in a real world scenario (outside of a laboratory environment). to obtain empirical traffic traces, we collected packet-level traces on a 100 mbit/s ethernet link that connects the faculty of electrical engineering, university of ljubljana (fe) to the external internet, as shown in figure 3 . the traces th -nov. 16 [cit] . we used a traffic capturing system on a personal computer running linux os and with a dag network interface card. the traces were collected, filtered and stored in .cap format. we focused on the most frequently used rdc traffic, microsoft remote desktop protocol, which was recognized by transport layer (tcp) port numbers, configured to 3389 by default. to manage long traces, the individual files were limited to 0.5 gb. the traces contain rdc flows generated by students and staff of the laboratory of telecommunications, fe. the measurement procedure resulted in a total of 37 traffic traces which comprised 18.5 gb of rdc traffic. each trace consisted of multiple rdcs and corresponding flows. a total of 1364 sessions were established when summarized across all traces (excluding sessions with a session length less than 10 s). figure 4 shows the cdf for the different session lengths that were collected in the traces. results show that while the majority of sessions were of short length, there is a significant portion of longer sessions, some lasting up to multiple days. this is indicative of the fact that users established rdcs and left them open for extended periods of time, even though the connections were for the most part idle. fig. 4 . cdf portraying session lengths for collected traces figure 5 summarizes the duration of total time that users spent engaged in the given behaviors across all considered traffic traces. results clearly show that for the majority of time (92.91 %), the rdc was idle, indicating that no end user activity was detected. document editing (6.14 %) proved to be the most common activity, with small percent- this supports the assumption that the users involved in this study generally did not watch video content via a remote desktop connection, most likely due to both the nature of the tasks that users perform when using rdc, and also due to poor video quality resulting from network delays. while delay is not a determining issue in lan scenarios, the use of rdc services in wan scenarios clearly leads to the increased impact of rtt."
"users taking part in the evaluation were asked to complete a set task under given test conditions, after which they provided overall qoe ratings using a five point absolute category rating scale (1-bad, 2-poor, 3-fair, 4-good, and 5-excellent), commonly used as a de facto standard in qoe studies and specified in itu-t recommendation p.800.1 [cit] (referred to as mean opinion score, mos). users worked on each task individually, while a test administrator supervised the testing process and collected the results in a specified form. the users were not aware of the set parameters, and their evaluation was based on their own experiences with the same services when run locally on their pc. the following tasks were considered:"
"the main contribution of the gan-daemlp can be summarized in three points. first, the dae is introduced in the role of the generator, which can prevent the overfitting and divergence of the model. second, the original data instead of randomly generated noise were used as the input of the generator to reduce the training epochs and accelerate the convergence of the model. third, a framework is designed for disease gene prediction."
"changes legitimate ap's mac address using mac spoofing technique, nothing can be done to identify a mitm attack in a particular network. other kernel based patches such as antidote and anticap are used to avoid updating of host arp cache that contains a mac address different from the one already in the cache. but these patches are only used with some specific kernel."
"the loss function of the mlp is denoted as d, and that of the dae is denoted as g. let g (x ) represent the sample generated by the generator. the training process of the novel generative adversarial network is shown below."
to understand the reasoning behind this approximation let us consider the influence of a particular transformation and weight function pair (t k and w k ) on some pixel p. we can distinguish the following three cases:
"to achieve a consistent transfer between images, we need a highly accurate alignment of corresponding image regions. a powerful tool for computing such an alignment is the lucas-kanade (lk) algorithm and its extensions [cit] . the basic lk procedure computes the parameters of a warp function m by an iterative minimization of the color mismatch between is(p) and the corresponding warped pixels in some target image it(m(p)):"
we tested our implementation on an imac intel core i7 3.4ghz computer with 16gb memory. we report timings and statistics for a source edited targets edited source
"the result was a highly reliable supply of anonymized cases presented on a realistic dicom viewer requiring only a reliable internet link and a modern browser with java enabled. the platform thus could run on windows, mac, and linux operating systems. the department has computers available for student use, but the students also used the university library facilities and also their own home computers."
"the inner sum of weighted affine transformations has been reduced to a single transform that is easily inverted. despite the approximation to the true inverse warp update, this solution robustly converges to a pixel-accurate matching even in challenging cases (see figure 1 ) and it is very efficient to compute."
"finally, nine genes were selected and considered to be associated with huntington's disease. gene functional analysis and pathway analysis demonstrated that neurodegenerative disorders are comorbidities associated with many other neuropsychiatric disorders."
". that is to say, if we have m pairs of weight functions and transformations, then this space is parameterized by only 6m degrees of freedom. our optimization then becomes:"
"the warps for each target are independent of each other and thus their required computation is trivially parallelized. we leave this for future work, as it would strengthen the exploration of transferable edits to see all matched images as soon as possible."
"working in this subspace is advantageous in two ways. first, it provides a controllable balance between computational expense and the expressiveness and accuracy of the mapping. despite optimizing only the parameters of a few affine transformations, the above content-adaptive weighted interpolation allows for mapping functions that faithfully follow the shape even of complex objects. second, the reduced subspace acts as regularization which avoids the common pitfalls and local minima issues found in optical flow techniques. this allows us to match significant changes in illumination, viewpoint, non-rigid deformation, occlusions, and enables us to accurately transfer edits of a source image to multiple target images. hence we name our framework \"transfusive\" image manipulation."
"student satisfaction with the 6-day image reporting exercise was positive with a mean score of 4.19 (maximum score of 5). their overall satisfaction with the peer review process itself was less (2.82, again out of maximum score of 5). the mean and median of the student responses to the three statements relating specifically to peer review were as follows (actual statement followed in parenthesis by average (standard deviation, (sd), and median of the students' responses):"
"we note that the indicated values were set symmetrically in both uplink and downlink directions, i.e. 100 ms delay corresponds in fact to 200 ms rtt. further, the tests were designed so as to test the impact of each network performance parameter individually. hence, in order to test the impact of a given parameter, the other parameters which were not being manipulated were set to optimal values (e.g., delay set to 0 ms and loss set to 0% while testing the impact of reduced bandwidth). in the case of testing delay and loss impact, no bandwidth limitations were imposed, hence users had access to a 100 mbit/s lan."
"-idle -no actions for 10 seconds, static screen (desktop image); -document editing -editing of a word document (including text writing, picture pasting, text copying etc.); -browsing -searching for accommodation on the www.booking.com web page; -audio -listening to a 128bit/s online radio station from www.radio365.com; -video -watching a 10 minute full-screen movie on www.youtube.com."
"the form also required that the peer assessors state their own anonymous id code, the case number, and the anonymous id code of the student they are assessing."
"in the following sections, we first address user behvior detection while using rdc services, and then further link these different behavior categories to qoe studies to identify network performance requirements from an end user perspective."
"in this study, to improve the prediction accuracy and solve the problem of the small sample size in biomedicine, which limits the application of deep learning methods in biomedicine, we designed a generative adversarial network with an mlp as the discriminator and a dae as the generator."
"passive forensic analysis tool that works in the background to check the packets coming from host server to dig out data such as operating system, sessions, open ports etc."
"both laboratory test environments were setup in the same way and involved two pcs connected via a third pc emulating various test conditions. the measurements were performed using ms windows rdp, with one pc acting as a client from which the user establishes rdc connections and the other acting as the remote system. the workstations used were high performance pcs that did not influence the measuring processes with their characteristics, for their impacts were negligible in comparison to the network characteristics. in all cases, screen resolution was set to 1024 x 768 with 16 bit color depth. default settings of compression were used while caching and data encryption were turned off."
we reduce the search space of this optimization by only considering warps m defined by weighted linear combinations of a small number of affine transformations (see eq. (1)). if the per-pixel weight functions w k are precomputed then the only parameters of this subspace are the elements of the affine transformation matrices
"limitations. like all lucas-kanade based techniques, our method assumes some amount of smoothness between source and target and thus struggles to converge to a meaningful matching when the level of granularity in the matching is too small or the images are too fragmented with high frequency details. defined warp degrees of freedom for each fragment would produce an ill-posed problem with likely no or at best slow convergence. this is a long-standing unsolved problem in image matching, and this limitation is not limited to our method."
"unless explicitly specified, all our examples converged starting from the fully automatic, sift-based initialization described in section 3.5. figure 6 shows an example where our method succeeds in converging for two targets using the fully automatic initialization. at first the third target fails, but after the user manually selects just 10 loosely-corresponding points, our method is able to converge and produce a meaningful result."
"the paper is structured as follows. in section 2 we give an overview of traffic classification approaches for thin client services, and further discuss studies addressing qoe in the context of such services. section 3 focuses on traffic feature extraction and a traffic classification algorithm for rdc employing a machine learning approach. in section 4, we use our algorithm to analyze actual user behavior captured in 18.5 gb of real rdc traces in an academic setting. section 5 reports on qoe studies addressing the relationship between network conditions and qoe for the chosen set of previously addressed user rdc behavior categories. section 6 presents concluding remarks and future research."
the denoising learning process forces the encoder function f and decoder function g to learn the structure of the probability distribution of the input data. denoising auto-encoders are trained to reconstruct clean data from noise datax . this can be achieved by minimizing the following loss function
"a fundamental framework for many techniques related to accurate alignment or matching of image regions is the lucas-kanade (lk) method [cit] . we present an extension of this framework that performs matching in a linear blend skinning (lbs) subspace that drastically reduces the degrees of freedom compared to techniques like optical flow, while improving the flexibility and accuracy of the matching and thereby enabling pixel-accurate edit transfer between images."
"we extracted rna-seq data of the striatum tissue of a huntington's disease mouse model from http://www.hdinhd. org. the model included 6-month-old and 10-month-old experimental mice. there were six genotypes for the experimental mice at each stage, including poly q20, poly q80, poly q92, poly q110, poly q140, and poly q175. the detailed description of the samples used in this study is illustrated in table 1 . there were eight samples for each genotype. the modifier genes used in this study were extracted from literature [cit], including 89 disease-related genes, and 431 non-disease genes."
"executable computer scripts written in the python programming language are used to generate much of the online content. these automatically generate links to the appropriate cases for each day's activity, the up to date randomized peer review assignments, and the daily update of the teaching point summaries. the only input required to run the course comprises two csv files, one containing the case numbers of material to be made available for each course day and the other containing the random student codes. these files are prepared and uploaded to the server once only, prior to the start of the course. the python scripts are run daily at appropriate times on the web server as cron jobs [cit] . the only other teacher input required with respect to software is to change the student viewing rights on the copenhagen university, absalon software. this is done daily so that after cases are reviewed in seminar, case reports are made visible to all (to allow peer review). prior to that, individual student reports for the day's cases are hidden from view to all but the report author and teachers. the python script files mentioned above together with python script that was used to rename and anonymize the dicom images, and some sample data are available for download as supplemental files. these can be inspected, used, and modified as desired. the reader is recommended to view the supplemental file breadme^in order to get full value from these files."
"section 2 of this paper gives a brief introduction about tools used for arp spoofing, its vulnerabilities and mitigation. section 3 provides the mechanism involved in arp poisoning, section 4 describes its mitigation using dhcp snooping dynamic arp inspection and section 5 concludes the paper."
"as summarized in table 1, existing methods do not have all of these properties, in particular smoothness at fixed values. we therefore extend the smooth bounded biharmonic weights (bbw) [cit] ] to incorporate image content-awareness. in the following, we first provide a brief summary of the basic bbw for completeness, and then describe our generalization."
"implementation details. solving for the weights w k as described above amounts to sparse quadratic programming (qp) with constant inequality constraints. we utilize the mosek solver [cit] . also, following [cit], we solve for each weight function separately, dropping the partition of unity constraint (11) and then normalizing the weights to sum up to 1 (see supplemental material for pseudocode of the whole pipeline)."
"a key challenge in making such solutions viable from an end user point of view is meeting the stringent network performance requirements dictating low delays and high response times [cit] . even small increases in delay may dramatically impact user perceived quality, an issue that is of particular concern in wireless and mobile networks [cit] . as opposed to a standard, \"local\" desktop, whereby user inputs are locally processed and rendered nearly immediately, rdcs require inputs to be transmitted to a remote computer, processed, and returned to the thin-client [cit] . consequently, the screen updates and response times become a critical issue, in particular in wan environments."
"wireshark [cit] and networkminer [cit] are other important tools used in this paper in arp attack mechanism with cain & abel. wireshark is open source packet analyzer that is used for network troubleshooting and analysis. it allows the user to put the network interface card in promiscuous mode in order to see all traffic visible on that interface, not just the traffic addressed to one of the interface's configured addresses. networkminer is forensic analysis tool that can be used as passive packet capturing tool in order to detect sessions, operating systems, hostnames etc without putting any traffic on the network. it collects the data about hosts on the network rather than to collect data regarding the traffic on the network."
"the imaging exercise described here was introduced in response to these problems and tailored to the course's needs. previous experience with peer review in radiology teaching with more senior students had been positive [cit] . for the current implementation, student performance and satisfaction was monitored."
"the process of editing photographs is nearly as old as photography itself. digital techniques in recent years have greatly expanded the spectrum of possibilities and improved the quality of these edits. types of editing operations range from global tone adjustments, to color histograms (e.g., [cit] ) to localized pixel adjustments achieved by highly trained artists using specialized userinterfaces and software (e.g., [cit] ) . with the increasing availability of large digital photo collections, we currently witness a growing demand to process entire sets of images of similar scenes, taken from different viewpoints, exhibiting varying illumination, dynamic changes such as different facial expressions, and so on [cit] . as pointed out by hasinoff and colleagues [cit], the manual effort of applying the same localized edit to a multitude of photographs of the subject is often too great, causing users to simply discard some images from a collection."
"1. in general, i consider my peers qualified to give feedback on my work. their comments have validity (average response 2.91 (sd 1.1) out of 5, median 2) 2. in this exercise, i learned something relevant to radiology by giving peer review (average response 3 (sd 0.8) out of 5, median 3) 3. in this exercise, i learned something relevant to radiology by receiving peer review (average response 2.55 (sd 0.9) out of 5, median 2)."
"with respect to packet losses, document editing tolerated even up to 10% loss, web browsing up to 5% loss, and audio up to 1% loss in each direction (downlink and uplink). as regards bandwidth consumption, document editing was the least bandwidth-intensive application, while both web browsing and streaming audio resulted in mos values above 3 at available bandwidths of 2 mbit/s or greater."
"-document editing: delay: 0 ms, 100 ms, 200 ms, 300 ms, 500 ms; loss: 1%, 2%, 5%, 10%; bandwidth: 9.6 kbit/s, 56 kbit/s, 128 kbit/s, 2 mbit/s -web browsing: delay: 0 ms, 100 ms, 200 ms, 300 ms, 500 ms; loss: 1%, 2%, 5%, 10%; bandwidth: 56 kbit/s, 128 kbit/s, 2 mbit/s, 10 mbit/s -audio streaming: delay: 0 ms, 100 ms, 200 ms; loss: 1%, 2%, 5%; bandwidth: 128 kbit/s, 2 mbit/s, 10 mbit/s -video streaming: delay: 0 ms, 100 ms, 200 ms; loss: 1%, 2%, 5%; bandwidth: 128 kbit/s, 2 mbit/s, 10 mbit/s"
"occlusions are always challenging for computing image alignments, but thanks to our content-adaptive weights and our employment of a robust error norm, our optimization is able to properly ignore occluded regions such as the bushes and lamppost in figure 1 . to ensure that edits are not transferred on top of occluding objects, we first modulate them according to a threshold on the error image."
"we then project the piecewise affine map msift to the space of maps spanned by our degrees of freedom, i.e., we find t 0 k that best reproduce msift in a least-squares sense:"
our strategy for optimizing eq. (8) relies on strong assumptions about the weight functions w k used in our subspace reduction. namely we assume that our weights possess the following properties:
"where n (i) is the set of mesh neighbors of vertex i, and αij is the angle opposite of the directed edge from i to j on the incident triangle lying to the left of the edge (if it exists)."
"nowadays, network attack tools have improved tremendously which made this black art of arp poisoning very easier to accomplish. a number of windows and linux based tools are available freely that can handle the attack mechanism. the attacker no longer requires the sophisticated knowledge to use these tools. every network whether it is public, residential or organizational is susceptible to arp spoofing. one of the simplest and windows based tool is cain & abel. [cit] cain & abel is a free and simple password recovery tool that will be quite useful for network administrators, professional penetration testers, forensic staff and security software vendors. it allows recovery of various kinds of passwords by sniffing the network, cracking encrypted passwords using various types of attacks, analyzing routing, protocols, recording voip conversations, revealing password boxes, uncovering cached passwords etc. it has several built in utilities that can initiate a number of intelligent attacks on the target computer."
"it has been observed that address resolution protocol is susceptible to spoofing attacks. although arp is inevitable in the network protocol architecture, measures and mechanism need to be devised to protect this vulnerable protocol against spoofing attacks. in this paper, a test bed to check the vulnerabilities of the residential, organizational or private networks to arp spoofing attacks has been created. in this work the scope of spoofing has been extended to https unlike commonly available tools which are limited to http connections only. various techniques have also been presented in this paper to protect the users from such vulnerabilities like man-in-the-middle attacks, mac cloning etc. mostly, the available mitigation software's are applicable to specific kernel and require persistent traffic monitoring. a versatile mitigation technique that is suitable for almost every kernel has been used here and it is able to provide complete defense to these spoofing attacks. its only disadvantage is the high cost of switches. so the work is in progress to develop a novel algorithm to mitigate such attacks taking into account cost factor as well. the present work can also be taken to advantage to improve the efficiency of existing techniques."
"learning to communicate in one's specialist domain fosters a sense of identity. it is central to social theories of learning. the process of writing and speaking like a radiologist or medical doctor alters the student's self-perception [cit] . peer review in the learning process fosters a sense of responsibility and ownership for learning among peers, enhances skills in selfassessment [cit], promotes self-learning, improving selfconfidence [cit], and provides experience for the workplace [cit] . reported obstacles to communication and peer review in radiology relate to implementation [cit], student perceptions of validity [cit], and lack confidence in performing reviews [cit] . outside the sphere of teaching and learning, peer review is used as a quality control tool in radiology departments [cit] . as a learning tool and for its own sake, student exposure to peer review is important."
"since the freedom degree of the gan is too high, it should be well synchronized between the discriminator and generator, which is hard to balance in the actual training process. we use an alternate iterative strategy to train the dae and mlp. moreover, to avoid pattern loss and accelerate the convergence speed, we employ the samples in the training set as the input of both the dae and mlp and utilize batch normalization [cit] during the training process of dae. the prediction residual error of the mlp is backpropagated to the decoder part of the dae, modifying the captured probability distribution of input gene expression data for the dae. mini-batch random gradient descent training is used to train the networks in both the generator and discriminator. adam is used as the optimizer with a learning rate of 0.0002 [cit] . until the dae learns the intrinsic features of the training data, the performance of both the generator and the discriminator are all optimized. then, the trained discriminator is used to predict the labels of unlabeled samples with gene expression data."
"what is clear is that tcp is not efficient in network environments with limited bandwidth, high delay, and packet loss. hence, newer solutions (e.g., newer versions of rdp) are focusing on incorporating support for user datagram protocl (udp) as well, in particular for multimedia streaming services that are tolerant to a certain amount of packet loss, but intolerant to high delays and low throughput."
"the framework of gan-daemlp for disease gene prediction is shown in fig. 1 . based on the gan-daemlp, we designed a pipeline to prioritize disease genes. the details of the pipeline are illustrated in fig. 2 . finally, we ranked genes in descending order according to their disease risk scores. top ranking genes are considered to be the most likely disease genes."
"in the \"inverse compositional\" lk a different incremental warp update is utilized, in which the roles of is and it are exchanged compared to eq. (2). this enables highly efficient implementations since the hessian and other expensive steps can be precomputed. however, the sought global warp has to be iteratively composed with the following update rule:"
"we presented a method for image edit propagation using contentaware warping, constructed as a spatially-varying weighted combination of affine deformations. the weighting functions are computed by fourth-order energy minimization over the image manifold in feature space, with constant bound constraints, which makes them smooth everywhere except at strong image edges. this leads to successful non-rigid registration between a source image and multiple target images of the same subject using our adaptation of the lucas-kanade framework."
the main contribution of this paper is to comprehend the underlying mechanism of arp poisoning and to check the vulnerability of the hosts with these attacking tools. the research also illustrates how to root out this problem by adopting appropriate mitigation technique.
"next, we used disease gene expression data to train gan-daemlp for disease gene prediction and use non-disease gene expression data to train gan-daemlp for the non-disease gene prediction. let s d (g) represent the disease gene prediction score, and s nd (g) represent the nondisease gene prediction score for gene g. the disease risk score of gene g can be computed by"
"we used the annotations from psymukb (http://www. psymukb.net/) to annotate the nine genes. seven of them are protein-coding genes. meanwhile, traip (traf interacting protein) locates at 3p21. 31 . it has been reported that seckel syndrome 9 and seckel syndrome are associated with traip. gene ontology annotations related to this gene include ligase activity and obsolete signal transducer activity, downstream of the receptor. ppp3ca (protein phosphatase 3 catalytic subunit alpha) locates at 4q24. epileptic encephalopathy, infantile or early childhood 1, and arthogryposis, cleft palate, craniosynostosis, and impaired intellectual development are associated with the gene. gene ontology annotations for the gene are calcium ion binding and enzyme binding. pmepa1 (prostate transmembrane protein) locates at 20q13. 31, which functions as ww domain binding and r-smad binding. rgs4 (regulator of g protein signaling 4) locate at 1q23.3. schizophrenia and psychotic disorder are associated with rgs4 [cit] . gene ontology annotations related to this gene include gtpase activity and g protein alphasubunit binding. ppp3r1 (protein phosphatase 3 regulatory subunit b, alpha) locates at 2p14. diseases associated with ppp3r1 include extracranial neuroblastoma and cervical neuroblastoma. gene ontology annotations related to this gene include calcium ion binding and calmodulin binding. it has been reported that ppp3r1 is related to a higher cerebrospinal fluid tau level. moreover, single-nucleotide polymorphisms (snps) located in the gene regulatory subunit of ppp3r1 (rs1868402). our study showed ppp3r1 is also a critical differentially expressed gene, consistent with previous research. chn1 (chimerin 1) locates at 2q31.1. duane retraction syndrome 2 and duane syndrome type 2 are associated with chn1 [cit] . gene ontology annotations related to this gene include gtpase activator and ephrin receptor binding. st8sia3 (st8 alpha-n-acetyl-neuraminide alpha-2,8-sialyltransferase 3) locates at 18q21.31. gene ontology annotations for this gene are sialyltransferase activity and alpha-n-acetylneuraminate alpha-2,8-sialyltransferase activity."
"if sift features are only found in one part of the source image, the above system may be underdetermined as there might exist transformations t k whose corresponding weight functions w k are zero or near zero for all pixels in t . in these cases we regularize against the best-fit global affine transformation a found using ransac on the sift features, adding the following term to the above minimization:"
"the scores obtained in the concluding summative mcq examination were compared with those obtained in the peer review exercise. thus, three course parameters of performance were available for each student: average peer review score received, average peer review score awarded, and the score obtained in the mcq examination."
no major technical problems were encountered. problems were limited to the java installations on the students' machines. the server does not run a security certificate with the software so the teaching url had to be specified in the security exceptions tab in the local java control panel.
"when a device needs to communicate with any other device on the same wireless network, it checks its arp cache to find the mac address of the destination device. if the mac address is found in the cache, it is used for communication. but if it is not found in local cache, the source machine generates an arp request. the source broadcasts this request message to the local network. the message is received by each device on the lan since it is a broadcast. as arp is a stateless protocol; therefore all client operating systems update their cache if a reply is received, inconsiderate of whether they have sent request for it or not. since arp does not offer any method for authenticating replies in the network, these replies are vulnerable to be manipulated by other hosts on a network [cit] ."
"finally, to verify the disease gene prediction performance of gan-daemlp, we conducted various comparative experiments with different hyperparameters and network structures to select the best performing model structure. then, the best performing gan-daemlp model was further compared with deseq2 [cit], edger [cit], limma [cit], t-test [cit], fc [cit], mlp, and gan [cit] approaches. the comparison results showed that gan-daemlp achieves the best performance. moreover, disease-related genes prioritized by this model were further analyzed. nine top-ranked genes were selected to explore the molecular mechanisms under the complex disease phenotypes. functional annotation and enrichment analysis of the nine genes showed that biological processes, such as calcineurin-nfat signaling cascade, mitotic cell cycle phase transition, and protein dephosphorylation, were severely affected in the striatum tissue of huntington's disease mice."
"meanwhile, l is a loss function, which constrains the distance between g (f (x )) and x. the training objective function of the denoising auto-encoder can be written as"
"for gene expression data analysis, the denoising autoencoder (dae) is a more suitable choice for the generator. due to the large amounts of noise, such as missing values, technical variations, sample variations, etc., hidden in the gene expression data, the gan with a three-layer perceptron as the generator is not suitable for capturing the statistical characteristics of the input data. nevertheless, the dae can be seen as a multilayer perceptron that is trained for denoising to some extent. the incomplete learning of the dae can well learn the intrinsic features of the data and capture the most significant statistical characteristics. through the incomplete learning representation, the dae can filter out the noise and capture probability distributions of input data, then reconstruct that original data as high-quality output [cit] . on the other hand, the pattern is often lost in the learning process of a gan, which could lead to the degeneracy of the generator and the generation of data with the same probability distributions, consequently stopping the learning process. however, the incomplete representation of learning of the dae, which is a means for dimension reduction, can prevent the divergence or overfitting of the generator in the traditional gan model. based on the above discussion, to solve the problem of small sample sizes in the field of biomedicine, we designed a new generative adversarial network model with the dae as the generator and the mlp as the discriminator (gan-daemlp). first, an mlp with more than three layers can fit arbitrarily complex functions and learn the mapping relationship well between the input data and output labels. the prediction residual error of the discriminator was backpropagated to the decoder part of the dae, modifying the captured probability distribution. through the game between the generator and discriminator, the prediction performance of the mlp could be greatly improved. based on the gan-daemlp model, we further designed an analytical pipeline to predict disease candidate genes with rna-seq data."
"by up-link we refer to packets originating from the rdc client, while down-link refers to packets originating from the rdc server. the reason for choosing an epoch of 10 seconds is due to the fact that previous research has shown this approach as providing good results in terms of accuracy [cit] . traffic features were extracted from the network traffic traces using a custom built java-based parser."
"-web browsing -users were asked to open a web browser and browse through a predefined set of web pages; -audio -users listened to a song streamed for a duration of 20 seconds per test configuration, encoded using aac-lc at an average bit rate of 256 kbit/s, sampling rate 44100 hz, played by the gom player; -video -users watched a full-screen cartoon for a duration of a 60 seconds per test configuration."
update the mlp by descending stochastic gradient of a loss function 10: update the decoder part of dae using gradients introduced from mlp 11: end for 12: end for
"with the rapid development and decreasing cost of sequencing technologies, whole-genome sequencing data have become commonly available, providing not only opportunities but also challenges for decoding the pathophysiologic mechanisms of chronic complex diseases from a molecular system level. it is an important research topic in the field of bioinformatics to develop effective and reliable computational tools to screen disease biomarkers and drug targets using omics data."
"transferring pixel-level edits from one image to another requires accurate mappings between corresponding image regions. finding such correspondences is a long-standing problem in many areas of computer vision and graphics, and there exists a variety of basic, general purpose correspondence estimation techniques ranging from dense optical flow (e.g., [cit] ) to sparse features (e.g., [cit] ) . techniques for optical flow computation and dense feature tracking allow for sophisticated edit propagation in video [cit] . however, those methods can handle only small image displacements and appearance changes [cit] ] and hence are not suitable for the types of image collections we are aiming at. methods for stereo correspondence estimation may cope with larger differences between views, but require camera calibration and static scenes [cit], while we would like to work also on uncalibrated images of non-rigidly deforming subjects. model-based tracking [cit] can handle such deformations, but is generally restricted to face tracking. sparse feature matching and tracking [cit] does not produce the dense correspondences required for multi-view image editing applications. using optical flow in the sift domain results in a dense matching [cit] ], but the resulting warp is not smooth enough for detailed edit transfer."
"to acquire a deep insight into the dynamic molecular mechanism and the pathological mechanism underlying complicated clinical disease phenotypes, we performed gene ontology analysis and enrichment analysis."
our current method is limited to transferring edits made within the region of interest on the source image and correspondingly the warped region of interest on the target. exploring methods to extrapolate our warp function beyond the user-selected region of interest is an interesting and tangible direction of future work.
"the first step in our work was to train an ml algorithm to classify different types of rdc behavioral activities, to be subsequently used for the analysis of real rdc traffic traces. the behavior labelled traces which were used for training the decision tree algorithm were collected at the department of telecommunications, faculty of electrical engineering and computing, university of zagreb. the capture was performed on dell optiplex 390 computer (configuration: i3@3,3 ghz, 4gb ram, ati radeon hd 6450) connected to another dell optiplex 390 via a 100 mbit lan. the behavior categories we have addressed are as follows:"
"for these reasons, a number of algorithms specifically designed for applying edits to multiple images have been recently proposed. for specific object classes, e.g., using face detectors or automatic labeling based on learned features, photo manipulations such as tonal adjustments or object removal have been successfully demonstrated [cit], but a general pixel-accurate mapping between images as in figure 1 is not supported. [cit] combine various complementary feature detectors, cluster those features whose centers define a homography between images, and then use the homographies for edit transfers on large image collections. however, as discussed in their paper, homography-based edits are limited to relatively planar regions and cannot appropriately handle the nonplanar or non-rigid differences between images that we are aiming at. [cit] present an extension to the patchmatch algorithm [cit] ] that enables partial, non-rigid image correspondences with impressive results for applications such as color, mask, and blur transfer between images, but it is optimized towards those types of edits that do not require pixel-accurate correspondences between large, user-defined regions. as we show in our results, our approach complements these works by addressing some of their fundamental limitations."
"the university of copenhagen learning management software (absalon, itslearning as -p.o. box 2686-5836 bergen, norway) is used via web access as the point of entry for students. each student has a personal login to this software throughout their undergraduate course, and each course has its own section within the software. the imaging course page includes six folders each with links to clinical information on three cases. all students have access to the same folder each day, and they cannot preview the next day's material. a link to case information also gives access to an open text field for the student to report on the case. since each student has an individual login, case reports are linked to the individual student author. the software requirements for this system are very basic; they are simply to present a case history and provide a text field to record input. we used the university system because it was to hand and was a familiar start point for the students, but this component could have been replaced by a more open source solution."
"there is a weak positive correlation between the summative mcq examination score and the scores received by peer review for the case reports. the same data were used to stratify the students into bhigh^and blow^course performers, and there was a difference in peer review performance between the groups. both observations suggest that peer review has some validity; those performing the assessments can be credited with performing good reviews. that the students could perform peer review was expected, and it has been shown previously that medical students can perform peer review acceptably [cit] . the correlation between peer review scores and performance with the summative mcq examination suggests that not only were the students capable of peer review but also that the communication or case reporting task itself was relevant to student learning. the mcq examination used for the course tests a wide range of skills: radiation safety, radiography, radiographic anatomy, disease interpretation, and knowledge of imaging modalities. while at first sight these topics may appear to span a broader range than that required to write a report on a radiological study, a deeper insight into the subject tells that the good radiologist will use knowledge and understanding in all these areas to prepare a full report. a good report will be expected to comment on the safety and radiographic issues of the study, to identify abnormalities, to interpret them, to place them in a clinical context, and to advise on future progression for the patient. the overall aim of the veterinary imaging course is to enable students to produce diagnostic images safely from veterinary patients and to make valid and useful interpretations. the final 6 case reporting days are thus well aligned to the overall course and to the summative mcq examination. the presence of a correlation can also be interpreted as an indicator that the case evaluations with peer review component of the course were well aligned with the summative examination."
"timeline, independent study, and group sessions the 6-day case reading session with peer review runs to a fixed time line. to mimic a real life situation, where reading and interpretation of radiographs is needed within a limited time span, the cases for each day automatically become available at 5 min past midnight and are available for 24 h only. students are told to write independent reports and submit them online. they are instructed not to work in groups. they are free to ask general questions as they arise for the cases, the guideline being that if the information needed is available in a textbook, one is free to ask a colleague, perform an internet search, or use textbooks to find an answer. students are instructed not to ask or to say the particular diagnosis or a differential diagnosis list for a particular case. a dedicated chat line (kiwi irc. https://kiwiiec.com) is available on a web site for students to communicate, while reporting, if they wish. the case reports are to be completed by 2:00 pm each day. at that time, all students taking the course (typically 20 in number) meet with one of the teachers for a 1-h seminar where each case is displayed, reported orally by one of the students and discussed. at the end of this session at 3:00 pm, the teaching points for the day's cases and the peer review assignments are automatically updated on the course web page. in addition, viewing rights for the case reports are altered at this time, so that all students can see all reports for the day. with knowledge gained from preparing a report for each of the day's cases, from the case discussion in the seminar and from reading the teaching points, the students are as well prepared as possible to perform peer review. a web page is generated which randomly links students to peer review tasks. students are requested to have completed their reviews before midnight on the same day while the images remain available online. a graphical representation of this time line is shown in fig. 1 the peer review platform"
"figure 2: left to right: the user-input region of interest rs on the young man. we visualize a selected content-aware bounded biharmonic weight function optimized on various downsamplings of the domain. we see diminishing returns past 12.5%, and thus use this resolution for all our remaining examples."
"however, in the field of biomedicine and bioinformatics, samples with labels are rare, which significantly limits the application of deep learning methods. to address this problem, we developed a generative adversarial network model. through the game between the generator and discriminator, the prediction performance of the discriminator is greatly improved. moreover, the denoising auto-encoder was capable of filtering out noise in gene expression data and deepened our insights into the distribution of gene expression data in the latent space."
"the measurements for each of the behavior categories lasted 10 minutes, i.e., during those 10 minutes, only a given action was being performed. the actions recorded were performed by student volunteers of the faculty of electrical engineering and computing. we note that previous cited efforts [cit] have not considered the category of idle, even though this is commonly observed in rdc traffic. the reason is that previous efforts focused either on identifying certain applications being run over an rdc or studying end user qoe, while our goal is to study overall user behavior exhibited when using rdcs. following identification of a number of traffic features to be used for classification, we conducted a traffic analysis and specified a decision tree algorithm. we then used our collected traces to train the ml algorithm. finally, we validated the algorithm using a validation dataset."
"in future work we would like to improve both precomputation steps of our algorithm: weight computation and matching. for the quadratic programming we are currently using an interior point solver (mosek) which cannot efficiently benefit from initial guesses. however, good initial guesses can be constructed by solving on a lower resolution and/or employing the unbounded biharmonic solution. for example, it is evident that the solution is stable and consistent for different image resolutions (see figure 2) . moreover, when many weight functions are used, the support of each individual weight function significantly decreases, implying that optimization on a much smaller area than the whole region of interest is possible."
"on the other hand, a survey has been conducted on the tools used to detect and mitigate these attacks so that the networks can be secured to larger extent. but it has been observed that there is no universal defense against these attacks. in fact one of the simplest methods is to use static arp entries. as static entries cannot be updated, spoofed arp replies can be ignored. but this method is not suitable practically for large networks to manually add each entry into the cache."
"we tested the degree and type of correlation between the score received in peer review and that received in the mcq examination. a positive correlation would suggest that the peer review scores have some validity. furthermore, we tested the correlation between score awarded in peer review and scores received in the mcq. the outcome of this test addresses the question bdo higher performing students grade more (or less) severely than lower performing students?ŵ e also tested for differences in peer review scores received and awarded according to overall course performance. students with mcq scores in the first quartile were considered low performers, those with scores in the third quartile, high performers."
"in character animation, this subspace of deformations is called linear blend skinning (lbs). working in the lbs subspace makes the optimization computationally feasible, and when carefully designing the minimization process and the weight functions w k, this subspace proves to be sufficiently expressive. it leads to accurate and intuitive warps, which have the required balance between smoothness and content-adaptiveness necessary for successful edit transfer."
"in order to study the impact of different network conditions on end user qoe for different addressed behavioral categories, we have conducted a series of subjective tests. the tests were conducted across two different laboratories: qoe evaluations for document editing, web browsing, and streaming video were conducted at laboratory facilities at the faculty od electrical engineering university of ljubljana, while qoe evaluations of audio streaming were conducted at the university of zagreb, faculty of electrical engineering and computing."
"for φ one typically employs some robust error norm for increased robustness to image inconsistencies like occlusions. specifically, we employ adaptive thresholding with spatial coherence approximation [cit] . linear appearance changes can be accounted for within the same framework to compensate for changes in illumination during the matching, detailed in section 3.4."
"features are attributes of flows calculated over multiple packets, used to train a ml classifier in associating sets of features with given application types [cit] . behavior labelled traces were processed such that for an epoch (time window) of 10 seconds, six features were extracted:"
"classic bbw. the bbw have been introduced for realtime deformation of 2d images and 3d shapes. each weight function w k is associated with a handle h k, which is a region (or just a single point) in the domain fully controlled by that weight function. hence w k attains the value of 1 on h k and 0 on all other handles. the weight functions are defined by optimizing the laplacian energy subject to constant bound constraints: arg min"
"the steps involved in run time environment are explained as follows. 1) to do a man-in-the-middle attack against the target host, start cain and we will find the sniffer tab on the main panel. we have to activate the sniffer tab to allow the software to sniff packets. we need to select the active network adapter by clicking on the configure option as shown in figure 2 ."
"peer review data from all students participating in the course were analyzed. for each student, the average grade received for all reports they authored was calculated. the average of all scores awarded by each student was also calculated."
"we introduced a communication and peer review exercise to improve student satisfaction with and involvement in a 5-week veterinary imaging course. we hoped to achieve this without increasing teachers' workload and by using available open source or free software to its maximum advantage. satisfaction with image reading exercises in the existing course was suboptimal according to previous formal and informal post course student feedback. the image reporting exercise was found to be a source of frustration for the students. problems identified included students remaining passive in peer groups, unwillingness to commit or form opinions about the images, and a desire to constantly seek input from teaching staff in preference to independent research and thinking."
"our experiences with the implementation and assessment of peer review in a veterinary radiology course are reported here. in particular, we report the logistics of providing web access to radiology images for teaching and we examined the skill of students in performing peer review, the possibility of differences in peer review performance according to the overall level of performance in the course, and the students' level of satisfaction with the material and peer review. we hypothesized that online delivery would be realistic and convenient, that student performance in peer review would correlate with performance in the summative course examination, and that students would see value in the peer review process."
"currently, applications of machine learning and deep learning methods are becoming ubiquitous in the field of biomedicine to screen disease-related genes. furthermore, the accumulation of the omics data and single-cell sequencing data have greatly promoted the development of statistical machine learning methods, evolutionary methods, and deep learning methods. our understanding of the molecular mechanisms of complicated disease can be further deepened with the accumulation of large amounts of omics data. additionally, the prediction accuracy can be significantly improved with the advanced algorithms."
"following the classification and analysis of different rdc behavioral categories in the previous sections, a key concern in aiming to meet end user quality expectations is an understanding of the impact of underlying network performance on perceived quality. in this section we therefore focus on quantifying the relationship between network performance and qoe, and compare our results to those obtained in related studies."
"we used gene expression of the huntington mouse model from brain samples of 6-month-old and 10-month-old mice with genotypes of poly q80, poly q92, poly q110, poly q140, and poly q175 to train gan-daemlp for disease gene prediction, and used non-disease gene expression of samples of 6-month-old and 10-month-old mice with those genotypes to train gan-daemlp for non-disease gene prediction."
"this new class of warp functions m can be readily integrated into the standard \"forward additive\" variant of the lk algorithm. in practice, however, this variant of lk is computationally inefficient, as it requires a re-computation of the hessian matrix at each iteration of the algorithm. instead we derive an alteration of the highly efficient \"inverse compositional\" variant of lk to search over our parameterized subspace (see, e.g., [cit] for details about the different variants of lk)."
student responses in the free text format were very positive with the majority (80 %) focusing on the overall learning experiences from the cases. a minority was concerned with the peer review process itself recognizing the value in comparing reports others produced with one's own work. other features of the course identified to be of value by the students were the requirement to view and report in writing before dealing with the material in plenum and being bforced^to work alone for some part of the exercise. criticisms were centered on the workload with the major complaint (20 %) being that the amount of material presented each day (3 cases) was excessive.
"due to the clinical heterogeneity of the chronic complex neurodegenerative disease, it is imperative and necessary to conduct pathway analysis to identify key pathways associated with the disease. we further annotated the seven genes using geneanalytics (https://ga.genecards.org/). the seven genes mainly expressed in globus pallidus, caudate nucleus, cerebral cortex, subthalamic nucleus, pons, hystem +tgfbeta3+gdf5-induced sk11 cells, and medulla oblongata in the brain. moreover, it has been reported that seckel syndrome 9, epileptic encephalopathy, infantile or early childhood 1, arthrogryposis, cleft palate, craniosynostosis, and impaired intellectual development, duane retraction syndrome 2, duane syndrome type 2, duane retraction syndrome, seckel syndrome, undetermined early-onset epileptic encephalopathy, and autosomal dominant non-syndromic intellectual disability are associated with these seven genes."
"similarly we can see the images, dns, files, credentials entered by the user, sessions etc by clicking on the specific tab. in this way the whole mechanism of arp attack is performed. used to learn network protocol internals;"
"go terms for the seven genes include calcium-dependent protein serine/threonine phosphatase activity, calcineurin complex, calcineurin-nfat signaling cascade, cyclosporine a binding, calmodulin binding, response to amphetamine, wnt signaling pathway, and calcium modulating pathway. pathways associated with the seven genes include darpp-32 phosphorylation, g-alphaq signaling, nnos signaling at neuronal synapses, nur77 signaling in t cells, initiation of transcription and translation elongation at the hiv-1 ltr, mapk-erk pathway and tacrolimus/ cyclosporine pathway."
"while previous studies have shown that delay in terms of rtt is the main performance parameter to be considered, we conduct tests also under different bandwidth and loss values in order to extend the parameter consideration and provide a clearer view on the qoe impacts of different conditions. what is clear is that given that tcp is being used as the underlying transport protocol, packet loss will result in retransmissions and observed delays or retransmission time outs. different test conditions were manipulated for the different behavioral categories in order to try and test all possible mos scores, ranging from excellent to poor, and to find the behavior-specific lower bounds in terms of acceptable network degradations. the following test conditions were manipulated, ranging from a high-speed and low delay network to a slow network with high delay and packet loss (the rational being that such conditions may still found today during traffic peaks in mobile networks):"
"2) after selecting the active network adapter, we will start scanning for all the active hosts in our subnet. to do this, we will click on the blue \"+\" icon. a dialog box will appear and we have to select \"all hosts in my subnet\" and then click ok as shown in figure 3 ."
"(1) describes a vast space of expressive warps. because each weight function w k varies over the domain, the resulting warps are in general far more interesting than constant or even piecewise affine transformations. the same applies to the blends of other per-weight function parameters like bias and gain parameters described in section 3.4."
"access to radiological images (radiographic, magnetic resonance imaging, and computed tomography) for student reporting with peer review was introduced and continues in use as a 6-day module within a 5-week veterinary radiology course. it is placed at the end of the course, which typically comprises 20 to 22 students, all of whom will have completed courses in basic anatomy and pathology. the course runs four times per year, and data from 82 students participating in four courses are reported here. it covers basic undergraduate requirements in veterinary imaging, radiation safety, radiographic technique, interpretation, and radiographic anatomy. in preparation for the case review module, students are instructed on how to write formal structured radiographic reports. two to 4 days prior to the start of the module, one of the course leaders uses a 40-min session to tell the students the purposes of the module, how to access case material, how to enter reports, and how to perform peer reviews. for the last of these, students are told how to access a peer review form, informed that it comprises a series of check boxes, and told when peer review is to be performed. the peer review form asks the extent to which essential components of a radiographic report were present (the peer review questions are provided below). in addition, students were told that the performance of peer review is voluntary, but the honorable course of action is to perform careful peer review. no formal information was provided on the pedagogic merits and rationale of peer review. the course concludes with a summative multiple choice examination (mcq), which contains approximately 40 questions that cover the entire course content. performance or activity in peer review is not factored into the final score for the course. students are informed that participation in the image reporting and peer review process is formative and not summative."
"we conducted large amounts of experiments on gene expression data of the huntington's disease mice. to investigate the effectiveness of gan-daemlp proposed in this study, we also conducted experiments with the t-test, fc, deseq2, edger, limma, mlp, and gan. the comparison results of the eight methods are shown in fig. 5 and fig. 6 . it is clearly shown that gan-daemlp performed best compared with other approaches. moreover, gan-daemlp and mlp had a high prediction accuracy for top ranking genes. although the training accuracy of mlp was high, it was hard to prioritize disease genes (fig. 6) ."
"solving on meshes with the fine resolution of our source image can be too expensive for interactive performance. fortunately our weights are resilient to changes in discretization resolution. we may thus downsample our image before optimizing our weights and then upsample each weight function to full resolution using bicubic interpolation. downsampling too much eventually has a smoothing effect and changes the support regions of each function, however we see diminishing change in the values and supports of our weights as the discretization approaches full resolution (see figure 2 )."
"rdc traffic generally corresponds to encrypted video bitmaps exchanged between a remote virtual pc and a thin-client using a single server port, imposing challenges in terms of detecting end user tasks and applications that are being remotely run (e.g., a user editing a document, browsing the web, or viewing audio/video content) [cit] . rd protocols (e.g., microsoft remote desktop protocol, rdp) commonly run over tcp and offer a reliable connection. different user behaviours while using rdc (in terms of conducted task or application being used) result in different traffic characteristics and different impacts of network performance on user perceived quality [cit] . therefore, a prerequisite in effectively managing the end user quality of experience (qoe) is related to determining the user behavior in the context of rdc for the purpose of accurately mapping between qoe and performance indicators (i.e., delay, bandwidth). [cit] stress the need to consider qoe-driven utility functions in the context of optimally allocating resources (i.e., cpu, memory, bandwidth) to virtual desktop flows."
"we note that while our studies focused on rdc traffic transmitted using tcp (currently still the most common case), future work in this area should focus also on conducting qoe studies with newer rdc software supporting also udp (e.g., newer versions of remote desktop protocol available on windows 8, or vmware's pcoip -pc over ip protocol) and adaptive graphics, particularly in the context of media streaming. ongoing developments are focusing on optimizing and adapting the server encoding process in order to improve user perceived quality in light of different network conditions and media requirements."
the chart shown in figure 2 is a boxplot of task times for each pair of 5 tasks. it can be seen that all the five median values for the mobile phone are higher than their corresponding task for the laptop. the six outlier values shown as numbered points on the chart were excluded from the test.
"the mobile phone's smaller screen could just show some part of the website while the laptop could show the whole width of a home page on screen. therefore, if people wanted to find particular information with mobile phone, it would take longer to look around and scroll across the screen to find the information. moreover, for older participants who might have poorer vision, to read the text in the small screen, they had to zoom into the text. however, once they enlarged the text font they could not see the whole information of the website. so to then check the whole page, they had to decrease the font size. this problem meant that participants had to repeatedly enlarge and decrease the font size when they were looking for specific information with the mobile phone, which increased the completion time of each task."
"miss clicking: in the questionnaire, almost 30 percent of the participants stated that it was not so convenient when they wanted to click a specific link on the touch screen. this problem also related to the relatively small size of the screen, so that users frequently clicked the wrong button or link on the web page. possibly a touch screen stylus could solve this."
"this study had certain characteristics that could have affected the results. firstly, as all the participants were from loughborough university design school they might have already used the school webpages before, so the completion times may have been affected by their previous experience. however this was not a noticeable effect during the study. secondly, although the shortest path for each task made each task pair equivalent, there were different ways to complete some of the tasks, so the number of steps followed varied depending on the route followed which may have affected the equivalence of the task pair for a particular participant."
"the analysis of task performance shows that across all the five task pairs, performance with the laptop was greater (or faster) than with the mobile phone. this was in general due to the smaller size of the mobile phone screen and consequently smaller size of information, web-links and touch-screen keyboard."
"the purpose of the study was to explore the differences between a mobile phone and laptop for accessing the internet by asking users to complete 5 tasks with each device. the results show the completion times for the mobile phone were only significantly longer than the laptop for task pair 1 and 2 and not the other three. there was no statistical significance in subjective rating of whether the phone or laptop was easier or more difficult so that while using the mobile phone may have taken longer to perform a task, it was not necessarily seen as harder than the equivalent task on the laptop."
"the project used performance testing to compare the difference between the laptop (with a 13.3 inch screen and physical keyboard) and mobile smart phone (with a 4 inch screen and touch screen keyboard). although a laboratory test has some limitations compared to a field study [cit], this style of evaluation is more straightforward for conducting a controlled comparative study."
"for task pair 4, the spread of performance times was noticeably broader with the mobile phone than the laptop. as the smaller mobile phone screen was only able to show part of the map, participants needed to zoom in and out to find where the building was. however with the mobile phone participants sometimes scanned information less thoroughly than on the laptop and if they thought they might be on the wrong page (or part of the map) and could not find the information they wanted, they just returned to the search page and started the task again. for this reason some participants might have found the location of the building for task 4 on the mobile phone as quickly as on the laptop."
"from the average time difference (table 2), it shows that when using the mobile phone all tasks took longer than using a laptop. but in terms of difficultly rating, table 3 shows that participants did not necessarily consider using the mobile phone harder to use than the laptop and the wilcoxon test showed no significant difference in participants' selection of either device as being easier or harder. there might be several reasons for this. firstly, even though using a mobile phone may take longer to finish the task, it does not mean that using mobile is harder than laptop. even if the user spent more time using a mobile phone to search for information, if the process was going smoothly, they may have thought that mobile phone was as easy to operate. secondly, the outlier values shown in the boxplot in figure 2 may have disproportionately affected the average time differences. because of these outliers, the results show that the mobile phone took longer than the laptop to complete each task when people may not have felt that the mobile phone was harder to use than laptop. thirdly, participants may have not liked to admit or consider that a task was harder for them when they realised that they just missed some information that they felt they should have seen or recognised that there was an easy way to complete the task. thus they may have given a more positive rating."
the testing recorded time to complete five tasks with each device and used a paired t-test used to determine if there was a significant difference between times for the mobile phone and laptop. the five tasks used with each device were designed to be equivalent to make the two sets equivalent in difficulty. the order of presentation of the tasks and device was varied between participants to avoid order effects.
"the range of times taken for task pair 1 and 2 is smaller than the ranges for task pair 3, 4 and 5. the main reason for this might be the clarity of the searching operation and the interference of related information. in task pair 1 and 2, the required information to find is simple and clear so did not cause too much confusion for the user. but in task pairs 3, 4 and 5, there were related information showing on other pages which did not provide the right answer, which may have caused participants to spend too much time searching on the wrong page. and in task pairs 3, 4 and 5, there were more than one way to find the right information and some of them were more complicated than others, so this needed more time to complete the task."
"screen size: 75 percent of participants reported that the screen size was a key problem when using the mobile phone to search for information since during the searching process. they needed to zoom in and out multiple times in order to read the information detail and also browse the whole website. one of the participants suggested that a \"word wrap\" function may make it easier to read specific text. the \"word wrap\" function is provided on the blackberry mobile phone and some htc mobile phones. so when the user zooms in to the website to read the detailed information, the text will re-format automatically so the user won't need to move left and right across the screen but just scroll the page up and down."
"after the user trial each participant was invited to design a reduced home screen for loughborough university to be suitable for a mobile phone. although smart phones already have relatively large screens for phones, the information of a typical website is still too much to display for comfortable use. thus a reduced mobile phone version home screen might be useful and necessary."
"this aim of the study was to compare a mobile phone (with a 13.3 inch screen and physical keyboard) and a laptop (with a 4 inch screen and on screen keyboard) for accessing the internet. in addition, it was intended to record the user's experience of the loughborough university website and ideas for a reduced mobile version of the home screen."
"the participants included members of staff and students (phd and postgraduate students) from loughborough university; thus they were familiar with the website and could give more useful suggestions about its improvement. a total of 24 persons aged from 25 to 55 years old took part in the study with a median age of 35. ten participants were female and 14 were male. most of the testing was carried out in the loughborough university design school, although for the convenience of two of the participants, it was carried out in their own homes."
"most participants reported some problems in using the mobile phone including: small screen size, missed clicks and inconvenience of the touch screen keyboard. these problems could be addressed with some extra functions or tools such as a simpler reduced home screen for the phone which should make it easier to view the information. \"word wrap\" would also be helpful when reading detailed information on a website. a pen or stylus for input when the buttons and links are smaller and a combination of touch screen and physical keyboard could also make the phone easier to use for internet access. voice is also an alternative for keyboard input. it is likely that there will be further innovation in the future to assist users of smart phones to be as efficient as laptop users when accessing the internet."
"mobile phones are not just a device for calling and texting people but have become a necessary and indispensable tool in our daily lives. in combination with the internet, mobile phones already have the functions of a personal computer but in a handy portable size. however the relatively small screen and keyboard size of a mobile phone can make an ordinary webpage or ecommerce website difficult to use [cit] . smart phones can display most websites without modification, however the user is often unable to read the text or see useful content without pinch-zooming. responsive websites can adapt to the device or operating environment although the technology still faces challenges to overcome such as non-fluid advertisements [cit] . this study explored the difference between mobile phone and laptop when using them to perform search tasks on the internet. the project was also inspired by the need for loughbor-ough university information to be easy to obtain via a mobile device which is often student's preferred way of accessing the internet."
"from the sketches made by the participants, most people chose to include a search bar, school and department tabs or links, and current student and staff functions on their mobile home screen. an example sketch is shown in figure 3 . this shows a mobile version which incorporates a split screen with fixed, generally useful, options on the left and a scrolling list of more specific options on the right. tab buttons are also provided at the top of the screen to allow the user to swap between the normal full screen version and the mobile version as they wish."
"keyboard: 25 percent of participants reported that the keyboard on the mobile phone was harder to use than that on laptop, especially the male participants who generally had larger fingers than the female which hampered typing on the touch screen keyboard. some smart phones which have both a real keyboard and a touch screen may improve on this situation."
"another kind of modulator is the delay-based one [cit], its operation relies on a time delay t d in the switching loop that generates phase of:"
"the amplifier's output stage is a h-bridge made of pmos transistors for the upper side, and nmos transistors on the low side. their r dson are about hundred milliohms each. a lower r dson would require larger output transistors so the efficiency would be higher but also the ic area use and idle current consumption. a 0.25µm process with thick oxide transistors is used to keep the circuit compatible with mobile phone batteries (2.3-4.8v) and 5v applications like laptops."
"the integrator amplifier is a two-stage class-a ota. r fba, r fbb and c int define the integrator's 0db point f 0 . r in, r fba and r fbb also define much of the noise performance of the circuit so their value should not be too high. however, a too low value would result in high feedback currents and high integrator ota consumption. equation 6 shows that the gain at f sw is constant and shared between the integrator c(f) and the comparator k(f). if the integrator has a high f 0 its output swing ve will be large and the comparator will have a low gain k(f), reciprocally if f 0 is low then ve will be small and k(f) will be large. this means that the choice of f 0 has no impact on f sw and is a tradeoff between large capacitors and noise sensitivity on one side and ota slew rate and excursion requirements on the other side."
"at the switching frequency, the loop gain is always unity. the oscillation criterion is then only tailored by the phase condition: the placement of poles and zeros in the transfer function. the system presented in this paper contains two poles and the delays in the loop (mostly the comparator propagation delay) provide the excess phase to reach 180°."
"standalone mobile class-d amplifiers have a linear preamplifier in front of the switching stage [cit] . it is a differential amplifier used to reject the input common mode, to provide the gain and to set the input impedance. due to mismatch in the feedback resistors, this stage limits the psrr to around 60-70db. fig. 5 shows the signal chain as realized."
"the most common class-d modulation scheme is carrierbased pwm [cit] . in order to reduce the effects of nonlinearities and to improve the power supply rejection ratio (psrr), a feedback loop is added around the amplifier. while being easy to implement and having a constant switching frequency, its drawback is a limited loop bandwidth hence a lower output error correction and a higher thd."
"the thd+noise vs. output power is plotted in fig. 6 in mono operation, compared with a carrier-based pwm configuration (all other building blocks are the same). fig. 7 compares stereo and mono operation. in stereo the performance is reduced at high output levels, a parasitic coupling between the channels is likely to be the cause of this problem, probably through the substrate, and the power supply bond wires of the ceramic package used for prototyping. table 2 summarizes the other characteristics and performance. a layout view of the ic is shown in fig. 9 . the study of different modulation schemes showed that the phase-shift self-oscillation one is an interesting option for battery-operated systems. improving the thd figures was the main target and has been achieved, by almost a decade compared to carrier-based pwm. despite the increased thd in stereo mode, the theoretical and practical results are encouraging for the use of new modulation schemes in mobile class-d amplifiers."
"the switching frequency variation is an important concern for mobile phone applications, and has led the authors to the choice of the phase-shift self-oscillating modulation. its detailed theory of operation will be depicted in the next section."
"it should be noted that self-oscillating modulation leads to a non-constant switching frequency, depending on the modulation index m (defined between +1 and -1 and related to the duty-cycle d):"
"battery-powered applications bring severe challenges on current consumption. this explains the wide use of switching (class-d) amplifiers for the audio amplification part. class-d modulation has several advantages compared to class-ab, especially the lower idle current and the much higher efficiency, both leading to an increased playback time. one problem with class-d amplifiers, though, is their inherent non-linearity."
"self-oscillating modulators have an increased bandwidth compared to pwm ones [cit], leading to a reduced output error thus a lower thd. also, there is no need for a triangle wave or sawtooth oscillator anymore so its current consumption can be spared. this paper will show the analysis and realization of a selfoscillating amplifier. section ii will present the different pwm modulation schemes, carrier-based and self-oscillating, and compare their characteristics. the theory of phase-shift self-oscillating modulation will be explained in section iii, as well as the specificities for a mobile stereo amplifier. the realization of the integrated circuit will be shown in section iv with a focus on the modulator stage and the differences with a carrier-based pwm architecture. section v will present the measured performance and, finally, a conclusion will be drawn."
"no theoretical work so far f f sw table 1 lists the characteristics of the different modulation schemes. it shows that the hysteresis-based topology has the highest theoretical gain, but also the highest variation in switching frequency. the latter is a problem when the amplifier is often used at its full excursion (as it is for mobile applications) since the switching frequency can fall down into the audio band. also, any delay in the loop (comparator, output stage…) has an impact on the loop gain that gets reduced to a first order function [cit] ."
"for mobile applications, a well-spread technique is to use a 3-levels modulation (nbdd) instead of the normal 2-levels one (nadd) as in fig. 4 [1, 2, 3, 8,9 ]. this reduces the output high frequency content and permits the use of a more compact output filter. using a h-bridge output controlled by two modulators, receiving input signals in opposite phase will generate a 3-levels output. because of components mismatch, the two paths will not work at the exact same frequency and phase and the output hf spectrum will not be optimally reduced. previous work [cit] showed a coupling scheme that provides satisfactory results and that is used here."
"here the loop is made of a combination of gains, poles and zeros defining a frequency where the system turns into an oscillator when the loop gain is unity and the phase is 180° (barkhausen's criterion). an additional low-frequency input is used to modulate the duty-cycle, image of the audio signal."
"the distribution in (5) is defined as a laplacian scale mixture. note that, for most choices of ( ), we do not have an analytical expression for ( )."
wherẽis an exemplar patch located at the th position and denotes the th patch in the similar patch group. the objective function (17) is then modified to
"we have proposed an effective structured amp algorithm for cs image reconstruction. our work has the following contributions. first, guided by structure sparsity theories, we introduce the laplacian scale mixture distribution to model the nonlocal sparsity for higher-order sparse representation of natural images, and then it is used as a prior constraint for cs problem. by taking the scale parameter as a random variable, it makes the practical representation much more feasible for achieving a better spatial adaptation. second, in order to maintain differences between packed patches for irregular structures, we combine local sparsity and global sparsity to soften and complement the nonlocal self-similarity structure assumption. it can substantially enhance the details of image. afterward, an effective algorithm based on the em and amp frame is proposed in this paper to solve this model. the derived inference procedures are efficient to estimate both the sparse coefficients and the scale parameter. after learning the prior parameters, the amp algorithm is utilized to solve the singular value minimization problem for achieving the accurate image reconstruction. finally, our simulations and experiments on a variety of natural images demonstrate the superiority of the proposed algorithm to the original amp algorithm and several tree-based and nonlocal sparsity-based algorithms or solvers in cs image recovery."
"image nonlocal self-similarity has been widely adopted in patch based cs image reconstruction methods. despite the great success, most of the existing works exploit the nonlocal sparsity from the degraded image, which may cause the mismatching issue in the block matching stage. moreover, they treat irregular and regular structures equally, resulting in an oversmoothed outcome. in fact, unlike large-scale edges, the fine-scale textures have much higher randomness in local structure and they are hard to characterize by using a local or nonlocal model. in this paper, we soften and complement the nonlocal sparsity for irregular structures by combining with local and global priors and propose a side informationaided lsm prior model for achieving detail-preserving and content-aware cs image reconstruction."
"where is the total number of similar patch groups; ‖ ‖ * is the nuclear norm of, taking a sum value of its singular values, namely,"
"in this section, we formulate the side information-aided lsm prior model and apply the map theory to estimate the original signal. as discussed earlier, the global information of image refers to a prediction that is actually a reference image. we use this reference image denoted by as side information which is supposed to be known in each iteration to assist the reconstruction."
"despite the steady progress in nonlocal methods, they still tend to smooth the detailed image textures, degrading the image visual quality, for the reason that the lack of selfrepetitive structures and corruption for data is unavoidable."
"finally, we have the proposed objective function for cs recovery. these four terms are the data fidelity term, side information-aided term, the nonlocal regularization, and the local regularization, respectively."
"in order to preserve fine details, we distinguish irregular structures from regular structures through the similarity of packed patches. the local regularization and side information-aided term are only utilized for irregular structures. they are adopted to maintain differences among packed patches. we use the normalized mean squared error (nmse) as the similarity measure:"
"we conduct experiments on 22 natural images, as shown in figure 4 . besides the nl-lsm-amp algorithm, we compare the si-lsm-amp algorithm with image reconstruction algorithms, including the original amp method [cit], two tree-based algorithms: watmri [cit] and turbo-amp [cit], and two nonlocal sparsity algorithms: nlr-cs [cit] and bm3d-amp [cit] . for fair comparisons, all codes are downloaded from the authors' websites. all algorithms terminate after 50 iterations, except 20 iterations for turbo-amp and 260 iterations for nlr-cs. for the watmri algorithm, in order to achieve the best result, the regularization parameters are tuned to 0.8 and 0.35. the original amp uses daubechies wavelet to decompose the images. then the wavelet denoising is applied with a threshold 0.8 . for the rest of algorithms, default settings in their codes are adopted. all experiments are on a desktop with 3.80 ghz amd a10-5800k cpu. [cit] ."
"to deal with this issue, local and global priors are designed to soften and complement the nonlocal sparsity for irregular structures so as to preserve image details. more specifically, the nonlocal sparsity is only imposed on a set of patches with limited influence from neighboring pixels, while the global prior refers to a prediction used as a reference which integrates the outcomes at the independent processing stage and can maintain the entire consistency of image; moreover, a compensation-based constraint term of local patch is utilized to enforce a small (i.e., sparse) prediction error. both local sparsity and nonlocal sparsity are represented by laplacian scale mixture (lsm) [cit] models, which are adopted to force coefficients, that is, singular values of local patches and similar packed patches, to be sparse. each coefficient is modeled as a laplacian distribution with a variable scale parameter, resulting in weighted singular value minimization problems, where weights are adaptively assigned according to the signal-to-noise ratio. on the other hand, the reference image can be used as side information. finally, we obtain a side informationaided lsm prior model for cs image reconstruction. to solve this model, the expectation-maximization (em) [cit] method is adopted, turning the cs recovery problem into a prior parameter estimation problem and a singular value minimization problem. in particular, owing to its promising performance and efficiency, we are motivated to apply the approximate message passing (amp) algorithm [cit], which is an iterative algorithm that can be used in signal and image reconstruction by performing denoising at each iteration, to solve the latter. experimental results on natural images show that our approach can achieve more accurate reconstruction than other competing approaches."
"noise environment. in this subsection, we conduct similar experiments with noisy cs measurements subject to strong measurement noise to demonstrate the robustness of the proposed si-lsm-amp to noise. the standard derivation of additive gaussian noise is 15. the subjective quality comparison results on baboon image and boat image are shown in figures 11 and 12, respectively. one can observe that (1) the quality of all reconstructed images degrades seriously as measurement noise increases; (2) compared to the reconstructed images in the presence of lower-power measurement noise, the ones in higher-power noise environment tend to be more noisy (e.g., amp, turbo-amp, watmri, and nlr-cs) or smooth (e.g., bm3d-amp and nl-lsm-amp); however, the si-lsm-amp algorithm can better remove the artifacts and preserve important image structures more effectively even when the measurement noise is high. the corresponding psnr and ssim results of these algorithms are also provided, from which we can see that the proposed algorithm achieves the highest quantitative results. this indicates that the si-lsm-amp algorithm is shown to be more robust to noise. the corresponding cpu time versus psnr curves and iteration number versus psnr curves are given in figures 13 and 14 . from these figures, we can see that all algorithms converge to reconstructed results more quickly in higherpower noise environment. among them, the psnr results of the proposed si-lsm-amp algorithm are higher than all other competing methods."
"in this section, we simultaneously learn the hidden parameters and do inference. to accomplish this task, we embed the amp algorithm within an em framework."
"third, for irregular structures, the packed patches are less similar. we wish to preserve the relative difference among these patches. supposing that the reference image u is available, we wish to get a solution that has a small euclidean distance from u while being constrained by the self-similarity prior:"
"performing coordinate descent in the auxiliary function (θ, ) leads to the following updates that are usually called the e step and the m step:"
"compressive sensing (cs) [cit] allows us to reconstruct highdimensional data with only a small number of random samples or measurements, if the original signal can be sparsely represented by some given appropriate basis. owing to the fact that image prior knowledge plays a critical role in the performance of compressive sensing reconstruction, much efforts have been made to develop an effective regularization term or signal model to reflect the image prior knowledge. standard cs methods exploit the sparsity of signal in some domains, such as dct [cit], wavelets [cit], total variation (tv) [cit], and learned dictionary [cit] . unfortunately, these methods are less appropriate for many imaging applications. the reason for this failure is that natural images do not have an exactly sparse representation in any above basis. these models favor piecewise constant image structures and hence tend to smooth much the image details."
"we could find that, compared to other nonlocal methods, the si-lsm-amp algorithm has great superiority in higher-power noise environment. these results are reasonable because the mismatching issue is worse when the measurement noise is higher, leading to the difficulty in finding similar patches. however, other nonlocal algorithms depend on self-similarity structure only, which makes them hard to perform well. in contrast, besides the self-similarity structure, the si-lsm-amp algorithm also uses the local and global sparse prior of natural images and thus has a soft assumption of the self-similarity structure. as a result, one advantage of the si-lsm-amp algorithm is that it is more appropriate for compressive imaging under severe environment, where compressive samples are subject to strong measurement noise."
"noise environment. figures 7 and 8 show the visual comparisons of the reconstructed results on baboon and boat images with 20% sampling by different methods with measurement noise with standard deviation 5, while the corresponding iterative curves are given in figures 9 and 10, respectively. from figures 7 and 8, we can clearly see that four nonlocal sparsitybased methods are still better than others. among them, the si-lsm-amp algorithm enjoys great advantages over the nl-lsm-amp algorithm in producing clearer image, for example, on the area of hair ( figure 7 ) and beach ( figure 8 ). it can not only perfectly reconstruct large-scale sharp edges but also well recover small-scale fine structures. the images reconstructed by the nl-lsm-amp algorithm and the bm3d-amp algorithm are too smooth. these two methods all have a strong assumption of the nonlocal self-similarity structure. however, many images with irregular structures do not strictly follow this assumption. we could find that the si-lsm-amp algorithm has great superiority on the images with irregular structures, because we combine local and global sparsity, which soften and complement the nonlocal selfsimilarity structure assumption for irregular structures. we also compute the psnr as well as the structural similarity index (ssim), which better reflects the visual quality of the images. the si-lsm-amp algorithm achieves the highest quantitative results. the superiority of the proposed si-lsm-amp in visual quality could be demonstrated by these results."
"the cpu time and psnr are traced in each iteration for each of the methods. figures 9 and 10 present the cpu time versus psnr curves and iteration number versus psnr curves, respectively. we can see that the si-lsm-amp algorithm achieves higher psnr results after about 30 iterations in figures 9(a) and 10(a) and after about 100 seconds in figures 9(b) and 10(b) . note that, considering the fact that it is hard to distinguish irregular structures from in figures 9 and 10 . these curves demonstrate that the si-lsm-amp algorithm can converge to a good reconstructed result in a reasonable amount of time."
"can be recovered by aggregating all reconstructed patches. because is the singular value of the prediction error, the reconstructed image solved by this subproblem is the prediction error in fact. thus, we add v to obtain the final image"
"we generate the cs measurements by randomly sampling the fourier transform coefficients of test images; that is, is partial fourier transform with rows and columns. thus, the sampling ratio is / . we follow the sampling strategy of previous works ( [cit] ), which randomly choose more fourier coefficients from low frequency and less on high frequency and set the sampling ratio near to 0.2, as cs imaging is always interested in low sampling ratio cases. all measurements are mixed with gaussian white noise with standard deviations 5 and 15, representing the environments with low noise and high noise, respectively. peak signal-tonoise ratio (psnr) is used for quantitative evaluation."
"phd students' perspectives about how to reduce or solve the problems specific in formation acquisition; \"i would like i nform ation that i s not just i nfor mation, but information relevant for me.\""
"the contrast of the phd students' reflections regarding the video resources was an interesting issue [cit] to investigate why this contrast existed and to find out more details about what the motivations were and the thoughts behind these reflections. [cit] discusses different strategies, when learning matters. [cit] discussed that positive interactions may influence users' attitudes and hence it consequently leads to enhance users' achievements and learning. however, as discussed above, this variation of opinions may be connected to the learners' research areas, the different types of video resources, how the learners use the video resources, and what types of video resources they mainly use in their education. even though the video-based items may not be as high priority as most useful or quite useful information resources (required by above 80 or 90% of the phd students), they are still important since more than half of the respondents believed in the usefulness of these three types of video resources and the respondents reflected positively on the use of video resources. moreover, many respondents added comments (shown in table 3 ) to the open-ended questions regarding the use of the video resources."
"many research studies have highlighted the low completion rate and slow progress in phd education. universities strive to improve throughput and quality in their phd education programs. in this study, the perceived problems of phd education are investigated from phd students' points of view, and how an information and communication technology support system (ictss) may alleviate these problems. data were collected through an online open questionnaire sent to the phd students at the department of (the institution's name has been removed during the double-blind review) with a 59% response rate. the results revealed a number of problems in the phd education and highlighted how online technology can support phd education and facilitate interaction and communication, affect the phd students' satisfaction, and have positive impacts on phd students' stress. a system was prototyped, in order to facilitate different types of online interaction through accessing a set of online and structured resources and specific communication channels. although the number of informants was not large, the result of the study provided some rudimentary ideas that refer to interaction problems and how an online ictss may facilitate phd education by providing distance and collaborative learning, and phd students' self-managed communication."
"information acquisition about the existing information; some usef ul i nfor mation is al rea d y avai lable at the li brary specif ic presentations for research in each of the research areas, their specific practices, common methodologies, thesis str ucturing (incl ud ing monograph or collection of pa pers, a nd why) etc. however, there is a need of refer ring to them and inform phd students about t he availability of such resou rce."
"in this study, we investigate the phd students' perspectives on insufficient supervision or lack of communication with supervisors. moreover, in this study, other issues such as a lack of peer communication and a lack of structured information, which may have negative effects on the process and quality of the phd studies, are also investigated. based on the result of this study and the phd students' requirements, an ictss for phd studies was developed and tested at the department. the ictss is aimed at reducing problems and challenges in phd education to provide more control on the study process, to facilitate accessing related e-resources, to support flexible and distance learning, and to plan better and provide quicker updates to reduce the stress and enhance the control of the situation. the methodology used in this study was an online open survey, sent out to all the phd students at the department (the institution's name has been removed during the double-blind review)."
"the contribution of this study is a complement to the previous studies and reflects the importance of including phd students in different study situations and learning styles in the design of an ictss. it also raises the question of how an ictss can be useful to reduce interaction problems. a holistic view of what the main problems were and details about what resources would be useful for phd students to reduce these interaction problems were listed above. the text responses provided by the respondents were analyzed using text-mining techniques and showed that the students mostly discussed courses, communication, information, stress, and so on. these issues were covered by the resources and functionalities of the ictss to provide support, reduce the interaction problems, and facilitate phd students to achieve better educational outcomes with more control of their educational processes with less stress. the results of this study provided some rudimentary ideas that developing an ictss can facilitate better information access and communication in phd education to reduce the interaction problems. this would strengthen the ictss development process to more likely be able to offer open and flexible support and contribute to phd education with more effective study situations both on campus and off campus."
"in addition to the discussion above, regarding the use of video resources, there was an interesting contrast between the two groups of respondents, as shown in table 3 . some respondents had positive reflections about the use of relevant and concise videos. they believed that the use of video in their phd studies would be a helpful tool to save time and gain information more quickly and easily. meanwhile, there was another group of respondents with negative reflections, who believed that using the video resources would not add any value to the system and might do the opposite. acquiring the relevant information and referring to it later would be easier and more efficient by reading rather than watching videos. the respondents' views regarding the use of the videos in phd education were very different and a group of the respondents believed that video resources could be very useful, while others did not share this opinion and were not very positive toward them. however, as shown in table 2, this variation of opinions may be due to the different types of video resources: software tutorial videos, video tutorials regarding how to access scientific databases, and videos with previous phd students' presentations. it is also a personal matter which learning methods (video-or text-based resources) learners would prefer to use as learning materials. even though, these video-based items may not be as high priority as some other issues that were required by above 80 or 90% of the phd students (shown in table 2 ), these type of recourses are still important since at least half of the respondents believed in the usefulness of this type of resource."
"in order to achieve an inclusive coverage of phd student population at the department-including oncampus and off-campus students as well as part-time and full-time students-this study adopted a survey strategy [cit] . data were collected using online questionnaires in order to reduce turnaround time, and to gain easy and immediate access to informants (ibid.). as this study had both evaluative and exploratory aims, the questionnaire included closed-and open-ended questions [cit] ). the closed-ended questionnaire items, which constituted the quantitative part of the study, asked students about their perceptions of the importance of different aspects of their phd studies, and about the perceived usefulness of functions of a newly designed ictss for phd education. students rated their perceptions using a four-point ordinal scale that ranged from 1 (very useful) to 4 (not useful at all). openended questions complemented the closed-ended questions by collecting respondents' opinions and perceptions in their own words, as well as their reflections in more detail [cit] . two postgraduate students and two senior researchers tested the questionnaire before launching it. the final questionnaire consisted of 34 questions, of which five were background questions. of the rest, 22 were closed-ended questions, and seven were open-ended questions."
"approximately three out of four of the responding phd students experienced increased stress due to their study situation. concerning the usefulness of ictss, the results turned our attention in a certain direction: towards a greater transparency in phd education, including a better overview of relevant information, a tool for planning and reviewing, and in general, easier access to information and interaction. this result indicates that a certain specific information and guidance support is required to reduce the stress of phd students. based on the findings, an ictss will support phd students to have better control of the situation by finding the information through online structured resources and having better communication channels. the ictss may support interaction with peers and supervisors or arrange face-to-face communication. however, as mentioned above, an ictss is only considered as an online complementary support to the face-to-face interactions and information resources and will provide support for the phd students and cannot function as a supervision system without supervisors. hence, designing and developing an ictss is not a replacement for individual supervision meetings or face-to-face peer communications, but facilitates better control and arrangement of the different types of interaction."
"this study systematically investigated the phd students' perspectives regarding the most challenging issues that negatively impacted their phd studies. moreover, this study evaluated the phd students' perceptions of usefulness of an ictss in phd education. in order to shed some light on these issues, the following research questions were constructed:"
"in addition, in most of the institutions in sweden (the institution's name has been removed during the double-blind review), there is an individual study plan that is adjusted to the individual study situation to guide the phd process. based on the description that is approved by the national agency for higher education, an individual study plan is a document that describes and establishes an individual curriculum and schedule for each phd student. moreover, the individual study plan is a description of the phd student's commitments and departmental responsibilities. the study plan has to be updated annually by phd students and their supervisors and needs to be approved by the department board. to specify the general regulations (the institution's name has been removed during the double-blind review), the head of the department has to approve both the general and individual study plans."
"insufficient communication channels or structured information resources cause difficulties for the phd students to know what research involves [cit] . problems such as not receiving important information on time, not being aware of rules and regulations of the phd process, or not getting information about the time constraints, create confusion for phd students, and hence consequently cause disturbances in concentration for their research. this negatively influences phd students' ability to finish their phd studies on time or not at all [cit] . this case is even worse for the phd students who are rarely on campus, since the distance communication is poor and non-supportive to educate learners at higher levels."
"universities strive to improve throughput and quality in their phd education. however, different problems and influential factors besides increasing number of phd students make these efforts extra challenging."
"planning: possi bi lity of u pdating the study plan on line a nd pri nting the u pdated version (i.e., in a pdf format)"
"having a phd education that is too flexible and without milestones may reduce the achievement of the learners. hence, milestones will guide learners how to plan, how to fulfill the plans, and how to achieve smaller outcomes in order to get a vision of the phd education process and reach the ultimate goal, which is finishing their phd education and getting their phd degrees."
"a multi-disciplinary study with the similar research question would increase the generalizability of the study result and conclusion. access to the phd students (the institution's name has been removed during the double-blind review) was only possible through the university email. in some cases, the phd students might have stopped their phd education or the emails might have become obsolete. to assess the real reflections about the support system, a test group of the first pilot of ictss for phd education would add value to the development process. supervisors' perspectives, to examine the problems and solutions from another point of view, would also add value to the study."
"the use of educational technology in higher education enables new affordances for information gathering, communication, and learning. in many ways, online ict (information and communication technology) systems facilitate the acquisition of information [cit], provide support for transferring knowledge and collaborative learning [cit], and advance the different types of interaction [cit] . ict support systems (ictsss) increase learners' motivation by facilitating the interactions synchronously or asynchronously with real people and even provide opportunities to participate in real world events [cit] ."
"based on the responses to the close-ended and open-ended questions, more than 70% of the respondents requested availability of structured information resources, guidelines for essential steps, and better support for peer communications. as mentioned by the respondents, information acquisition by accessing structured e-resources, online planning and updating the phd study plan, and communicating with peers through specifically designed interaction channels could be useful for the phd students and have the potential of reducing the stress of the phd students. based on the results of the study, figure 3 reflects the phd students' perspectives on the problems and the requirements of an ictss as a solution to part of the problems. in figure 3, the three categories of resources (useful resources for almost all phd students, useful resources for most of the phd students, and additional resources) were developed based on the agreement percentages of the respondents with the usefulness of each resource and function (as shown in tables 1 and 2) . table 2 and figure 3, the following issues have been developed to illustrate how each functionality or recourse discussed above can be useful to reduce the four categories of the problems in the phd education."
this facilitates phd students to convert the information into a pdf file and use it as their latest portfolio (cv) connected to their recent competencies and publications.
"the results from statistical text mining on open questions showed that there were several words the respondents used in describing the problems they had and the solutions they proposed in order to enhance an ict-based support system. as shown in figure 2 the word cloud in figure 2 shows the most frequent terms the respondents used when discussing the problems and the solutions. despite that the highlighted words in the word cloud may not strongly backup for concrete arguments about the problems and needs, such summarization would lead to identify focusses of the responses, which may be useful in design and development of an ict support system (e.g., inform equals information which is connected to the information requirements; communication which is connected to the need for better communication channels; research which is connected to the importance of doing research and the focus on the requirements to facilitate research and the research process; stress which is an important issue in the process and supporting the phd education through the use of ictss may positively affect reducing stress; and time as a factor to successfully manage the phd education and the phd students' stress levels). in addition to figure 2, table 3"
"planning: ict enabled system ca n h el p to build a platform to provi de a proper sched ule, to be visible for all the releva nt parties, e.g. supervisors, peers, administration departments, etc."
"positive responses about the use of video resources negative responses about the use of video resources \"sounds like a good idea i strongly prefer textual descriptions to videos.\" \"videos could be supplementary for the text.\" \"in each 3 minutes of video about 500-600 words are covered.\" \"may help to save time and get the info quicker. the video has to be relevant and concise, and be interesting to watch.\" \"video based information will help the most to manage and reduce stress.\""
"peer interaction; \"more peer comm unication woul ld definitel y add some values to th e system and comm unication matter.\" \"more peer commu nication wou ld be better\"."
"the first question that may come to mind with respect to phd studies is what is a phd degree? different researchers [cit] have discussed this question. based on the discussions in different studies [cit], the academic level known as a phd (abbreviation for doctor of philosophy) may vary considerably according to the country, institution, or academic discipline. in sweden, a phd is a program that nominally comprises 240 credits including a thesis of at least 120 credits that is equivalent to four years of full-time study (högskoleverket, [cit] ) . phd students must complete a number of courses besides writing a dissertation (thesis) and defending it at a public oral examination (högskoleverket, [cit] ) . in sweden, for each subject in the phd program, there should be a general study plan that is determined by the faculty board. the general study plan shall contain a description of content and structure, and what applies for admission to a phd program."
"lack of the availability of online ictss and distance support has some negative impacts on the quality of the outcomes of the phd studies and may increase the stress 1 level for the phd students. in this study, stress refers to a lack of control of the situation, worriment, or emotional tension due to a lack of keeping track of the study process and getting nervous due to insufficient information and communication and lack of remote access to the required information and missing the important issues. there is a stress reaction when an individual experiences a difficult situation as a threat; something with which he or she cannot cope [cit] . the relationship between perceived demands and perceived control of work is critical for the reactions. for example, high demands in combination with a low level of control are associated with negative stress reactions [cit] )."
"the web-based questionnaires were sent to all 90 [cit] . the questionnaire was delivered to the phd students by email with two reminders with a time interval of one month [cit] . the reminders increased the overall response rate by about 10%, with the final response rate being 59%, which was satisfactory according to what the methodology literature reports to be within standard deviation for electronic surveys [cit] . however, the response rate exposed the results of this study to non-response bias through refusal [cit] . the participants received no benefits from participation. informed consent was ensured at the beginning of the questionnaire as well as in the content of each email. the data collection was completely anonymous and the participants were informed about the anonymity. no private information was used or connected to any response and all the background questions were optional."
"communication channels and information resources play important roles in education and collaborative learning by facilitating interactions. [cit] there are three types of interactions for distance learning: (a) between the learner and the instructional and informative contents (learner-content interaction), (b) between the learners and the instructors or supervisors (learner-supervisor interaction), and (c) among learners (peer interaction). since learners in the phd education face similar challenges, opportunities, and threats as learners in distance education, an ictss facilitates different types of interaction in the phd education. in this study, the learner refers to the phd student and the instructor refers to the phd student's supervisor or co-supervisors."
"information and guidance acquisition; accordi ng to a respondent, based on a regular survey, t he ethical trai ni ng for phd stud ents in sweden is quite i nsuff icient a nd the ph d students need more information and resou rces in this regard, besides other releva nt issues. \"ethica l cond uct woul d probably have a strong influence on h ow resea rch is conducted, an d thereby its outcome, if it wou ld be given the space to do so.\" information acquisition; pu blication for ums and their rankings a re already available onl in e; but making the relevant i nfor mation to the ph d studi es explicit and pu blicly avai lable a nd accessibl e for the phd students is useful."
"since one department ca nnot deliver a ll the relevant and req uired cou rses for the phd st udents, information should be given about whi ch other universities (which cou ntri es) provide such cou rses and ph d stu dents are allowed to ta ke them. moreover, cla rify what a re the conditions and specifi cations."
"as the results of this study show, most of the respondents believed that designing an ictss for phd education could facilitate easier and more convenient interactions for all three types of interactions [cit] . this means that the use of ictss in the phd education may facilitate flexible and open learning and communication, which learners believe would be useful both on-and off-campus."
"\"… it is difficult to acquire information from video tutorials (i'm perhaps too old).\" \"is not of interest.\" \"waste of resources and time to develop this.\" \"real time interaction during classes is a key value.\" \"why would i want to spend that little time i have available to watch 30 minutes of video, when i could read the same material in 5 minutes and then easily refer back to it later.\" \"video learning is for lazy students who don't want to wake up for lectures, and is a poor learning tool.\""
"a similar case concerns the use of ict for phd studies to reduce the stress level of the students. based on the discussion above, stress is defined as a lack of control of a situation or tension resulting from adverse or very demanding circumstances. the result of this study showed that 76% of the respondents felt extra stress from their phd studies, while the rest did not share this feeling. there was no general rule to show whether a support system would help reduce stress or not. \"people react differently to the same stimuli\" as mentioned by one of the respondents. there were different perspectives, both positive and negative reflections on the effect of using ictss for phd education. more than half of the respondents indicated that an ictss would help them to manage tasks more efficiently, and it might hence reduce their stress."
"based on the description above and the closed questions of the study about stress, 76% of the respondents felt extra stress from their phd studies. the results showed that 50% of the respondents indicated that the use of ictss would be useful to reduce their stress (by having better control of the situation, keeping track of the study process, and getting information when needed), 30% were neutral, and around 20% did not find the system useful to reduce their stress. table 4 shows both positive and negative responses to the open-ended questions regarding the usefulness of the ictss for reducing the stress in their phd education. table 4 phd students' perceptions about the usefulness of the ictss for reducing the stress in their phd"
"some prefer to learn through text, while others learn more easily through pictures and visualization, communication, videos, or combinations. the idea here is to reflect on the diversity of people in a group, such as phd students. it may also depend on other factors, such as the quality of the produced videos, phd students' study discipline, and learning and teaching methods. [cit], although phd students' training and learning experiences may be different from one to another depending on different factors, a substantial part of its outcomes is constant. this means that there is not a uniform way of learning, but video based, text-based and, graph-based resources may be useful in different circumstances and for different purposes."
"from the results, one main concern of the phd students was to get online access to the critical information, such as information about educational requirements and goals, tasks and responsibilities, the study process and the courses, the administrative information, traveling, and budgeting. communication online was also shown to be important, especially for enhancing peer interaction but also for communicating with the supervisors. most of the respondents rated the usefulness of many online information resources and functionalities of the ictss as very or rather high. more than 90% of the respondents indicated the usefulness of the following: travel information, plan and maintain phd courses and required course credits, updating and accessing the individual study plan online, structured and online information from the doctoral handbook, managing the conference expenses, and access to the list of the well-known journals and the publication ranking. more than 80% of the respondents indicated the usefulness of online access to the following: the annual phd budget, description about the time plan, plan and maintain a set of milestones, a phd forum for peer communications, and even supporting the learnersupervisor communication and conversation. the rest of the functionalities were indicated as useful by more than 50% but by less than 80% of the respondents."
1. what are the most important problems in phd education from phd students' perspectives? 2. how do phd students think that an ictss will facilitate reducing interaction problems and stress in a phd education?
"in addition, a shortage of structured e-resources and concrete guidelines is also another part of the problem in higher education as well as a problem in education at the bachelor's and master's levels [cit] . there is no online support for phd education, from which phd students can remotely access and learn from the online and structured resources. however, phd education is about distance learning and independent research. there is a shortage of structured information resources and communication channels to facilitate interactions [cit] both on campus and especially for distance education and communication. in many institutions, there are not enough available information resources aligned with the goals of phd education to guide the phd students [cit] . peer communication is also helpful for supporting information acquisition and transferring information (reference deleted during double-blind review)."
"based on the respondents' reflections shown in table 4, there were many resources and functions that might enhance the control of the situation and facilitate better study management. acquiring information might also positively affect the management and reduction of stress. however, a small group of respondents did not find the direct connection of using a support system to reduce stress."
"be able to plan and maintain a set of milestones: to see the current position in the entire phd process, the past and future tasks/steps."
"information and guidance acquisition; wishing for a system to gui de phd stud ents th rough t he process of fi lli ng i n the forms such as travel documents and expenses, and hel p to u pdate the study plan, getti ng information a bout the status of the ann ua l phd budget, etc."
"planning: a course gu ideline would be helpful i n the way to design a check box for each course, possi ble to be checked by both phd students and thei r su pervi sors: \"i or my supervisor cou ld check a box: \"course a done\" \"obligatory course x lacking\" \"n individ ual credits passed \" a nd we cou ld press \"print\" and a n u pdated version could be signed\"."
"the ictss may even be used to arrange face-to-face meetings and plan for physical interaction. however, designing and developing an ictss and providing online information resources and communication channels is not a replacement for individual supervision meetings or interpersonal communications. it is in order to facilitate the interaction on distance and give the phd students the possibility of having access to the information and online communication channels off-campus or from a distance."
"updat·ed information acquisition; phd students req uire new i nformation: \"... it would be easier to see new info and to navigate i n what is important to me\". the respondent a lso menti ons that \"...some information are either cu rrently not available or we are not aware a bout, but would disti nctl y be im portant.\""
"insufficient supervision and lack of communication with supervisor(s) 51% table 2 covers the close-ended questions and illustrates to what extent phd students may find the following resources and functionalities useful in their phd studies. the percentages represent the agreement of the respondents with the usefulness of the following resources and functionalities. since very useful and rather useful represent positive responses, they were combined to give the percentages of agreement on each question in table 2 . more details about the answers of the phd students to the open ended questions are presented in the appendix."
"the questionnaire was sent out to 90 phd students and 59% of them responded. one quarter (25%) of the 53 respondents were female. regarding age, 83% of the respondents were between 26 and 45 years old, and the rest were more than 45 years old. of the respondents, 40% had been phd students for two or less than two years, 35% between three and four years, and the rest (25%) had been phd students for five years or more. three in five respondents (61.5%) were fulltime phd students and the rest were halftime or part time phd students. the respondents came from 11 subject fields at the department (the institution's name has been removed during the double-blind review): business process management and enterprise modeling, cyber systems security, data and text mining, digital games, e-government and e-democracy, ict for development, interactive design, it management, language technology, risk and decision analysis, and technology enhanced learning."
"portfolio (or cv producing) function to make a list of your skills and expertise, and show a list of your publications. this is aimed to facilitate phd students to convert the information into a pdf file and use it as their latest cvs."
"as mentioned below, most of these functionalities indirectly support learners by providing access to the information through resources and communication channels, which also saves learners time and reduces their stress. it also enables learners to use their supervision time to develop their research rather than focusing on getting general information from their supervisors. the following list shows how the functionalities or recourses can be useful to reduce the interaction problems in phd education."
"this study investigated the most important interaction problems in phd education from phd students' point of view and how phd students think that an ictss will facilitate reducing interaction problems and stress in a phd degree. most previous studies focused on problems related to supervision, however this study found that there were other important issues besides supervision problems. the results revealed a number of problems in phd education, the most prevalent being insufficient online information (72% of respondents), insufficient online peer communication (70% of respondents), lack of appropriate timelines (58% of respondents), and insufficient supervision (51% of respondents)."
"such that if only one player k uses a different strategy from its corresponding π k, it does not observe a utility improvement greater than"
"learning techniques such as the brd are highly constrained for real system implementations since they require the network to be static during the whole learning processes. on the contrary, all the other techniques allow the dynamics of the network to be captured by their statistics as long as they are stationary. this is basically because, contrary to brd, all the other techniques determine whether to play or not a particular action based on the expected utility rather than instantaneous utility."
"there are two main trilateration algorithms investigated for an unbounded n number of beacons at positions (xi, yi) and distances di. both algorithms are described in the following two subsections [cit] ."
"in case that the location (i, j) is not occupied by a source at time t+1, the source contribution term f t+1 i,j is equal to 0. the constantc 2 depends on the resolution of the space-time grid, and for unit stepsizes cfl condition [cit] suggests it is less or equal to 1/ √ 2 in 2d domains, and 1/ √ 3 in 3d domains, to preserve the stability of the scheme."
", if source at location r (2) in other words, this partial differential equation is homogeneous for all regions of space not being occupied by sources. at the remaining positions, it will contain a non-zero right term f ( r, t) which represents the contribution of the sound source at the position r at time t. the constant c represents the sound propagation speed in the medium."
"finally, in figure 5, we show for the 2-players 2-channel case, the trajectories of the algorithm during the transient phase. in this realization, brd it can be observed that brd does not converge. the two transmitters repeatedly select synchronously the same channel. fp and sfp converge to the best performing ne while crl converges fast to a steady point with no game theoretical meaning. in the trajectory of juste, it is possible to see that it converges to the best performing ne, for that particular channel realization. similarly, rm also converge very fast to the best ne."
"regarding the learning dynamics, we have presented the best response dynamics (brd), fictitious play (fp), smooth fictitious play (sfp), regret matching (rm), reinforcement learning (rl) and joint utility and strategy estimation based reinforcement learning (juste-rl). we have identified the pertinence of these algorithms for wireless communications in terms of system constraints (continuous/discrete actions, required information, synchronization, signalling, etc.) and the performance criteria (utility achieved at the steady state, convergence speed, etc.). as further work in this direction, we point out that existing results regarding the analysis of equilibrium in wireless networks strongly depend on the topology of the network. indeed, a general framework for the analysis of equilibria and learning dynamics adapted to time-varying topology networks is still an open problem. moreover, we must consider that some equilibrium notions, e.g., ne and -ne, might be inefficient from a global point of view. thus, learning algorithms to achieve pareto optimal solutions with partial information is a further direction of research."
"unfortunately, this apparently requires the estimation of the entire pressure field, which is an ill-posed problem, even in the noiseless case (since (1) has infinitely many solutions). generally, to regularize ill-posed problems, one seeks the solutions which satisfy a certain data model. this is often done by encouraging solutions that embed some form of sparsity. indeed, there is knowledge about the signal which can be exploited. it is known that the sound pressure obeys the acoustic wave equation:"
"at each stage, all players (simultaneously or sequentially, as in the brd) choose their current action by optimizing their expected utility with respect to the beliefs on all the other players, i.e.,"
"in this article, we first present an overview of various equilibrium concepts. later, we introduce a set of learning algorithms particularly relevant to achieving equilibrium in wireless networks. for each algorithm, we discuss the required information that each cr must possess at each iteration and the convergence properties."
"therefore, there is not a priori commitment. it follows, in particular, that every ce is a cce [cit] . now, if the players choose their strategy following independent individual probability distributions"
"the human body wave power absorbiton was also taken into account -rssi readings were collected for line-of-sight (los) facing towards the beacon and non-los directed back to the beacon. the measurements were collected on a 5m wide hallway (presented in fig. 3 ) with walls made of two materials: from one side glass (windows) and another side, reinforced concrete. all data (rssi(p)) in the position were aggregated into a single rssim(p) value by computing median:"
"another important data processing has been applied, that is, a filter for input data. the survey indicated whether the processing was required and what kind of processing was preferred. model (1) for distance estimation was used. the following methods of filtering data (all collected samples) in order to obtain a single rssi(b, p) reference value for beacon b in a specific point p were applied:"
"the brd can be used for both continuous and discrete action sets, whereas in their standard versions fp, sfp, rm, crl, and juste-rl are designed for discrete action sets. for instance, action sets are discrete in problems where a channel, constellation size or discrete power levels must be selected, whereas continuous sets are more common in power allocation problems [cit] ."
"rejecting 50% values from histogram that are not close to average reduces standard deviation and final distance estimation error. in case of median value distance estimation for the assumed model, it is less accurate because of the sorting phase while significant values can be shifted from the center."
"(24) 5. the average value after deleting 50% samples of the input data at a specific point. the subset contains acquired rssi with the smallest error with respect to the median of all data. (in short: avg. of 50% near median); methodology: the same as in 4), but rssiref is represented by the median from rssi (as in method 2) values rather than the average. the aforementioned techniques allow for obtaining a single rssi value (reference value) at a specific distance from the beacon. the results of distance estimation using filtering data are shown in figures 15 and 16 and also in table i . the candles plots represent the variation of error changes over the distance. the average value from the subset of 50% values that are near average value results in less standard deviation and less error dynamics. the average distance estimation error is 1.09m for distance 0-9m and equals to 4.05m for higher distance from the source; overall, the error is equal to 1.75m for 1-20m. for both examples (fig. 15, 16), the model is accurate enough in the range 0-9 m, while over 10 m, dynamics of estimation error increases. it means the model proposed can be investigated and applied in the near range distance."
"3) eddytsone tlm (also known as telemetry) allows devices to send small pieces of data such as temperature, battery voltage and so on. eddystone extends the ibeacon standard, enabling users to send small pieces of data. the beacon can transmit different types of frames over time."
"june 15, 2011 draft 6 rm relies on the assumptions that at every stage n, player k is able to both evaluate its own utility,"
the relationship between distance from the beacon and rssi for both los and non-los was calculated using pearson correlation coefficient (4) which gave a value of 90.1%.
"there are several path-loss models available to measure the distance from wave emitter by measuring signal strength [cit] . typically, log-distance path loss model was investigated:"
"conversely, the latter calculates it as the time-average of the instantaneous observations of the achieved utility. this requires a large number of observations to obtain a reliable approximation to the expected utility. we do not state any particular comment on the speed of convergence of brd and rl since, in the former the scenario is considered fixed and the latter, it does not necessarily converge to an equilibrium strategy. however, conclusions for a particular case are stated in the following section."
"there are two main problems with this approach, and they are both related to the dictionary ψ. firstly, tailored green's functions often need to be computed numerically, since the analytical solutions exist only for some (simple) spatial geometries. the second issue is practical: the matrix ψ is usually dense and its size grows polynomially with dimensions, making the optimization problem quickly intractable in storage and computational cost."
"2) knowledge and calculation capabilities: learning algorithms such as brd, fp, sfp and rm involve an optimization problem at each iteration [cit], that is, either the maximization of the (expected or instantaneous) utility or minimization of the regret. therefore, generally, highly demanding calculation capabilities are required to implement them. more importantly, solving such optimization requires the knowledge of a closed-form expression of the utility function. this implies that each player knows the structure of the game, i.e., set of players, action sets, current strategies, channel realizations, etc. in this respect, rl and juste-rl algorithms are more attractive since only algebraic operations are required to update the strategies. in terms of knowledge, in both rl and juste-rl, players are only required to know, at each iteration, the action they actually played and the corresponding achieved utility. indeed, one can say that players are not even aware of the presence of other players."
"power-based positioning techniques rely on the signal attenuation property of the radio wave propagation to estimate distance from wave emitter [cit] . there are two common approaches to determine an object's position. one is creating the radiomap of a room in offline (static) phase, this means there are many rssi once-collected samples in many points stored in the database and in the online phase, when an object collects samples of rssi to determine its position, some nearest-neighbor algorithms determine the object's position comparing this to samples from the database [cit] . another one employs surveying to build path-loss signal model that estimates the distance from emitter based on signal-strength. by knowing three or more distances, the trilateration algorithms can be applied in order to obtain the final position of an object."
"conversely, juste-rl exhibits a lower performance when the number of possible actions increases. this is basically because, in juste, all players play all their actions with non-zero probability in order to improve their utility estimation. thus, this immediately implies that increasing the number of actions, increases the time that players are playing actions different from the optimal actions."
"in positioning context, the better results can be achieved by correlating rssi's with accelerometer, gyroscope and other sensors. another solution is to use more sophisticated metrics than euclidean, such as: excluding zones where an object cannot move, restricting situations when objects moved back while accelerometer measurements does not notice this fact. such problem must be under investigation in the near future."
"since ω is a square invertible matrix, the analysis and the synthesis problems are formally equivalent and ψ could theoretically be computed by taking its inverse. however, the dictionary ψ is not sparse [cit] and the analogous implementation of the sparse synthesis 1 admm optimization would require multiplying ψ and ψ t in thep k -update step, which would effectively scale as o (ijt ) 2 ."
"many indoor positioning techniques [cit] have been proposed for mobile devices and for more specified, dedicated hardware. some of them are based on custom hardware utilizing bluetooth classic [cit], especially in scanning phase to obtain rssi (radio signal strength indicator) or link quality [cit] . unfortunately, bluetooth classic scanning phase is energy-consuming, because obtaining link quality metrics requires devices connection. the bluetooth 4.0 (also known as low energy or ble-bluetooth low energy) [cit] and today, it is widely supported by smartphone vendors. this has opened a new opportunity for identification of devices and obtaining rssi in the lower energy cost way. ble beacon devices broadcasts short packets in specific interval, which gives a new possibility to use beacon standard in a wide range for a population of people holding these devices in their pockets. [cit], apple released ibeacon standard [cit] as a proximity location method, which utilizes bluetooth 4.0 generic attribute (gatt) profile and standardized frame data contents. the ibeacon standard by apple enabled mobile devices to recognize beacon tags by receiving bluetooth signals from them. in addition, the eddystone standard was proposed by google in order to extend the ibeacon, which are both widely supported by mobile device vendors in hardware and in software by implementing ibeacon and eddystone into operating systems running on these devices. then, many vendors started producing low-cost hardware broadcasting beacon frames, which h can be discovered by mobile devices. this standard is natively supported on apple ios mobile devices, also, it is easy to carry out in every platform which delivers access to bluetooth host controller interface (hci), such as on devices running on android or windows phone. the ibeacon frame is sent over the air with an interval of about 350ms. its structure is presented in figures 1 and 2 . in figure 1, the bluetooth device is sending an advertisement type packet without establishing a connection, that is, a device invites to connect by sending some data but rejects incoming connections. [cit], google released extended specification of the standard called eddystone. the standard is very similar to the ibeacon in the way it works, but"
"where x denotes rssim(p) value and y denotes the distance at point p from the beacon and: evidently, the distance from the source has the highest impact on signal intensity. moreover, there was a weak correlation between standard deviation and distance, even when the input samples was reduced by filtering 20% extreme values with respect to median (fig. 5 ). the signal quality indicator cannot rely on this metric."
"regarding the conditions for convergence, only sufficient conditions are available. as shown in table i, the considered algorithms typically converge in certain classes of games to know when to play and when to observe the actions of the others. in wireless communications, this requirement implies the existence of a given protocol for signalling messages exchange. conversely, when players require only an observation of their individual utility, such a synchronization between all the players becomes irrelevant. here, only a feedback message from the receiver to the corresponding transmitters per learning iteration is sufficient."
"damian e. grzechca, piotr pelczar, and lukas chruszczyk t the frame content is different. eddystone (so far) assumes three types of data frame [cit] : 1) eddystone-uid (user identifier) which is the same as beacon frame, the data packet is filled by 16 byte beacon id (10 bytes namespace id and 6 byte instance id), 2) eddytone-url filled by encoding and shortened url that can be opened by mobile devices without accessing the cloud to relate broadcasted id by beacon to assigned url. this url is directly from the source iot (internet of things) or physical web url. mobile customers can open real online application found under this url or only run some designed custom uri protocol in the operating system."
"in figure 1, we plot the average spectral efficiency of the system as a function of the snr, in the case where only 2 orthogonal channels are available. here, all the algorithms iterate the same number of times (40 iterations). in figure 1, it is interesting to note how algorithms such as fp, sfp and rm converge always very close to the best ne, i.e, the ne associated with the highest network spectral"
"the trilateration algorithms (3.a. and 3.b.described in section 3) position estimation and validation of the tuned model of the radiomap was created for the exemplary room. one hundred rssi samples from 5 different beacons in each radiomap point were acquired. the radiomap was placed on the 8m  6m mesh with a gap of 1m ( fig. 9 ). a basic statistic like median for all accessible devices (anchors) in each point was computed. a beacon (an anchor) was mounted under the ceiling (on the top part of the wall to minimize furniture signal absorption) at a height of 2m from the floor, but measurements were collected at the height of 1m, because it seems to be a natural position for smartphones while being used by humans. exemplary radiomap for one beacon device is presented in fig. 10 ."
"in particular, note that rl and juste-rl are less performing, but at the same time, less demanding in terms of information. interestingly, the brd demands the same information assumptions than fp, sfp and rm. however, the performance is even worse that rl. this is due to the fact that brd does not necessarily converge to a ne in this particular game. in figure 2, we plot the network spectral efficiency of the algorithms as a function of the number of iterations for the case of two channels. here, rm and brd appear to be the best performing and worst performing algorithms, respectively. with respect to the brd, such a performance is due to a ping-pong effect between two particular action profiles. in detail, since players are simultaneously selecting the channels with the highest gain, it may happen that the best channel is the same for both players. thus, for instance, at odd iterations they both share the same channel and in the next one, they both select different channels. this effect will continue at the infinite preventing the algorithm to converge. in figure 3, we show how the algorithms perform when a higher number of channels is available, i.e, 4 channels. brd improves its performance, with respect to the other algorithms. this is mainly because the higher number of channel reduces the probability of the ping-pong effect described above."
"future work will be aimed towards real-world experiments and extended scenarios. one can envision cases where some physical properties are not known in advance, e.g. wave propagation speed, boundary type or shape. additionally, since a signal estimate is also produced by the approach, it may be used to perform source signal separation or to deploy virtual microphones. the cosparse regularization could be one key to solve challenging inverse problems such as these."
"we have presented a method, based on cosparse data model, for sound source localization behind the obstacle that blocks the direct propagation path. the experimental results confirmed the assumption that sparse analysis based on the physical model of the wave propagation performs well even in complicated spatial domains and long time spans, where the equivalent sparse synthesis model is intractable. furthermore, it is possible to scale the problem to three dimensions without significant impact on the accuracy."
"if the domain includes an obstacle between the microphones and the sources, as presented on figure 1, the problem is insolvable by traditional goniometric methods [cit] . indeed, most of these methods are based on the time difference of arrival (tdoa) approach. it usually involves computing the cross-correlations between the recorded signals, this work was supported in part by the european research council, please project [cit] -277906)."
"in order to create a relatively high precision path loss model, data acquisition and fundamental statistical analysis were applied. for a single position in the room, a set of 100 rssi readings per beacon (anchor) was collected. the distance between following points was set to 1m which gave a total of 21 points in a straight line. (11) where: rssi(p) -the set of rssi readings from a beacon in the point p;rssi(p, n) -the n-th reading from a beacon in the point p"
"when a steady state is achieved by one of the algorithms under consideration, such state may correspond to one of the equilibrium notions presented in sec. iii. in particular, when brd and fp converge, the strategy of the players at the steady state is a ne. in the case of the rm, it converges to an element of the set of cce. here, we highlight the fact that, even though the notion of cce relies on the idea of the recommendations studied in sec. i, it does not require the existence of recommendations to converge to an element of the set of cce. when sfp or juste-rl achieve a steady state, it corresponds to an -ne. on the contrary, in the case of rl, a steady state not necessarily corresponds to a particular notion of equilibrium."
"he customer's positioning is important in relatively wide indoor areas like museums where localization and pathway creation may be crucial. moreover, observation and analysis of the customer behavior in the hipermarket may improve different actions (discounts) addressed to a particular group of people."
"to discretize the d'alembertian operator we use the finite difference method through the leap-frog scheme [cit] . for a 2d spatial domain and unit stepsizes, we can express (6) with the following causal relation:"
"the convergence of fp is not ensured in games with cycles and its ability to explore the whole action set is highly constrained. to overcome these issues, a simple variation of the fp has been proposed under the name of smooth fictitious play (sfp). the assumptions on which sfp relies are the same as fp and actions can be updated either simultaneously or sequentially. the main difference between sfp and fp is that, at each stage n, player k does not choose a deterministic action."
"the notion of cognitive radio (cr) has gained momentum in recent years to build flexible and efficient networks. indeed, crs are nowadays widely accepted as a suitable solution to rationally exploit shared spectral resources and increase spectral efficiency. the main idea behind cr relies on the capability of a given radio device to self-configure its own communication parameters in an intelligent, autonomous and decentralized manner, as a result of its interaction with the environment. in this context, the choice of a particular communication configuration by a given cr is highly influenced by the choice of all other radio devices. within this framework, non-cooperative game theory appears as a suitable paradigm to study and analyse such scenarios. therefore, the idea of equilibrium, namely, nash equilibrium (ne), becomes particularly relevant. indeed, at the ne, the transmit configuration of each cr in the network is optimal with respect to the configuration of all its counterparts. interestingly, in some cases, an equilibrium can be reached by using particular iterative procedures similar to learning processes [cit] ."
the aforementioned approaches were used to define estimated positions at given points on the radiomap. both algorithms were compared against input data after distance estimation using the path-loss model in order to measure errortolerance.
"estimated distances were utilized as input parameters for trilateration algorithms to compute the final position. the average error of all points in a whole radiomap was measured by comparing the estimated position and real radiomap position. results are presented in table iv. second algorithm (3.b) achieved better results. in each case, the effectiveness of estimating position is strictly related to the quality of input data, the smaller the distance estimation error using filtering method, the greater the precision of position estimation achieved. average-based filtering gives surprisingly good results. it was proven that the filtering methods may give better results than median preprocessing, while the second approach to compute metric is less time and memory consuming. in addition, the error of distance estimation using the average of 50% data close to the average value which reduces the standard deviation of estimated distance significantly (table i and table iii ) have no positive effect on trilateration algorithms results. the symbols: #) denotes that the path-loss log model has been built with data processed by the indicated method; *) represents the model calculation with the use of median. most of final position estimation errors after trilateration using the second algorithm (3.b)are less than 2.5m (60.87%), what is presented in fig. 19 ."
"bluetooth low energy and ibeacon standard opened a novel way to build low-power based positioning techniques. obtained rssi can be used for the estimation of device positioning but the preprocessing and good path-loss model is required. distance estimation model gives more information than standard api's defined in ibeacon standard, which returns only proximity range name if the object is immediate, near or far away (without estimating distance as a value). ble is widely supported on devices so it can be utilized to customer's waypath or shop indoor segments tracking."
the discretization (7) yields an operator ω which is extremely sparse: each row can have at most seven non zero elements. from a computational point of view this is very favorable and the benefit can be observed in the iterative update steps of the (scaled) admm (alternating direction method of multipliers [cit] ) algorithm used for numerically solving the optimization problem (5):
we further introduce the concept of pure ne (pne). a pne is obtained by restricting the players to deterministically choose one of their actions instead of choosing it by following a probability distribution.
"the obtained results are in accordance with physics of propagation. the well-known huygens-fresnel principle suggests that there is a minimal door widthw beyond which it will be impossible to detect the sources in the other half of the room: it will always appear as if they are located at the door position. this is exactly what happens for very small values of w in our experiments. figure 6 is the precision/recall graph for the threedimensional setup. for conveniently chosen range of thresholds, it was possible to accurately localize the sources in 9 out of 10 experiments. the computational time per experiment was approximately 2 to 3 times higher than needed for the 2d experiments presented before. this experiment could not have been conducted using the equivalent synthesis approach, due to its extremely high computational and storage requirements."
"analyzing table i (and table iii ), it can be said that data preprocessing (see explanation below in table iii ) indicated lower distance estimation error: about 0.04m which is only ~2%. it is doubtful whether model calibration for specific metric is needed. the results of estimating distance for radiomap ( table iii) are better than distance estimation presented for the tuned model (table i) because almost all distances in radiomap are in the range 0-9 m, where estimation error is about 1m for tuned model (fig. 13) . the right most colomn shows average distance estimation error for (rssi0 calculated by indicated algorithm), γopt calculated by median (*) (check) and indicated method (#), respectively. the symbols: #) denotes that the path-loss log model which was built with data processed by the indicated method; *) represents the model calculation with the use of median."
"n j, a given joint probability distribution over the set a, with φ a (n) being the probability of observing a (n) as an outcome of the game."
"an important remark is that, following the notion of cce, players are assumed to decide, before receiving the recommendation, whether to commit to follow it or not. at a cce, all players are willing to commit to follow the recommendation given that all the others also choose to commit. that is, if a single player decides not to commit to follow the recommendations, it experiences a lower (expected) utility. a special case of cce is the correlated equilibrium (ce, [cit] ). the difference between the cce and the ce is that, in the latter, players choose whether to follow or not a given recommendation, after it has been received."
"at each iteration of a given learning algorithm, each player must obtain some information about how the other players are reacting to its current action, in order to update their strategy and choose the following action. broadly speaking, in algorithms such as brd, fp, sfp and rm, players must observe the actions played by all the other players. this implies that a large amount of additional signaling is required to broadcast such information in wireless networks. in some particular cases, this condition can be relaxed and less information is required [cit] . however, this is highly dependent on the topology of the network and the explicit form of the utility function [cit] . other algorithms, such as rl and juste-rl, only require that each player observes its corresponding achieved utility at each iteration. this is in fact, their main advantage, since such information requires a simple feedback message from the receiver to the corresponding transmitters [cit] ."
"the purpose of this section is to provide additional insights about the performance and pertinence of the learning algorithms described above in the context of decentralized wireless networks. in the following, we compare the algorithms in terms of several fundamental features. we summarize this discussion in table i ."
"the speed of convergence (when it is observed) is highly influenced by the amount of information available for the players. for instance, fp, sfp and rm converge faster than juste-rl since the formers calculate the expected utility relaying on a closed form expression."
"acoustic source localization is a challenging problem that arises commonly in fields such as robotics [cit], speech and sound enhancement [cit], acoustic tomography [cit] and many others. reverberations make the problem harder to solve and it becomes particularly difficult if the sound sources are obscured by an obstacle (a wall, for instance - figure 1) ."
"in this paper, we have presented several notions of equilibrium and several learning dynamics that allow wireless networks to achieve such equilibria. in particular, we have described a general notion of equilibrium, namely, the coarse correlated equilibrium (cce). then, we introduced some particular cases of cce, such as correlated equilibrium (ce) and nash equilibrium (ne), are also analysed."
"the second step (s 1/ρ (·)) is just an element-wise soft thresholding, while the update of auxiliary variable u k requires only vector addition. hence, the first step is the most computationally expensive, since it imposes solving the linearly constrained linear least squares problem. however, it involves the sparse matrix ω and the subsampling matrix m, thus the problem scales as o (ijt )."
"the process of learning equilibria is basically an iterative process. each iteration of the learning process can be broadly divided into three phases: (i) the observation of the environment at iteration n, which"
"the proposed path-loss distance model is a good solution to determine device distance from the beacon in a range 1 -9 m, because 1.09 m error is acceptable (fig. 15, 16 ). for surveyed radiomap in room ( fig. 9), almost all distances are in the range 1 -9 m which mean that estimated distances can be used as input of trilateration algorithms. the standard deviation of surveyed samples does not depend on distance from the beacon. research proved that the wave multipath, interferation, diffraction has an impact on the rssi distribution [cit], especially, the human body absorbs the signal strength what should be included while determining position. samples distribution are not gaussian in all surveyed points. filtering data for assumed path-loss model has no significant impact (table iv) on the final position but other advanced methods may improve accuracy significantly. after trilateration, the positions with average error 2.45m (table iv) was achieved."
"where 1 m − does not vary appreciably in the interval of interest for raman measurements. singularities, oscillatory, or fast decreasingly behavior of the quantum response of detectors or gratings could induce some artifacts when integrating the above expression. in our case, m was obtained by measuring the emission of a tungsten lamp and comparing it with the emissivity of a black-body at same temperature."
"the aim of the present study was to make a systematic characterization and optimization of the experimental variables involved on a serds setup by testing the system in situations of biomedical interest (in vitro and in vivo). as far as we are concerned, similar applications of serds are not present in the literature."
"for ensuring grid independence in the cfd predictions, calculations have been performed for a typical airfoil shape and flow configuration, using different grid resolutions. the variation of the predicted lift coefficient (normalized by that of the finest grid with) with the total number of grid nodes (ngn), for the considered case is displayed in figure 5 . as it can be seen in the figure, a sufficient grid independence is achieved for grids with larger than 50,000 nodes. following these findings, the grids are generated by the developed automatic grid generator, using the same grid topology and strategy, with an even finer resolution corresponding to number of nodes about 60,000, for assuring sufficient grid independence."
"for ensuring grid independence in the cfd predictions, calculations have been performed for a typical airfoil shape and flow configuration, using different grid resolutions. the variation of the predicted lift coefficient (normalized by that of the finest grid with) with the total number of grid nodes (ngn), for the considered case is displayed in figure 5 . as it can be seen in the figure, a sufficient grid independence is achieved for grids with larger than 50,000 nodes. following these findings, the grids are generated by the developed automatic grid generator, using the same grid topology and strategy, with an even finer resolution corresponding to number of nodes about 60,000, for assuring sufficient grid independence."
"the inlet and outlet boundaries are placed ten and twenty chord lengths (c) away from the airfoil, respectively, which may be considered to be sufficiently far [cit] that the boundaries do not influence the results. at the inlet, uniform distributions of the velocity components (the approach velocity v relative to the airfoil, figure 2 ) and turbulence quantities are prescribed. the inlet values of the turbulence quantities are estimated assuming a turbulence intensity of 4% and a macro-mixing length [cit] of 25% of the chord length. at the outlet, a constant static pressure is prescribed along with zero-gradient conditions for the remaining quantities. at the pair of periodic boundaries, obviously, periodic boundary conditions are applied. the extension of the domain between the two periodic boundaries (l) corresponds to the space between two neighbouring blades in a turbine wheel. in an application, in general, this size varies with the number of blades on the wheel, as well as the radial position of the considered blade-to-blade section."
"assuming production by glass fibre-reinforced synthetic material moulding, a trailing edge with finite thickness is considered, which is defined by points p 1 and p 5 that are connected by a tiny straight line. in addition to the coordinates of the five points, the directions of the vector couples as well as their lengths describe the shape of the airfoil. the number of degrees of freedom can be reduced by introducing certain constrains on these parameters, which will be addressed below."
"the optimization cycle is depicted in figure 1 . the blade shape is represented by a finite number of degrees of freedom (to be described, below, in more detail). they are denoted as \"geometry parameters\" in figure 1 . the optimization cycle is started by the optimization algorithm, which generates the geometry parameters and transfers them to the mesh generator to create a cfd grid. at the first cycle, the geometry parameters are, however, not generated by the optimization algorithm but stem from the original geometry. the response surface methodology (rsm) module defines the aerodynamic operation points according to certain rules (to be defined later in more detail) that are fed to the computational flow solver (cfd) as boundary conditions, together with the computational mesh. based on the cfd results, two values are calculated and passed over to the optimization module by the rsm module. these are the average thrust, that is, the tangential force (the force component in the direction of rotation that generates the power) and its standard deviation within the considered space of flow variables. the maximization of the former (maximum power) and the minimization of the latter are the objective functions (efficient and stable operation under varying wind conditions). the optimization algorithm (to be described later in more detail) proposes a new set of geometry parameters for the better fulfilment of these objectives, which closes the cycle."
"some methods have been developed in order to reduce luminescence and extract the vibrational information from the scattered raman signal. among them are polarization modulation [cit], time-resolved picosecond excitation pulse and gating [cit], shifted-excitation raman difference spectroscopy (serds) [cit], and computational algorithms for automated background subtraction [cit] ."
"the airfoil is positioned in a rectangular domain, with periodic boundaries on both sides in the circumferential direction, along with an inlet and an outlet boundary. the solution domain and the boundary definitions are sketched in figure 2, where the velocity vector relative to the airfoil, the flow angle (fa) and blade angle (ba) are also indicated."
"for ensuring grid independence in the cfd predictions, calculations have been performed for a typical airfoil shape and flow configuration, using different grid resolutions. the variation of the predicted lift coefficient (normalized by that of the finest grid with) with the total number of grid nodes (ngn), for the considered case is displayed in figure 5 . as it can be seen in the figure, a sufficient grid independence is achieved for grids with larger than 50,000 nodes. following these findings, the grids are generated by the developed automatic grid generator, using the same grid topology and strategy, with an even finer resolution corresponding to number of nodes about 60,000, for assuring sufficient grid independence."
"the authors declare no conflict of interest. differences in the velocity fields with changing airfoil shape can be observed (figures 12 and 13) . note that the performances indicated in figure 9 for different profiles are the overall (average) performances for the considered range of operation (table 1) but are not directly connected to a given operation point. still, the predicted velocity fields imply that the relative performance of the profiles qualitatively correspond to the trend indicated in figure 9, where moving from profile 1 to profile 2 and to profile 3 a decreasing acceleration on the suction side can be observed, implying a decreasing trend for the lift, that is, thrust, agreeing qualitatively with the behaviour shown in figure 9 ."
"but stem from the original geometry. the response surface methodology (rsm) module defines the aerodynamic operation points according to certain rules (to be defined later in more detail) that are fed to the computational flow solver (cfd) as boundary conditions, together with the computational mesh. based on the cfd results, two values are calculated and passed over to the optimization module by the rsm module. these are the average thrust, that is, the tangential force (the force component in the direction of rotation that generates the power) and its standard deviation within the considered space of flow variables. the maximization of the former (maximum power) and the minimization of the latter are the objective functions (efficient and stable operation under varying wind conditions). the optimization algorithm (to be described later in more detail) proposes a new set of geometry parameters for the better fulfilment of these objectives, which closes the cycle. the computational fluid dynamics (cfd) analysis is based on the general-purpose code cfd ansys fluent [cit], applying the finite volume method (fvm) for discretizing the governing differential equations. flow turbulence is modelled within a rans (reynolds averaged numerical simulation) approach [cit], where the time-averaged equations are solved for the time-averaged variables in steady-state. a more accurate approach would be large eddy simulations (les) [cit], which, being a three-dimensional and unsteady approach, would, however explode the frame in the present case. turbulence is modelled by a two-equation turbulence model, that is, by the shear stress transport (sst) model [cit], which were shown to perform quite reliably especially for wall-driven turbulent flows with potential to predict transitional effects [cit] . no wall-functions are used near the wall, resolving the near-wall layer. the coupling of the discretized navier-stokes and continuity equations are treated by a coupled solver [cit] . the convective terms are discretized by a formally third-order accurate quick discretization scheme [cit] for the navier-stokes equations and a second-order upwind scheme [cit] for the transport equations of the two turbulence variables. convergence criterion for the residual of each equation was set to 10 −5 ."
"sessarego el al. [cit] performed the multi-objective optimization of wind-turbine blades using a non-dominated sorting genetic algorithm. annual energy production, flapwise root-bending moment and the mass of the turbine blade were considered as the objective functions. the aerodynamic model was based on bem. [cit] performed a multi-objective optimization of wind turbine blades using the lifting surface method as the aerodynamic model. maximization of annual energy production and minimization of blade load including thrust and blade flap-wise moment were considered as the objective functions."
"an automated two-dimensional airfoil shape optimization procedure for small horizontal axis wind turbines (hawt) is presented, with emphasis on high thrust and aerodynamically stable performance. the procedure combines the computational fluid dynamics (cfd) analysis with the response surface methodology (rsm), the biobjective mesh adaptive direct search (bimads) optimization algorithm and an automatic geometry and mesh generation tool. in the cfd analysis, a reynolds averaged numerical simulation (rans) is applied in combination with a two-equation turbulence model. in the analysis, an emphasis is placed upon the role of the blade-to-blade interaction. the results show that improvements in the performance can be achieved by modifications of the blade shape, resulting in a pareto front and the present procedure can be used as a tool for blade shape optimization. prioritization of the properties while choosing a profile out of the pareto front would depend on the purpose of the application. profiles with high thrust may, for example, be preferred for a local, direct consumption of the generated electric power, while more stably operating profiles (with low standard deviation of thrust) may be preferred for feeding the power into the grid."
"wind power has been receiving increasing attention as one of the most promising renewable energy sources. wind turbine technologies for improved performance have been developed within the last two decades, where many aspects, including aerodynamics and aeroelasiticy, have been the subject of intensive theoretical, computational and experimental research [cit] . as the airfoil profile shape has a crucial impact on the aerodynamic efficiency of a wind turbine, an important research area for wind turbine technology has been blade design. within this context, automatic blade shape optimization procedures, which have a longer tradition in other areas such as compressors [cit] and turbines [cit] have also been used for the aerodynamic and structural optimization of wind turbine blades. the more classical intuition and experience based non-automatic optimization [cit] has been used. in our study, we focus on horizontal axis wind turbines (hawt)."
"an automated two-dimensional airfoil shape optimization procedure for small horizontal axis wind turbines (hawt) is presented, with emphasis on high thrust and aerodynamically stable performance. the procedure combines the computational fluid dynamics (cfd) analysis with the response surface methodology (rsm), the biobjective mesh adaptive direct search (bimads) optimization algorithm and an automatic geometry and mesh generation tool. in the cfd analysis, a reynolds averaged numerical simulation (rans) is applied in combination with a two-equation turbulence model. in the analysis, an emphasis is placed upon the role of the blade-to-blade interaction. the results show that improvements in the performance can be achieved by modifications of the blade shape, resulting in a pareto front and the present procedure can be used as a tool for blade shape optimization. prioritization of the properties while choosing a profile out of the pareto front would depend on the purpose of the application. profiles with high thrust may, for example, be preferred for a local, direct consumption of the generated electric power, while more stably operating profiles (with low standard deviation of thrust) may be preferred for feeding the power into the grid."
"for comparative purposes, the ft-raman spectra were also taken in each case. an ftraman spectrometer (bruker rfs 100/s) with a nd:yag laser at 1064 nm as excitation source was used."
"for the optimization, the airfoil geometry needs to be parametrized. historically, different approaches have been used. for the present purposes, the hermite curves, which are closely related with bezier curves, are found to be most suitable for generating airfoil shapes with a limited number of parameters and with sufficient geometrical precision. a hermite curve is defined by two points (p i ) and two tangent vectors (with lengths l ij and directions α i ) on both its ends. with this function, the geometry of the airfoils has been generated by using four piecewise hermite curves (hc i ), two representing the suction and two the pressure side, as shown in figure 4 ."
this regression reveals that the proposed fmemd is empirically a dyadic filter capable of separating the wgn into imimf components having mean frequency exactly half value of the previous components.
"subsequently, a set of sparse tracks, referred to as tracklets in the literature, are produced by grouping the linklets that indicate similar motion patterns (see figure 1 ). this produces two sets of independent tracklets, referred to as low-and high-level tracklets. we adopt markov chain monte carlo data association (mcmcda) to estimate an initially unspecified number of trajectories. to this end, we formulate the tracklet association problem as a maximum a posteriori (map) problem to produce a chain of tracklets. the final output of the data association algorithm is a partition of the set of tracklets such that those belonging to each individual object have been grouped together."
"this section introduces the proposed multivariate extension of emd algorithm. the main content of this section includes: (i) definitions of the direction-dependent mimf (dmimf) and the direction-independent mimf (imimf), (ii) methods for computing the dmimf, (iii) pseudo-imimf and its calculation, (iv) the specific algorithm of fmemd and (v) some supplementary statements."
"we first welcomed the participants and introduced them to the study procedure. then we handed out a declaration of consent to the participants. having declared consent, they were given a detailed explanation of the study conditions. each participant was then assigned to one of two groups. the groups were exposed to the conditions in counterbalanced orders. before we started the first trial, we took a measurement on the preferred level of control to collect a baseline. then the participants were exposed to both conditions. after finishing the trials we took a further measurement on the preferred level of control. now the participants could determine their preference in reference to the varying degrees of automation. then we gave the participants the possibility to review the results and again took a measurement. this time we determined the preferred level of control in reference to their perception of the quality of control in each condition. this addressed the missing review opportunity mentioned earlier. after all trials and measurements the participants were debriefed and thanked for their participation."
"the memd performs well in many applications from various fields such as process control [cit], mechanical health diagnosis [cit], biomedicine [cit], data denoising [cit], etc. however, the major drawback of memd is that it requires a long computation time especially for high-dimensional signals. more precisely, a total number of k · p sifting operations are required for each memd-iteration on a p-variate signal with k projection directions [cit] . note that sifting operations is the most time-consuming step in emd since it involves the extrema-based cube spline interpolation to obtain the signal envelope. recently, several dynamical sampling schemes [cit] have been provided to improve the computational performance of bemd/memd. notice that these improvements are continuous and incremental, without avoiding the envelope interpolation on multidimensional space."
"the second method is called the frequency-based soft handover. in this scheme, when the user goes into the intersection area, the second bs starts serving it using some of its subbands reserved for handover. the sinr from both stations will be lower at the intersection area, however, this will be compensated by the additional bandwidth provided to the user. frequency-based soft handover data rate is show with the dash-dot line in fig. 3 . this method also reduces the data rate fluctuation considerably, compared to the hard handover method."
"surprisingly, we also found that the people we recorded behaved more naturally in front of the camera, which was also confirmed by the operator. due to the remote control and the automatic bouncing mode, we could remain at a distance from the setup. we had never considered any e↵ects on people in front of the camera before the shooting."
"this facilitated the refinement of camera calibration parameters. thereafter, the software searched for more points in the images to create a dense 3d point cloud, from which the final model was created. point clouds were processed on an intel core i7-4710hq laptop with windows 7, 16 gb of ram memory and a 2 gb nvidia geforce gtx graphic card with a graphics processing unit (gpu)."
"volume 6, 2018 figure 9. image of a work table which is obtained from underexposed condition., n) . similarly, the rgb based color image fusion can be extended by applying the methodology to three channels (red, green and blue) of a color image separately [cit] ."
"in recent years, advances in signal acquisition tools have highlighted the requirement for synchronous processing of multichannel data [cit] . as a result, the standard emd has been extended to various versions including those suitable for the bivariate [cit], trivariate [cit] and multivariate signals [cit] . bivariate emd (bemd) [cit] estimates the bivariate local mean by mapping the input to a number of real-valued projections and then averaging the respective local mean obtained from these projected signals. following the same idea, ur rehman and mandic [cit] extended the emd trivariate emd (temd) by taking the three-dimensional signal as a pure quaternion. similarly, the multivariate emd (memd) [cit] further generalizes this concept for multivariate signal and efficiently solve the uncertainty of imfs arrangement [cit] ."
"based on the identified scenarios and requirements, we built a setup focusing on solo operation with task delegation to an assisting system. together with a mechanical engineer, we determined the necessary motor torque and acceleration for actuating the payload of 20 kg horizontally and 6 kg vertically. an overview of the main units is presented in figure 1 . the components, wiring diagrams, source codes of the control software, plans of the 3d-printed parts, documentation of the wireless control protocol (using bluetooth low energy) and sample footage are provided electronically [cit] in further detail. the final implementation is presented in the following section in figure 2 ."
"as indicated earlier, tracklets are obtained from both the low-and high-level codebooks, c l and c h, constructed in section 3.1. two codewords are assigned to each pixel p(x, y) at time (t) in the video. therefore, in a video sequence of temporal length t, a particular pixel p(x, y) is represented by two sequences of assigned codewords 1 :"
"each participant was asked to perform the task of following a person with the camera in movement direction while framing the person at the first third in the direction of the movement (rule of thirds technique). the three levels of the independent variable for interaction technique were full manual control (no motion control, human baseline), software joystick (motion control, remote control baseline) and touch-based control directly on the camera stream (motion control, touch-based remote control). for manual control, the slider carriage needed to be manipulated physically by the participants to move the camera and to frame the person. in the other conditions the slider carriage was driven by a motor and needed to be controlled via a remote control user interface o↵ering continuous control options. in a within-subjects design each participant executed all conditions. the order was counter-balanced based on a latin square design."
"in the hard handover method, the user is served by one source at a time. as the user moves from one cell to the other, the received power from the base station (bs) (in our case the light source) goes below a threshold value. then, the user drops its connection with this bs and connects to another bs. as seen in fig. 3, the rate fluctuation is high for hard handover. to alleviate this, soft handover methods are used. in a soft handover scheme, the user is served by two bss when he is in the intersection area, therefore, his data rate does not drop off suddenly."
"in this paper, we propose two different vlc handover mechanisms. we first show the need for such handover mechanisms quantitatively, on a scenario case. then, we explain our two solution proposals. finally, we present simulation results to show that our solutions increase both the overall system performance and the performance of each user."
(ii) two such vectors are combined to form a bivariate signal. then the fmemd method is applied to this data and resulting in m imimfs.
"here we conservatively associate two responses only if they are in consecutive frames and are close enough in space and similar enough according to their assigned codewords. thus we obtain, two sets of trajectories, called x l and x h (see figure 4 ). it is obvious that the number of linklets is generally more than the number of objects in the scene and that many trajectories might belong to a single object. in addition, we note that the number of linklets created by a single object is much smaller in x h than the ones in x l . ideally we are interested in obtaining a single trajectory for an object. thus the linklets it seems that a single person may produce more than a single trajectory. we expect this because our algorithm does not involve any person or object detection. we deal with this issue in the next section, which describes a data association process that rejects certain tracklets as false positives."
"an unmotorized slider, for the manual control condition, and a motorized slider, for the other conditions, were mounted on tripods at the same height and placed in front of each other. to provide the participants with the same delay that would appear in the video stream in the remote control conditions, the video stream was also displayed in this condition. a smartphone was therefore mounted on top of the manual slider carriage to display it. the stream was recorded by a dslr camera (canon eos 60d) mounted on the carriage of the motorized slider. the camera was connected to a video encoder (teradek cube 255) via hdmi cable situated on top of the camera. the encoder provided an rtsp video stream of the camera image via wifi and recorded the stream for post-hoc evaluation. the stream was also presented on the tablet used for the software joystick and touch control interfaces."
"as is commonly known, the power extracted from a solar panel is strongly affected by three factors: irradiance levels, ambient temperature, and load characteristics [cit] . generally, pv systems are designed to produce the maximum available power regardless of the irradiation intensity and temperature. load impedance determines the output power of the solar panels and it can be a dc load with or without batteries. when the solar panel is connected directly with the load, the operating point of the system will be at the intersection point of the i-v curve with the load line, which may not be at the maximum power point, which leads to the loss of power. to overcome these limitations and improve the produced power from the solar panel, a dc-dc converter is included between the solar panel and the load. by controlling the dc-dc converter, the impedance matching between the solar panel and the load can be achieved. the seeking of the maximum power point (mpp) is accomplished by the mppt system. this system will change the duty cycle and implicitly the input resistance of the dc-dc converter, until the operating point of the system reaches the mpp. since solar power systems have become widespread, many mppt algorithms have been developed and published [cit] . they vary in many aspects, such as complexity, cost, sensors required, implementation hardware, convergence speed or range of effectiveness [cit] . it has been observed in recent years that there is a passion to propose new mppt techniques based on artificial intelligence [cit] or modified hybrid optimizations [cit] to increase the efficiency of the energy production of photovoltaic system. however, in most cases, there is no need to use a more complicated or a more expensive algorithm, where a simpler and less expensive system can result in similar outputs. for this reason, an incremental conductance algorithm was implemented to achieve the process of the maximum power point tracking. generally, pmsms use the dc batteries to induce the magnetic fields in the windings of the stator. these batteries usually have a low life span and need to be replaced every two years on average, which makes the cost of installation and maintenance of such systems high. another justification of the proposed design in this study is the advantage of replacing these batteries with a pv module system that has a longer life, which will reduce both the cost and the faults that can happen when the battery needs to be replaced [cit] . such systems can be used in residential applications, aircraft systems, hybrid electric vehicle applications, and pumping systems [cit] ."
"in fig. 5, we present sinr results for modeled vlc network employing ffr. as noticed, sinr at cell edges become very low. therefore, if a user moves towards to cell edge, the data rate of the user will be considerable lower. this is the main reason for the proposed soft handover algorithms. we present the cumulative distribution function (cdf) of user data rates under different handover algorithms for vlc in fig. 6 . in order to make a handover, neighboring cell is required to have empty sub-channels to assign. we control the amount of handover sub-channels with β which represents the ratio of assigned sub-channels to overall sub-channels that a cell have."
"over the past decades, permanent magnet synchronous motor (pmsm) has been the most requested motor used in the field of electrical machines due to its many distinct advantages, such as high efficiency, superior performance, and the ability to operate with a full load at low speeds [cit] . to achieve the high performance in pmsm, the precise rotor position information is needed. to get the rotor position, the mechanical sensors located on the rotor shaft are commonly used for this purpose. these sensors have many disadvantages, such as increasing the cost and size of the motor, decreasing the reliability of the system, and the requirement of special arrangement for mounting these sensors. for that reason, controlling the pmsm drives without using position sensors turns into a prevalent research issue in the literature. many of these studies proposed observations to estimate the rotor position in the medium and high speeds, such as using the back electro-motive force (bemf) observers, sliding mode observer (smo) [cit], and the extended kalman filter (ekf) [cit] . however, these methods will not be effective in the low speeds because of the lack of electromotive force (emf) and flux amounts that are needed to estimate the speed. to cover the zero and low speeds, high frequency signal injection is normally used (hfsi). the injection will track the saliency of the pmsms by injecting high frequency voltage to the daxis current, which will cause vibration in the motor if the rotor position is not correctly estimated. this method could cause an audible noise in the motor that may make it unsuitable in some applications [cit] . another method to accomplish control in a low speed is by using the low frequency (lf) signal injection [cit] . the injected signal will cause oscillations in the stator back electromotive force (bemf) that can be detected from the voltage response of injecting pulsating harmonic current into a d-axis current. if the injected harmonic current does not yield the precise zero steady-state error, it will cause a problem in detecting the zero speed. in addition, this method requires information about a large number of machine parameters [cit] . the injection systems are suitable for low speeds; therefore, they are always combined with other methods to estimate the wide speed range, which will make the system too complex and hard to be implemented in real life [cit] . one major objective of this paper is to estimate the wide speed range of pmsm by using one powerful estimating method instead of two. for that, the ekf is chosen for its many advantages, such as reducing input noises, including both system and measurement noises and handling the parameter variation and the ability of motor start up from any initial position [cit] . the second objective of this paper is to use solar photovoltaic (pv) energy to feed the pmsm. in such systems, the efficient operation of pv generator cannot be independently accomplished from the pmsm drive system and the operation of the overall system must be tested to demonstrate the reliability of the operation under a wide range of operating conditions."
"we additionally examined two particular research questions in controlled studies. first we wanted to gain insights on how workload as well as sense and quality of control were a↵ected by the introduction of a low degree of automation compared to a full human baseline. in conclusion, we could not find any negative e↵ects regarding either sense or quality of control in our data. although we could table 2 . the methods we used in our mixed-methods design and evaluation approach"
"workload we determined the workload using the tlx for each user interface. analyzing the overall workload for the di↵erent uis with the friedman test, we found no significant main e↵ect ( 2 (2)7.00, p.03). with mean values of 45.09 (sd15.18) for manual control, 37.78 (sd13.47) for software joystick and 46.16 (sd13.06) for touch control (figure 5, right)."
"where τ n is the temporal distance between the end of a tracklet and the start of its immediate successor. the motion consistency probability, p m, is modeled by assuming that the trajectories follow a constant velocity model and obey a gaussian distribution."
"the 3d models generated were validated with the actual values using regression methods. prior to the regression analysis, a correlation analysis was conducted to make an initial examination of bivariate relationships among the variables. pearson's correlation coefficient was used to analyze simple linear relationships between the estimated values generated by the model and actual ground truth values. the root mean square error (rmse) and mean absolute percentage error (mape) indicators were calculated for error estimation. both statistics provide an overall measure of how well the model fits the data."
where the residual signal r (t) represents the trend within the signal. the procedures used for extraction of imfs from a real-valued signal x (t) are summarized in algo. 1.
"the data determining quality of control was extracted from the recorded video material after the study. we developed an analysis tool to detect the face of the person walking by in each frame thus further determining the person's center (also available electronically [cit] ). the distance from the first third in movement direction to the person's center (blue area in figure 4, left) in pixels (px) was logged. figure 4, right) ."
"first we consider the encoding of the likelihood of tracklets in (6) . the observations, that is, the tracklets, can be either true or false trajectories of the object. therefore, the likelihood of a tracklet, given the set of trajectories, s, can be modeled by a bernoulli distribution:"
"plant phenotyping allows researchers to gather information about plant architecture, which is fundamental to improve plant characterization, selection and discrimination [cit] . plant models through phenotyping processes are useful for assessing growth, physiology, architecture, stress, yield and every development in the plant, which allows plant management to be more comprehensive [cit] . plant modeling can be used to characterize stress from biotic or abiotic factors, biomass production, weed discrimination, fruit characterization, yield, leaf traits, root morphology, and photosynthetic efficiency, among other factors. traditionally these factors have been assessed by experts relying on visual scoring, which creates differences between expert opinions and is time-consuming. thus, the goal of plant phenotyping is to measure plant characteristics accurately, avoiding appreciative differences from different judges. plant phenotyping is able to measure complex shapes and, therefore, is useful in decision-making for plant selection, treatment or agronomical management [cit] . however, increasing knowledge and expertise require technological developments in sensing devices and processing methods. many of the current sensing techniques are based on two dimensional characteristics, such as hyperspectral or thermal imaging, which are highly dependent on angle and distance to the target plants. currently, 3d modeling is being proposed for the morphological characterization of plants. three-dimensional modeling is rapidly expanding and new techniques are becoming more attractive. these techniques include visible images [cit], lidar (light detection and ranging) [cit], structured light [cit], spectroscopy [cit], and thermal images [cit] . the most common technique is visible imaging based on sensitive sensors within the range of what is visible, due to its economic price and ease of operation. the obtained images, under controlled conditions, can be related to yield, nutrition stress, vigor, biomass or other related parameters."
"where t m denotes the computation cost, and n m is the number of sifting times consumed for one mimf extraction. in contrast to memd, the p-variate input of fmemd is projected and then operated univariable using the standard emd. therefore, the total consumption of fmemd for a p-variate imimf computation only involves"
"we propose two soft handover solutions. both methods assume the ffr scheme depicted in fig. 2 . however, for handover to succeed, each bs should reserve a certain percentage of its sub-bands for users arriving from neighbor cells. otherwise, the bs can use all of its resources on its existing users and cannot serve incoming users. therefore, we assume, each bs reserves a certain percentage of its sub-bands for handover."
"image fusion is the process of registering and combining multiple images from different modalities to improve the imaging quality, while reducing randomness and redundancy such that the new image is more suitable for the purpose of human visual perception and computer-processing tasks [cit] . one of the major requirements for successful image fusion is that images from the different modalities have to be aligned correctly. moreover, as real world images are usually nonlinear and nonstationary, which render the traditional techniques invalid, a fully adaptive approach is required [cit] ."
"the procedures can be re-applied by setting the mean signal s (t) as a new input, then the iteration continues until a monotonic trend r (t) is obtained. in consequence, an indirect multivariate extension of the emd algorithm (namely, fmemd) is realized. for convenience, the rest of this paper uses imimf instead of the defined pseudo-imimf."
"to address the issue of a too coarse measurement tool, we now used a visual analog scale to ask for the preferred level of control. the captions on each side of the visual-analog scale read \"i control the system and the results manually\" and \"the system controls the results\". additionally, we also increased the overall number of data points by taking multiple measurements during the study trials."
"are combined locally, based on coefficients computed from the local variance estimates at each spatial point, resulting in a fused image z as given by (33) where α i (m, n) and β i (m, n) are weights determined by location (m, n), on the basis of the relative values between local and total variance for each mode"
"the proposed method may be improved from following aspects: (i) a more efficient emd algorithm, (ii) algorithm to solve the overdetermined system of linear equations, (iii) method for generating a more suitable set of direction vectors (emd cannot decompose invalid projection, for example when the energy of the highest frequency component is much smaller/larger than that of the remained components) and (iv) a more scientific criterion to determine the number of directions. he has authored/co-authored over 100 journal papers and four books. he has undertaken more than 10 national and provincial funds and state key projects as the principal investigator, including the national outstanding youth science foundation of china, the national natural science foundation of china, the teaching and research award program for outstanding young teachers in higher education institutions of moe, p.r.c., and projects for national high technology research and development program of china (863 program). his research interests include process control and optimization theory and application."
"as illustrated in figure 4, the tracklets obtained after clustering are not quite reliable for long term object tracking, but do a relatively good job of encoding the moving object motions in the short term. the main advantage of constructing the tracklets based on the two codebooks is that no object detection is required. although a set of representative trajectories is created for all moving objects in the video, there is no guarantee that an object would be represented by a single trajectory. moreover, in crowded scenes, the representative trajectories may correspond to more than one object. however, if the motion pattern changes, then the trajectories would separate."
"in summary, we have described a method to construct tracklets, given online observations. then the probability of a tracklet being part of an actual track has been calculated by formulating the data association problem as a map estimation. initial observations are taken to be the low-and high-level codebooks obtained by an event detection system. the lowlevel codebook codes the local motion patterns, while the high-level codebook codes global motion patterns in videos while considering the scene context. they are then tracked in consecutive frames, which produces two sets of dense tracks of small temporal length, called linklets. these dense linklets are then grouped to produce a small number of representative object tracklets. the representative tracklets are then linked to form long-term object trajectories. the data association framework we have adopted has two main advantages:1) it can reject certain tracklets by considering them as parts of false trajectories, and 2) it uses low-level tracklets as supportive information for filling the gaps between high-level tracklets, thereby producing smooth trajectories."
"on the other hand, the proposed method shows more desired results than that from the memd method. as shown in the first two mimfs of fig. 5, large distortion and leakage are presented all over the data channel, especially those in d 2 of x 1, x 2, x 3 and x 5 . in contrast to the memd method, the proposed fmemd is less prone to leakage and sheds better light on the intrinsic characteristic of modes id 1 and id 2 . the explanation to the poor performance of memd is similar to the description in section. v-b. the memd algorithm may compromise the t-f information in low sampling rate, since it enforces the same number of iterations for all data channels."
"addressing the single degree of automation issue, we now used two designs with a low and medium degree of automation. for the low degree condition, we again used the software-joystick interface of the prior study. for the medium degree condition we prototyped a ui that used keyframe selection as an input technique. the user interface used a through-the-lens [cit] approach. the participants could see the scene on the tablet and use it as a viewfinder. they could move the tablet freely and select certain positions as keyframes. the motion-controlled slider would then interpolate a motion path between all chosen keyframes. for the implementation of such a technique, known points of reference are necessary. these can be provided, e.g., via optical tracking or by synchronizing the tablet with a virtual model of the study room. these models consequently need to be updated when the tablet is moved, e.g., through the tracking of markers or the accelerometer data of the tablet. such an implementation is technically complex as it needs to handle noisy data and requires low latencies. as we were mainly interested in the e↵ects on the perception of our participants, we prototyped it in a wizard-of-oz style. to create the illusion of an automated system capable of this functionality, we asked the participants to select a pre-defined set of keyframes by showing sample images during the explanation of the study task. the motion control tool was accordingly pre-programmed so that the \"selected\" keyframes were matched by the resulting motion."
"our proposed algorithm provides an alternative by strictly using only local motion patterns and contextual information within a data association framework. in contrast to the figure 2 : observations are represented by low-and high-level codebooks. first, the video is densely sampled scales to produce a set of overlapping stvs and subsequently, a twolevel hierarchical codebook is created. (a) at the lower level of the hierarchy, similar video volumes are dynamically grouped to form a conventional fixed-size low-level codebook, c l . (b) at the higher level, a much larger spatio-temporal 3d volume is created. it contains many stvs at and captures the spatio-temporal arrangement of the volumes, called an ensemble of volumes. similar ensembles are grouped based on the similarity between arrangements of their video volumes and yet another codebook is formed, c h [cit] ."
"with ρ very close to 2. in practice, parameter ρ can be calculated from the slope of the straight line. the empirical relationship between the number of zero crossings and the bandpass filter index can be approximated by"
"this inequality illustrates that running of the proposed fmemd is p times or more faster than that of the memd. in practice, according to our experiments, the multiple is much larger than p."
"given the resulting tracklets, high-level trajectories can be generated by linking them in space and time. we achieve this by formulating the data association required as a maximum a posteriori (map) problem and solve it with the markov chain monte carlo data association (mcmcda) algorithm. the observations are taken to be the constructed tracklets in section 3.2:"
", contain scale-aligned intrinsic joint rotational modes [cit] . 1 a (p − 1) sphere (hypersphere) is an extension of the ordinary sphere to an arbitrary dimension and is represented mathematically in 2, where r denotes the radius and c i is the ith coordinate value of center point c. this work adopts the terminology that a (p − 1) sphere resides in a p-dimensional euclidean coordinate system."
"combine eq. (11), eq. (12) with definition 2, the dmimf d θ k (t) that defined on vector v θ k can be properly addressed after l 0 times multivariate sifting, which satisfies the following relationship"
"evaluating and quantifying such aspects is a limitation of the presented work. with sense of control, we did consider an experience measurement beyond the performance measures of workload and quality of control. in our field observa-tions, we also observed how the use of our tools influenced expressiveness and exploration of various speed settings in order to shape di↵erent versions for post-production. expressiveness and exploration are, for example, dimensions measured by the csi. we did not investigate these or any other dimensions covered by the csi in detail."
"compared to virtual camera control, physical camera control faces di↵erent challenges due to in-situ constraints. therefore it is hard to apply findings from virtual environments to the physical world. our system is designed for application in the physical world and can be used in lab environments and in field studies alike. sometimes, natural interaction processes and phenomena such as unexpected use emerge only in-the-wild. this is described by marshall and colleagues [cit] . these phenomena can give further insight into the users' actions, reasoning and experiences. in our case, one surprising and mentionable field insight was that untrained people tend to behave more naturally when filmed by an autonomously operating or remotely operated system. we also observed that the operators would ask to explore more variations of the same shot at di↵erent speeds, to have more options in choosing the fitting material for the rhythm of the cuts and the soundtrack in post-production."
"a novel multivariate extension of the standard emd, namely the fast memd (fmemd), has been provided. three key concepts including the direction-dependent mimf (dmimf), the direction-independent mimf (imimf) and the pseudo-imimf are first defined through properly extending the notion of multivariate extrema. then fmemd is generalized from the univariate emd by solving an overdetermined system of linear equations. it is shown numerically that the proposed fmemd, similar to the standard emd, follows a dyadic filter bank structure (channelwise) for multivariate white gaussian noise."
"we conducted a shapiro-wilk test for the collected data. it showed no significance and therefore, a normal distribution of the data can be assumed. to stay"
"first the participants were welcomed and informed about the study and how their recorded data was handled. they then were handed out a declaration of consent. after declaring consent, a demographic questionnaire was handed out and after its completion, the rule of thirds framing task was explained. an example video of the expected results was presented."
"the rest of this paper is organized as follows. traditional emd and memd are first reviewed in section ii. section iii details the proposed fmemd approach. in section iv, the filter bank properties of fmemd are analyzed by decomposing multichannel white gaussian noise. a comparative study between fmemd and memd is presented in section v. section vi studies the fmemd performance with three real world applications. it is followed by conclusions in section vii."
"(iii) these bivariate imimfs are separated and reconverted back to image-matrices, which yield m scale images for each input image, denoted by x i and"
"solid 3d models were processed off-line in the open source software meshlab ® (figure 3 ). this software was used to manage and plot the stored data, generating a model with the point cloud previously created in agisoft. the software processes unstructured 3d models using remeshing tools and filters, which enables cleaning, managing, smoothing, and tools for the curvature analysis, visualization and processing of the created 3d models. the meshes were processed in different steps."
"overall, we see a strong indication that our prototyping platform generally meets the requirement of high quality smooth camera motions and of system stability for field use we had set out to test. even though we are aware that it does not reach the level of sophistication of professional equipment, we believe it can still serve as a research platform for new user interfaces prototypes and field observation. during the recording, the slider was indeed controlled wirelessly. however, this was done by a second trained person using a command line interface on a laptop holding it with one hand and typing with the other. we focused on evaluating the collected hardware requirements and not yet the user interface requirements. the user interfaces for end-user control presented below were designed based on insights gained from this first field evaluation."
"the presented theorem 1 also shows the irrationality of the original memd method [cit] . for each specified direction vector, the t-f feature of the current projection is different from other projected signals, therefore, distinct numbers of iterations are required for extracting different direction-dependent mimfs. as a result, running of the original memd, which enforces the same number of sifting for all data channels, may induce the effect of overdecomposition."
"as we used a wizard-of-oz approach with a pre-defined result, we did not collect data on the quality of control as in the prior study."
"empirical mode decomposition is one of the most powerful time-frequency (t-f) tools and has been extensively studied and widely applied in numerous engineering applications [cit] . unlike traditional techniques such as fourier and wavelet transform that project signal onto predefined basis functions, the bases of emd are derived from the data and thus can be nonlinear and nonstationary [cit] . by processing on the local characteristic time scales, the input data is decomposed into a complete and finite set of localized amplitude/frequency modulated (am/fm) components called intrinsic mode functions (imfs) [cit] . the imfs indicate the natural oscillatory modes embedded in the signal and serve as the basis functions, which are determined by the signal itself rather than preset kernels [cit] . due to the ability of emd to analyze nonlinear and nonstationary processes, the number of publications on emd has been increasing steadily over the past decade [cit] ."
"vision-based systems can be connected and evaluated. additionally, it can (d) be transported easily and therefore is suited for lab as well as field evaluations."
"expanding automation in the domain of creative expression seems paradoxical at first. delegating camera motion to machines might result in a more robotic aesthetic than in manual operation as found in related work [cit] . however, such tools can add to the vocabulary and expressiveness of the discipline as in timelapse, high-speed or aerial shots, which can hardly be controlled manually. additionally, smart, easy to use tools o↵ering fast implementation of ideas that might lead to more exploration."
"the idea of the proposed schemes is depicted in fig. 4 . the figure shows the use of sub-bands in the intersection area. in the hard handover, the user either uses the sub-bands of the closest bs. in the power-based soft handover, the second bs starts transmitting the same sub-bands as the first bs in the intersection area. the frequency-based soft handover, the second bs increases the bandwidth of the user by starting transmission in its reserved bands."
", by assuming that the pmsm is a nonlinear system affected with centered and white noise, the discrete time of the system nonlinearity can be expressed as follows:"
"user roles we handed out an online questionnaire to camera operators. focusing on qualitative statements, we distinguished two usage contexts: solo operation, where one operator is out alone on location and thus needs to control everything that is usually delegated to multiple assistants. we also found mentions of the classic collaboration with task delegation, where tasks are shared and delegated to separate operators. this is often used on film sets."
"where m denotes the slope, c represents the y-intercept of the straight line and is dependent on the length of the data been investigated. accordingly, the base-2 exponent of eq. (24) is given below"
"inspired from theorem 1, it is observed that processes 'dmimf extraction + projection' and 'projection + imf extraction' satisfy interchangeability. therefore, the ideal direction-independent mimf (imimf) as an intersection of all meaningful dmimfs should also meet the same property. mathematically, it requires the theoretical imimf id (t) of the analyzed signal x (t) satisfies the following equation"
"for the study we recruited 18 participants (14 male). the average age was 24, with ages ranging from 21 to 31. prior knowledge in tools for camera motion was reported by 4 participants."
"in order to meet the requirement of displaying the camera stream, we chose to implement the user interfaces on a tablet capable of this task. we implemented a touch-based ui that used the whole screen as an input area for direct control on the camera stream. this design led to less occlusion of the stream by visual interface components and can be extended by further image-based control techniques. we compared this design alternative to a status quo software joystick that served as a baseline condition for remote control. both remote control interfaces were also compared to full manual control, a human control baseline without motorization. the human baseline condition allowed us to interpret the data collected beyond a relative comparison between both remote control conditions."
"high data rate applications such as high definition video streaming are being widely used. meeting the required wireless bandwidth for these applications is one of the prominent challenges of new wireless communication systems. as a result, alternative wireless systems are being investigated. visible light communication (vlc) is one of the alternatives."
"the basic idea behind emd is to evaluate the local envelopes by signal interpolation, using the local extrema as interpolation points. given an specified interpolating technique, high envelope reconstruction fidelity requires the abundant and accurate information of extreme points. under low sampling rate the extrema cannot be located exactly by searching the sampling sequence [cit] . therefore, most researchers believe that a successful application of the emd algorithm requires high sampling frequency [cit] ."
"the data set under study is provided by a plant of eastman chemical company [cit] . the uncompressed nonlinear and nonstationary data were sampled from the control system every 20 seconds on each of the indicators. in order to facilitate the results for displaying, only the last 10 channels data (denoted by tag 21 to 30) from total 30 indicators with 2,000 samples are investigated. the decomposition results obtained by fmemd are presented in fig. 7, where each tag denotes a single data channel or signal dimension. for simplicity, the first five adjacent imimfs are added together. the top row of fig. 7 gives the process output, and the second and third rows are id 1 − id 5 and id 6 respectively. the fourth row (id 7 ) denotes the plant-wide oscillations. id 8 is shown in the fifth row and last row (r) is the remainder of the process data."
"according to definition 1, the extracted multivariate extrema of x (t) and the univariate extrema of x θ k (t) fulfill the following equation set are univariate extrema of x θ k (t) respectively. lemma 1 shows that projection of the cubic spline interpolation curve of a multi-dimensional point set is equivalent to the interpolation calculated from the projected point set. therefore it can be concluded that"
"to determine whether the hardware requirements were met, we asked a professional camera operator to evaluate our setup during five assigned shootings. we made sure that he did not know the identified requirements to avoid a bias. the features concerned with speed changes, bouncing and safety stops were essential and thus already tested during the development of the hardware and firmware. the requirements of system stability and smoothness of motion hence remained to be verified. the setup was used in five on-assignment shootings for exhibition films in a modern art museum (figure 2). at these recording sessions the system worked in a stable way. horizontal shots were recorded at ground level and at waist height. also, diagonal shots from waist height to ground level and vertical shots at a height beyond two meters were recorded. the cameras were moved with constant speed and bounced between both ends for longer periods of time. the limit switches were used for each calibration and prevented the slider from hitting an end. moves and ramps were programmed remotely and wirelessly by a second trained person using a laptop connected to the system via bluetooth (figure 2). field evaluation of the prototype in di↵erent scenarios with a professional camera operator during an assignment for exhibition films of a modern art museum."
"in this paper, we have introduced the use of motion descriptors obtained by an event detection algorithm for multiple object tracking. we have shown how pure motion descriptors for event detection could be employed to build a tracker without requiring an object model. thus, each individual object is tracked by modeling only the temporal relationships between sequentially occurring local motion patterns. the algorithm is based on the descriptors of moving objects, obtained at two hierarchical levels. by considering both local and global motion patterns, two sets of initial tracks, called linklets, are obtained. then, a set of sparse tracks, referred to as tracklets, was created by grouping linklets showing similar motion patterns. we then developed associations between them in order to produce longer trajectories."
"vlc is a promising communication method for high data rate applications especially in indoor environments. in this study, we propose two soft handover methods for vlc to improve the performance of the users at the cell boundaries. according to our results, our proposed handover methods improve the user data rates. especially, frequency-soft handover outperforms both power-soft and hard handover."
"in which e u (t) (e θ k u (t)) and e l (t) (e θ k l (t)) represent the upper and lower envelopes of x (t) (x θ k (t)), respectively. on the basis of eq. (8), the relationship between the local mean (m (t)) of x (t) and the local mean ("
"ii) unlike memd or fmemd that repeats the cubic spline interpolation for many times, s t s −1 s t is computed only once during the whole process of the fmemd decomposition, which greatly saves the computational time."
"due to the ability of memd on time-frequency analysis of multivariate nonstationary signals, it has found a number of different real world applications from various fields, such as signal filtering [cit], biomedical engineering [cit], image analysis [cit], industrial controlling [cit] and others. similarly, the proposed fmemd as a more precise extension of the standard emd, can also be effectively used in these areas. since it has demonstrated that fmemd outperforms memd in multivariate signal decomposition and operating speed, more widespread applications of the emd-base extensions can be expected. in this section, three representative cases including: (i) plant-wide oscillation detection in process industry, (ii) electroencephalography (eeg) analysis and (iii) image fusion application, are added to verify the above statements."
"in this paper we concentrate on creating long-term trajectories for unknown moving objects by using a model-free tracking algorithm. as opposed to the tracking-by-detection algorithms [cit], no object detection is involved. each individual object is tracked only by modeling the temporal relationship between sequentially occurring local motion patterns. this is achieved by constructing two sets of initial tracks that code local and global motion patterns in videos. these local motion patterns are obtained by analyzing spatially and temporally varying structures in videos. initially, the video is densely sampled, spatio-temporal video volumes (stvs) are constructed, and similar ones are grouped to reduce the dimension of the search space. this is called the low-level codebook. then, a large contextual region containing many stvs (in space and time) around each pixel is examined and their compositional relationships are approximated using a probabilistic framework. they are then employed to form yet another codebook, called the high-level codebook. therefore, two codewords are assigned to each pixel, one from the low level and the other from the high level codebook. by examining pairs of sequential video frames, the matching codewords for each video pixel are transitively linked into distinct tracks, whose total number is unknown a priori and which we will refer to as linklets. the linking process is separately performed for both codebooks. this is done under the hard constraint that no two linklets may share the same pixel at the same time, i.e. the assigned codewords. the end result at this step is two sets of independent linklets obtained from the low-and high-level codebooks."
"aforementioned approaches that attempt to track objects either by detection or learning an appearance model of the objects, our goal is to construct a hierarchical model for all moving objects in a scene."
"(i) in memd, the (approximately) zero mean rotational signal is obtained only after all the projected signals fulfill the stopping criteria. however, not every channel of the real world signal needs to have the same number of iterations. as a result, enforcing the same number of iterations for all data channels may excessively sift more extrema to the following modes, thus causes the phenomenon of overdecomposition."
"where γ t k is the trajectory of the object at a time instant t. the chain consists of an initialization term, p i, a probability to link the tracklets, p l, and a termination probability, p t, to terminate the trajectory. it is assumed that a trajectory can only be initialized or terminated using the tracklets obtained from the high-level codebook, t h . therefore, the probabilities of initializing and terminating a trajectory are written as follows:"
"the performance of the proposed method is evaluated on two multi-exposure images that are shown in fig. 9 and fig. 10 respectively. the first image is obtained from a work table, which is severely underexposed. observed that the small plate in the right region is nearly hidden, whereas the left part, showing details of the book, is well exposed. in the second input image, on the other hand, the details on the book are covered due to the strong light, but the right area is exposed properly. fig. 11 shows the result of the fusion obtained from applying fmemd based scheme. as desired, the fused image retains all the detail of the original images and any spurious fusion artifacts are kept into a minimum."
"for the purpose of fmemd application, an unique synthesized mimf should be developed, which means a representative mimf that is independent of the projection direction is preferentially required. more precisely, a direction-independent mimf (imimf) should satisfy the characteristics of dmimfs from all directions."
"handover in vlc has certain differences compared to cellular systems. the serving area for a light source is considerably smaller in vlc. moreover, since the directivity of visible light is higher than the radio waves, received power drops off more rapidly as users move away from the light source. therefore, the handover region is much smaller. the challenge here is to act very quickly for successful handover. on the other hand, mobility rate is rather limited and precise position estimation via vlc is possible. this helps in predicting the handover need and being prepared before the user moves to handover region. one of the first studies on the subject handles 978-1-4799-8091-8/15/$31.00 ©2015 ieee intensity based handover for vlc [cit] . other existing studies mostly focus on clustering and power adjustment methods for handover [cit], handover protocols [cit], extensions to ieee 802.15.7 for handover support [cit], and cell zooming methods [cit] . rf-hybrid systems that utilize wi-fi when a vlc link is not available have also been considered [cit] . however, to the best of our knowledge, there is no solution for soft handover specifically tailored to vlc."
"are corresponding imfs extracted from the projected signals. since k is an infinite positive integer, the above system of overdetermined linear equations is unsolvable in engineering. to address the problem, a pseudo definition of imimf with relaxed constraint is presented by limiting k into a finite positive integer, which is, definition 4: for any given set of direction vectors"
"(1) filtering and removal of outliers and noise reduction by filter, where individual points were removed using statistical filters and points out of the grid by more than 0.5 cm were automatically removed. (2) cleaning of neighboring weed plants, where plants were manually deleted from the point cloud. the target weed plant was isolated after removing every source of interference inside the bounding box. although, the target plant was manually isolated by removing other plants, some leaves of nearby plants could interfere into the model due to their wide coverage. some filters were tested to remove these parts. however, no filter was useful for isolating the target plant because of the high similarity in color and the continuous point cloud without significant space between the plants since the ground connected every plant into the model with a separation lower than 0.5 cm. thus, the filter did not allow the automatic separation and a manual cleaning process was necessary. (3) processing of the resulting meshes to extract the main parameters, which were compared with actual values. plant height was estimated with meshlab and compared with actual plant height. similarly, leaf area (la) was calculated from the triangulated 3d mesh by the sum of the areas of all triangles. the estimated la was compared with actual la, as well as dry biomass, for validating the models."
"where iα and iβ are α-β stator currents, vα and vβ are α-β stator voltages, ls is stator inductance, rs is the stator resistance,  m is the flux linkage of the rotor magnets, p is the number of pole pairs, ω is the electrical speed of the motor, and  is the electrical angle. the state equations of the system are expressed as follows:"
"recently, the uses of multivariate extension of emd [cit] were proposed for the fusion of multiple out-of-focus and multi-exposure images. it was shown that the bivariate emd outperforms wavelet and pca based approaches, particularly in retaining edge-based information from different image modalities. in this section, the proposed fmemd is adopted to merge two multi-exposure images to a single improved image as an illustration."
"plant phenotyping is possible through sfm and mvs technologies. indeed, results of this work showed that weed plant reconstruction at a high level of detail is possible with a budget and simple system usable in several scenarios, even in outdoor conditions. nevertheless, further efforts should focus on extracting additional structural information, such as leaf inclination, overlapping and stem diameter, which could be useful for decision-support systems. aspects such as the time-cost relationship and the need for details in the different approaches should be assessed. end-details could be necessary for breeding programs or botanical classification; however, the integration of these models in on-field precision agriculture treatments would require less details and lower processing times. the limits and implications of detectable details should be studied. in this study, the target plants had different structures which led to different resolutions, while the image acquisition and operational processing systems were the same. the complexity and shape of the plants would need to be classified for automated decision-making programs."
"advanced tools, such as camera cranes [cit], are complex to manage, since multiple degrees of freedom (dof) need to be controlled in real time. this usually requires years of training, is expensive, complex to build and requires high e↵orts in transportation. for the quick translation of new interaction concepts, not all of the o↵ered dof are always necessary in order to evaluate alternative designs on a conceptual level. as pointed out by nielsen [cit], filmmakers do not always apply all possible movements and exploit all dofs. they rather carefully choose dofs and movements as stylistic devices depending on the content of the scene. often shots that are outside the \"regular\" human visual experience need to be motivated in particular by the content and are thus used less often. our prototype supports some go-to types of shots that can often be seen in diverse contexts such as advertising, image, short, feature and documentary films."
"a dc-dc boost converter is utilized in the simulation. by controlling the duty cycle of the switching elements, the pv terminal voltage is kept at the point that maximum power is obtained and the output voltage of pv panel is matched with the desired load voltage. input-output dc-dc boost converter equation is"
"given the early development stage of our prototype and the controlled study design and goal-oriented study task (adapted sdlp), the environment was not particularly well suited for determining aspects such as \"results worth e↵ort\" or \"enjoyment\". these are clearly important aspects that csts should support and that should be evaluated. however, we believe they also require an open-ended task and potentially an even more stable system."
"vlc is an alternative technique for wireless communication applications. since vlc is intended to be utilized for especially high data rate applications, it requires high signalto-interference-plus-noise ratio (sinr) values. for this reason, most of the vlc application requires line of sight (los) paths. therefore, vlc channels can be modeled with optical channel dc gain calculations with generalized lambertian intensity to include direct los and first order reflected diffuse paths [cit] ."
"p, q and r are set in diagonal matrices to make the calculation easier on the system and these matrices are found by using trial and error methods. the initial values of these matrices are:"
"three weed species with contrasting shape and plant structures were selected for image processing on a commercial maize field (arganda del rey, madrid, central spain). two dicots, xanthium strumarium l. and datura ferox l., and one monocot, sorghum halepense l., were chosen for the experiment, looking for structural differences. these weed species were selected for their contribution to economic losses due to yield reduction in maize crops. [cit] when maize was at stage 12 to 14 of the bbch (biologische bundesanstalt, bundessortenamt und chemische industrie) scale [cit] and weeds ranged from bbch 10 to bbch 20. at sampling time, weed height ranged from 4 cm to 30 cm. the samples were chosen for their different weed stages. a total of ten samples per weed species were monitored, which were selected randomly within the field looking for a representative sample of the population, that is, within a 95% confidence interval of mean plant height. weeds were usually located between the maize lines, a crop planted in lines 75 cm apart. our study focused on this area between crop lines, which in the early stages of growth are not covered by the crop. consequently, the crop plants did not interfere in the model creation and only weeds were monitored. in addition, the adjacent weed plants were removed in order to avoid interference between monocots and dicots that could affect the model reconstruction of each group of weeds. since the adjacent weed plants were often of the same species, the use of filters based on color could not discriminate and isolate the target weed, which would have affected the accuracy of the model. three 10 cm graphic scales were located on the ground following a triangular shape around the plant, leaving the target plant in the center of the triangle. graphic scales were useful not only for model creation but also for scaling the model to actual measurements in post-processing."
"leds offer various advantages such as lower cost, lower energy consumption and higher lifetime, compared to incandescent light bulbs widely used today. therefore, led based illumination is predicted to become the dominant lighting method in the near future. since leds support high switching rates, it is possible to modulate the intensity of led-generated light faster than human eye can perceive, enabling their use for communication as well as illumination. [cit] . since then, high data rates up to 500 mbps were reported [cit] ."
"in summary, it can be concluded from this simulation that the alignment of imimf based frequency bands, in case of fmemd, is more capable to result in the stabilization of the dyadic filter bank structure."
"sense of control for analyzing the self-reported data regarding sense of control, we also used the non-parametric friedman test as the use of parametric tests on self-reported data in rating-scale format is controversial as pointed out by carifio and perla [cit] . here also, no significant e↵ect was found ( 2 (2) 5.03, p.081), with median values of 75 for manual, 70 for software-joystick and 60 for touch (figure 5, left)."
"with such high data rates, vlc is envisioned to be used in high data rate indoors applications. some of these applications include information dissemination in shopping malls for product information, and in museums for data on exposition pieces. in such applications, users move from one light source to the next. meanwhile, their communication should not be effected by this switch over different light sources. therefore, effective handover mechanisms for vlc are needed."
"in this section, the implementation of ekf on the smpmsm is discussed. ekf is defined as an optimal recursive estimation method used to estimate the nonlinear systems based on the least-square sense [cit] . to build the extended kalman filter model, first the mathematical representation of pmsm in α-β form is described"
belonging to the same object must be merged in order to create a single representative track that describes the motion of the object. here we follow the idea of clustering trajectories to create a representative object trajectory [cit] .
"the results indicate that although the correct detections we obtain with our algorithm are comparable to the state of the art, they include more false positives (see table 1 ). perhaps one can expect this, since no object detection is employed in our algorithm. recall that the scene observations that we use are motion descriptors and do not incorporate object appearance, as do object-centric trackers."
"we contribute an open source motion-controlled camera slider with independent control and powering units that is inexpensive and o↵ers open wireless access. with this platform, various types of uis balancing delegation with control can be prototyped and evaluated. it was tested by a professional cinematographer in five assignment shootings. we found strong indication for its high quality in smooth motion and system stability. we then used it to conduct two controlled user studies. in our first study, we examined the e↵ects of motorized tools for camera motion on the sense and quality of control of the participants. in a subsequent study, we investigating the influence of reviewing results by the participants during the evaluation process of the recorded material. this helped us to validate the e↵ects on the participants' sense of control."
"part i: if the emd is not convergent on the projected signal, none of the imfs can be extracted from x θ k (t). therefore, it is impossible to find such a positive integer l that causes part ii: if the emd algorithm is meaningful on the projected signal, the projected signal must yield an univariate imf after integer times sifting. by supposing it costs l 0 times sifting for decomposing the current imf ("
"step in this step, the kalman gain is used to minimize the errors between the measured and predicted values. to calculate the kalman filter gain, the expression below is used t1"
"we recruited 12 participants (8 male) for the study. the average age was 25, with ages ranging from 21 to 32. no prior knowledge in tools for camera motion was reported by the participants."
two tracklets are linked if they are consistent in the time domain and show similar motion patterns. we assume independency and decompose the probability of linking the tracklets into two probabilities. therefore (11) is rewritten as:
"mastering precise and aesthetic camera motion in filmmaking is central to camera operators. performing it manually is hard and errors are likely. to reduce errors, task sharing and the use of support tools have been established. in one single camera move, usually three -sometimes even more -operators work together simultaneously in a choreography, performed behind the camera [cit] . in this process, subtasks are delegated to human operators or to machines. both perform very well in di↵erent areas: humans, for example, outperform machines in recognizing visual patterns and aesthetic judgment; in contrast, machines can move heavy weights smoothly, precisely and repeatedly [cit] . originally, many of such support tools were purely mechanic. [cit] s 1 . fueled by further technological advances in the decades to follow, a multitude of novel tools was introduced and is now summoned under the label of camera motion control systems. going beyond the pure mechanics, nowadays high-tech tools such as industrial robots [cit] or drones [cit] became part of the tool palette in camerawork."
"to date, most of the reported approaches for tracking rely on either robust motion or appearance models of each individual object or on object detection, i.e., they are object-centric. thus a key assumption is that a reliable object detection algorithm exists [cit] . this remains a challenge, particularly in complex and crowded situations. these methods use the detection response to construct an object trajectory. this is accomplished by using data association based on either the detection responses or a set of short tracks called tracklets figure 1 : overview of the algorithm. the goal is to estimate the trajectory of the moving objects in the video without invoking object detection. initially two sets of linklets are constructed by chaining; the low-level considers small window fragments, while the high-level analyzes a larger region in order to impose a contextual influence. they are obtained by exploiting an activity understanding system. the resultant tracks (chains) are filtered and replaced by a set of sparse representative tracks, the so-called tracklets. longer trajectories are then generated by using the markov chain monte carlo data association (mcmcda) algorithm to solve the maximum a posteriori (map) problem using tracklet affinities. thus this procedure uses low-level tracklets to connect high-level tracklets when there is a discontinuity in motion or time."
"the system used in this paper is simulated in both low and high speeds to evaluate the validity of the proposed method to run the motor in the wide-speed range as shown in figs. 3-7 . the algorithm of ekf is implemented in the model by using the s function. the inputs of the s function are the currents and the voltages in stationary (α-β) reference frame. the outputs are the speed and the rotor position estimated values. through the collected simulation results from pv system ( fig. 3(d), fig. 4(d), fig. 5(d), and fig. 6(d) ), we can notice that the mppt system was able to find and track the maximum power point. it is also obvious that it was able to make the operating point of the system exactly at the mpp. thus, the withdrawal of the energy from the solar panel was very close to the optimal. in this paper, the extracted power for three mppt algorithms, which are perturbation & observation (p&o), fuzzy logic (fl) based algorithm, and ic algorithm, is presented. the obtained results are given in table ii . although fl-based method gives slightly higher results, it was not preferred in this study due to its high computational complexity. v. conclusions this paper presents a new method that makes the ekf estimates very low speeds of pmsm fed by solar pv system. this method reduces the complexity of controlling the pmsm in a wide speed range, which makes the estimating system work perfectly on both the fpga and dsp systems. also, by fixing the d-axis current in the system, it will help to reduce the errors that happened between the estimated and the real speed, especially when the motor uses high input voltages. this method will be suitable for pumping systems and electric vehicles, which will reduce the cost and size of the motor hence, increasing the utilization of pmsm in other applications. in addition, with the view to optimizing the power production from solar panels, incremental conductance approach based mppt was introduced. simulation results showed that the ic algorithm has a good tracking ability. it was able to obtain maximum power in terms of variable conditions and increased stabilization of load."
"to obtain the multivariate envelope e θ k (t); 5: for a set of k direction vectors, the mean of envelope curves, m (t), is given by"
"a typical industrial case is presented to demonstrate the effectiveness of the proposed method for plant-wide oscillation characterization. plant-wide oscillation is one of the most common abnormal phenomena encountered in process industries [cit] . the presence of oscillatory variables may severely impact the normal operation of the process plant, therefore, it strongly motivates the research for techniques to automatically detect the plant-wide oscillations [cit] ."
"as our physical implementation is mainly based on open source technology, we also provide it for reproduction and customization. therefore the changing needs of researchers and practitioners can be met in varying contexts beyond the traditional scope of cinematography, as shown by the examples presented in section 2.identifying user roles, hardware and feature requirements with experts in a user-centered fashion helped us to minimize the risk of possible user rejection due to poor design or build quality in the implementation of our prototype. experts in cinematography are often used to high-end equipment and thus bringing a prototype to the set has to be considered with caution. this was reflected by the fact that only after the first session and the screening of the results we were granted further access to the expert users and field environment and consequently were able to gather our insights."
"the proposed fmemd is a well developed extension of the univariate emd for time-frequency analysis of multivariate data. unlike traditional memd, the raised method is able to not only remove the dependence of the computational complexity on dimension of the input signal, but also intuitively present a more desired decomposition result. to verify the statements, a comparative study around the decomposition performance of memd and fmemd is elaborated."
"a mathematical proof of the proposed lemma is presented in appendix. a. proof: let x (t) denotes the investigated multivariate signal, x θ k (t) represents the projection of x (t) along v θ k, the relationship between x (t) and x θ k (t) is given as"
"we identified several challenges, that currently hinder research in this field: on the market, there are mainly expensive tools without access for connecting new user interface (ui) prototypes. expertise in multiple fields, such as mechanical engineering, electronics, human-computer interaction (hci) and computer science is necessary in order to build them. this makes it hard to quickly translate new ideas into prototypes. furthermore, the research literature on physical cinematographic camera motion and its control is not very elaborate to the best of our knowledge. there is a lack of ethnography, studies on systems and interaction designs and their evaluation for user-centered research. in the field of virtual camera motion there is plenty of literature available, but its findings cannot simply be adapted to physical camera control to suit the needs of enthusiasts and professionals on location. existing support tools are meant to be used in the physical world and also serve the artistic expression. the latter often involves a trial and error experience of unforeseen dynamic changes depending on how a situation unfolds. therefore, their use and control is hard to simulate in a virtual environment [cit] . in addition, operators want to delegate tasks, but also want to be in control of the recorded images [cit] . delegation and being in control, however, are often contradictory [cit] . therefore finding the right balance for di↵erent user groups is non-trivial. to be meaningful to operators, systems therefore need to balance user control with automation [cit] . this might be achieved best by introducing high level controls, but further research on systems and interactions is necessary."
"the mode-aligned property of fmemd contributes to the automatic clustering of similar frequency modes as well as providing both time and frequency information in nonstationary time series. compared with memd, operation of the proposed fmemd is independent of the dimension of the multivariate signal, and this approach avoids the problem of over-decomposition when processing on the white noise as a filter bank. moreover, numerical simulation shows that the fmemd method presents better decomposition performance in processing data with low sampling frequency. such superiorities can facilitate more widespread applications of the multivariate emd method, especially for the implementations in analyzing long data and signals in which the dyadic filter bank decomposition is relevant. simulations and several real world cases have verified the effectiveness of proposed method."
hardware and user interface requirements an additinal professional operator was interviewed about technical hardware and user interface requirements. he was experienced in camera operation and in consulting major producers of cinematographic equipment. from a 45 minute semi-structured interview we derived the following requirements (table 1) .
"the remainder of the paper is organized as follows. section ii provides the related works. section iii includes power calculations for vlc. in section iv, we present our proposed handover algorithms. section v includes simulation results. lastly, the conclusions are presented in section vi."
"before addressing the problem of calculating (pseudo) imimf for the purpose of fmemd development, the direction-dependent mimf (dmimf) is first computed in combination with a theoretical demonstration of the interchangeability between (multivariate) sifting process and direction-base projection. given a specified direction vector v θ k, the sifting process of standard emd (step 1 to step 4 in algo. 1) can be extended multivariable for decomposing dmimf from a p-variate signal x (t), as given by : 1 and minima"
"the presented examples implement machine benefits, but hardly o↵er a human-machine interplay allowing operators to contribute. one of the few examples o↵ering such an interplay is presented by stanciu and colleagues [cit] . here, a crane with a camera mounted on one side automatically frames a userselected target and adapts to the manual crane operation of a human operator on the opposite side 3 . this human-machine interplay is important for the operators as they want to actively express their personal view and therefore want to feel in control [cit] . for interactive control, a futuristic vision of novel and natural forms of interaction was already presented with starfire [cit] . the video prototype showcased a camera crane that was controlled by a tablet used as a remote viewport. so far however, no implementation followed the concept 4 ."
this equality proposes a complete scheme for computing the dmimf on such specified direction. the projective relationship between the calculated dmimf and the univariate imf extracted from the projected signal has been proved theoretically.
"given our experiences from the field evaluations, we were surprised not to find a di↵erence. we had expected that a missing screening of the results might have a↵ected the reports, in particular, on sense of control. in the full manual condition more and more noticeable jerky motions were observable in the recorded material. participants however might have been unaware of it without a review phase. we hence conducted a second controlled user study examining the e↵ects of adding a review phase to the evaluation process. based on our collected data, we concluded that the introduction of a review phase had no significant e↵ect on the participants' perception of control even when increasing the level of automation. to mimic the recording practice we observed on set and given that jerky motions might only be discovered after a recording, we would still recommend its integration in the evaluation process in field as well as laboratory environments."
"it can be proved that if the standard emd algorithm is meaningful on the projected signal, there exists a positive integer l so that eq. (4) meets the requirements for defining a dmimf."
"in this section, a simple comparison between the computational loads of memd and fmemd is presented. it is noteworthy that some other fixed-point arithmetic operations are negligible compared with the sifting procedure. 5 similarly, the infrequent matrix operation, eq. (22), which is utilized for the solution of imimf, also has very small effect on the total computational time due to two reasons:"
"(2). therefore, as users move, their data rates will fluctuate. we depict this in fig. 3 . the straight line shows the rate experienced by the user as he moves, when hard handover method is used."
"because of its reputation, the museum set high standards for the aesthetic and technical quality of its representation. the material recorded with our system eventually appeared in six exhibitions films and was approved and published by the museum. we take this as an indicator for our system's capability to produce acceptable results. only one recording session was planned in the beginning. the additional five sessions were initiated by the operator only after the results of the first were screened. in a summarizing debriefing the operator was generally satisfied with the systems stability, but also revealed some issues he found."
"given the assigned codewords (labels) for each pixel, we obtain an over-segmented representation of the video (see figure 3 ). in this over-segmented representation, each segment represents a set of pixels that are similar in terms of local motion patterns. therefore, it is a simple task to create a short trajectory for each pixel by examining the temporal coherence of its assigned codewords. this is comparable to the concept of so-called \"particles\" [cit] ."
"the fmemd method enables features of the signal within the same subband from different channels to be categorized in the same mode which allows the neural information in different frequency bands can be studied across different channels. the imimfs preserve not only the amplitude and frequency features but also the phase characteristic of synthesized components. unlike other advanced signal processing techniques (e.g., independent component analysis (ica) and principal component analysis (pca).) utilizing statistical algorithms to extract common information, only the cross-channel common components are considered which could ignore the phase difference among different channels."
"eq. (10) proves that operations 'multivariate sifting + projection' and 'projection + univariate sifting' satisfy interchangeability. this useful property can be exploited for fmemd development, which will be further discussed in section iii-c."
"although our algorithm possesses no information regarding either an object's color pattern or a human body model, it achieves promising results on challenging data sets. as stated previously, the major drawback of our algorithm is the number of false positives and some problems in maintaining the trajectory identity when objects have similar shape and motion. further improvements would include incorporating color information to reduce the number of id switches."
"1) mode alignment. this property indicates that fmemd should ensure the overlapping of the frequency response of corresponding imfs from different channels which are associated with the same index. 2) dyadic filter. a dyadic filter is capable of separating the white noise into imimf components having mean frequency exactly half of the previous one. 3) unbiasedness. since the contribution to the total spectrum energy comes from each fourier component of a white gaussian noise is uniform and equivalent, the spectral shapes associated with the corresponding imimfs from multiple indexes should exhibit some self-similarity. in consequence, this section sets out to investigate the filter bank structure of fmemd from three aspects: (i) the averaged spectra of wgn realizations, (ii) regression on number of the zero crossings in each imimf, and (iii) self-similarity analysis."
"since transmitted signals in vlc are real and positive, relationship between transmit power and receive power can be directly calculated with channel dc gain (h(0)) [cit] . therefore, the received power can be found as"
"to ensure consistency and durability, the checkpointing process periodically stores data to persistent storage. state-of-the-art solutions employ various checkpointing techniques. many research studies [cit] apply double checkpointing. their checkpoints are replicated on another location to avoid a single point of failure. kaminotx [cit] ) is an improved double checkpointing scheme. kamino-tx moves the update of backup copy outside the critical path of a transaction through asynchronous updates with lock protection. it reduces the nvram storage of double checkpointing by maintaining only the most recently modified objects using an lru policy. a lot of data persistence systems [cit] adopt cow checkpointing. when updating persistent data, they checkpoint a copy of the data on nvram and update the original data in place. consistent and durable data structures (cddses) [cit] adopt cow with multiple versions to allow atomic updates without requiring logging."
"we understand that existing nvram-based persistence solutions are mostly at a prototyping stage. in a practical system, we identify two potential problems, namely dangling objects and memory leaks, that should be addressed by the runtime system. supporting features that handle them could somehow affect the overall performance, but is necessary to give a real performance picture on a real system nonetheless. so, we design and implement a robust, easy-to-use persistent object management that can avoid subtle problems of dangling pointers and memory leaks into non-volatile memory."
"rpl is a distance vector routing protocol for networks with constraints on processing power, memory, and energy. during the network construction phase, the rpl builds a directed acyclic graph (dag) topology. all edges of the dag are oriented in such a way that no cycles exist. this graph is partitioned into one or more destination-oriented dags (dodags) and the rpl calls these destination nodes dodag roots. within a dodag, the rpl uses an objective function (of) to select and optimize routes according to different metrics."
"simpo implements an efficient lockless consumer-producer circular log array [cit] to record operation logs on nvram for persistence, as shown in figure 3 . the log entry, consisting of two pointers and three variables, is space efficient. when the program starts, persistent functions are located on nvram. context is copied to nvram by persistent functions, as shown in listing 3 (lines 14 and 19). the log entry includes pointers to the function and its context."
"in our future work, we plan to add a mechanism for rrd+ to distinguish static nodes from mobile nodes in order to improve the performance of rrd+ for different degrees of mobility. moreover, energy consumption is another critical problem we need to investigate in the future. right now we do not use sleep mode, since topology information updating relies on information broadcasting and collection in a timely manner and a sleep mode would delay this process. rpl is energy-efficient by design; compared to other mobility supports, rrd+ has lower overhead, and thus consumes less energy. based on this, we plan to integrate a dynamic duty cycle in rrd+ according to different degrees of mobility in order to reduce energy consumption. in the long run, it is also interesting to improve rrd+ to support mobility in data dissemination scenarios."
"from step (3) we obtain the following set of facts of the form (aggr feat pat, val, fp pat (ts i )), where each fact gives the value val of the aggregation aggr of different values of the feature feat for each occurrence of the pattern pat in the time series ts i . we next put these results together and we obtain the following facts where each fact is of the form range(r, pat, prop, min, max ), where prop is a feature of the pattern pat . min (resp. max ) is the smallest (resp. largest) value that the feature prop takes when evaluated wrt each time series of the cluster cl . finally r is the range obtained by subtracting min from max ."
"many network simulators exist, such as network simulator 2 and 3, omnet++ (objective modular network testbed in c++), cooja, tossim (tiny os simulator), etc. we chose to use the cooja simulator (contiki 3.0, adam dunkels, sweden). it is a emulator for contiki systems. it is compatible with real hardware and the simulation code can be used on real-life nodes. in addition, contiki comes with standard rpl implementation on which we added all the different enhancements including rrd+. we made some modifications on the default parameters of cooja in order to make it more realistic, especially when it comes to emulating signal propagation. there are four propagation models in cooja [cit] . one of them is multi-path ray-tracer medium (mrm) which takes reflections and refractions into account to simulate real environment. in order to make it more suitable for urban and unstable environments, we included a random behaviour to its path loss calculation. indeed, we added a gaussian random variable in the path loss formula of mrm in order to simulate instability of the radio links. we calibrated the randomness in order to make transmission range randomly fluctuate between 30 m and 50 m independently for each transmission. the path loss formula in decibels is shown in (4)."
"-a simple programming model that (automatically) classifies functions into instant and deferrable ones, and transactionizes a group of functions based on an optimized logging scheme in which synchronization is triggered by instant functions only; -a deferrable execution model that performs deferrable functions with one or multiple server threads and thereby improves data locality and concurrency for a variety of applications; -a buffered-dual-copy checkpointing scheme tailored to hybrid memory architecture, which reduces the number of memory operations and hides nvram's slow writes; -a high-level object persistence api for efficient object management on nvram; our design has avoided issues of dangling pointers and memory leaks, which take permanent effects on non-volatile memory."
"2.1. self-similar propagation. the growth and propagation of root are considered very important for plant to adapt to soil environments since it is the only organ to obtain water and inorganic nutrients below the surface of the soil [cit] . in a root system, its architecture is well known to be a major determinant of root functions in acquiring soil resources [cit] . because most root systems have the characteristics of selfsimilarity and are considered as approximate fractal objects over a finite range of scales [cit] (as shown in figure 1 ), fractal geometry has been widely used to assess the architecture and distribution of root systems in soil [cit] ."
"in this paper, all functions use their standard ranges and variable data. the experiments compare the performance using all accuracies of algorithms for a fixed number of function evaluations. the max evaluation count is 10,000. experiments have been carried out using matlab 7.0 on a standard 2.5 ghz desktop computer. all parameters in ccpso2, abc, and mde pbx are set as their original values. the population size of four algorithms is 50. in order to do meaningful statistical analysis, each algorithm runs for 20 times, and the mean value and standard deviation value are taken as final results. in rgo, the number of root apices in main root group is thirty percent of the selected root apices in each generation. max and min are set as 3.0 and 1.0, respectively. and are all set as 1.0. all the benchmark functions are listed in tables 1 and 2 . as can be seen in table 3, with dimension of 2, abc performs better than others on functions 3, 5, and 6 . mde pbx shows the best performance on functions 1 and 2 . ccpso2 shows the best performance on functions 4 and 11 . though rgo only gets the best results on functions 7, 8, 10, and 12, it gets satisfactory accuracy on other functions. all algorithms get the best results on function 9 . from table 4, rgo performs much better than abc, ccpso2, and mde pbx on most functions except 3, 4, and 12 . from tables 5, 6, and 7, we can see that most of the best results are obtained by rgo. it outperforms other algorithms obviously in terms of accuracy on high dimension functions."
"in order to assess the efficiency of rrd+ in dealing with mobility we compared it to other existing methods studied in the related work section. these protocols are the original rpl (or-rpl), co-rpl [cit], me-rpl [cit], and the reverse trickle timer algorithm (rt-rpl) [cit] . we used four performance metrics to evaluate the efficiency of these protocols: (1) packet delivery ratio; (2) number of dropped packets; (3) average end-to-end delay; and (4) number of control packets. for each network size, we generated 10 different random mobility scenarios. each performance metric is averaged over 10 iterations for each network size. figure 2 shows packet delivery ratio of different rpl variants based on different degrees of mobility. rrd+ outperforms other rpl and its enhancement protocols when the degree of mobility is above 25%. when the ratio of static nodes reaches 75%, rrd+ no longer has an advantage and is even outperformed by mr-rpl and rt-rpl in 60-node scenarios as shown in figure 2d . this is mainly due to the fact that mr-rpl and rt-rpl have a mechanism to broadcast mobile nodes to other nodes in order to avoid being selected as next-hop nodes. compared with mobile nodes, static nodes can offer more stable transmission paths. therefore, mr-rpl and rt-rpl perform better when there are enough static nodes in the network to offer routes for the sink. on the other hand, mr-rpl and rt-rpl perform worse when there are not enough static nodes to chose from. results also show that co-rpl has limited contribution in mobility. this is mainly due to the fact that co-rpl does not offer a method to update the c_id value, which we already discussed in section 3."
"based on adaptive growth behaviors of plant roots, root growth optimization algorithm is present in this paper. [cit] test functions, were used to test its performance. the results were compared with abc, ccpso2, and mde pbx. comparing results show that the performance of rgo outperforms other algorithms on most test functions. rgo has also demonstrated faster convergence speed with acceptable solutions, which helps reduce computing cost, especially time cost. it is very meaningful in dynamic environments with limited computing resources. moreover, rgo is potentially more powerful than other algorithms on functions with high dimensions."
"in this article, we have presented the design and implementation of simpo, a scalable in-memory object persistence framework, and its programming and execution model. simpo provides programmers with a toolkit to exploit nvram for fast persistence in a user-transparent manner, thanks to our management of persistent objects and other metadata. our programming model works with a transactionized function grouping mechanism to support memory persistence with streamlined logging at instant function boundaries only. our execution model for running deferrable functions in groups can maximize data locality and concurrency. persistent objects are made durable through a buffered-dual-copy checkpointing mechanism that effectively masks the slow writes of nvram. experimental evaluations on both emulated and real-life hybrid memory machines confirm that our persistence framework induces runtime overhead of up to 5% only, and helps high-concurrency applications run twice as fast."
"and human-computer engineering ) . through penetrating analysis of the requirements on the interface's design, the authors proposed a context-aware adaptive model for lbs interface along with the framework for its application and described the adaptive process of dynamic interaction between users and the interface. a standard usability evaluation test was conducted to show the performance improvements by the proposed model. research work of this paper provided a valuable reference for the design of adaptive interface based on context awareness."
"there is a close correlation between the architecture and propagation strategies as botanists have discovered. during the growing process, root can perceive their external physical environments and implements different strategies. if there are enough resources, it will produce many lateral roots at the same time of elongating forward. otherwise, few lateral roots are produced. over time, the similar propagation occurs at different positions in variant scales. as a result, the whole root system will cover the most profitable area with self-similar architecture."
"a further extension of the current rgo may result in even more effective optimizing algorithms for solving complex multimodal problems. future research efforts will be focused on improvements of the algorithm, theoretical analysis on self-similar propagation and optimization, and applications to practical engineering problems."
"where κ θ p¨q is a kernel with parameters θ (the choices of kernel will be discussed in the following). given the above estimator, the objective function is written as follows:"
"when it comes to distributed cases, e.g., sensor networks, massive data are collected/stored by each sensor, and data from all sensors are needed in the quantization task to make use of the overall data information. however, due to the limited power and limited communication resource of nodes, transmitting the large amounts of data to a processing center might be a heavy burden for the nodes. in the following, we show that our iterative solution obtained for the centralized case can be easily extended to distributed cases."
"the routing protocol for low-power and lossy networks (rpl) [cit] by the ietf roll (internet engineering task force routing over low power and lossy networks) working group for low-power and lossy networks (llns). the rpl is designed to meet the requirements of many applications, which are mainly suitable for static networks [cit] . rpl defined a numerical metric, called the rank metric, that represents the topological position of a node with respect to the sink in order to avoid routing loops. a node is not allowed to send data packets to neighbours with lower ranks. in mobility scenarios, due to the movement of nodes, the position of a node will change. however, rpl specifications do not offer methods to update the rank in a timely manner. this causes loops when a parent node becomes a descendant node. the trickle algorithm is used in rpl to disseminate control information over the network [cit] . this algorithm is used in static networks in order to reduce overhead. indeed, it reduces control traffic generation rate when the topology is stable. however, in mobility scenarios, the trickle algorithm cannot disseminate information in a timely manner and fails to cope with the changes of the topology."
"sections iii-b and iii-c presented and evaluated two prediction models. on the one hand, the results established that the model with neural network performs better than the constraint programming model, and is also well adapted for real time prediction as it requires no data classification."
"second, some studies, such as heapo [cit] ) and pvm [cit], propose native nvram management schemes by directly improving the operating system. their methods are to extend the virtual memory subsystem to reap the benefits of nvram. they use modified system calls like mmap() and brk() to provide nvram allocator apis."
"as discussed in section 2.3.2, we design a deferrable execution model to make the best use of operation logging. each operation log is space efficient (shown in figure 3) . programmers usually protect shared data in every function, e.g., by lock-based code. through executing a group of dfs of the same persistent object, simpo improves both data locality and concurrency while maintaining the necessary execution order. therefore, simpo can reduce thread contention and shorten the run time of native execution. simpo includes two execution methodologies: combining and concurrency boosting. programmers' hints, i.e., the dependency information such as lock types given, can be channeled to simpo via the optional parameter dhint. this can help simpo further improve performance without runtime overhead."
we select the matrix multiplication in the linpack benchmark [cit] to represent the class of hpc workloads. we treat the matrix as a persistent object and define the multiplication as an if.
"further, we evaluated the time needed by the csp to generate new time series according to the length of the time series. figure 10 shows that for any resolution from 1 to 41, the time needed to generate a time series of corresponding length, that is of length 24 up to 984 is less than one second. this good performance is explained by the paradigm of propagation constraint [cit] that is used to solve the csp model and generate solutions."
"where n refers to the number of successfully received packets. results show that rrd+ suffers from high average end-to-end delay compared to or-rpl, me-rpl, and rt-rpl. the reason is that rrd+ does not have the mechanism to distinguish static nodes and mobile nodes. therefore, compared with me-rpl and rt-rp, rrd+ will select more mobile nodes as next-hops. mobile nodes will indeed increase the delivery delay. co-rpl also suffers from the same problem. the reason for co-rpl is due to the fact that not updating c_id would increase the number of hops during transmission, and hence, increase the delay. in addition, as we only compute delays for received packets, packets generated from lower rank nodes will have a low average end-to-end delay compared to packets generated from higher rank nodes. as with rrd+, higher rank nodes are able to successfully deliver their packets; this makes the average end-to-end delay of rrd+ higher. figure 5 shows the number of control packets of different rpl variants based on different degrees of mobility. results show that rrd+ outperforms co-rpl and me-rpl in all scenarios. this is mainly due to the fact that ripple control message management helps reduce overhead according to rank updating. since rrd+ does not have a mechanism to distinguish static nodes and mobile nodes, there is no significant difference in overhead when the degree of mobility changes. co-rpl periodically broadcasts dio, therefore there is also no significant difference in overhead for different degrees of mobility. or-rpl has the lowest overhead cost. this is mainly due to the fact that trickle algorithm, which is used in static networks, helps reduce overhead. me-rpl suffers from a huge overhead cost when all nodes are mobile and gradually decreases the overhead when number of static nodes increases. this is mainly due to the fact that me-rpl introduces a dynamic dis management and me-rpl will suffer from a huge number of dis messages when there is more mobile nodes in the network. rt-rpl outperforms rrd+, co-rpl and me-rpl. the reason is that the reverse trickle algorithm is a modified trickle algorithm, which can deal with mobility and reduces overhead at the same time. however, it cannot deal with random movement, and thus compared with rrd+ it reduces overhead but does not increase the packet delivery ratio."
root growth mechanism presents a wonderful inspiration for designing a new optimization algorithm. it can be seen that the artificial root growth model has described the original appearance of natural plant roots with few assumptions. corresponding algorithm is consequently given based on the above model. the pseudocode of rgo is listed in pseudocode 1.
"in order to test the performance of rgo, pso, abc algorithm, and de algorithm are employed for comparison as they were widely used in recent years [cit] . according to the state of the art, eventually we select ccpso2 [cit] and mde pbx [cit] to replace the basic pso and de since they have been reported to perform much better than the original versions. in the experiments, twelve test functions, including eight classical functions (table 1 ) [cit] test functions (table 2), are used to test its efficiency."
"creating, sharing, and expanding a persistent object entails access to the pot. we illustrate steps for creating or sharing a persistent object in figure 9 . to guarantee all-or-nothing semantics for pot, the update of pid list is the commit point. the pocreate (detailed in figure 9 ) and porealloc functions update the pid list in the end. however, pofree first deletes the corresponding pid in the list and also checks the existence of other processes in the pid list. pofree also deletes invalid pids in the list. the constructor of the pobase (in section 3.1.1) class internally calls pocreate to create or recover a po with the poid. furthermore, with the same poid, simpo allows multiple processes to get access to the same po. and the destructor of the pobase class invokes pofree. if a fault like a system crash occurs during pot updates, then simpo relies on our po manager (in section 3.4.3) to free the entry left behind and thereby avoids memory leaks."
"when the environments change, roots must adjust their behaviors to adapt to new conditions. in fact, most of the roots are able to respond effectively to the variational conditions even when they meet unexpectedly [cit] . typically, they will stop growing in the area and shrink away from poor conditions, which make the whole root architecture a robust system with high diversity in searching for water and nutrients as much as possible."
"dangling pointers are a critical problem when using nvram. for example, when an application crashes, its page table will be lost and its pos will dangle, i.e., existent but cannot be found. an os crash may result in even more dangling effects. in simpo, users can easily recover a po by calling the constructor function with its poid. simpo searches the pot by the poid and maps the po from nvram to the user space, as shown in figure 9 the pocreate function."
"the remainder of this paper is organized as follows. section 2 talks about some topics about root growth. section 3 models the root growing process. section 4 presents the root growth optimization algorithm step by step. experiments and results are given in section 5. section 6 discusses some unique characteristics of rgo, and section 7 outlines the conclusions."
"in recent years, many heuristic algorithms inspired by collective intelligent behaviors of insects and animals were proposed to solve complex optimization problems. for example, ant colony optimizer (aco) simulates foraging behaviors of ants [cit] . particle swarm optimizer (pso) simulates swarm behaviors of birds and fish [cit] . bacterial colony optimizer (bco) [cit] and bacterial colony foraging optimizer (bcfo) [cit] simulate typical behaviors of bacteria during their lifecycle. artificial bee colony (abc) algorithm simulates foraging behaviors of a swarm of bees [cit] . compared to traditional mathematical methods, these heuristic algorithms have no central control, and performance of the population will not be affected by individual failures. therefore, they are more flexible and robust when dealing with complex, multimodal, and dynamic problems."
"to evaluate the quality of the time series generated, we ensure that it respects all the properties of the time series from the learning set. those properties consist of all the features learned from the real workload traces. to solve the csp model and generate solutions, we use a csp solver. for these benchmarks, we used sicstus prolog solver [cit], on a computer running mac os 10.10.5 yosemite, with 16g0 of memory and an intel core i7 processor at 2.93 ghz."
"according to fitness values, the whole root mass is divided into three groups. the group with the best fitness values is called main roots. the group with the worst fitness values is called aging roots. the rest of root mass is called lateral roots. in the three groups, except for aging roots that will stop growing in the next generation, main roots and lateral roots implement different growth strategies."
"in figure 23 (a) and figure 24 (a), for ifs, simpo shows little overhead (2% and 5% on amd and intel platforms) compared to native. bdb-ft is 21% and 29% slower than native. nvheaps [cit] ) reports around 5% overhead over native. although simpo takes no advantage of multiserver flat-combining for ifs, it still shows a better performance than nvheaps."
"in this article, we propose a new approach to efficient, scalable object persistence using nvram. to evaluate this approach, we implement a c++ application framework, dubbed scalable in-memory persistent object (simpo) (pronounced \"simple\"). simpo basically employs the writeahead operation logging and checkpointing to achieve persistence. with simpo, applications are immune to common failures such as power loss and system crashes. we propose a new programming model coupled with a technique called transactionized function grouping (xfg) to streamline the logging and execution process. this xfg-driven programming model classifies functions into deferrable and instant functions, which can be automatically detected by our provided toolkit. deferrable functions (dfs) are lazy operations defined for a persistent object (po), which access only the fields of the po or local variables, and do not return any value. instant functions (ifs) are operations defined for a po that involve access to variables outside the po, or return any value. for example, mutator methods and accessor methods of a po are classified as dfs and ifs, respectively (assuming the mutators only update the po's fields). for every persistent object, xfg means coalescing a collection of functions into a transaction. the transaction can begin with a df or an if, but it always ends with an if. depending on the classification, simpo can effectively optimize away the overhead of log flushing from cache to nvram, which is triggered only when an if is encountered. this can guarantee persistence while narrowing down the abort window of each transaction. to make the best use of operation logging, simpo includes a deferrable execution model to run dfs in batches. the system runs dfs with one or multiple server threads to improve data locality and concurrency for multicore architecture. we also propose a buffered-dual-copy checkpointing scheme tailored to hybrid memory architecture. this scheme reduces redundant operations for checkpointing and dampens the slow write latency issue of nvram. we also implement efficient po management including recovery of dangling pointers and memory leak prevention, as befits the nature of nvram."
"in order to design a scheduling heuristic that aims at optimizing resource utilization, we need to know in advance the eventual distribution of the resource usage in time. to do so, we need a workload prediction model based on historical traces. this section presents two different approaches to build such a model. the first approach makes use of constraint programming while the second one uses a neural network."
"the above equation depends on the global original data distribution p pxq, which is unknown in advance, and more importantly, is hard to estimate in distributed cases. fortunately, as seen from formula (3), with the use of k-l divergence, the partial differential formula is a mathematical expectation over p pxq. for such a situation, we can employ the robbins-monro stochastic approximation method [cit] to solve the above equation effectively. the r-m method is an online iterative algorithm which directly solves the kind of equation above by using one data sample per iteration without estimating the total distribution of data. thus, it avoids the difficulty of estimating the global data distribution, especially in the distributed cases. this is the reason that we use k-l divergence rather than c-s divergence [cit] to design our objective function in this paper. the iterative solution to our problem given by the r-m method is simple, as below:"
"to overcome these issues, we elaborated a new model based on neural networks, the model is presented in section iii-c and does not need a clustering phase."
"generally, the performance of a vector quantization algorithm is measured by the average (or total) distortion between input vectors and their corresponding reproduction vectors, nois off pjq, where nois off pjq stands for the number of iteration steps (nois) needed before the node j becomes off."
"we also used the neural network to generate time series. to do so, we start from a small prefix, i.e. a prefix of length as expected, none of the time series generated with the neural network respects all of the learned characteristics. another disadvantage in trying to generate a new time series with the neural network is that the neural network needs to start with a non-empty small prefix that it completed to create a full time series. on the other hand, the csp model can rapidly generate full times series from scratch that verify all learned constraints."
"when the program executes, simpo records logs in the log array. the log array has four cursors to indicate the logs' states. when logs in non-resilient states exceed a threshold, simpo conducts bdc checkpointing. when logs in resilient state exceed a threshold, simpo truncates the resilient logs, compresses and copies them to the underlying storage like ssd. the system will delete all resilient logs on ssd when the program frees the persistent object. the thresholds are predefined experimentally to suit most cases. figure 8 shows an example execution flow of a program with the persistent object in listing 3. in this example, when the count of non-resilient logs reaches six, simpo will execute bdc checkpointing. in t1, simpo starts a transaction via xfg for persistent object a. in t1 and t2, the transaction is in the logging phase. the system records two df logs. in t3, simpo commits the logging phase of the transaction and then executes logged functions in the execution phase. in t4, t5, and t6, simpo starts another transaction and records three new df logs. in t6, the system detects that persistent object a has six non-resilient logs. simpo conducts bdc checkpointing. when the program deletes the persistent object (t9), the system frees all data copies and corresponding metadata."
"we design a microbenchmark to compare the native program without persistence support (native) against the settings in table 7 . all variants except simpo and simpo-all execute flush every 1,000 functions. simpo-all flushes every operation logs into nvram. simpo applies if-triggered synchronization. the microbenchmark includes a twodimintarray object of 500 elements and a master thread that dynamically creates a team of slave threads. after all worker threads have terminated, the master thread aggregates the throughput results. to study scalability, we vary the core count from 1 to 60 and 6, respectively."
"moreover, we test our distributed algorithm under the cases of node unbalance. the unbalance degree is classified into five levels. in level 1, for each node, the input data amounts of two different moons (up, down) are equal. in level 2, for five nodes, the percentages of input data of (up, down) moons are (60%, 40%), respectively, and for the other five nodes, the percentages are (40%, 60%). the percentages are (70%/30%, 30%/70%) for level 3, (80%/20%, 20%/80%) for level 4, and (90%/10%, 10%/90%) for level 5, respectively. the total percentages of two different moons over the whole network are kept equal."
"traditional solutions adopt synchronization per log in persistent storage within a transaction execution to guarantee the logs' sequence and durability. for example, spark [cit] ) saves every log on a fault tolerant file system. as a result, persistence of every log via flush and memory fence operations on nvram brings about considerable runtime overhead. nvram write-ahead logging (nvwal) [cit] ) proposes a transaction-aware lazy synchronization methodology for nvram. it works by group-based flushing and memory fencing of the pending logs in the transaction on commit. then the system flushes a commit bit to mark the transaction's commit state. it can optimize performance as it allows logs within a transaction to flush without ordering constraints. however, for nvwal, the \"abort window\" for a transaction lasts from the beginning to the flush of the commit bit."
"the rank value is proportional to the increase of the metric [cit], and therefore we get the rank computation of the node itself, which is shown in equation (2) ."
"dealing with mobility in wsns and llns is a challenging task for which a compromise should be found between efficiency and complexity. one of the most dominant routing protocols designed for llns, rpl, was an original designed without special support for mobility. this is a major drawback that prevents many applications from using it. in this paper, we proposed a mobility enhancement mechanism called rrd+, designed for convergecast scenarios. rrd+ can easily be applied to the rpl routing protocol. rrd+ monitors rssi values and updates rank values accordingly in order to avoid loops, and it also dynamically manages the interval of control messages. rrd+ monitors the direction of moving nodes based on the variation of rssi values. when a node detects that one of its potential next-hops in the parents set is moving away, it will anticipate a link failure and try to use another node from the parents set. this helps nodes update their next-hop choice in a timely manner in mobile scenarios."
"deferrable functions (dfs): lazy operations defined for a po, which access only the fields of the po or local variables, and do not return any value."
is a time series of length 24 that has the same patterns learned from the real workload traces. the next section presents the evaluation of the quality of the time series generated by this model.
"berkeley db (bdb) [cit] ) is a software library that provides a high-performance embedded database for key/value data. we modify the core data in bdb, \"db\" and \"dbenv,\" as persistent objects. bdb provides a native data persistence mechanism via transaction-based checkpoint/ redo logs. we improve the native persistent mechanism to store logs and checkpoints in emulated nvram on amd machine and in nvdimm on intel machine using ram-disk (bdb-ft). we compare three versions of bdb: (1) native bdb without persistence (native); (2) bdb-ft: ported from the native transaction system in bdb to save transaction logs and checkpoints on a ram-disk instead of hard disk, with nvdimm or emulated slow nvram write latency; and (3) bdb with simpo support that is not based on any existent transactional systems."
"to apply flat-combining, we propose a deferrable execution model with xfg. the design includes space-efficient function-level logging and flat-combining across cooperative threads. by analyzing a group of deferrable functions of the same persistent object, simpo can improve data locality and concurrency of function execution. therefore, simpo is more than a classical persistent solution that only degrades the runtime performance; rather, it can lead to speedup."
"figure 4(c) shows the inst function and figure 5 is a possible execution flow of the example in listing 4. in step d of figure 4 (c), simpo first collectively flushes all logs in the xfg-generated transaction. in step e, simpo iterates all pending functions and assigns them to server threads based on dhint. applications with high contention would see higher locality for shared data access and lower overhead of managing contention. as functions are executed with maximal concurrency, simpo also benefits applications with high concurrency. in steps g-i, the system checks whether to conduct bdc checkpointing. in step h, it confirms that all instant logs are in done state before checkpointing. the lock is released after checkpointing. in step i, because the logdone cursor (as shown in figure 3) is updated in step g, the system releases the lock first and then executes the if. after executing the if, it stores the return value via the context pointer in the redo log. therefore, when the application recovers, it can get the results without double execution."
"for the evaluation of this model, we used the same data as in the case of the constraint programming model (section iii-b4). to train the network, we extract prefixes of length 3 to 23 from each time series of the learning set, and feed the network with couple comprising those prefixes together with the next value of the time series. the number of training examples is therefore increased, since for each time series of length 24, from the learning set we extract 22 prefixes of length from 3 to 23. the neural network has 24 neurons in the input layer, one neuron in the output layer and one single hidden layer. the number of neurons in the hidden layer as well as the learning rate η were experimentally set to 65 and 0.2 respectively. figures 4 and 7, shows that both prediction models follow the same patterns. the explanation is that the neural network learns the patterns from the learning time series. the peaks in the rmse prediction curve correspond to time points where there are many possible candidate patterns. the valleys in the rmse prediction curve correspond to time points where the current pattern of the time series has been identified. when the pattern is identified, the prediction is more accurate than when it is not yet identified. this observation remains true for the peaks and valleys observed in the rmse prediction curve of the constraint programming model in figure 4, that is why both curves follow the same patterns."
"in figure 23 (b) and figure 24 (b), for the deferrable-intensive case, simpo shows 33% (up to 88%) and 73% (up to 103%) throughput increase over native on the amd and intel platforms. the speedup of simpo for the put transactions is due to our programming and execution model, which agrees with our microbenchmark."
"the prediction is done in three steps. given a prefix time series (pref (t)), we first determine to which clusters it may belong. the second step gives an interval for the potential values of the prefix time series (pref (t)) at time t + . finally, we refine the interval to make the prediction more precise. of potential values at time (t + ), and note this interval by i cl (t + ). we make the union over the different compatible clusters and denote it by i(t + ). (3) we reduce the size of the interval i(t + ). to do this, we consider the center time series of each compatible cluster cl i and compute the footprint of the patterns strictly increasing sequence and strictly decreasing sequence to identify locations where these patterns occur. we use this information to reduce the interval i(t + )."
"root number explosion is absolutely harmful to the adaptability of a root system. from some points of optimization view, it will make the algorithm plunge into local optima and lose essential diversities since the total number of root apices is rigidly limited. in fact, the phenomenon is rarely seen in natural plant roots because plant hormones play an important role in inhibiting explosive propagation, which has been described in section 2.2."
"3.1. basic concepts. in the artificial model, an objective function is treated as the growing environments of plant roots, and the initial roots are considered as a homogeneous biomass [cit] . each root apex stands for a feasible solution of the problem. all roots try to adjust their growing directions and propagation strategies in order to search for the optimal growing conditions, which feed back to improve root growth further."
"based on these definitions, we show a classification example in table 3 by analyzing six common data structures and their major functions. the dfs like enqueue do not return error codes. if the error handling is necessary, then programmers have two choices. the first one is to ensure success with ifs like isfull. the second one is to write ifs to return error code like enqueuewithreturn. dfs can be commonly found in big data computing workloads. table 4 shows df survey from a the ratio comes from the number of times each application executes dfs and ifs."
"6.2. self-similarity. in early years, biologists have already found that root systems have self-similarity and were considered as approximate fractal objects over a finite range of scales [cit] . until now, the architectural characteristic of root systems has been drawing much researchers' attention [cit] ."
"after the updating, each node sends its local estimates to its neighbors. then each node combines the information from its neighbors to obtain fused estimates of reproduction vectors,"
"in this paper, we have developed a distributed k-l-based vector quantization algorithm. we only transmit limited intermediate estimates rather than original data, and thus communication complexity is reduced and data privacy is protected to some extent. simulation results show that the d-kl algorithm can achieve performances similar to the corresponding c-kl algorithm. besides, both the centralized and distributed k-l-based vq algorithms show more robustness to outliers than the centralized lbg and the centralized som algorithm."
"currently, double checkpointing and cow checkpointing are two major schemes to make data persistent. all nvram write operations during checkpointing are combined with the flush function to guarantee memory persistence. double checkpointing (see figure 6 (2)) requires one checkpoint operation (step 2) and one copy (3)) incurs even more overhead than double checkpointing does. it requires a pair of alloc and free operations, one copy and one checkpoint operations. below we introduce our checkpointing scheme which results in less runtime overhead than these counterparts. figure 6 (1)). each persistent object maintains two copies of data on nvram. one copy pointed by the po's resilient index (an 8-bit integer) is the current resilient copy. the system conducts new checkpointing on the other copy. every checkpointing process involves one checkpoint operation and one resilient index update. simpo requires one less copy operation than double checkpointing and kamino-tx, and no additional operations like (alloc and free) as in cow checkpointing."
(1) regrowing. this operator means that a root apex regrows towards a local best position where there are better water and nutrient conditions. the operator is formulated as the following expression:
"more precisely, from a read-only account on the vmwarevcenter, the solution recovers static and dynamic information. the static information collected is: the datacen-ter/cluster/server/vm architecture, static information of physical servers (server model, cpu model, cpu frequency, cores number, ram available, etc.), static virtual machine information (vcpu (virtual cpu) number, allocated memory, reserved memory, vmware tools status, vmdk size, etc). following the recovery of this information, the probes dynamically monitor physical servers and virtual machines in order to get their consumption and virtual machines lifecycle. various resources are monitored: processor, memory, network and disk. this monitoring is realized every 30 seconds, without impacting vmwarevcenter performance. data are stored in a classical relational database (mysql)."
"at the initial stage, each time a node receives a dio and adds a new node into its parents set, a timer is set for this parent. we call this timer the lifetime of a parent. a parent is kept in the parents set for the duration of its lifetime timer and can be selected as the next-hops. a parent is deleted from the parents set when the timer expires. however, before the timer expires, if a new control message is received from this parent, the timer will be reset and lifetime is renewed. according to the rrd+ mechanism, we set two sorts of lifetimes: long lifetime and short lifetime."
"the gradient descent algorithm: we recall that in a neural network, the learning problem is to find weights w and biases b that minimizes a cost function of w and b. in our case the cost function is the mean squared error function c (w, b) ."
"in today's society, machine learning has been in an extensive demand in the areas associated with human's psychology and behaviors, such as ubiquitous learning, e-commerce, online customer service, behavioral finance analysis, and government emergency management. we believe that machine learning could be the most promising, sometimes even the only, way to accomplish the complex computation on human psychology and behaviors in the ubiquitous environment."
"in the natural world, while most of animals develop toward a predetermined body plan, plants demonstrate iterative growth and constantly produce new organs and structures by actively dividing meristems [cit] to adapt to the differing environments. as another species of biology, however, plant has attracted little attention in the field of bioinspired computing [cit] even though it has evolved for a longer period of time. compared with animal, plant cannot move but grow. there is neither a brain nor neurons in its body. as a result, it seems insensitive to external information, dull to take actions, and far away from intelligence. in some biologists' opinions, however, plant can also be regarded as \"intelligent organisms\" [cit] . during the growing process, plant shows considerable plasticity in its morphology and physiology in response to varieties of environments [cit] . for example, their roots can properly cope with the prevailing conditions in soil, such as avoiding obstacles and exploring nutrient-rich patches or water zones by its hydrotropism, chemotropism, gravitropism, and so on. the iterative propagation mode makes them extremely flexible and adaptive in detecting resources and concentrating their efforts on areas that are the most profitable [cit] . consequently, roots are always able to find the best position with nature-designed growth strategies. this is a perfect heuristic for designing optimization algorithms. thus, inspired by the growth behaviors of plant roots, this paper presents a new algorithm named root growth optimizer."
"the eight classic benchmark functions are widely adopted by other researchers to test their algorithms in many works [cit] . among these functions, sphere is a unimodal function with separable variables which is easy to solve. schwefel 1.2, schwefel 2.22, and rosenbrock are unimodal functions with not change the shape of the function. however, when one dimension of vector is changed, all dimensions of vector will be affected. thus, the rotated function differs totally from the original function in the view of searching."
"programming model with transactionized function grouping. inspired by lazy evaluation in functional languages [cit], we design a programming model based on operation logging that classifies functions into deferrable and instant execution types against an object-oriented background."
"in this paper, we have proposed two original workload prediction models for cloud infrastructures. these two models, respectively based on constraint programming and neural networks, focus on predicting the cpu usage of physical servers in a cloud data center. the predictions could then be exploited for designing energy-efficient resource allocation mechanisms like scheduling heuristics or over-commitment policies. we also provide an efficient trace generator based on constraint satisfaction problem and using a small amount of real traces. such a generator can overcome availability issues of extensive real workload traces employed for optimization heuristics validation. while neural networks exhibit higher prediction capabilities, constraint programming techniques are more suitable for trace generation, thus making both techniques complementary. our future work includes providing a website to access on-demand datasets produced by our generator using various real workloads that cannot be made directly publicly available."
"(1) the first benchmark presented in figure 2 evaluates the percentage of cases where there is at least one cluster that is compatible with the prefix time series. (2) the second benchmark presented in figure 3 evaluates the percentage of cases where the actual (k + ) th value of a test time series belongs to our predicted interval. from the curves depicted in figures 2 and 3, we observe that the quality of the prediction depends on the cluster compatibility part of the prediction as both curves have a similar evolution."
"another issue encountered in designing resource optimization heuristics for data center is the availability of real workload traces to validate the proposed algorithms. in most of the cases, for reasons such as confidentiality, the real data are not available in enough quantities. one of the few available traces in the community comes from google and provides a one-month trace for about 12,500 machines [cit] . yet, this kind of hyper-scale clouds are only minor contributors to the global energy consumption of data centers (under 4%), while small-and medium-sized data centers account for half of this global electricity consumption [cit] . in this paper, we focus on these small and medium energy-hungry clouds that present workload patterns different from the ones experienced in largescale public clouds [cit] ."
"routing loop avoidance is a challenging task in mobility scenarios. in order to avoid this, rpl uses the rank value. the rank of a node must be greater than the rank of all nodes in its potential parents set and nodes cannot forward data packets to nodes with higher or equal ranks. however, rpl does not support rank update. a current next-hop may become a descendant due to untimely update of the rank value, and loops will occur. we propose monitoring link existence and movement direction to allow nodes to update their ranks in a timely manner. the goal is to update the rank of a node when it is about to lose its link with its current parent based on the link monitoring mechanism. algorithm 1 depicts the rank update process. rank_r stands for the rank of the receiver of the control message and rank_s stands for the rank of the sender of the control message. li f etime_l stands for the long lifetime and li f etime_s represents the short lifetime. minincrease stands for the value of minhoprankincrease. figure 1 shows a scenario with four nodes a, b, c, d, e, f and g. a is neighbour of nodes b, c, d, e and f. d and f are neighbours of g. we use a dotted circle to represent rssi threshold_1 of node a and dashed circle to represent rssi threshold_2 of node a. note that due to the nature of wireless signal propagation, in reality both rssi thresholds and transmission range are most likely to look like a cloud that changes from one transmission to another. indeed, in our simulation model we used a probabilistic propagation model to take into account coverage zone instability. b, c, d, e and f will execute algorithm 1 whenever they receive a dio message from a. g will execute algorithm 1 whenever it receives a dio message from d and f."
"for functions accessing independent data streams, simpo assigns them to different server threads to achieve higher concurrency. simpo provides users with a special flag all (shown in figure 3 ) for functions that modify many parts of shared data. for a function with dhint set as all, simpo assigns it to the main server thread and inserts a global barrier blocking other server threads from entry until its execution finishes. this guarantees the execution order."
"monopodial branching. according to monopodial branching strategy, a main root itself regrows to form an axis firstly, and then branching roots appear in the lateral position. as a result, the growth strategy contains three operators as follows."
"if nvram is not enough when executing pocreate or porealloc, then the po manager first checks the pot to clean up entries with empty pid lists. these entries crash in a fault discussed in section 3.4.1 and cause memory leaks. if nvram is still not enough, then the po manager applies the swap policy. it swaps out pos according to an lru policy. it writes the nvram data of selected victim to next-level storage like ssd as a file. then it updates the victim's nv address in the pot entry to refer to the file. finally, the po manager frees the nvram of the victim. when a swapped-out po is accessed again, simpo loads it from non-volatile storage onto nvram. therefore, simpo avoids memory leaks and maintains critical pos on nvram to solve the space usage problem."
"wireless sensor networks (wsns) have been widely developed and deployed in the last decade. their ease of deployment and auto-configuration features have helped them gain this success. typical wsns are mostly static and nodes rarely change positions. however, with emergence of new applications, mobility has become an important requirement. in some scenarios, the monitoring nodes are mobile, such as in disaster response applications where a forest-keeper should be able to request event-related data from sensor nodes while moving inside a forest [cit] . in other scenarios, objects being monitored need to be mobile, for example in animal monitoring applications. sensor nodes attached to cows moving inside a field send information towards sink nodes [cit] . in these scenarios, the degree of mobility varies in various applications. in some applications, only some of the network nodes need to be mobile, such as in farming equipment monitoring [cit] . on a farm, some equipment is installed at fixed points and some equipment is mobile, for example ploughs. in some applications it is required that all nodes be mobile, such as in health-care monitoring. in health-care monitoring systems all patients are equipped with sensor nodes to transmit data to a base station and are free to move inside a hospital [cit] . in this paper, we concentrate on highly-mobile scenarios where a significant part of the network or all nodes are free to move and send data to a fixed sink node. when only few nodes are mobile in a highly-connected network, there will not be a great impact on the topology. indeed, when only few nodes change positions, most nodes do not need to rebuild the routes to reach the destination. however, in the case of a high degree of mobility, the changes in the topology have a greater impact on routes. consequently, routing protocols need to react fast to adapt to the movement before making a decision for a node to select the next-hop."
"it is important to be able to estimate if a link is about the break in order to anticipate and find a new next-hop. in order to do so, we monitor the existence of links and try to anticipate if the link is about the break or not based on the variation of rssi values."
"where w i is the weight of the edge from input i to the node j of the hidden layer, and b j is the bias of node j of the hidden layer. this formula assumes that the neural network has one single hidden layer, but can be adapted to more than one layer."
an artificial neural network is a structured and interconnected group of nodes that reads an input in and computes an output. figure 5 shows a simple neural network.
"as far as optimization is concerned, it can be confirmed that the self-similar propagation is profitable for roots to exploit resource-rich areas rapidly. as a novel search technique, the correlation between self-similar propagation and searching in multimodal continuous space remains to be an interesting problem which will be investigated in the near future work."
"vector quantization is a signal processing method which uses reproduction vectors to represent original data vectors while maintaining necessary fidelity of the data [cit] . as one of the data compression methods which are able to reduce communication and storage burdens, vector quantization has been intensively studied in recent years. a vector quantizer is a system that maps original/input vectors into corresponding reproduction vectors drawn from a finite reproduction alphabet. many famous vector quantization algorithms have been proposed. the linde-buzo-gray (lbg) algorithm [cit] and the self-organization map (som) algorithm [cit] are two of the most popular vector quantization algorithms. based on information theoretic concepts, vector quantization algorithms which aim to minimize the cauchy-schwartz (c-s) divergence or the kullback-leibler (k-l) divergence between the distributions of the original data and the reproduction vectors have also been devised and have been proven to perform better than the lbg and som algorithms [cit] ."
"the selected trace represents six months of activity, for a database of about 2gb. all of the results presented below are based on these traces."
"as shown in listing 3 (lines 16 and 21), the execution of persistent functions is encapsulated in defer and inst functions. as shown in figure 4 (b), logging of dfs is lock-free, as it only requires a primitive sync_fetch_and_add to get index. if the number of pending functions exceeds a threshold, then the system will call an internal empty if sync (as shown in table 5 ) to commit the current xfggenerated transaction and conduct bdc checkpointing (section. 3.2)."
"we also evaluate the recovery time of these applications on the intel machines with six cores as shown in figure 22 . we randomly crash the application at any time and restart the application again with simpo recovery to finish the execution. simpo-recovery is the sum time of the application execution time before the crash, the recovery time and the execution time till application finishes. from figure 22, we can conclude that the recovery time is less than 3% of the total execution time."
"the paper is organized as follows. in section 2, an overview of rpl protocol is presented. in section 3, we present the related work concerning mobility in rpl. section 4 describes our mobility support mechanism. in section 5, we analyse the results obtained when our mobility support mechanism is integrated in rpl. finally, we conclude the paper along with future investigations in section 6."
"our methodologies can be divided into buffered-dual-copy checkpointing, the execution model and the programming model. to analyze every facet of our system, we implement several variants with different mechanisms summarized in table 7 . to show the performance of buffered-dual-copy checkpointing, we implement a data persistence mechanism with cow checkpointing (cow). we also port the native transaction-based data persistence mechanism used in berkeley db (bdb-ft) to the nvram environment. we conduct a series of experiments using microbenchmarks, data structure benchmarks, hpc applications and berkeley db. we compare simpo with all the variants and the nvram-based bdb-ft through their achieved throughput. all benchmark programs are multithreaded and using the pthread library. when assigning threads to cores, we adopt the common proximity-first policy: a thread will not be placed on another numa node until the current node becomes full. all experimental data are reported as averages of five runs. our experiments are conducted on two platforms: a 64-core amd machine and a 6-core intel machine whose specifications are listed in table 8 . the amd machine is used to run an emulated platform. we develop an emulator based on dram to emulate nvram access latency (see table 1 ) for running the experiments in a multicore environment with both dram and emulated nvram. similar to related work [cit], we limit our emulation to write operations only and assume nvram has twice the write latency of dram. we also implement a flush function composed of clflushopt operation for each cache line of the data and sfence to ensure the flush operations have completed, as is the case in other studies [cit] . for non-cacheable writes to nvram, we add proper delays after every flush function. we record the delay time from the beginning of nvram modification to the end of the flush macro based on the processor's timestamp counter. the intel machine has a 16gib nvdimm (an nvram alternative with dram access latency) with which we can validate simpo through a power failure. we conduct evaluations on the intel machine directly."
"we evaluate performance using the well-known tpc-c benchmark [cit] ). the tpc-c benchmark generates transactions with random keys and values. it is a non-deterministic program. we configure tpc-c benchmark to execute its stocklevel transactions with 100% get or 100% put. for simpo, get function is an if and put is a df. we show results in figure 23 and figure 24 ."
"there are some crucial parameters in our proposed algorithms, such as the degree of node unbalance, the threshold value, the number of reproduction vectors, the network structure, etc. in the following simulations, we study the effects of the various parameters on the proposed algorithms in detail."
"because the newborn root apices may be classified into the main root group with high probability in the next generation if the area is really nutrient-rich enough and all main roots will elongate and propagate again, the number of roots in this area may increase explosively in several generations, which we called \"root number explosion. \""
"from formula (4) we can know that the smaller local ( ) is, the more root apices will be removed in the next generation. on the one hand, rapid local increase of roots is controlled in this way so that root number explosion can be avoided. on the other hand, essential diversity can be kept to prevent the algorithm from prematurity."
"2.2.1 nvram technologies. we survey today's nvram technologies and summarize their characteristics in table 1 . phase-change memory (pcm) [cit], being the most mature to date, has three orders of magnitude lower latency than flash memory does . spin-transfer torque ram (stt-ram) [cit] ) offers even lower latency than pcm does and may replace dram in the future. although current stt-ram still has 2 to 4 times longer write latency than dram [cit], stt-ram is often denser, equally fast for read access, and much more energy efficient. as new nvram technologies (e.g., stt-ram, 3d xpoint dimm) do not yet go into mass production, nvdimm is today's alternative. nvdimm, composed of dram, nand flash and supercaps, is an industry-standard dimm form factor that combines dram's performance with nand flash's non-volatility. nvram technologies have great potential to enable faster and more reliable persistence systems later on."
"where 1 and 2 are local learning factor and social learning factor, respectively. compared with formula (7), in rgo, the elongation of main root apex is determined only by the local best position and local learning factor, which means that social learning factor and global best fitness value have no influence on behaviors of a main root apex. the reason contains two points. firstly, as far as natural plant is concerned, there is no biological proof that a root apex can get global information which may be on another apex far away from it."
"to maintain the topology and exchange routing information, rpl defines four types of control messages: dio, destination advertisement object (dao), destination advertisement acknowledgement (dao-ack), and dodag information solicitation (dis). a dodag information object (dio) message is used to carry information that allows a node to discover its neighbours a destination advertisement object (dao) message is used to propagate destination information by a child node to a selected parent node. a destination advertisement acknowledgement (dao-ack) message is sent in response to a dao. a dodag information solicitation (dis) message is mainly used to probe its vicinity by soliciting a dio message from a neighbour node. rpl transmits dios using the trickle algorithm that contains three main parameters i min, i max, and k. at the beginning, a variable i will be set to a value in the range of [i min, i max ], where i min is the minimum duration and i max is the maximum duration separating two consecutive dios. the first dio interval i dio will be randomly taken from the range [i/2, i]. in order to avoid depleting the energy of nodes in a low-power network, the redundancy constant parameter k has been defined. nodes can retransmit a packet to the same node a maximum of k times. whenever nodes receive a transmission, a counter c is used to record the number of transmissions. if i dio expires and the counter c is less than k, nodes will transmit dios. after this transmission, nodes double the interval for the next transmission until i reaches i max . if the transmission is inconsistent or c is larger than k, nodes reset i dio to i min, c to zero and start a new interval."
"nvram is of higher speed than persistent storage like ssd and is byte addressable. in section 3.2, we point out that both double checkpointing and cow checkpointing have redundant operations on nvram, requiring additional overhead. we propose a buffered-dual-copy checkpointing scheme that incurs less overhead for hybrid memory and mitigates slow writes of nvram by using dram portions as a big writecombine buffer. we also reduce the storage overhead of buffered-dual-copy checkpointing using the lru policy to store only the most recently modified objects on nvram."
"we also evaluate the recovery time of the stocklevel transactions on the intel machines with 6 cores. because the tpc-c benchmark is a non-deterministic program, simpo recovers the database persistent objects to continue running berkeley db when a crash happens. the results are shown in figure 22 . we can conclude that the recovery time is less than 0.2% of the total execution time."
"we use the noisy double-moon data as the synthetic experimental data, which are widely used as a benchmark in signal processing and machine learning [cit] . the noise distribution considered is heavy-tailed, which leads to data samples containing more outliers than those under gaussian noise. though, according to the analysis in section 2.3, our d-kl algorithm has advantages over the c-kl on communication complexity when the data sample amount is large, for easy implementation of the simulation, we set a relatively small number of data samples for an individual node, which is 700 (in each run, for each node, 200 samples are used as training data and the other 500 samples are used as testing data)."
"listing 4 illustrates how to utilize the refactored persistent objects in a multithreaded environment. the only rule is to call the generated user interfaces (with the simpo prefix) to access the data of a persistent object. if isolation is needed, then users should apply locking techniques to protect critical persistent functions. simpo provides a wrapped function pthread_create_simpo for thread creation based on the pthread library to record the persistent thread id (potid), which allows threads to find their corresponding logs during recovery."
"manager. simpo needs to avoid memory leaks, which are more pernicious in a non-volatile setting. once a region of nvram storage leaks away, it is difficult to reclaim the region. we design a po manager to solve the problem. moreover, since bdc checkpointing requires more space to improve efficiency, we propose an nvram po swap policy in the po manager."
"first, some studies, such as mnemosyne [cit], nvml [cit], bpfs [cit], and pmfs [cit], employ the direct access (dax) memory-mapped nvram file to provide nvram allocator apis. a memory-mapped file is a segment of virtual memory that has been assigned a direct byte-for-byte correlation with the file. kernels provide page cache to buffer reads and writes to files. for devices that are memory-like, the page cache would generate unnecessary copies of the original storage. current kernels provide dax memory mappings. dax removes the extra copy by performing reads and writes directly to the storage device. therefore, the kernel can directly map the whole nvram into the user space."
"with mean vector m i, precision matrix σ i, and degree of freedom υ i . if υ i ą 2, the covariance matrix of the distribution is υ i pυ i´2 q´1σ i . as υ i tends to infinity, the t-distribution converges to the gaussian distribution. equipped with this heavy-tailed kernel, the vector quantization is supposed to be more robust to outliers."
"biologists have discovered that there is a mutually inhibitory interaction between auxin and cytokinin. if one of them increases out of balance, the other will promote the signaling of inhibitors [cit] . for instance, when roots grow rapidly in a nutrient-rich area, abundant cytokinin will be synthesized in meristematic cells of newborn roots. nevertheless, there is no proportional auxin yet provided in time by polar transport. thus, cytokinin signaling will promote the expression of auxin signaling inhibitors so as to form a negative feedback loop until the root growth rate returns to a balancing level. in summary, roots will never propagate explosively in a local area even if all environmental conditions meet their needs."
"flow. simpo provides two levels of recovery: persistent objects and the application. many frameworks, such as mapreduce [cit] and spark [cit], use recomputation for persistent data within a job and require user code to be deterministic. we provide the application recovery under the same deterministic assumption. if only persistent objects need to be recovered, then simpo can support both deterministic and nondeterministic applications."
"on the other hand, the advantage of the constraint programming model is that it learns a set of constraints that characterizes the time series of each cluster. this very same model can then be used to generate time series that are compatible with a given cluster. real data on server workload are in general not available in large quantities. such a generator is very useful as it can generate as many data as needed. the generated data have the same patterns as the real workload data from the learning sets. the two models are thus complementary, as each one has its strengths. this section describes how new workload traces are generated in two steps from real workload data set. the first step builds a constraint satisfaction problem out of the available real workload traces, while the second step solves the problem to generate new traces. before detailing the two steps, we recall some background on the notion of constraint satisfaction problem."
"shrinkage. both monopodial branching roots and sympodial branching roots consume local resources to keep growing all the time. the decrease of local resources may lead to loss of activity of roots in nondynamic environments. according to the model, if a root axis has been active for a long time and still fails to join the elite team, it will cease to function and be classified into the aging group in the next generation. in another word, the root system will shrink away from this area after the local resources are exhausted."
"the evaluated hpc applications highly parallel with little lock contention, and most operations in them are ifs. from the results, we can conclude that persistence support using simpo shows little runtime overhead for common applications."
"we present two complementary models, one based on neural networks and the other one based on constraint programming to address these issues: workload prediction and lack of real traces. therefore, the contribution of this paper is twofold."
"to analyze the influence of the xfg-driven programming model, we test simpo-no, simpo-all and simpo-group in table 7 with slave threads to repeatedly execute the allinc function. simpo-no executes the flush function every 1,000 functions. and simpo-all uses the flush function to ensure every operation log is persistent on nvram, which guarantees the logs to flush in order as shown in figure 2(a) . simpo-group applies our if-triggered synchronization as shown in figure 2(c) . figure 14 and figure 15 show the throughput results of the microbenchmark. simpo-no and simpo-group show similar performance that proves that the if-triggered synchronization successfully reduces the flush overhead. from simpo-all, we conclude that the flush function causes 20% overhead on both platforms. with the if-triggered synchronization, the performance of simpo-group should lie between simpo-no and simpo-all and varies with the df:if ratio."
"(1) initialize the positions of root apices (2) calculate the fitness values of each root apex (3) while not meet the terminal condition (4) divide all the root apices into main roots, lateral roots and aging roots (5) for each main root apex regrow with the root regrowing operator branch with the root branching operator evaluate the fitness value of new root apices implement inhibition mechanism of plant hormones end for (6) for each lateral root apex produce a new apex replacing the original one end for (7) implement shrinkage operator (8) rank root apices and label elite roots (9) end while (10) postprocess results pseudocode 1: pseudocode of rgo."
"where max and are the maximal iteration number and current iteration number, respectively. ini is the initial standard deviation depending on the value of searching range and fin is the final standard deviation determined by expected accuracy standard in the program."
"the state-of-the-art of rpl mobility enhancements show that most of the proposals are suited for networks with few mobile nodes. indeed, in this paper we will show the performance of these proposals when the network has few fixed nodes or only the sink as a fixed node. we will also propose a mechanism that is suited for such scenarios."
2) learning time series with a neural network : this section presents the design of our neural network as well as how it is trained to predict future values of time series.
"there is only one rule on writing a c++ class for making persistent objects. all data should be allocated in the contiguous virtual memory referred by the poroot pointer, which is inherited from pobase (see listing 1 line 2). programmers can then use our tools to generate their own classes for creating persistent objects. first, the automatic df/if classification tool adds directives to declare persistent functions (see listing 2). it analyzes codes following df and if definitions and checks whether a function accesses only data of the persistent object or its own local variables without returning values. for any function call in a persistent function, the tool tries to find the source code of the called function. if the source code is not available, then the persistent function is treated as an if by default. otherwise, the tool goes on the classification by recursively checking whether all code along the function call hierarchy is deferrable. the tool also outputs which lines make a function instant, as a code optimization suggestion for the programmer. second, our code refactoring tool handles preprocessing of all the added directives and generates a new source file like listing 3; it generates user interfaces with the function prefix simpo (lines 13-29). the interfaces allocate the generated context structure on nvram and then call the persistent functions via the defer and inst functions in class pobase, providing programmers with an optional parameter dhint to specify dependency hints. functions without the dhint parameter use the default value all (-1) (lines 26-29)."
(2) branching. this operator means that a root apex produces some new apices around it. the number of newborn root apices is calculated as follows:
"recovery of persistent objects. to recover a persistent object, simpo first checks the resilient index value. if the value is masked, then the system will directly recover the dram copy from the corresponding nvram copy, change the last instant log as the resilient state, and unmask resilient index. the recovery is finished in this case. if the value is not masked, then we first recover the dram copy from the corresponding nvram copy. then simpo will check the log array from the logckpt cursor to execute all non-resilient state logs. due to the xfg-driven programming model, all persistent objects are guaranteed to recover to the last states before the aborted xfg-generated transaction. as shown in figure 8, simpo recovers object a from the nvram copy."
"to validate simpo, we adopt the same method presented in heapo [cit] ) on an amd machine. we ran a set of stress tests (including microbenchmarks and applications) thousands of times with up to 60 threads to check the memory safety and the correctness of the execution. to test the recovery, we ran tests and killed the program at random intervals. we observed no fault and error during execution. the recoveries and consistency checks all succeeded."
"we consider a general network modeled to be a connected graph with no nodes isolated. each node is connected to several nodes which are called neighbors. each node communicates only with its one-hop neighbors. in this case, we introduce the diffusion cooperation among nodes to develop the corresponding distributed vector quantization algorithm. the distributed estimation algorithm with the diffusion cooperation consists of two steps, local updating and fusion-based, on information exchanging."
the core idea behind the training of a neural network that predicts future values of time series is to learn a mapping from a prefix time series to its next value.
"the rest of the article is structured as follows. in section 2, we present the technical background and challenges behind this work. section 3 details our design of runtime logging, function 200,000-500,000 3,000,000 10 5 ∞ execution, checkpointing schemes, and persistent object management. experimental evaluation is given in section 4. finally, section 5 concludes this article."
"computing. the floyd-warshall algorithm, which finds the shortest path in a weighted graph with edge weights, represents a classical graph computing workload. we build the graph using our data structure library and encapsulate weight computation as an if."
"a long lifetime is given to the parent node in the case that the new rssi is bigger than threshold_1. in the case the new rssi is smaller than threshold_1, movement direction will be estimated first and a short lifetime is given to parent node when the movement direction is towards parent node, otherwise the rank value will be updated. when the rank value is updated, the current parent is removed from the parents set. indeed, when the parent node is in a zone where the radio link is about to fail, a short lifetime will help avoid using this parent node for a long period in case we no longer receive its control messages."
"while processors are moving towards many cores to support high-performance computing (hpc) and big data workloads, reliability becomes an increasingly important concern, since a single fault in the system could have wasted hours to days of computation. there are a plethora of solutions adding fault tolerance to computing systems or parallel applications. they commonly employ checkpointing or journaling mechanisms to save state snapshots in persistent storage, so the system or application can restart from the last state in case of any failure. depending on the level of implementation (application, library, compiler, runtime, operating system, or hardware), they differ in terms of transparency, granularity of persistence, and checkpointing or logging overhead. for object-based applications, it is a common practice to use persistent object store apis [cit] to achieve fine-grained or selective persistence. however, besides burdening the programmers, the associated object serialization and disk access overhead negates the advantages of using them for high performance. even using modern flash drives, the performance gap between main memory and storage is still obvious (access time of tens of nanoseconds vs. tens of microseconds). these years, nvram technologies like mram and 3d xpoint dimm have started to materialize. they have strong implications for the enabling of faster, more energy-efficient, and more reliable computing. nvram brings forth a persistent memory era in which persisting objects to disk-based database or file storage is no longer a decent solution. research is emerging on the use of nvram for data persistence stores [cit], which mainly focus on logging and checkpointing mechanisms."
". we select pbzip2 [cit] ) to represent i/o-intensive workloads. we modify the queue that records the zip or unzip progress, making it a persistent object using our data structure library."
"as we see from (7) and (8), though the objective is to minimize the divergence between the global data distribution ppxq and the distribution of reproduction vectors, we do not need to know (or estimate) ppxq, which is nontrivial in the distributed environment. each node only needs its own data in equation (7). each node repeats the above process until its fused estimates converge. in detail, we let a node come to off state when the maximum change of fused estimates during a period of iterations is less than a threshold. off nodes stop computation or communication and their neighbors use the last results transmitted by the off nodes to continue updating and fusion. the algorithm ends when all nodes become off. as the diffusion cooperation process goes on, local data information is diffused over the whole network without transmitting the original data. finally, all nodes obtain consistent local estimates of reproduction vectors based on global data information."
where dio_interval dynamically changes due to the change of rank of nodes in mobility. the base_interval is the smallest dio_interval. rank stands for the current rank value of a node. the time_unit is the incremental step in the dio frequency.
"to the best of our knowledge, it can only get local information by hydrotropism, chemotropism, gravitropism, and so forth. secondly, from the perspective of optimization, actual global optimum does not always locate near the temporary global optimum found so far in multimodal environments. there is even no direct evidence that one can find the global optimum with higher probability while running towards the temporary global optimum. in comparison, local learning is necessary because a group of individuals may be in pursuit of the same actual local optimum in a unimodal area, where running towards a better solution is reasonably beneficial to one's own fitness improvement. as a result, social learning does not work as effectively as people expect in the presence of elite strategy. since roots implement \"the fittest propagate\" principle in rgo, temporary global optimum is not worthy for all root apices to follow."
"nvram is non-volatile and byte-addressable at the memory level, setting it apart from modern block-addressable flash drives. exploiting nvram to achieve data persistence entails providing nvram allocator apis to user-level programs. prior proposals pivoted around mainly two methodologies as follows."
"in this subsection, we provide an analysis of the communication complexity of our distributed algorithm. in each iteration loop, each node transmits m local estimates of reproduction vectors to its neighbors. letˇˇb jˇd enote the number of its neighbors, and then the communication complexity for one node in one iteration loop is opˇˇb jˇm q. let t j denote the number of iterations executed by the node, then the total communication complexity for the node is opˇˇb jˇtj mq. on the other hand, the traditional centralized algorithm needs to gather all input data to a processing center. let n j denote the number of input vectors of node j, then the communication complexity is opn j h j q, where h j is the number of hops from the node to the central processor. since the number of input vectors usually is very large compared with the other quantities, the proposed distributed algorithm can significantly reduce the communication complexity in such cases."
"despite growing research efforts to design persistence systems exploiting nvram, there are still open challenges regarding performance. first, most persistence systems [cit] are using transactional semantics to execute programs. a transaction usually contains multiple modifications, each of which generates a log. a general transaction system calls fsync or flush to persist every modification's log once generated. if a crash happens before the commit point, then the transaction aborts and the system recovers to the last state before the transaction began. further research on reducing the overhead of persisting logs and minimizing the abort probability is necessary for tapping into the real performance advantage of nvram. second, many systems [cit] employ a combination of modified value logging (a.k.a. aries-type physiological logging) [cit] ) and checkpointing to achieve persistence. other studies [cit] utilize transactionally consistent command logging (a.k.a. operation logging) and checkpointing. although command logging can reduce the number of logs written to nvram, it is challenging to use it for striking a balance between execution speed and recovery efficiency. third, existing work [cit] largely borrows the checkpointing schemes used in flash-based persistence systems. they either keep and synchronize a complete data copy elsewhere as a backup snapshot all the time (i.e., double checkpointing) or employ the copy-on-write technique to make a temporary backup copy when checkpointing (i.e., cow checkpointing). since writes on flash memory or even nvram are slower than on dram, it remains a challenge to design a good checkpointing scheme with fewer persistent data writes for nvram. last, apart from dram-like memory management, persistence systems require further data management to support data recovery. most of the current work implements essential persistent data management based on either native heap management or a dax memory-mapped file (managing the whole nvram as one file). their efficiency measured is without features in a reallife system, such as recovery of dangling pointers and prevention of memory leaks into nvram."
"finding real traces for data center's activity is very difficult. for this, we have partnered with a french sme company: easyvirt, a company specializing in virtualized data center analysis. in part of its activities, easyvirt deploys software probes in the infrastructure of their clients and archives data in a mysql database. these probes collect system resources' consumption of the physical servers and virtual machines. for confidentiality reasons, the companies in which the data were collected are not cited. similarly, we can not distribute collected raw data. but, the analysis can be distributed. for this paper, we selected a representative trace of a mid-sized data center embedding 50 physicals servers for about 1,000 vms."
"according to the ieee 802.15.4 mac (medium access control) layer, nodes keep retransmitting frames until the number of retransmission attempts reaches a fixed maximum value after which the frame is dropped. the number of maximum retransmissions is three according to the standard. the number of dropped packets is the number of packets that are dropped after exceeding the maximum number of retransmission attempts. this evaluation metric shows how efficient the routing protocol is in finding a new parent when the link with the current parent is lost. figure 3 shows the number of dropped packets of different rpl variants based on different degrees of mobility. results show that rrd+ outperforms all other rpl variants in all scenarios. this is mainly due to the fact that rrd+ helps rpl to adapt quickly to mobility and efficient rank updating can pro-actively help nodes avoiding selecting a next-hop with a bad link quality. therefore, rrd+ reduces retransmission attempts and the number of dropped packets is reduced as well. co-rpl does not have proactive features, thus it suffers from a high number of dropped packets. with the increasing of the number of static nodes, me-rpl and rt-rpl begin to show their performance for the reason discussed previously in packet delivery ratio results. figure 4 shows the average end-to-end delay of different rpl variants based on different degrees of mobility. the delay is computed on the delivered packets, lost packets do not appear in these results. the average end-to-end delay is computed according to the following equations."
"however, in doing so, the machine learning research needs to pay attention to the following new aspects which may be beyond the ability of computer science and technology and requires more novel interdisciplinary ideas and methods: (1) a systematic model such as the social neuroscience mechanism [cit] which can describe the neural activities and dominant process of human psychology and behaviors and helps the machine to understand its globally structural features and therefore reduce the computational cost by learning from the limited samples of a big data set, (2) the comprehensive context awareness in physical, cyber, and psychosocial spaces, as well as the information fusion processing and computing ability, which has been called cyber psychosocial and physical (cpp) computation by dai [cit], and (3) smart learning that enables the machine to cope"
"to compute the rmse of our prediction using the constraint programming based model, we considered the center of the predicted interval i(t + 1) to be the predicted value. figure 4 presents the rmse of the prediction for each prefix length from 3 to 23. the average value of the rmse for all prefix lengths is about 2800. this is satisfactory since the values of the time series used for these benchmarks range from 3000 to 29000. however the quality of the prediction relies on the quality of the classification of time series into clusters. meaning that, the results may go worse if new time series that we fit into the system are not correctly classified. since we are building a real time system, we can not neglect this aspect as new time series are also classified in real time."
"as discussed in section 2.2.2, prior studies apply mainly two methodologies to provide nvram allocator apis. we implement persistent object management (see figure 1 ) depending on these apis to support recovery, avoid memory leaks and provide an nvram swap policy. all persistent data are defined as objects. each po has an identifier (poid), data copies for checkpointing and a log array, which is used in our execution model. to support recovery, simpo maintains a global persistent object table (pot) with metadata of pos. we illustrate the overview of persistent object management in figure 9 . simpo provides easy-to-use object-level interfaces in table 6 . table. to achieve consistency and durability, simpo maintains the metadata of pos in a pot. each table entry consists of the poid, object size, a pid list of the processes sharing the po, and nv address, which points to an extensible one-level nvram page table. we store the pot in a reserved extensible zone of nvram and keep its start address as a constant. pot is the most frequently accessed data structure in simpo, which requires considering nvram's wear management. therefore, simpo needs to move pot to write nvram evenly. simpo first allocates and reserves another range of nvram and then copies the pot data. after a successful copy, simpo updates the start address of pot as the new constant. this wear management only needs to be executed after a relatively long time (e.g., every month)."
"3) online step, prediction step: this section presents how we use the model built in the offline step to predicts the next values of a time series at run time."
"to find w and b that minimizes the cost function, we use the gradient descent algorithm [cit] . in all what follows, we we need to choose δv such that δc is negative i.e such that c decreases."
"in sympodial branching mode, the root apex produces a new branching apex at the lateral position instead of regrowing along the original direction, and the new branching apex grows into an axis by replacing the original one. the new branching apex may locate at a random position around the original root with a random angle . this strategy is formulated as follows:"
"as for data structures, rbm, linpack, pbzip2, floyd, and get transactions for bdb, simpo shows negligible overhead (mostly lower than 5%) for instant-intensive cases. the overhead comes from logging, checkpointing and po management. these are well optimized in simpo with a highly scalable design. from microbenchmark and put transactions for bdb, we conclude that simpo is scalable for multithreaded execution in deferrable-intensive cases. simpo can run faster than state-of-the-art persistence solutions and even the baseline execution without persistence support. for recovery, simpo spends 7.5ms on checking every million resilient logs."
"in rgo, it can be seen from formulas (2) and (3) that when a main root apex has a good fitness value, it will be vigorous for propagation. according to formula (3), along with main roots elongating into the soil, the positions of their newborn daughter root apices comply with the same distribution law at different time points, except that distribution range becomes smaller and smaller. in the meanwhile, newborn roots may become main roots in the next generation and propagate in the same way. with this pattern, approximate fractal architecture with self-similarity characteristics will be shaped."
"by construction of our csp model from the learned features, and by definition of a solution of a csp, every solution i.e. every generated time series respects all features that the model learned from each cluster. we confirmed this theoretical expectation by checking weather each generated time series of length from 24 to 984 respects the properties extracted from the respective learning sets. for each length of time series, there are at least 40 learned properties, modeled as constraints."
"from comparison shown in tables 3-7, we can see that rgo is a very promising algorithm. it has manifested quite strong optimizing ability on test functions. when dimension of functions increases, rgo shows more obvious advantage than other evolutionary algorithms."
"for more details, simpo uses the resilient index with a log-resilient mask (value 8) to ensure recovery correctness (see figure 6 ). after the checkpoint operation finishes, simpo updates resilient index with the mask. then, simpo updates the last instant log as resilient state and updates the logckpt cursor (shown in figure 3) . finally, simpo updates resilient index without the mask."
"the environment around roots is an open and dynamic system varying with the time. in a local optimal area, water and inorganic nutrients may diffuse gradually because of the gradient effect. meanwhile, roots themselves keep consuming resources all the time. therefore, the soil will become more and more infertile unless external resources are added in time."
"the convergence results of classical benchmark functions with 30 dimensions are present in figure 2, which prove that the rgo can converge much faster than other algorithms to the best results on most of functions except schwefel function. [cit] test functions in figure 3 . additionally, convergence characteristics show that rgo usually converges quickly to an acceptable solution with fewer generations, which can be seen typically in (a), (b), (c), (d), and (g) of figure 2 . figure 3 also shows the features. it is a proof that heuristic information of root growth in the algorithm always works well so that appropriate searching directions can be guided by main roots. only the shifted rotated griewank function without bounds (as shown in figure 3(d) ) makes an exception. obviously, the lack of bounds in this function makes challenge to the searching capability of rgo. therefore, the strategies of searching in variant-scale area, for example, large scale area, should be reconsidered in the algorithm."
"combining methodology. when executing pending dfs, simpo assigns contending functions to the same server thread. each server thread maintains a queue to record functions and combines their execution, thereby increasing the data locality and reducing the contention on shared data."
"where, μ i is the mean of object in s i . once this preliminary step has been done, one can start learning the characteristics of the traces in each subset."
the range gives information on the amplitude of variation of a feature. thus we can restrict the features of interest for the model of the cluster by setting a maximum allowed range. a time series ts is thus compatible with the model m(cl ) of the cluster cl if and only if :
"in this paper, we only study convergecast scenarios to different mobile degrees. in these scenarios, there is only one data collection sink node that is fixed and does not change its position. other nodes of the network are mobile or static according to the degree of mobility and generate constant bit rate traffic towards the sink node."
"in the era facilitated by the internet of things, ubiquitous communications as well as cloud services, sensing means, and human-computer interfaces are becoming all-pervasive and online. this makes it more possible for us than ever before to study engineering problems, human activities, and social behaviors through machine learning analysis of the big data produced in the ubiquitous environment. looking at the recent history and new trends, machine learning has made attractive progress in wide areas of applications, from natural language to nonverbal communication, from engineering application to humanities, arts, and social studies, and from the real world to cyber space."
"by equation (2) a node computes the rank value of itself and then broadcasts the rank value using a dio message. a node that receives this dio message may use equation (1) to compute the rank value of the sender. if the rank of the receiver node is higher, it means this control message is from a potential next-hop. note that the receiver node adds all senders with lower ranks to its parents set."
"we implement a library of six common data structures with the same interfaces as in the c++ standard library. table 3 shows the common apis and their classifications. these structures normally can be implemented with an array or linked list. simpo is designed to harbor large size persistent objects, such as an in-memory key-value store [cit] . it is not suited for maintaining small size objects, for example, i-node or socket. therefore, we apply array-based implementation to these data structures. for every data structure, we run the microbenchmark with each slave thread, randomly calling apis. figure 18 and figure 19 show the execution time of simpo, hybrid, normalized to native. we find that simpo incurs a certain amount of overhead for short ifs, like those in instant-intensive structures (queue or stack). however, for functions in tree or graph, simpo shows negligible overhead. the reason is that simpo has constant overhead, e.g., log recording and checkpointing, for each persistent function. therefore, the overhead ratio of execution depends on the length of persistent functions. in this sense, simpo favors big data and hpc applications using defer-intensive classes tree or graph."
"in formula (3) we can see that when the value of increases during the iterative process, will become smaller and smaller. in this way, the similar architecture will appear in variant scales. as a whole, all root apices will form approximate self-similar architecture."
"where m i is the mean vector and σ i is the covariance matrix. inspired by the works on robust mixture modeling using t-distribution [cit], in this paper, we also introduce the multi-dimensional student's t-distribution as the heavy-tailed alternative choice of kernel,"
1. use parts of the node's local input vectors to iteratively update the estimates via equation (7). 2. transmit the local estimation results to neighbors. 3. fuse the results from the neighbors to obtain fused estimates of the reproduction vectors via equation (8) .
"initialization: initialize the threshold value, the kernel parameters, and reproduction vectors for each node. computation: each node performs the following process until the termination rule is satisfied."
"where max and min are the best and the worst fitness values in the generation, respectively. is the fitness value of the original root apex. max and min are the maximal branching number and the minimal branching number which are preset. the positions of new root apices surround the original root apex with gauss distribution (, 2 ). the standard deviation is calculated as follows:"
"we compared rrd+ to other existing mobility enhancements for rpl and standard rpl. simulation results show that the rrd+ mechanism enhances rpl on many levels: successfully delivered packets, packet loss, number of dropped packets, end-to-end delay, and overhead. however, results also show that rrd+ is suited for networks where more than 25% of nodes are mobile nodes. when there are fewer mobile nodes in the network, rrd+ has a limited contribution to network performance."
"we run the microbenchmark with each slave thread calling the allinc function on the amd machine. we inject a fault that causes the program to crash when the sum of finished operations among threads reaches 640,000. the recovery includes two parts: recovery of persistent objects (data) and recovery of the application to continue execution (app). figure 16 illustrates the recovery time against an increasing thread count. without any data persistence mechanisms, directly rerunning the application (re) takes a similar amount of time as the native program execution (native). with simpo, recovery of persistent objects is fast, taking 0.8-3% (less than 40ms) of the native program execution time. following the recovery flow in section 3.3.2, simpo takes 4-10% to recover the program and continue the execution."
"the trickle algorithm is used by rpl in order to reduce overhead in static networks. the trickle algorithm starts with a short interval between two dios in order to construct the network fast. each time, the interval will be increased until it reaches i max . however, when the interval increases, the trickle algorithm cannot cope with information update in a timely manner, which is important in mobility. therefore, we propose a new dio interval management called ripple control message management, which copes with topology changes and reduces overhead. this algorithm dynamically modifies the sending interval of dios according to rank updates as described in what follows. dio messages are broadcast by the sink node and propagated by other nodes until they reach leaf nodes. the main function of dio messages is to help children nodes find parent nodes. if we consider the rank as a virtual range, based on the sink, the rank in the network will be like ripples in the water and the sink is the center of the ripples. we think a dio message that comes from a lower rank is more important than a dio message that comes from a higher rank due to the fact that lower rank dio messages will help more nodes find parent nodes. thus, nodes that are closer to the sink should send dio messages more frequently and the frequency of dio messages may be reduced for nodes with higher ranks. hence, we designed a dynamic dio interval management according to rank updates to reduce overhead. the dio interval calculation is shown in equation (3)."
"in this paper, we develop a distributed divergence-based vector quantization algorithm that can solve a global vector quantization problem without transmitting original data among nodes. we firstly start from the centralized case. considering the limitations in distributed cases, we define the objective function based on the k-l divergence between the distribution of global original data and the distribution of global reproduction vectors, and then use the robbins-monro (r-m) stochastic approximation method [cit] to efficiently solve the divergence-minimizing problem online. we show that the obtained iterative solution for the centralized case can be easily extended to distributed cases by introducing diffusion cooperation among nodes, which is a frequently-used technique in distributed processing [cit] . under the diffusion cooperation, each node cooperatively estimates the reproduction vectors with its neighbors by exchanging some intermediate estimates rather than transmitting original data. simulations show that the local estimates obtained at different nodes are quite consistent. besides, the performances of the distributed algorithm are very close to the corresponding centralized algorithm."
"we implement three training algorithms of machine learning models: logistic regression, auto encoders, and restricted boltzmann machines (rbm). we treat the training data as persistent objects and define the train function as a df for every iteration. the barrier synchronization is an if. as these algorithms show similar results, we only present the rbm results."
"a crashed deterministic application can easily recover through restart. the recovery time is largely reduced, as figure 8 illustrates. the application fig. 9 . management of persistent objects in simpo and steps for creating or sharing a persistent object."
we modify and evaluate four widely used applications with machine learning and hpc workloads on two platforms. figure 20 and figure 21 show the performance results. native represents the official version of hpc applications without fault tolerance and running with only dram memory.
"the development process and architecture of a root system are also determined by internal interactive action of all kinds of plant hormones. among them, auxin and cytokinin are well known to be the two crucial hormonal signals, and root growth is mainly regulated by their cross-talk [cit] . both of them can be generated by meristematic cells. as far as we know, auxin is a key factor for elongation of cells which is mostly generated on the shoot and transported to root tips [cit], while cytokinin works locally to enhance the rate of cell division which is mostly generated in roots [cit] . only with certain ratio of auxin to cytokinin that root grows and develops into regular architecture [cit] ."
"to simulate the inhibition mechanism of plant hormones in the model, we will calculate the local standard deviation local ( ) of new apices produced by a main root and then get rid of some apices according to the calculating results by greedy principle. the operator is implemented as follows:"
"where d is the distance in kilometres between the antennas, and f is the frequency in megahertz. shadow stands for the gaussian random variable in the interval [−2, 2] with mean value of 0 and a standard deviation of 1. with this random behaviour, we fixed the hysteresis value to −1. this means that if the difference between the old rssi and the new rssi is 1db, we consider that it is most probably the effect of the randomness of the propagation model and not the effect of mobility (note that the randomness of the propagation model is related to the nature of the environment and that the value of hysteresis is closely related to the randomness of the propagation model). table 1 summarizes the rest of the simulation parameters we used."
listing. 3 : example of twodimintarray : refactored code. simpo generates persistent functions and provides wrapped user interfaces with prefix simpo calling internal defer or inst functions.
"in this experiment, we compare three different reconstruction approaches: the conventional fbp method, image domain iterative iris, and raw data domain irt with the proposed methods. we have used 10-iterations with a~2"
"the architecture description languages (adls) [cit] have been defined to precisely specify a software architecture consisting primarily of functional components described in terms of their behaviour, interfaces and interconnections. the existing adls often have a specific characteristics related to their motivation, their use and possibly the associated formal semantics [cit] . due to the increasing complexity of the hardware/software components interactions, these languages become difficult to use in some aspects of the development cycle of software systems, especially the specification of deployment mechanisms and reconfiguration. in fact, the deployment specification of software architecture is usually \"mistreated\" despite that the consistency of the operation of installing software entities on those of hardware type [cit], must be ensured for designing applications."
"composition: as in any graph type, the composition of bigraphs creates a new bigraph by combining two or more bigraphs. the composition g h f  o of two bigraphs f and h is a hosting operation of bigraph f ( fig.2 ) in the bigraph h. thus, it is necessary that each region of f has a free site in h, i.e., there are enough free sites in h to contain all the regions of f (one site per region). the connection of the two bigraphs is achieved via a matching between outer interfaces of f with inner interfaces of h. fig. 2 shows how a region (bigraph f) can be hosted in a site of a contextual bigraph h."
note that the proposed method exhibits significant modeling error reduction in terms of visual evaluation and nrmse taking into account the nonlinear scaling of log-log plot.
"modern ct systems are highly complex, and different reconstruction algorithms go to various lengths to model such complexities. in this paper, we show that the accurate modeling of system components such as focal spot area, flying focal spot, and active detector area can make a significant difference in the quality of reconstructed images."
"in the in vivo study, we only show clinical evidences of the proposed approach with subjective assessment. the proposed method is compared with conventional fbp and image domain ir algorithm (iris) in a low dose scan. in this case, we used the same raw data for image reconstructions. this study was conducted in compliance with the health insurance portability and accountability act (hipaa) and used a scan protocol approved by the massachusetts general hospital institutional review board (irb). we obtained written informed consent as per federal u.s. guidelines. all procedures in this study were performed in accordance with the approved protocol."
"computed tomography (ct) is one of the most commonly used diagnostic imaging modalities in modern medicine. ct enables rapid, non-invasive image acquisition at high resolutions. however, ct also exposes the patient to radiation [cit] . ct dosage can be decreased by lowering either the voltage or the flux. lowering the voltage implies that the emitted photons are less energetic, reducing their ability to penetrate through the body. lowering the flux reduces the number of photons emitted, further degrading the signal-to-noise ratio of the acquired data. therefore, the consequence of low-dose ct imaging is that the resulting images are considerably noisier than images acquired with todays clinical doses [cit] ."
"on the basis of a common set of nodes representing the physical or virtual entities of a distributed application, a bigraph is formed of two independent structures: the places graph, having the structure of a forest that shows the spatial distribution of the application, the links graph is an hypergraph establishing the model of connectivity between various nodes [cit] . while an arc in the places graph shows the relationship of spaces between the nested elements of the application, an arc in the graph links establishes a connection between the ports of these elements. the two structures are orthogonal, so links between nodes can cross locality boundaries. each tree in the places graph represents a region of space that can contain sites, corresponding to the leaves of the tree, and where other bigraphs can be hosted."
"the deployment of a software application on a specific execution environment is crucial and delicate stage that can guarantee its efficiency. in this paper, we proposed a mathematical model, based on brs, allowing the modelling of the two deployment tasks, installation and reconfiguration of an aadl architectural application. we have first defined a mapping between the architectural elements of aadl and those of bigraphs, offering generic transformation rules (meta-rules). then, we have formalized the two aadl structures, application and runtime platform, contained in an aadl configuration declaration, by two distinct bigraphs g s and g h . these are composed to produce a new bigraph showing the installation of the application on the runtime platform. aadl system reconfiguration was also formalized thanks to our brs-based model, exploiting bigraphical reaction rules. various and promising results were obtained while validating our developed brs-based model by the bigmc model checker. in a subsequent work, we intend to automate the checking process of the installation and the reconfiguration of any aadl software application. we project also to enrich the properties set to check dynamically."
"the installation task is often described as a set of specific steps dictated by the software producer to install a given application on a particular platform, regarding a specific technology. this task will be defined by a composition operation of two or more bigraphs. it will be guided by the regions and sites number/labels and also by the correspondence between inner and outer names."
"future work will also address a variety of dose reductions on cadavers, and we anticipate being able to reduce computation time for the proposed advanced system modeling by implementing it on parallel computing architectures."
"we show two experimental results in this section. for the phantom study, we focus on the comparison between the effects of each model on the ls images with a cone beam phantom (qrm, moehrendorf, germany) with respect to conspicuity improvement, noise statistics, and resolution. in an in vivo study, we show clinical evidence that supports the proposed approach with subjective assessment. the proposed method is compared with conventional fbp and image domain ir (iris) algorithms in a low dose scan. in this case, we used the same raw data for image reconstructions."
"the siddon projector calculates only the weighted sums of the portion of the ray that intersects through each voxel without considering and compensating for the neighboring voxels, generating aliasing artifacts [cit] . multiple rays in the volume beam can be used to compensate for this aliasing effect at the expense of over-sampling the image grid [cit] . we have additionally implemented a version of the siddon projector which does not require recursion [cit], thus making it amendable to parallel implementations [cit] . figure 3 -(b) shows how we divide active sub-elements to compute ray-sums. in figure 3 -(b), a single element model, as well as a multiple element model that strictly limits the active area of the detector (i.e., middle sub-figure), is depicted. we have noticed, however, that applications with reconstructions on voxel sizes that are finer than the detector size itself requires a greater over-sampling of the detector. in this case, not only does the computational demand increase, but the gap between the active areas of two adjacent detectors begin to introduce artifacts. as such, we have implemented the active area model depicted in the right sub-figure of figure 3-(b), where the rays intersect the major boundary points, leading to a higher quality reconstruction. 1,1) ) shows high quality image even in ls without regularization term and significant noise suppression effect on tv. doi:10.1371/journal.pone.0111625.g005"
"where e d (y,x) is the data fidelity term between image x and sinogram y via the projection process. the second term e(x) is the prior, or regularization term, and a is the weighting term. we formulate the fidelity term as:"
"our phantom and patient studies show that the proposed technique can improve image quality (low contrast, noise statistics, spatial resolution, and visual impression). we have introduced a modular system modeling framework for a sophisticated clinical ct scanner. even within the same system, some functions can be turned off or manipulated for clinical purposes. the advanced functions of state-of-art ct scanners need to be modeled accordingly for high quality image reconstruction. these parameter changes are meticulously recorded in the header files of raw data. none of these functions can be ignored for accurate system modeling to develop high-fidelity characteristics of an iterative algorithm."
"software reconfiguration constitutes an important activity of the deployment process. at architectural level, reconfiguration of component-based system provides a set of transformations preserving some properties despite system runtime changes. adding, removing and refining components or interaction links may be some of reconfiguration examples [cit] . aadl language offers the concept of modes to represent operational, alternative and predefined states of a component or a complete system. thus, in aadl architectural systems may be reconfigured by switching from one mode to another."
"we also tabulate the snr and cnr of the low-dose scans, in table 2 and 3, respectively. the proposed method preserves the signal/contrast at much reduced noise for the low-dose acquisition. in this paper, we choose the regularization parameter based on clinician's feedback so it can be improved by processing more cases with broad feedback from multiple radiologists."
"the models include a geometric projector (p geom ), a focal spot model (a fs ), and an active detector response function (a det ). by decomposing a system matrix h into sub-components, the implementation of complex clinical scanner modeling becomes more feasible. this approach also increases the usability of a single developed code across multiple ct systems, as opposed to requiring entirely different projectors for each system."
bigmc (bigraphical model checker) is a model-checker designed to operate on brs based models [cit] . the model checking in this case is achieved through an exhaustive search of all the possible states of the system specified by a brs.
"in some previous works [cit], authors show the relevance of this formalism to specify architectural styles and some of their relevant operations (reconfiguration, style conformance, etc). in this work we extend the use of brs to specify deployment tasks declared in an aadl specification. firstly, we give bigraphical formalization of the two structures: software application and runtime platform. secondly, we define the installation and the reconfiguration tasks by respectively, a bigraphical composition and a transformation one. finally, we prove the correctness of the proposed models by using a model checker for bigraphs [cit] . this paper is organized as follows: we introduce in the next section the aadl language via an illustrative example. section 3 is dedicated to the presentation of brs. in section 4, we give the formal description of the two aadl architecture structures, namely the platform and the application scenario. we exploit, in section 5, our proposed brs based model to specify the installation and reconfiguration operations thanks to the composition and transformation operations on bigraphs. section 6 is devoted to present the validation of our model by using a model checker tool. finally, a conclusion will summarize our contribution and give some perspectives for a future work."
"in the following, we will show how brs based models formalize architectural reconfiguration of aadl specification thanks to modes handling. we denote any system configuration by a pair (s, m) expressing the dynamic of a system s in a mode m. reconfiguration is specified by transitions between configurations thanks to mode changes. to achieve its formalization, we exploit dynamic behavior of bigraphs. reaction rules expressing various changes between bigraphs may be used in this context. we show in the following example (table 7, fig. 10 ), how our approach is applied to manage the substitution of a processor component by a faster one, thanks to the arrival of a trigger event from \"s_ev\" port."
"where s a and s b are mean signal intensities of signal producing structures of the liver (s a ) and the mean signal of the camper's fascia (s b ), respectively. to obtain cnr background statistics, the standard deviation of the camper's fascia over 45 slices was computed, and is denoted by s by in equation (7). a comparison of reconstruction algorithms for the half-dose scan is shown in figure 10 . although the added noise associated with this low-dose scan is apparent, no undesired texturing appears in this set of images either. the proposed method shows better visual impression compared to fbp and iris."
"to compare images, we defined a priori the regions of comparison. for snr, the signal is defined over the region containing the hepatic artery, while the background standard deviation is chosen from patches (over 45 slices) of the liver without vasculature. we define cnr as:"
"at 50% dose, both iris and the proposed tv with advanced system modeling were found to be diagnostically acceptable. although the proposed tv provided objectively superior images in terms of snr and cnr, this image quality was achieved through a significant amount of processing. as a technique that achieves fast computations while maintaining good image quality, a hybrid method (such as iris) may potentially be a promising approach."
"through recent studies, iterative reconstruction (ir) algorithms have been shown to be more robust than fbp algorithms in regards to the presence of noise and artifacts [8, [cit] . numerous researchers have discussed different aspects of formulations [cit] and optimization approaches [cit] . however, we have found that having a high fidelity model of the imaging system is also a critical factor in the reconstruction of high quality images; this is an aspect of iterative reconstruction algorithms which has often been either neglected or substantially simplified [cit] ."
"a critical component of tomographic ir algorithms is the accuracy of the forward system model. in positron emission tomography (pet), the forward system model consists of a geometric projection matrix and a sinogram blurring matrix, which can be either measured or simulated [cit] . it is shown that the combined model improves resolution and contrast-tonoise ratio in pet imaging [cit] . it is also possible to reuse the stored system matrix to improve computation time because the pet scanner is stationary, making it relatively easy to factorize the system model based on symmetric geometry. a similar method is applied to single photon emission computed tomography (spect) for the estimation of the depth-dependent component of the point spread function (psf) [cit] . however, it is a challenging task to derive an explicit system matrix in clinical ct for the following reasons: i) each scan has a different scan length and pitch based on the scanning protocol, and ii) it is very hard to find symmetries in cone beam helical ct scans because the source-detector set has a functional misalignment (i.e., a quarter of a detector offset [cit] ) and view-by-view deflections of the x-ray source spot (i.e., flying focal spot (ffs) [cit] )."
"to observe the effects of iterations, we simulated a siddon-type projector with proper ffs model (0,1,0) without fs and dm, and a complete model (1,1,1) including all modular models in section 2. both methods used the same optimization algorithm and code (c++ and open mpi) to reconstruct images and consecutively calculate the nrmse (normalized root mean square error):"
"to validate our model, we use the bigmc tool [cit] . it is a model checker dedicated to brs. checking process is based on applying reaction rules defined in the bigraphical model. in the first step, we want be sure if our model, obtained by composition and which may represent the application deployed on its runtime platform, is correct. in the second step, we will check the consistency of the reconfiguration task, expressed by reaction rules. table 8 summarizes our bigraphical model expressed in the appropriate grammar (of the bigmc tool). we specify some reaction rules to simulate the operation of bigraphical composition. these rules are specified exploiting disjoint bigraphs of figures 4 and 5, we obtain a new bigraph modelling the installation task."
"as shown in figure 4, there are sub-modules of the system that cause a small mismatch in system modeling, but these can be propagated through iterations, making them very hard to correct or compensate for by post-processing or utilization of regularization terms. many studies claim that they can produce high quality snr images with simple phantoms (having a few high density structures with homogeneous background) in low dose imaging; however, it is very hard to contain the small low-contrast structure in the final results without advanced system modeling. without satisfying the fidelity term of the energy functional in equation (3), we cannot guarantee that the reconstructed image is ''the only stable solution'' of this ill-posed image reconstruction problem."
"also, there are algorithms (asir, iris, idose, veo, etc.) implemented in clinical scanners by vendors, but the technical description and detailed methods are not available to the research community. in this paper, we systematically demonstrate the necessity of implementing focal spot area, flying focal spot, and detector area in the forward system model to generate higher quality images. we also compare our raw-data-domain irt with a mathematical formulation of image domain iteration called iterative reconstruction in image space (iris). the purpose of this paper is to examine the role of high-fidelity system models in the performance of the iterative image reconstruction approach minimizing energy functional. this paper is organized as follows. in section 2, we mathematically describe iterative image reconstruction and the components of proposed system models. in section 3, we present some initial results on phantom and in vivo data. in section 4, we summarize our findings and conclusions."
"transformation: two types of transformations are possible on bigraphs. the transformation of the places represents the arrival or departure of an entity. the transformation of the links, expresses the connecting or disconnecting of a node through one of its inner or outer interfaces. this dynamic is defined by a bigraph transformation rules, called reaction rules [cit] . a reaction rule is a couple of bigraphs: redex and reactum (before and after transformation)."
"in this paper, we show the systematic implementation of accurate system modeling for an irt in clinical ct. a similar approach for pet [cit] was derived from an analytical formula for calculating error propagation in a reconstructed image from the system matrix. in addition, in the cone-beam ct, the beam divergence and the rotation of the x-ray source and detector unit give space-variant effect on image. since we do not use a system matrix as in pet, we integrate all the functional misalignment and fabrication limitations with on-the-fly calculation method so that the space-invariant nature is embedded in the forward model. therefore, when we run image reconstruction algorithm, we set up on/off parameters for each modular model. that is one of major differences of our results compared to the previous 2d or phantom simulation works."
"when l~+ and p~1, e(x) becomes a total variation (tv) regularizer, which is commonly used to suppress noise and preserve edges in the image [cit] . from a modeling perspective, we make the assumption that h, the system matrix, can be decomposed into a series of component models:"
"starting from the model presented above, associated to an aadl specification, we establish a formal description of software application installation on runtime platform. besides, we associate also to the reconfiguration task a brsbased model."
"the drive towards lower dose ct imaging (while maintaining the diagnostic quality of ct) has been an area of focus for the entire ct community [cit] . numerous approaches to dose reduction have been implemented in commercial systems including the use of filters [cit], collimators [cit], dose modulation [cit], prospective triggering [cit], patient-specific protocols [cit], and more [cit] . one additional component to the current repertoire of low-dose ct scanning techniques is the use of new image reconstruction techniques."
"in figure 4, we can visually compare image qualities of siddontype model with ffs (0,1,0) and the proposed method (1,1,1) including focal spot and detector models to acquire a more accurate system model and to remove moire patterns. there are only small differences between the two models, especially around the edges of the image, but eventually these will cause a significant change in the final image (i.e., tv regularization), especially in low dose scans. to suppress noise in low dose imaging, we frequently use regularization terms in equation (2) with which we suppress noise by keeping the structure components of the image. when there are small model discrepancies related to the fidelity term in equation (3), the mismatches can be concealed by noise and may cause resolution degradation and eventually poor contrast."
"in this section, we describe the mathematical formulation of irt and a detailed forward system modeling method. the forward system modeling method can be decomposed into a series of components to increase modeling accuracy. we structure a three-component model that incorporates the most important elements of the system model. each component can be replaced by a specific scanner parameter or vendor-specific model. the accuracy of this system model is critical in the improvement of image quality of a reconstructed image."
"in this paper, we used the least-squares (ls) solution without the regularization term and tv solution in equation (2) and (4) for comparison. the lagged diffusivity fixed-point method [cit], where we iteratively approximated the cost by a weighted quadratic cost and then solved the resulting linear normal equations using pre-conditioned conjugated gradient (cg) iterations, is used to minimize the energy functional in equation (2) [cit] ."
"to compare artifact propagation, we compare the ls and tv images with a soft contrast section of the phantom. the three cross-sections of the soft contrast region are displayed in figure 5 -(a). 1,1,1) . by the number of iteration (i.e., ls: 10,20, and 40 iteration, tv: 10 iteration), we can observe circular line artifacts on ls and tv, as well as on the siddon-type model. it is especially more obvious in a very low contrast case ( figure 5-(b) ). however, the images reconstructed by the proposed method show high quality images without any artifacts in both ls and tv. as expected, the tv images show significant noise suppression."
"the image reconstruction formulation in equation (2) emphasizes that the energy functional aggregates a fidelity term and a regularization term. when the system model is not accurate enough to model details of the system, the tv-regularization (equation (4)) of the energy functional (equation (2)) smears or even loses the signal components associated with lower hu rather than noise when enforcing the smoothness constraint (i.e., l 1 -norm) as in equation (4). this can even occur to greater signal components in low dose ct data, when the system model is inaccurate and iteration proceeds to suppress amplified noise. on the other hand, the accurate system model sustains small signal components in the fidelity term so that it eventually reveals hidden signal components under the noise components."
"the declaration of an aadl abstract component is divided into type and implementation parts (see tab.1). the declaration type of a component may contain clauses defining its interfaces (features), flows (flows), etc. the interface of a component defines its interaction with other components in the form of a port, a group of ports, access to a bus (requires / provider bus access). an implementation specifies the internal structure of a component in terms of subcomponents, connections between these subcomponents, or modes to represent its operational states. the implementation the_system.impl (tab.1) is a possible implementation of the component the_system. table 2 shows the declaration of subcomponents involved in this example. aadl configuration represents a graph of components and connections. moreover, aadl specifications (see tab.2) define implementation details of the hardware components (the_processor, the_memory and the_bus). the properties bring more details on the operational aspects of these components. in the aadl declaration of tab.2, software component the_process consists of two subcomponents of thread type, thr_s (sender) and thr_r (receiver). these threads interact through a connection cnx, defined between the event ports: inport and outport (tab. 2). other implementation details of threads, contained in the process, are given by properties such as period and execution time (see tab. 3). tools developed around aadl [cit] give the possibility to specify aadl architecture from components involved in an aadl model. but, they do not offer a complete graphical view of an operational system instance (hierarchy of components). also, connections and relationships between software components and hardware ones are not considered in these tools notations."
"after the gateway received the broadcast information, the gateway will send a unicast acknowledgment packet to make the smart device change it state to listen to a service request. when the gateway received the device and service description, the gateway will forward the information to the websocket server and websocket server will push the device and service description to the web user application. the web user application will be display the graphical utility interface that matched with the device and service description sent by the smart device. when the interface displayed, user will be able to choose which device will be controlled and which service will be requested. figure 4 and figure 5 show the system entities flowchart contained of smart device, gateway, and the user application. figure 4 (a) explained the flowchart from smart device as the provider of the device and service discovery data. the smart device will broadcast the information to the gateway/coordinator and will call a receive handler to listen to the ack message that will be sent from the gateway, the smart device will change it back to the broadcast device and service discover state if the ack message not received in 30s."
"evaluating peers work is a great means of learning. in face to face classroom situations peer evaluation often leads to a conversation where both parties interact richly and gain important understandings about the work, through back and forth communication. moocs provide forums for communication and networking, however they are often flooded with community discussion, weeding out paired peer discussions because individual social connections are not readily made."
"we hypothesize that having identifiable peers and matched incentives will increase the feedback quality and lead to more communication than blind peer reviewing. to test this, we conducted a between subject experiment with 3 conditions."
"we aimed to reintroduce identity in a humanize form where peers introduce themselves to each other. this shared identity showed value, driving conversation and improving feedback quality. inspired by the design studio concept [cit] and cmoocs where learners make meaningful connections and learn by giving feedback, we designed the ipr framework and witness early results of students' response, compared to a control condition. at the same time adding incentive matching in which reviewers and those being reviewed both had the incentive to be honest in their responses. we believe these results introduce incentive compatible interaction design [cit] to moocs and that this kind of design offers significant opportunities for ongoing improvement in this field."
"the active process is placed in the third layer, data processor and object manager. the active process keeps working in the server while is performing the defined user's task. this process has a continued and direct communication with the midgar store (figure 1.6 ), which is a part of the midgar core because the midgar store contains the database with the services, the objects, the actions, and the data. the last layer is the layer that contains the objects. in this case, our ip camera. these objects implement the message interface to keep a permanent and bidirectional connection with the server (figure 1.7) . however, the ip camera cannot implement this message service because the ip camera software is private. nevertheless, it can send pictures by http protocol. then, the ip camera has to send the picture using the representational state transfer (rest) of midgar service. after, the midgar service, which is in the third layer, realises that this is a picture since midgar analyses the multipurpose internet mail extensions (mime) type of the request and sends this request to the computer vision module (figure 1.7) . the computer vision module analyses the picture and responds if the picture has or not a person."
"in moocs, social connections and networking are attractive features for improving learning and providing a platform for desperate learners to interact and learn together. since moocs are often open to the public, the diverse nature of students can be an asset for improving learning performance, increasing innovations, creativity and critical thinking rather than negativity [cit] . the cmooc [cit] approach explains peers as a learning source and finds that the more connections achieved in network with diverse perspectives from participants leads to a richer learning environment. by providing feedback and interactions in viewing assignments, projects, and online discussions as opportunities for crowd-sourcing, this approach leads to superior results that otherwise cannot be achieved individually by students (or the instructor) [cit] . however, the cmooc approach requires significant integration and system familiarity, so it is not widespread. peer assessment is a great way of learning from each others in the community, yet mooc designs tend to be so complex that no student can simply choose a peer to align with for feedback. our framework provides a connection and communication mechanism where peers can talk and learn directly. such discussions and interactions with diverse groups increase the learning significantly [cit] . in particular, an hci course on coursera encouraged students to post assignments to forums in getting feedback, leading to a conversation of feedback with identified peers and to more connectivity on fellow peers. in this hci mooc, more than 75% students were in favor of sharing their assignments on the public [cit], where they assert that blind peer review only drives decreased social connection and availability of diverse perspectives."
"our research successfully developed the pervasive and service discovery protocol in interoperability xbee-ip network. our system proposed xbee as smart device and the gateway that able to made interoperability into the websocket. the smart devices will be broadcast the pervasive and service description data into the gateway and the gateway will push the data to the user application with websocket protocols. the user application will be able to send the service data to the smart device via the gateway. our research developed smart door device prototype with close, open, lock services and smart door lamp with turn on and turn off services. the user application will be displayed the device and each of the device services in the web browser and able to send the door and lamp services data to the smart device."
"in this subsection, we are going to explain the new implementation of midgar. firstly, we are going to explain the flow through the different layers. afterwards, we are going to describe the functionality and the interconnection of our computer vision module in midgar. lastly, we are going to talk about the functionality that the ip camera offers."
"the internet of things allows the interconnection of physical and virtual things. these things can be objects of the physical world or information of the virtual world. this interconnection can be at any time, maybe meanwhile you are moving around the world, in continuous motion, or at any time during the day, in any place like outdoor or indoor, and between anything like human to human (h2h), machine to machine (m2m), or between humans and machines (h2m) [cit] . the iot allows creating a smart world, which is the fusion of heterogeneous and ubiquitous objects, smart cities, smart towns [cit], and smart homes [cit], with all devices capable of interacting between themselves [cit] . the iot has originated the development of object automation through the internet to exchange information [cit], allowing the creation of this smart world. however, many heterogeneous things compose the iot. the most important are the wireless sensor networks (wsn), which are the core of the internet of things [cit] . a wsn interconnects sensors, in order to obtain data, with a server or special system to work and maybe, automate tasks in one place. other components are actuators, which allow executing actions, like motors, fans, machines, and so on. another type of network is the fusion between a wsn and the actuators, known as wireless sensor and actuator network (wsan). besides, smart objects are other important components because they can perform actions as actuators, they can sense because usually they have sensors, and they are smart to process information or data and perform actions. nevertheless, all these components need a connection with the internet, but currently, almost every objects can be connected to the internet [cit] . this is why the definition of the iot is the next: the internet of things is the interconnection of heterogeneous and ubiquitous objects between themselves through the internet [cit] . the goal of the iot is to interconnect the whole world through the creation of different smart places to automate, improve, and facilitate our daily life [cit] ."
"3) canon ip camera we used the canon vb-s30d as ip camera but before, we had updated the firmware to the last version, the 1.2 of may 26 [cit] . we connected the camera through ethernet connection. this camera is a smart object because it recognises its own data and it can make decisions according to the video. this ip camera allows doing streaming using a url or send pictures or emails when the ip camera detects some changes in the video. for instance, if it recognises some movement or the modification of some object in the scene, and it can send pictures or emails with the pictures of that moment. in figure 3 we can see the five detection types that the camera offers. the first one is the 'moving object detection' and this consists of detecting some movement. secondly, the 'abandoned object detection' which consists in notifying the presence of new objects. another is the 'removed object detection' which detects when any object disappears from the default scene. the fourth is the 'camera tampering detection' which consists in detecting when the camera was manipulated. the last one is the 'step detection' which detects a movement over a defined line. these five events are configurable and allow us to receive pictures or emails only when the scene was changed. besides, we can choose two modes in the camera. we can analyse the streaming or analyse the pictures that we received. for working with the camera in midgar, firstly, we had to modify the ip camera configuration by setting the ip and port of the midgar rest service, where the camera had to send the picture. afterwards, we had to register the ip camera in the platform so that the camera could be selected in moisl. then, we could select the camera in moisl, create the interconnection, and work with the camera."
"we analyzed the peer reviews done by all 3 conditions from the communicating message box shown in figure 1 . the control group blind peer reviews contained only 29 messages responding to the feedback they received while the identified condition contained 53 and the aligned incentives condition received 49. the control condition group messages did not contain meaningful communication while treatment groups both had students interested in further communicating together. for example control case messages were limited to one or two words such as \"thanks, ok\" while the treatment groups had messages like \"i like to network too, it is great to be connected to someone new, thanks for the advice -kit.\" the message box also gave students means to ask and respond to questions about the assignment or feedback, for example, a student was able to inform their reviewer that a broken link to the assignment for review had been repaired."
"a possible impact of the incentive alignment condition would be to make the gap between the best and worst students larger as the best students help each other, and the worst students are never given high quality feedback. in this initial study, we have not seen evidence of this effect, and hope to explore this in more detail in the future."
"in order to add the capacity of computer vision to midgar, we had to create a module with this capacity. the rest of midgar is equal to the previous platform [cit] . then, the difference is when midgar receives a picture. when this happens, midgar detects that the request contains a picture because midgar is able to analyse the mime type of the request. for instance, when an object like an arduino or other, which has the possibility to deploy applications, is connected to midgar, this object can send a message using the xml standard style of midgar. in this case, the canon ip camera does not allow modifying the software, as occur with others ip cameras, but we can analyse the mime type to see if it is a picture as we can see in figure 2 . then, when midgar receives a picture, midgar saves the picture in a folder. when midgar spends five seconds waiting for pictures and more pictures have arrived, midgar sends the picture sequence to the computer vision module. after that, the computer vision module analyses the folder, which contains the whole picture sequence, to find people in at least, one picture. in the case that the module finds one or more people, it will respond to midgar with a 'true', in another case, with a 'false'. then, midgar will store this response in the database, as if it were a sensor with only these two possible states."
"april 20-21, 2017, cambridge, ma, usa each student reviews three other students' work, and receives reviews from three students. unlike most peer reviewing systems, ipr does not randomize peer review assignment, instead aligning incentives by matching a student with reviewers based on the usefulness of that student's previous feedback. students are motivated to provide their peers with high quality feedback to get high quality peer feedback in return. the next round of review allocations are made when feedback has been rated. only after a student has received and rated all their feedback are grades made visible, so good feedback ratings can't be bought with inflated grades."
"in this case, the important thing is the quality of the pictures. this is why, it is probably better to analyse sequences instead of isolated pictures, which is the thing that we do. according to this article, we can say that it is possible the use of computer vision in the iot. besides, with a very interesting application. in addition, we have shown that we can use pictures as sensors using not a perfect or weak model because if we analyse all the sequence that we need, we can improve the results. however, it is better and more recommendable the use of a good model with the sequences because we could improve the accuracy. automating is one of the ways of the internet of things. in this way, if we could improve and add new functionalities with computer vision, we could facilitate the use of the internet of things in our daily life and we could create new ways to communicate us with our environment. in this case, we can use our research to automate and improve the security of our homes, towns, cities, industries, and the earth."
"assessment is important to the pedagogy of moocs [cit] . moocs utilize peer assessment to assess student's work in a scalable way, not dependent on a one to many teacher to student relationship, but, optimal methods of choosing graders and assignments to grade remains an open question [cit] . the benefits of peer assessment include improvement of higher order thinking skills, consolidation of topical knowledge, and individualized feedback for each participant [cit] . giving and getting feedback has been identified as an effective way to learn in online [cit] and in the classroom."
"midgar is an internet of things platform to investigate different solutions for the iot [cit] . in this paper, we try to find a solution for the integration of computer vision in an iot platform for analysing pictures from ip cameras in order to find a determinate object in the pictures and use the pictures of the ip camera as sensors. in this section, we are going to describe the changes that we did in the midgar platform and show our proposed solution to add the computer vision module in midgar."
"this research will continue through testing the influence ipr has on larger cohorts' in a fully develop mooc. for decades, online peer reviewing has been blind, we aimed to integrate identity in assessment on an mooc, by carefully designing interactions leading students to discuss and have reason to improve their feedback quality. initial experimentation showed minimal advantage using incentive alignment over randomized review assignment, but both test conditions performed significantly better in review usefulness and in starting conversations. we believe this is a consequence of the small sample size, so we will explore efficiency with a larger cohort where our ultimate goal is to provide effective feedback to students with meaningful connections, so they can benefit from the diverse crowd in a mooc."
"our framework is design based on assessment interactions with visible incentivized peers. anonymity is commonly practiced in online and in person class peer review systems. due to the comfort of peers in providing critical feedback [cit], the bigger problem in online reviewing is due to increased anonymity and reduced community affiliation [cit] . visible identities lead to more constructive feedback than anonymous ones [cit] . thus through providing non blind review, our framework may encourage accountable reviewing."
"the ipr framework is built upon 4 phases: submit assignments, review peers, rate feedback, receive grades (figure 1 ). in the first phase, students submit their assignments. in the second phase, students are given a peer introduction form to provide a short public introduction to be displayed in reviews they conduct. next students review peer's work with the review form, an interface with four targeted feedback fields and grade input field. students then receive feedback on their own work from other peers in the feedback form, which initially shows only the feedback fields, a text input to converse with the reviewer, and a five point scale to rate the usefulness of the feedback. once all rating is finished, students will receive the grades their reviewers assigned them."
"in the second phase, we programmed the camera to send a picture when the camera detected any movement in the area. in this case, the camera sent picture sequence from the first moment that the camera detects the movement until the last detection of the same movement. then, we analysed all the sequence with each model to obtain if this method is valid to use the computer vision with an ip camera as a sensor. in figure 6, we show some pictures of this sequences about how the camera detected the initial movement in the first picture and after, it continued sending one picture per second until the movement ceased. the camera is capable of sending from one picture per second to thirty pictures per second. however, to do the evaluation, we selected the maximum value in the camera configuration. the number of pictures depends on the movement time. the camera sent each picture to another web service created in python with flask to avoid the interaction of midgar. the reason is that in this way we test only our computer vision module without a possible interference of the midgar iot platform. in this case, we intended to evaluate if we could obtain an improvement of the system using the camera sensor or maintain the same level. besides, this way allows us a reduction of the network traffic, a reduction of the necessary process computer, and avoid using the streaming option. for this phase, we used 972 pictures divided into 17 sequences from which 8 sequences, which were composed by 817 pictures, were movements where appeared people and 9 sequences, which were composed by 155 pictures, were movements where did not appear people. clarify, the sequences had many pictures without people because they have only a half body or an arm. then, these sequences contained many invalidated pictures but these pictures were part of the sequence. for the negative sequences, we used the movement of different objects in front of the camera like balls, mugs, or papers."
"cristian gonzález garcía currently, we live in the information era. we have many things in our daily life with access to the internet and capable of making our daily life more comfortable like smartphones, tablets, computers, some cars, smart tvs, and so on. every new day, we have more devices and better internet connection [cit] . these devices are able to run programmes, which use the devices' sensors, or to do other tasks like creating alarms or notifications, turning on or turning off the device, and et cetera. these objects are known as smart objects [cit] . smart objects provide us with many possibilities and every day we have more different smart objects. however, smart objects can be more useful in the case of being interconnected with each other and with other objects like sensors or actuators [cit] . this interconnection is called the internet of things (iot). the internet of things allows creating huge or small networks in order to obtain a collective intelligence through the processing of objects' information. the first example is how the iot was born because the first idea was its implementation in supply chains [cit] . other examples are the object identification in chemistry plants, people, and animal using smart tags as radio frequency identification (rfid) and near field communication (nfc) [cit] . the iot can be applied in cities, also known as smart cities, in order to offer different services that improve citizens' 'livability' [cit] . smart towns use a similar application although they use the iot to preserve their culture, folks, and heritage in small cities and towns [cit] . in other cases, we can use the iot to create smart homes that can control and automate certain things of our houses like doors, windows, fridges [cit], irrigation systems, lights, distribution multimedia [cit], and so on. by the contrary, some governments apply the iot to control and care in the better way the earth, which is also known as smart earth [cit], in front of different dangers like fire, earthquakes, tsunamis, or floods. moreover, we can use the iot to anticipate and prevent human disasters like the case of the deepwater horizon at gulf of mexico [cit] or the problem in the security system of the nuclear central of fukushima to detect the tsunami and automate the turn off of the diesel motors or activate an protection over the motors. nonetheless, these systems need a central service to control and manage their data and their objects. besides, sometimes these systems need to create intelligence and take the decisions. thus, they need an iot platform. there are several iot platforms for many and different uses: business platforms, research platforms, platforms in beta state, and open source platforms [cit] . all these platforms are more or less similar because they allow working with objects and interconnecting them. some platforms offer people an application programming interface (api) to facilitate this task."
"our research using xbee [cit] as a zigbee device and will communicate with the websocket technology to perform the pervasive device and service discovery protocol. with topology shown in figure 1 . there is xbee area network and the tcp/ip network that will be communicated each other using a gateway. we provide 4 xbee act as smart device and configured as xbee router device, gateway plugged with xbee act as coordinator (using raspberry pi), websocket server and web-based user application installed on a computer. our research proposed pervasive device and service discovery shown in figure 2 . at the first stage, the web-based user application will be connected with the websocket server to make the web user application always ready to receive data from the websocket server. if the gateway device turned on, it will be automatically connected with the websocket server. at the next stage, when there is a smart device around the gateway network with pan id 1aaa, smart devices will inform its presence to the gateway by sending broadcast information along with it device and service description presented in figure 3 . if the first smart device active, it will inform the gateway that the smart device is lamp the type is an actuator and provide turn on and turn off services. if the second device active, it will inform the gateway that the smart device is door as an actuator and have open, close, and lock services."
"permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. peer assessment on moocs often leads to inaccurate grades and low quality feedback [cit], due to laziness, collusion, dishonesty, retaliation and a lack of time, experience or interest [cit] . to counter this, humanizing feedback prompt phrasing [cit] and offering bonus points to feedback deemed helpful by the receiver [cit] have offered improved efficacy. in point systems, if a reviewer provided helpful feedback, the receivers will rate it, however without intrinsic motivation to provide a rating, this leads to single sides systems."
"computers can only process zeros and ones. nevertheless, years ago, the artificial intelligent (ai) was born to offer the possibility of creating programmes that allow computers to learn. [cit] in the conference of dartmouth [cit] . inside the ai, one of the fields is the computer vision. computer vision is the field that allows computers to 'learn' to recognise a picture or the characteristics of a picture. this allows identifying objects, humans, animals, or a position in a picture. thus, the goal of the computer vision is that a machine can understand the world [cit] . for reaching this goal, there are many algorithms for the long process of recognising something in a picture. some algorithms to obtain the features of the dataset that you can use to train the model are histogram of oriented gradients (hog) [cit], local binary patterns (lbp) [cit], hog-lbp [cit], scale-invariant feature transform (sift) [cit], and speeded-up robust features (surf) [cit] . other algorithms are the necessaries to train the model using the extracted features obtained previously. some examples of these algorithms are support vector machine (svm) [cit] and logistic regression [cit] . however, the task of obtaining a good model is very difficult. you need to take many good pictures and try many times with another group of different pictures to check that your model works well. besides, you need to create a model to solve your problem, because the use of general models could reduce the accuracy. examples of these models could be the sample models of some applications like opencv. even though, in this paper, we will use this type of models. in our proposal, we use computer vision to recognise the presence of people in the pictures that the camera sends to the iot platform in order to perform an action in the positive cases. thus, we want to use the pictures as a special sensor."
"87 participants collected from an online advertisement placed through authors social media channels and participants were randomly assigned one of three conditions. we created an assignment on the subject \"creativity and innovation\", requiring no previous lessons or special subject knowledge. we measured how students perform in each condition, and how useful the feedback was to them."
"for this first phase, we used the manual mode of the ip camera. we divided the pictures into two folders according to pictures without people and pictures with people. we analysed each folder with our module. for each folder, we obtained a new folder with the detected person inside a green rectangle. after it, we reviewed manually each picture taking into consideration the expected result because, maybe, the green rectangle could mark an incorrect thing like a wardrobe or a signal instead of a person. in that case, we count the picture as wrong. for this phase, we took 160 pictures manually: 64 with people and 96 without people. in figure 5, we show an example with three pictures: two with a person and another without people. afterwards, we process these pictures with our computer vision module to detect the accuracy of our module using the three different models. with this test, we tried to evaluate the accuracy of each model that we use in our computer vision module."
"in this section, we describe with all detail the methodology that we did to evaluate our hypotheses. after that, we show the results that we obtained in our evaluation. we have divided the evaluation into two phases: manual pictures and automatic pictures. moreover, we have compared the results of three different models in order to conclude what model was better for our proposal. we are going to explain each subsection through these two phases."
"our system enriched with the keep-alive features to detect if there is smart device that not able to function properly and inform the web user application if the smart device will be inaccessible. from the conducted experiment, the average delay when the smart device is recognized by the user application is around 10.13 ms delay. the average service delay when the user application sent service data to the smart device is around 2.34 ms delay. the pervasive device and service discovery protocol in the xbee-ip network interoperability able to"
"websocket becomes more popular since it enhanced the http performance by adding realtime bi-directional, and persistent connection with the server [cit] . websocket also popular to be used as control interface in smart technology, since it deployed above the widely used http protocol, websocket able to be created with intuitive graphical utility interfaces in the web browser [cit] . our research proposed a gateway design and implementation for xbee and the tcp/ip network that able to connect seamlessly by providing the pervasive device and service discovery protocol. by using our method and architecture, the user will be able to more focus on the service provided by the xbee based devices and not bothered by the complexity of the network architecture and configuration."
"in the rest of this article, we will discuss, in section ii, about what the internet of things and smart objects are, and explain the different iot applications like smart cities, smart towns, and smart homes. besides, we will explain a brief of computer vision, we will continue talking about the current and more relevant iot platforms, and we will present the related work. in section iii, we will describe the case study, in our case, the midgar iot platform. after, in section iv, we will explain the methodology that we used and show all the results of our evaluation and the discussion of the results. to finalise this paper, we will present our conclusions in section v and show possible future work to do from this research in section vi."
"the gateway will run three threads which are xbee receive the callback, keep-alive messages, and websocket receive the callback. xbee receive the callback is used to listening package from the smart device in the xbee network. gateway will receive device and service description data from the smart device and will be sent it to user application via websocket protocol. the keep-alive thread is use to detect an inactive smart device and inform user application that the smart device is unavailable. the keep-alive thread will send a \"ping\" message to the smart device periodically and if the smart device is not replied the message the smart device will be considered as an inactive node."
"in this section, we are going to describe the results. in order to improve the understandability, we design tables and graphs that represent the results of testing the pictures with our computer vision module with each model and the results of the module classified in the different four groups:"
"peer assessment in massive open online courses (moocs) affords grading open ended assignments of many students, but this approach often can't provide the level of feedback that students need. alternatives such as automated grading enable grading at scale, but require specialized assignment design to facilitate accurate algorithmic judgment, and can't deal well with open ended task designs, sacrificing student learning for ease of grading [cit] ."
"smart device will broadcast its presence to the gateway and will be forwarded to the user application. figure 6 displayed the gateway status when received the broadcast device and service description. the displayed information showed that there is two devices lamp and door that identified by each xbee source long address. after the gateway received the broadcast message, the gateway will send the information to the websocket server and push it to the user application interface. user application interface presented in figure 7 and displayed two smart device which is lamp and door. the smart devices identified uniquely in the port field by the smart devices long address. the door device is identified in the 0013a200400dca4e6 and the lamp device is identified in 0013a20040e88458. the user will be able to send the service data request for each device displayed in the service field. the door device has an open, close and lock services, while the lamp device provides turn on and turn off services. the smart device received the service requested by the user via gateway as shown in figure 8 . we provide time delay measurement to check the response time when the user sent the service request until the smart device receive the service requested. our research conducted a two experiment scenario. the first scenario is to measure the delay when the smart device start to send the broadcast device and service information until it has been received by the user application. the second scenario is conducted by measuring the delay when the user application sent the service data to the smart device. table 1 displayed the delay information when the smart device broadcast its device and service description to the websocket server and user application. the result showed that the average delay between smart devices with the websocket server is 0.205 for the lamp device and 0.215 for the door device. the average delay between the websocket server and user application is 9.927 ms for the lamp device and 9.93 ms for the door device. figure 9 presented the delay measurement from the smart device to user application with average delay 10.137 ms. the second experiment scenario conducted is to measure the service delay between the user applications to the smart device. user application will be able to send the service data (such as turn on or turn off lamp) after the user application able to recognize each of the devices and service descriptions. table 2 showed the experiment result for the service delay sent by the user application. the average delay from the web browser user application to the gateway is 2.288 ms. and the average delay from the gateway to the smart device is 0.060 ms. the average delay from the user application to the smart device is 2.34 ms and presented in figure 10 ."
"in this second phase, we analysed the pictures of the 17 sequences. we used 9 sequences without people and 8 with people. thus, the system can obtain a maximum of 9 true negatives or 9 false positives, and 8 true positives or 8 false negatives. in this case, we analysed all the sequence as one item instead of analysing each picture separately. we can see some of the detected pictures in figure 10 ."
"we chose as a possible solution the creation of a separate module. in this way, we could call this module when we need to evaluate a picture or a picture sequence. the decision was to separate the implementation of the computer vision module from the iot platform. this allows us to execute different tests with the same module and the same architecture for the evaluation of this proposal and then, we only have to change the parameters that we use to call the computer vision module. the computer vision module was developed in python due to the requirements of the open cv library that performs the body detection work. the use of open cv also required the use of the library numpy. the workflow of this module consists of loading an image from a file, converting the image to a bytes array, transforming the image to grey scale, and using the opencv library to detect the number of bodies in the image. if there is anybody, the module will return the boolean value 'true' else, the module will return the boolean value 'false'. however, we chose to improve the module recognition by using of picture sequences instead only one picture. in this way, we could obtain more accuracy because our objective is the detection of a dangerous movement. however, opencv needs to setup a few variables. the first one is to indicate the scale factor. the scale factor is necessary to create the scale pyramid that the algorithm uses to find objects in different depth inside the picture, which we set this value to '1.01'. the second variable is the minimum near detections that are required to compose a single object. we set up this second one with the value '10'. the last parameter is the minimum size of each detection window that we set up to '(200, 200)'. nevertheless, the values that we used to set up the opencv library depend on the context. furthermore, an external xml file is required because opencv loads the classifier from an external file in order to reuse the same code with different classifiers. in our proposal, we decided to use three sample classifiers that allow opencv to detect upper bodies, frontal faces, and the combination of heads and shoulders. if we wanted to detect other things, we would create new classifiers extracting the needed features. furthermore, these models are examples and they are a bit weak. for this reason, if we want a better recognition, we should train a new model to obtain a better and more specific classifier. however, we could improve in any way the movement detection in the case that we analyse the sequence to obtain at least one picture, which means that it is a positive detection. for instance, in the case that we would have a better classifier, we could increase this number and require at least three or five pictures with the object that we want to recognise. it is very useful if, for instance, we want to use this system to detect dangerous people like burglars, thieves, and so on in our home, or people in some private or dangerous area. in these cases, it is preferred that the module gives false negatives instead false positives cases. the reason is because if we received a false negative, we only received a false alarm. on the contrary, if we obtain a false negative, we could have a thief in our home. in order to test the body detection, another module was developed that skips midgar. this module, also developed in python, uses the library flask to receive the images from the camera and follows the workflow of the other module although it does not return a boolean value. this module saves the images that the camera sends in a directory and saves another image if the module detects a body, drawing a green rectangle around the body in the image. with this information, we will be able to do the evaluation of our proposal."
the receive handler function is also used to receive service data requested by the user application. figure 4 (b) displayed the flowchart and show the process when the gateway forwards the pervasive device and service discovery data from the xbee network to the user application. our research proposed the gateway method as a development from previous research [cit] by added websocket protocol as the tcp/ip communication between the gateway and user application.
"additionally, mooc peer reviews are carried out double blind where peers are not made aware of whom they are reviewing or who has reviewed them. this leads to a rising problem in mooc assessment where due to increased anonymity, there is reduced accountability and eroded community affiliation [cit] . blind peer grading has been practiced in many face to face classroom environments and online learning environments to prevent grading bias and mitigate targeted criticism or bullying. a disadvantage of blind reviews arises in moocs when students provide lower quality and less insightful feedback because, being anonymous in review, they are not socially accountable."
"feedback improves performance by changing students' locus of attention, focusing them on productive aspects of their work [cit] . however, not all peers provide great feedback and some leave limited comments with no coherent message for improvement, or they are rogue reviews [cit] . rogue reviews are insufficient reviews caused by laziness, collusion, dishonesty, retaliation, competition, or malevolence [cit] . to improve on this, peerstudio [cit] peer assessment system is designed to encourage more feedback comments by showing short tips for writing comments just below the comment box. for example, if a response has no constructive feedback, it may remind students with phrases like: \"quick check: is your feedback actionable?\" by triggering heuristics word count on feedback [cit] . students see such comments as more useful than rubrics in reviewing [cit] . similar techniques are used to improve the quality of product reviews online [cit] . our framework uses a simple interface design reflecting these lessons by integrating four pointed questions with separate response areas."
"nowadays, the internet of things is one of the most used technologies with interest for some countries, like the united states of america [cit] and the united nations [cit] . nevertheless, the iot needs many improvements because it was born a few years ago and has many problems to resolve as we can see in different recent articles about smart towns [cit], protocols [cit], security [cit], or others [cit] . the goal is to use the iot to interconnect everything, from food to computers and then, automate different processes to improve our daily life. now, we have different smart objects with a small but smart functionality in our life. besides, we can use these devices to make our life easier in some smart cities with special services like santander [cit], for instance, to park or manage our smart homes, or automate some tasks like the irrigation service. however, sometimes this is very difficult. for example, if we have a burglar alarm in our house and we have pets, we would have a problem because, maybe, the alarm would sound due to the animal whereas we only want that the alarm sound when the alarm recognises a person. we propose a solution based on the use of computer vision through the midgar iot platform [cit] ."
"here anfis performs as an estimator which uses ocv and temperature as the inputs to predict soc as the output as shown in figure 7 . indeed, anfis needs to be trained to learn the relationship of the battery parameters (i.e. ocv in this case) and temperature with soc. the battery parameters are identified and used in real-time as an indicator of soc. so, a nonlinear mapping tool is required, that is anfis here. (it should be noted that other mapping tools may also be applicable as well as anfis.)"
an adaptive neuro-fuzzy inference system (anfis) was used for battery pack's soc estimation in this study. this is about the last part of the whole framework presented in figure 5 .
"the asymmetric unit in the title compound contains two crystallographically independent triphenyltelluronium cations, two bromide anions, and one disordered acetone molecule (see fig. 1 ). the solvent molecule can be located in two alternative positions with the site occupation factors 58 (1):42 (1)."
"where () it is the battery current (a) (assumed positive for discharging and negative for charging).  is the battery's coulombic efficiency (dimensionless) and t c is the total capacity (as). in this representation, the soc value is a number between 0 and 1 that 0 indicates a fully depleted state and 1 represents a fully charged state. 20"
"identification was repeated every 0.1% change in soc using a two-minute time window. one of the main advantages of the proposed identification formula is its high speed that makes it suitable for real-time applications. in this case, the identification process was repeated every 6 seconds however, it is possible to run it much faster [cit] -a) . on the other hand, the identification algorithm has limitations as well: (i) as shown in figure 11, there are fluctuations in the identification results depending on the current profile. there is an error in the identification results especially at high discharge rate. (ii) the algorithm has also limitation during continuous discharge since identifying battery resistance and ocv is challenging in this case. no drop or jump is detectable during a continuous discharge pulse. to overcome these limitations, an averaging technique is used by repeating the identification faster and doing the estimation in a longer window. soc estimation was performed using the last 20 identification points which means every 2% change in soc in this case. the reason of using a number of identification points (20 points in this case) for every estimation attempt is to make the estimation results more 23 robust against the identification uncertainties. consequently, one or two faulty points in the identification process can not affect the estimation result very much."
"internal resistance and open-circuit voltage were obtained in real-time using current and voltage measurements. the main advantage of the proposed identification algorithm is its simplicity and speed that makes it suitable to be easily embedded on battery management boards. another advantage of the proposed framework is its flexibility that makes it usable for other applications like state-of-health estimation and other battery types too. drawbacks of the proposed bms algorithm were also investigated. there were fluctuations in the identification results especially at high discharge rate. the identification algorithm has also limitation during continuous discharge pulses without any drop or jump in between. using the solutions like averaging technique and anfis, battery soc estimation result was satisfactory against the limitations of the identification unit."
"a generic framework has been used for real-time battery measurement, model identification and state estimation as demonstrated in figure 5 . although the idea of this framework was discussed in another study [cit] -a), it has not been tested in real-time before. in the outputs of the identification unit (estimates of unknown parameters) are then used by the estimation unit which uses an artificial intelligent technique (described later in the paper) and is 14 trained to find the relationship between the battery parameters and soc. the effect of the temperature is also taken into account in this unit however; the results of this study are presented for a constant temperature."
"the number of parameters and variables which are used in a battery model depends on the required precision. in some cases, high-fidelity models are needed whereas in other cases, a fast low-fidelity model is desired. in this study, a simple and fast model is needed for real-time soc estimation. the idea of soc estimation is to find an inverse function (nonlinear mapping) which is able to predict soc by using the identified parameters. being aware of the influence of soh on our results, we assume fixed value for this variable and plan to extend our algorithms in our future works. so, the following equation is used for the sake of simplicity:"
"there are power source/sink devices available in the market which can be also used to charge/discharge a battery pack based on a real driving scenario. the quality of the measurements might be also better than the sensors used in this study. however, the high noise level in measurements in the proposed rig is an advantage for bms development since it would be more challenging. here the goal was to simulate the ev battery's working condition as much as possible similar to real working conditions. the proposed test rig represents a scaled-down ev powertrain system with even less expensive sensors that are used in a typical ev. this allows the testing of the bms algorithms in an environment that is more representative of real-world dutycycles and thereby reduces the risk of failure at a later implementation stage."
the above formulas give us optimal values of oc v and int r corresponding to the least rmse. some points that should be considered when using this analytical solution are:
figure 2 the two-dimensional planar packing of the tetrameric (ph 3 tebr) 4 units. the solvent molecules and the hydrogen atoms have been omitted for clarity.
"in a case study, a 64.8 v nimh battery pack was tested using the hil test rig and the proposed algorithms were used in real-time. the test was performed according to udds standard [cit], under constant temperature at 25 °c, starting from fully-charged state (100% soc) to fully-discharged state (zero soc) when the terminal voltage drops below 54 v (i.e. the cut-off voltage of the battery pack). figure 8 demonstrates how the battery terminal voltage has changed over 5 consecutive udds cycles. since the sampling rate was 10 hz, the time in the figures is presented in decisecond (0.1 second)."
"regarding the estimation results shown in figure 11, the theoretical (ideal) coulomb-counting was used as a benchmark for evaluation of the anfis method. it is worth mentioning that the 'ideal' coulomb-counting is not applicable in a real application because of its practical limitations such as cumulative noise effect (due to the integration of the current signal as presented in equation 14) and the need for 'known' initial condition [cit] . in figure 11, coulomb-counting is calculated based on an assumption that we know the initial soc value."
"the parts of the vehicle not represented by the test rig are implemented in software simulation model in matlab and simulink. a typical ev model is developed which has been parameterized to represent the nissan leaf with specifications listed in table 2 [cit] . the rig is used to simulate vehicle-level driving cycles such as the well-known urban dynamometer driving schedule (udds) [cit], and because of the inclusion of physical components, it provides a good representative electrical load at battery level. as the components used in the test rig are not full size, appropriate scaling has been implemented to ensure that torques and speeds are appropriately matched to the components in use."
"the battery parameters (pi) are obtained by using the system identification algorithm and then a nonlinear function like g in bellow (i.e. anfis) is utilized for soc estimation. structure that combines the gradient-descent and the least squares methods for the parameter tuning [cit] . anfis has been used in a wide range of applications and particularly, it has been used for battery soc estimation in previous studies [cit] . [cit] ."
"in cc, battery soc is obtained by integrating the current over time. assuming 0 soc as the initial soc at time 0 t, the battery pack's soc at time t is:"
"the summation of the opposing forces is produced by the load machine (right em) using a reference torque calculated in vehicle dynamics block as demonstrated in figure 3 . a torque sensor measures the real torque between the ems and the measured signal (em torque in figure 3 ) is feed backed to the control block which is responsible to control the torque. the last block is related to battery model identification and state estimation using the real-time measurements as demonstrated in figure 3 . this block has two inputs, current and voltage of the battery pack, and a number of outputs depending on the battery model's structure. battery soc is the main output of this block and is estimated by the proposed algorithms explained in more detail in section 3."
"while the soc estimation for lead-acid batteries can be relatively simple [cit], direct measurement of physical properties of the battery is possible, due to the battery's tolerance for overcharge and overdischarge to some extent. however, this is not the case for other battery types. li-ion batteries for example are sensitive to overcharge and overdischarge. so, the soc must be known when dynamic currents are applied, as in electric vehicles, to guarantee the safety and an optimal lifetime [cit] . less uncertainty in the soc estimation allows battery operations closer to its limits without compromising safety or reliability. for these reasons many soc estimation methods were developed for li-ion batteries with accuracies up to 1% [cit] . here, the estimation effort, and therefore the precision, also depends on the use. while most simple consumer electronics use basic methods, the soc estimation for electric vehicles is usually more complex since it has to be robust against a wide range of user conditions and highly dynamic current profiles."
"as illustrated in figure 3, the first part of the proposed model is a driver model to follow the drive cycle. the driver model uses the drive cycle and velocity as inputs, and calculates an acceleration/deceleration command ( a d or b d ) as output. the driver model is a proportional-integral (pi) controller tuned using ziegler-nichols method [cit] . the output is scaled between 0 and 1 showing released and pushed pedal statuses."
"although the effect of regenerative braking is not considered in this study, the algorithms are able to deal with instantaneous charging pulses (between the discharge pulses) as well. however, a continuous charging profile needs particular considerations. identification and estimation results are presented in figure 11 including battery internal resistance and ocv as discussed in section 3.2 and battery soc estimation using anfis explained in section 3.3. according to the results demonstrated in figure 11, the identification and estimation algorithms perform well against the high level of noise (caused by using low-cost sensors) which is designed by purpose to test the algorithms in the worst scenario."
"2) a limited number of latest measured data points are temporarily stored on the bms board to be used for identification. the data is updated continuously at each time step by eliminating the oldest point and adding the most recent point. for the test explained later in section 4, a two-minute time window was considered. consequently, the time window contains 1200 data points for each variable (i.e. current and terminal voltage)."
"the vehicle dynamics block in figure 3, contains different parts as follows. the required tractive force ( t f ) is simply calculated based on electric motor's (em) maximum torque and the driver's command as follows:"
"however, the anfis method is able to recognize the initial state by using the ocv value. on the other hand, each estimation try by anfis is independent of its previous try. so, there is no cumulative error when using anfis technique. the drawback of the anfis technique is its robustness against the identification accuracy (ocv values in this case) that can cause to fluctuations in the estimation results. performance of the proposed bms algorithm was tested properly using the hil test rig, representing a scaled-down electric vehicle powertrain system. using such a hil test rig for bms algorithm development, is a quite useful approach to evaluate the proposed algorithms in a more realistic scenario by considering the effects of noise and uncertainties. a generic framework was explained and tested for real-time battery model identification and state estimation. battery"
"the bms algorithms development in this study mainly contains two parts: (i) using an ecn model, battery test rig is explained in detail in section 2 and the identification and estimation techniques are presented in section 3."
"before applying a battery model or estimation algorithm in a real ev, a battery simulation environment is essential for testing the developed models and algorithms. this study describes a hardware-in-the-loop (hil) test rig for the test and development of electric vehicle battery parameterization and state-estimation algorithms. hil testing of bms algorithms is a common technique in the literature to validate the software functionality under real working condition [cit] ."
"for validation of the estimations, 'ideal' coulomb-counting (cc) technique is used as a benchmark. cc is a theoretical method which cannot be used in practice because of its restrictions however; it is a good benchmark for checking other techniques [cit] ."
"the driver model followed the driving cycle by generating an acceleration command when the actual velocity is less than the reference velocity. rotational speed of the electric motors was measured and a velocity tracking error was calculated. the reference velocity and measured velocity are illustrated in figure 8 (for the whole test) and figure 9 (for the first cycle only), demonstrating a good tracking performance. variations of battery pack's current and voltage are also depicted in figure 8 (for the whole test) and in figure 9 and figure 10 (for the first cycle only). as shown in figure 8 to figure 10, the measurements are quite noisy due to using inexpensive sensors. this is done by purpose because we were interested in providing a real challenging platform for bms algorithms testing. such a testing platform is similar to a real automotive application with budget constraints. in addition, we believe that testing and validation of the proposed bms algorithms using such noisy data would be sufficient to be sure about other 'less challenging' situations."
"two battery packs are used in the test rig: the 'main' battery pack under test (connected to the traction machine) and a 'secondary' lead-acid pack (connected to the load machine). the secondary pack provides the load and is not the subject of study: the role of the ev traction battery is supplied by the main battery pack. the secondary pack consists of five 12 v lead-acid batteries connected in series, giving a nominal 60 v which matches the motor controller's voltage range. in this study, for the main battery pack under test, a nimh battery chemistry has been selected as a first attempt while keeping this option to replace it with any other battery type in the same scale. the nimh battery pack was built with the general aim to simplify development of the test rig but still being sufficiently representative of the automotive context. nimh batteries have advantages in experimental development due to their high safety in charge and discharge and tolerance to abuse (overcharge and overdischarge), their good volumetric energy, power and thermal properties, and their simple and inexpensive charging and control circuits [cit] . as an initial configuration, radio-controlled car cell packs (with six cells each) were used with their original charging power supplies, since they charge the packs similarly to a defined point. here nine modules in series (shown in figure 2 ) are used to create a nominal voltage (64.8 v) within the window of the motor controller. the layout of six nimh cells per module, connected in series, is particularly close to automotive applications since it is also used in hybrid cars [cit] . table 1"
"3) identification process is done at certain points (not continuously) using the updated batch of data (1200 data points for each current and voltage). identification is repeated based on a soc window. for example, every 0.1% change in soc that simply means 1000 repetition of the identification process if we start a test from 100% soc and continue until the depleted state (0% soc). figure 6 shows a battery identification trigger that works based on soc change in real-time. in this mechanism, an integrator calculates the summation of the energy flow from/into the battery and it resets to zero value every time that the summation value equals or becomes bigger than a threshold (i.e. 0.1% soc). the battery model parameters are then updated every time that an identification process is completed; otherwise they keep their previous values."
"the battery measurement consists of recording load current and terminal voltage during the tests. temperature is controlled to be fixed at 25 °c during the experiments by using the thermal chamber (depicted in figure 1 ). using a simple resistive potential divider circuit. the divider resistors are selected such that the current through them is less than 0.1 ma and the output voltages are less than 10 v, which is a limit of dspace system."
"(v) the proposed method is simple and fast enough to be used in real-time applications. in addition, anfis models are adaptive and can be retuned very fast."
"in the test presented in figure 8, speed following of the traction motor is performed without braking (i.e. no negative command, no energy regeneration). in this scenario, an acceleration command was generated when increase in speed is required and zero acceleration when decrease in speed is required. this works in practice because of the friction in the whole system and also small rotational momentum of the system (vehicle inertia is neglected for the sake of simplicity)."
"4) the estimator also works based on a similar (but separate) trigger that updates the in this study, the average value is calculated using 20 points (identified parameter values every 0.1% soc) as discussed in section 4."
"the structural features of organotellurium salts r 3 tex are governed by weak tellurium-anion secondary bonding interactions, which expand the ax 3 e trigonal pyramidal geometry around the tellurium atom generally into a sixcoordinate environment. the two crystallographically independent tellurium atoms in the title compound, however, show different coordination environments. the trigonal pyramidal geometry around the te1 atom is expanded into an octahedron by three te···br contacts, whereas te2 shows two te···br contacts. te2 becomes six-coordinated through the"
"in most conventional battery simulators, the duty cycle (or 'load profile') is obtained from simple models of the powertrain components. for example, simplified models of electric machines are used, and the detail of power electronics is often neglected or treated as a simple efficiency map with ideal instantaneous switching [cit] . 6 this study avoids these limitations as real, physical components are used within the simulation loop. a brushless dc (bldc) machine driven by three-phase ac is used, together with its associated mosfet-based power electronics. this type of machine is common in electric vehicles, and it would be hard to obtain a computational model that was sufficiently representative of its real-world transient behaviour."
"the monitoring process consists of real-time measurements of variables such as current, voltage and temperature, which are used to calculate some 'unmeasurable' variables like battery state-of-5 charge (soc) and state-of-health (soh). the simplest method of battery soc calculation, called 'coulomb counting', works based on integration of the battery current over time to calculate the amount of energy flowing through the battery. because of the practical limitations of coulomb counting such as cumulative noise effect and the need for a known initial condition, more advanced battery soc estimation techniques such as kalman filter-based estimators [cit], sliding mode observers [cit], and fast model identification techniques [cit] -a) have been developed in the literature."
"because of the low computational effort and relatively good precision, ecn models have been the subject of studies in a wide range [cit] specifically for automotive application [cit] ."
"the hil test rig and its main components are shown in figure 1 . it includes two back-to-back connected electric machines, the respective power electronic controllers, a target battery pack (in 7 this case, containing nimh cells), a lead-acid battery pack to provide independent power sourcing and sinking, a dspace real-time simulator, a thermal chamber and a pc for humanmachine interface. the test rig contains two bdlc machines of 5kw each. one of theseshown to the left in figure 1 is used to represent ev traction motor and the second machine is used to apply dynamic loads ('load' machine) representing the torques experienced in a vehicle-level duty cycle. the 'traction' machine is connected to the battery pack under test. the load machine is connected to a bi-directional power source consisting of a separate dc supply built from readily available lead-acid batteries."
"as mentioned above, 5 kw brushless dc (bldc) machines are used in the battery test rig. a bldc machine has a trapezoidal air gap flux distribution obtained through electronic commutation implemented through power electronics. in a three-phase ac machine of this type, a 'current controller' converts a 'torque reference' from an 'outer' speed controller to the respective current references for each of the three phase windings; to achieve this, the current controller outputs gate trigger signals to mosfets in the (hardware) motor controller block 8 [cit] . the current controller employs nonlinear hysteresis type current comparators, which are the most rugged analog controllers. the rotor position required for the correct commutation of the windings is obtained from hall-effect position sensors in the stator. the same rotor position signals are used to calculate the rotor speed. since the machine has eight permanent magnet poles, three hall-effect sensors are placed in the stator, 15 mechanical degrees apart. in the speed calculation algorithm, any change in the hall-effect sensor output (corresponding to a rotor pole movement) is detected and the time between two consecutive such changes is calculated. the inverse of this time multiplied by the angle rotated (15 degrees) gives the rotational speed of the machine."
"viper s650 (adept). the length of the needle that can bend is 12.6 cm long. the needle young's modulus is assumed to be that of steel, i.e. 200 gpa. the insertion is done in a homemade porcine gelatin phantom embedding several spherical targets. visual feedback is provided to the controller using a sonixtouch research 3d us scanner (ultrasonix medical corporation) with a 4dc7-3/40 motorized 3d us probe maintained fix during the experiments. the acquisition parameters are set such that a volume has a depth of 8 cm, a frame field of view of 78"
"therefore, the real vsc1 active power output will be expressed by introducing a time constant in (12): accordingly, the total extra active power pie which supports the emulated inertial response consists of: 1) the portion provided by the dc link capacitors; and 2) the portion provided by the remote side grid through vsc1. pie will be released into the ac grid from the vsc3 where vq control is adopted, which is expressed in (13):"
"using this method, the intensity of the feedback provided to the user is thus proportional to the velocity that is currently applied to the needle tip by the controller defined in (8) . this ensures the safety of the framework by natural driving the haptic device toward a position where no tip velocity is applied by the controller. depending on the exact definition of k h, which can be non-diagonal, different guiding effects can be created by applying a different stiffness in different directions. we describe in the following the three different methods used to define k h and represented in fig. 4 . a) method 1: we define a stiffness matrix k iso h such that it provides an isotropic stiffness, i.e. the force provided to the user is directed toward the haptic center and is proportional to the distance from this center. k iso h is thus defined as a diagonal matrix with a constant factor k in the diagonal. this way the feedback is directly proportional to the velocity applied to the tip. using this definition, the user has a full control over the trajectory of the needle tip and does not feel constrained in any direction. however, no haptic information is provided on the target position or on the degree of feasibility of the tip motion. b) method 2: we define a stiffness matrix k tip h such that the stiffness is anisotropic with a greater stiffness applied in directions in which it is difficult to move the tip without deforming the tissues and with a low stiffness in the other directions. this allows the user to be naturally guided in the direction in which the needle tip can be inserted without bending the needle and pushing laterally on the tissues. this method should increase the safety of the insertion procedure by limiting the lateral motions of the needle."
"in this paper we presented a proof-of-concept for a teleoperation framework to perform the real-time robotic insertion of a beveled-tip flexible needle under 3d ultrasound guidance and haptic feedback. the framework enables the user to intuitively guide the trajectory of the needle tip in the image while the controller handles the complexity of the 6d motion that needs to be applied to the needle base. a mean targeting accuracy of 2.5 mm could be achieved in gelatin phantoms, validating the whole framework. we compared different ways to provide the haptic feedback as well as different levels of control given to the user on the tip trajectory. limiting the user input to the insertion speed while automatically controlling the trajectory of the needle tip seems to provide a safer insertion process, however it may be too constraining and can not handle situations where more control over the tip trajectory is required, for example if unpredicted obstacles need to be avoided. on the contrary, giving the full control of the 3d tip velocity to the user and applying a haptic feedback to guide the user toward the target proved to maintain a low level of needle bending and tissue deformation."
"there are usually multiple remote vsc stations in the mtdc system, and all of them can be involved in the ie control. so the remote vsc1 and vsc2 can both employ ie control. the main ac grid frequency performance will be transferred to the remote vsc stations through dc link voltage vdc according to (4) since the voltage deviation is proportional to that of the deviation of the ac grid frequency fmeas. the vsc1 or vsc2 will adjust the active power output reference pref according to the rate of change of the vdc. the active power contribution of one single remote vsc is expressed in (6): where kw is the control parameter of the remote vsc stations. accordingly, the control diagram of the vsc1 or vsc2 is illustrated in fig. 2(b) . substitute (5) and (6) into (3), the equivalent inertia constant is expressed in (7): where hs is the equivalent system inertia constant, m is the number of the remote vscs that are involved in the ie control. according to fig. 1, m will be either 1"
"x . however, as far as we know, its closed form is unknown as the pdf of the ratio of two correlated complex gaussian random variables with non-zero means is not available. therefore, instead of providing the mse, the upper bounds of the average snrs ofq"
"conventional synchronous generators are widely used in . the kinetic energy expressed in (1), which is noted as ekin, is stored in the rotating mass of the generators or motors."
a load increase scenario at l7 is analyzed in matlab/ simulink. the initial load condition of the generation units and the load of the local ac network is listed in table ii .
"where [κ x, κ y ] are the mrc coefficients, and [·] t denotes the transpose. it can be seen from (1) that the coefficients depend on both the aoa and polarization state which can be estimated using the proposed array."
"the overall structure of a wind farm integrated system using vsc-hvdc transmission is illustrated in fig. 1 . the hvdc system under test in this paper is a four-terminal hvdc network. the constant active, reactive power control, which is referred to as pq control, is applied to the vsc1, vsc2 and vsc4. the constant dc link voltage and constant reactive power control is applied to vsc3, which is referred to as vq control. the c represents the converter dc link capacitance. in order to exclusively analyze the impact exerted by the mtdc network, each remote wind farm is simplified as an ideal synchronous generator, which is also illustrated in fig. 1 . so sg1 and sg2 which are located in the previous remote wind farm side are noted as remote sgs in the following paragraph. the simplified northeast power coordinating council (npcc) system model is employed as the main ac grid model [cit] ."
"this task ensures that the needle base automatically stays aligned with the insertion point, hence limiting the tissue deformation near the surface. the desired variationė d 1 of the task is set to regulate θ toward zero according tȯ"
"as illustrated in fig. 2(a), the real time dc link voltage vdc will follow the reference dc voltage vdc,ref if the vsc controller is appropriately designed. therefore the real time vdc can be expressed as (8) ."
"in order to close the control loop and to ensure the success of the needle insertion, feedback needs to be provided to the user all along the procedure. visual and haptic feedback are usually used together to provide complementary information on the state of the insertion. in the following we describe the visual and haptic information that we provide to the user in order to perform the teleoperated needle insertion."
"the inertia constant is defined to be the time duration during which the machine can supply its rated power solely by the kinetic energy stored in the rotating mass [cit], which is expressed in (2): where srated is the generator rated power, fm is the rotating frequency of the machine, and the inertia constant is noted as h in seconds."
"y, respectively. as each (product) term has a phase of u x (u y ) in the absence of additive noise, the sum of them will constructively increase the signal component with the phase, u x (u y ). therefore, this leads to faster convergence and higher accuracy for the tracking. then, the analog and digital beamformers are adjusted accordingly to have the array align with the estimated u x and u y, i.e.,α"
"where x in, y in and z in are the coordinates of the insertion point p in expressed in the needle base frame (see fig. 2 )."
"the reception using an interleaved array leads to the outputs of the neighbouring subarrays, s m x (t), s m y (t) and s m+1 x (t), s m+1 y (t), having a constant phase difference which depends on the aoa of the wave [cit] . this holds as long as p s (θ, φ) is nonzero. therefore, an estimate of aoa can be evaluated by extracting the phase of the cross-correlation of the outputs. this is known as the dbt. we denote the cross-correlations between two adjacent subarrays in x-and y-directions by [r xx, r yx ] and [r xy, r yy ], respectively. assuming the noise components to be independent, we have"
"the first task e 1 is used to control the safety aspect of the insertion. it is defined as the angle θ, represented in fig. 2, between the needle base axis and the insertion point p in at the surface of the tissue, such that"
"to simplify the analysis, the ie control is implemented in vsc1 at the remote side and vsc3 at the local side. the ie control transfer function can be derived following (4)- (7), of which the input is the ac system frequency per unit value fac,pu and the output is the active power contribution ppu provided from the local side vsc3."
"it can be observed that the psg dropped sharply at the moment of load change, and gradually increased back to the previous value when the ac system frequency went back to stabilization, which is indicated by the solid black curve. the vsc3 active power injection pvsc3 is indicated by the red dashed curve, and the active power contributed by vsc3 without ie control is illustrated by the pink dashed curve. the active power contribution curves of the sg1 and vsc3 have the same trend. similar to the results in the load increase scenario, the response of the vsc3 ie control lags the sg1 inertial response by ~0.1s. the fnadir, rocof, and phase angle are also evaluated in this scenario. the results are illustrated in fig. 9 . similar to the results illustrated in fig. 8, we can find that the output frequency, the rocof, and the phase angle difference between each generation units measured from the two cases respectively are overall close with each other. the results measured from case 1 is slightly greater than the results from case 2 it can be observed that the frequency response of the ac system with the mtdc inertia emulation control is very close to that with an equivalent sg according to fig. 8 and fig. 9 . however, the inertia emulation control cannot instantaneously respond to the system power imbalance, resulting in a delayed active power contribution. so the frequency nadir and the rocof of each generation unit in case 1 are greater than those in case 2 in both test scenarios. however, by observing the active power contribution in fig. 7 and the system frequency curve in fig. 8 and fig. 9, we found that both two cases follow the same trend of frequency response. the mtdc inertia emulation control can achieve a similar inertial response compared with that of the sg. the control response delay will lead to an active power injection delay, so a greater fnadir, rocof and phase angle difference is observed."
"according to (4) and (5), the vdc,ref is determined by the deviation of the system frequency after the disturbance, and the change of vdc will release/ consume the active power in dc link capacitors, which is expressed as (9): according to (6), the per unit value of the active power reference for vsc1 will change with the rate of change of vdc after a disturbance, which is described as (10) and (11): where pref represents the vsc1 reference active power deviation due to the ac system frequency change."
"not approach to zero. therefore, as i approaches to ∞, both the snrs also approach to ∞, implying that the noise components in (29) and (30) gradually become negligible as the iteration proceeds. as a result, the noise components can be omitted after a sufficient number of iterations, leading to the ratio,q (i)"
in this section we describe the experiments that we performed to validate the teleoperation framework and we present the results of the comparison between the different control and feedback methods.
"in order to increase the safety of the insertion procedure, it is important to avoid breaking the needle or tearing the tissues. we thus compared the mean and maximum values of the needle bending and of the tissue deformation obtained during the insertion with the different methods. the needle bending is taken as the integral of the absolute value of the needle curvature along the whole needle shaft. the tissue deformation is taken as the mean displacement of the tissue from its initial position along the inserted needle (see fig. 2 ). both are estimated using the current state of the interaction model presented in section ii-b. the average and maximum values obtained for each method are summarized in table ii. as expected, the amount of needle bending and tissue deformation is greatly reduced when the tip motion is fully constrained by the controller (method 4) compared to the 3 dof control. this is mainly due to the fact that the controller aligns the needle with the target as soon as the insertion begins, so that little lateral motion of the needle base is then required for the remaining of the insertion. guiding the user toward the target (method 3) also seems to decrease the induced amount of needle bending and tissue deformation compared to the non-guided method 1. this validates the usefulness of providing a haptic force feedback to the user to guide the insertion. however, contrary to what could be expected, guiding the user along the direction of least lateral needle base motion (method 2) seems to actually increase the amount of induced needle bending and tissue deformation. this can actually be explained by the same reasons that justified the good behavior of method 4. using this kind of guidance at the beginning of the insertion will indeed tend to delay the alignment of the needle with the target, which will then require more tissue deformation to be achieved later when the needle is deeper inserted."
"y, converge to e * y /e * x and e x /e y, respectively, as the number of iterations increases. therefore, the estimate of the polarization state obtained by cprpt is asymptotically unbiased."
"needle insertion is a widely performed medical procedure used for a large variety of applications, such as the treatment and diagnosis of cancers. such procedures require a high accuracy in the placement of the needle tip to ensure the success of the operation and to avoid medical complications or misdiagnosis [cit] . many research work have focused on the modeling of the needle/tissue interaction to predict the deformation of the needle during the insertion [cit] . such models have then been used in robotic control frameworks to assist the surgeon and to improve the targeting accuracy [cit] . robots can indeed be of great help to perform repetitive gestures with consistency and accuracy."
"the structure of the paper is organized as follows: part ii introduces the principle of the inertial response, the mtdc system model and the ie control method that is evaluated in this paper. part iii presents the transfer function of the ie control. part iv provides the simulation results illustrating the effectiveness of this technique."
"sin γ cos θ cos φe jη − cos γ sin φ sin γ cos θ sin φe jη + cos γ cos φ,"
"many teleoperated robotic systems have been designed for needle insertion procedures [cit] . however, the surgeon is still often required to directly control the different degrees of freedom (dof) of the robot, which can have various and complex effects on the actual tip trajectory. in addition, teleoperated robots that are designed to work in the limited workspace of mri or ct are often controlled in a discontinuous manner due to the non real-time nature of the imaging process. the insertion is then performed step by step and alternates between image acquisition, correction of the needle base placement and insertion steps [cit] ."
"the emulated inertial support is realized by integrating ie control to the hvdc vsc stations. the coordinated ie control strategy is used in this paper, which features using both the electric energy stored in dc link capacitors and the energy transferred from the remote grid side based on the variation of the main ac grid frequency [5, [cit] . so both the local vscs and remote vscs are required to employ ie control. in order to integrate ie control into the vq control of vsc3, the dc link voltage reference vdc,ref changes according to the deviation of the ac grid frequency, which is expressed in (4): where kc is the proportional control parameter of vsc3, fnom is the initial frequency reference, fmeas is the real time grid frequency, vdc,nom is the initial dc link voltage reference. the active power contribution of the dc link capacitors after system disturbance is expressed in (5): where n is the number of the dc link capacitors, svsc is the rated power of the vsc3. the dc link capacitors will release or store energy following the change of the ac grid frequency according to (5) . accordingly, the control diagram for vsc3 is illustrated in fig. 2(a) ."
"in this section we present and discuss the results of the experiments. five insertions were performed for each method. trajectories obtained during one of the insertions performed with method 1 are shown in fig. 5 . we can see that the user is able to reach the target at the end of the insertion. note that the trajectory obtained by integrating the 5 . trajectories in the frame of the ultrasound probe during a typical example of insertion using the 3 dof tip control and without haptic guidance (isotropic stiffness). the black curve represents the trajectory of the target, the red curve is the trajectory of the tip of the needle model, the green curve is the trajectory of the tracked needle tip and the blue curve is the position obtained by integrating the velocity input given by the user through the haptic interface. input of the user diverges from the actual trajectory of the tip, mainly due to the damping introduced in the control law (2) . however this does not seem to hinder the targeting performance since the visual loop is closed by the user. we first compare the performance in terms of targeting accuracy, segmented manually in the us volume at the end of the insertion. table i gives the mean final targeting accuracy obtained across the 5 insertions for each method. good targeting performance can be obtained with all methods, with a mean targetting error under 2.5 mm and a mean lateral error under 2.0 mm, which is on par with the accuracy obtained in current research work on needle steering under us feedback and is sufficient for most medical applications."
"which has the subarray main beam directed towards the direction/aoa represented by the angles (θ, φ ). the outputs of the beamformers are then converted into digital signals via analog-to-digital (a/d) converters. the samples sampled at the it th instance are denoted by"
"the short wavelength of mmwave enables the use of massive antenna arrays in the transceivers to overcome the path loss and to provide beamforming and spatial multiplexing. the arrays can be configured in a number of architectures which reflect the trade-offs between the performance and the costs of hardware implementation, power consumption"
"in this paper, we study hybrid partially-connected massive arrays using dual-polarized antennas to enhance the reception of an information-bearing mmwave signal. the estimation methods of aoa and polarization state are studied for two typical partially-connected arrays, i.e., the interleaved and localized arrays [cit] . we show that the new methods lead to polarization diversity which coherently combines the signals from the dual-dipoles to produce enhanced snr. the contributions also include"
"in the second case, only the insertion velocity is controlled by the user, while the direction of the tip velocity is maintained oriented toward the target. v d tip is then computed aṡ"
"the low pass filter block is placed after the kws block to filter out the high frequency harmonic components generated by the vdc differentiation, and it can also be regarded as a first order transfer function expressed in (15) ."
"eq. (12) indicates that the polarization state included in e x and e y can be obtained through the ratios, q xy /p x and q xy /p y . given q xy, p x, p y, two cross-correlation-to-power ratios can be respectively expressed as"
"this work is supported by the center for ultra-wide-area resilient electric energy transmission networks (curent). fig. 8 . g1-g5 fnadir, rocof and g2-g1, g3-g1, g4-g1, g5-g1 phase angle difference for step load increase scenario. fig. 9 . g1-g5 fnadir, rocof and g2-g1, g3-g1, g4-g1, g5-g1 phase angle difference for step load decrease scenario."
"the basic principle of the cprpt algorithm is to compute the polarization state by exploiting the ratios given by (13) and (14) . as the aoa estimation proceeds, the directivity of array will be gradually adjusted to the real aoa, leading to increasing powers of the signal components in both x-and y-axis dipoles. in addition, the increase of signal power also results in increasing snr for the cross-correlation (see section v for details). as a result, it can be seen that the accuracy of the estimation improves as the iteration proceeds."
", where t represents the sampling interval which equals the width of a symbol carried by the wave. as shown in fig. 2, these samples are used to estimate the aoa and to update the phase shifters accordingly. the samples from x-and y-axis dipoles are weighted and summed separately by digital beamformers as"
"the 6 dof of the needle base are controlled in this framework, so that lateral motions can be applied to the base. due to the needle flexibility and its interaction with the tissues all along its shaft, the needle shape can be modified inside and outside of the tissues, so that the widely used kinematic models of unicycle [cit] can not be used here."
"in the case of a continuous insertion whose velocity control is left to the surgeon, the robot motion is often limited to the insertion and rotation around the needle axis [cit] . continuous needle insertion usually requires the use of us imaging to obtain a fast visual feedback and automatic needle insertion under 2d or 3d us guidance have been the focus of many work [cit] . however there have been only few work on continuous needle steering under us guidance using all available dof of the needle base [cit] and, to the best of our knowledge, none of them included the surgeon in the control loop for an robot teleoperation."
"denote the noise components induced in the ith iteration. they can be approximated by complex gaussian noises with zero means, and noise powers (27) and (28), respectively. the cprpt algorithm estimates the polarization state using the ratio,q (i)"
"in order to have an overall evaluation of the main ac grid, g1~g5 are simulated in terms of the following indices: (1) frequency nadir fnadir; (2) rocof; (3) phase angle difference between different buses directly connected by the generators. the results are illustrated in fig. 8 ."
"it can be observed from the figures in the first row of fig. 8 that the output frequency curves of g1~g5 for case 1 are similar to that for case 2, indicating that the hvdc ie control provides a good inertial response. the frequency nadir zoom-in figures are illustrated in the second row of fig. 8, where the frequency curve with ie control has a lower nadir point than that with a sg inertial response. similarly, the rocof with ie control is greater than that with a real sg inertial response, especially right after the disturbance. the phase angle differences between the sg connected buses are illustrated in the last row in fig. 8, where a very slight increase of phase angle is observed."
"however, little research related to the capability of the vsc-hvdc to achieve the ie control has been addressed. the synchronous generator inertial frequency response will instantaneously regulate the system frequency when there is a grid power mismatch, i.e. the inertial response which results from the generation and demand imbalance will mitigate the grid frequency deviation. however, the system frequency deviation is generally used as the indicator of power imbalance in the ie control design, which is in reverse order compared with that of the sg inertial response. therefore, the hvdc inertia emulation algorithm is not only required to inject a certain amount of active power, but also required to function as fast as possible regarding a system power mismatch. so the impact of the practical control process on the ie accuracy and efficiency needs to be considered. in this paper, a detailed vsc control module, including the phase-locked loop (pll) and low pass filter (lpf), is considered in order to analyze the mtdc network impact on the ie performance. the analysis indicates that the frequency performance of the ac network with ie integrated hvdc transmission can be nearly as good as that directly connected with the high inertia ac subsystem, except for the short time delay introduced by the vsc control modules. the effectiveness of ie performance will be compromised if the response delay is significant. the simulation results have verified the analysis."
overall the results validate the good performance of the framework in terms of targeting accuracy and stress out the importance of carefully choosing the way haptic guidance is provided to the surgeon in order to ensure a safe procedure.
"in this section, we study the estimations of aoa and polarization state using a localized array. the dbps algorithm is proposed to remove the phase ambiguity in aoa estimation, and to determine the signal, the power of which is to be used in polarization state estimation."
"where α is a positive scaling factor used to convert a distance into a velocity, (3) is the rotation matrix from the tip frame to the frame of the haptic device (see fig. 3 ), p h is the current position of the handle of the haptic device and p h,0 is a default position at the center of the haptic device workspace, which will be used as a reference position for both the control law and the haptic feedback (see section ii-d.2). using this kind of control law, the more the user moves the handle of the haptic device away from the default position, the higher is the velocity applied to the tip. the user can thus freely control the trajectory of the tip and they are in charge of ensuring that it is effectively driven toward the target."
"y, and thus the dipole with higher signal power will have higher relative value which leads to better performance. the search procedure using one scanning frame is summarized in table 2 . once the values for p and q, and the dipole are determined, they can be used in conjunction with dbt and cprpt for the estimations of aoa and polarization state."
"since the results are user-dependent, future work would include a larger scale user study with experts and beginners in order to confirm the obtained performance. as the different components of the framework can already handle the effect of external tissue motion with little to no adaptation, tests should also be conducted to validate its ability to perform automatic motion compensation during the teleoperation."
"in this section, we consider the reception of the wave using an interleaved array. in order to produce the maximum snr at the decoder, the array calibrates its analog and digital beamformers and the mrc coefficients through the estimations of aoa and polarization state. the estimations are given by an iterative procedure, where in each iteration the aoa and polarization state are estimated based on the current and previous samples."
"imaging modalities such as magnetic resonance imaging (mri), computed tomography (ct) or ultrasound (us) are usually used to perform needle insertion procedures under visual guidance. in the case of mri or ct, surgeons have a restricted access to the patient because of the bulky scanners which limit the size of the workspace or because of the ionizing radiations emitted during the ct image acquisition. on the contrary, with us the workspace is less limited and the insertion can be performed at the same time as the imaging process. however the accuracy of the needle insertion can be reduced by freehand probe holding since the surgeons have to coordinate the position of both the us probe and the needle [cit] . in each case, teleoperated robotic systems can offer better operating conditions for the j. chevrie is affiliated with univ rennes, inria, cnrs and irisa, france."
"it can be observed that the ie performance bandwidth will be influenced with the introduction of these practical modules, reflected as the magnitude and phase damping of the ie frequency response curve in fig. 4 . this will lead to a performance time delay compared with the real inertial response. the delay will be more and more significant with the increasing of the control module time delay. the ie effectiveness and accuracy will be negatively impacted if the frequency response of ie transfer function cannot track that of the sg inertia response within the concerned bandwidth."
"a partially-connected array typically consists of several subarrays of antenna elements. depending on the grouping of the elements, an array can be configured as an interleaved or localized array [cit] . fig. 1 illustrates these configurations using a uniform planar array (upa). as shown in the figure, the interleaved array has subarrays with the elements distributed in a scattered manner over the whole array, whereas the localized array puts together neighbouring elements to form subarrays. in line with the enormous research on the signal processing techniques for massive antenna arrays, the considered arrays are assumed to employ antenna elements with omnidirectional radiation patterns [cit] . each element has two spatially collocated orthogonal dipoles [cit], denoted by x-axis and y-axis dipoles, respectively, which without loss of generality are assumed along the x-and y-directions respectively. the dipoles are used to measure the components of the incoming electric field projected onto the directions of x-and y-axes. therefore, an incident wave can simultaneously stimulate two signals in an antenna, each from a dipole. accordingly, in an array, two sets of signals collected from all the x-and y-axis dipoles respectively can be formed. from the perspective of signal reception, they represent two replicas of the signal (wave), which can be coherently combined to enhance the snr at the decoder. the diversity combining depends on the individual snrs of each dipole, and thus requires their estimates which can be obtained through the estimations of aoa and polarization state of the incident wave."
"the jacobian matrices associated to the first and third tasks can be computed based on geometry. however, the jacobian matrix j tip associated to the second task needs to be estimated using a model of the needle interaction with the tissues. we describe in the following section ii-b the mechanics-based model that we use to compute j tip ."
"the voltage source converter based high-voltage dc (vsc-hvdc) transmission system is accepted as the solution for long distance bulk power transmission because of its attractive features, including the flexible reactive power support, small size of filters, ability to work with weak grid, small power loss, and black-start capability [cit] ."
"where is the machine nominal speed of rotation in rad/s, j is the moment of inertia of the rotating mass in kgm 2 .when there is a power imbalance between the power supply and demand, the kinetic energy stored in rotating machines will instantaneously be released or consumed during the speed change."
"the ac system inertial response during a large frequency disturbance is considered as the aggregate effect of all the sgs in the system, which will have a critical influence on the rocof in a traditional ac network. assume the initial rotating frequency is f0, the relation between the inertia constant h and the kinetic power p is expressed in (3): where f is the frequency of the ac network in hz."
"2 p c ( û (k) ) 2 in (45) will be always less than that calculated using p 1 (û (i),ρ (i) s ), i.e.,"
"the reception of the wave using a two-by-two dualpolarized localized array and the associated signal processing modules are illustrated in fig. 2, where the rf and down conversion components are omitted for simplicity. the replicas are processed separately and identically in analog and digital domains for the estimations of aoa and polarization state. the modules for x-and y-axis dipoles are included in the figure, with those for x-axis dipoles elaborated in the red and blue dashed boxes and y-axis dipoles enclosed in the corresponding solid ones. although the numbers of analog and digital beamformers are doubled, they are the necessary components for polarization estimation and coherent combination."
"in this paper we propose a preliminary study of a control framework that enables the real-time semi-automatic teleoperation of the needle tip 3d trajectory during its insertion in soft tissues. the framework uses all 6 dof of the needle base and intuitively gives the surgeon the control of the tip trajectory, while automatically performing an automatic orientation of the bevel and computing the optimal needle base velocity that eventually needs to be applied by the robot to achieve the desired tip motion. the insertion is performed using 3d us feedback in order to get a real-time tracking of both the needle and the target, which are considered as inputs for the controller, and to provide visual information to the surgeon. a haptic interface is used to get the input from the surgeon and to give an informative haptic force feedback. different methods of computing the force feedback are here compared in terms of targeting performance and safety of the procedure. this paper is divided as follows: the different constitutive parts of the control framework are presented in section ii. fig. 1 . control framework and setup used to perform a teleoperated needle insertion. elements in blue interact together to provide the visual interface: the target and needle tracking algorithms, the ultrasound probe and the needle/tissue interaction model send data to be displayed on the screen. elements in red interact together to provide the haptic interface: the target tracking algorithm and the needle/tissue interaction model are used to compute the haptic feedback sent to the haptic device, while the haptic device sends position data to the controller."
"1) visual feedback: in order to monitor the good proceeding of the insertion, a visual feedback is provided to the user in the form of three orthogonal slices of the last acquired us volume, as can be seen in fig. 3 . the slices are automatically selected such that they intersect each other at the current location of the tip of the needle model, such that the tip is always visible during the insertion. additional information are also displayed on each view in order to help the surgeon interpreting the images and performing the insertion toward the target. the needle model is projected onto each view to give the currently estimated position of the needle body. the results of the needle and target tracking algorithms are also projected on each view, so that it is easy to identify the direction in which the needle should go to reach the target."
"in this section, we present an analysis on the cprpt in the context of a linear hybrid dual-polarized array with m interleaved subarrays. 45 it is shown that the ratios,q (i)"
", and thus can be extracted by exploiting their relative values. we employ the relative values given by the stochastic cross-correlation and powers of the signal components of (10) which, denoted by [q xy, p x, p y ], can be expressed as"
"ii. system inertia response and mtdc system model in this section, an wind farm integrated ac system with mtdc is introduced since the wind power integration is one of the most important application of the hvdc. however, it should be clarified that the remote side power source is not necessarily to be wind power. generally, it can be any ac system which is able to provide active power to the local ac network."
"millimetre wave (mmwave) communication is one of the most promising technologies for future wireless services. thanks to the large spectrum ranging from 30 ghz to 300 ghz, mmwave communication has potentially enabled extremely high data rate which is unprecedented in conventional radio frequency (rf) systems. in addition, the vast spectrum is an effective supplement to augment the currently saturated rf bands (700 mhz to 2.6 ghz) for wireless communications. as a result, a large number of applications, e.g., 5g cellular systems, wireless local area networks and ad hoc networks, have been investigated using the mmwave [cit] ."
"in the presence of additive noise, q xy, p x, p y are estimated by using the outputs of the digital beamformers. in line with the aoa estimation, they are evaluated iteratively, with the values derived by the ith iteration,"
"the joint dbt and cprpt algorithm, and dbps algorithm are blind adaptive since no knowledge about the reference signal is required. these provide not only efficient online estimation approaches, but also the benefit of bandwidth saving and no need of specific training sequences. they are applicable to the scenarios where the system available bandwidth is limited and/or training sequences are not specified. when there exists relative motion between the transmitter and the receiver, the doppler effect will happen and the aoa and polarization state may vary with the time. however, as shown in (6), (7) and (12), the phase shift e j2πf d t induced by the doppler frequency shift f d affects neither the crosscorrelations [r x, r y ], nor the cross-correlation and powers q xy, p x, p y, since this phase shift is the same for any beamformer output signal at any given instance. that means f d does not affect the instantaneous estimates of aoa and polarization. when the aoa and polarization state are timevarying, the proposed adaptive filtering based algorithms can track aoa and polarization state adaptively. as a result, the proposed algorithms are also doppler resilient."
the different strategies for the control and for the computation of the haptic feedback are also detailed. in section iii we describe the experiments performed to validate the teleoperation framework and we discuss the results obtained from a comparative study of the different kinds of haptic feedback. finally section iv provides conclusions on this work and directions for future work and improvement.
"the mtdc network impact on the ie performance is analyzed in this paper. the analysis indicates that the hvdc ie performance can be nearly as good as the inertial support provided by the directly connected ac subsystem with a relatively high inertia constant. the response delay introduced by the vsc station control modules will correspondingly introduce a delay to the ie performance, which will further the frequency nadir fnadir and rocof. furthermore, the effectiveness of the inertia emulation will be compromised if the delay time is too long. therefore, the inertia emulation control is required to be carefully designed in order to achieve a good inertial response."
"the setup used for the experiments can be seen in fig. 1 . we used a chiba biopsy needle (angiotech mcn2208) with internal diameter 0.48 mm and external diameter 0.7 mm attached to the end effector of a 6 dof manipulator target fig. 4 . representation of the different definitions of the stiffness for the haptic force feedback, overlaid on the needle tip. from left to right: isotropic stiffness (method 1), anisotropic stiffness in the tip direction (method 2) and anisotropic stiffness in the target direction (method 3). the ellipses represent a set of velocity vector that would induce a force feedback of constant intensity. green and red arrows indicate velocity directions in which the intensity of the provided force feedback is low or high, respectively."
"since the above algorithm uses the ratios of crosscorrelation to powers of the beamformed output signals to obtain the polarization state information and to track it adaptively, it is referred to as cross-correlation-to-power ratio polarization tracking (cprpt). the joint dbt and cprpt algorithm is summarized in table 1 ."
"respectively. as shown in (31) and (32), the snrs increase with increasing number of iterations. for a value for i great enough, both p s ( û (i−1) ) and p c ( û (i) ) approach to ones, indicating that"
"in this paper, we have proposed a hybrid dual-polarized adaptive antenna array, which not only significantly lowers full digital implementation complexity, but also makes use of polarization diversity to greatly enhance the capacity and reliability of mmwave communication systems. for the interleaved and localized configurations, we develop a joint dbt and cprpt algorithm, and a dbps algorithm respectively, which are blind adaptive and doppler resilient. the former is applicable for tracking fast time-varying aoa and polarization state due to its high estimation convergence speed. the latter is suitable in the aoa and polarization acquisition stage due to the ambiguity caused by the localized array. furthermore, we have formulated the polarization state estimation as the ratio estimation of cross-correlation to powers of two beamformed signals under recursive nuisance parameters, and proved the cprpt estimator to be asymptotically unbiased. numerical and simulation results show that the proposed algorithms can be efficiently and effectively performed for aoa and polarization state estimations with a large hybrid dual-polarized antenna array of subarrays."
"in fact, the search can be extended by using multiple scanning frames (see fig. 6 ). in addition to the values for p and q, (23) also reflects the powers collected by x-and y-axis dipoles for all the possible aoas. in the absence of additive noise, identical values are expected for the estimates of polarization state given by volume 7, 2019 y . the search returns the dipole that collects more signal power for cprpt. this is because the additive noises in x-and y-axis dipoles result in identical noise powers inp"
"the vsc phase-locked loop (pll) and low pass filter, which are practically included in the vsc control modules, are also taken into consideration in the transfer function. the typical linearized pll model is illustrated in fig. 5 [cit], where the phase detector (pd) is equivalent to the magnitude of the phase voltage vm according to (14), and the pi controller is employed as the loop filter (lf). the pll is equivalent to the first order transfer function if appropriately tuned. the time constant of the pll is defined as tm, which is expressed in the control block diagram in fig. 3 ."
"the active power injection curve at bus 12 provided by the vsc3 in case 1 and the directly connected sg1 in case 2 is illustrated in fig. 7(a) . the sg active power injection psg is indicated by the solid black curve, which results from the inertial response of the generator rotating mass. according to fig. 7(a), the psg increased sharply at the moment of load change, and gradually decrease to the previous value when the ac system frequency is stabilized."
"the vsc3 active power injection pvsc3 is indicated by the red dashed curve. it can be observed that the power curves of psg and pvsc3 are very close to each other, except that the response of the vsc3 ie control lagged the sg inertial response by ~0.1s. this power injection delay verifies the analysis about the control response delay in section iii. compared with the case adopting hvdc ie control, the vsc3 active power injection without ie control is illustrated by the pink dashed curve, where the pvsc3 is almost constant, indicating that no active power support is provided during the change of ac system frequency."
"surgeons, who do not need to be directly near the patient anymore to operate, but can be positioned in an ergonomic way with an optimized access to the different available intraoperative feedback modalities."
"the associate editor coordinating the review of this manuscript and approving it for publication was zhenyu xiao. and real-time signal processing. the fully digital array is a performance-orientated architecture in which each antenna is connected to a dedicated rf chain. it is seamlessly compatible with the classic multiple-input multiple-output (mimo) technologies and thus leads to the best performance in terms of data rate. however, the implementation of a fully digital array involves prohibitive costs, particularly for a large array. alternatively, hybrid architectures can be employed in the array to reduce the associated costs. in these architectures, analog beamformers (phase shifters) are used to connect the rf chains with antennas and to adjust the directivity of the array. depending on the connection, a hybrid array can be configured to be fully-or partially-connected. a fullyconnected array is known for the total connections between each of its rf chains and all the antenna elements. it provides narrow beams for its rf chains and thus leads to sub-optimal data rate [cit] . the challenge of implementing fully-connected arrays stems from the dimensions and power consumption demanded by an enormous amount of phase shifters. the hybrid partially-connected array is a practical solution for the transceivers [cit] . unlike its fully-connected counterpart, each rf chain in a partially-connected array is connected to a subset of elements (subarray) only. this is more suitable for hardware and physical deployment with massive array, and thus makes it a topic increasingly studied in recent years. a partially-connected array can be categorized into two types of regular configurations according to the topology of subarrays, i.e., interleaved and localized arrays. they have different characteristics and satisfy different demands [cit] . the interleaved array provides narrower beam width, while the localized array generates smaller side lobes. therefore, the former is more applicable to generating multi-beam for space division multiple access, while the latter is preferred to support systems with larger angle-of-arrival (aoa) range. from the view of hardware implementation, the localized array is easier to assemble multiple modules to form a large array in feeding networks."
"more recently, [cit] extended this similarity-aware index to work with dynamic data. in their proposed approach, new records can be inserted dynamically into the index allowing the index to grow. the authors stated that the average record insertion time (around 0.1msec), and the average query time (less than 10msec) were approximately constant as the index grew for a large database with 2.5 million records. while this dynamic similarity-aware indexing technique is based on the idea of standard blocking described before, in this article, we propose a novel dynamic real-time indexing approach based on the sorted neighborhood method, as we will describe next."
"where t is the number of trees in the m-dysni data structure, and k t i is the number of nodes in a tree. as for the zipfian distribution, the maximum number of candidate records using multiple trees can be calculated as"
"as this example illustrates, a fixed-size window can lead to both unnecessary comparisons with records in nodes that are unlikely to have a high enough similarity to be matching with a given query record (like r3 from node n3), as well as missed potential true matches that are outside the window (such as the records attached to node n5 with key value 'pedrosmith')."
"indexing techniques are employed in the area of database systems to improve the performance of search operations. major indexing techniques that are used include techniques based on hash tables and tree data structures hector garcia-molina and ullman [cit] . the main hashing-based indexing techniques are extensible hashing [cit], linear hashing [cit], and partial-match hashing [cit] ]. the main tree-based indexing techniques include avl trees [cit] (which are usually used with main memory indexing), b and b+ trees [cit] ] (which are mostly used with disk-based indexing), and t trees [cit] (which evolved from avl trees and b trees and are commonly used with main memory indexing). more details about indexing techniques that are used with database systems can be found in hector garcia-molina and ullman [cit] ."
"to generate candidate records, we retrieve (from the inverted index or database table d) all records that are stored in the tree nodes in the window. while the record comparison process in the dysni compares all attribute values between the query and the candidate records to calculate an overall record similarity, in the simdysni we only need to compare attributes that are not used in the sk. to calculate the overall similarities between the query record and candidate records, we retrieve the precalculated similarities from the s p and s n lists, retrieve the corresponding records from d using the record identifier lists of these tree nodes, and then calculate the similarities of those attributes that are not used in the sk. therefore, the more attributes are used in a sk the more similarities can be precalculated, but at the cost of a larger tree (as likely a larger number of distinct skvs will be generated). in our experimental evaluation we investigate how different sk influence the amount of memory required to build the index, the percentage of query records that benefit from the precalculated similarities (i.e., queries with indexed skvs), as well as the reduction in comparison time that can be achieved."
"here we investigate how the simdysni is able to improve query time. first, we measure the average time needed to compare a query record with a single candidate record for queries where a skv has been indexed previously and already exists in the index, using the simdysni fig. 14 . the plot on the left presents average times required to compare a query record with a single candidate record for the simdysni approach using different numbers of attributes as sorting keys. the table on the right shows the average total query time (in ms) for queries with indexed skvs (section 5) using various sks. the oz-1 dataset was used for both the table and the plot. note that the case of having no pre-calculated attributes is the same as the original dysni."
"the proposed dysni approach groups records in the dataset with the same skvs into one node (i.e., block). to obtain a better understanding of the number of candidate record pairs that will be generated for a certain query record, we assume two types of distributions that are common in attributes used for er, namely, the uniform and zipfian distributions."
"assuming a uniform distribution for the frequency of attribute values will lead to a uniform distribution of the frequencies of the skvs, which means all nodes in the index tree are of the same size. on the other hand, having a zipfian distribution for the skvs means that a few skvs have a high probability of occurrence, while the majority of skvs occur only rarely. according to the zipfian law [cit], for a list of values ranked according their frequencies, the frequency of any value is proportional to its rank in the ranked list of values and can be estimated as 1/r, where r is the rank of a value."
"the build phase is the same as the build phase of the dysni described in section 4. however, after the build phase a similarity calculation phase is conducted where the precalculated similarity lists s p and s n are added into the built dysni tree index. both lists are ordered according to the distance of the neighboring node from the query record's node (i.e., the first element in these lists is the closest neighboring node, and so on). the process of calculating similarities is conducted for all nodes in the tree. in this phase each node n i in the tree is visited and the similarities between the attributes that are used to generate this node's skv and the attribute values that are used to generate the skv of the neighboring nodes within the window (in both the previous and next direction) are calculated using an approximate string similarity function."
"importantly, the average number of generated candidate records for the different window sizes and number of tree nodes is constant with increasing dataset sizes. this indicates that on average the number of generated candidate records is not affected by the increasing size of the index, which confirms the experimental results in figure 11 (discussed in section 8.2.2) where the average query time is nearly constant with the growing size of the index."
"the idea behind the similarity-based dynamic sorted neighborhood index (simdysni) is to precalculate the similarities between the attribute values used to generate the skvs, and to store these similarities in the tree. these precalculated similarities are used in the query phase to reduce the time required for the calculation of similarities between records. a similarity-based brt is used to build the index where precalculated similarities are stored within nodes of the tree index."
"8.2.8. required memory size. table iii shows the memory requirements of the different tree index data structures using different sorting keys. as can be seen, with concatenated sks the number of unique skvs increases significantly and therefore the size of the tree index structure also grows. the additional overhead of the simdysni compared to the total amount of memory required by the tree structure is negligible for small trees, but can be quite significant for large trees."
"m-dysni achieved better recall and query time results compared to both baseline approaches. recall values for dysimii and qgi have dropped compared to results in figure 12 because the dataset used in this set of experiments has a larger number of corrupted duplicates. we also noted from running this set of experiments that for m-dysni using attributes that have less dependency between them (e.g., 'firstname,' 'postcode') as sks gave better recall results than attributes with higher dependency (e.g., 'suburb' and 'postcode')."
"this approach aims at matching a certain minimum number of candidate records that can be processed within a certain period of time. in a real-time environment this allows for a controlled number of candidate records to be returned for detailed comparisons. in practice, users can investigate different numbers of candidate records to achieve the required maximum query time. the minimum total number of candidate records to be returned, δ, is used to stop window expansion regardless of the similarities between skvs."
"for robust tde, ma and nikias introduced [cit] the following p l -norm cost function about the delay d and the attenuation factor β :"
"while real-time er is becoming more important, most current er techniques are based on batch algorithms that are only suitable for static databases. such algorithms compare and resolve all records in one or more database(s) rather than resolving those relating to a single query record. therefore, there is a need to develop new techniques that support er for large dynamic databases that can resolve (streams of) query records in real time. a major aspect of achieving this goal is to develop novel indexing techniques that allow dynamic updates and facilitate real-time matching by generating a small number of high-quality candidate records that are to be compared with a query record."
"for a uniform distribution all nodes in the tree data structure are assumed to have a uniform size of n/k where n is the number of records in the database and k is the number of skvs in the tree. assuming the fixed-size window approach, the number of candidate records generated in this case will be affected only by the number of nodes that are included in the generated window 2w + 1 (for a fixed-window approach) and the number of records n in the database. therefore, the estimated number of generated candidate records in c is"
"the er process encompasses several steps [cit] ] : data preprocessing, which cleans and standardizes the data to be used; indexing, which reduces the number of candidate record pairs to be compared in detail; record comparison, which compares candidate record pairs in detail using a set of similarity functions; classification, where pairs or groups of compared records are classified into matches (records that are assumed to correspond to the same entity) and nonmatches (records that are assumed to correspond to different entities); and finally, evaluation, where the er process is evaluated with regard to matching accuracy, efficiency, and completeness using various measures."
"approach (as shown in algorithm 4) with a different number of attributes being used as the sk. we ran the experiments using the oz dataset with 1, 2, and 3 attributes used as sks for different possible combinations of the four attributes: 'firstname,' 'surname,' 'suburb,' and 'postcode.' the average comparison times over these combinations are shown in the left plot in figure 14 . the results show that for queries where the node is preexisting, simdysni can significantly reduce the time required to compare a query record with a single candidate record. this improvement in time is almost linear with the number of attributes used in a sorting key. the results show that for a one-attribute sorting key the comparison time reduction is around 20%, for a two-attribute sorting key it is around 40%, and for a three-attributes sorting key it can be up to 70%."
all results confirm that the dysni is well suited for use with real-time er where a stream of query records needs to be resolved against a large and dynamic database.
"after having indexed all records in r, the index is ready for resolving query records. the complete records in r with all attribute values are also indexed into an inverted index or disk-based database table d, where the actual attribute values of records can be retrieved efficiently during the record comparison step, which is part of the query matching process."
"moreover, our results illustrated that using the similarity-based simdysni reduces the average comparison time between 20% and 70% (based on the number of attributes used to generate skv) while it increases the memory footprint between 13% and 40% for various sks. finally, the drawback of sensitivity to errors and variations at the beginning of skvs was addressed by proposing a multitree indexing approach that improved the matching quality while maintaining efficiency. our results also show that sks that are based on a concatenation of more than one attribute value are more suitable for real-time er since they reduce query time significantly while still achieving high matching accuracy."
"since many services in both the private and public sectors are moving online, organizations increasingly require real-time er (with subsecond response times) on query records that need to be matched with existing databases [cit] . these databases are often not static, but rather dynamic as queries potentially result in a record being modified, added, or even removed (depending upon the application)."
"in this paper, a new method for enhancing time delay estimation (tde) in colored noise is presented by joining noise reduction and p l -norm minimization. we first perform the improved subspace method for enhanced signals corrupted by colored noise and then we use the p l -norm minimization based tde method to estimate the time delay from the enhanced signals. experiment results show that the proposed joint algorithm can obtain more accurate tde than several conventional algorithms in colored noise, especially in the case of low signal-to-noise ratio."
"in addition, the results show that having a different number of duplicates in the febrl datasets does not affect window expansion for dysni-d, and similar to the results from the oz dataset from the previous set of experiments, dysni-d still has very limited expansion in the adaptive window. recall and query time values are also almost constant, which confirms the findings from the previous set of experiments where most of the found duplicates are located in the query record's node or its nearest neighboring nodes, which limits the expansion process for the dysni-d approach to a very small number of neighboring tree nodes."
"precision, which for a query record is the fraction of retrieved true matches over the number of retrieved candidate records, was not used in the experimental evaluation. this is because our approach returns the y top-matched candidate records when resolving a query record, which means that the number of retrieved candidates is always the same (y) for all resolved query records."
"presented an efficient two-steps procedure [cit] . in the first step, the global optimum β is estimated for each given d. the estimation for the global optimum β has the following three cases."
"time. in this set of experiments, we evaluate the effect of using multiple trees and different sk combinations (using all possible single attributes and concatenated pairs of attributes) on recall and average query time. dysni-s with a similarity threshold between 0.5 and 1.0 was used on the oz-1 dataset for this set of experiments. a single record in this dataset has an average of five corrupted duplicates. from figure 15 we can see that using more trees increases recall at the cost of a slight increase in the average query time. we can also see that although skvs generated from singleattribute values give better recall values, they require a longer time to resolve queries, which means that they are not suitable for real-time er. on the other hand, skvs that are generated from a concatenation of two attribute values reduce the average query time significantly but still achieve high recall values. the figure also shows fig. 15 . results for running m-dysni on the oz-1 dataset using different numbers of trees. \"s\" refers to using all single attributes as sk. \"d\" refers to using all sks generated from the concatenation of two attributes. \"xt\" refers to the number of trees. dysni-s is used with similarity thresholds from 0.5 to 1.0. fig. 16 . plot (a) shows the average recall values for the oz-x datasets with different corruption rates using all possible sks generated from the concatenation of two attributes. plots (b) and (c) show the average insertion and query times for the full nc dataset compared with the baseline. the m-dysni is used to generate the plots."
"in this section, we describe the experiments conducted to evaluate our proposed approaches. we start by describing the various datasets we used and the experimental framework, followed by a discussion of the obtained results."
"build phase: in this phase, records are loaded from dataset r, their skvs are generated, and they are inserted into the brt. the skvs become the key values skv used as tree nodes. if the skv of an inserted record is new (i.e., has not been indexed earlier), a new node is created in the tree for this skv, whereas if a skv already exists in the tree as a node key, then the identifier of its record is added to the list i of this node. for example, node n1 in figure 2 was generated when record r1 with skv \"percysmith\" was inserted into the empty index, while for record r8 with svk \"sallytaylor\" node n7 already exists in the brt (the node was generated when record r6 was inserted), and so the identifier r8 can be directly added to the list i of n7."
"-nc dataset 1 is a large real voter registration dataset from the u.s. state of north carolina. we have downloaded the nc dataset every 2 [cit] to build a compound temporal dataset. this dataset contains the names, addresses, and ages of around 8 million voters, as well as their voter registration numbers (the used attributes are 'firstname,' 'surname,' 'city,' 'zipcode'). each record has a time stamp attached that corresponds to the date a voter originally registered, or when any of their details have changed. this dataset therefore contains realistic temporal information about a large number of people. we identified 142,673 individuals with two records, 3,566 with three, and 92 with four records in this dataset. this dataset is used for scalability evaluation since it has a large number of records. -oz-x datasets: we generated four datasets with various corruption ratios using the geco data generator and corrupter [cit] for the purpose of investigating the effect of having different levels of data quality in attribute values on matching quality. the four datasets each contain 345,876 records of personal details ('firstname,' 'surname,' 'suburb,' 'postcode') selected randomly from a clean australian telephone directory, modified by adding duplicate records that had randomly corrupted attribute values based on typing, scanning, and ocr errors, or phonetic variations. \"x\" refers to the number of corrupted attributes in the dataset that we used (from oz-1 to oz-4). each entity is represented on average by five duplicates. these datasets are used to evaluate the effect of using different thresholds for the different proposed window approaches, and to evaluate how different levels of noise (i.e., different data quality) in a dataset affect the performance of the proposed approach. -febrl datasets: we generated three fully synthetic datasets where we specified the average number of records per entity (person) using the febrl data generator [cit] . this allowed us to evaluate our proposed approaches with regard to how datasets with different numbers of duplicates affect the dysni adaptive window approaches. the three datasets each contain 100,000 records consisting of name and address attributes. in the first dataset (named febrl-5) each entity is on average represented by five records (with a maximum of eight records per entity), in the second dataset (named febrl-10) each entity is on average represented by 10 records (with a maximum of 15 records per entity), and in the third dataset (named febrl-20) each entity is on average represented by 20 records, with a maximum of 30 records. records were generated by first creating an \"original\" record for an entity, followed by the application of various modifications to generate \"duplicate\" records (by applying keyboard edits, phonetic and ocr modification, and setting values to missing). these sets are used to evaluate the effect on the proposed approach of having a different number of duplicates in a dataset."
"query phase: in this phase (shown in algorithm 1, figure 3 ), a query record q is matched against the built index in real time. we assume that all query records are added to the dysni. when a query record arrives, the first step is to generate the skv for the record (line 1) and a new unique record identifier q.id is assigned to it (in the generatekey function). this skv and q.id are then inserted into the brt in the same way as records were inserted during the build phase (lines 3-7). the query record is also added into d."
"to facilitate real-time er, proposed a similarity-aware indexing technique where similarities between attribute values are precalculated during the building phase of the index. this approach is based on standard blocking and uses phonetic encodings to overcome errors and variations in attribute values to ensure that similar values are inserted into the same block. an average query time of 10msec per record on a large database of 7 million records was reported by the authors. however, this index was only applicable for static databases."
we have presented a dynamic tree-based sorted neighborhood indexing technique that can be used for real-time er on large databases. the technique was shown to be scalable with large databases as it has fast insertion and query times. we improved query times using a variation where we precalculate the similarities between the attribute values that are used to generate the sorting key values. we investigated several query matching approaches using both a fixed-size window and various adaptive window techniques. we showed that both the fixed-size window and the candidate-based adaptive window approaches provide more control over the time used to resolve queries. we also showed that the similarity-based adaptive window approach achieves better matching quality at the cost of requiring more time to resolve queries. any sorted indexing technique has the drawback of being sensitive to errors at the beginning of the sorting keys. we addressed this issue by proposing an index with multiple dynamic trees where each tree uses a different sorting key. we evaluated the proposed techniques using a large real-world and two synthetic datasets. this evaluation showed that the proposed dysni is suitable for real-time er.
"the estimates in equations (2), (3) and (4) also apply for m-dysni since the difference between dysni and m-dysni is that the latter index has several distinct trees that are built using different sks. this implies that the estimated maximum number of candidate records will be influenced by the number of trees in the index. the maximum number of candidate records using multiple trees, assuming a uniform distribution, is calculated as"
"among the various proposed window types, dysni-s has shown better recall values at the cost of increased query time, which makes it suitable for applications that need high-quality query matching results. as for dysni-f and dysni-c, they both can be used with applications that require a controlled time for resolving queries."
"where the denominator is the harmonic number of the partial harmonic sum [cit] b ]. the number of candidate record pairs s w in a window that includes 2w+1 nodes is then calculated as (4), with n the number of records in a dataset and k the number of nodes in the tree index."
"in the second test, we perform the four algorithms with the input snrs being 5 db via different values of d * . figure 3 and figure 4 display their rmse results of the four algorithms in factory noise and babble noise, respectively. from the two figures, we first see that the proposed algorithms outperform tde without speech enhancement in any value of d * . second, the proposed tde algorithm can get a lower value of rmse than the other two tde algorithms with noise reduction, based on the mmse-mss and map-mss estimators, respectively. this indicates that the proposed algorithm can obtain the best accurate tde in any value of d * ."
"the experimental results described previously illustrate the effectiveness of dysni. the fast insertion and query times achieved by the approach, and the ability to facilitate querying of large and dynamic datasets, make it effective for real-time er. moreover, simdysni improves query time by storing pre-calculated similarities between skvs of neighboring nodes in the index, but at the cost of extra memory requirement."
"only limited research has so far concentrated on real-time er, or on er for dynamic databases. a first approach for real-time er is based on a collective classification technique [cit] . the idea behind this approach is to not use all records in a database to resolve a query, but rather to extract only those records that are related to a query, and to build a collective clustering structure using these records only. although this approach achieves high matching quality, the authors stated that the average time needed to resolve one query record was 31.28sec for a database that contained 831,991 records. thus, this approach is not suitable for real-time er, and it also is not scalable to large databases since it is computationally expensive."
"in the er process, the comparison step is usually the most time-consuming step because of the calculations performed when candidate records are compared. estimating the number of comparisons beforehand gives users an insight about the expected runtime required to match a query record with a dataset of a certain size. in this section, we provide a way of estimating the number of generated candidate records using dysni and m-dysni."
"build phase: during the build phase, multiple trees are built using different sks where a record is inserted into every tree in the index. building one tree is similar to the build phase described in section 4 where records are loaded from a database, and their skvs are generated and inserted into the tree data structure. the steps for building one tree are repeated to build all trees using different sks. for example, we can use a 'firstname' attribute as a sk to build the first tree in the index, a 'surname' attribute to build a second tree, a 'postcode' attribute to build a third tree, and so on."
"in this section, we introduce a new method for enhancing time delay estimation (tde) in colored noise, based on joint noise reduction and p l -norm minimization. an improved subspace method for colored noise reduction is first performed. the time delay is then estimated by using the enhanced signal, based on the p l -norm minimization. the proposed tde algorithm is listed in algorithm 3. compared with conventional tde algorithms, the proposed tde algorithm can greatly reduce the interference of colored noise such that the tde accuracy is enhanced."
"8.2.10. scalability of the multiple-tree index. in this set of experiments we evaluate the scalability of the m-dysni on the full nc dataset using different numbers of trees. the results illustrated in figure 16(b) show that the average insertion times using the various numbers of trees is not affected by the growing size of the index data structure, while plot (c) shows that the average query time only increases slightly as the index becomes larger. as expected, the results show that using more trees increases the average insertion and query times, but the achieved times are still very fast (around 1 ms and 15 ms insertion and query time, respectively) for three trees. the memory required for the index of one, two, and three trees for the full nc dataset was 1.8, 3.6, and 4.3 gigabytes, respectively."
"outline: in the following section we provide an overview of relevant work related to real-time and dynamic er, and in section 3 we provide the required background including details about the sorted neighborhood method and the data structures that we use. in sections 4, 5, and 6 we detail the different variations of our approach. next, in section 7 we provide an analysis of the proposed approach in relation to estimating the number of generated candidate records. we experimentally evaluate our approach on different datasets in section 8, and conclude our article in section 9 with a discussion of future research directions."
"8.1.2. experimental baseline. two baseline approaches are compared with dysni. the first is the dynamic similarity-aware index (dysimii) [cit] ]. this approach is a dynamic indexing technique that is based on standard blocking. the dysimii has a build and a query phase. in the build phase, the similarities between attribute values are precalculated and stored in the index to reduce the required calculation in the query phase when a query record is matched. this method can be used for real-time er with dynamic, large databases. the second baseline approach is a qgram-based inverted index (qgi) [cit] b; [cit] ] that converts the attribute values of each record in the database into a list of q grams. each unique q gram becomes a key in the inverted index where its value is the list of all records in the database that have this q gram in their attribute values. to fig. 9 . the average time needed to generate candidate records for different types of trees. the plot is generated using the oz-1 dataset. match a query record with the q-gram inverted index, its attribute values are converted into a q-gram list, then it is compared only with records that have a certain number of common q grams that achieve a minimum similarity threshold. the approach returns a list of all records that have a jaccard-based similarity with the query record that is greater than the minimum similarity threshold."
"one of the main drawbacks of the snm is its sensitivity to data quality of the attributes used as skvs. specifically, if a skv has an error or variation at the beginning, then its record will potentially not be placed close to similar records, and therefore will likely be missed. for example, \"christine\" and \"kristine\" will not be close to each other in the sorted array if a \"firstname\" attribute was used as a sk. [cit], 1998 ]."
"er is related to general similarity search approaches, which involve finding similar entities from unstructured databases (such as e-mails, news articles, or scientific publications) based on a collection of relevant features that are represented as points in high-dimensional attribute spaces [cit] . however, such approaches are less suited for structured databases that contain well-defined attributes with short values, such as personal names, addresses, or dates of birth. records in structured databases can be matched using structured query language (sql) join statements if unique entity identifiers, such as passport or social security numbers, are available. however, such identifiers are commonly not available, and therefore er approaches need to be employed [cit] ."
"massive amounts of data are nowadays being collected by most business and government organizations. given that many of these organizations rely on information in their day-to-day operations, the quality of the collected data has a direct impact on the quality of the produced outcomes. various data cleaning practices are employed to improve the collected data. one important practice in data cleaning is the task of identifying all records that refer to the same real-world entity [cit] . this process is commonly called entity resolution (er) [cit] ] . a realworld entity can be a person, a product, a business, or any other object that exists in the real world. examples of multiple records in a database representing a single entity include a patient who is represented several times in a hospital database, a product that is inserted many times into an inventory list, or a voter who is registered more than once in an electoral roll. these duplicates, if not removed or merged, can lead to serious consequences for organizations or individuals. a patient's information could, for example, be dispersed between duplicated records leaving medical staff unaware of the patient's overall condition, which can potentially affect diagnosis and treatment, while duplicate records in an electoral roll can lead to voting irregularities."
"as for indexing techniques that are used in er, standard blocking and the sorted neighborhood method (snm) are commonly used. standard blocking [cit] b ] is based on inserting records into blocks according to blocking key criterion and only comparing records that are in the same block. the snm [cit] arranges all records in the database(s) to be matched into a sorted array using a sorting key criterion. then a window is moved over the sorted records, comparing only those records that are within the sliding window at any one time (explained in more detail in section 3.1). both blocking and sorting keys are usually based on one or a concatenation of attribute values."
"the process of retrieving candidate records from a single tree is similar to what is described in section 4 using any of the different window approaches. however, candidate records are retrieved from every tree in the m-dysni each adding candidate records into the overall candidate record set c, which becomes the union of candidates returned from the different trees. then the query record q is compared with all unique records in c in detail using similarity comparison functions [cit] in the same way as is done in the dysni. the process of merging the returned sorted candidate records from each tree in the index has an overhead that is not present when only one tree is used."
"we implemented our dysni, simdysni, and m-dysni approaches using python (version 2.7.3) and ran all experiments on a server with 128gbytes of main memory and two 6-core intel xeon cpus running at 2.4ghz. to facilitate repeatability of our experiments, the prototype codes and the synthetic datasets are available from the authors."
"various other indexing techniques have been developed for er, including q-gram indexing [cit] ], suffix array indexing [cit], canopy clustering [cit], mapping-based indexing [cit] ], and hashing-based indexing [cit] . however, all these techniques are aimed at offline batch processing of databases and are limited to indexing of static data. this means that once an index is created it is difficult to modify if new records need to be added, or when the values in existing records are changing."
"indexing techniques that are based on sorting records in the database using a sk have the drawback of being sensitive to errors that occur at the beginning of a skv [cit] . to overcome this issue, we propose a multitree index based on the dysni (m-dysni). the index consists of multiple tree data structures where each tree is built using a different sk. in real-world data, attribute values are likely not completely independent of each other. however, we assume that sks are selected by domain experts to be as complementary to each other as possible (a technique for learning optimal sorting keys has recently been developed [cit] ). using several trees with different sks can help improve the quality of results in cases where errors and variations occur at the beginning of attribute values. for example, 'christine' and 'kristine' will not be inserted into the same tree node if a 'firstname' attribute was used as a sk, but they might be inserted into the same node in another tree where a different sk is used. the m-dysni has two phases."
"our proposed dynamic sorted neighborhood index (dysni) is based on a brt as described previously. the aim of the dysni is to dynamically index and resolve a stream of query records in real time. we assume we keep all records in the dataset r unmodified after they are created, since they can provide evidence about earlier queries on individual entities. an example application is applying for consumer credit where an individual's credit history needs to be retrieved and evaluated before a new loan can be approved. replacing records with their cleaned and merged versions will likely result in a loss of accuracy, because details such as previous names or addresses of a customer are lost. the dysni has an initial build phase where a certain number (possibly none) of entity records from an existing database are inserted into the brt. the built index is then used to generate candidate records to resolve query records during the query phase. the dysni is dynamic since query records can be added into the brt as they arrive."
to evaluate our approaches we conducted several sets of experiments as described next. note that the sk used for conducting all of the following experiments is the concatenation of 'surname+firstname' attribute values.
"with real-time er, the aim is to match a query record with all records in the dataset that represent the same entity as the query record in the least possible time (subsecond time). thus, we focus on measuring the quality of obtained results, and the efficiency of the compared approaches. we use recall (the fraction of retrieved true matches over the total number of true matches) to measure the quality of the compared approaches, and both insertion time (time required to insert a single record into the index) and query time (time required to resolve a single query record) to measure efficiency."
"for future work we plan to extend the proposed index using a combination of a b+ tree and brt to work with disk-based memory to allow indexing of very large datasets that do not fit into main memory. we also plan to parallelize the multiple-tree index to improve performance. moreover, we plan to explore how dysni can be integrated with classification and clustering techniques [cit] ] to make the complete er pipeline applicable for real-time matching."
"the window of neighboring nodes can now be generated (line 8). all record identifiers that are stored in the nodes within the window are added to the candidate record set c. whole records (for each record identifier within c) are then retrieved from the inverted index or database table d, and the attributes of q are compared with the retrieved records using similarity comparison functions [cit] appropriate to the content of each attribute (line 9). the compared candidate records are returned in the list m sorted according to their overall similarities with the query record (line 10)."
"as can be seen from figure 11, the dysni approaches significantly outperform the earlier dysimii by up to one order of magnitude faster insertion time. as for the query time, the results show that the various proposed approaches have between one to two orders of magnitude faster query time than the dysimii approach while achieving similar recall values. moreover, the results show that the average insertion times are not affected by the growing size of the index data structure, while the query time only increases slightly as the index becomes larger. as expected, larger window sizes lead to slower query times, and the dysni-s approach is slower than the dysni-d and dysni-c adaptive approaches. this is due to the fact that the calculation of similarities between skvs is an overhead of the dysni-s approach that does not occur with the other two adaptive approaches. additionally, the different proposed approaches generate different sets of candidate records that lead to different query times. however, the results show that the different variations of the proposed dysni approach achieve very fast average query times that range between 0.02 and 3.0msec per query record. 8.2.5. the effect of using different thresholds on quality and efficiency. in this set of experiments, we investigated the effect of using different window sizes and thresholds on the quality of the obtained results and the efficiency of the approaches. we compare the various proposed approaches and the dysimii and qgi baselines. the oz dataset (with only one duplicate per record) was used for this set of experiments."
"is not suitable for dysni. both baselines performed better than dysni-d in regard to matching quality, but dysni-d achieved lower query time. note that the recall values for qgi increases when the jaccard similarity threshold decreases. however, it achieves almost constant query time for all different thresholds."
"query phase: in this phase, a query record q is first inserted into the m-dysni and then it is matched in real time against all trees that were constructed in the build phase. a new unique identifier is created for q and the different skvs that are associated with the different trees in the index are created. q is then inserted into all trees using these skvs and its record identifier is added in the same way records were inserted during the build phase (section 4). the full attribute values of q are also added to d."
"another major drawback of the basic snm is the fixed setting of the window size w. if w is set too small, true matches are likely missed; on the other hand, if it is too large, unnecessary comparisons between records will be conducted. this problem has recently been addressed by two approaches that adaptively adjust the window size according to the characteristics of the skvs or database records. one approach expands the window size if skvs are similar with each others' according to a minimum similarity threshold [cit], while an alternative approach expands a window if a certain minimum number of records are classified as matches within the current window [cit] ."
"8.2.6. the effect of having different number of duplicates. the aim of this set of experiments was to investigate the effect of the number of duplicate records on recall, query times, and on the expansion of the adaptive window in both dysni-s and dysni-d approaches. these experiments are conducted using the febrl datasets. from the results shown in figure 13, we can see dysni-s outperformed qgi with regards to both recall and query times. qgi did not perform well for febrl datasets. we can also see that dysimii gives better recall values than both dysni-s and dysni-d and that, in general, the recall values achieved by the two adaptive approaches are less than the recall values achieved with the oz dataset in the previous set of experiments. this is due to the fact that in the oz dataset the maximum number of duplicates that a record can have is one, while in the febrl datasets the average number of duplicates ranges between 5 and 20. having a larger number of modified duplicates in the datasets increases the chance of having an error in the first character of the attribute value that is used as a skv. additionally, because the proposed approach is based on sorting records alphabetically according to skvs, this increases the chance of having records located far away from other records that represent the same entity in the index data structure. this issue can be resolved by building multiple trees using different skvs (section 6) to increase the chance of having records that represent the same entities close to each other. the results of using multiple trees are shown in figure 15 ."
"to be able to conduct the er process in real time we must consider completing each of its steps within the minimum time possible. indexing is one of the most important steps in real-time er as it reduces the search space, which leads to reducing the number of comparisons required in the comparison step. moreover, when comparing records, we should aim to use efficient comparison functions and limit the required calculations to perform this step (refer to section 5 for a suggestion on how to reduce such calculations). in the classification step, most classification models can be used regardless of which indexing techniques we use. however, for real-time er, classification techniques that depend on the summation of comparison vectors to reach a classification decision can be more suitable as they are computationally efficient compared to more complex supervised and unsupervised techniques. in this article, we mainly focus on the indexing step. the following provides a review of relevant work on indexing, as well as the area of real time and er."
"this third adaptive approach is based on [cit] . the authors used an adaptive window size that grows or shrinks based on the number of classified matches that are found within the window. the window slides over the static array starting from the first to the last record in the index to match records in the whole database. in our approach (shown in algorithm 3, figure 5 ), we adaptively expand a window on each side of the query tree node based on the following steps."
"the snm in its current form (when using a fixed or an adaptive window size, or when using several runs of different sorting keys) is only suitable for indexing static databases and for batch-oriented er. due to the static nature of the sorted array it does not work for real-time er applications where a stream of query records needs to be matched against a database consisting of entity records, and where these query records are commonly inserted into the database after matching. our proposed technique, described in section 4, provides an indexing technique that facilitates real-time er, and can handle dynamic databases. next, we present the basic data structures we use in our indexing approach."
"second, we measure the average total query time for queries with indexed skvs (where the skv of the query record already exists in the index) for both dysni and simdysni approaches. the table in figure 14 presents the average overall query time required for queries with indexed skvs using various skvs for of the oz dataset. the results show that using simdysni for queries with indexed skvs the overall average query time has improved by 44% to 93% for the various skvs. table iii provides the percentages of both queries with new skv and queries with indexed skv when using various sks for the nc dataset. results show that between 13% and 99% of arriving queries can benefit from query time improvement by simdysni for queries with indexed skv for the different sks."
"in this set of experiments we evaluated whether the proposed dysni scales to large databases while facilitating real-time er. we measured the average time required to insert a single record, and the average query time required to resolve a single query record across the growing size of the index structure. these experiments were conducted on 2.5 million records from the nc datasets. the proposed approach with a fixed-size window (dysni-f), a candidate-based adaptive window (dysni-c), a similarity-based adaptive window (dysni-s), and a duplicate-based adaptive window (dysni-d) was compared with the dysimii baseline technique. threshold selection for the various proposed approaches was based on achieving the same recall value for all compared approaches."
"the similarity-based (dysni-s) approach showed better recall results since the expansion decision in this approach depends on the similarities between skvs. because these skvs are sorted, neighboring nodes are likely to have similar skvs. the duplicate-based (dysni-d) approach does not work effectively because in dysni all records with the same skv are inserted into the same tree node, which limits window expansion and reduces the quality of the achieved results. candidate-based adaptive windows (dysni-c), on the other hand, can be used to control and limit the number of comparisons to achieve lower query times by choosing a low minimum number of candidates threshold."
"that when using three trees recall can be increased significantly compared to when using a single tree. also, using multiple trees allows using more strict sks (like the concatenation of more than one attribute) to build multiple trees with smaller node sizes while maintaining matching quality. we obtained similar results with the other oz-x datasets where two, three, or four attribute values have been corrupted (see figure 16(a) )."
"er is challenging because databases usually do not contain unique entity identifiers. in this case, partially identifying attribute values (such as names and addresses) need to be used for the matching process. however, such attribute values are often of low quality, as they can be incomplete, contain errors, or change over time [cit] . therefore, approximate matching techniques are generally required."
"the original snm uses a static array data structure to store the skv of all records in the databases that are to be deduplicated or matched [cit] . however, a static array is not suitable for dynamic data because each time a new record is added to the index the existing elements in the array would need to be shifted to maintain the order, leading to a worst-case complexity of o(n), where n is the number of records in a database. real-time er on dynamic databases requires an index data structure with efficient searching, inserting, and retrieving capabilities. search trees are more efficient than sorted arrays [cit] and are commonly used for indexing in different application domains. in the following, we describe different search tree data structures."
"query phase: in the query phase of the simdysni approach we benefit from the precalculated similarities that are stored in each node to reduce the time needed to resolve a query. querying the built index (from the build phase) is based on two cases (as shown in algorithm 4, figure 5 ): the first case occurs when the skv of a query record q is new and it has not been indexed earlier. the second case occurs when the skv of a query record q has been indexed previously and it already exists in the tree."
"(1) new skv: in this case (lines 4-10), because q is new, we create a new node n q for this query and we resolve it using the original dysni approach as described in section 4 (i.e., without benefiting from any precalculated similarities). after resolving the query, we generate the two precalculated similarity lists (i.e., s p and s n ) for both directions for n q by calculating the similarities for its w next and previous neighboring tree nodes (if we are using a fixed-size window) or until a similarity threshold is reached (if we are using a similarity-based adaptive window). next, we update the similarity lists for all w previous and next tree nodes of the newly inserted tree node (lines 9 and 10). this step ensures that the precalculated similarities are up to date at any time. (2) indexed skv: in this case (lines 11-15), because the skv of the query record already exists in the s-brt, there is no need to create a new node, and all required precalculated similarities are ready for use. in this case, a query can benefit from using these similarities as described in the next paragraph."
"to estimate the minimum and maximum numbers of candidate records for a query q generated using a fixed-size window approach (assuming window size w) we calculate s w according to the illustration given in figure 7 . the largest number occurs when the 2w tree nodes that are neighbors of the query tree node are the 2w largest tree nodes. on the other hand, the smallest number of candidate records are generated if the query node n q is at either end of the tree, and the w neighboring tree nodes are the smallest in size. figure 8 illustrates estimates of the minimum, average, and maximum number of candidate records for increasing sizes of datasets based on equations (3) and (4). from the figure, we can see that the maximum estimated number of candidate records increases linearly with the growing size of the dataset, while the minimum estimated number decreases with larger datasets, because the number of nodes k increases, which leads to smaller numbers of records in each node."
"in order to derive guidelines for the transformation, we start by considering a fork program that only consists of one function, does not perform recursion, and where the initial group is not split. all processors of the group e.g. synchronously access a shared array with read, write, and another read, cf. left half of fig. 3 . one would think that this function can be directly transformed into one kernel consisting of a grid with one block, and where all threads work on the shared memory of an sm. however, we already run into problems here. first, the number of processors might be larger than 512, the largest possible size of one block. this means that the grid must consist of more than one block, with the possibility that two blocks are executed simultaneously on two different sms. thus, placing the shared array in the shared memory of an sm is not possible, and we have to place all shared variables in the global memory. we will also do this with private variables, on the one hand because the shared memory is very small, on the other hand because values of private variables sometimes must be transferred from one kernel to the next, see below. for groups with more than 32 processors, we also run into the problem that not all threads are running synchronously, either because a block comprises more than 32 threads, or because more than one block is used, and the blocks are scheduled one by one to the same sm. therefore, the threads must be synchronized after the write access to the shared array, and before the following read access. however, synchronization cannot be done within the kernel, as synchronization between blocks may lead to a deadlock if one block has not yet started because it is scheduled to the same sm that is currently busy with the block that tries to synchronize. also it is not possible to call a synchronization by the host program from the slave program. therefore, the only possibility is to split this fork function into two parts that are executed in separate kernels by the host program, cf. right part of fig. 3 . the first kernel devicefunc1 comprises the read and write access, and the second kernel devicefunc2 comprises the second write access. the synchronization between them is necessary to ensure that the first kernel has completely terminated and all caches have been flushed, i.e. that all writes are written to the global memory, before any of the reads from the second kernel is executed."
"in this paper, we investigate whether fork programs can be (semi-)automatically transformed into cuda programs and how much overhead is incurred by arbitrarily large group sizes, dynamic group partitioning, and use of large memories. our conclusions are that automatic transformation is possible, but that further work might be necessary to tune performance. our preliminary experiments indicate that the cuda program on a gpu achieves a notable speedup of almost 100 over the sequential pram simulator running on the cpu, and depending on the application even over a sequential program solving the same problem on the cpu. we analyze the overhead incurred, mainly kernel synchronizations, and conclude that future gpus with tighter integration to the cpu could largely reduce this overhead, so that beyond classroom use, fork programming might also become more competitive in real life."
"another possible source of optimization is the design of the original fork programs, which often have been written with a certain space cost model in mind. for example, for the sbpram with its strict embedding of private address subspaces into the size-limited global shared sbpram memory, it was more space-economic to pass readonly parameters to functions as shared parameters (on the shared stack) rather than as private ones, because this reduces overall memory usage while there is no difference in execution time. this however leads to worse performance with the transformation approach, where private parameters could be passed on private stacks stored or cached in gpu shared memory instead, while shared ones have to stay in gpu global memory."
"in the situations considered so far, all processors have formed a single group. if we now consider the code from fig. 1, the group of processors is split into two subgroups, because the condition in the if-statement depends on a private variable, in this case on the processor id. the different subgroups execute the then and else branches of the if-statement, respectively, and then are merged again into the previous group, and perform the final assignment. the code of the resulting host program can be seen in fig. 4 . the kernel devicefunc1 contains the first assignment to variable loc and the evaluation of the condition. then, function computesubgroups of the host program computes how many of the n pram processors take the then path (n1) and how many take the else path (n − n1). the synchronization after devicefunc1 is necessary to ensure that all condition results have been written before they are evaluated by the host program. the host function computesubgroups also updates the group structure in global memory, i.e. transfers data to the global memory. for each subgroup, the participating processors must be stored, their new group-local ids have to be assigned, and so on. now the kernels devicefunc2then and devicefunc2else are started with the respective number of threads, and as the subgroups are independent, they can be executed in parallel. after the kernels, a synchronization is necessary to ensure that both kernels have completed and all data values are written to the global memory. the group structure is updated in host function restoregroup to restore the previous group, and the final assignment to variable loc is performed in the kernel devicefunc3. note that if the then or else branches would contain further statements, then further kernels would be necessary, as the statements in both branches write to shared variables, so that a synchronization in the subgroups would be necessary prior to follow-up statements."
"in the right part we see that the transformed parallel quicksort on the gpu is slower than a sequential quicksort on the cpu. this is not surprising as the amount of work in sorting 1 million numbers is small compared to the overhead of repeated group split. we consider this still an advantage as the compiled fork code on the pramsim was not able to handle such a data size. with a growing number of threads, a runtime reduction is also notable, indicating that the transformed code is scalable."
"here, we describe recent work toward a theory of cortical visual processing. unlike other models that address the computations in a given brain area (such as primary visual cortex) or attempt to explain a particular phenomenon (such as contrast adaptation and specific visual illusion), we describe a large-scale model that attempts to mimic the main information-processing steps across multiple brain areas and millions of neuron-like units. a first step toward understanding cortical functions may take the form of a detailed, neurobiologically plausible model, accounting for the connectivity, biophysics, and physiology of the cortex."
"wsns require secure routing schemes to prevent adversaries from finding out the source node location by back tracing attacks. to meet these requirements, this paper has proposed a location based random routing scheme. the scheme divides the wsn domain into two regions and packets are randomly forwarded to the sink node through random diversion or mediate nodes according to the location of the source node. the diversion and mediate nodes are strategically positioned in the sensor domain to ensure that packets for source nodes near the sink node are routed through longer and more random routes as compared to other schemes. if the adversary decides to perform a back tracing attack against the proposed scheme, it will certainly be confused in the effort to guess the route for the next packet. a cautious adversary will achieve very little progress towards the source node since the routing paths have high path diversity and successive packets from the same source node use highly random routes. simulation results show that the proposed scheme can provide strong source location privacy and outperform traditional routing schemes, even when a source node is located in near-sink regions. the proposed scheme is applicable in systems which require high source location privacy and can tolerate an acceptable increase in energy consumption and communication overhead. as part of future work, techniques to improve energy consumption in the network will be considered."
"considering a cautious adversary who waits for a specified amount of time at a node and rolls back to a previous node when the timer expires, the success rate of the adversary is considerably reduced in the proposed scheme as compared to the shortest path, phantom single-path, and directional random routing schemes. owing to the highly random routing paths in the proposed scheme, the adversary will wait for a much longer time at a node before it receives a packet at the same node again. for example, in figure 5, if the adversary receives k_ f1 forwarded by node m p, it will back trace to node m p . however, the next packet k_ f2 is routed through the mediate node m q, which is very far away from m p . if the waiting time exceeds the waiting timer, the adversary will find itself rolling back to the previous node and make insignificant progress towards the source node. for a successful back tracing attack to the source node, the adversary needs to intercept many packets. if the packets use very different routes in the network, it will take longer for the adversary to receive enough packets to intercept and successfully locate the asset. the adversary might find itself using a longer time than the safety period of the scheme and the asset will possibly move to a new location before the adversary locates the source node."
"not all nvidia processor architectures provide a stack, so not all support recursion in hardware. also, not all nvidia architectures provide caching of the global memory. if they do, there is an l1-cache for each sm, but those caches are not coherent. furthermore, there is a single l2-cache which is shared by all sms. the fermi architecture e.g. provides caches and stack, while the tesla architecture does not."
"to ensure the proposed scheme provides higher privacy compared to the shortest path, phantom single-path, and directional random routing schemes, the selection of the mediate node or diversion node is random and location-based. source nodes in region h randomly forward packets to sink node through diversion node regions p or q. source nodes in region nh randomly forward packets to sink node through mediate node regions p or q. the use of either the mediate or diversion node regions p or q significantly increase the privacy level of the proposed scheme as compared to the shortest path, phantom single-path, and directional random routing schemes. as the regions p and q are strategically positioned on opposite sides of the sink node, successive packets will use highly random routing paths and may arrive at the sink from completely different directions as shown in figure 5 . these paths will not be related as they do in the shortest path, phantom single-path, or directional random routing schemes."
the remainder of this paper is organized as follows: section 2 presents a review of the literature on routing schemes for source location privacy. section 3 gives overview of the system including the network and adversary models. the proposed strategic location-based random routing scheme is described in details in section 4. privacy analysis of the proposed routing scheme is presented in section 5. performance analysis and evaluation together with the simulation results are presented in section 6. the paper concludes in section 7.
"the role of the anatomical back-projections present (in abundance) among almost all areas in the visual cortex is a matter of debate. a commonly accepted hypothesis is that the basic processing of information is feedforward, 30 supported most directly by the short times required for a selective response to appear in cells at all stages of the hierarchy. neural recordings from it in a monkey 12 show the activity of small neuronal populations over very short time intervals (as short as 12.5ms and about 100ms after stimulus onset) contains surprisingly accurate and robust information supporting a variety of recognition tasks. while this data does not rule out local feedback loops within an area, it does suggest that a core hierarchical feedforward architecture (like the one described here) may be a reasonable starting point for a theory of the visual cortex, aiming to explain immediate recognition, the initial phase of recognition before eye movement and high-level processes take place."
"agreement with experimental data. [cit] s, 24, 29 the model in figure 2 has been able to explain a number of new experimental results, including data not used to derive or fit model parameters. the model seems to be qualitatively and quantitatively consistent with (and in some cases predicts 29 ) several properties of subpopulations of cells in v1, v4, it, and pfc, as well as fmri and psychophysical data (see the sidebar \"quantitative data compatible with the model\" for a complete list of findings)."
the remainder of this paper is organized as follows: section 2 presents a review of the literature on routing schemes for source location privacy. section 3 gives overview of the system including the network and adversary models. the proposed strategic location-based random routing scheme is described in details in section 4. privacy analysis of the proposed routing scheme is presented in section 5. performance analysis and evaluation together with the simulation results are presented in section 6. the paper concludes in section 7.
"upon reception of the packet, the mediate node determines a group of neighboring nodes with shorter hop distance to the sink node than the mediate node itself. one neighboring node from the group is randomly selected as the next-hop node. the mediate node randomly forwards the packet to the next-hop node and eventually to the destination sink node. figure 5 shows an example packet routing strategy using the proposed scheme. in the figure, s f and s g are the source nodes located in regions nh and h, respectively. the figure shows that if s f sends two packets k_ f1 and k_ f2, the packets will use very different route paths depending on the selected mediate node. packet k_ f1 uses the mediate node m p from the mediate nodes in region p, while k_ f2 uses the mediate node m q from the mediate nodes in region q. similarly, for s g, packet k_ g1 uses the diversion node d p from the diversion nodes in region p, while k_ g2 uses the diversion node d q from the diversion nodes in region q. figure 6 shows the flowchart of delivering a packet from the source node to the sink node using the proposed routing scheme. node. the source node randomly forwards the packet to the next-hop node and eventually to the randomly selected mediate node in regions p or q. phase 2 upon reception of the packet, the mediate node determines a group of neighboring nodes with shorter hop distance to the sink node than the mediate node itself. one neighboring node from the group is randomly selected as the next-hop node. the mediate node randomly forwards the packet to the next-hop node and eventually to the destination sink node. figure 5 shows an example packet routing strategy using the proposed scheme. in the figure, sf and sg are the source nodes located in regions nh and h, respectively. the figure shows that if sf sends two packets k_f1 and k_f2, the packets will use very different route paths depending on the selected mediate node. packet k_f1 uses the mediate node mp from the mediate nodes in region p, while k_f2 uses the mediate node mq from the mediate nodes in region q. similarly, for sg, packet k_g1 uses the diversion node dp from the diversion nodes in region p, while k_g2 uses the diversion node dq from the diversion nodes in region q. figure 6 shows the flowchart of delivering a packet from the source node to the sink node using the proposed routing scheme."
"packet routing of the proposed scheme begins when a source node detects an asset. upon detection of the asset, the source node generates and encrypts data packets to send to the sink node through multi-hop routing."
"to compare the shortest path routing and the proposed scheme, if a node is located four hops away from the sink, the shortest path routing scheme will find the shortest route to the sink, which may be not more than four hops to the sink. this shortest route allows the adversary to successfully back trace to the location of the source node within a short time. additionally, using the shortest"
"as chip density still grows exponentially, following moore's law, the near future will bring architectures that accommodate both cpu and gpu on the same chip, such as amd fusion (fusion.amd.com). this will provide a much higher bandwidth between cpu and gpu compared to today's pcie interconnect, a tighter integration and sharing along the cpu memory hierarchy with the gpu, and a much finer task granularity than what is efficiently doable with today's gpus. we expect that this will improve the performance achievable on such systems for fork programs translated into cuda or opencl."
"we have presented the transformations for a source-to-source compiler from fork programs for prams to cuda c for gpus, to enable the use of fork for practical pram programming beyond toy examples, with reasonable runtimes. while many improvements are still possible, our preliminary experimental results indicate that for the simple, non-recursive sieve application, we achieve speedup compared to both the pram simulator on a cpu, and a sequential version of the program, while for the highly recursive quicksort, we can cope with data set sizes previously out of reach for the simulator, and \"only\" suffer a slowdown of 6 compared to a highly optimized sequential version of quicksort."
"d in the figure 2 model, we assumed unsupervised learning from v1 to it happens during development in a sequence starting with the lower areas. in reality, learning might continue throughout adulthood, certainly at the level of it and perhaps in intermediate and lower areas as well."
"the fork statement is used in fork to explicitly split a group into several subgroups, by having each processor evaluate two private expressions that return its new group and member index. when a fork statement is reached, the control flow is handled similarly to an if-statement. the difference is that in a fork statement, more than two subgroups can be formed, so that the updates to the group structure are more involved."
"if a grid comprises more blocks than sms are available, the blocks are scheduled in arbitrary order. as soon as an sm gets available by completion of its current block, it executes another block."
"it is assumed a network operator will perform pre-deployment phase to determine the network size and division of the network according to figure 3 and as explained in section 4 above. network initialization process follows after pre-deployment phase is complete. it is assumed that each node is informed about its own location, location of the neighboring nodes and of the sink node during network initialization process. the first step in network initialization process is to load each sensor"
"upon detection of the asset, the source node generates a bias random number r n ranging from 0 to 1 and compares it to a predefined threshold t. if r n is less than the threshold t, the source node randomly selects one of its neighboring nodes in the direction of the mediate node region p. one mediate node is randomly selected from the mediate node region p. otherwise, the source node randomly selects one of its neighboring nodes in the direction of the mediate node region q. one mediate node is randomly selected from the mediate node region q. the source node determines a group of neighboring nodes with shorter hop distance to the randomly selected mediate node than the source node itself. one neighboring node from the group is randomly selected as the next-hop node. the source node randomly forwards the packet to the next-hop node and eventually to the randomly selected mediate node in regions p or q."
"unsupervised learning in the ventral stream of the visual cortex. with the exception of the task-specific units at the top of the hierarchy (\"visual routines\"), learning in the model in figure 2 is unsupervised, thus closely mimicking a developmental learning stage."
"our visual cortex may serve as a proxy for the rest of the cortex and thus for intelligence itself. there is little doubt that even a partial solution to the question of which computations are performed by the visual cortex would be a major breakthrough in computational neuroscience and more broadly in neuroscience. it would begin to explain one of the most amazing abilities of the brain and open doors to other aspects of intelligence (such as language and planning). it would also bridge the gap between neurobiology and the various information sciences, making it possible to develop computer algorithms that follow the information-processing principles used by biological organisms and honed by natural evolution."
"a key computational issue in object recognition a is the specificity-invariance trade-off: recognition must be able to finely discriminate between different objects or object classes (such as the faces in figure 1 ) while being tolerant of object transformations (such as scaling, translation, illumination, changes in viewpoint, and clutter), as well as non-rigid transformations (such as variations in shape within a class), as in the change of facial expression in recognizing faces."
"the main contributions of this paper can be summarized as follows: (1) to address the limitations of shortest path routing, phantom single-path routing and directional random routing schemes by proposing a new routing scheme that uses strategically positioned mediate and diversion nodes to significantly improve path diversity of the routing paths between the source and sink nodes; (2) to conduct a series of experiments to evaluate the performance of the proposed routing scheme; (3) to demonstrate that the proposed scheme provides stronger source location privacy than shortest path routing, phantom single-path routing, and directional random routing schemes."
"a similar situation occurs when a function call occurs in a fork function. as not all gpus support a stack, and as cuda device function visibility is restricted to its source file, the function call cannot be implemented within a device function. also in this case, the fork function is split, and the host program calls two kernels: one that contains the code prior to the function call, the other contains the code after the function call. the function call itself is implemented via a call in the host program (at the place where the synchronization occured above) to the host function implementing the called function."
"gpus. a graphics processing unit (gpu) is a massively parallel processor on a chip. it mainly serves for rendering images, but can also be used for general purpose computations (gpgpu computing). figure 2 provides a very simplified view of a gpu architecture. several streaming multiprocessors (sm) form the processing hardware. each sm is equipped with a number of hardware-multithreaded processing units called scalar processors (sp) that work synchronously on a small, fast shared memory (16 to 48 kbyte). all sms can access the large but slow global memory of the gpu, which is also used for data transfer to and from the host, i.e. the cpu. the global memory (together with a constant memory and a so-called local memory, which we will not discuss further) are realized by dynamic random access memory (dram) on the graphics card, external to the gpu chip."
"a set of (up to 32) threads that are executed simultaneously and synchronously on the processing units of a sm is called a warp. warps are executed like simd operations; where control flow diverges in a warp, the warp executions along the different paths are serialized, i.e. processors not participating in the current path are masked out. each block is scheduled for execution on an sm. so if the block comprises more threads than fit into one warp, the block's threads are executed warp by warp; the dynamic scheduling of warps is done by a hardware scheduler and not under user control."
"wsns require secure routing schemes to prevent adversaries from finding out the source node location by back tracing attacks. to meet these requirements, this paper has proposed a location based random routing scheme. the scheme divides the wsn domain into two regions and packets are randomly forwarded to the sink node through random diversion or mediate nodes according to the location of the source node. the diversion and mediate nodes are strategically positioned in the sensor domain to ensure that packets for source nodes near the sink node are routed through longer and more random routes as compared to other schemes. if the adversary decides to perform a back tracing attack against the proposed scheme, it will certainly be confused in the effort to guess the route for the next packet. a cautious adversary will achieve very little progress towards the source node since the routing paths have high path diversity and successive packets from the same source node use highly random routes. simulation results show that the proposed scheme can provide figure 7a shows the path diversity of the analyzed schemes. privacy is directly related to path diversity. as the path diversity available for each packet is enhanced, higher source location privacy is guaranteed. path diversity can be categorized into length and path variations."
"one of the challenges that face multi-hop communication wireless networks is creating secure and private applications. this is owing to their potential to expose important information as packets are broadcasted across the network. security measures, such as cryptographic techniques like encryption, decryption, and authentication, are used to secure the integrity of data and protect the content of the packets but the context of the broadcast remains exposed to adversaries. adversaries can use expensive radio transceivers to interact with the network, monitor the pattern of broadcasts and back trace them to the location of the source node [cit] . this has motivated researchers to design schemes for preserving source location privacy in wsns."
"content-based recognition and search in videos is an emerging application of computer vision, whereby neuroscience may again suggest an avenue for approaching the problem. [cit], we developed an initial model for recognizing biological motion and actions from video sequences based on the organization of the dorsal stream of the visual cortex, 13 which is critically linked to the processing of motion information, from v1 and mt to higher motion-selective areas mst/fst and sts. the system relies on computational principles similar to those in the model of the ventral stream described earlier but that start with spatio-temporal filters modeled after motion-sensitive cells in the primary visual cortex."
"these stages can be learned during development from temporal streams of natural images by exploiting the statistics of natural environments in two ways: correlations over images that provide information-rich features at various levels of complexity and sizes; and correlations over time used to learn equivalence classes of these features under transformations (such as shifts in position and changes in scale). the combination of these two learning processes allows efficient sharing of visual features between object categories and makes learning new objects and categories easier, since they inherit the invariance properties of the representation learned from previous experience in the form of basic features common to other objects. in the following sections, we review evidence for this hierarchical architecture and the two correlation mechanisms described earlier."
"the adversary can only perform passive attacks, such as eavesdropping the network communication, and does not interfere with the proper operation of the network. it cannot modify packets, alter the routing paths, or destroy sensor devices, since such activities can easily be detected and could put the adversary at risk of being caught. the adversary is local and cannot monitor the entire network. kerckhoff's principle is applied and the worst case scenario with an adversary is assumed. in the worst case scenario, it is assumed that the adversary knows the methods being used by the network. the adversary is aware of the location of the sink node, the network topology, and the routing algorithm used in the network. however, it does not know the possible location of the asset. the back tracing strategy of the adversary is summarized in algorithm 1. the adversary is cautious and records all the sensor nodes it has visited to avoid revisiting the same nodes and getting trapped in loops. it uses a timer to limit its listening time at a node and returns to the previous node after the set timer expires. if (packet comes to adversarylocation from immediatesender before timer timeout) then 7:"
learning and plasticity. how the organization of the visual cortex is influenced by development vs. genetics is a matter of debate. an fmri study 21 terial http://serre-lab.clps.brown.edu/ resources/ [cit] for details).
"application to computer vision. contributedarticles es, 13 finding that the model of the dorsal stream competed with a state-ofthe-art action-recognition system (that outperformed many other systems) on all three data sets. 13 a direct extension of this approach led to a computer system for the automated monitoring and analysis of rodent behavior for behavioral phenotyping applications that perform on par with human manual scoring. we also found the learning in this model produced a large dictionary of optic-flow patterns that seems consistent with the response properties of cells in the medial temporal (mt) area in response to both isolated gratings and plaids, or two gratings superimposed on one another."
"the network is event-triggered. an event occurs when a sensor node detects an asset and becomes a source node. the source node starts sending event packets periodically to the sink node based on a multi-hop routing communication scheme. the network employs the k-nearest neighbor tracking approach [cit] to track the asset. nodes may follow a sleeping schedule, and are kept silent when no asset is detected. when a node detects the asset in its monitoring area, it remains active until the asset moves away. packets are encrypted and contain source node id and a timestamp to show where and when the asset is detected. only the sink node can decode the encrypted packet. it is assumed that at any one time, only one sensor node will become a source node to avoid data redundancy and inefficient energy consumption in the network. many sensor nodes can detect an asset but only the first node to detect the asset will become a source node and other nodes will overhear the communication from the source node and stop their communications. if multiple source nodes detect one asset at a time and each source node initiates communication with the sink, it will cause data redundancy and inefficient energy consumption. furthermore, when multiple similar packets are reported back to the sink one at a time, it becomes easier for an adversary to trace it back to the source node. when the asset moves to a new location, it triggers another sensor node to become the new source node."
"so far, our implementation of the transformation tool only consists of a runtime library of functions that provide code for initialization, data structures for group management and so on. hence, we have tested the transformation with three applications that have been hand-transformed according to the transformation rules, without further optimization: sieve of eratosthenes, quicksort, and matrix multiplication. we compare the performance of our transformed code both with compiled fork code executed in the sequential pram simulator pramsim on the cpu, and with sequential versions of the applications running on the cpu. as platform, we use an intel core2 cpu (e6600, 2.4 ghz) and an nvidia geforce gt 430 graphic card, with a gpu equipped with 2 sms and a fermi architecture."
"nate future images of these two people without seeing other images of them, though it has seen many images of other people and objects and their transformations and may have learned from them in an unsupervised way. can the system learn to perform the classification task correctly with just two (or a few) labeled examples? imagine trying to build such a classifier from the output of two cortical cells, as in figure 1 . here, the response of the two cells defines a 2d feature space to represent visual stimuli. in a more realistic setting, objects would be represented by the response patterns of thousands of such neurons. in the figure, we denote visual examples from the two people with + and -signs; panels (a) and (b) illustrate what the recognition problem would look like when these two neurons are sensitive vs. invariant to the precise position of the object within their receptive fields. can be found between the two classes. it has been shown that certain learning algorithms (such as svms with gaussian kernels) can solve any discrimination task with arbitrary difficulty (in the limit of an infinite number of training examples). that is, with certain classes of learning algorithms we are guaranteed to be able to find a separation for the problem at hand irrespective of the difficulty of the recognition task. however, learning to solve the problem may require a prohibitively large number of training examples. in separating two classes, the two representations in panels (a) and (b) are not equal; the one in (b) is far superior to the one in (a). with no prior assumption on the class of functions to be learned, the \"simplest\" classifier that can separate the data in (b) is much simpler than the \"simplest\" classifier that separates the data in (a). the number of wiggles of the separation line (related to the number of parameters to be learned) gives a hand-wavy estimate of the complexity of a classifier. the sample complexity of the problem derived from the invariant representation in (b) is much lower than that of the main point is not that a low-level representation provided from the retina would not support robust object recognition. [cit] s were based on simple retina-like representations and rather complex decision functions (such as radial basis function networks). the main problem of these systems is they required a prohibitively large number of training examples compared to humans."
"(2), respectively [cit] . the energy consumption for packet transmission was proportional to the square of the transmission distance d. eelec denotes transmitting circuit loss. the energy consumption model uses both, the free space (d 2 power loss) and the multi-path fading (d 4 power loss) channel models. if the transmission distance is less than the threshold d0, the power amplifier loss is based on free-space model. if the transmission distance is equal or greater than the threshold d0, the multi-path attenuation model is used. efs and eamp are the energy required by power amplification in the two models. simulation results in figures 7 and 8 demonstrate the performance of the schemes. performance metrics include the path diversity, safety period, attack success rate, delivery latency, and energy consumption, and are used for further analyses: figure 7a shows the path diversity of the analyzed schemes. privacy is directly related to path diversity. as the path diversity available for each packet is enhanced, higher source location privacy is guaranteed. path diversity can be categorized into length and path variations. in reference to the length variation, the source node can forward packets to the sink using paths with different lengths at each time it forwards a packet. longer paths increase the safety period and privacy, and vice versa. in reference to path variations, each individual packet follows a different route to the sink. higher path variation improves the privacy by making it more difficult for an adversary to guess which route the next packet will use. the figure shows the path diversity in terms of the path length for a single source node which generates and forwards 100 packets to the sink. it shows that the proposed scheme has a much higher path diversity than the shortest path routing, phantom single-path routing and directional random routing schemes. the tree-based diversionary routing scheme has a higher path diversity than the proposed scheme because it integrates many routing techniques. it creates backbone routes which are directed to the network border with many diversionary routes as the branches. packet routes are directly routed to the network edge from the sink node. at the end of each diversionary route, the scheme generates fake source nodes which emits fake packets periodically to lead the adversary away from the real packet route. the scheme also uses phantom nodes located far away from the source node. combination of all these routing techniques provides a much higher path diversity. the average path diversity for the proposed scheme is 148 hops, while the shortest path routing has 42 hops, the phantom single-path routing has 92 hops, the directional random routing has 108 hops, and the tree-based diversionary routing has 211 hops. the high path diversity in the proposed scheme is achieved by the longer and more random routing paths, which go through either mediate node or diversion node regions."
"understanding the processing of information in our cortex is a significant part of understanding how the brain works and understanding intelligence itself. for example, vision is one of our most developed senses. primates easily categorize images or parts of images, as in, say, an office scene or a face within a scene, identifying specific objects. our visual capabilities are exceptional, and, despite decades of engineering, no computer algorithm is yet able to match the performance of the primate visual system."
"models provide a much-needed framework for summarizing and integrating existing data and planning, coordinating, and interpreting new experiments. they can be powerful tools in basic research, integrating knowledge across multiple levels of analysis, from molecular to synaptic, cellular, systems, and complex visual behavior. however, models, as we discuss later, are limited in explanatory power but should, ideally, lead to a deeper and more general theory. here, we discuss the role of the visual cortex and review key computational principles underlying the processing of information dur-contributedarticles ing visual recognition, then explore a computational neuroscience model (representative of a class of older models) that implements these principles, including some of the evidence in its favor. when tested with natural images, the model performs robust object recognition on par with computer-vision systems and human performance for a specific class of quick visual-recognition tasks. the initial success of this research represents a case in point for arguing that over the next decade progress in computer vision and artificial intelligence promises to benefit directly from progress in neuroscience."
"privacy performance of a routing scheme increases as the length of routing paths (hops) between source node and sink node increase [cit] . when routing paths are longer (more hops), it takes longer for an adversary to back trace and locate the source node. as number of nodes in the network increase, length of routing paths (hops) between source and sink nodes may increase. with the use of random rn for each packet in the proposed scheme, the longer routes become random and more unpredictable to the adversary. figure 8 shows performance of the routing schemes as number of sensor nodes in the network increase. the path length of all schemes increases as the number of nodes increase. since privacy level is proportional to the routing path length [cit], it can be perceived that source location privacy of the routing schemes increases with increase in node density. similarly, increased number of neighboring nodes and nodes in regions rm and rd of the proposed scheme will increase the path diversity between successive packets. for example, assume a source node s3 has n neighboring nodes, probability of selecting a particular neighboring node is 1/ . when the random number rn is applied during selection of neighboring node, the selection becomes random. if region rd has u diversion nodes, the probability of s3 selecting a particular diversion node in region rd is 1/ . then, the total possible number of routing paths between s3 and the sink node is . this shows that, number of routing paths is proportional to number of neighboring nodes and nodes in regions rd and rm. the improved path diversity between successive packets will make the routes more confusing to the adversary and improve privacy level."
"cuda. nvidia provides the cuda c programming language for its gpus, where programs are executed in masterslave style. a host program is started on the cpu, then a so-called kernel is spawned on the gpu, which provides a return value to the host program after completion. this is repeated until the host program terminates. a kernel is executed on the gpu by a so-called grid, i.e. by one or several blocks of threads. the number of threads per block (at most 512) and the number of blocks can be chosen by the programmer."
"in reference to the length variation, the source node can forward packets to the sink using paths with different lengths at each time it forwards a packet. longer paths increase the safety period and privacy, and vice versa. in reference to path variations, each individual packet follows a different route to the sink. higher path variation improves the privacy by making it more difficult for an adversary to guess which route the next packet will use. the figure shows the path diversity in terms of the path length for a single source node which generates and forwards 100 packets to the sink. it shows that the proposed scheme has a much higher path diversity than the shortest path routing, phantom single-path routing and directional random routing schemes. the tree-based diversionary routing scheme has a higher path diversity than the proposed scheme because it integrates many routing techniques. it creates backbone routes which are directed to the network border with many diversionary routes as the branches. packet routes are directly routed to the network edge from the sink node. at the end of each diversionary route, the scheme generates fake source nodes which emits fake packets periodically to lead the adversary away from the real packet route. the scheme also uses phantom nodes located far away from the source node. combination of all these routing techniques provides a much higher path diversity. the average path diversity for the proposed scheme is 148 hops, while the shortest path routing has 42 hops, the phantom single-path routing has 92 hops, the directional random routing has 108 hops, and the tree-based diversionary routing has 211 hops. the high path diversity in the proposed scheme is achieved by the longer and more random routing paths, which go through either mediate node or diversion node regions."
"metaphor for the central mystery of how the brain produces intelligent behavior and intelligence itself. they also provide experimental tools for information processing, effectively testing theories of the brain, particularly those involving aspects of intelligence (such as sensory perception). the contribution of computer science to neuroscience happens at multiple levels and is well recognized. perhaps less obvious is that neuroscience is beginning to contribute powerful new ideas and approaches to artificial intelligence and computer science as well. modern computational neuroscience models are no longer toy models but quantitatively detailed while beginning to compete with state-of-the-art computervision systems. here, we explore how computational neuroscience could become a major source of new ideas and approaches in artificial intelligence."
"the adversary has some technical advantages over the network sensor nodes. it is well equipped and has sufficient resources, such as adequate computation capabilities, memory and energy resources. it is equipped with antenna and spectrum analyzers, so it can observe the wireless communication within a certain detection range. the adversary is mobile, initially residing in the vicinity of the sink node listening for arriving packets. upon detection of a packet, the adversary can measure the angle of arrival of the signal and the received signal strength to identify the immediate sender node and to perform a back tracing attack by moving to the immediate sender node location without any delay. once at the immediate sender node, the adversary keeps on listening on the communications between the node and its neighboring nodes and continues to back trace to the source node. the adversary never misses a packet when it is within the transmission range of the receiving node."
"a key challenge posed by the visual cortex is how well it deals with the poverty-of-stimulus problem, or simple lack of visual information. primates are able to learn to recognize an object in quite different images from far fewer labeled examples than are predicted by our present learning theory and algorithms. for instance, discriminative algorithms (such as support vector machines, or svms) can learn a complex object-recognition task from a few hundred labeled images. this number is small compared to the apparent dimensionality of the problem (millions of pixels), but a child, even a monkey, is apparently able to learn the same task from a handful of examples. as an example of the prototypical problem in visual recognition, imagine a (naïve) machine is shown an image of a given person and an image of another person. the system's task is to discrimia within recognition, one distinguishes between identification and categorization. from a computational point of view, both involve classification and represent two points on a spectrum of generalization levels."
"in this paper, a routing scheme with strategically positioned diversion and mediate nodes is proposed to provide a high level of source location privacy. the routing of packets is location-based. nodes located in a near-sink region route their packets through randomly selected diversion nodes, while nodes located away from the sink region route their packets through randomly selected mediate nodes. diversion and mediate nodes are strategically positioned to guarantee that successive packets use different routes to arrive at the sink. if an adversary decides to trace back the routes to the source node, it finds multiple possible routes through randomly selected diversion or mediate nodes, thus making it difficult to guess the route for the next packet. the scheme operates in two phases. in phase 1, packets from a source node are randomly forwarded to a random diversion or mediate node. in phase 2, diversion or mediate nodes randomly forward packets to the sink node using random walk routing."
"showed the patterns of neural activity elicited by certain ecologically important classes of objects (such as faces and places in monozygotic twins) are significantly more similar than in dizygotic twins. these results suggest that genes may play a significant role in the way the visual cortex is wired to process certain object classes. meanwhile, several electrophysiological studies have demonstrated learning and plasticity in the adult monkey; see, for instance, li and dicarlo. 15 learning is likely to be both faster and easier to elicit in higher visually responsive areas (such as pfc and it 15 ) than in lower areas."
"a wireless sensor network (wsn) is a network which consists of spatially distributed autonomous sensors with the aim of monitoring various physical and environmental conditions including asset monitoring and tracking [cit] . a recent implementation of an asset monitoring network is the wildlife crime technology report which is based on a wsn that is used to monitor a large area where animals roam [cit] . in monitoring applications, nodes operate by monitoring their surroundings to detect the presence of an asset. when the asset is detected, the node which detects the asset becomes a source node and transmits a message to the sink to report the presence of the asset in its surroundings [cit] . often, the distance between the source node and sink is longer than the transmission range of sensor nodes, making multi-hop communication a viable mode of transmission [cit] ."
"considering a cautious adversary who waits for a specified amount of time at a node and rolls back to a previous node when the timer expires, the success rate of the adversary is considerably reduced in the proposed scheme as compared to the shortest path, phantom single-path, and directional random routing schemes. owing to the highly random routing paths in the proposed scheme, the adversary will wait for a much longer time at a node before it receives a packet at the same node again. for example, in figure 5, if the adversary receives k_f1 forwarded by node mp, it will back trace to node mp. however, the next packet k_f2 is routed through the mediate node mq, which is very far away from mp. if the waiting time exceeds the waiting timer, the adversary will find itself rolling back to the previous node and make insignificant progress towards the source node. for a successful back tracing attack to the source node, the adversary needs to intercept many packets. if the packets use very different routes in the network, it will take longer for the adversary to receive enough packets to intercept and successfully locate the asset. the adversary might find itself using a longer time than the safety period of the scheme and the asset will possibly move to a new location before the adversary locates the source node."
"upon reception of the packet, diversion node determines a group of neighboring nodes with shorter hop distance to the sink node than the diversion node itself. one neighboring node from the group is randomly selected as the next-hop node. the diversion node randomly forwards the packet to the next-hop node and eventually to the destination sink node."
region h is the hotspot region near the sink node where nodes have a bigger load of packets to forward than nodes away from the sink. 2) region nh is the non-hotspot region and is located far away from the sink node where nodes have a smaller load of packets to forward. 3) diversion nodes are a set of random nodes located in region nh. they are used to forward packets for source nodes located in region h. 4) mediate nodes are a set of random nodes located in region nh. they are used to forward packets for source nodes located in region nh.
"supervised learning in higher areas. after this initial developmental stage, learning a new object category requires training only of task-specific circuits at the top of the ventralstream hierarchy, thus providing a position and scale-invariant representation to task-specific circuits beyond it to learn to generalize over transformations other than image-plane transformations (such as 3d rotation) that must be learned anew for each object or category. for instance, poseinvariant face categorization circuits may be built, possibly in pfc, by combining several units tuned to different face examples, including different people, views, and lighting conditions (possibly in it)."
"it is assumed a network operator will perform pre-deployment phase to determine the network size and division of the network according to figure 3 and as explained in section 4 above. network initialization process follows after pre-deployment phase is complete. it is assumed that each node is informed about its own location, location of the neighboring nodes and of the sink node during network initialization process. the first step in network initialization process is to load each sensor"
"it is assumed a network operator will perform pre-deployment phase to determine the network size and division of the network according to figure 3 and as explained in section 4 above. network initialization process follows after pre-deployment phase is complete. it is assumed that each node is informed about its own location, location of the neighboring nodes and of the sink node during network initialization process. the first step in network initialization process is to load each sensor node with a unique identifier (id). thereafter, the sink node obtains its location information using a global positioning system (gps). the sink node then broadcasts a beacon packet to all sensor nodes in the network and sets its hop counter to zero. each node receives the beacon packet, stores the hop counter value with a sender node id, increments the hop counter by one, and rebroadcasts the beacon packet to its neighboring nodes. the hop counter number indicates how many hops away the sensor node is from the sink node. this gives each node information and knowledge about its neighboring nodes and its location with respect to the sink node. when a sensor node receives multiple packets, it only stores the minimum hop count in its buffer and deletes other hop counter information. after that, each sensor node calculates and records a set of its neighboring nodes. each node informs its hop-distance to the sink. at the same time, the network operator will assign the role of the sensor nodes according to their location in the network. a sensor node is labeled as a node in the hotspot region if it is located within the radius r h and in the non-hotspot region if it is located in radius greater than r h . parameter r h is predefined and used to determine regions h and nh. similarly, the diversion nodes in the ring r d and the mediate nodes in the ring r m are determined and regions p and q of the diversion and mediate nodes are determined."
"the main contributions of this paper can be summarized as follows: (1) to address the limitations of shortest path routing, phantom single-path routing and directional random routing schemes by proposing a new routing scheme that uses strategically positioned mediate and diversion nodes to significantly improve path diversity of the routing paths between the source and sink nodes; (2) to conduct a series of experiments to evaluate the performance of the proposed routing scheme; (3) to demonstrate that the proposed scheme provides stronger source location privacy than shortest path routing, phantom single-path routing, and directional random routing schemes."
"upon detection of the asset, the source node generates a bias random number r n ranging from 0 to 1 and compares it to a predefined threshold t. if r n is less than the threshold t, the source node randomly selects one of its neighboring nodes in the direction of the strategically positioned diversion node region p. one diversion node is randomly selected in the diversion node region p. otherwise, the source node randomly selects one of its neighboring nodes in the direction of the strategically positioned diversion node region q. one diversion node is randomly selected in the diversion node region q. the source node determines a group of neighboring nodes with shorter hop distance to the randomly selected diversion node than the source node itself. one neighboring node from the group is randomly selected as the next-hop node. the source node randomly forwards the packet to the next-hop node and eventually to the randomly selected diversion node in regions p or q. phase 1 routing ends when the packet reaches the diversion node."
"as an example of a simple routine consider a classifier that receives the activity of a few hundred it-like units, tuned to examples of the target object and distractors. while learning in the model from the layers below is stimulusdriven, the pfc-like classification units are trained in a supervised way following a perceptron-like learning rule."
"fork. fork is a c-based high-level programming language that was created to allow for experimentation with pram algorithms, e.g. to study scalability or to get an impression about the constant factors in their time complexity."
"to ensure the proposed scheme provides higher privacy compared to the shortest path, phantom single-path, and directional random routing schemes, the selection of the mediate node or diversion node is random and location-based. source nodes in region h randomly forward packets to sink node through diversion node regions p or q. source nodes in region nh randomly forward packets to sink node through mediate node regions p or q. the use of either the mediate or diversion node regions p or q significantly increase the privacy level of the proposed scheme as compared to the shortest path, phantom single-path, and directional random routing schemes. as the regions p and q are strategically positioned on opposite sides of the sink node, successive packets will use highly random routing paths and may arrive at the sink from completely different directions as shown in figure 5 . these paths will not be related as they do in the shortest path, phantom single-path, or directional random routing schemes."
"to compare the shortest path routing and the proposed scheme, if a node is located four hops away from the sink, the shortest path routing scheme will find the shortest route to the sink, which may be not more than four hops to the sink. this shortest route allows the adversary to successfully back trace to the location of the source node within a short time. additionally, using the shortest routes increases the chances that the packets from the same source node use the same routing paths where the adversary is likely to receive successive packets and easily back trace to the source node. on the other hand, if the proposed scheme is used, when a source node is four hops away from the sink, the packets will be routed first to a random diversion node before they are randomly rerouted to the sink. this route may increase the route path from four hops to more than eight hops depending on the location of the diversion node. this difference in routing paths may also be observed in figure 2 where the source node a uses the shortest path routing e and the proposed scheme routing path g to route packets to the sink node. overall, the proposed scheme can improve the complexity for the back tracing of the source node by the adversary, and provides effective source node location privacy protection. a possible limitation of the proposed routing scheme is when a source node chooses a diversion node near its own location. this may cause a short routing path for that particular packet. however, this is not a serious limitation as there is a very high probability that the generated random number r n for the next packet will be different and the next packet will use a diversion node selected from another region causing a completely different routing path. successive packets from the same source node are guaranteed to use completely different routes depending on the value of r n, and so adversary tracing back attacks become more difficult."
"the runtimes for eratosthenes and quicksort are depicted in fig. 6 . in the left part we can see that the transformed sieve program on the gpu is faster than both the sequential program and the simulator on the cpu. the speedup with a growing number of threads is notable, although the runtime only shrinks to about 70% when doubling the threads. the speedup compared to the sequential simulator is almost 100, so that runtimes of hours are now reduced to below a minute, which greatly increases usability in classes."
"several lines of evidence (from both human psychophysics and monkey electrophysiology studies) suggest the primate visual system exhibits at least some invariance to position and scale. while the precise amount of invariance is still under debate, there is general agreement as to the fact that there is at least some generalization to position and scale."
"we compared the performance of the model against the performance of human observers in a rapid animal vs. non-animal recognition task 28 for which recognition is quick and cortical back-projections may be less relevant. results indicate the model predicts human performance quite well during such a task, suggesting the model may indeed provide a satisfactory description of the feedforward path. in particular, for this experiment, we broke down the performance of the model and human observers into four image categories with varying amounts of clutter. interestingly, the performance of both the model and the human observers was most accurate (~90% correct for both human participants and the model) on images for which the amount of information is maximal and clutter minimal and decreases monotically as the clutter in the image increases. this decrease in performance with increasing clutter likely reflects a key limitation of this type of feedforward architecture. this result is in agreement with the reduced selectivity of neurons in v4 and it when presented with multiple stimuli within their receptive fields for which the model provides a good quantitative fit 29 with neurophysiology data (see the sidebar)."
"the tree-based diversionary routing scheme [cit] operates in two phases. it establishes the direct backbone route path to the network edge based on phantom routes for establishing homogeneous trees. subsequently, it establishes many redundant diversionary routes in non-hotspot regions with abundant energy. it works by letting each node create its own root path that extends to the network border. the root path helps divert the attention of adversary from the real packet route. it establishes a phantom node away from the source node and then establishes a tree routing path towards the sink with strategically created diversionary routes as its branches. fake source nodes are placed at the end of the diversionary routes to confuse the adversary. [cit], 18, 2291 5 of 18 uses considerable energy for each node to create their root paths that extend to the end of the network border. although the path diverts the adversary from real paths and increases the privacy level, the energy consumption is too high."
"the visual system may be using a similar strategy to recognize objects, with the goal of reducing the sample complexity of the classification problem. in this view, the visual cortex transforms the raw image into a position-and scale-tolerant representation through a hierarchy of processing stages, whereby each layer gradually increases the tolerance to position and scale of the image representation. after several layers of such processing stages, the resulting image representation can be used much more efficiently for task-dependent learning and classic the idea of sample complexity is related to the point made by dicarlo and cox 4 about the main goal of processing information from the retina to higher visual areas to be \"untangling object representations,\" so a simple linear classifier can discriminate between any two classes of objects."
"stream of the visual cortex, neurons become selective for stimuli that are increasingly complex-from simple oriented bars and edges in early visual area v1 to moderately complex features in intermediate areas (such as a combination of orientations) to complex objects and faces in higher visual areas (such as it). along with this increase in complexity of the preferred stimulus, the invariance properties of neurons seem to also increase. neurons become more and more tolerant with respect to the exact position and scale of the stimulus within their receptive fields. as a result, the receptive field size of neurons increases from about one degree or less in v1 to several degrees in it."
"conclusion demonstrating that a model designed to mimic known anatomy and physiolhow does the model 29 perform realworld recognition tasks? and how does it compare to state-of-the-art artificial-intelligence systems? given the specific biological constraints the theory must satisfy (such as using only biophysically plausible operations, receptive field sizes, and a range of invariances), it was not clear how well the model implementation would perform compared to systems heuristically engineered for these complex tasks."
"privacy performance of a routing scheme increases as the length of routing paths (hops) between source node and sink node increase [cit] . when routing paths are longer (more hops), it takes longer for an adversary to back trace and locate the source node. as number of nodes in the network increase, length of routing paths (hops) between source and sink nodes may increase. with the use of random r n for each packet in the proposed scheme, the longer routes become random and more unpredictable to the adversary. figure 8 shows performance of the routing schemes as number of sensor nodes in the network increase. the path length of all schemes increases as the number of nodes increase. since privacy level is proportional to the routing path length [cit], it can be perceived that source location privacy of the routing schemes increases with increase in node density. similarly, increased number of neighboring nodes and nodes in regions r m and r d of the proposed scheme will increase the path diversity between successive packets. for example, assume a source node s 3 has n neighboring nodes, probability of selecting a particular neighboring node is 1/n. when the random number r n is applied during selection of neighboring node, the selection becomes random. if region r d has u diversion nodes, the probability of s 3 selecting a particular diversion node in region r d is 1/u. then, the total possible number of routing paths between s 3 and the sink node is nu. this shows that, number of routing paths is proportional to number of neighboring nodes and nodes in regions r d and r m . the improved path diversity between successive packets will make the routes more confusing to the adversary and improve privacy level."
"the past 60 years of experimental work in visual neuroscience has generated a large and rapidly increasing amount of data. today's quantitative models bridge several levels of understanding, from biophysics to physiology to behavior. some of these models compete with state-of-the-art computer-vision systems and are close to human-level performance for specific visual tasks."
"the remainder of this article is structured as follows. in section 2, we briefly recall relevant facts about prams, gpus, and their programming. in section 3, we describe major issues in the transformation of fork programs to cuda. in section 4, we provide preliminary experimental results, while section 5 provides a conclusion with an outlook on future work."
"to demonstrate the sensitivity to secret key of the dfbc, we decrypt the encrypted images twice. in the first run, we use the exact encryption keys ( . we conduct the experiments on the images of lena, cameraman, monkey, and parking, and the results are shown in figure 7 . as we can see, even if we only change the key extremely little such as 10 −15, the decrypted images are completely different from the ones decrypted with the correct keys, validating that the proposed dfbc is highly sensitive to secret key."
"traditional image filtering cannot be reversed; that is to say, it usually cannot recover the original image with the filtered image and the filters. to overcome this shortcoming, very recently, hua and zhou have designed a special filter with which the original image can be recovered from the filtered image. and they have proposed a novel approach called ic-bsif for image encryption, where image filtering is applied to image encryption for the first time [cit] . the experimental results demonstrated the effectiveness of the proposed ic-bsif."
"the extreme sensitivity of an image encryption algorithm implies that even one bit changed in the keys will lead to a completely different encrypted image. in other words, if the security key changes a little, the decrypted image will be completely different from the input image."
we present our approach informally in fig. 1 . it shows the structure of the parallel numerical scheme and the areas of the grid which communicate with each other. areas bordered by black lines represent the grid areas allocated to separated processors. red stripes indicate how calculations are performed. blue stripes pose organization of data transmission between the neighboring processes.
"the histograms of the plain images and their corresponding encrypted images are shown in figure 8 . the histograms of the plain images are shaped like some mountains or valleys while all the histograms of encrypted images are very close to a uniform distribution. these histograms of encrypted images indicate that the proposed dfbc have the ability to resist histogram attacks. the fifth image histogram especially has a highly unusual histogram, where most of the pixels have the highest gray scale levels so that the rectangles of low gray scales are very short. and our method can encrypt the image histogram to be an even and flat histogram like the other photographic images. it confirms that our method works very well for man-made graphical images like histogram."
"we randomly change one pixel and then add 1 on gray value in the plain images to compute one value of npcr and uaci. we repeat the process 10 times and the average scores of npcr and uaci of the corresponding image encryption schemes are recorded in tables 4 and 5, respectively, as the final npcr and uaci."
to do diffusion in image encryption. they also introduced proposition 1 and its proof to identify that image filtering operation satisfying some conditions can be reversible [cit] .
"appendix a.2 discusses the synthesis of convex relaxations of bilinear equalities, which allows us to replace each bilinear equality by a set of linear inequalities. using them, a convex relaxation of the above optimization problem can be stated as"
"given exactly three views, the modulus constraints of (7) correspond to a system of three quartic polynomials in three variables, for which the 64 roots may be found, typically using continuation methods. also for the three-view case, an additional cubic equation available from the modulus constraints [cit] can be used to eliminate several spurious solutions, reducing the number of possible solutions to 21."
"the rest of this paper is structured as follows. a brief description of the 7d hyperchaotic system and filtering is given in section 2. in section 3, the novel image encryption algorithm, dfbc, is proposed in detail. experimental results are reported in section 4. finally, we conclude the paper in section 5."
"technical problems of wireless charging are mainly focused on three areas: (1) coil design, (2) compensation topologies, (3) power electronic converters and control methods [cit] . efficiency is one of the most important problem in wireless charging field, because the efficiency of wireless charging is lower than that of conductive charging, especially when the coils are not well aligned. in addition, the electric vehicles are charged in a constant current-constant voltage (cc-cv) operation, as shown in fig. 2 . in the charging process, the equivalent load resistance varies in a large range, especially in cv charging. it makes the wireless charging"
"the affine upgrade, which is arguably the more difficult step in stratified autocalibration, is succinctly computable by estimating the position of the plane at infinity in a projective reconstruction, for instance, by solving the modulus constraints [cit] . previous approaches to minimizing the modulus constraints for several views rely on local, gradient-based methods with random reinitializations. these methods are not guaranteed to perform well for such non-convex problems. moreover, in our experience, a highly accurate estimate of the plane at infinity is imperative to obtain a usable metric reconstruction."
"on the other hand, in this paper, we use chirality constraints derived from the scene to compute a theoretically correct initial search space for the plane at infinity, within which we are guaranteed to find the global minimum [cit] . in practice, for a moderate number of cameras, the initial region determined by the chirality constraints are tight enough to allow rapid convergence of the search algorithm. our initial region for the metric upgrade is intuitively specifiable as conditions on the intrinsic parameters of the camera and can be wide enough to include any practical case."
"the statistical analysis is another widely used and effective way to analyze a cryptosystem. we adopt some typical statistical analysis, such as histogram analysis, information entropy, and correlation analysis, to evaluate the performance of the proposed dfbc. a cryptosystem with good performance of statistical analysis can resist all kinds of statistical attacks. the experimental analysis demonstrates that our method dfbc also can handle images especially like the graphical histogram very well."
"thus, noting that the infinite homography is conjugate to a rotation matrix and must have eigenvalues of equal moduli, one can relate the roots of its characteristic polynomial"
"note that the cost function in (12) is a polynomial and some recent work in computer vision [cit] exploits convex linear matrix inequality (lmi) relaxations to achieve global optimality in polynomial programs. however, this is a degree 8 polynomial in three variables, which is far beyond what presentday solvers can handle [cit] . we instead consider the equivalent formulation:"
total complexity of time-integration step for each processor is o n rm log m p operations and requires o(pm ) data elements for o(p) data exchange operations.
"to have a further correlation analysis, we randomly select 4000 pairs of neighboring pixels in horizontal direction from the input images and the encrypted images by the dfbc to show their neighboring pixel distribution maps in figure 9 ."
for s(f ) we use skeleton decomposition of coagulation kernel and fast algorithms of linear algebra with overall algorithmic complexity of solution scheme to be o(m r log m ) at each grid node along x axis [cit] . whereas without utilization of these ideas the complexity becomes o(m 2 ). we discuss this method with more details in section 2.2. we get significant acceleration for evaluation of smoluchowski integrals if the rank r of the coagulation kernel is a modest number. for approximation of the advection part a(f ) we exploit well-known tvd scheme [cit] and pml layers [cit] allowing us to keep monotonicity of numerical method. detailed relations for advection part are presented in section 2.3.
"image security is one of the most important branches of information security. to enhance image security, in this paper, we have proposed a novel image encryption algorithm that combines a 7d hyperchaotic system with 5 positive les, dynamic filtering operation, and bit cuboid operations, namely, dfbc, for image encryption. extensive experiments have shown that the proposed dfbc can outperform some state-of-the-art image encryption schemes in terms of several evaluation criteria, indicating that the dfbc is effective for image encryption. in the future, we will study construction of more complex key for the dfbc. furthermore, since the dynamic filtering can be performed in parallel, we will also implement the algorithm on the framework of cuda and use gpu to accelerate it."
"he is currently the director of the state key laboratory of automotive safety and energy and the cheung kong scholars chair professor with the department of automotive engineering, tsinghua university, beijing, china. his main research interests include powertrain systems of energy-saving and new energy vehicles. volume 7, 2019"
"step 5. transform the pixel plane to a bit cuboid as described in section 3.4.1. the transformed bit cuboid has as uniform width, height, and depth as possible."
"as an illustration of higher-level concepts, we show construction of convex under-estimators for the non-convex objective in (13). the actual objective we minimize incorporates chirality bounds and is derived in sect. 6.4. let us suppose it is possible to derive a convex underestimator conv(γ i 1/3 α i ) and concave over-estimator conc(γ i 1/3 α i ) for γ 1/3 i α i . then the following convex optimization problem underestimates the solution to (13):"
information entropy (ie) is the average rate at which information is produced by a stochastic source of data. here it is used to reflect the complexity or orderliness of the encryption system. the intensity of an 8-bit grayscale image has 2
"from the analysis, we can see that the proposed dfbc is robust to a certain extent. however, as pointed out by hua and zhou, resisting noise, rotation, and cropping attacks are current limitations for image encryption by filtering [cit] . the proposed dfbc will be significantly improved when such limitations are resolved."
"according to the result of coil current, the input power p s, the output power p b, and the active power p a can be calculated. for p s and p a, energy output is defined positive, whereas for p b, energy absorption is defined positive."
-highly accurate recovery of the plane at infinity in a projective reconstruction by global minimization of the modulus constraints. -highly accurate estimation of the diac by globally solving the infinite homography relation. -a general exposition on novel convexification methods for global optimization of non-convex programs.
"a significant drawback of local methods is that they critically depend on the quality of a heuristic initialization. to be considered truly optimal, an algorithm must converge to the global optimum regardless of the choice of initialization. branch and bound methods require a demarcated region of the search space as initialization. arbitrarily choosing a small initial region might compromise optimality, since the true solution might lie outside that chosen region. on the other hand, choosing a very large region might lead to a ponderous convergence rate for the branch and bound algorithm."
"in this section we revisit description of the fast numerical algorithm for advectioncoagulation equation from work [cit] . first of all, we use an explicit euler time-integration scheme with time step ∆t for solution of the cauchy problem:"
"branch and bound algorithms are non-heuristic methods for global optimization of non-convex problems [cit] . they maintain a provable upper and/or lower bound on the (globally) optimal objective value and terminate with a certificate proving that the solution issuboptimal (that is, within of the global optimum), for arbitrarily small . for greater details on the branch and bound framework for global optimization, we refer the reader to standard texts such a [cit] ."
"the procedure of the dfbc can be categorized into five stages: hyperchaotic sequence generation (step 1), pixel-level diffusion (step 2-4), pixel plane to bit cuboid transformation (step 5), bit-level permutation (steps 6-10), and bit sequence to pixel-plane transformation (step 11). the dfbc diffuses images with dynamic filtering, global pixel diffusion, and xor operation on pixel-level data. and it permutes the images on bit-level data. it is worth pointing out that since the permutation on bits can change the values of pixels, the bit cuboid operations in the dfbc are capable of diffusing the images; that is to say, the bit cuboid operations have the effects of both permutation and diffusion."
"the method of autocalibration presented in this paper is a stratified one, whose first step upgrades the projective reconstruction to an affine one, while the next step performs the upgrade to a metric reconstruction. an affine reconstruction restores certain aspects of the scene, such as parallelism between 3d lines, while the metric reconstruction restores characteristics such as exact angles and length ratios."
"thus, our method, at the expense of some computational effort, poses the optimization problem in terms of an interpretable quantity and finds estimates which are at least as good as, or better than, those obtained by using the standard normalization."
"here, p i are estimated coordinates of the plane at infinity, f 1, f 2 represents the two focal lengths, (u, v) stands for the principal point and s for the skew. p 0 i, f 0 1, f 0 2, u 0, v 0 and s 0 are the corresponding ground truth quantities."
"an important consideration for noisy situations is the sensitivity of the chirality bounds to outliers. similar to (nistér 2004), for noisy images expected in a real world scenario, chirality bounds are computed using only the camera centers. the reason is that usually there are far more points fig. 7 comparison of the globally optimal metric upgrade algorithm with a traditional linear method for estimating the diac. the accuracy of the reconstruction is deduced by the extent to which orthogonality is preserved in the metric reconstruction. the solid red curve plots the deviation from orthogonality when a local optimization method is used for estimating the plane at infinity and a linear method is used for estimating the diac. the dotted black curve uses a linear method for diac estimation, but the optimal plane at infinity is used for affine upgrade. the dashed blue curve uses optimal algorithms for both the affine and metric upgrade steps. the reported quantities are averages of 50 trials than cameras and the camera centers are likely to be estimated more robustly than 3d points."
"the ies of input images and encrypted images by different encryption methods are listed in table 2 . the third column under df (dynamic filtering) is the entropies of diffusion by dynamic filtering. the fourth column under dfbc is the entropies of final encrypted images. all the entropies of diffused images except barbara are improved after our bit cuboid operations, which indicates that this random permutation is very effective. it can be seen that the ies of input images are far below 8, while those of encrypted images are very close to the ideal value. the ies of dfbc are within [7.9972, 7 .9993], demonstrating that the dfbc is secure enough to resist entropy attack. our dfbc achieves 5 out of 8 optimal values while hc-dna and ic-bsif achieve only 2 and 1 out of 8 optimal values, respectively. although the ies by df are not the highest ones, they are very close"
"basically, the security keys of the proposed dfbc are composed of 7 initial values, i.e., ( of cryptology, the size of the key space larger than 2 100 can provide a high-level security [cit] . therefore, the key space of the dfbc is large enough to resist all kinds of brute-force attacks from current computers. in addition, the start position of chaos sequence to form the random filter mask can also be used as a key to further enhance the key space."
"in this section, we will outline our general strategy for the construction of a convex underestimator for an arbitrary non-convex function. this strategy will be employed to underestimate the objective functions that arise in both the affine and metric upgrade stages of autocalibration."
"image security is one of the most important types of information security. in recent years, image encryption has become a hot topic in the field of image security. however, since images usually show some intrinsic properties such as bulky data capacity, high redundancy, and strong correlation, traditional encryption algorithms such as data encryption standard (des), advanced encryption standard (aes), and international data encryption algorithm (idea) for common data are not able to be applied to images directly to achieve good results [cit] . to cope with this issue, various researchers have been devoted to proposing image encryption algorithms to enhance image security in recent years."
"sometimes, a consideration of practicality of the convex relaxation influences the choice of the algebraic form of an objective function. indeed, the most straightforward way to minimize the modulus constraints would be to use the simpler formulation of (12) and construct a multi-level relaxation for the quartic polynomials by successively using the bilinear relaxation of sect. a.2. however, in our experience, such multistep relaxations are very loose in practice, so the branch and bound algorithm will not converge in a reasonable amount of time. thus, the least squares version of the modulus constraints we globally minimize corresponds to the reformulated version of (13)."
"our dynamic diffusion mask of filtering comes from the hyperchaotic sequence. and the indicator of direction of filtering mask is determined by one number in the hyperchaotic sequence too. if the number is even, a horizontal mask is applied to the image. otherwise, an odd number means a vertical mask."
"finally, once the plane at infinity has been estimated, the globally optimal metric upgrade of sect. 7 is compared against the traditional linear approach. the drawbacks of the traditional linear approach discussed previously, namely, overlooking the positive semidefiniteness requirement of the diac and choosing a suboptimal scale factor, are illustrated by these experiments."
"complexity 13 npcr concentrates on the absolute number of pixels which change values in differential attack, while the uaci focuses on the average difference between the corresponding encrypted images. for an image with 256 grayscale levels, the expectations of npcr and uaci of an ideally encrypted image are 99.6094% and 33.4635%, respectively [cit] . generally speaking, the more npcr gets close to 100% and the bigger uaci is, the more effective it is for the encryption scheme to resist differential attacks."
"for the success of a branch and bound scheme, it is of utmost importance that the convex relaxations be as tight as possible. the second order cone programming based convex relaxation that we develop for solving the affine upgrade step and the semidefinite programming based convex relaxation for the metric upgrade step satisfy this requirement, while also being very fast to compute in practice."
"as discussed above, the coil efficiency is maximized when current balance condition (8) is achieved. if the current on both sides deviates from the balance condition, the coil efficiency will decrease, as shown in fig. 5 . overall, κq is a key parameter for the coil efficiency but it can only be improved at the design stage. the phase difference of the coil currents has a quite slight effect on the efficiency in a large scale. the current balance condition is important to the coil efficiency and it can be adjusted by circuit control. however, as described in part i, the load condition varies dramatically in constant current -constant voltage (cc-cv) charging operation, which makes the current balance in the whole cc-cv charging difficult."
"in this paper, a wireless charging system with active power source in receiver is proposed. the active power source is able to adjust its voltage and phase angle, and the efficiency can be improved in part of the operating range. a novel calculation method is used to obtain the efficiency of series-series compensation wireless charging circuit with active power source, and the results are verified by simulation. this paper explains that the efficiency improvement comes from two aspects: adjustment of power distribution and improvement of current balance. the efficiency improvement is demonstrated by an experimental system. the efficiency increases from 84.9% to 95.7% in a low coupling and slightly detuning condition."
"which is typically solved linearly by ignoring the positive semidefiniteness requirement on the diac. for the cases where the linear solution does not yield a positive semidefinite diac, the closest positive semidefinite matrix is estimated as a post-processing step by dropping the negative eigenvalues. it is well-documented that this may lead to a spurious calibration in practice [cit] ."
"transformation of integrals written as sum of r lower-triangular convolutions, hence, with the use of fft algorithm, total algorithmic complexity of the numerical integration using the relations above yields to o(m r log m ) operations at each from n points of spatial grid. hence, total cost of evaluation of smoluchowski integrals is o(n m r log m )"
"six publicly accessed images and two images we collected, monkey and parking, with different sizes are used to test the proposed dfbc, as listed in table 1 ."
for which the global minimum is estimated using the method outlined in this section. please refer to sect. 9 for further discussions on this reformulation.
"in the experiment, a resistor, rather than a battery, is used as the load. moreover, the system does not work exactly at the resonant point, and the coupling of coils is low because of a deliberate misalignment. that means the efficiency equations (20) and (22) are not applicable to this experiment. however, the efficiency improvement can still be observed and the two mechanisms explained above are still applied to the experiment. fig. 19 shows waveforms from the experiment, and the result of the experiment is shown in fig. 20 ."
"according to the theory of cryptography, an image encryption scheme should effectively resist the differential attack. thus a good image encryption algorithm needs to be very sensitive to the plain images; that is, any trivial change (e.g., a bit or a pixel the number of pixels change rate (npcr) and the unified average changing intensity (uaci) are two important metrics for differential attack analysis. the npcr is defined as the variation ratio of two encrypted images whose plain images are slightly different, meaning the dissimilitude between two encrypted images. the uaci indicates the average intensity of the differences between the same encrypted images. in most cases, only one pixel or even one bit chosen is randomly changed to compute the npcr and uaci. mathematically, npcr and uaci between two encrypted images 1 and 2 can be formulated as follows, respectively."
"in (12), u a and α represents the amplitude and the phase of the complex variableu a, and other complex variables are expressed in the same way. in addition, the battery load is connected to a diode rectifier, which means the phase ofu b is exactly opposite toi̇ 2 ."
"the objective function of the above optimization problem is convex quadratic. the constraint set includes linear inequalities and a positive semi-definiteness constraint. such problems can be efficiently solved to their global optimum using interior point methods and a number of software packages exist for doing so. we use sedumi in our implementation [cit] . the user of the algorithm specifies valid ranges for the entries of the calibration matrix k. from this input, we derive intervals [l jk, u jk ] for the entries ω * jk of the matrix ω * using the rules of interval arithmetic [cit], which specifies the initial convex region d in (24)."
"with the operation of global bit permutation, the bit sequence can be transformed to a pixel plane as the encrypted image. it also can be transformed into a bit cuboid for further bit-level permutation."
"a typical approach to calibrating a camera involves using several images of a known calibration grid. once a correspondence can be ascertained between scene points (or higher order features like curves) and their counterparts on the image plane, it is straightforward to recover the camera parameters. the term autocalibration stems from its key premise that it obviates the requirement for an explicit calibration grid. instead, it tries to locate the image of the socalled absolute conic, which is an imaginary object on the plane at infinity, whose location stays fixed in any metric reconstruction. its image can be shown to be related to the internal parameters of the camera, so locating the image of the absolute conic is equivalent to calibrating the camera."
"nevertheless, even modest simulations presented in our previous report required quite a lot of cpu-time. in this work we propose a parallel implementation of this algorithm. we should also emphasize, that there exists a special parallel algorithm allowing to perform evaluation of aggregation and fragmentation sums of the similar structure as coagulation integrals. even though, a reasonable speedup of calculations was presented at [cit], the parallel scalability of the algorithm is relatively poor [cit] . this fact lies in motivation to exploit one-dimensional domain decomposition approach along spatial coordinate x for advection-coagulation equation but not along axis v and not two-dimensional domain decomposition."
"if we try to use (12) to obtain the expression of the circuit efficiency, that will lead to a series of complex expressions which is difficult to solve, because the currents of primary and the secondary coil are coupled with each other. hence, a current decoupling algorithm is adopted to get an approximate solution of the efficiency."
"in conclusion, the active power source have two potentials for efficiency improvement: change of power distribution and improvement of the current balance. as for the first aspect, the efficiency improvement only occurs when the active power source provides energy to the outside. for the second aspect, the efficiency can be improved when the active power source works as either a load or a power source. it depends on whether the current difference is reduced."
"however, it can be observed in this experiment that the efficiency improvement only occurs when the active power source provides energy to the outside. when the active power source works as a load, the current balance gets worse so the efficiency decreases accordingly."
"as shown in fig. 16, the power transfer circuit includes two batteries, two inverters, coils, resonant capacitors and a load resistor. the batteries are 12v lead-acid battery which can provide nearly constant dc voltage output. the two batteries with two inverters work as primary power source and active power source respectively. a common inverter controller is used to set the frequency and the phase of the two inverters. the screen of the controller is shown in fig. 17 . the square coils without magnetic core are used since that the coil design do not influence the efficiency validation. the parameters are defined in fig. 16 and given in table 2 ."
"although guaranteed to find the global optimum (or a point arbitrarily close to it), the worst case complexity of a branch and bound algorithm is exponential. while this may initially appear to be discouraging, we will show in our experiments that exploiting problem structure leads to fast convergence rates in practice."
"all the results that we have reported are for the raw output of our algorithm. in practice, a few iterations of bundle adjustment following our algorithms might be used to achieve a slightly better estimate."
"since the 7d hyperchaotic system described in section 2.1 has good properties for image encryption, we use it to generate the hyperchaotic sequence applied in the proposed image encryption approach. the generating process consists of three steps."
"a crucial concern in branch and bound algorithms is the exponential dependence of the worst case time complexity on the number of branching dimensions. the number of branching dimensions in most computer vision problems scales with the number of points and views, which can quickly translate into an impractical branch and bound search. in this paper, we exploit the inherent problem structure of autocalibration to restrict our branching dimensions to a small, fixed number, independent of the number of views. in our experiments, this allows the runtime of algorithms proposed in this paper to scale gracefully with the number of views."
"where ω is the operating frequency of the circuit, m is the mutual inductance of the two coupling coils, i 1, i 2 are the amplitude of the coil current, and ϕ 12 is the phase difference between the two currents. the coil resistance causes power losses in the wireless power transfer process. if we take r 1, r 2 as the primary coil resistance and the secondary coil resistance, the power loss can be obtained and the coil efficiency η can be approximately calculated as follows."
a good encryption scheme should have an enough large key space and be extremely sensitive to any small changes in its security key. both a large key space and extreme sensitivity can resist brute-force attacks. so key space and sensitivity to secret key are two essential points in encryption.
"the last thing that has to be done with respect to numerical solution of advection equation is incorporation of pml to emulate the absence of the right boundary condition in the grid. hence, we modify the advection equation into following:"
the efficiency analysis result in (20) and (22) is validated by this simulation model. the result is shown in fig. 13 . the errors between simulation and theoretical calculation are less than 1%. the error may be caused by the calculation approximation in the current decoupling algorithm.
"the mutual inductance can be expressed as in (4), where κ, l 1, l 2 are the coupling coefficient and the self-inductance of both sides."
"a digital image is inevitably contaminated by noise or has data loss during storage and transmission. image encryption should have the ability to resist both noise and data loss. the experimental results of the dfbc for robustness analysis are shown in figure 10 . we firstly add 0.5%, 1%, and 2% salt and pepper noise to the encrypted lena, and the corresponding decrypted images are shown in figures 10(b)-10(d), respectively. it can be seen that, for 0.5% and 1% salt and pepper noise, although the decrypted image contains noise, it can clearly recover the original image very well. however, when the noise increases to 2%, we can only see the outline of lena. when the encrypted image has 0.4% and 1.56% data loss, the proposed dfbc can recover lena, as shown in figures 10(e)-10(f), respectively. for 4% data loss, the decrypted image retains some information for us to recognize lena, as shown in figure 10(g) ."
"the advantages of this hyperchaotic system are two aspects: (1) the algebraic structure is very simple, so it is easy to implement and (2) the system exhibits complex dynamical behaviors, and hence it is suitable for image encryption. therefore, in this paper, we use this hyperchaotic system for image encryption."
"in table 4, although the npcr scores by the dfbc are not as good as those by cdcp, they are very close to the maximum theoretical scores. dfbc apparently outperforms hc-dna in terms of npcr, and it achieves comparable results with chc and ic-bsif. in table 5, dfbc obviously outperform all the other schemes in 6 out of 8 cases, whereas hc-dna still gets the poorest results in all cases. the values of npcr and uaci indicate that the dfbc is able to resist differential attacks."
"the colored, +1 is the center of the mask of filtering which is 1 exactly. it pairs with the pixel, which needs to be filtered. the other weights in the mask after the colored, +1"
"step 2. according to the index sequence, rearrange the original -th bit in the bit sequence as the -th one in the permuted bit sequence."
"the correlation values of input images distribute near the diagonal of coordinate plane, indicating strong correlation of input images. the images monkey, histogram, and parking especially have very obvious lines made by correlation points on the diagonal direction. however, the values of encrypted images distribute almost on the whole plane randomly, which shows very weak correlation of encrypted images. in other words, most of the correlation is removed by the dfbc."
"the test equipment includes a power analyzer pw6001, an impedance analyzer im3570, an oscilloscope, and an acrylic test bench. the entire experimental system shows in fig. 18 ."
"for the optimal solution to the infinite homography relation, we note that both ω * and h i ∞ are homogeneous entities, so our cost function must correctly account for the scale factor before it can be used to search for the optimal diac. moreover, the optimization algorithm itself must take into account the positive semidefiniteness of the diac."
this is a semi-definite program (sdp) in 9 variables and can be solved very efficiently using interior point methods [cit] . the upper bound u i can be computed similarly by computing the reciprocal of the minimizer of the above. the relaxed optimization problem can now be stated as:
"consequently, the minimum attained by the problem (11) will always be at least as low as the minimum attained by (10). in effect, we have constructed a convex problem whose minimum always underestimates the minimum of the nonconvex problem we wished to optimize. the solution to (11) corresponds to the construction of the lower bounding function f lb discussed in sect. 4. an intuitive illustration of the procedure is depicted for a 1-d function in fig. 2 . while the variable s is allowed to attain values only on the graph of the function f (x) in the original problem (10), it can attain any value within the larger region between the convex and concave relaxations in the relaxed problem (11)."
"the active power source is introduced in the secondary side of the wireless charging system. it is connected to a single-phase full-bridge inverter, which is called an active inverter. the active power source with the active inverter are equivalent to an ac active voltage source, as shown in fig. 6, fig. 7 . (8) is achieved, so the maximum efficiency is 98%. the active power source u a is able to adjust its amplitude and the phase between u a and the primary power source u s, by the control of the active inverter. meanwhile, the active power source is either being charged or discharged. the active power source provides additional degrees of freedom and makes it possible to realize the efficiency improvement."
"after the dynamic masks are set, the convolution begins as the normal convolution does except that the mask has to be renewed for each pixel."
"for simplicity of notations we revisit this scheme ignoring volume index i because at each grid point along size coordinate v i we perform these operations along spatial axis x. in fact, there are a lot of possible choices for c j which were introduced by many authors [cit] . in our work, we use the one that is called \"monotonized central flux limiter\" [cit] ."
there is also an opportunity for additional increase of performance of the presented method. as far as coalescence integrals are evaluated sequentially at each processor their calculation can be additionally accelerated by use of gpu and cuda technology. we will apply our ideas to more intricate multicomponent coagulation models [cit] requiring special implementations of multidimensional convolutions based on utilization of low-rank tensor decompositions [cit] .
"during stage corresponding to fig. 1a each process evaluates the coagulation integrals along vertical axis v (lines 1-3 the pseudocode) and keeps two additional vectors for the boundaries assignment (presented as white stripes). stage from fig. 1b corresponds to synchronization of the particle size distributions along axis v (from blue stripes into red of the neighboring process) for the edge coordinates x between the processes (lines 4-5 of the pseudocode). it is necessary for the correct work of the tvd-scheme stencil. during stage from fig. 1c the advection operator is evaluated along axis x for each particle size v (horizontal stripes corresponding to lines 6-10). all in all, during stage from fig. 1d we provide calculation of the time-step result for both coordinates v and x (lines 11-16 of the pseudocode)."
in this work we propose a parallel implementation of the numerical method solving advection-coagulation equation from work [cit] . the numerical scheme is parallelized along spatial axis x which promotes a good speedup of calculations. in the presented benchmarks we show that our scheme of finding the numerical solution of the problem might be accelerated by hundreds of times giving an almost linear speedup with respect to the amount of used cpu cores.
"suppose we can construct a convex underestimator conv(f i ) and a concave overestimator conc(f i ) for the function f i (x). then, the following convex optimization problem minimizes the same objective as (10), but with a \"relaxed\" constraint set:"
in section 2 we present a detailed description of novel parallel algorithm. section 3 is devoted to tests of performance of the proposed algorithm at different computing clusters and lomonosov supercomputer. we obtain speedup of calculations more than by 300 times. this allows us consider a broader class of problems of potential interest for mathematical modelling and studies of advection-coagulation processes. in conclusions we discuss the presented results and possible directions for further generalization of the presented ideas.
"however, there are some problems in practical use. first, extra energy loss will be generated during the chargingdischarging operation of active power source, which will weaken the effectiveness of efficiency optimization. moreover, a significant efficiency improvement requires an active power source with an energy level comparable to that of the vehicle battery, which may bring problems of vehicle economy and spatial arrangement. follow-up researches can be carried out on this basis. his research interests include wireless power transfer and intelligent electric vehicle charging systems."
"operation. permutation conducted on pixellevel data or higher level data (e.g., blocks of pixels) cannot change the statistical characteristics of an image [cit], while permutation on bit-level data can change such characteristics. furthermore, for a fixed process width (bits processed in one run), the bit-level image encryption can involve more pixels than higher level data (e.g., dna-level, pixel-level, or blocks of pixels), for both permutation and diffusion. therefore, to enhance the security of image encryption and to speed up the algorithm, in this paper, we will perform bit-level encryption. specifically, we first transform an image into a cuboid that is very close to a cube, and then various bit plane operations are conducted on the cuboid."
"by now, the proof of proposition 2 has been completed. it has the similar formation with (9). as we can see that, for proposition 2, even if every pixel, is filtered by different template, + +1, proposition 2 still holds and then is detailed as proposition 3 as follows."
"step 2. according to the index sequence, rearrange the original -th bit plane at the direction of depth as the -th bit plane in the rearranged bit cuboid. (4) zigzag. zigzag can disturb the high correlation for images to enhance the security of encrypted images [cit] . in this paper, we use zigzag to permutate the bit-planes in the bit cuboid one by one, as illustrated in figure 5 . in each bit plane, the zigzag confusion path starts from the upper left bit and ends at the bottom right bit. when scanning a plane is completed, the scan starts from the upper left bit of the next plane, and so on. when all the bit-planes are scanned, a bit string is obtained. zigzag can not only permute the bits, but also diffuse corresponding pixel-level data."
"the publicly accessed images lena, cameraman, barbara, and airfield are widely used in the field of image processing, including image encryption. the image histogram is a graphical one which has an almost similar grayscale background and histogram which means most of its pixels are highly correlated. this image is quite different from other real photographs from cameras. the image terrain is an image downloaded from the internet which has a very single sharp mountain shape histogram. the image parking is captured by the author. it has a big area of gray cloud background which has a very high correlation between pixels. the image monkey is a real image captured from a video which is different from these well-known images. it is to validate that our dfbc can be applied to any real images robustly and easily."
"errors in affine and metric properties for three synthetically generated, mutually orthogonal planar grids. the graphs plot (a) angular deviation from parallelism after the affine upgrade and (b) angular deviation from orthogonality after the metric upgrade, both measured in degrees. all quantities reported are averaged over 50 trials deviation from parallelism in the reconstructed grid lines, while the quality of the metric upgrade is inferred from the deviation from orthogonality. table 2 reports the results of this experiment and fig. 5 shows the results graphically. again, we observe that the algorithm achieves very good accuracy for reasonable noise and performs quite well even for 1% noise. with just 5 cameras, it is quite likely for the configuration to be ill-conditioned or degenerate, which causes the algorithm to break down in some cases."
"in the numerator of the expression of active power p a, the order of magnitude of r 1 u 2 a is smaller than the other term. as a result, the sign of p a depends on sinα."
"where ( ) is the probability that the pixel value appears [cit] . when of encrypted image has the same probability, i.e., 1/256, ie reaches maximum of 8. it matches the uniform distribution of the encrypted image perfectly."
"the framework of the dfbc is shown in figure 6 . after generating the hyperchaotic sequence, the encryption procedure consists of three steps: pixel-level diffusion (dynamic filtering and global pixel diffusion), transformation of pixel plane to bit cuboid, and bit-level permutation. the diffusion changes the gray value of every pixel while the permutation rearranges the positions of bits."
"chirality constraints demand that the reconstructed scene points lie in front of the camera. while a general projective transformation may result in the plane at infinity splitting the scene, a quasi-affine transformation is one that preserves the convex hull of the scene points x and camera centers c."
"when an image encryption algorithm is performed on an image, it usually processes block-level data (a block of pixels) [cit], pixel-level data [cit], deoxyribonucleic acid-(dna-) level data (2 bits) [cit], bit-level data [cit], or bit plane-level data [cit] . for fixed processing power, the lower the processing level is, the more pixels the algorithm can handle in one time. for example, if the algorithm can process 8 bits in one time for 256-level gray images, it means that it can process 1 pixel, 4 nucleic acid bases, or 8 bits, which involves 1/4/8 pixels at the most, respectively. therefore, encryption algorithms associated with low-level data usually show better performance in image encryption."
"as outlined in our general recipe for constructing convex relaxations (sect. 5), we have reduced the non-convexity in the above optimization problem to a set of equality constraints. the quadratic inequality constraint is convex and is known as a rotated cone [cit]"
"the series-series (ss) compensation topology is widely used in electric vehicle wireless charging system because of its simplicity, low cost, and the feature of load-independence and coupling-independence [cit] . in this paper, the ss topology is applied to analyze the efficiency improvement of the new configuration. however, the conclusion is applicable to other compensation topologies."
"motivated by the above analysis, in this paper, we aim to propose a novel image encryption scheme that combines a 7d hyperchaotic system with 5 positive les, dynamic filtering, and bit cuboid operation, namely, dfbc, for image encryption. the main steps of the dfbc are as follows. (1) a 7d hyperchaotic system is used to generate a pseudorandom sequence for subsequent encryption operations. (2) 1d dynamic filtering is conducted on each pixel with random filters derived from the pseudorandom sequence. (3) the 2d image of pixel plane is transformed to a 3d bit cuboid. (4) various types of permutation, such as rearranging, symmetry, rotation, zigzag, and global bit permutation, are performed on the bit cuboid."
"all the experiments were conducted by matlab 8.3 (mathworks, natick, ma, usa) on a 64-bit windows 7 (microsoft, redmond, wa, usa) with 8 gb memory and 3.4 ghz i3 cpu."
"the first step of the algorithm is to remove the resistance items and obtain the approximate coil currentsi̇ 10,i̇ 20 . this two currents are decoupled and easy to solve."
"the outline of the rest of the paper is as follows. section 2 describes background relevant to autocalibration and sect. 3 outlines the related prior work. section 4 is a brief overview of branch and bound algorithms. section 5 describes the general strategy that we employ for constructing the convex relaxation of a non-convex function, while sects. 6 and 7 describe our global optimization algorithms for estimating the plane at infinity and the diac, respectively. section 8 presents experiments on synthetic and real data and sect. 9 concludes with a discussion of further extensions."
"since this normalization \"equates\" the scale on the two sides of the infinite homography relation, estimating the diac can now be posed as a least squares problem:"
coagulation and fragmentation processes stand in the basement of a wide class of physical phenomena starting from micro-polymer chains growth [cit] and finishing at the scale of stars formation from interstellar dust [cit] and generalized by hans muller [cit] into the form of the following integro-differential equation:
"it can be equivalent to a current balance condition (8) is achieved. if a wireless charging system aims at a coil efficiency of 95%, the κq must be greater than 40."
"a crucial aspect of designing a global optimization algorithm based on branch and bound is the choice of initial region, which must be principled and guaranteed to contain the optimal solution. arbitrarily choosing a very large initial region will lead to impractically long convergence times for the branch and bound, while too restrictive a choice might not contain the globally optimum point. our affine upgrade step addresses this issue by incorporating chirality constraints within the convex relaxation for the modulus constraints. in practice, this limits the location of the plane at infinity to a small region of the search space. for the metric upgrade step, the entries of the diac that we wish to estimate are related to more tangible entities corresponding to the internal parameters of the camera. so, a user can easily specify reasonable bounds on the focal length, pixel skew and principal point, which are propagated to initial bounds on the diac using interval arithmetic."
"note that while (l1) is a basic stipulation for a convex underestimator, (l2) is a cauchy continuity requirement specific to branch and bound algorithms. indeed, several popular convex underestimators such as linear matrix inequality (lmi) relaxations [cit] ) and sum-of-squares relaxations for polynomial systems [cit] do not fig. 1 the basic mechanism of global optimization using a branch and bound framework, illustrated for a univariate function satisfy this requirement, thus they are rendered unsuitable for our purposes."
"the associate editor coordinating the review of this manuscript and approving it for publication was bhaskar prasad rimal. fig. 1 shows a typical configuration of wireless charging system of ev. it is based on the technology of inductive power transfer. the power from grid is converted to high frequency voltage (79 -90 khz for ev [cit] ). an alternating magnetic field is generated in the primary coil and then a voltage is induced in the secondary coil. the coils of both sides produce a large amount of reactive power, which should be compensated by capacitive networks."
"in order to verify the theoretical analysis results, it is necessary to build a wireless charging experimental system with active power source and carry out experimental verification."
"(5) global bit permutation. after zigzag, the encryption scheme will obtain a bit sequence. the global bit permutation is to permute all bits in the bit sequence with a hyperchaotic sequence [cit] . the steps are as follows."
"here are some discussions about the practical value of the active power source. as described in part i, the electric vehicle usually charges in a constant current -constant voltage (cc-cv) way. the voltage and the current varies significantly, and as a result, it is difficult for a unidirectional wireless charging system to meet charging requirement and achieve current balance simultaneously. the active power source provides a feasibility to adjust the power flow. it can work as a load at the earlier stage of charging, and work as a power source (or a negative load) at the later stage. in this way, it is possible to realize efficiency improvement and energy balance of active power source during the whole charging process."
"typical charging process and battery characteristics of electric vehicle. in this example, the equivalent load increases from 17.9 to 18.5 in the cc charging process, and increases from 18.5 to 73.9 in the cv charging process."
"since the voltage and the current parameters are complex variables, we define that the phase angle ofu s is 0, and other angles are shown in the simplified equation (12) ."
"(1) bit plane rearranging. bit plane rearranging can be conducted at different directions for a bit cuboid. since the operations at each direction are almost the same, here we use the operation at the direction of depth as an illustration for simplicity. the rearranging steps are as follows."
step 3. conduct global pixel diffusion by (17)- (18) as described in section 3.3. the operation will expand a little change in one pixel to other pixels.
"histogram is a popular and effective way to measure the distribution of pixel values in the plain image and the encrypted image for image encryption. the histogram of a plain image is usually unevenly distributed while that of an encrypted image by a good encryption scheme has a uniform distribution as much as possible. a uniform distribution of histogram indicates a totally randomlike image and the least probability of recovering its plain image. in other words, in terms of the performance of encryption schemes, the flatter of the histogram of the encrypted image is, the better the encryption scheme is."
