text
"we use the transactional forwarding algorithm (or tfa) [cit] to provide early validation of remote objects, guarantee a consistent view of shared objects between distributed transactions, and ensure atomicity for object operations in the presence of asynchronous clocks. for completeness, we illustrate tfa with an example (see [cit] for detailed examples). in figure 2, suppose that six transactions (i.e., t 1, t 2, t 3, t 4, t 5, and t 6 ) request object o 1 from the object holder. assume that t 2 validates o 1 from t 1 to t 2 . write transaction t 1 that has requested o 1 before t 2 starts validation will abort when t 1 validates at t 3 . t 2 creates a new version of o 1, which is different from the version that t 1 has requested, so t 1 aborts. in the meantime, while t 2 validates, all other requesting transactions (i.e., t 4, t 5, and t 6 ) abort."
"in addition, gbp is known to be unstable [cit], that degrades its performance. we then append to this paper a study on the damping of update equations of gbp messages to stabilize them. we give a profile of the damping evolution along the snr to obtain better convergence and accuracy of gbp."
"the rest of the paper is organized as follows. we present preliminaries of the d-stm model and state our assumptions in section ii. we describe rts and analyze its properties in section iii. section iv describes our experimental studies. we overview past and related efforts in section v, and conclude in section vi."
"there were 12 conditions for the hi listeners (2 noise types â 3 snrs â 2 unprocessed/processed) and 8 conditions for the nh listeners (2 noise types â 2 snrs â 2 unprocessed/processed). half of the listeners in each group were tested using the ssn conditions before the babble conditions, and the other half were tested in the opposite order. the snr conditions were blocked and randomized for each listener within each noise type. the presentation order for the unprocessed/processed conditions was randomized, but because it is the most critical comparison, they were juxtaposed for each noise type and snr. finally, the sentences were heard in a single fixed order, allowing the sentenceto-condition correspondence to be random. each listener heard 13 sentences in each condition, for a total of 156 (hi) or 104 (nh) sentences. the subset of sentences heard by each nh listener was drawn randomly from the set heard by the hi listeners."
"steal-on-abort, car-stm, and bimodal enqueue aborted transactions to minimize future conflicts. in contrast, rts enqueues live transactions which conflict with other transactions. the purpose of enqueuing is to prevent closednested transactions from restarting. of course, enqueuing live transactions may lead to deadlock or livelock. thus, rts enqueues those live transactions with a low cl and assigns different backoff times for each."
"in this work, we focus on low-density parity-check (ldpc) codes [cit] . these codes are capacity-achieving codes and usually decoded by bp. when approaching high signal-to-noise ratios (snrs), ldpc codes though reveal an error-floor [cit] due to special error events, the trapping sets, that are not corrected by bp. studies were conducted to lower floors. the most of them were dedicated to codes design [cit] but a very few was oriented toward decoding strategies [cit] . here, we propose a novel decoding strategy based on the gbp. no dedicated region-graph has been proposed in the literature for ldpc codes. this paper contributes to the choice of regions by highlighting constructions that are likely to give good results for ldpc codes. we propose to select regions from trapping sets, each one being equivalent to a local loopfree region-graphs on which gbp optimally performs."
"we conducted our experiments in a distributed system testbed comprised of 80 nodes, each of which is an intel xeon 1.9ghz processor, running linux, and interconnected by message passing links."
"rts minimizes the number of aborts of parent transactions, preventing committed nested transactions from aborting. however, some parent transactions holding committed nested transactions may abort due to early validation. also, anticipating an exact execution time is too optimistic. an assigned backoff time may expire before the transaction can obtain an object, so parent transaction may lose all committed nested transactions. thus, there are two causes to abort nested transactions. first, a nested transaction aborts due to the early validations or inconsistency of objects. second, a nested transaction aborts due to its parent transactions' aborts. we measure the number of nested transaction aborts caused by the two aforementioned cases. table i shows the abort rate of nested transactions (i.e., nested transaction aborts due to parent transaction's abort / total nested transaction aborts) under ten thousand transactions and 80 nodes. the number of nested transactions per transaction are randomly decided. vacation and bank benchmarks take longer execution time than other benchmarks, so the abort rate of their nested transactions increases. in high contention, the number of write transactions frequently validate, so the abort rate increases. under rts, the abort rate of nested transactions decreases approximately 60%."
"the performance difference between hi listeners and their nh counterparts (particularly younger nh listeners) is evident in the current results, where different snrs were required and different baseline (unprocessed) scores were obtained. similar differences were found in our previous work, despite the variety of tasks employed (e.g., [cit] . but a consistent result is that hi listeners with processing can approach or match the intelligibility demonstrated by young nh listeners without processing (see fig. 8 ). this suggests that the proposed algorithm can be an effective approach for helping hi listeners in reverberant- noisy environments where the intelligibility gap relative to nh listeners is clearly large. figures 9 and 10 compare scores for the widely used objective intelligibility metric stoi and its updated version estoi with actual intelligibility scores obtained from human listeners for the same stimuli. discrepancies were found between the predicted and obtained values. these discrepancies were particularly large for the estoi predictions of the current reverberant-noisy conditions. this suggests that, although stoi/estoi are widely used as indicators of human speech intelligibility, they cannot serve as a substitute for actual human-listener testing. the inclusion of reverberation in the current study appears to make these objective metrics even less accurate. better metrics need to be developed that deal with both background noise and room reverberation, as well as the effects of hearing loss."
"we consider two kinds of aborts that can occur in closednested transactions when a conflict occurs: aborts of nested transactions and aborts of parent transactions. closed nesting allows a nested transaction to abort without aborting its parent transaction. if a parent transaction aborts however, all of its closed-nested transactions are aborted. thus, rts performs two actions for a losing parent transaction. first, determining whether losing transaction is aborted or enqueued by the length of its execution time. second, the losing transaction is aborted if it is a parent transaction with a \"high\" contention level. a parent transaction with a \"low\" contention level is enqueued with a backoff time."
"3) we search for the largest children that intersect regions of the upper generation, 4) we associate to each of these children a region in the current generation, 5) each children is then connected by directed edges to its parents of the upper generation. we repeat steps 3)-4)-5) while last built regions intersect. we exhibit in fig.1 a factor graph and a region-graph in fig.2 . figure 2 . a region graph for the factor graph in fig.1 . the highest regions are the clusters."
"proof: consider a transaction that includes multiple nested-transactions and accesses multiple shared objects. in the worst case, the transaction has to update all shared objects."
"a widespread approach for dealing with loopy factor graphs is to convert them to equivalent loopfree graphical models. for the most of practical cases, such an approach is intractable because the size of the systems is too large. approximate methods are then necessary. the region-based approximation (rba) can do so absorbing loopy subgraphs in larger nodes, called regions inside a bayesian network called the regiongraph [cit] . the generalized belief propagation (gbp) is an iterative algorithm that passes messages between regions along the edges of the region-graph. messages are then used to compute the marginal probabilities for regions, that solves the inference. gbp is able to dramatically outperform bp if and only if the regions are wisely chosen [cit] . different choices of region graphs indeed give different rba which gbp algorithms offer various trade-offs between the complexity and the accuracy. besides, regions chosen regardless of the factor graph topology surely lead to very poor results. how to optimally choose regions for a gbp algorithm, though, remains an open research problem."
"when the irm is used for reverberant-noisy speech segregation, a key question involves what we should attempt to extract from the reverberant-noisy speech. said differently, what mask should serve as the algorithm's training target? one straightforward choice is to remove both reverberation and noise so as to approximate anechoic-clean speech, as adopted currently. however, according to intelligibility studies using the ibm [cit], another reasonable choice would be to extract both the direct sound (anechoic speech) and its early reflections (e.g., those arriving within 50 ms after direct sound; [cit] ) . to compare these two choices, two different models were trained with the same underlying dnn structure but using either anechoic speech or direct sound plus early reflections as the desired signal. both models also removed noise. informal listening indicated that the first choice was no poorer than the second one, resulting in the current use of anechoic-clean speech as the target. using anechoic-clean speech as the target signal in the irm definition may also result in better speech quality, because more reverberation is removed."
"the best case of scheduler b for aborted transactions is that its communication delays for m objects to visit all nodes invoking n transactions is incurred on shortest paths. thus,"
"if a parent transaction with a short execution time is enqueued instead of aborted, the queuing delay may exceed its execution time. thus, rts aborts a parent transaction with a short execution time. if a parent transaction with a high cl aborts, all closed-nested transactions will abort even if they have committed with their parent and will have to request the objects again. this may waste more time than a queuing delay. as long as their waiting time elapses, their cl may increase. thus, rts enqueues a parent transaction with a low cl. we discuss how to determine backoff times and cls in section iii-b."
speech intelligibility testing was conducted on hi and nh listeners. the results clearly show that the dnn model produced substantial improvements for hi listeners and also some improvement for nh listeners.
"we also elevate the gbp instability by a new damping scheme, improving the computation time and the ber. the experimental results have shown that rba can be applicable to ldpc codes, which gbp outperforms bp in the error-floor."
"we stressed that rba is dedicated to deal with specific bp failures, i.e. other trapping sets of different sizes need other region-graphs to be neutralized. designing other rbas for each trapping set would offer even better improvements."
"ats and pts determine contention intensity and use it for contention management. unlike ats and pts, which are designed for multiprocessor stm, predicting contention intensity will incur communication delays in d-stm. thus, rts collects the cl -a history of how many transactions have requested -to measure the contention intensity. the purpose of the cl is not only to manage contention, but also to reduce the retries of nested transactions. unlike multiprocessor stm, two communication delays will be incurred for a retry, one for requesting an object and the other for retrieving it."
"since the stoi and estoi were developed for predicting the intelligibility for nh listeners, the following discussion is limited to the results for nh listeners and their corresponding test snrs. the mean improvements in stoipredicted intelligibility were 27 and 43 percentage points for ssn (0 and à5 db snr), and 15 and 33 percentage points for babble (5 and 0 db snr). figure 9 compares stoipredicted recognition scores and actual recognition scores. the actual improvements were substantially smaller than those predicted in all conditions. in general, these results suggest that the stoi tends to underestimate human performance for unprocessed reverberant-noisy speech and overestimate human performance for processed speech. the current observation is somewhat different from those for noisy speech enhancement, for which the stoi overpredicts intelligibility for both unprocessed and processed signals [cit] . figure 10 compares estoi-predicted recognition scores and actual recognition scores. the estoi underestimated the intelligibility of both unprocessed and processed speech, especially for unprocessed speech. therefore, a better mapping function should be developed for estoi scores."
"whenever an object is requested, rts performs algorithms 2, 3, and 4. we use a hash table for objects and a linked list for transactions. the transactions will be enqueued as many as cl threshold. the time complexity is o(1) to enqueue a transaction. to check duplicated transactions in all enqueued transactions, the time complexity is"
"for a given factor graph, we may find numerous valid sets of regions that build numerous region-graphs. all region-graphs offer distinct approximations (3) of various accuracies. in the next parts, we then focus on the choice of r."
"we now present the algorithms for rts. there are three algorithms: algorithm 2 for open object, algorithm 3 for retrieve request, and algorithm 4 for retrieve response. the procedure open object is invoked whenever a new object needs to be requested. open object returns the requested object if the object is received. the second procedure, retrieve request, is invoked whenever an object holder receives a new request from open object. finally, retrieve response is invoked whenever the requester receives a response from retrieve request. open object has to wait for a response and retrieve request notifies open object of the response."
"testing began with brief practice in which listeners heard sentences not used in formal testing. the background noise was that used in the first test, and the snr was the middle of the three hi test snrs. five sentences were presented unprocessed in quiet, followed by ten sentences algorithm processed, followed by ten unprocessed reverberant-noisy sentences. exceptions were for the first three hi listeners run (hi7, hi10, and hi11), who heard five sentences at each practice stage. this practice was repeated halfway through the test session prior to switching noise type. new sentences and only the processed and unprocessed reverberant-noisy conditions were employed. feedback was provided during practice but not during formal testing."
"belief propagation (bp) is known as a reliable algorithm to approximate inference in factor graphs [cit] in a wide variety of disciplines, including statistical physics, artificial intelligence, and digital communications. originally created by pearl [cit], bp is likely to exhibit suboptimal performance when graphs have a loop-like topology [cit] . alternative algorithms, searching for the same solutions as bp, have been developed but are relatively slow to converge [cit] ."
"given the challenge of improving hi listeners' speech intelligibility in reverberant-noisy conditions, we prioritized performance over implementation issues such as amenability to real-time processing and computational efficiency. from the perspective of real-time processing, one limitation of the current dnn algorithm is its use of future frames in irm estimation, making the algorithm non-causal. although future frames clearly carry useful contextual information, it has been recently shown that recurrent neural networks encode past context better than the feedforward dnn used in the current study, resulting in a causal system with no poorer performance [cit] . future work will investigate such causal methods. shorter time frames and smaller networks will also reduce processing latency."
"two background noises were used. one was speechshaped noise (ssn) generated using voicebox [cit] . the other was 20-talker babble noise taken from an auditec cd (st. louis, mo). each noise was approximately 10 min long, with the first 8 min used for training/validation and the remaining 2 for testing."
"after feature extraction, a dnn was employed to estimate the irm in order to remove room reverberation and background noise from the reverberant-noisy speech. at time frame t and frequency bin f, the irm is defined as follows:"
"we consider closed-nested transactions in d-stm, which is more efficient than flat nesting and guarantees serialization [cit] . (open nesting is similar to closed nesting, but may need different semantics for concurrency control [cit] .) we present a transactional scheduler for closed-nested transactions, called the reactive transactional scheduler (or rts), which considers both aborting or enqueuing a parent transaction including closed-nested transactions. rts decides which transaction is aborted or enqueued to protect its nested transactions according to a contention level, and assign the enqueued transaction with a backoff time to boost transactional throughput. we implement rts in a java d-stm framework, called hyflow [cit], and conduct experimental studies. our results reveal that transactional throughput is improved by up to 88% over d-stm without rts. to the best of our knowledge, rts is the first ever transactional scheduler for nested transactions in d-stm."
"there are two kinds of aborts on tfa in figure 2 . first, t 1 aborts due to the early validation of t 2 . in this case, the transactions may have requested multiple objects and may have already conducted some operations on those objects. so they must abort. second, losing transactions t 4, t 5, and t 6 abort while t 2 validates. a validation in distributed systems includes global registration of object ownership, which take a relatively long time due to communication overhead. transactions that request an object being validated must abort. many existing transactional schedulers optimize the order for re-executing aborted transactions to avoid their repeated aborts (e.g., [cit] . if a nested transaction commits, its modifications become visible to its parent transaction. these changes only become visible for other transactions when the parent transaction commits [cit] . thus, rts determines that parent transactions, which are designated to abort due to the second case of aborting in tfa, are aborted or enqueued to minimize aborts of its nested transactions."
"we implemented rts in the hyflow d-stm framework [cit] for experimental studies. we developed a set of six distributed applications as benchmarks. these include distributed versions of the vacation benchmark of the stamp benchmark suite [cit], bank as a monetary application [cit], and four distributed data structures including linked-list (ll), binary-search tree (bst), red/black tree (rb-tree), and distributed hash table (dht) [cit] as microbenchmarks. we used low and high contention, which are defined as 90% and 10% read transactions of one thousand active concurrent transactions per node, respectively [cit] . a read transaction includes only read operations, and a write transaction consists of both read and write operations. five to ten shared objects are used at each node. communication delay between nodes is limited to a number between 1 and 50msec to create a static network."
"ats assigns backoff times to aborted transactions. the backoff time indicates when the aborted transactions restart. if a parent transaction aborts, the backoff times may not be effective without considering nested transactions if they exist. rts focuses on whether a parent transaction is aborted or enqueued. if it is enqueued, rts gives the transaction a backoff time indicating when it aborts."
"we justify this construction by stressing that trapping sets are very harmful for bp when the snr is high. in this context, the subgraphs induced by trapping sets rarely all contain harmful errors in the same channel noise realization, very few of them are simultaneously concerned with bp failures. as a consequence, rba has not to take into account the whole tanner graph topology but only local structures. each one describes an error event that might be eventually corrected by gbp. we represent in fig.4 how we split a ts(5, 3) to obtain a loopfree region-graph shown in fig.5 . 2014 ieee international symposium on information theory now we have a well-suited region-graph to deal with ts (5, 3), we are about to run gbp on the tanner code. however this decoder is not stable at this time. in the next section, we propose a solution to stabilize and improve it in terms of computation time and ber performance."
"distributed stm (d-stm) has built upon these results, as an alternative to distributed lock-based concurrency control. in herlihy and sun's dataflow d-stm model [cit], transactions are immobile and objects are dynamically migrated to invoking transactions. the model requires a cache-coherence protocol, which locates an object's latest cached copy, and moves a copy to the requesting transaction, while guaranteeing one writable copy. contention management is also needed. when an object is attempted to be migrated, it may be in use. thus, a contention manager mediates object access conflicts, while avoiding deadlocks and livelocks. similar to multiprocessor stm, d-stm provides a simple distributed programming model (e.g., locks are precluded in the interface), and effective performance (e.g., [cit] )."
"algorithm 2 describes the procedure of open object. after finding the owner of the object, a requester sends oid, txid, mycl, and et s to the owner. mycl is set when an object is received. mycl indicates the number of transactions needing the objects that the requester is using. the structure of an execution time (et s) consists of the start time s, the requesting time r, and the expected commit time c of the requester. if the received object is null and the assigned backoff time is not 0, the requester waits for the backoff time. if it expires, open object returns null and corresponding transaction retries. otherwise, the requester wakes up and receives the object. the t ransactionqueue holding live transactions is used to check the status of the transactions. if a transaction aborts, it is removed from the t ransactionqueue. in this case, even if an object is received, there is no transaction that needs the object, and therefore it is forwarded to the next transaction. algorithm 3 describes retrieve request, which is invoked when an object owner receives a request. if get object gives null, it is not the owner of oid. thus, 0 is assigned as the backoff and the requester must retry to find a new owner. if the corresponding object is locked, the object is being validated, so retrieve request has to decide whether the requester is aborted or enqueued on et s and contention t hreshold. static variables bks represent backoff times for each object. an object owner holds as many bks as holding objects and updates corresponding bks whenever a transaction is enqueued. unless the contention level of the requester and the object owner exceeds contention t hreshold, the requester is added to scheduling list. as soon as the object is unlocked, it is sent to the first element of scheduling list. send a message to the object owner;"
"our work illustrates the idea of enqueuing a live parent transaction to prevent its nested transactions from aborting due to a conflict. doing so will boost transactional throughput by preserving the commits of nested transactions. however, whenever a conflict occurs, enqueuing all live parent transactions does not always improve throughput, because the probability of conflicts also increases. our transactional scheduler, rts, determines transactional contention level (heuristically computed) to decide on whether the live parent transaction aborts or is enqueued, and a backoff time that determines how long the live parent transaction waits."
"to generate reverberant-noisy speech, reverberant speech was mixed with same length random cuts of the ssn or babble noise at various signal-to-noise ratios (snrs). no reverberation was added to the two types of background noise when generating reverberant-noisy speech, as is commonly done in monaural studies [cit] . part of the reason is that it is not straightforward to spatialize a multisource noise such as the multitalker babble used in the current study. for each noise, three snrs were created with reverberant speech considered as the signal in the snr calculation. for ssn, the snrs were 5, 0, and à5 db; for babble, the snrs were 10, 5, and 0 db. these snrs were selected to produce a range of unprocessed-stimuli intelligibility scores for the hi listeners, both above and below 50% correct. the hi listeners were tested using all the snrs, whereas the nh listeners were tested using the two lower snrs for each noise type, because their unprocessed scores were expected to reach ceiling levels at the highest snrs. obtain the t-f representation of the signal, a 320-point fast fourier transform was applied to each frame, resulting in 161 frequency bins. as employed in our previous studies, a set of complementary features was extracted from reverberant-noisy speech and fed to the dnn as input. specifically, the feature set included 13dimensional relative spectral transform perceptual linear prediction, 15-dimensional amplitude modulation spectrogram, 31-dimensional mel-frequency cepstral coefficients, 64dimensional gammatone filterbank power spectra, and their deltas (i.e., differences between the feature vectors of consecutive frames) to capture the dynamic nature of these features. thus, at each time frame, a 246-dimensional feature vector of reverberant-noisy speech was extracted. moreover, a context window with 9 frames to each side of the current frame was utilized to incorporate temporal information of adjacent frames."
"intelligibility was based on percentage of sentence keywords reported. figures 4 and 5 display intelligibility for each hi listener. results for the reverberation plus ssn conditions are displayed in fig. 4 and those for the reverberation plus babble conditions are displayed in fig. 5 . each panel corresponds to a different snr, which is indicated. the black columns represent scores for unprocessed reverberantnoisy speech, and the shaded hatched columns represent scores following algorithm processing. the algorithm benefit for each listener corresponds to the difference between these columns. as anticipated, scores for unprocessed signals generally decreased as hi listener number increased, reflecting poorer baseline performance for the individuals with greater hearing loss."
"when a conflict between two transactions occurs, the contention manager determines which transaction wins or loses, and then the losing transaction aborts. since aborted transactions might abort again in the future, reactive schedulers enqueue aborted transactions, serializing their future execution [cit] . past studies show that, such schedulers often causes only small number of aborts and reduces the total communication delay in d-stm [cit] . however, aborts may increase when scheduling nested transactions. in the flat and closed nesting models, if an outer transaction, which has multiple nested transactions, aborts due to a conflict, the outer and inner transactions will restart and request all objects regardless of which object caused the conflict. even though the aborted transactions are enqueued to avoid conflicts, the scheduler serializes the aborted transactions to reduce the contention on only the object that caused the conflict. with nested transactions, this may lead to heavy contention because all objects have to be retrieved again."
"trapping sets of any size should be taken as relevant bases to run gbp on ldpc codes. each trapping set of arbitrary size (a, b) needs a deep study to be neutralized [cit] . here we focus on ts(5, 3), our goal being to show that rba is able to decrease their influence on the decoding process. as numerous region-graphs might be created from the tanner code, the challenge is then to present a region-graph which gbp is more efficient than bp."
"if the backoff time expires before an object is received, the corresponding transaction will abort. two possible cases exist in this situation. first, the transaction requests the object and is enqueued again as a new transaction. the duplicated transaction (i.e., the previously enqueued transaction) will be removed from a queue. second, the object may be received before the transaction restarts. in this case, the object will be sent to the next enqueued transaction."
"hearing loss is one of the most prevalent chronic health conditions, affecting approximately 10% of the population. a primary symptom of hearing-impaired (hi) listeners is reduced speech intelligibility in background interference. although traditional speech enhancement methods fail to improve intelligibility, it has been recently established that supervised speech segregation based on deep neural networks (dnns) can produce substantial intelligibility improvements. [cit], who employed an extended version of a dnn-based monaural segregation algorithm . the algorithm estimated the ideal binary mask (ibm) when provided only with features from speech mixed with noise. considerable intelligibility improvements were found for hi listeners as well as for normal-hearing (nh) listeners in both steady (stationary) and modulated (nonstationary) noises, with the largest improvement occurring for hi listeners in modulated noise."
"from now on, we are able to run gbp expecting relevant performance in terms ofk and ber. in the next section we present numerical results of our work."
"proactive schedulers take a different strategy. since aborted transactions should not abort again when re-issued, proactive schedulers abort the losing transaction with a backoff time, which determines how long the transaction is stalled before it is re-started [cit] . determining backoff times for aborted transactions is generally difficult in d-stm. for example, the winning transaction may commit before the aborted transaction is restarted due to communication delays. this can cause the aborted transaction to conflict with another transaction. if the aborted transaction is a nested transaction, this will increase the total execution time of its parent transaction. thus, the backoff strategy may not avoid or reduce aborts in d-stm."
"in this subsection, an intelligibility metric, the shorttime objective intelligibility (stoi) [cit] and its extention, extended stoi (estoi) [cit], were used to evaluate the proposed algorithm. these objective metrics provide intelligibility predictions based only on analysis of the acoustic signals. the comparison to the human intelligibility scores reported in the previous subsection should facilitate the development of accurate objective speech intelligibility metrics under reverberant-noisy conditions. another benefit of providing these objective results is to help the interested reader in replicating the current speech-segregation results, as the correct replication will produce the same (or very close) objective scores. the value range of stoi/estoi is typically from 0 to 1, where higher values indicate better predicted intelligibility. since both reverberation and noise are removed by the proposed algorithm, anechoic-clean speech was used as the reference signal when performing stoi/estoi evaluation. table i shows the average stoi scores for the sentences used in the intelligibility testing at different snrs for reverberant-noisy speech and corresponding enhanced speech. estoi results are presented in table ii . clearly, substantial stoi/estoi score improvements were produced by the proposed algorithm. in addition, as snr increased, the predicted amount of improvement decreased. this is broadly consistent with human performance. stoi/estoi scores do not directly correspond to intelligibility in percentage points. in order to obtain percentcorrect numbers, the following logistic function was applied to map stoi/estoi numbers to predicted intelligibility scores [cit] :"
"any region function b r (x r ) is weighted by a counting number c r to comply with the bayesian rule. consider an example with function nodes f a, f b, f c and four variable nodes such that"
"three types of nesting have been studied in multiprocessor a complimentary approach for dealing with transactional conflicts is transactional scheduling. broadly, a transactional scheduler determines the ordering of concurrent transactions so that conflicts are either avoided altogether or minimized. two kinds of transactional schedulers have been studied in the past: reactive [cit] and proactive [cit] . these schedulers cannot directly be used to schedule nested distributed transactions."
"listeners were tested individually in a double-walled audiometric booth, seated with the experimenter. the experimenter controlled the presentation of sentences and recorded responses. the listeners were instructed to repeat the sentence back as best they could after hearing each and were encouraged to guess if unsure. no sentence was repeated for any listener. the total duration of testing was approximately 1.5 h for the hi listeners and less than 1 h for the nh listeners."
"the first step of rba is to cluster the factor graph: 1) we determine loopy subgraphs in g that we want to neutralize, 2) we associate to each of these subgraphs a region, called a cluster, in a set r. if r is valid according to (c1) and (c2), it defines the first and highest generation of the regiongraph. however, r is incomplete because it does not define a valid factorization yet (6) . we complete r building other generations of regions by this algorithm:"
"b α is incorrect because x 2 is counted twice too many on the right side. on the contrary, b β is valid as x 2 is well-balanced between the numerator and the denominator. a factorization is then valid if and only if:"
the data structures depicted in algorithm 1 is used in algorithms 3 and 4. the data structure of requester consists of the address of the transaction identifier of a requester. requester list maintains a linked list for requester and a contention level. getcontention() gives the total contention level of objects representing how many transactions have requested. scheduling list is a hash table to hold a requester list including requesters for an object with object id.
"the stimuli consisted of sentences from the institute of electrical and electronics engineers (ieee) corpus [cit] . this corpus consists of 72 lists, each containing 10 sentences. the 720 sentences were spoken by a female talker and recorded at 44.1 khz with 16-bit resolution. they were down-sampled to 16 khz for processing and presentation. the sentences have moderate to high semantic context, and each contains five key words for scoring intelligibility. our preliminary data indicate that scores near 100% correct can be achieved by most but not all hi and nh listeners when these sentences are presented uncorrupted by noise or reverberation. sentences were selected from lists 1-50, lists 68-72, and lists 51-66 for the training, validation, and test data, respectively."
"support for nesting transactions is essential for d-stm, for the same reasons that they are so for multiprocessor tm -i.e., composability, performance, and faultmanagement [cit] . composability is the ability to group atomic operations into larger atomic operations. many libraries or third-party software contain atomic code, and application developers often desire to group such code, with user, other library, or third-party (atomic) code into larger atomic code blocks. this can be accomplished by nesting all atomic code within their enclosing code, as permitted by the inherent composability of tm and d-stm. but doing so -i.e., flat nesting -results in large monolithic transactions, which limits concurrency: when a large monolithic transaction is aborted, all nested transactions are also aborted and rolled back, even if they don't conflict with the outer transaction. further, in many nested settings, programmers desire to respond to the failure of each nested action with an action-specific response. this is particularly the case in distributed systems-e.g., if a remote device is unreachable or unavailable, one would want to try an alternate remote device, all as part of a top-level atomic action. furthermore, inadequate performance of a nested third-party or library code must often be circumvented (e.g., by trying another nested code block) to boost overall application performance. in these cases, one would want to abort a nested action and try an alternative, without aborting the work accomplished so far (i.e., aborting the top-level action)."
"the outline for the paper is as follows. in section ii, we review the factor graphs, the rules of the rba and the basics to construct a region-graph. in section iii, we introduce our method to associate a well-suited region-graph to trapping sets of ldpc codes. we detail in section iv how we damp gbp to stabilize its update equations. we end by section v where we present numerical results on the binary symmetric channel (bsc) and the additive white gaussian channel (awgnc)."
"yoo and lee present the adaptive transaction scheduler (ats) [cit] that adaptively controls the number of concurrent transactions based on the contention intensity: when the intensity is below a threshold, the transaction begins normally; otherwise, the transaction stalls and does not begin until dispatched by the scheduler. dolev et. al. present the car-stm scheduling approach [cit], which uses per-core transaction queues and serializes conflicting transactions by aborting one and queueing it on the other's queue, preventing future conflicts. car-stm pre-assigns transactions with high collision probability (application-described) to the same core, and thereby minimizes conflicts."
"we consider two properties of the cc protocol. first, when the tm proxy of t i requests o j, the cc protocol is invoked to send t i 's read/write request to a node holding a valid copy of o j in a finite time period. (a read (write) request indicates the request for t i to conduct a read (write) operation on o j .) second, at any given time, the cc protocol must locate only one copy of o j in the network, and only one transaction is allowed to eventually write to o j . figure 1 shows a code example that illustrates a nested transaction. the tx begin and tx end delimiters mark the beginning and end of a transaction, respectively. t 1 is a parent transaction of its first nested transaction t 1−1 . when t 1 starts, the cc protocol locates objects x and y to conduct the ++ operation. the cc protocol independently locates the object i for t 1−1 . after t 1−1 commits, the protocol requests object z for t 1 . objects x and y are still in use unless t 1 commits or aborts."
"transactional memory (tm) promises to alleviate the difficulties with lock-based concurrency control. with tm, programmers organize code that read/write shared memory objects as transactions, which appear to execute atomically. two transactions conflict if they access the same object and one access is a write. when that happens, a contention manager [cit] resolves the conflict by aborting one and allowing the other to proceed to commit, yielding (the illusion of) atomicity. aborted transactions are restarted, often immediately. thus, a transaction ends by either committing (i.e., its operations take effect), or by aborting (i.e., its operations have no effect). in addition to a simple programming model, tm provides performance comparable to highly concurrent, fine-grained locking, especially during high contention situations [cit] . multiprocessor tm has been proposed in hardware, called htm (e.g., [cit] ), in software, called stm (e.g., [cit] ), and in a combination, called hybrid tm (e.g., [cit] )."
"attiya and milani present the bimodal scheduler [cit], targeting read-dominated and bimodal (i.e., those with only early-write and read-only) workloads. bimodal alternates between \"writing epochs\" and \"reading epochs\" during which writing and reading transactions are given priority, respectively, ensuring greater concurrency for reading transactions. kim and ravindran extend the bimodal scheduler for distributed stm [cit] . their scheduler, called bi-interval, groups concurrent requests into read and write intervals, and exploits the tradeoff between object moving times (incurred in dataflow distributed stm) and concurrency of reading transactions, yielding high throughput."
"given a fixed number of transactions and nodes, object contention will increase if these transactions simultaneously try to access a small number of objects. the threshold of a low or high cl relies on the number of nodes, transactions, and shared objects. thus, the cl's threshold is adaptively determined. assume that the cl's threshold in figure 3 is decided as 3. when t 4 requests o 1, the cl for objects o 1, o 2, and o 3 is 2, meaning that two transactions want the objects that t 4 has requested, so t 4 is enqueued. on the other hand, when t 5 requests o 1, the cl of objects o 1 and o 4 is 4, representing that four transactions (i.e., more than the cl's threshold) want o 1 or o 4 that t 5 has requested, so t 5 aborts. as long as the waiting time elapses, their cl may increase. thus, rts enqueues a parent transaction with a low cl, which is defined as less than the cl's threshold."
"in this section we apply rba to decode ldpc codes. we stress that rba is aimed at neutralizing specific patterns in factor graphs. we then focus on particular bp failures that we want to treat: the trapping sets. after that, we describe the method we use to absorb and neutralize them."
"lock-based concurrency control suffers from scalability, programmability, and composability challenges [cit] . these difficulties are exacerbated in distributed systems with nodes, possibly multicore, interconnected using message passing links, due to additional, distributed versions of their centralized problem counterparts [cit] ."
"the contention level (cl) of an object o j can be determined in either a local or distributed manner. a simple local detection scheme determines the local cl of o j by how many transactions have requested o j during a given time period. a distributed detection scheme determines the remote cl of o j by how many transactions have requested other objects before o j is requested. for example, assume that a transaction t i is validating o j, and t k requests o j from the object owner of o j . the local cl of o j is 1 because only t k has requested o j . the remote cl of o j is the local cl of objects that t k have requested if any. t i 's commit influences the remote cl because those other transactions will wait until t k completes validation of o j . if t k aborts, the objects that t k is using will be released, and the other transactions will obtain the objects. we define the cl of an object as the sum of its local and remote cls. thus, the cl indicates how many transactions want the objects that a transaction is using."
"apparent from fig. 4 is that all hi listeners received benefit in all reverberation plus ssn conditions. at least half of the hi listeners received benefit exceeding 20, 40, and 30 percentage points for the snrs of 5, 0, and à5 db, respectively. the benefit in ssn exceeded 10 percentage points in over 85% of the 36 cases (12 hi subjects â 3 snrs). apparent from fig. 5 is that all hi listeners received benefit in the least-favorable babble snr, and most also received benefit at the two more favorable snrs. for the 3 exceptions (of 36 cases), unprocessed scores were high (98%, 97%, and 85% correct). at least half of the hi listeners received benefit exceeding 20, 30, and 45 percentage points for the snrs of 10, 5, and 0 db, respectively. the benefit in babble exceeded 10 percentage points in over 80% of the 36 cases."
in this paper we have dealt with the choice of regions in rba to run an efficient gbp for ldpc codes. we presented a new approach where regions are made with split trapping sets and added bottom-up to the tanner graph. gbp then becomes locally loopfree on trapping sets.
"under long execution time and large cl's threshold, vacation and bank benchmarks suffer from high contention because their queueing delay is longer than that of the other benchmarks. in the mean time, under long execution time and short cl's threshold, the aborts of parent transactions increase. at a certain point of the cl's threshold, we observe a peak point of transactional throughput. thus, in this experiment, the cl's threshold corresponding to the peak point is determined."
"where j denotes the fuel consumption, x(t) is the state variable, i.e., the battery soc, and the engine torque t e is considered as the control variable u(t). based on (5) and (6), the following state function can be constructed, as: (8) whereẋ(t) denotes the variation rate of the soc. in addition, x and u are subject to the following constraints,"
"as discussed above, in order to determine s chg and s dis, we need to first analyze the relationship between the battery energy variation and fuel energy variation. to this end, an energy balance model of the powertrain system is built, and its main function is to describe energy transfer routes when the vehicle operates, as shown in fig. 7 . as can be seen in fig. 7 (22) where e bat_int and e bat_end denote the initial electric energy and remaining energy of the battery, respectively."
where e − bat_fuel can be regarded as sum of the regenerative energy and supplemental energy generated from the extra engine power. e + f _fuel is the fuel energy consumption that partially drives the vehicle and meanwhile residually powers the motor for charging the battery.
"after calculating these two boundary equivalent factors, p(t) needs to be determined. here we suppose that the maximum discharging energy and maximum charging energy of the battery are respectively e + s dis (t) and e − s chg (t) from the current moment until the end of the trip. in order to ensure that the ending soc equals with the initial value, we attain:"
"energy management for phevs cannot only improve the fuel economy with the premise of satisfying the driving power demand, but also extend the battery lifetime and reduce the emissions."
"where p + dem denotes the demanded power of the vehicle, p + mot means the output power of the motor, p + ice is the engine power. it is necessary to note that the superscripts + and − in this paper denote the default value of the variable is positive and negative, respectively.η m means average efficiency of the mechanical transmission system. m expresses the vehicle mass, g is the gravity acceleration and equals 9.8 m/s 2, f presents the rolling coefficient, α is the slope of road, c d expresses the drag coefficient, a is the frontal area and supposed to be 2.25 m 2, ρ a is the air density, v is the vehicle speed, and a denotes the vehicle acceleration."
"where e + bat_ele denotes the difference between the driving energy from the battery and regenerative braking energy, and e + f _ele denotes the consumed fuel energy in this mode. to calculate s chg and s dis, another special condition needs to be considered, in which the engine power equals the driving power and the regenerative energy is utilized to charge the battery. now, we can attain: (25) where e − bat_0 denotes the energy stored in the battery from regeneration, e + f _0 indicates fuel energy of the engine in this condition. by substituting (25) into (23) and (24) and eliminating e + pos and e − neg, the relationship between the fuel energy and battery energy under these two extreme conditions can be obtained, as:"
"now, we can conclude that based on (26) and (27), s chg and s dis can be determined by analyzing the fuel consumption and battery status under these three extreme conditions."
"in this study, two indicators are applied to evaluate performances of the proposed control strategy, of which one is the fuel consumption in the end of the trip and the other is the final soc value. we assume that the initial soc equals 0.3, which means the vehicle certainly operates in the cs stage. after a variety of trial and error, the optimal equivalent factors, i.e., s udds, s nedc and s witc, under these three driving cycles can be found for the typical ecms, as listed in table 2 . then, we apply each parameter to realize the energy management of phevs under these three cycles. all the nine soc variation curves in comparison with those based on the proposed algorithm are shown in fig. 9 . to fairly compare fuel savings, a fuel consumption correction technique, formulated in (34), is applied to make sure that the ending soc is the same."
"as shown in fig. 12 (b) -(e), it can also be observed that in order to maintain the soc near the initial value, the operating points of the engine and motor vary obviously. as mentioned before, when the battery capacity degrades, its internal resistance increases. to reduce the energy lost dissipated by the internal resistance during driving, the motor operating points will move downward on a large scale, meaning that the motor power and bus current will decrease when driving the vehicle; and to ensure that enough regenerative power can be absorbed, the motor operating points remain basically the same as before when the vehicle decelerates. in this case, the output power of the engine needs to increase to compensate the gap caused by the motor, thereby meeting requirement of the driving demand. from table 6, we can find that when the simulation is conducted with the traditional ecms based on the degraded battery, the fuel consumption is the same as that based on the healthy battery, however, the ending soc becomes lower. this is because the traditional ecms still adopts the previous optimal equivalent actor, which keeps the operating properties of the engine and battery almost unchanged, the ending soc exhibits obvious drop caused by the internal resistance increment and capacity attenuation."
"another type of optimization-based algorithms is mainly based on the optimal theory and can be divided into two categories: global optimization algorithms and instantaneous optimization methods. global optimization algorithms can be dynamic programming (dp) [cit], convex optimization [cit], quadratic programming (qp) [cit], and linear programming. these algorithms can find the global optimal solution given prior knowledge of driving conditions [cit] . they are usually regarded as the benchmark for evaluating other strategies' performance. dp is firstly adopted to achieve the energy distribution for a hybrid electric truck [cit] . a main concern when applying dp is the induced intensive matrix computation, i.e., the so-called curse of dimensionality. convex optimization can be applied to find the optimal energy distribution only if the vehicle model can be built as a single or multi convex functions [cit] . qp can be applied to optimize the fuel economy if the fuel rate can be approximated with a series of quadratic equations [cit] . in engineering applications, global optimization strategy lays a solid foundation for developing more feasible and real-time energy management strategies."
"considering the aforementioned problems and emerging technologies, in this study, a novel a-ecms is proposed for the phev based on energy balance principle of the hybrid system, which can be easily applied during the charge sustaining (cs) stage of the vehicle. the strategy aims to improve the real-time controlling performance and reduce the calculation complexity. in this paper, the proposed a-ecms consists of two layers. in the upper layer, the energy balance model of the powertrain system is built and the relationship between the battery electric energy and fuel heat energy is determined. meanwhile, the future transportation information needs to be acquired ahead of departure, and based on it, the variation range of the equivalent factor is defined by calculating a pair of boundary equivalent factors. the main function of the bottom layer is to determine the probability factor according to real-time energy variation during operation and regulate the equivalent factor adaptively based on the determined probability factor within the restrictions determined in the upper layer. simulations were conducted under three typical driving cycles, i.e., the urban dynamometer driving schedule (udds), new european driving cycle (nedc) and worldwide harmonized light vehicles test cycle (wltc), to verify performance of the proposed strategy. results manifest that the proposed algorithm can gain superior fuel savings under different driving conditions, compared to the typical ecms. in addition, the proposed algorithm also takes the battery degradation into account. simulation results show that when the battery is degraded, the savings can reach up to 12%, proving effectiveness and robustness of the proposed algorithm. furthermore, simulation with inaccurate road information was also performed to validate the algorithm's adaptivity when faced with imprecise global driving information. the main contributions of this study can be attributed to the following two aspects: 1) a novel a-ecms is proposed based on the energy balance principle of the hybrid powertrain system. the strategy can regulate the equivalent factor dynamically according to the driving condition and battery performance variation. compared to the typical ecms, generality and robustness of the strategy are obviously improved. 2) the energy balance model of the phev is built, and the transformation relationship is built between the battery power and engine power, by which it becomes easier to apply the proposed algorithm."
"from fig. 11, it can be found that the soc trajectory based on the proposed strategy is kept almost the same with that of the healthy battery. nevertheless, since the equivalent factor of the traditional ecms is fixed, the soc trajectory when the battery is degraded differs obviously from that when the battery is healthy, and the ending soc under the degraded condition deviates from the setting 0.3. as can be found in table 4, when the battery parameters change after degradation, the proposed a-ecms can dynamically update the boundary equivalent factors, and the charging equivalent factor s chg increases, whilst the discharging equivalent factor s dis decreases. as can been illustrated in fig. 12 (a), due to variations of the boundary equivalent factors, the real-time equivalent factor based on the proposed algorithm changes accordingly, thereby guaranteeing that the ending soc maintains near the initial setting value."
"to verify performance of the proposed real-time energy management strategy, simulations were carried out based on autonomie, which is an effective and precise vehicle simulation tool developed by argonne national laboratory [cit] . since the battery degrades gradually with usage and the driving conditions are possibly acquired with some error, in this study, we divide the whole simulation into three parts: 1) study with healthy battery, 2) study with the degraded battery, and 3) validation with inaccurate global driving condition. as such, the algorithm adaptivity can be substantially validated. in this paper, three typical driving cycles, i.e., udds, nedc and wltc, were simulated to compare the fuel savings with respect to the proposed a-ecms and traditional ecms. corresponding speed profiles of these three driving cycles are shown in fig. 8, demonstrating that the vehicle speed profile includes the highway and the urban condition."
"as a combination of electric drive and traditional engine drive, hybrid electric vehicles (hevs) can effectively reduce fuel consumption and exhaust emissions, compared to internal combustion engine (ice) vehicles [cit] . with the development of battery technologies, plug-in hevs (phevs) not only incorporate all functions and merits of hevs, but also can supply a certain all-electric range (aer) powered by the built-in battery pack [cit] . since phevs contain at least two energy sources, usually the engine and battery, an indispensable control problem arises that the power distribution between these two energy sources needs to be properly dealt with, referred to as the energy management [cit] . an efficient"
"for the engine, the common modeling manner mainly includes the theoretical analysis, numerical simulation and experimental data fitting method. since the engine is a high volume 7, 2019 nonlinear time-varying system, in this study, we adopt the experimental data fitting method to lessen computation burden without much influence on modeling precision. by interpolating the steady experimental data, the engine fuel rate and efficiency can be calculated as,"
"she is also a lecturer of mechanical and power engineering with the chongqing university of science and technology, chongqing. she has published more than15 peer-reviewed journal papers and conference proceedings. her research interests include energy management of plug-in hybrid electric vehicles and optimal control of intelligent electric vehicles."
"now based on (29)- (34), p(t) can be determined. then, according to the determined s chg and s dis together with p(t), the real-time equivalent factor trajectories can be sequentially determined. in the next step, simulation is conducted to calculate the equivalent factor, apply the proposed ecms strategy and demonstrate its efficacy."
"1) ahead of departure, the global transportation information needs to be acquired with the help of gps, gis and its, and a pair of boundary equivalent factors can be estimated according to the global driving information."
"as discussed in section iii, the physical meaning of s(t) is the coefficient when the battery electrochemical energy is converted into the lower fuel heat energy of the engine."
"as is well known, ecms is a typical energy management strategy for hevs stemmed from pmp. the essence of ecms is that the energy released by the battery is transferred to the equivalent fuel consumption, and on this basis, the optimal torque distribution scheme, that can minimize the instantaneous equivalent fuel consumption in the premise of satisfying the driver's demand, is allocated to the engine and motor, respectively."
"for optimization-based strategies, investigators usually utilize them to find the optimal or quasi-optimal solutions. one type of these strategies is based on intelligent algorithms that need a series of training actions [cit] . popular solutions include neural networks (nn) [cit], genetic algorithm (ga) [cit], simulated annealing (sa) algorithm [cit], particle swarm optimization (pso) [cit], etc. for these types of methods, one main constraint is that they all need the detailed trip information, which is difficult to acquire in real application. another point is that they rely on a variety of optimal operation data, which should be obtained by other optimization algorithms. different from rule-based strategies, these methods usually train the control model as a black box with a variety of offline data and typical driving conditions. no doubt, they are cost-effective, time-consuming, and demand intensive computation labor and considerable storage space."
"where p + (t) denotes the battery output power, e(soc) presents the open circuit voltage (ocv), i + (t) denotes the loop current, and r(soc) expresses the internal resistance. in addition, the battery soc is a key control parameter when devising the energy management strategy. in this study, the coulomb counting method is employed to estimate the soc, as:"
"as presented in table 7, when the rmse of the constructed driving cycle becomes larger, the charging boundary equivalent factor will increase, and the discharging boundary equivalent factor will decrease. this is arisen by large fluctuation of the vehicle speed when the noise is added. from fig. 14, we can find that when the 10noise cycle is simulated, the soc trajectory is similar with that based on the original udds cycle; and when the rsme becomes larger, the soc trajectory derivates more obviously from the setting 0.3. from table 7, we find that the corrected fuel consumption of the 10noise cycle is almost the same as that of the original udds cycle; however, when the rmse becomes larger, the fuel consumption greatly increases. to summarize, we can say that large speed errors would certainly lead to increment of the fuel consumption, and when there exists limited difference between the acquired road information and real driving speed, the proposed algorithm can still take effect in saving the fuel consumption."
"where φ x t f is a penalty term for converging the final soc to the initial value. in this study, since what we concern is minimization of the fuel consumption during the cs stage, we assume that both the initial soc and terminal soc are the same and equal soc 0 . in addition, the main parts of the powertrain are subject to the following constraints, as:"
"as can be seen in table 5, when the battery degrades, its operating efficiency would certainly decrease due to increase of the internal resistance, and accordingly the proposed algorithm adaptively reduces the motor power and improves the engine power. in this manner, the battery internal energy loss can be reduced, and thus the proposed algorithm regulates the battery power output with higher efficiency, compared to that of the traditional ecms. as such, the fuel savings can be further improved. from table 6, we can find that the proposed a-ecms is much more superior than the traditional ecms when the battery degrades. the corrected fuel savings can reach up to 5% to 11%, proving self-adaptivity and robustness of the proposed algorithm."
"similar to the engine modeling process, this study employs the experimental data fitting method to model the electrical motor. the motor's efficiency and speed characteristics can be acquired by interpolation of experimental data, as shown in fig. 4, and it can be found that its highest efficiency is more than 90%."
"in contrast, instantaneous optimization strategies can consider transient characteristics of the motor and engine under different operating conditions in real time, thereby determining proper working modes of the hybrid system and achieving energy distribution. the most representative candidates belong to the equivalent consumption minimization strategy (ecms) [cit] and pontryagin's minimum principle (pmp) [cit] . both strategies are essentially the same and declare to find optimal solutions by solving the hamilton function. the ecms converts the current electric energy consumption into the future fuel consumption of the engine in an equivalent manner, and then calculates the minimum instantaneous equivalent fuel consumption, by which the optimal energy distribution ratio is decided [cit] . the ecms exhibits strong dynamic adaptability, which does not require prior knowledge of driving conditions and can theoretically achieve the optimal or near-optimal solutions, compared to dp. on this account, it has attracted wide attention from industry and academia that have make much effort to improve its controlling performances."
"in this study, the main target is to improve the fuel economy and maintain the soc around the stable value under the cs mode. to achieve this target, the vehicle longitudinal dynamics model needs to be established,"
"the remainder of this paper is organized as follows. section ii models the phev and powertrain components. in section iii, adaptive adjustment of the equivalent factor is introduced in detail. section iv provides corresponding simulations and verifies feasibility of the proposed algorithm, followed by the main conclusion drawn in section v."
"based on (13),u(t) can be controlled to minimizeṁ equ (t), thereby optimizing the global target function j . for ease of analyzing the energy conversion, the equivalent power consumption p equ (t) can be calculated by multiplying the fuel's lower heat value, as:"
"in this study, an assumption is made that the whole road conditions can be acquired in advance based on gps, gis and its. however, precise acquisition is not an easy task. usually, the anticipated global driving information is estimated by calculating the average speed of vehicles on road within certain intervals [cit] . thus, some difference may exist between the real vehicle speed and statistical average speed. to further validate the fuel savings when acquiring the global driving condition with different errors, the udds is adopted and the white noise with amplitudes of 10, 20 and 30 is respectively added. then, a one-order lowpass filter is imposed to smooth the vehicle speed profile. in this manner, the new driving condition with certain errors is generated, as shown in fig. 13 ."
"in this paper, a novel a-ecms is proposed to achieve the energy management of phevs during the cs stage. to achieve it, the global speed information is acquired ahead volume 7, 2019 of departure, and a real-time varied probability factor and a pair of boundary equivalent factors are dynamically determined according to the energy balance function and battery status. then, the equivalent factor is adaptively regulated according to the global information and boundary equivalent factors. based on it, the proposed a-ecms finalizes the energy distribution of the target phev in real time. simulation results manifest the effectiveness and self-adaptivity of the proposed a-ecms, in particular when the battery capacity degrades and when inaccurate road information is acquired."
"next step work will be focused on improving the fuel economy further by refining control parameters. in addition, the hardware-in-the-loop (hil) validation and actual vehicle test will also be conducted based on the proposed algorithm. jie li received the b.s. [cit] . he is currently pursuing the m.s. degree in mechanical engineering with chongqing university, chongqing, china."
"as depicted in fig. 13, the 10noise, 20noise and 30noise denote the updated driving cycles which are processed by the lowpass filter after adding the white noises with amplitudes of 10, 20 and 30, respectively. as can be seen, the vehicle speed varies more obviously than before, and the rootmean square error (rmse) between the constructed driving cycle and original cycle is 3.75, 7.12 and 10.94, respectively. related simulations were conducted, the soc variation curves are shown in fig. 14 and detailed results are listed in table 7 ."
"where soc 0 is the initial value of the soc, and c bat denotes the battery rated capacity, i.e., 41 ah. in the next step, the ecms will be employed to achieve real-time energy management of phevs."
"the vehicle studied in this paper is a parallel phev. the main powertrain topology is sketched in fig. 1 . it can be observed that there exists a clutch between the engine and integrated-starter-generator (isg), and an automated manual transmission (amt) is equipped between the isg and final drive. the main parameters are listed in table 1, where we can find that the maximum engine power is 105 kw and the maximum motor power is 52 kw. it can be intuitively judged that the engine occupies most of the power demand during the cs stage. the amt has five gear ratios, i.e. 2.56/1.55/1.02/0.73/0.52, and can be shifted automatically according to the speed and power demand, leading to easier design of the energy management strategy."
"3) by finding the control variables that minimize the hamilton function, the optimal control target can be optimized and consequently the toque commands of the engine and motor can be determined. 4) repeat steps 2) to 3) until the vehicle reaches the destination."
"in (17) is the equivalent fuel consumption corresponding to the battery power, and we can say that the physical meaning of (17) can be regarded as the current instantaneous equivalent fuel energy consumption. now the instantaneous equivalent fuel rateṁ equ can be expressed a sum of the fuel rate of the enginė m f and equivalent fuel rate of the batteryṁ bat, as:"
"in this study, an effective but simple battery model, which consists of a voltage source and a resistor connected in series, is adopted based on the experimental data and empirical formula, as shown in fig. 5 . as can be seen in table 1, the rated voltage of the battery is 259 v and its rated capacity is 41 ampere hour (ah). according to the kirchhoff's voltage law, the current of the battery can be expressed as,"
"according to (35) and (36), the corrected fuel consumption results are listed in table 3 . from fig. 9, it can be observed that the proposed a-ecms can maintain the ending soc in the vicinity of the setting value, however, the traditional emcs can only guarantee that the optimal equivalent factor corresponding to the current driving cycle can maintain the ending soc value as desired and may lead to obvious drift if the unmatched equivalent factor is applied to the current driving cycle. by comparing the corrected fuel consumption listed in table 3, the proposed algorithm can achieve more fuel savings than the traditional ecms. for instance, under the udds cycle, when the equivalent factor of the traditional ecms equals to the optimal equivalent factor s udds, the corresponding fuel saving is 0.61% more than that based on the proposed algorithm. however, if the equivalent factor is not equal to the optimal factor calculated based on the current driving profile and instead equals the optimal factor based on the nedc or wltc cycle, the fuel saving of the traditional ecms will be worse than that of the proposed algorithm. in contrast, the adopted a-ecms algorithm can dynamically regulate the equivalent factor according to the driving condition, thereby ensuring the superior fuel economy all the time."
"since the phev can be charged from an external power source, the vehicle can be powered only by the motor until the battery soc drops to an allowable low threshold. in this study, the proposed a-ecms is only applied during the cs stage to manage the energy distribution, thereby achieving superior fuel economy and maintaining the soc within the designed region. the a-ecms that is based on the energy balance principle is structured in fig. 6, from which it can be clearly observed that two control layers exist in the whole framework. detailed calculation process can be described as follows:"
"to meet these needs, we created a comet chart. the starting point for the chart is the situation discussed in section 2: we assume there are two aggregate quantities of interest, p and q, such that:"
"mix effects can cause particular problems for interactive visualizations: as interactivity becomes more common, slicing and dicing data into segments becomes a routine action. in a world where static graphs are painstakingly prepared for publication by professional statisticians, one might hope that any chart would provide proper context. today, however, lay users and experts alike can flip quickly through multiple ways of segmenting data sets. as the reaction to the new york times article shows, even among a relatively sophisticated audience (readers of the nyt business section) a significant number of people don't realize they need to account for mix effects."
"as illustrated by the examples above, a rule provides a way of representing a molecular interaction with consideration of the site-specific details involved. rules are executable, 14 meaning that they are formal elements of a model that can be simulated, and their precision makes them a useful way of summarizing information even if one does not intend to simulate a model."
"interactions among molecules are visualized with arrows in an extended contact map (fig. 5) . the same set of interactions can also generally be represented with rules, and thus an arrow in a map can be linked to one or more rules in a model. this connection is made through a model guide: arrows in a map are numbered, and rules and sections in a model guide are numbered to correspond with arrows. an arrow may correspond to more than one rule if a set of rules share a reaction center. a reaction center is defined as the set of vertices (components) that undergo modification in a graph-rewriting operation defined by a rule. 15 when a reaction center is common to multiple distinct rules, it means that the rules are representing a common interaction that takes place in multiple contexts. rules that share a common reaction center can be mapped to a single interaction arrow in an extended contact map and the contextual differences need not be captured in the map, as these differences are accounted for in the rules themselves."
"the same issues can arise with forms of aggregation other than averages or sums. we have already seen one example: medians for unemployment data. like weighted averages, the population median is generally not equal to the median of the subpopulation medians."
"typically, rules contain contextual information, but every interaction in an extended contact map can be trivially associated with a context-free rule. thus, every extended contact map corresponds to a set of rules that comprise an (1) and (2), which include contextual components: u and rhd, respectively. if these contextual components are omitted, the rules of eqn (1) and (2) become context-free rules."
"in addition to representing protein substructure, an extended contact map can provide other information about a protein, namely its location(s) and products of proteolytic cleavage. to indicate the possible compartmental locations of a protein, one can attach a compartment tab to a molecule box. labels within the tab represent different compartments. a label need not be included for a compartmental location that can be inferred. for example, the compartment tab of the syk molecule box in fig. 4 contains the label 'c' (cytosolic) but not 'm' (plasma membrane). this is because membrane association of syk can be inferred by the association of syk with fceri, a membrane protein. if a model includes rules for translocation of proteins, such as the rule of eqn (2), a tab can be associated with multiple labels to indicate all of the compartments in which a protein can be found, and the compartment tab can also be associated with a set of translocation rules, which can be listed and annotated in a model guide. to indicate that a protein is divisible (i.e., cleaved by the action of a protease into two or more smaller proteins), one can use a dotted molecule box to represent the protein. this also applies to the representation of divisible components. however, a dotted box should only be used when the protein fragments that result from proteolytic cleavage are relevant for understanding the system depicted in a map. one would not use a dotted molecule box to simply indicate that a protein is degraded."
"we now demonstrate how the conventions described above can be used to represent various biochemical processes found in cell signaling systems ( fig. 6-8 ). bngl-encoded rules to accompany these diagrams are provided in appendix s3 (esiz), which serves as a primer on using rules to represent cell signaling processes. other primers are available."
"for any test function, we refer to the probability of false alarm (resp. the miss probability) as the probability that the fc decides hypothesis h 1 (resp. h 0 ) under hypothesis h 0 (resp. h 1 ). the neyman-pearson test consists in rejecting the null hypothesis whenever the (normalized) log-likelihood ratio (llr) l n defined by"
"given that comparisons of weighted sums and averages are a staple of scientific, business, and policy discussions, a natural design problem is to find ways to make mix effects transparent. searching for a single visualization technique that is equally effective for all data sets and all users seems unlikely to succeed, so we narrowed our goal."
"when an arrow branches, a short diagonal segment or pair of diagonal segments can be introduced, which helps identify the box from which the arrow originates (see arrows 1, 3 and 4 in fig. 4) . a catalytic arrow can be broken and extended to point to multiple modifications flags (see arrows 4, 7 and 8 in fig. 4) . if an arrow crosses a modification flag that it does not affect, it may be drawn continuously or broken into segments; breaking of a line into segments is a stylistic option that does not affect the meaning of an arrow. recommended arrows are summarized in fig. 5 . unless otherwise noted, all arrows drawn with solid lines should be assumed to depict trans interactions; cis interactions are depicted with dotted lines. this convention can be reversed if convenient, e.g., in a case where most arrows in a map represent cis interactions. a reversal of the convention should be duly noted."
"two other examples that seem appealing at first glance are found in agresti's textbook categorical data analysis [cit] and a graphic illustrating simpson's effects on wikipedia [cit] . in these diagrams, for each subpopulation there is a rectangle or solid circle centered on a line, whose x position represents the value, and whose area represents the weight. unfortunately, in a prototype system we quickly discovered this method has a serious drawback when used with larger numbers of segments. large circles obscure small ones, and it's hard to see at a glance which subpopulations have changed most."
"the use of relays seems to be attractive since they can improve the performance of wireless networks. unfortunately, current hardware cannot enable transmission and reception at the same time and frequency. but the inherent loss in spectral efficiency can be reduced if bidirectional communication is considered. further, since spatial mimo techniques improve the performance, we assume multiple antennas at all nodes."
which means that not only the received signal y 1b at the (virtual) receiver 1b but also y 2 at the receiver 2 are (stochastically) degraded with respect to the received signalỹ 1a at the (virtual) receiver 1a.
"during the first round, our goal was to learn how the comet chart worked with real users, real data, and real analytic questions to decide if we should pursue this further or try something else. the results were promising. analysts engaged with the tool actively; one said \"this is crazy fun!\" they generally confirmed they saw things they expected to see and also discovered new things."
"with n 1a, n 1b ∼ n (0, σ 1 ) and n 2 ∼ n (0, σ 2 ). now, each (virtual) receiver is only interested in one message. receiver 1a wants to know the confidential message m c, receiver 1b the individual message m 2, and receiver 2 the individual message m 1 . again, m c has to be kept secret from receiver 2, but, of course, need not be kept secret from the (virtual) receiver 1b."
"we expect that the ideas presented here will be immediately useful for the visualization of (large) rule-based models, as well as for more general-purpose visualization of cell signaling systems when one is concerned about protein substructures and site-specific details of protein interactions. models can be evaluated more efficiently when their contents can be visualized and their connections to biological knowledge can be identified. a map and an associated guide provide an effective way of making these connections for rule-based models. we have attempted to anticipate the needs of those who wish to build large rule-based models of cell signaling systems, considering the visualization of an array of molecule types and molecular interactions found in cell signaling systems (see fig. 4 and 6-9) . also, to help ensure serviceable recommendations, we have leveraged the notational conventions of kohn and co-workers. 66, 72 however, at present, the development of large models is not routine, and the guidelines presented here may require modification at some point. in the immediate future, we are dedicated to using and testing these guidelines in our modeling efforts."
". in our previous work [cit] we analyzed the discrete memoryless case with finite input and output alphabets. there, we established the corresponding secrecy capacity region."
"although the problem with omitting weights in a visualization may be obvious in the abstract, it's rarely apparent in practice. the reaction to the new york times article in the introduction is an excellent example."
"next, for california (fig. 5) and ohio (fig. 6 ) the chart reinforces our naive assumption that county-level changes are generally directionally similar to the state-level changes. in contrast, in michigan (fig. 7) and texas (fig. 8 ) the state unemployment rate is almost unchanged, belying potentially important county-level information."
"first, we consider the case where the channel matrices h 1 and h 2 are square and invertible. then, the channel model (1) can equivalently be expressed as"
"to arrive at the comet chart, we prototyped a series of ideas. some included versions of the previous work discussed in section 3.4. for example, we worked with a design much like agresti's with circles showing weight, position on a line for value, and lines drawn between the same circles on two scenarios to show change in value [cit] . unfortunately, with data sets larger than just a few items, this led to busy, unreadable displays in which weight changes were difficult to compare meaningfully. a second prototype used a connected scatterplot, showing more than two scenarios on a single screen. for more than a few segments, however, this ended up creating an overwhelming level of visual clutter and lost any sense of directionality."
"proteolysis and protein degradation. cells routinely degrade proteins: unnecessary or misfolded proteins are dismantled, and protein degradation is used to regulate the rates of biochemical reactions. much protein degradation takes place in proteasomes. 76 in an extended contact map, degradation can be simply depicted as a double-headed arrow pointing from the degraded protein to a 'null' symbol (fig. 6b) . proteases catalyze cleavage of peptide bonds between amino acids. this process has a role in protein degradation as well as in regulation of enzymatic activity. for example, caspase signaling involves caspase-catalyzed cleavage of caspase view online proteins, which liberates enzymatic subunits to assemble into active caspase enzymes. 77 the uncleaved form of a protein may be represented with a dotted border, indicating that it is divisible (fig. 6c) . the proteins that result from the cleavage event are represented within this box. they are connected by a solid line with squares at either end, representing a covalent bond. a 'no' arrowhead points from the catalytic domain of a protease to the covalent bond, indicating that the bond is cleaved. a more elaborate example of representation of a proteolytic cascade is provided in fig. s4 (esiz), which depicts proteolytic cleavage of complement component c3 to c3d. [cit] this figure illustrates how a proteolytic cascade that results in cleavage of a protein at multiple sites can be represented in an extended contact map."
"having briefly reviewed the background material presented above, we are now prepared to introduce the concept of an extended contact map, which combines features of a plain, model-derived contact map ( fig. 2 ) with features of a mim (fig. 3 ). our intention is to provide a means to visualize sitespecific details of molecular interactions in cell signaling systems as well as to provide a means to illustrate and annotate rulebased models, which typically account for such details."
"it may be useful to point out how fig. 4 differs from fig. 2 and 3 . fig. 4 contains information not shown in fig. 2 . this missing information in fig. 2 is information that cannot be directly derived from the bngl-encoded specification of the fceri model, 22, 23 which is given in the esiz (model.bngl). as mentioned above, explicit representation of catalysts is usually missing in bngl-encoded rules, and the list of rules included in model.bngl (esiz) is not an exception. thus, enzyme-substrate relationships are not revealed in fig. 2, whereas such relationships are revealed in fig. 4 . this is one reason why we refer to fig. 4 as an extended contact map. another example of information provided in fig. 4 beyond that provided in fig. 2 is identification of the individual sites of phosphorylation within the linker region and activation loop of the ptk domain of syk. when an extended contact map is used to illustrate a model, we recommend that the map illustrate the biological knowledge underlying the model specification, i.e., the information available to the modeler and considered in model formulation. comparison of an extended contact map and the corresponding model-derived contact map can then reveal how biological knowledge of a cell signaling system has been translated into a formal specification of a model for the system."
"following the formalization of section 2, our goal is to visualize all six quantities that might matter to an analyst: starting and final values, starting and final weights, and the changes in value and weight. we assume that the analysts are in exploration mode, trying either to confirm that most segments are similar to the aggregate or to identify areas to analyze further."
"the two examples in fig. 1 illustrate how a comet chart works, using simulated data on unemployment rates and labor force size for six demographic groups. the left image shows an example in which the aggregate unemployment rate (the black comet) and the rates for each demographic segment of the population (gray comets) are moving in exactly the same way. in this simple scenario the size of the segments-i.e., the mix-hasn't changed, so the segment comets have no horizontal component and are colored gray."
"discontinuous binding sites: biotin and streptavidin. binding sites may be composed of parts of distinct components of a protein or protein complex, and there are various possibilities for how such binding sites and their interactions can be represented in an extended contact map. for example, the four biotin binding sites in a streptavidin tetramer are formed by residues of adjacent monomers that interact as functional dimers. 95 in fig. 9e, the interaction of biotin with a streptavidin monomer is shown to be activated by a neighboring monomer. this diagram can be considered nonstandard. in such a case, a reference to an explanatory note can be included in a diagram. here, 'n' is a label that refers to the explanatory note 'adjacent monomers form biotin binding sites.' in general, a rectangle enclosing a label can be introduced to clarify aspects of map by providing a reference to a note of explanation."
"a desire for additional interactive and exploratory capabilities was a consistent theme. as one analyst put it, changes to the visualization itself would have \"not as much benefit as flexible axis, multiple charts, more filters, etc.\" a few users chose not to restrict themselves to data that fit the \"value-weight\" construct, and used the tool to more generally explore change over time in two different metrics. many of the requested features were ones which would aid exploration, like more complementary charts. or, the ability to use comet charts to more easily explore more complex datasets: more than 2 points in time, multiple dimensions, or a hierarchy of dimensions to use as segments."
"in particular, it would be extremely useful to find ways to present mix effects to non-experts. expert analysts usually need to explain their insights to business leaders, who may not have a statistical background; journalists need to write stories that can be widely read. because the comet chart requires a learning curve, it is not optimal for a casual user; so this remains an important area for future investigation. given that basic mix-effect situations involve as few as eight numbers (two values and two weights, across two scenarios), finding a clear way to present them is a crisp, simply-stated research problem."
"to deploy this new technique, we built it as part of a more comprehensive tool (fig. 10 ) in which analysts input data, select color, scale, or filter, hover on or select comets to get more details, and see the same data in a supplementary table. these elements make the visualization technique accessible and enable analysts to create and explore comet-type charts with their own data."
"the site-specific details of protein-protein interactions are difficult to capture in a conventional model, such as an ode-based model, because of combinatorial complexity, an inherent feature of cell signaling systems. [cit] on the other hand, such details can be naturally incorporated into a rulebased model. rule-based modeling provides a needed capability for mechanistic modeling of cell signaling systems, and accordingly, it has been applied to model various aspects of various systems. [cit] a number of software tools have been developed to enable rule-based modeling. [cit] 15, 16, [cit] with the availability of these tools, we can expect to see more applications of the rule-based modeling approach. a discussion of how to visualize and annotate rule-based models seems timely."
"a final issue is that long comets draw more attention than short ones. when these changes are the key issue, or when comparing changes across segments with different weights or values, this is an advantage. however, some users pointed out that segments with large changes may attract undue attention while smaller changes might be harder to see. similarly, it can be hard to see the aggregate comet when its value and weight haven't changed significantly relative to the chart scale, because then the corresponding black comet is small. one user suggested adding a prominent text description of the aggregate the original median wage example: the strong horizontal motion, especially in \"bachelors+\", indicates important mix shifts. the aggregate's slight upward tilt, showing an increase in median wage, differs from the downward trajectories of all subgroups. the strong horizontal movement emphasizes the importance of the weight changes. highly paid categories are growing, while low-wage categories shrink. in aggregate, the shift in the labor force towards higher education/higher wage segments offsets the declining wages of each segment. color is continuous, defined by change in weight. change."
"view online eqn (1) is identified as rule 5 in the model-specification file. getbonnie 50 provides a tool, rulebuilder lite, for drawing rules and exporting bngl code and for automatically visualizing bngl code according to the conventions illustrated in fig. 1 . the graphs displayed in fig. 1 are examples of pattern graphs or site graphs."
"mix effects that don't involve the dramatic reversal of simpson's paradox are in some ways even more dangerous. it's easy to imagine a scenario in which, say, wages drop 1% overall, but only 0.5% in all subpopulations. in such cases an analyst should note that the 1% drop comes from a combination of factors. but, without the red flag of a differing sign, such a situation may not draw the attention it deserves."
"divisible proteins. all proteins are divisible, i.e., their peptide bonds may be cleaved. however, in some models it is relevant to track the cleavage of a particular protein. in such cases, a special notation for divisible proteins is useful. a protein that may be cleaved is represented with a dotted molecule box, which encloses the fragments that result from cleavage. a divisible protein, caspase-3, is visualized in fig. 6c . caspase-3 is cleaved by the action of caspase-10, which allows the p17 and p12 components of the casc domain of caspase-3 to assemble into an active caspase. 77 a representation of complement component c3 is given in fig. s4 (esiz)."
"it is beneficial to reinterpret this scenario by splitting the legitimate node 1 into two virtual receivers: one for the individual and one for the confidential message. then, the aligned mimo gaussian bbc can be equivalently represented by"
"exchange: ras. gtpases in the ras family of proteins are hydrolase enzymes that bind and act on guanosine triphosphate (gtp) to yield guanosine diphosphate (gdp). in cell signaling, gtpases function as switches, being 'on' when bound to gtp (i.e., able to bind an effector) and 'off' when bound to gdp (i.e., unable to bind an effector). transitions between these two states are mediated by gtpase activating proteins (gaps), which stimulate a gtpase's intrinsic catalytic activity thereby accelerating the rate at which gtp is converted to gdp, and guanine nucleotide exchange factors (gefs), which facilitate exchange of gdp for gtp by loosening the binding of a gtpase to both gtp and gdp. gtp is at a higher concentration than gdp in cells and is more likely to bind an empty binding site. hras is a gtpase that is acted upon by p120"
"unlike the case of treemaps, mix effects are a well-documented problem in the cartography literature [cit] . for example, imagine a state map of low birth weight rates by county, which showed highest rates concentrated in the east. how many viewers would realize that it's possible that when they drill down into state maps for each race, smoking/non-smoking, and gender combination for every map the highest rates are concentrated in the northwest [cit] ? maps are especially susceptible to mix effects for two reasons: (1) their \"size\" dimension, geographic area, is often inversely correlated with population size and (2) confounding variables (race, age, etc.) are often spatially correlated with the variable of interest."
"a third analyst wrote: \"comets goes a long way to solving a problem that often goes unnoticed: aggregate metrics rarely tell the whole story. countless times, presentations treat the mean as if it represents the whole. in an effort to simplify, we reduce data about hundreds of pieces to a single number.\" his actions supported his words, as we found him using the tool late one evening for an urgent analysis."
"an extended contact map can be associated with a map guide or a model guide. a map guide complements an extended contact map by providing annotation about molecules and interactions visualized in a map. a model guide goes beyond a map guide by attaching formal elements of a rule-based model, molecule type definitions and rules, to boxes and arrows. an example of a model guide is provided in appendix s1 (esiz). we recommend that a model guide be organized so that sections in the guide correspond to blocks of a bionetgen input file. 15 a model guide essentially serves as a specification of a rule-based model, although the specification need not be complete. it can serve to annotate not only an extended contact map but also the underlying model illustrated by the map. we recommend that rules in a model guide be specified using bngl 15 because of the availability of various bnglcompatible software tools. [cit] 41, 42, 45, 50 however, any language for specifying rule-based models could be used."
"the diagrams presented above were handcrafted using a general-purpose drawing tool, omnigraffle (the omni group, seattle, wa). the diagrams that appear in fig. 4-10 are provided electronically in the templates.graffle file in the esi.z the esiz also contains an omnigraffle stencil package, which provides access to the glyphs of fig. 5 and should facilitate rapid construction of maps compliant with the guidelines recommended here. for instructions on using the stencil, see readme.txt (esiz). omnigraffle is only available for the mac platform. comparable software available on the windows platform includes microsoft visio. files can be exchanged between omnigraffle and microsoft visio using the microsoft visio xml file format. we provide no software for automatically drawing an extended contact map for a given set of rules or for automatically writing context-free rules for a given map. the requirement for manual construction of a map should not be onerous but there are potential pitfalls. for example, a map could be drawn incorrectly so that it is not entirely consistent with an underlying model as intended, or during the process of model development, map and guide updates could fall significantly out-of-sync. however, our goal has been to present a set of standards that are easy to follow and, if followed, should facilitate the understanding and reuse of rule-based models."
"despite the wide range of applications, the data required for the comet chart tool can be expressed in a simple comma-separated text format that our users found intuitive. users enter data either by copying/pasting from a spreadsheet or submitting a special query identifier to import data from a commonly-used internal database. the latter option proved to be one of most valued features of the tool."
"cellular responses to environmental changes and signals are mediated by cell signaling systems. a cell signaling system is composed largely of a network of interacting proteins, which are responsible for information processing. a typical signaling protein contains multiple functional components. the components found in signaling proteins include catalytic domains, 1, 2 modular protein interaction domains, 3 linear motifs, 4 and sites of post-translational modification. 5 understanding the functional roles of protein components or sites is critical for a thorough understanding of cell signaling, because protein interactions generally depend on site-specific details. for example, many protein-protein interactions are modulated by tyrosine phosphorylation. 6 a large amount of information is available about the site-specific details of protein interactions. there is a need to be able to use this information to make predictions about system behaviors. in other words, we need mathematical/computational models to better understand cell signaling, which is complex. 7, 8 with recent developments in simulation methodology, [cit] rule-based modeling, 14 discussed in detail below, now offers a viable approach for studying large numbers of protein interactions with consideration of site-specific details. here, with the goal of making this modeling approach more accessible, we demonstrate how rule-based models can be better visualized and annotated, which is important for modeling efforts that aim to comprehensively capture the molecules and interactions involved in an entire cell signaling system or set of systems. a large, detailed model is of limited use unless it is presented in an understandable manner. the proteins and interactions included in a model, as well as the justification for modeling assumptions, should be communicated clearly and precisely if a model is to be understood, critically evaluated, and reused. to enable clear communication of rule-based models, we introduce the concept of an extended contact map, which serves to illustrate the scope of a rule-based model. we also introduce the concept of an associated model guide. a model guide attaches rules, which are formal representations of interactions, to arrows in an extended contact map. it also attaches molecule type definitions, which are formal representations of molecules, to boxes in a map. a map and a guide that annotates a complete model together provide a visual and executable means to document information about the site-specific details of molecular interactions in a cell signaling system. we expect that the concepts presented here should be useful for modelers as well as others interested in applying systems approaches to the study of cell signaling."
"+ that lies outside the desired region (7) and assume that this rate triple is achievable by an arbitrary strategy (not necessarily the one presented above) for the aligned mimo gaussian bbc with confidential messages. without loss of generality, we can assume that the matrix power constraint fulfills s 0, cf. [7, lemma 2] for a detailed discussion."
"returning to our original example from the new york times, using the comet chart as shown in fig. 9 we can now see the mix effects at play in the median wage data."
"often diagrams of simpson's paradox are created for pedagogical reasons. a good example is the beautiful interactive tool from berkeley's visualizing urban data idealab [cit] . this diagram, while clear and impressive, does not actually show all six quantities of interest. it also requires significant interaction to see the full story. as a result, it's not suited to a situation where an analyst needs to quickly understand a data set. the b-k diagrams [cit] similarly aim to clarify intuition, and are less suitable for analyzing complex data sets."
"14 it should be noted that mim-like diagrams can be specified using the entity relationship (er) language of sbgn. 57 a mim provides a visualization of a biological system by using boxes to represent molecules and a variety of symbols and lines/ arrows to represent different types of interactions and influences. a mim for the fceri model 22, 23 is shown in fig. 3 . annotation of this mim is provided in appendix s2 (esiz). the main purpose of appendix s2 (esiz) is to explain our use of mim notation, i.e., why we used mim notations as we did in our attempt to provide a mim that accurately reflects the fceri model. 22, 23 the conventions of a mim call for the representation of a molecule only once so that all interactions involving a molecule can be traced to a common origin. this feature of a mim, which is highly desirable as it avoids the need to represent every chemical species that can be populated, as in a conventional reaction scheme, is shared by a contact map. unlike the situation for contact maps, software is not available for drawing mims automatically from model specifications. a mim is a handcrafted illustration, although mim construction is aided by a pathvisio 67 plugin (http://discover.nci.nih.gov/mim)."
"large rule-based models are on the horizon. the motivation to develop such models derives in part from the need for analysis tools, such as models, to interpret molecular properties of cancer cells and to guide the treatment of patients on the basis of molecular profiling data. 100 as models become larger, it will become increasingly important that models of cell signaling systems be documented and communicated in an the visualization and annotation guidelines recommended here for rule-based models are likely to aid modelers in three specific ways: (1) in specification of a model, (2) in communication and evaluation of a model, and (3) in reuse of models. as a starting point for modeling, an extended contact map can provide a way of summarizing and assembling information about interactions of interest before the formal elements of a model are specified. a map also provides an outline for organizing the elements of a model. in fact, a map can be used to organize the work of model specification and model annotation: sections in a guide corresponding to elements of a map can be completed one by one using appropriate parts of fig. 10 (or appendix s1, esiz) as templates. model communication and evaluation are aided because a map and guide together provide documentation of the basis for a model. in the hands of a reviewer, a map should be especially useful. a map identifies what molecules and interactions are included in a model. the accompanying guide explains how these molecules and molecular interactions are modeled. if one is an expert on a particular molecule or is concerned about representation of a particular interaction, one can use a map and guide to quickly identify the parts of a model that should be scrutinized. finally, model reuse is facilitated in part because biological knowledge and modeling assumptions are clearly delineated in a guide. many parts of a guide, perhaps especially the parts related to biological knowledge, can likely be reused if a model is revised and/or extended, easing the burden of model specification and documentation for modelers who wish to build on the work of others. in fact, because a model specification is divided/organized into units (the sections of a model guide), new models can be quickly built through composition of these units. these benefits are perhaps meager for small models but they should be invaluable for large models and more apparent as more models become available."
"more generally, learning what to look for and how to interpret the chart quickly does take some time and experience, as it would for learning to interpret any new type of chart. one analyst confirmed, however, that it did make sense: \"in a clear case, where the comets are all over the place, the message is brought home very elegantly in the chart.\""
"an extended contact map (e.g., fig. 4 ) and a model guide (e.g., appendix s1, esiz) capture more details about a biological system than a bngl-encoded specification of a model for the system (e.g., model.bngl, esiz) or a plain modelderived contact map (e.g., fig. 2) . as discussed previously, explicit representations of enzyme-substrate interactions are often omitted from rules, which is reflected in a model-derived contact map. in contrast, enzyme-substrate relationships are shown in an extended contact map. for example, lyn-mediated phosphorylation of the linker region in syk is shown in fig. 4 but not in fig. 2 . the reason for extra details being included in an extended contact map is that these details are considered in the formulation of a model. if information is collected by a modeler and used to formulate a model, the information should not be lost or separated from a model specification simply because model simulations do not require the explicit incorporation of the information into the formal elements of a model. in addition, an extended contact map and a model guide elucidate modeling assumptions. for example, the bngl-encoded specification of the fceri model 22, 23 (model.bngl, esiz), contains a number of modeling assumptions, such as the lumping together of multiple tyrosine residues in the linker region of syk as a single component, l. accordingly, a l component appears in fig. 2, without information about the tyrosine residues that are phosphorylated. in contrast, fig. 4 identifies three tyrosine residues in the linker region that are phosphorylated during signaling. fig. 4 also identifies specific tyrosine residues in the activation loop of the ptk domain of syk and in the b and g itams of the receptor that are not shown in fig. 2 . as illustrated by these examples, an extended contact map and a model-derived contact map can be compared to reveal the assumptions of a model."
"not only did readers react with surprise but, as the author put it, \"some readers deemed it evidence that i must have made a mistake.\" this reaction illustrates the natural difficulty that many people have when interpreting statistics that are affected by \"mix\"-a difficulty made worse when data is presented using standard graphics that omit the changing sizes of subpopulations."
"one user described the tool as an \"intuitive way of visualising all the moving parts within a population\" while another explained that it \"was simple to use and incorporate one's own data.\""
"the ability of a treemap to represent multiple dimensions gives it more flexibility than, say, a bar chart, but mix effects continue to present problems. in fact treemap visualizations can be misleading if there are mix effects present in the data, a fact we believe has not been noted in the literature."
"but the chart also reveals an important second story about the changing demographics of the labor force during this time. the slope and color of the comets shows that education categories with higher unemployment rates are decreasing as a percent of the labor force (comets slanting left and colored orange) while those education categories with lower unemployment rates have an increasing labor force (comets slanting right and colored blue). the country-wide unemployment rate drop is driven both by decreases in unemployment within each segment and the fact that the labor force is more highly educated than four years earlier. without the comet chart, the latter factor would be easy to overlook since the changes in subpopulation unemployment rates are directionally the same as the aggregate. the next example in fig. 4 [cit], both shortly after a recession. like the first example, we see a relationship between unemployment and changes in the size of the labor force. however, the changes in unemployment rate are now much smaller compared to the changes in relative size of the labor force. in fact, the three largest segments show increasing unemployment rates while the aggregate shows a very slight decline. just looking at the value changes, it might be tempting to assume that the aggregate decline is driven primarily by the decline in unemployment for those without a high school degree. but, in the comet chart, it's clear that this segment is much too small to offset the increases of the larger segments. the fact that the comets are mostly horizontal rather than vertical shows changing mix. essentially there is a trade-off between the increasing size of the labor force for the categories with lower unemployment rates driving unemployment down, offset by the increases in unemployment rate within each large category."
"where s is a positive semidefinite matrix. for the bbc phase we assume that the relay has successfully decoded the individual messages m 1 from node 1 and m 2 from node 2 that it received in the previous mac phase. then the relay transmits both individual messages to the corresponding nodes and an additional confidential message m c to node 1 which has to be kept secret from node 2. the ignorance of node 2 about the confidential message m c is measured by the concept of secrecy [cit], i.e., we require"
"even for viewers who understand the subtleties of mix effects, the issue remains that standard business graphics don't provide ways to show both value and weight comparisons at once. this makes it very difficult to tease apart the effect of mix from the effect of changes in value or to notice relationships between value and changes in size."
"to your surprise, all segments lost ground, even though the trend overall was upward! is that really possible? could there be something wrong with the data?"
"2 who had used the provided business-specific demo data. 10 were active analysts (9 in finance, 1 in marketing) while 1 was a software engineer. these 11 represented 9 different work-groups in 5 offices, and on 3 continents."
"in fact, the data is correct, and what you're seeing is known to statis- ticians as \"simpson's paradox\" [cit] . the resolution of this apparent paradox can be seen in table 2 ."
"these problems are not unique to treemaps. any method where area is used to represent a dimension (mosaic plots, marimekko charts, and bubble charts) suffers from the same drawback. one potential way to fix this problem would be to base sizes on the values in the beginning period. this would have the obvious drawback, however, of not showing the most recent data. in other words, there's an essential tradeoff in treemaps between accuracy and timeliness when choosing the size coordinate."
"it is important to note that arrows are drawn as specifically as possible; in other words, they extend as many layers into the molecule as available knowledge allows, but not further. if an exact binding site is not known, an arrow is terminated at an outer layer and may even terminate at the outermost border of a molecule box. to accommodate space limitations in a map, arrows may branch. as seen in fig. 4, a catalysis arrow from lyn branches to show phosphorylation of the b and g chains."
". similarly as in [3, proposition 1] the proof uses the same ideas and techniques as used for the non-degraded case [cit] taking the characteristic of the bidirectional communication into account. therefore, only slight adaptations are needed."
"simpson's paradox is an extreme example of a more general class of phenomena informally known as \"mix effects\": the fact that aggregate numbers can be affected by changes in the relative size of the subpopulations as well as the relative values within those subpopulations. one can also see this as a case of what statisticians call \"omitted variable bias\" or \"confounding covariates,\" where an unexamined dimension of the data has an effect. social scientists refer more generally to resulting misleading inferences about parts and wholes as the \"ecological fallacy.\""
"the tool enables filtering on certain criteria: increase or decrease in value, increase or decrease in weight, or increase or decrease in the combined weight * value metric. comets that don't meet the desired criteria are shown nearly transparent."
"this allows us to construct an enhanced mimo gaussian bbc by replacing the noise covariance matrix σ 1 of the (virtual) receiver 1a with its enhanced version σ 1, cf. (16). then, (14a) becomesỹ"
"theorem 2 states that when the order of the quantizer tends to infinity, the error exponent k n associated with the np test converges at speed 1 n 2 to the error exponent k that one would have obtained in the absence of quantization. roughly speaking, if β n,n (α) represents the miss probability of the np test of level α, the approximation"
"first, like a scatterplot, the comet chart reveals outliers in terms of unemployment rate, as in imperial county in california, or in the size 3 of the labor force, as in los angeles in california (fig. 5) . moreover, the comets also show outliers in terms of the change in the unemployment rate, as in hale, swisher, and floyd counties in texas (fig. 8) ."
"the first version of the tool had only the comet chart. test users asked for a complementary table showing segment names and exact values. adapting the \"sortable table with bars\" from the popular d3 library example [cit], we added a sortable table with six mini-horizontal bar charts (fig. 10) . the bar charts proved useful as a supplement to the comet chart and as a standalone feature. unemployment rate decreases overall and in most counties. however, the chart reveals outliers: imperial's high unemployment rate and los angeles' large population of laborers. color is discrete, defined by change in value (unemployment rate)."
"since there is no mechanical formula for protecting against mix effects during an analysis, visualization can potentially play a critical role in helping guide analysts toward useful questions and correct conclusions. to find and communicate the real story behind the data, it would be useful to have visualization methods that portray mix effects accurately. in fact, given the pervasive nature of mix effects, it's something of a scandal that no standard business graphics do so."
"further, current cellular system operators offer for several users different services simultaneously subject to certain secrecy constraints. due to the broadcast nature of the wireless medium, a transmitted signal is received by the intended user but can also be overheard by non-intended users. consequently, the design of systems that enable secure communication to certain receivers becomes an important issue especially for the transmission of confidential information, where non-legitimated receivers should be kept ignorant of it."
"rasgap, a gap, and by sos1, a gef. 88 in fig. 8, hras is drawn with a branched interaction arrow pointing to gtp and gdp. a unidirectional chemical reaction arrow from gtp to gdp represents the conversion of gtp to gdp. a cis (dashed) catalytic arrow from hras to the reaction arrow indicates that hras catalyzes the cleavage of a covalent bond and converts gtp to gdp. exchange of gdp for gtp is represented with a special exchange glyph consisting of a pair of bent arrows. an activation arrow from the p120 rasgap -hras interaction arrow indicates that rasgap stimulates gtpase activity. an activation arrow from the hras-sos1 interaction arrow pointing to the exchange glyph indicates that sos1 stimulates gtp/gdp exchange. as depicted in fig. 8, interaction between hras and the rem domain of sos1 allosterically activates gef activity. 89 the hras molecule that allosterically activates sos1 is distinct from the hras molecule affected by the gef activity of sos1, and gdp-and gtp-loaded hras have different allosteric effects, but these distinctions are not made in an extended contact map. instead, rules in an associated model guide would clarify the mechanism depicted in the map. see appendix s3 (esiz). as depicted in fig. 8, the gtp-bound form of hras is able to bind raf-1. 90 the dependence of this interaction on gtp loading is indicated by the activation arrow extending from a solid dot on the gtp-hras interaction arrow to the interaction arrow between hras and raf-1. the diagram of fig. 8 contains a number of activation arrows. as mentioned earlier, we generally discourage the use of activation and inhibition arrows, but fig. 8 provides an example of where these arrows are useful for representing allosteric regulation."
"a guide can be used to specify and annotate a rule-based model, and an extended contact map can be used to illustrate the model. the map provides an extended description of the model, one that goes beyond that provided by the formal model specification. for example, fig. 4 provides an extended description of the model specified in appendix s1 (esiz), in that fig. 4 is more detailed than fig. 2, which is derived directly from the model and is therefore representative of the formal model specification. although fig. 4 is more detailed than fig. 2, fig. 4 is restricted in scope to the same molecules, molecular components, post-translational modifications, and interactions considered in the fceri model. 22, 23 consider dephosphorylation. phosphatases play an important role in regulating fceri signaling 99 but no specific phosphatases are included in the model. instead, unspecified phosphatases are assumed to be available in excess. accordingly, no phosphatase is shown in fig. 4 . similarly, phosphorylation and dephosphorylation of a c-terminal tyrosine residue of lyn is important for regulating lyn activity and fceri signaling, 99 but this residue is not included in the model. rather a certain fraction of total lyn is assumed to be in active form, a form in which the c-terminal regulatory tyrosine is not phosphorylated. as a general guideline, we suggest that an extended contact map be drawn to reflect the biological knowledge that underlies the model being illustrated by the map."
"sites of multiple modifications: histone h3. histone modification regulates chromatin structure. as depicted in fig. 9b, lysine 9 in histone h3 may be modified in two possible ways, by acetylation and by methylation. the balance between these two modifications may influence gene regulation over the course of the cell cycle. overlapping linear motifs: cd3e. the cd3e chain of the t-cell receptor (tcr) contains a proline-rich sequence (prs) and an itam that overlap. in the region of overlap there is a tyrosine residue (y188), which is a substrate of kinases and phosphatases. as part of the itam, y188 is phosphorylated during tcr signaling. phosphorylation of y188 inhibits binding of the prs to sh3 domains in interaction partners, and binding of the prs inhibits phosphorylation of y188. 94 thus, it is relevant to show that the prs and itam overlap. in fig. 9d, the prs and itam are represented as overlapping boxes with y188 located in the overlapping region. the two component boxes can be distinguished by using box lines that differ in shading (as shown) or color. in complicated cases, it may be necessary to explain overlaps in a note or map/model guide."
"users can select from a range of color options to draw attention to different characteristics of the data: absolute change in weight, change in the \"combined metric\" defined by weight * value, increase or decrease in value, percent change in weight, etc. color may be drawn either as a continuous spectrum showing a range of values, or a discrete orange/gray/blue scheme to show negative/zero/positive changes."
"of course, the comet chart is tuned for a specific use case and audience: an analyst looking at aggregates of many items, comparing two different time periods. changing any of these conditions could lead to an opportunity for a new technique. could there be an easy way to compare multiple time periods and scenarios? what if there are new segments or dropped segments? are there techniques that might work for aggregates of just a few items or perhaps of hundreds or thousands? how can we compare categorical scenarios rather than temporal?"
"while the comet chart is a general technique, the tool was specifically designed to help financial analysts at a fortune 500 company quickly visualize disaggregated data and identify if mix mattered for key business metrics. we gathered qualitative feedback by sitting with experts as they used the tool or, if out of town, by asking them to demo and send feedback. interviews included observation, answering questions, asking questions about what they were doing or noticing, discussing interpretations and discoveries, and asking for desired features. there were two rounds of feedback: an initial round to inform chart and tool development and a second round to validate if the tool worked in practice. in total, we received feedback from 28 individuals with relatively little overlap between rounds. the first round of interviews occurred during development of the tool and helped inform its design. this included conversations with 15 people, including 9 likely target users and 6 others who would have a valuable perspective as statisticians or senior analysts, familiarity identifying mix effects from earlier work, or as a fellow tool-creator. 3 target users provided their own data, while relevant datasets were supplied to the other 6. 5 sessions were one on one, while 1 was with a team of four who worked closely together. most conversations were in-person over the course of one week in the company's home office, while we followed up by email or video conference with 4 others who worked in other offices including abroad."
"the rest of the paper is organized as follows. in section ii, we describe the observation model and evaluate the error exponent in the ideal case where the fc has perfect access to the observations. in section iii, we introduce high-rate quantizers, and evaluate the degradation on the error exponent when the decision is made using quantized observations instead of the ideal ones. we determine relevant quantization strategies allowing to reduce this degradation. section iv illustrates our results for the detection of a gaussmarkov signal in noise."
"alternate subunits: apc/c. many enzymes are multimeric proteins. an example is apc/c, a cullin-ring domain e3 ubiquitin ligase, the specificity of which is determined by a regulatory subunit. the regulatory subunit can be either cdh1 or cdc20. 91 in fig. 9a, a component box is introduced for a regulatory subunit in which the two possible components are included, separated by an xor symbol, indicating that only one may be associated with core apc/c at a time."
"how does complicated real data compare to our idealized situation? to demo the comet chart \"in action\" we chose a dataset that compares changes over time rather than differences between categories, as this best reflects the type of data relevant to our target analysts."
"to summarize, our initial deployment showed that the comet chart provided value to many of our users. at the same time, feedback revealed that the comet chart handled certain cases better than others, and that there are areas for improvement."
"in other words, the horizontal \"motion\" indicates that understanding mix is critical to analyzing this data. however, unlike our idealized dataset in which the aggregate changed in just value and the segments changed in just weight, in this example the aggregate metric shows almost no change while all segments change in value and weight. while we might want to dig deeper to learn exactly what is going on, it's certainly clear that the aggregate is not representative of the segments and that the changing demographics of the labor force are important to understanding the relationship between wages and education."
"in early testing, several users confirmed that our likely audience was analysts, skeptical that it would work for presenting results to business leaders. however, once we had deployed the tool and analysts started making discoveries, the top feature request was easy ways to share what they found with non-analyst partners: emailing a url, converting to a \"presentation-ready\" format, or embedding in a dashboard."
"an extended contact map for early events in fceri signaling is shown in fig. 4 . in fact, this map illustrates the fceri model. 22, 23 a guide for the map of fig. 4 and the associated annotation of this mim is provided in appendix s2 (esiz). note that this mim is intended to be read using the combinatorial interpretation of mim notations. view online model is included in the esiz (appendix s1) and will be discussed later. the guide lists and annotates proteins and interactions included in the model. arrows in the map are numbered to correspond to sections in the guide. each section includes a summary of available knowledge about an interaction and a set of rules. the rules in a set are related, in that they share a common reaction center. in other words, the rules in a set describe the same interaction, but in different contexts."
"in the statistics literature there are a variety of diagrams used to visualize mix effects, but we found few that meet our criterion of explicitly showing all six quantities described above. for example, consider the scatterplot from the original study on the berkeley dataset, reprinted [cit] (reprinted with permission). the diagram shows a negative relationship between department admission rates and proportion of female applicants for larger departments, which explains the \"paradox.\" however, it provides no indication of overall or individual differences in admission rates."
"by whatever name, however, mix effects are ubiquitous. experienced analysts encounter them frequently, and it's easy to find examples across domains. a famous example of a simpson's-like reversal is a berkeley graduate admissions study in which 44% of males were accepted but only 35% of females. the discrepancy seemed to be clear evidence of discrimination, yet disappeared once analyzed at the per-department level: it turned out that departments with lower acceptance rates had proportionally more female applicants. similar reversals have appeared in studies of race and the death penalty in florida [cit], standardized test scores and education spending per student [cit], studies of the \"hot hand\" in basketball [cit], baseball hitting averages [cit], treatments for kidney stones [cit], and in an active debate around the diagnosis of meningococcal disease [cit] ."
"having [7, lemma 1] in mind, we immediately obtain from theorem 2 with a matrix power constraint (3) the corresponding result for an average power constraint (2) which, of course, characterizes the practically more relevant case. corollary 1. the secrecy capacity region c bbc (p ) of the mimo gaussian bbc with confidential messages under the average power constraint p is given by the set of all rate"
"the finance analysts at this company were our targeted users. such analysts don't necessarily have a degree in statistics, but all have a strong quantitative background and work with data daily. they investigate data in order to inform decisions made by leaders in sales, product, or finance. previous analyses had revealed that mix issues were common and important. the analysts needed an easier, quicker, and more systematic way to diagnose situations where mix mattered."
"in this section, we describe the probabilistic model underlying the observations. we assume that the observed time series y k follows a hidden markov model under both hypotheses h 0 and h 1, with different transition kernels. our hypothesis testing problem shall thus reduce to the question: which kernel underlies the observation process ?"
"at the same time, we heard helpful critiques as well which are included in section 7.5. in particular, this feedback helped us to clarify the scope of the tool."
"in practice, the situation can feel even more complex. the ratios between weights and values in the two scenarios-that is, a i /b i and x i /y i -are sometimes more important than the absolute numbers. for instance, in the berkeley example, a key metric was the ratio of female to male applicants per department, a i /b i ."
"common visualization methods are generally not equipped to show all four dimensions at once. for example, a bar chart might compare p and q directly, and then allow a drill-down showing comparison bar charts of only the x i and y i . without showing the weights a i and b i as well, the resulting picture is obviously incomplete."
"we now investigate the case where the final decision is made from quantized observations., we introduce some useful characteristics of a given quantizer q n :"
"corresponds to taking a weighted average of the x i and y i . in a weighted average model for the berkeley case, one could set a i to the fraction of all women who applied to each department and b i to the fraction of men. the quantities p and q would then be the university-wide acceptance rates for women and men."
"the a i and b i represent weights and the x i and y i represent values for segments 1 to n segments, assuming the same set of segments in both scenarios. to represent all four of these dimensions at once, we use a variation of a scatterplot in which the x-axis represents weights and the y-axis represents values. for each i, we then draw a modified line segment from the point (a i, x i ) to the point (b i, y i ). the resulting \"comet,\" flowing from tail to head, represents the change for the ith subpopulations between the two scenarios."
"in general, if an interaction depicted in a map occurs in more than one contextual setting, then a rule can be provided for each contextual setting of interest. also included in the esiz is a version of fig. 4 that is more aligned with the diagrammatic conventions of sbgn 57 ( fig. s3, esiz). however, sbgn does not presently provide conventions for illustrating molecular substructures, site-specific details of molecular interactions, or rule-based models. thus, fig. s3 (esiz) serves as a proposal for an extension of the conventions for er diagrams in sbgn, which complements existing proposals for er language development (http://sbgn.org/er_development)."
"by combining these criteria, one can show only those comets, for example, that have decreased in value and increased in weight and decreased in the combined metric. this may reveal groups of segments with similar characteristics."
"allosteric regulation of a metabolic reaction. allosteric regulation occurs when an effector molecule alters an enzyme's activity by binding to a site on the enzyme that is distinct from the active site. the result may be either an increase or decrease in catalytic activity. an example of an enzyme controlled by allosteric regulation is phosphofructokinase-1 (pfk-1). this enzyme catalyzes a key, irreversible step in the glycolysis pathway, and it is a central point of regulation. for example, pfk-1 is positively regulated by fructose-2,6-bisphosphate. 76 in an extended contact map, allosteric regulation of enzymatic view online activity by a small-molecule effector is represented as follows. a direct physical interaction arrow is drawn between the enzyme and effector. an activation or inhibition arrow then originates from the interaction arrow and points to the catalysis arrow between the enzyme and substrate (fig. 6d) . we generally discourage the use of activation and inhibition arrows because they tend to be ambiguous, but they are useful for representing allosteric regulation. in this example, plain text is used to represent metabolites, rather than boxes, to make a distinction between small molecules and macromolecules. if a material component considered in a model is not treated as a structured object (i.e., a graph) in a model, it and the reactions in which it participates can be represented using conventional means for representing biochemical reaction networks. fig. 4 . the reverse process, dephosphorylation, is the enzyme-catalyzed removal of a phosphate group from an amino acid residue. dephosphorylation can be just as important as phosphorylation in regulating protein interactions and catalytic activities. unregulated basal dephosphorylation by unspecified phosphatases can be omitted from an extended contact map, as in fig. 4, because it would necessitate an additional arrow for every phosphorylated residue, making the map less readable. however, it is sometimes significant that a specific phosphatase acts on a specific substrate. for example, dephosphorylation of the c-terminal regulatory tyrosine in the kinase lck by shp-1 prevents the formation of an intramolecular bond, which regulates lck kinase activity. 81 as in the mim of fig. 3, kohn and co-workers use a jagged line to represent dephosphorylation. 68 as an alternative that is more compact and more consistent with our notation for catalysis of covalent bond formation, we suggest depicting dephosphorylation (and more generally cleavage of a covalent bond) with a 'no' symbol (fig. 6f) . in the case of lipids (e.g., dephosphorylation of phosphatidylinositol (3,4,5)-trisphosphate by pten), 82 dephosphorylation can be represented as a standard chemical reaction with a catalysis arrow pointing from the enzyme to the reaction (fig. 6e) . we also use this example to demonstrate an interaction between a lipid and a protein: pip3 binds the pleckstrin homology (ph) domain of pdk1, recruiting pdk1 to the plasma membrane. 83 transport. an extended contact map does not aim to illustrate transport or trafficking between compartments, but a map can be used to indicate compartmental locations of molecules. compartments and transport between compartments can be represented explicitly using cbngl. 51 the names of the compartments in which a molecule can be found can be included in an extended contact map in the form of a tag attached to a molecule box. in fig. 6g, two location labels, 'cyt' and 'nuc', are included within a single location tab attached to a molecule box for nf-kb. the tag indicates that nf-kb is considered to have two possible compartmental locations. a location tag can be associated with a rule, such as the rule of eqn (2), to clarify details about trafficking between compartments. in the case of fig. 4, molecules are considered that are found in three compartmental locations, and all the molecules are represented in the same map. in more complicated cases, it may be convenient to draw separate maps for separate compartments. note that compartmental locations that can be inferred from interactions need not be included in a map. for example, the location tag attached to the syk molecule box in fig. 4 only indicates that syk is cytoplasmic. it can be inferred that syk is membrane associated when it interacts with fceri, so a membrane location label is not included in the syk location tag."
"the essential problem with standard business graphics (bar, line and pie charts) is that they show only a subset of the relevant six dimensions, often values only (as in a bar or line chart) or weights only (as in a pie chart). combining charts via small multiples, in the form of trellis, lattice, grid, or panel charts, can help and is one of the best approaches available. however, these still only show a subset of these six critical dimensions or don't show them in context with one another."
"mix effects are ubiquitous and confusing. they can surprise and mislead lay users of visualizations, and even expert analysts can benefit from better tools to understand their implications. as we have seen, common business charts and maps make it difficult to show the effects of simultaneously varying subpopulations' size and metrics. furthermore, we have shown how more sophisticated tools that can show multiple dimensions, such as treemaps, mosaic plots, and bubble charts, are nonetheless susceptible to perceptual biases. just as there is no statistical silver bullet to account for mix effects, there is likely no single magic visualization that makes them obvious. instead, it is the responsibility of analysts to look for these effects and for information designers to make sure that any visualization of aggregate data makes it clear how overall data relates to subpopulation metrics. one purpose of this paper is simply to heighten awareness: we believe that many visualization designers and users underestimate the challenges of presenting aggregate statistics. despite the many treemaps published on the web, for example, we know of none that attempts to correct for the upward bias caused by changing sizes. at a more prosaic level, it's common to see visual displays (as in the new york times article) that omit any reference to mix."
"we aimed to find a display technique that would help experts sort through problems related to mix effects. in particular, this project was motivated by the first author's experience as a senior analyst on a team of financial analysts at a fortune 500 company. the author had done numerous painstaking analyses in which the conclusion was that a mix shift had driven top-line changes in critical metrics. a typical approach was to \"slice and dice\" the data, including using small multiples to see many slices at once, exploring interactively and iteratively. this was slow, required considering both size and value changes, became unrealistic for more than a small number of segments, and usually only caught big shifts while missing more subtle but meaningful patterns. the author wasn't alone: analysts in finance, product, engineering, sales, human resources, and marketing all worked on problems affected by changes in mix over time."
"the righthand image in fig. 1 illustrates an example in which mix effects come into play. in this scenario too, the aggregate rate (black comet) has increased. however, the unemployment rate within each segment has remained constant! what has happened is that segments with high unemployment have grown (blue comets) while those with low unemployment have shrunk (orange comets). this is a simpson'slike situation. the fact that the segments have changed only in weight is indicated by the pure horizontal direction of the segment comets."
"other testers noted that datasets with different numbers of segments needed different design solutions; for them, the comet chart worked best for 6-100, was weaker for over 100, and failed over 500. too few segments resulted in a poor data to ink ratio and simpler tools like a table would suffice. too many segments resulted in occlusion or challenges due to extreme outliers. in practice, we found that there was a great need for visualizing data in this middle \"goldilocks\" space of 6-100 segments. analysts found that this technique also worked for long-tailed distributed data, which they faced often and which seem especially prone to mix effects since different factors may be causing meaningful changes to size and value of head, torso, and tail segments."
"finally, to prove theorem 2 it remains to extend the secrecy capacity region of the aligned bbc (6) from the previous section to the general case (1), where the channel matrices h 1 and h 2 need not be necessarily square and invertible. basically, this is done by approximating the (arbitrary) channel matrices by square and invertible matrices so that theorem 3 for the aligned case is applicable. the approximation follows [3, sec. iv], where the corresponding result is proved for the classical broadcast channel with confidential messages."
". we denote its density by p i,n (z 0:n ). the np test consists in rejecting the null hypothesis for small values of the llr l n,n associated with the quantized observations:"
"since the error exponent k n does not depend on the particular choice of the quantization alphabet ξ n, we assume that each ξ n,j coincides with the center of cell s n,j . we separately study each term"
we analyzed secrecy in bidirectional relay networks and characterized the secrecy capacity region where it shows that a strategy that superimposes two signals -one for the bidirectional and one for the confidential communication -is optimal. this is surprising insofar as in contrast to the discrete counterpart [cit] no additional randomization is needed.
"protein synthesis and interaction of a transcription factor with a dna binding site. according to the central dogma of molecular biology, protein synthesis consists of two basic steps: transcription of dna into mrna, and translation of mrna into a polypeptide. 76 these steps may be regulated in many ways and additional steps may be involved in de novo protein synthesis; however, we are often only interested in the relationship between a gene and its protein product. in this case, one can use a shorthand notation to indicate synthesis of a protein encoded by a gene (fig. 6a) . a double-headed arrow points from a molecule box for a gene to a molecule box for a protein to represent the multistep process of transcription/ translation. the double arrowhead is intended to suggest that steps are not shown. dna is represented as a pair of parallel lines, and boxes for genes, promoters and other regulatory elements are embedded within these lines. this example also shows binding of a transcription factor (tf) to dna and indicates that this interaction stimulates transcription/translation. a solid dot placed on the dna-tf interaction arrow serves as a point of origin for an activation arrow. in general, a dot is placed on an arrow when it is necessary for another arrow to begin or end at that point. a similar combination of symbols could be used to represent other synthetic processes."
"versions of the latter source signal. each observation is quantized on log 2 (n ) bits before being transmitted to the fc. our aim is to evaluate the impact of quantization on the error exponent, and to characterize relevant quantizers allowing to maximize the error exponent."
"in the berkeley admissions case, assuming n departments, the weights a i and b i could be the numbers of women and men applying to each department, and the values x i and y i are the acceptance rates for women and men, respectively, in each department. the quantities p and q are the total women and men admitted."
"the point is that even when one can reasonably attribute a change in value to lower-level changes in mix, that alone cannot tell the correct interpretation. additionally, sometimes the essential thing is not to adjust for mix but to discover why mix is shifting. therefore, it is important to be able to examine aggregate changes together with the disaggregated data."
"in our post-deployment survey, we sought to find out if the analysts were aware of and needed to understand mix, how they currently addressed this need and if their current methods were satisfactory, and if the comet chart tool met this need well. our respondents' answers showed a need to understand mix and that current tools are not effectively meeting this need. 10 out of 11 respondents said that mix issues come up \"sometimes, often, or very often\" in their work. despite being aware of mix issues and encountering them regularly, 5 said that their current tools reveal mix issues \"not at all well or slightly well\" while 5 others said \"somewhat well\" and only one said \"very well.\" in contrast, 9 respondents said the new comet tool worked \"very well\" or \"extremely well.\" one user summarized it saying that \"i don't have another quick way to visualize mix effects that could be taking place.\""
"note that the noise of the (virtual) receivers 1a and 1b has the same covariance matrix σ 1 as the noise of the legitimate node 1 in (6) . similarly, the noise of receiver 2 has the same covariance matrix σ 2 as the one of the non-legitimated node 2 in (6). therefore, any strategy that achieves a certain rate triple for (6) will do likewise for (14), and vice versa, so that both scenarios have the same secrecy capacity region."
"in summary, current charting tools are not meeting the needs of users, experts and amateurs alike. at the same time, visualization designers often do not take into account the potential surprises that mix effects can cause. we've described one method, the comet chart, for handling these issues, and we believe that finding ways to address mix effects in visualizations is an important and promising area for future research."
"some users questioned the \"comet\" device for showing direction of line segments. early on, we switched from an \"arrow\" to a \"comet\" interpretation based on feedback from users that \"comets\" matched their natural intuition. nonetheless, we found first-time users often checking the legend, some finding the direction more natural than others. the issue generally resolved as users learned the convention."
"for the mimo gaussian bbc without confidential messages [cit], the capacity-achieving strategy combines both individual messages based on the network coding idea. here, we have an additional confidential communication so that the optimal processing is by no means self-evident. interestingly it shows that a superposition strategy that superimposes two signals, one for the bidirectional and one for the confidential communication, achieves the desired secrecy. the analysis of the secrecy capacity is the indispensable basis for the design of further signal processing applications and algorithms."
"our target users frequently encounter log-normal or similar heavytailed distributions. as with many standard charts, allowing the option of a log scale for one or both axes proves helpful. it also allows us to compare either absolute or relative differences in value or weight."
"rule-based modeling is a relatively new modeling approach in biology that is well-suited for capturing the dynamics of interactions among proteins. 14, 15 the approach can be viewed as a particular type of agent-based modeling, in which agents (molecules) interact according to rules consistent with certain physicochemical principles."
"). the x-coordinate represents the average segment weight. the y-coordinate represents aggregate value. in all, this distinguished comet, shown in black, represents the difference between the overall scenarios so that the aggregate can be compared to the individual segments."
"here, we emphasize visualization of proteins, but an extended contact map can also include other types of macromolecules, such as dna, as well as small-molecule compounds, such as lipids, drugs, and metabolites. we recommend that boxes be reserved for macromolecules and we recommend that smallmolecule compounds be represented using plain text."
"the error exponent k provides crucial information on the performance of the np test when the number n of sensors is large. by lemma 1, the evaluation of k simply reduces to the asymptotic analysis of the llr l n under h 0 . in this framework, a number of works in the literature derived and analyzed the error exponent for various observation models (see for instance [cit] and reference therein). however, most of these works assume that the fc has a perfect knowledge of the sensors' observations. unfortunately, in a wsn, the amount of information forwarded by each sensor node to the fc is usually limited, due to imperfect links between nodes of the network. therefore, a large numbers of papers have been devoted to the construction and the analysis of decentralized detection schemes [cit] . in this framework, each sensor has the ability to compress/quantize its observation before transmission to the fc, with the aim of decreasing the information transport burden to be supported by the network. for instance, [cit] investigate the detection of deterministic signal in noise, while [cit] study the case where all observations are independent and identically distributed (i.i.d.)."
"is valid when both the number n of sensors and the order n of quantization are large, but n n . the loss in error exponent depends on the quantizer family only via its model point density ζ. expression (10) provides the optimal choice of ζ i.e., the model point density which minimizes the loss in error exponent."
"is the observation noise. we mention that in this case, all densities have infinite support so that, strictly speaking, the assumptions made in this paper are not satisfied. nevertheless, the above model can be slightly modified to be consistent with our assumptions. for instance, in order that the transition kernel of (15) strictly fits assumptions a1-a2, it is sufficient to replace the distribution n(0, 1) of u k with the corresponding truncated distribution on an arbitrarily large support. in order to simplify the presentation, we do not go into more details and keep model (14)- (15) with slight abuse. we compare different quantization strategies in terms of the error exponent loss d ζ :"
"mix effects can appear whenever we compare aggregated and nonaggregated data. mathematically, we can formalize one of the most common situations-that of weighted averages and sums-as follows."
"after the initial launch, we introduced the tool with a wider group of potential users via introductory presentations and workshops. we then reached out to known users for a second round of interviews and with a survey. the interviews included qualitative one-on-one in-person sessions with 3 analysts, and session with a group of 4 from another department. all interviewees provided their own data. additionally, 11 people replied to the survey, 9 who had input their own data and (1) and in most counties. the power-law type distribution of county size is typical: toggling between a linear and log-scale on the x-axis helps users explore this type of distribution. color is discrete, defined by change in value."
"next, we use this to construct an enhanced mimo gaussian bbc that reveals some kind of degradedness. for this purpose let σ 1 be a real symmetric matrix that satisfies"
theorem 2. the secrecy capacity region c bbc (s) of the mimo gaussian bbc with confidential messages under the matrix power constraint s is given by the set of all rate triples
"the \"head\" of the line segment is thicker than the tail, to create a sense of flow. initially, we were inspired by holten and wijk's a user study on visualizing directed edges in graphs [cit] . assuming the lessons from directed graphs might apply to disconnected directed lines, we started with the \"needle\" style taper they recommended. however, early user testing showed that the natural interpretation was inverted: people saw these disconnected shapes as \"comets\" or \"fish\" rather than needles. in response, we kept the taper but reversed the direction to the untested \"comet\" type taper."
"rule-based models can be encoded in the bionetgen language (bngl), 15 which is used by a number of software tools. [cit] 15, 16, 41, 42 this language is closely related to kappa, which is used by yet other software tools 44, 47 (http://kappalanguage.org). in the graphical formalism upon which bngl is based, 14, 15, 17, 48, 49 proteins and other molecules are represented using molecule type graphs, chemical species graphs and pattern graphs, which are called site graphs in kappa. 47 the vertices of these graphs represent components, the functional parts of proteins (e.g., domains, linear motifs, and sites of post-translational modification). the vertices representing components may be associated with variable attributes, referred to as internal states. an internal state is often a useful abstraction, which can be used to represent the conformation, location, or posttranslational modification status of a protein component. protein-protein and other molecular interactions are represented using graph-rewriting rules, which designate what is required of molecules for an interaction to occur and how molecular components are affected by an interaction/ transformation (see below). rules are associated with rate laws (functions of properties of reactants, typically including the population levels of reactants), which are used to assign rates to transformations defined by rules."
"imagine you're an economist analyzing us unemployment. [cit], you see that overall median weekly wages went up by 0.9%, after adjusting for inflation [cit] . a natural question is how different population segments fared. for instance, if you group the data by level of education, you'll presumably see some groups doing better than 0.9% and some doing worse. since this has obvious policy implications, you reanalyze the data and produce table 1 ."
"although median wages went down in each segment, something else happened as well: the number of jobs increased for higher educated groups and declined for lower. [cit] data. the summary number of +0.9% depends not just on the change within population segments, but on the change in the relative sizes of those segments. the counterintuitive fact that an aggregate measure can contra- dict all subpopulation measures is known as simpson's paradox. [cit] article in the new york times [cit] which contained a description of this data accompanied by multiple graphs. five days later, the times published a follow-up article because readers had noticed the discrepancy between the direction of change in wages overall and the direction of change in wages within subpopulations."
"unfortunately, treemaps for comparisons still can be subtly affected by changes in mix. consider a simple \"market map\" that portrays two companies with rectangles of equal area, one having gone up 25% and one down 25%. what has happened overall? in fact, the market has fallen by 6.25%: although the sizes of the companies are equal at the end of the time period, the company with the falling stock price had to have been bigger at the beginning of the time period. in other words, because treemaps for comparisons only show sizes based on the most recent data-that is, only half of the mix data-they subtly emphasize elements that have gone up in value, and thus have a systematic bias toward showing an increase in value."
"ubiquitin is a small protein that may be covalently coupled to copies of itself and to other proteins. ubiquitination (ub) tags proteins for degradation and serves various other functions. 84 fig . 8 visualization of ras regulation. hras, which has gtpase activity, is drawn with branched arrows pointing to gtp and gdp to indicate that hras contains a single binding site for the guanine nucleotides. the reaction arrow between gtp and gdp represents the transition from one bound state of gtpase to another. a catalysis arrow is drawn from the gtpase domain of hras to the gtp-to-gdp reaction arrow to represent the intrinsic catalytic activity of hras. the arrow is dotted to indicate that it represents a cis interaction (i.e., the gtpase acts on a gtp molecule that is bound to itself). a stimulation arrow is drawn from the gtpase-gap interaction arrow to the catalysis arrow to indicate gap-mediated upregulation of gtpase activity. exchange of gdp for gtp is represented with a pair of bent arrows, and an activation arrow indicates that sos1 stimulates exchange. representation of ubiquitination can be similar to representation of phosphorylation: a catalysis arrow can point from an enzyme to a substrate, where the type of modification ('ub' for ubiquitination) and the location of the modification are specified. however, unlike phosphorylation, multiple enzymes are involved in the ubiquitination process: an e1 activating enzyme, an e2 conjugating enzyme, and an e3 ligase. ubiquitin is bound to a cysteine residue in the active site of e1, transferred to the active site of e2, and then bound to the target substrate in a reaction catalyzed by e3. 86 representation of ubiquitination in an extended contact map may vary. a detailed representation of ubiquitination includes all three enzymes and the target substrate. arrowheads representing catalysis of covalent bond formation and cleavage can be used to implicitly represent transfer of ub from one protein to the next (fig. 7a) . these reactions result in transfer of ub, which can be alternatively represented with a transfer arrow, as depicted in fig. 7b . note that the arrowheads used for binding and transfer arrows are similar but distinct. see fig. 5 . further note that fig. 7a and b need not represent different models; the two diagrams could represent the same set of rules. in fig. 7a, the dotted arrow from e1 indicates that an e1 enzyme removes ubiquitin from itself, rather than from a second e1 molecule. a more specific representation of ubiquitination in the style of fig. 7a is shown in fig. 7c . in some cases, specific residues in ubiquitin or ubiquitin-like proteins may be of interest. in the example of fig. 7d, a specific glycine residue in the ubiquitin-like protein atg12 is shown to form covalent bonds with specific residues in atg7, atg5, and atg10. 87 in addition, activation arrows point from catalytic arrows to transfer arrows, which represent the sequential transfer of atg12 from atg7 to atg10 to atg5. the activation arrows, which emerge from dots on the catalytic arrows, are intended to indicate that enzyme-catalyzed cleavage and formation of the indicated covalent bonds serve to transfer atg12. dashed borders for the molecules containing atg12 indicate that these entities are divisible. note that fig. 7d illustrates how the styles of fig. 7a and b can be combined. lastly, it is worth mentioning that monoubuiqitination can be distinguished from polyubiquitination (i.e., formation of a ubiquitin chain) 86 in the label of a modification flag. for example, the label 'ub n k' can be used to represent a chain of n ubiquitin molecules."
"here the a i and b i are \"weights\" and the x i and y i are \"values\", corresponding to statistics for n different subpopulations or segments of an overall population. the two real-world situations producing p and q may be referred to as scenarios."
"written in this form, it's clear that the two quantities of interest, p and q, each depend on two dimensions: respectively, the individual values x i and y i and the \"mixes,\" or different weights, a i and b i . as a result, there are four relevant dimensions to the analysis."
"where the '@' symbol is used to indicate a compartmental location. nf-kb translocates from the cytoplasm to the nucleus when it is not bound to an inhibitor, ikb, which interacts with the rel homology domain (rhd) of nf-kb. 52, 53 note that inclusion of the component rhd (which denotes the rhd of nf-kb) in the rule of eqn (2) represents a contextual constraint on translocation. as indicated in the rule, translocation requires that this component be free (of ikb)."
"certain treemaps are safe from this problem, but still present difficulties when mix changes. in particular, some treemaps are explicitly designed to show changes in weights, using size to represent a dimension and color to show change in the same dimension. a wellknown example is the smartmoney map of the market [cit], in which size portrays market capitalization of companies and color shows the percent change in stock price (generally equivalent to change in market capitalization). this type of treemap, where color is linked to the change in the size dimension, is sometimes called a treemap for comparisons [cit] . it is immune to the purest form of simpson's paradox, since if the overall value of a sector goes down, one of the items must have fallen in value as well."
"a rule can be viewed as a coarse-grained representation of the kinetics of a class of (bio)chemical reactions. each reaction within a class involves a common reaction center and transformation, which can take place in multiple contexts, but is characterized by a common rate law as an approximation. if the transformations that occur in a system can be assumed to be independent of most aspects of molecular context, then a modeler can use rules to concisely and comprehensively capture the consequences of the interactions and obtain model predictions consistent with a traditional physicochemical model that is defined implicitly. thus, in the case of a wellmixed system, there exists a corresponding system of coupled ordinary differential equations (odes) that can in principle be derived from the set of rules. [cit] the granularity of a rule can be refined by adjusting the necessary and sufficient conditions that are required of reactants (i.e., the molecular context that must be satisfied for a reaction to occur). a modeler is free to control the coarseness of model assumptions. at the finest level, a rule uniquely specifies a single chemical reaction. thus, rule-based modeling can be viewed as a generalization of traditional modeling of (bio)chemical reaction kinetics."
"withñ 1a ∼ n (0, σ 1 ), while the channels for receiver 1b and 2 remain the same. since σ 1 σ 1, cf. (16), the covariance matrix of the noise for receiving m c for the enhanced bbc (20) is \"smaller\" than for the aligned bbc (14). hence, its secrecy capacity region is at least as large as of the aligned mimo gaussian bbc. moreover, (16) and (19) yield"
"the aim is to decide between probability measures p 0 and p 1 based on the observation of y 0:n, the state sequence x 0:n being unobserved. note that equation (3) implies that the distribution of the observation y k conditionnally to the state x k is identical under both p 0 and p 1 . roughly speaking, one can think of y k as a noisy version of a process x k to be detected, where the distortion (typically a measurement noise) does not depend on the hypothesis. we make the following assumptions."
"after some algebra, and using recent results on hmm [cit], we prove that evaluating these constants, and proceeding in the same way for the study of n 2 (k 0,n − k 0 ), we obtain the first and second points of theorem 2. from holder's inequality, it is straightforward to prove the third point."
"a priori, we don't know whether absolute or relative numbers will be most helpful in an analysis. there could actually be interactions between any of 6 quantities: values x i, y i, weights a i, b i, and relative ratios x i / y i and a i / b i . as a result, a tool designed for general exploration should provide a way to look at all six quantities at once to discover which combinations matter."
"for example, the chart for michigan (fig. 7) suggests a possible relationship between the size of the county and the increase or decrease in unemployment rates. a few large counties decreased their unemployment rate, as shown in orange on the right, while the unemployment rate increased in almost all of the small counties as shown in blue on the middle and left. using a log scale on the x-axis as shown, and selecting the appropriate filter and color scheme can make this more discoverable. obviously this type of relationship between county size and changing employment rates might have important policy or economic implications for the state and is likely worthy of further study."
"we believe that there is room for a portfolio of new techniques that can help designers and analysts alike. the \"comet\" visualization presented in this paper is an example: a straightforward modification of a scatterplot that displays the interaction between subpopulation sizes and values in a balanced view. it's designed for analysts who want to understand complex data. based on reactions from analysts at our company, we believe the technique has promise. future work could make it more valuable by enabling more exploration and more complex datasets."
"a third straightforward idea is to generalize a bar chart, with one rectangle for each subpopulation. the height of each bar might reflect the aggregate value of that population, and (unlike a traditional bar chart) the width of each bar could show the size of the subpopulation. unfortunately, this idea too is unwieldy in practice, due to the need to show two different subpopulation sizes and heights (one for each scenario). sketches that showed both at once were uniformly judged to be unintelligible, especially in the common cases where changes in size or value were relatively small compared to the absolute numbers."
"comet charts work best for certain types of data. the strong sense of motion conveyed by comets worked well for time-based comparisons, in fact, one county had experienced a manufacturing plant closure; the other two are geographically adjacent. aggregate is stable (2) . color is discrete, defined by change in value."
"all color schemes use blue/orange, so that users with red-green colorblindness can perceive the difference. more subtly, an orange/blue scheme does not have a strong good/bad connotation. since the comet chart may be used on many types of data, we did not want to mechanically assign a value judgment to positive and negative changes."
"in figure 2 [cit] . it clearly illustrates the crux of the famous berkeley discrimination case, but nowhere does the diagram show separate admission rates for women and men. this is ideal for the purpose of this particular explanation, but is inadequate for a general tool of analysis."
"proof. similarly as for the classical aligned mimo gaussian broadcast channel [cit] the proof of achievability is a straightforward extension of its discrete counterpart. to obtain the desired region (7) for the aligned mimo gaussian bbc we follow the proof of the discrete case restated in theorem 1, cf. also [cit], with a proper choice of auxiliary and input random variables. more precisely, with g ∼ n (0, q (c) ) and"
"abstract-we discuss how \"mix effects\" can surprise users of visualizations and potentially lead them to incorrect conclusions. this statistical issue (also known as \"omitted variable bias\" or, in extreme cases, as \"simpson's paradox\") is widespread and can affect any visualization in which the quantity of interest is an aggregated value such as a weighted sum or average. our first contribution is to document how mix effects can be a serious issue for visualizations, and we analyze how mix effects can cause problems in a variety of popular visualization techniques, from bar charts to treemaps. our second contribution is a new technique, the \"comet chart,\" that is meant to ameliorate some of these issues."
"in at least one case, the tool helped change a decision: an analyst explained \"i could see a striking change in mix, which invalidated high-level goal setting .... i needed to go back and set goals based on the current composition.\""
"the rule of eqn (1) specifies a reaction center, a set of components affected by a transformation. in the rule of eqn (1) and in the rules of appendix s1 (esiz), components in the reaction center are underlined. components that are included in a rule but that are not part of the reaction center are contextual. in eqn (1), the component u is contextual. the necessary and sufficient properties of reactants are specified on the left-hand side of eqn (1), which indicates that fceri, the high-affinity receptor for ige antibody (denoted rec here), interacts reversibly with the src-family protein tyrosine kinase lyn (lyn). the difference between the right-and left-hand sides of eqn (1) indicates that the interaction results from binding of the tyrosine-phosphorylated b chain of the receptor (bbp) to the sh2 domain (sh2) of lyn. the left-hand side of the rule indicates that the b component of rec must be in the p internal state (i.e., it must be phosphorylated) to bind sh2. furthermore, for a bond to form, the unique domain of lyn (u) must be unbound, which is indicated by including u in the rule without associating this component with a bond label. if the unique domain had no impact on the interaction, it would be omitted from the rule. a bond label is preceded by a '!' character. a bond, labeled '1,' is identified on the right-hand side of eqn (1). the '.' character on the right-hand side of eqn (1) is used to represent connectivity; here, it is redundant. the 'b' character precedes the name of an internal state of a component. finally, the rule indicates that the interaction is characterized by certain on-and off-rate constants (kpls, kmls). by convention, it is understood that the rate law associated with this rule has the form of that for an elementary reaction. non-elementary rate laws, such as the michaelis-menten rate law or a hill function, can be specified if desired. 13, 15 an additional feature provided by compartmental bngl (cbngl), not demonstrated in eqn (1), is the ability to explicitly represent compartments and trafficking of molecules between compartments. 51 for example, the following cbnglencoded rule represents translocation of the transcription factor nf-kb (denoted nfkb) from the cytoplasm (cyt) to the nucleus (nuc):"
"but in the following we show that for any achievable rate triple (r c, r 1, r 2 ) the weighted secrecy sum-capacity is bounded from above by"
"recall that a treemap represents two dimensions of a data set, using rectangle size to represent one dimension and color to represent another [cit] . rectangles in a treemap typically represent a tree structure through their arrangements into groups and subgroups. a central intuition behind treemaps is that the values of individual elements \"roll up\" to show the values of overall areas. that is, by looking at the colors of a set of individual rectangles, a user can get a sense of the overall trend of the population. unfortunately, it's possible for this intuition to be strikingly wrong. a treemap in which the size dimension is not directly related to the color dimension is subject to all the problems of mix effects seen above. in a typical example of such a treemap, rectangles might represent companies, size might represent market capitalization, and color could represent change in profits. due to mix effects, it's conceivable that all companies in a given market sector might show increased profits, even while profits have dropped for the sector as a wholesomething that would likely surprise a typical user. in a large treemap with many subgroups, the chances that there's a simpson's effect in one of the subgroups can be nontrivial."
"association. the extended contact map of fig. 4 demonstrates how direct physical interactions between protein binding partners (see arrows 1 and 2) and phosphorylation-dependent interactions (see arrows 5 and 6) can be represented. interactions that depend on other types of post-translational modifications can be represented in the same way as a phosphorylation-dependent interaction. a direct physical interaction between a protein and dna can be represented as shown in fig. 6a . a direct physical interaction between a protein and a small molecule can be represented as shown in panels d and e of fig. 6 . if two proteins are associated indirectly via an unknown linker, the boxes representing the proteins can be connected via a direct physical interaction arrow and the arrow can be attached to a note tag, a rectangle enclosing a reference to a note of explanation."
"one might hope for mathematical techniques that would resolve these issues, but unfortunately there is no statistical magic bullet. for example, adding control variables to regression analysis can potentially make estimates less accurate [cit] . a deeper issue is that disaggregation, even on all relevant variables, does not necessarily produce the correct interpretation in theory 1 [cit] or practice [cit] . the \"standardization\" approach, which tries to simulate \"apples-to-apples\" populations for comparison purposes [cit], is a technique commonly used 1 for example, arah warns that: \"it cannot be overemphasized that although these paradoxes reveal the perils of using statistical criteria to guide causal analysis, they hold neither the explanations of the phenomenon they depict nor the pointers on how to avoid them. the explanations and solutions lie in causal reasoning which relies on background knowledge, not statistical criteria.\" [cit] to adjust statistics shown in maps in order to control for the demographics [cit] . unfortunately this and related methods all have known shortcomings 2 [cit] ."
"that the aggregate is moving in a completely different direction from all segments may look strange-but that is exactly the point. it looks strange because simpson's paradox feels strange, and the chart forces the user to confront the underlying issues. color reinforces the positional change. in this example, the blue-orange color spectrum shows change in weight, reinforcing the horizontal shift and emphasizing the change in mix. blue shows increasing weight relative to the total, orange decreasing, and grey neutral."
"the next two examples seem quite similar to each other: both compare unemployment rates and the size of the labor force, segmented by education, for two different years [cit] . however, the two comet plots reveal different economic factors. the first (fig. 3) is much like our first idealized situation: all comets are flowing together. the second (fig. 4) is like the second idealized situation: the aggregate's change is clearly different from the segments. in both we see that the labor force is shifting towards education categories with lower unemployment rates. this shift affects how the segment-level changes in unemployment rate relate to the aggregate changes."
"below, we first provide an overview of the basic principles of an extended contact map and we then present several example visualizations. these examples serve to elaborate the concept of an extended contact map and to illustrate how various cell signaling processes can be visualized within the framework of an extended contact map. finally, we discuss the concept of a map guide, which can be associated with an extended contact map to document additional information about the molecules and molecular interactions visualized in the map, particularly the contextual dependencies of the interactions. a map guide can also be used to specify and annotate an executable rulebased model encompassing the molecules and interactions visualized in a map. the model specification may be partial or complete. if a guide serves to annotate a model, it can be referred to as a model guide. the conventions presented here can be used to visualize and annotate an existing model or to depict a set of interactions before they are formalized as rules."
"even without a simpson's-like effect, visualizing disaggregated data with the comet chart is still useful to highlight outliers, show when segments are behaving similarly or differently, suggest relationships between the same 6 key quantities, or reinforce/refute the base hypothesis that segments are changing similarly to the aggregate. this is especially helpful when there are many (more than 20) segments, which was typical of the data analyzed by our target users. to illustrate this, let's look at the year over year change in unemployment rates and labor force size for counties in four us states 3 [cit] ."
"as described above, proteins in an extended contact map are represented with nested boxes that correspond to hierarchical graphs, and sites of post-translational modifications are marked with modification flags. recommended box and flag glyphs are summarized in fig. 5 . the components of a protein are ordered from n-terminal to c-terminal. when this type of ordering is not possible, as with separate polypeptide chains in a multimeric protein, individual polypeptides may be arranged in a way that reflects their physical organization. for example, in the case of a multimeric cell-surface receptor (e.g., fceri), a mostly extracellular subunit (e.g., fceria) may be placed above other mostly cytoplasmic subunits (fcerib and g 2 ) ."
the above corollary provides the error exponent k n associated with the np test on quantized observations. a natural question is: how does the choice of the quantizer q n affect the detection performance ?
"but consistently confused users when there was no inherent order to the scenarios. for some users this was an inconvenience, while a dealbreaker for others."
"a map guide also serves to annotate the interactions represented by rules. each interaction arrow in an extended contact map corresponds to either a rule or a set of rules in which all rules contain a common reaction center. an interaction annotation, such as that shown in fig. 10c, has three parts: a summary of available information about an interaction, including citations from the primary literature; the rules used to model the interactions and/or to summarize the contextual dependencies of the interactions; and an explanation of the rules, including modeling assumptions. if a guide describes a fully specified model, rules will be associated with rate laws and estimates of parameters in the rate laws."
"as epros add to the volume and variety of data that are introduced into clinical care, it is imperative for epro reporting tools to exemplify best practices for visual design."
"successful adoption of technology requires seamless workflow alignment and integration to support the cognitive and physical work of clinical teams providing care. 19 the workshop explored how epro workflows often vary across local clinical settings and provided recommendations for how health systems can facilitate efficient and effective epro implementations. this includes (1) defining workflow actions for all roles, (2) designing workflows to improve data capture (ie, reduce missing data), (3) aligning epro workflows with existing clinical workflows, and (4) utilizing change management strategies. small group discussions allowed workshop participants to identify how stakeholders could support epro implementations at the system level where diverse use cases need to be considered. there were some similarities across all epros use cases (ie, preventive, specialty, and interventional care) and recommendations that apply globally. for example, all use cases emphasized the need to minimize data missingness, and workshop participants recommended using multiple data collection modalities to ensure complete data capture. however, participant feedback also highlighted how clinical user perspectives can vary across the three use cases, warranting tailored implementation strategies 20 to support training and adoption. for example, when epros were used for preventive care, workshop participants highlighted the importance of setting patient expectations for completing epro measures in preparation for every visit and setting provider expectations for appropriate responses to pro scores that indicate the need for clinical action. when pros were used for specialty and chronic care, workshop participants indicated that treatment plans are often individualized and consequently the need for epros (both content focus and cadence of deployment) will vary across stages of treatment and recovery. last, workshop participants noted that when used for interventional and surgical care, epros may require concerted efforts to educate patients and providers about how best to leverage epros to augment clinical decision making and outcomes assessment over time. while all use cases consistently reflected the need to have complete and efficient epro data collection, the workflows for how clinical teams responded to epro data varied significantly by use case. additionally, the readiness of stakeholders to adopt epros into practice is also influenced by factors such as organizational policies, culture, and the availability of resources. workshop participants therefore recognized that a \"one size fits all\" approach to epro workflow and training will not necessarily meet the needs of all stakeholders, further reinforcing the need to tailor implementation and training needs to local settings. as health systems increasingly collect data from patients, particularly outside of the clinical visit, they may need to adjust resources and policies to support data review and response workflows in new ways."
"epro stakeholders participating in the workshop echoed a fundamental premise throughout the day-adaptable hit systems are critical to balance the needs of large healthcare organizations with individual epro users. in order for epros to be successful at scale, governance, workflow, and informatics must all align to ensure epro tools are designed and deployed to provide actionable data at the right time to the right stakeholders. in this manner, health systems can integrate the patient's voice into care delivery and further advance patient-centered, personalized care."
"to recognize key nuances in epro deployment in clinical settings, workshop activities were organized around three common use cases to compare and contrast how epros can be used in different clinical contexts: (1) preventive care (eg, screening for depression), (2) specialty and chronic care (eg, managing chronic pain symptoms), and (3) interventional and surgical care (eg, assessing mobility after total joint surgery). although these use cases characterize common types of care decisions that are informed by pro data, pros are recognizably used in diverse clinical settings and for many different patient care purposes. 13 during the workshop, participants focused on each use case during problem-solving activities to assess the varied ways pros can support clinical care and decision making and to identify opportunities for standardization. structured notetaking templates were used to capture participant insights related to (1) system level challenges, (2) patient engagement, and (3) provider engagement. notes from small-and full-group discussions were compiled and analyzed using content analysis to synthesize recommendations and challenges that emerged throughout the day."
"14 the workshop addressed the human-centered design of health system epro tools, including (1) understanding the complexity of information needs across local (eg, clinical team) and system (eg, population health) stakeholders, (2) aligning epro reporting tools with clinical decision-making, and (3) identifying opportunities for hit to enhance the efficiency of epro capture and reporting. the workshop highlighted the importance of understanding the complexity of stakeholder information needs for epro reporting across such diverse goals as individual care decisions, population health monitoring, comparative performance assessment, and quality improvement needs. workshop participants identified additional opportunities for informatics and data science to enhance the effectiveness of epros use at scale. for example, increased use of standards for epro data storage, exchange, and score harmonization (ie, cross-walking epro scores from different measures to allow comparison within similar domains, such as quality of life or functional status) could facilitate the \"measure once, cut twice\" principle. the goal of employing these standards is to maximize the utility of epro data across the various reporting needs for clinical care, quality improvement, and population health. 18 additionally, as more robust epro datasets develop, this growing volume of epro data presents opportunities for algorithms to track and flag patients due for epros, and then predict and prompt appropriate clinical follow-up given epro responses and history. both of these examples could minimize the burden or potential duplication in epro data collection and further align epros with clinical decision-making processes."
"epros, if not governed thoughtfully at the health system level, may contribute to the onslaught of information that both patients and providers must manage as they try to personalize healthcare decisions. health systems need to establish policies that thoughtfully govern the selection and use of epro measures across clinical contexts and create expectations for the responsibilities of clinical teams in review and response (including medicolegal considerations). 14, 18 as such, the workshop reviewed key areas related to governance for developing repeatable and scalable models for the use of epros tools, including: (1) assessing stakeholder needs; (2) establishing governance structures and epro culture; (3) defining a pro measure selection strategy; and (4) understanding the capabilities and limitations of technical platforms. workshop participants highlighted that health system governance for epros is still an emerging practice and drew on individual experiences to articulate contextual factors that can impact the success of epros and inform a system-wide epro governance strategy. workshop participants identified that a critical function of governance is to provide leadership and communicate the value of epros. in addition to building a 'culture' for epros, governance structures create a platform through which continuous learning, feedback, and evaluation can take place. as local healthcare teams start to integrate epros into care delivery, governance can augment their work through the identification of opportunities for efficiencies or improvements at the system level. for example, governance teams could recognize the potential to synergize epro development efforts with ongoing patient engagement initiatives or address staff barriers to support epro workflows. most importantly, governance teams are well poised to evaluate and disseminate learnings across diverse implementations so that the health system at large can continuously improve. however, workshop participants recognized that no single governance model will serve all health systems. thus, participants articulated a need for research to describe the features or characteristics of governance models used to support epro implementation that best adapt and support goals for diverse healthcare settings."
"recent years have shown increasing interest in understanding how to best advance the use of epros. the pcori ehr-working group, the isoqol taskforce on implementing pros in clinical practice, ahrq technical expert panel on opportunities and challenges for pros and hit, and easi-pro pilot demonstration are just some examples of the concerted efforts to identify best practices that support the translation of epro tools into clinical care. [cit] yet, these efforts consistently cite unresolved barriers related to health system infrastructure, readiness of clinical users, and technical capabilities of ehr systems for epro use. these challenge areas were also echoed throughout our workshop presentations, discussions, and activities with specific feedback around: (1) leadership and governance, (2) workflow and human factors, and (3) informatics and data science. table 1 summarizes the recommended strategies presented during the workshop and persisting evidence gaps requiring further study that emerged from workshop discussion."
"as efforts to expand the use of epros grow, so does the need for collaborative forums where stakeholders and thought leaders can examine critical challenges that continue to prohibit the scale and spread of pros. these challenges include governance, informatics resourcing, data science approaches, and strategic resource allocation. such collaborative forums allow stakeholders from a variety of settings to share experiences with successes and failures, discover lessons learned, and identify common strategies as best practices that reflect the needs of diverse populations and clinical contexts. in this commentary, we report on proceedings from a half-day interactive workshop that focused on challenges and recommendations for integration of epros across health systems."
"epros have significant potential to facilitate more patient-centered, personalized care by aligning healthcare decisions with patient experiences, preferences, and voice. expanding epro integration in hit introduces both opportunities and challenges, and requires health systems to think strategically about the needs across the organization to ensure efficient design within shared resources and diverse needs."
"approximately 100 participants attended the workshop representing 43 unique settings involved in health research and practice, including international representation. workshop participants held a variety of clinical, administrative, academic, and government roles, with the majority reporting an average range of 1-3 years of experience implementing pros in clinical care settings. in addition to presentations on the four epro implementation dimensions, the workshop provided multiple opportunities for interactive discussion on challenges and opportunities for epros. throughout the workshop, participants engaged in conversation regarding epro best practices and shared their experiences with pro use in the field."
"patient-reported outcomes (pros) are a type of patient-generated data that provide clinically meaningful insight into screening, diagnosis, treatment response, and population health. 1 examples include improved recognition by clinical teams of chemotoxicity, 2 comparing treatment decisions for osteoarthritis, 3 and improved management of severe depression. 4 pros enhance the efficiency and patient-centeredness of clinical documentation 5 and facilitate individualized patient care, a key goal of precision medicine. traditional approaches to pro data collection focus on paper-based workflows, yet healthcare policy 6, 7 has prompted advancements in health information technology (hit) to promote patient engagement and interoperability across electronic health record (ehr) systems. in response to changing healthcare and policy environments, many health systems have prioritized the electronic capture and presentation of pros (epros), leveraging hit (eg, ehrs, patient portals, third party applications, smart on fhir) to enhance patient-centered, personalized care. however, epros have not necessarily been the silver bullet to scale the spread of pros in clinical care to date. the ability to administer pro surveys electronically resolves some challenges (eg, auto-reminders and distribution to patients to complete epros ahead of visits) and creates new opportunities for improving care delivery (eg, clinical and quality dashboards that present epros and clinical data collectively). 8 yet, epros can also amplify existing hit barriers (eg, low patient portal enrollment and limited functionalities of ehr systems) and introduce others, in particular information overload for clinical teams. [cit] although there is demonstrated value and increasing pressure to incorporate epros into clinical care, many health systems have met challenges when trying to bring epros to scale and balance the needs of individual users with the system at large. 13 this is due in part to the complex interplay of technology, workflow, and human factors that influence the success of epro adoption, as well as the leadership and governance that ensures the sustainability of epro implementations."
"inclusion criteria for participation in the studies were mostly school-related (e.g., 'majoring in math and science). other studies included a certain subgroup, including participants based on ability (e.g., low achievers), socioeconomic status (ses) or a certain health condition. twenty per cent did not specify inclusion criteria used for participants (fig. 1) ."
"while current infrastructures have difficulties in diversity of services and configurations, management of network communication for smart infrastructure becomes a bigger challenge; one of the main challenges is that existing networks and systems are generally not homogeneous and they are hardly able to communicate with each other. in addition, secured communications between systems is another significant challenge. what is needed is a well-established secure communication across different industrial domains to provide domain synergy. unification plays a significant role to foster reusability and interoperability in systems, and it reduces extra costs."
"denial of service (dos) and distributed dos attacks are historically considered as one of the major security threats and among the hardest security challenges. although there are lots of proposed defense mechanisms against them such as packet filtering or intrusion detection systems, they are making the headlines frequently and have become the hugest cyberattacks. in addition, they are improved and extended several times in different platforms, e.g. in case of mirai attack [cit] ."
"to defend against spoofing attacks such as ip (address) spoofing that exploits valid and authorized ip addresses, rina hides internal dif addresses (by f 3 ) and decouples port denial-of-service"
"things (iot) and more clearly, (from cisco) internet of every things (ioe). \"things\" including electronic devices, software, sensors, and actuators are connected and communicate with each other, enabling them to be deployed in a variety of environments and domains such as home and buildings, smart infrastructure, health, and mobility, i.e. in the whole society. being highly deployable in one hand, and the lack of security preservation in devices, with the potential risk of a large number of unsecured connected devices on the other hand, cause the concept of iot. the number of iot devices is predicted to reach 41 [cit] with an $8.9 trillion market [cit] . communication between iot devices through the \"smart, connected products\" needs a strong, secure, and scalable communication infrastructure."
"on the contrary, rina adopts the basic foundation of networking: \"networking is inter-process communication (ipc) and only ipc\" [cit] . it unifies networking and distributed computing: the network is a distributed application that provides ipc. moreover, it employs a secured layer with basic ipc mechanisms (i.e. necessary functionalities), and through a common api, the network administrator is allowed to arrange/stack these secured layers as needed recursively. each layer is called distributed ipc facilities (dif), and is able to be programmed through policies on-the-fly; policies determine how mechanisms could operate."
"in fig. 2, a sample arrangement of two layers of difs between two end-nodes and two routers is shown. an ipc process (ipcp) is an instance of the same code managing ipc in each layer at each node. the internal structure of every ipcp is the same, and it consists of the following mechanisms which operate at different timescales:"
"in recent years, there has been a growing interest in the potential of games as instructional tools in areas such as education, health and wellbeing, government, ngos, corporate, defense, marketing and communication [cit] . considering that the development and implementation of digital gamebased learning (dgbl) implies a substantial financial effort, there is an increasing need to determine the educational potential of dgbl in order to justify the investment [cit] . one major justification of this investment should be well-founded empirical evidence [cit] . while in recent years, there has been an increasing number of publications aimed at assessing the effectiveness of dgbl, there is still a lack of sound empirical evidence [cit] . the lack of an overarching methodology for effectiveness research on dgbl has led to the use of different outcome measures for assessing effectiveness [cit], varying methods of data collection [cit] and inconclusive or difficult to interpret results [cit] . moreover, questions have been raised regarding the validity of current effectiveness research on dgbl [cit] . a common methodology for assessing the effectiveness of dgbl would firstly create the opportunity to compare results and thus the quality of the different educational interventions across studies. secondly, claims regarding the effectiveness of dgbl could be made on a more general level. lastly, a common methodology could set a baseline for quality, which could serve as an evaluation tool for published studies and as a starting point for researchers desiring to conduct an effectiveness study on dgbl. the present study aims at mapping current research methods used for effectiveness research on dgbl and is a first part of a larger project aimed at the development of a standardized procedure for assessing the effectiveness of dgbl."
"referring to the table, dos attacks can be prevented/mitigated by f 5, f 10, f 11, and f 14 . this means that the attacker cannot be an outsider (f 5 ); he/she should join the dif first or create a new one. moreover, the preceding step in these attacks is flooding, but there is no listening port to be the target of such attacks (f 10 ). in addition, since connections in rina do not need to wait for explicit control messages to terminate (f 11 ), flooding attacks and their impacts are significantly mitigated. even for an insider, there are mandatory qos requirements that the flow should obey (f 14 ), and any deviation from that by the sender can result in dropping packets at routers."
"in sinkhole attacks, a compromised device tempts the others to use that them in a data routing process. in addition to secure routing protocols (provided by f 1, f 2 ) useful to prevent these attacks, the capability of having firewall functionality in routers, i.e. f 6 is helpful. this feature (f 6 ) can similarly prevent wormhole attacks as well. in addition, since performing some attacks such as dos is the prerequisite of the others to compromise a device, prevention of dos attacks can be used to prevent sinkhole attacks. however, to keep the table simple, we have mentioned the least features that can directly satisfy a security requirement, solve a challenge, or prevent an attack. for the rest of the network attacks in table i, it is shown how security features of rina can prevent/mitigate the attacks, which is straightforward, and hence, we skip discussing them."
"twenty per cent of the studies reviewed only implemented tests developed by the researchers and 24% used school tests or exams ('student achievement') as an accuracy measure. two studies (8%) used both test scores and student achievement as an accuracy measure. less than half (44%) implemented standardized tests, six of these (55%) only used standardized tests while 5 studies (45%) combined standardized tests with tests developed by the researchers. table 3 gives an overview of measures used in the studies. thirty-six per cent of the studies reported on how scoring on tests occurred. three studies (12%) included an independent coder, of which two controlled for inter-rater reliability. one study used several, non-independent coders to control for inter-rater reliability. in the pretest, teachers must indicate changes expected…in the post-test, teachers must identify positive and negative changes perceived in the dimensions indicated in the pretest… [19a] twenty-eight per cent did not report on the similarity between the pre-and post-test measurements. forty per cent employed the same test before and after the intervention, 8% changed the sequence of the questions and 8% used a similar test (e.g., other questions with the same type and difficulty levels). the latter did not report on how similarity of parallel tests was assessed. sixteen per cent used a dissimilar pre-and post-test, such as midterm exam scores and final exam scores. two studies also implemented a mid-test and for studies a follow-up test. assessing the lasting effect is, however, important considering that short-term interventions with a new medium can yield a novelty effect, overestimating the instructional value. different statistical techniques can be distinguished for quantifying learning outcomes. the larger part of the studies (76%) did a check on pre-existing differences between experimental and control group(s) and 36% of the studies included in this review reported on effect size. table 4 shows how analysis of tests occurred. 4 16 a 2 x 2 between-groups analysis of covariance (ancova) was conducted to assess the effectiveness of the interventions on students' computer memory knowledge. the independent variables were: (a) the type of intervention, which included two levels (gaming application, nongaming application), and (b) gender. the dependent variable consisted of scores on the posttest cmkt. students' scores on the pre-test cmkt served as a covariate in this analysis, to control for eventual pre-existing differences between the groups [15a] between groups comparison with repeated measures"
"authentication is decoupled from and performed before connection management in rina. this means that just insiders (authenticated ipcps in a dif) can attack. f 13 insiders resistance: rina uses a wider range of control field values (e.g. connection/qos id). given that an attacker can somehow compromise authentication or without the support of cryptography, rina's typical field lengths in packets are still long enough to make attacks harder to succeed, e.g. 2 48 possibilities to guess the connection information in rina compared with 2 29 possibilities in tcp during data transfer [cit] . f 14 qos: every connection in rina is established after the source represents its qos requirements which include maximum requested bandwidth [cit] . deviating from those, e.g. in dos attacks by congesting the network, can result in dropping its packets at the first routing node, which is some form of dos prevention. f 15 variable address space: every dif has its own address space, which could be smaller or larger, depending on the number of nodes in that dif. this saves more space in the packet header. hence, not only the addresses are not clear for attackers, but also the address length is not known which results in another obstacle in length attacks. f 16 mobility: mobility management in rina is smoothly performed since every ipcp at every dif can seamlessly join/leave difs without losing its name in its own dif. it just needs some local routing updates at lower difs, without any side-effects on security [cit] . in addition to mobility, rina can also improve multi-homing [cit] . f 17 resiliency: in each dif, (multi-path) routing is performed independently and transparently to the other difs. this means that each dif can provide resiliency services as well to the upper difs. in addition, this property provides \"transport over heterogeneous networks\" [cit] . f 18 performance improvements: in addition to the above security features, rina has some other important features which are all very appealing for iot networks [cit] . for example, through some research work 2 and international projects 3, it has been shown that rina can effectively improve the network performance in terms of throughput and delay without compromising security [cit] . f 19 complexity reduction: considering the number of protocols, required flows, and especially required distinct mechanisms, rina networks can satisfy security requirements with less complexity than in the current internet. moreover, the number of active instances of networking mechanisms is reasonably less complex in rina with a secured link layer [cit] . rina also reduces the size of routing tables [cit] . f 20 arbitrary arrangement: difs can be arranged/stacked arbitrarily to provide different operations such as peps, multipath routing, and in-network resource sharing without compromising security [cit] ."
"lastly, some issues have been raised on confounding elements by implementing the game in a larger program. empirical evidence on the possible impact of these elements in the context of dgbl research is, to the best of our knowledge, scarce. therefore, further research on the impact of several factors such as support by intermediaries, program elements and extra material provided, is required."
"educational evaluation aims at describing and explaining experiences of students and teachers and judging the effectiveness of education [cit] . two types of evaluation can be distinguished: formative and summative evaluation. formative evaluation aims at detecting areas for improvement, thus evaluating the process, whereas summative evaluation aims at determining to what extent an educational intervention was successful, thus judging its effectiveness [cit] . while summative evaluation can occur independently, formative evaluation cannot occur without a summative evaluation [cit] . educational evaluation is not the same as educational research which requires more rigorous standards of reliability and validity [cit] . educational research can be conducted in two ways: by using a naturalistic design, describing an ongoing process in its natural setting, mostly by using observations or by using an experimental design which evaluates the impact of an educational intervention on its desired learning outcomes. dgbl effectiveness research should thus strive for more rigorous standards of validity and reliability in order to be considered as educational research, which underlines the need for defining standards."
-delimiting: a mechanism for encoding service data units (sdus) coming from the upper layer/dif within pdus. fig. 2 . a sample rina topology with two end-nodes and two routers. every ipcp has the same internal structure.
"all studies implemented a quantitative research approach, 32% combined this with qualitative research such as observation, interviews and diaries. however, only 3 studies coded their qualitative data. all studies reviewed implemented an experimental design. all studies implemented a betweensubjects design, with the exception of one study that implemented a within-subjects design, where the game-based group also served as a control group (by implementing traditional classroom teaching before midterm exams and implementing the dgbl intervention before the final exams). forty-four per cent used a randomized controlled trial; 24% randomly assigned subjects while 20% randomly assigned classrooms to one of the conditions. twelve per cent did not randomly assign participants to experimental and control group(s), but 'matched' participants in groups based on certain characteristics such as previous test scores, and 44% did not specify on group assignment of participants (fig. 6 )."
"despite its maturity level, the development of security protocols for the internet is still in progress. likewise, in rina some aspects such as key exchange/management have been recently implemented, and the level of trust in management data, and integrity-protecting routing are also under active development. we consider further evaluation of these features as our future work."
"the trend towards connecting everything to each other and to the internet has raised serious security concerns. in this paper, we discussed some main architectural security issues of iot networks: security and privacy issues, security limitations of the current network stack and its employed protocols, domain synergy, future directions, and expectations from iot networks."
"operates on configuration objects, and layer management, -resource and flow allocation, -locating applications, -security management, access control, -enrollment, and authentication. referring to fig. 2, the dashed arrows show the path of data/message exchange between the two applications in nodes 1 and 2. every ipcp decides how to process a received pdu from the upper/lower layer; it can pass it to the lower layer, send it to the other ipcp if the dif is on the physical medium (i.e. it is a 1-dif), routes it to another ipcp in a lower dif if it knows where the destination is (e.g. the ipcps in 2-dif in the routers), or pass it upwards to the destination application (at node 2). therefore, there is a single type of layer with programmable functions, that repeats as many times as needed by network designers. it means that all layers provide the same mechanisms: instances or communication (flows) between two or more application instances, with certain characteristics (delay, loss, in-order-delivery, etc). however, the mechanisms are programmable/customizable through policies. in general, there are only 3 types of nodes in rina: hosts, interior 1 and border routers, and there is no need for middleboxes such as firewalls, nats, etc. because policies can customize the internal behavior of each ipcp (or dif), which consequently empowers nodes with any required functionality."
"one of the main issues in the design of the iot network stack is interoperability, i.e. how to guarantee that iot devices can communicate with existing internet applications and follow internet standards [cit] . this has made them adopt many existing protocols and apparently, inherit their vulnerabilities and design issues."
"for the application attacks such as phishing, authentication (f 5 ) and authorization (f 8 ) can be used for mitigation. the prevention of the other application attacks are also shown in the table. preserving the security requirements shown in the table will prevent some attacks. for example, confidentiality preservation can mitigate the impact of malicious scripts."
"a. how rina addresses security challenges rina approaches problems in a divide-and-conquer manner; it defines different scopes (difs) with their own security. every n-dif (including its ipcps) is responsible for its security; insider ipcps are all trusted/pre-authenticated. difs can be stacked arbitrarily without the need for developing any new layer/protocol. when a dif is secured, the same code is reused for upper layers. this mimics tunneling in the internet."
"c 2 is naturally solved by rina with dif recursiveness (we can arbitrarily arrange/stack secure difs) and programmability (difs can be customized via policies), and there is no need to develop new protocols as the same code (ipcp) is instantiated recursively 4 . c 3 is handled by creating difs to divide the widest scope into smaller, simpler scopes (difs) by f 2 with their own hidden and variable-length address space (f 3, f 15 ). c 4 is simply solved by the property that in rina, each dif is inherently a \"secured container\" (f 1 ), and difs can be arranged/stacked without compromising the security of each other (f 2, f 20 ). therefore, proxy operations do not interfere the security of other layers. c 5 is usually prevented by forcing users/ipcps to enroll themselves first in secure difs (f 1, f 5 ), and it is also more difficult for dif insiders to perform attacks (f 13 ). any future need, c 6, can be addressed through rina's programmability (f 7 ) which is inherent in the current rina architecture, and the other features such as qos (f 14 ), mobility (f 16 ), and resiliency (f 17 ) without compromising security. finally, domain synergy (c 7 ) can be simply handled by the recursion property of rina (f 2, f 20 ) and secure, common difs (f 1, f 4 ) without any side effects on the underlying security/performance, regardless of the beneath network environment (f 17 )."
"rina shows to be a promising network architecture that has shown significant improvements in many security and performance aspects. we briefly discussed rina modules and security features. by investigating rina for current iot attacks, security requirements and challenges in iot networks, we showed that rina has architectural solutions for each problem. in addition, it is programmable through policies which help extend its mechanisms. we believe that the recursiveness of rina and the natural security of each recursion enables us to build arbitrarily-large secure iot."
"the ntps scores were analyzed using a two-way mixed design anova, in which instructional treatment was a between-subject factor, while measurement occasion was a within-subject table 5 gives an overview of the main differences across studies regarding study design. these elements could serve as a foundation for the development of an overarching methodology for assessing effectiveness of dgbl, examining which elements and which ways of execution lead to more reliable and generalizing results on dgbl effectiveness."
"a classification of iot device attacks includes physical, network, software, and encryption attacks [cit] . physical attacks can be performed by short-distance attackers and a part of the countermeasure is to verify the device authentication [cit] . although rina is vulnerable to insider attacks, each device in the environment has to authenticate itself before communication which can prevent this kind of attacks. in addition, devices should employ an error detection system, and all of their information has to be encrypted to maintain data integrity and confidentiality, which is possible through dif programmability. for network attacks, authentication mechanisms and point-to-point encryption are proposed to ensure privacy of data and rooting security. again, rina authentication mechanisms in addition to the possibility of utilizing other security mechanisms such as encryption via sdu protection is suitable to defend against these attacks. since rina nodes (ipcps) also play the role of firewall, they can prevent illegal data access or harming the system against application and software-based attacks despite their vulnerabilities; this could be thought as a complementary defense in the presence of other tools such as anti-viruses. however, the last category which is called encryption attacks can be prevented as before by using other existing mechanisms in rina."
"the selection and coding of publications was conducted by one researcher, which can be considered a limitation of this study. this study is also limited to digital games aimed at cognitive learning outcomes. further research should thus be conducted on methodologies used in digital games aimed at skill acquisition and behavioral or attitudinal change. an interesting area for future research is exploring the possibilities for the development of an overarching methodology to measure effectiveness of dgbl. further research should therefore firstly focus on the development of an evaluation framework for assessing effectiveness of dgbl in order to develop a common methodology. to be able to develop this evaluation framework, a clear definition of effectiveness in the context of dgbl should be formulated. considering that there are a lot of stakeholders involved in this field (e.g., game designers, game researchers, adopters and governmental institutions providing funding), this definition should not solely be based on literature reviews, but should also include the conceptualization of effectiveness by these different stakeholders. moreover, both relevant stakeholders and experts in the methodology field (i.e., educational research and experimental methodology) should be involved in the development of a common methodology in order to find a balance between an ideal research design in terms of validity and what is practically possible."
"adopting rina, as a new protocol stack, does imply interoperability considerations which are now under investigation and implementation by some projects such as ocarina 6 that we are working on. as proposed by ocarina and also [cit], rina can be deployed as an overlay/underlay/alongside other networks including the internet; as an overlay, rina can operate on all phy, link, ip, and tcp/udp layers through its shim difs; as an underlay, it can seamlessly transmit, for example, tcp/ip traffic; and alongside other networks, through simple proxy ipcps, it has been shown how rina inter-operates with other network stacks. it has also been investigated how rina can operate on tiny, limited devices such as wireless sensors in the rinaisense project 7 ."
"the most implemented designs in dgbl effectiveness studies are quasi-experimental and survey design. a study of chen and o'neill [cit] has shown that in most empirical studies on dgbl effectiveness, no pre-test of knowledge is implemented. according to clark [cit] the absence of a pre-test of knowledge is problematic, because differences in learning outcomes could be due to knowledge differences between individuals or groups at the start of the intervention. consequently, this can lead to an overestimation of the instructional effect. moreover, when control groups are included in the studies, often no educational activity is implemented in the control group [cit] . according to hays [cit] the comparison to a control group, which does not receive an intervention or does not engage in educational exercises, is problematic in this type of research because, again, it might lead to an overestimation of the beneficial effects of dgbl. this is also supported by clark [cit] who states that one of the major motivations for the use of dgbl should be the justification of the investment made and should thus be compared to viable and less expensive alternative ways to teach the same knowledge and skills. according to clark [cit], this comparison should also be made on motivational aspects, and more specifically on motivation to learn through the game-based approach compared to other instructional programs. questionnaires are typically used to assess the motivational aspects of dgbl, gauging the motivations of participants for learning via the intervention received and their interest in participation [cit] . questions have been raised by several authors in the field about the validity of these measures [cit] considering student opinion on for example learning and motivation has previously been found to be unreliable and conflicting with direct measures [cit] . suggestions have been made towards physiological or behavioral measures (e.g., eye-tracking, skin conductance), because data can be collected during game play in a more controlled manner [cit] . furthermore, motivation as a construct in the context of dgbl effectiveness research needs to be further examined since questions can be raised on whether definitions of motivation in different studies truly represent motivation or other constructs [cit] . further, questionnaires are also implemented to assess other affective outcomes, such as attitudes [cit] . some studies use in-game assessment -referred to as stealth assessment -which is a technique that aims at accurately and dynamically measuring the player's progress, analyzing the player's competencies at various levels during game play [cit] . using technology, which strategies the player uses to solve certain problems can for instance be assessed in the game, giving the researcher information on the learner's progress [cit] . finally, qualitative methods such as interviews (e.g., attitudes before game play, player experiences after game play) and observation (e.g. behavioural performance after playing game, decision making and emotional reactions during game play) have also been used in the context of effectiveness studies of dgbl [cit] ."
"on the contrary to the other network stacks, rina recurses the same layer which is called dif. the lowest layer, shim dif, operates over any lower layer, which could be physical, or other protocols such as tcp or udp. difs are usually numbered from 1 (e.g. 1-dif, 2-dif) as the lowest, and n-dif refers to the current layer one focuses on."
"the software controller performs both online control for each decision epoch and offline control for a sequence of decision epochs. the offline control first constructs a dnn using previously collected data and the resultant weights of the dnn are sent to the hardware dnn as parameters for online inference. the online control at each decision epoch k performs action selection and q value update, during which state-action pairs (s k, a) for each action a are sent to the hardware dnn for the calculation of q values q(s k, a) (i.e., dnn inference). q values calculated from the hardware dnn are then sent back to the software controller for use in action selection and q value update. after the online execution at a sequence of decision epochs, the offline control takes charge again to update dnn weights with training based on the newly updated q values."
"activation unit: the most popular activation functions used for deep neural networks are sigmoid, tanh, and rectified linear unit (relu). in this work, we select tanh due to its convenience for sc implementation and comparable effectiveness as relu and sigmoid [cit] . the tanh function can be easily implemented with a k-state finite-state-machine (fsm) in the sc domain with significantly reduced hardware footprint compared to its conventional computing counterpart [cit] . figure 1 a zero if the current state is on the left half of the states, and a one otherwise. by this design, we have"
"as shown in equation (7), in practice we squash the original target q-value to the range [−1, 0] by first shrinking the original reward with a factor ρ and then clipping it if the target q-value estimate is smaller than −1. this can help speedup the training process by reducing the variance of q-value estimates."
"one major application scenario of drl is the embedded computing environment, such as in unmanned aerial vehicles, autonomous driving, robotics, wearable devices and mobile computing systems. however, dnns involved in the drl can be both compute and memory intensive. therefore, it is desirable to have dedicated hardware implementations (e.g., fpga, asic) for dnns in the drl for the embedded computing platforms, in order to utilize the distributed-computing and parallelism of hardware resources for enhanced computing speed, energy efficiency, and resiliency. stochastic computing (sc) [cit] as a low-cost substitute to the binary-based computing radically simplifies the hardware implementation of arithmetic units and has the potential to satisfy the low power and small hardware footprint requirements of dnns in the embedded computing environment."
"in this paper, we first present the general drl framework, which can be widely utilized in many applications with different optimization objectives, such as resource allocation, residential smart grid, embedded system power management, and autonomous control. followed by the introduction of three applications of the drl framework, one for the cloud computing resource allocation problem, one for the residential smart grid user-end task scheduling problem and one for building hvac system. the cloud computing resource allocation problem automatically and dynamically distributes resources (virtual machines or tasks) to servers by establishing efficient strategy. through extensive experimental simulations using google cluster traces [cit], the drl framework for cloud computing resource allocation achieves up to 54.1% energy saving compared with the baseline approach. the residential smart grid task scheduling problem determines the task scheduling and resource allocation with the goal of simultaneously maximizing the utilization of photovoltaic (pv) power generation and minimizing user's electricity cost. through extensive experimental simulations with realistic task modelings, the drl framework for residential smart grid task scheduling achieves up to 22.77% total energy cost reduction compared with the baseline algorithm. the building hvac system is designed for controlling a desired temperature within each zone with the factors of current zone temperature and outside environment disturbances. the proposed drl control algorithm can achieve 20%-70% cost reduction compared with the rule-based baseline control strategy, while maintaining the temperature violation rate below 1.0%."
"compared with conventional implementations in cmos circuits, stochastic computing (sc) enables low-power and smallhardware-footprint implementations of arithmetic units using standard logic elements [cit] . the sc paradigm significantly simplifies the hardware implementation and thereby allowing very high clock rates. in addition, it can provide a high degree of fault tolerance and an opportunity for trade-off between computating speed and accuracy even without changing the hardware implementation."
"[t k, t k+1 ) and the new state s k+1 at the next decision epoch; 22 22 store transition set 24 24 update q(s k, a k ) based on r k and max a q(s k+1, a ) based on q-learning updating rule. one could use a duplicate dnn q to achieve this goal; 26 26 update dnn weight set θ based on updated q-value estimates, in a mini-batch manner;"
"we simulate the control process using generated task sets and following a preliminary control policy. the state transition profile and q(s, a) value estimates are obtained through the simulation and used as the training data for offline dnn construction. we construct a three-layer artificial neural network with 26 hidden neurons, which is trained using the previously obtained training data. in the online phase, for each decision epoch k, according to the current system state s k, the action resulting in the maximum q(s k, a) estimate is selected using the -greedy policy. and q(s k, a) estimates are obtained by performing inference on the offline-trained neural network. based on the selected actions and observed rewards, q-value estimates are updated before the next decision epoch. at the end of one execution sequence, the neural network is updated for use in the next execution sequence."
"in conventional cmos circuits performing binary computing, a higher data precision will slow down the clock rate. however, in sc circuits the clock rate is now independent of data precision. in sc, a higher data precision is achieved by longer bit-streams, while the clock rate should be set to cover the operations in each pipeline stage on just 1-bit of data. to measure the performance of the sc pipelined architecture, we define delay as the bit-stream length times the clock cycle. in this way, the inverse of the delay is equivalent to the throughput of the pipelined architecture of the sc-based hardware dnn."
"additionally, as mentioned above, this paper investigates the stochatic computing (sc)-based hardware implementations of dnns used in drl using stochastic computing technique. to further enhance the performance (computing speed) and energy efficiency, pipelining techniques is employed in the scbased hardware design. the stochastic computing-based ultralow-power implementation consumes only 58771.53 μm 2 area and 7.73 mw power with 261.12 ns delay."
"in the cloud computing resource allocation problem, a server cluster consists of m physical servers that can provide p types of resources is considered. a first-come-first-served manner is deployed to process assigned jobs for the servers. a job will wait in the queue until sufficient resource is released in the server. we define the latency of a job as the actual duration from its arrival time to its complete time."
"from the above procedure, the drl can now handle extremely large state space (even infinite continuous state space) by using offline-trained and online-updated dnn. for the action space, it should be kept within a reasonable size, due to the necessity to enumerate the action space for action selection at a decision epoch."
"regardless of the length of bit-streams (i.e., precision), the multiplication unit is simply an xnor gate with two 1-bit inputs and one 1-bit output [cit] ."
"where stanh stands for the tanh function in sc domain. the k value represents the precision of stanh, and therefore higher accuracy can be achieved with a larger k value. we use a k value in the range of [−"
"this section demonstrates the effectiveness of our optimized hardware implementation. we adopted one drl network for the residential smart grid with one 26-neuron input layer, one 30-neuron hidden layer and one single-neuron output layer to implement the hardware application. therefore the input layer is consisted of 30 xnor gates for processing the inputs and weight, 30 26-input apcs and btanh as the activation function. the hidden layer mainly includes a 30-input apc. converters between stochastic and binary numbers are employed when processing the inputs and generating the outputs. table ii presents fixed network using conventional binary computing with the bit size ranging from 8 bits to 32 bits. it can be observed that the sc-based implementation can achieve a much smaller power and area cost compared with the binary-based hardware implementations. table iii shows the result of our proposed drl hardware implementation based on sc with the impact of pipelining. the bit stream length ranges from 256 to 1024. as showed in the table, the pipelined optimization can significantly reduce the delay, i.e. increase the system throughput, while maintaining small power and area cost."
"the building hvac system should be operated to maintain a desired temperature within each zone, based on current zone temperature and outside environment disturbances (e.g., ambient temperature and solar irradiance). the zone temperature at next time step is determined by the current system states, the environment disturbances, and the conditioned air input from the hvac system. we have developed a drl control algorithm to intelligently determine the optimal conditioned air flow input for each zone, for maintaining desired temperature while minimizing the total energy cost of the building hvac system [cit] ."
"in this paper, we first present the general drl framework, which can be widely utilized in many applications with different optimization objectives. this is followed by the introduction of three specific applications: the cloud computing resource allocation problem, the residential smart grid task scheduling problem, and building hvac system optimal control problem. the effectiveness of the drl technique in these three cyber-physical applications have been validated. finally, this paper investigates the stochastic computing-based hardware implementations of the drl framework, which consumes a significant improvement in area efficiency and power consumption compared with binary-based implementation counterparts."
"is maximized for each state s, where r is the reward rate, and γ and β are the discount rates. the value function v π (s) is the expected return when the environment starts in state s and follows policy π thereafter. eqn. (1) is for a discrete-time system, while eqn. (2) is for a continuous-time system."
"in order to significantly reduce the action space, we adopt a continuous-time and event-driven decision making mechanism [cit] in which each decision epoch coincides with the arrival time of a new job. in the offline phase, we harness the power of representation learning and weight sharing for dnn construction. specifically, we first employ an autoencoder to extract a lower-dimensional high-level representation of server group state for each possible server. the dimension difference reflects the relative importance of the targeting server group compared with other groups and results in reduction in the state space. next, for estimating the q-value of the action of allocating a job to servers in this group the neural network sub-q takes the server group state, job's state, all lowerdimensional high-level representations, and actions as input features. in addition, we introduce weight sharing among all autoencoders, as well as all sub-q's to reduce the total number of parameters and the training time. for the online phase, at the beginning of each decision epoch, the q value estimates are derived for each state-action pair by inference based on the offline trained dnn. an action is then selected for the current state using the -greedy policy. at the next decision epoch, q-value estimates are updated. after the execution of a whole control procedure, the dnn is updated in a mini-batch manner with the newly observed q-value estimates."
"the rest of this paper is organized as as follows. section 2 presents the related works on drl. in section 3, the general drl basics and framework are introduced. section 4 introduces three representative applications of drl, along with simulation results. in the following section 5, the hardware implementation of drl using the stochastic computing technique is presented. the corresponding experimental results are showed in section 6. the conclusion of this paper is presented in section 7."
"during the training process, our drl algorithm will try to maximum the reward function (5) for each zone. the first term measures the temperature violation in each zone, while the second term heuristically estimates the energy consumption cost contributed by each zone (which is assumed to be proportional to the air flow demand in each zone based on the total hvac system energy cost in the building cost(·))."
"we separately train a neural network for each zone by following the drl algorithm 1. each neural network is only responsible for approximating the q-value in one zone. at each control time step, all neural networks will receive the entire system states of buildings and then determine the control action for each zone separately. this heuristic can greatly improve the training efficiency by reducing the number of output units in the neural network."
"multiplication unit: the multiplication of two numbers represented by bit-streams (in bipolar encoding) can be calculated as logic xnor operation of the two bit-streams, as shown in figure 1 (a) . a brief derivation can be"
"reinforcement learning provides us a mathematical framework for learning or deriving strategies or policies that map situations (i.e., states) into actions with the goal of maximizing an accumulative reward [cit] . it has been widely applied for solving problems in different fields, such as manufacturing, finance sector, and robotic control systems. along with the resurgence of deep learning techniques, reinforcement learning has now evolved towards deep reinforcement learning (drl), where deep neural networks (dnns) are utilitzed in the policyderiving process [cit] . with offline-constructed and online-updated dnns, drl techniques demonstrate capabilities in handling complicated problems with high-dimensional state and action spaces and even enabling continuous action spaces [cit] . these features make drl distinguished from reinforcement learning. and recent breakthroughs in alpha go [cit] and playing atari [cit] indicate the great success of drl."
"the present research focuses on task scheduling of residential appliance operations to minimize an individual electricity user's cost in the smart grid factoring in photovoltaic (pv) power generation, due to the worldwide trend of transition to the smart grid and pv power usage in residential, industrial, and commercial sectors. in this work, we reduce users' electricity cost by applying the deep reinforcement learning framework for the user-end task scheduling in the smart grid equipped with distributed pv power generation devices under dynamic pricing."
"in this paper, we exploited time, location and periodicity information to effectively predict the user's next place through introducing the notion of the stp pattern and the application of gapped sequence mining. frequently-and periodically-observed visiting behaviors were recognized as stp patterns for a user, and the patterns were then used for representing the user's past visits as stp trajectories. subsequently, the extracted stp trajectories were further generalized to gstp trajectories to accommodate irregularities of visits, as well as to deal with exceptional stays."
"finally, a ros:capability might also use the message fields to identify how to parametrize the capability to achieve the desired behavior of the robot, in the case of a write capability, or which is the information carried by a message correlated to the specific capability."
"we set up our evaluation with the goal of showing how our ontologybased approach could indeed make robots more accessible to users without previous expertise in robotics or ros. to this end, we designed a user-interface wrapping our system, and asked nonexperts users to use this interface to instruct robots of different types to solve some high-level tasks. we measured indicators of the efforts required to achieve those tasks, as described below after an introduction to the interface."
"in the conveyor image, the number of edge and non-edge pixels are seriously unbalanced and about 90% are non-edge pixels. however, the traditional loss function does not balance the edge pixels and non-edge pixels reasonably. to solve this problem, this paper improves the traditional cross-entropy loss function and uses weighted cross-entropy loss to increase the weight of the edge pixels. the weighted cross-entropy loss can be expressed as:"
"a step towards this is the ros framework (robot operating system). 1 ros is a collaborative project developed with the aim to relieve developers from the management of low-level components. it has been considerably promoted by the robotics community in recent years, with many current robotic platforms (including commercial, low cost ones such as drones) having been developed directly with ros, or having been made compliant afterwards."
"unet is the cornerstone of medical image segmentation. in recent years, many effective methods in medical image field have been improved on unet. it consists of two parts: a feature extraction part that increases the receptive field to obtain context information, and an upsampling fusion structure that can achieve precision positioning. we use the dataset to train the unet network for 1000 epochs, and we save the best model to conduct experiments on the test images. then we post-process the result of unet by image erosion. the result images are shown in figure 7, column 5."
"besides, compared with unet, the deeplab series network is more popular in image segmentation tasks. deeplab v3 proposes a more general framework that replicates the last block of resnet and cascades it. an aspp (atrous spatial pyramid pooling) structure with a bn (batch normalization) layer is used to increase the receptive field and extract more abundant features without increasing the parameters. we train the deeplab v3 with the same training parameter settings and then test it. for the result of deeplab v3, we use the least square method for linear fitting. the results are presented in figure 8, column 6."
"besides the overall accuracy, we found out that the performance of the proposed methods significantly varied depending on the lifestyle of a subject. after the data collection experiment, we conducted a short survey asking about the regularity assessment for the subject's movements during the study period in terms of a 3-point likert scale. a score of 3 was reported by subjects 1-3, indicating that they managed highly regular life patterns. on the other hand, the score of subject 5 was 1, whereas the score of the rest was 2."
"from the scenario presented above, it is clear that the problem to tackle relates to the ability to program different robotic platforms to their full extent homogeneously and with reduced effort, i.e. without incurring into the time-expensive process of learning lowlevel ros programming for the specific set of robot architectures at hand. ontological representations have been used both as a mean to improve system interoperability and to provide meaningful, conceptual abstractions of complex and detailed domains. the question arising here is therefore: could an ontological representation of robot capabilities, able to abstract from the low-level implementation of the robot, improve the ability of non-experts to exploit such a robot to achieve specific tasks? it can be expected that answering this question positively would provide a way to facilitate the integration of robots in a larger variety of applications and environments, including smart-cities."
"a major difference of the above mentioned approaches with respect to fns is that the state variables of fns are real numbers. this implies that genuinely discrete systems cannot be modeled by fns. nevertheless, the use of real variables facilitates, in general, the use of more efficient analysis methods since the state explosion problem inherent to large discrete systems is avoided, and linear programming techniques can be applied. moreover, large discrete populations can be approximated reasonably well in many cases by means of real variables [cit] . with respect to the existing stochastic approaches and extensions, it should be said that they offer the possibility to perform useful statistical analyses, which usually require information about the probability distributions of the system, and often involve a significant computational cost. in contrast, fns do not require information about probability distributions, just about the intervals in which the uncertain parameters lay. this results in efficient analysis techniques based on linear programming."
"since our research was a part of a smart campus project that aims to study intelligent services facilitating better campus life, data collection was conducted only inside the snu campus, and all of the places considered were located within the campus. another reason for limiting the scope to the snu campus only was due to the availability of the wifi fingerprinting database required by the proposed approach. building a wifi fingerprinting database involves time-consuming tasks and is very costly, but only the database for the snu campus was available at the time of this research."
"the limitations described above could be overcome by integrating a middleware between the application and the robots, able to abstract from the technical implementation of the robotic platform, to enable access and exploitation of the robot's high-level capabilities (vision, movement, perception, actuation) in a simplified and homogeneous manner."
"the vertices of the net are connected by the edges in e s . each pair of vertices can be connected by at most one edge. the set e s is partitioned into two sets e t s and e p s, where e t s is a set of directed edges (or simply arcs) connecting transitions to intensity handlers and vice versa, and e p s is a set of undirected edges (or simply edges) connecting places and intensity handlers. thus, although both event handlers and intensity handlers are represented as dots, they can be easily distinguished by the arcs and edges that connect them to transitions and places. more formally:"
the number of tokens in a place p i is equal to the number of its idle tokens plus the number of its active tokens:
"as seen from figure 8, it is obvious that the conveyor edge detected by fcn-8s is a series of points, which can roughly describe the conveyor edge. however, there are many breakpoints and discontinuity, and the detection edge is blurred, which cannot meet the requirements of the task of"
"finally, it is necessary to be able to predict the next place for a user by utilizing only a small amount of observations available for the user, since collecting a user's mobile device log is usually a time-consuming and costly task. accordingly, the methods, such as rule mining and decision tree, that require a significant amount of history information for prediction do not appear to be a viable option when time and cost are an issue."
"similarly to event nets, an intensity net is a graph with three different types of vertices: places, transitions and intensity handlers. places and transitions have the same role as in event nets. each intensity handler connects a set of places to a set of transitions, and is associated with a set of linear inequalities that relates the number of tokens with the speed of the transitions, i.e. of the processes modeled by the transitions. the intensity, or speed, of a transition determines the rate at which actions are created in the transition. as in event nets, any solution of the inequalities can be used to determine the speed of transition, thus, linear inequalities can be used to model uncertainties in the dynamics of the system. in fig. 1, the intensity net is composed by the places, transitions, intensity handlers and arcs and edges in blue."
"the pytorch framework is used to implement this multi-scale feature fusion network, and the model is trained on nvidia geforce 1080ti gpu with a batch of 4 for 1000 epochs. the adam optimizer is used with an initial learning rate of 0.001. the batchnormlayer decay factor is set to 0.9 for the exponential moving average. the loss function is weighted with the cross-entropy loss described above, and the weight coefficient of the positive sample is set to 3 through several experiments. after about 3 hours of training with a five-fold cross-validation, the best model is saved and evaluated on 300 test images."
"notice that v 1 is not connected to any transition. this means that no process is required to move a token from p 1 to p 2 . for the sake of mathematical notation, it can be assumed that v 1 is connected to a fake transition, t f ake, that has no effect on the model. the matrices a 1, b 1, a 2 and b 2 that capture the inequalities associated with the event handlers are:"
"we argue that this is a waste of the potential of current robotic platforms. indeed, as support for smart cities for example, robots could be integrated in a number of applications such as parking monitoring, building surveillance or garbage collection. without the expertise of dedicated robot developers, however, the implementation of such scenarios is only limited to using the capabilities allowed by the commercial platforms (e.g. using a remote-controlled drone to record photos or videos), while more advanced usages, supported in principle by those systems, are not actually achievable (e.g. programming the drone to autonomously survey an area through several of its sensors)."
"these metrics give an estimate of the effort required by a ros developer to solve the specified tasks. lines of code set a lower bound for the implementation time, while number of components and messages outline the complexity of the solution."
"as in the event and intensity nets, constraints (10) and (17) can be added to (20) to force the execution of actions and the activity of places."
"this section introduces event nets, which can be denoted as t v p nets, i.e. actions in transitions t produce and consume tokens in places p through event handlers v . event handlers connect places and transitions, and determine the marking changes according to the actions in the transitions. in contrast to petri nets, the net elements that produce changes in the marking are the event handlers, and such changes are allowed to be nondeterministic."
"when the level of geographical granularity for prediction comes into consideration, a more precise level is desired to enable further sophisticated services. through discovering the next places at the level of users' daily lives, such as local shops and school cafeterias, various customized applications can be enabled, including recommendation of tailored information, such as automated reservation and personalized advertisements [cit] ."
"in this paper, we consider a weekly periodicity for extracting stp patterns of a user, as most people have weekly visiting patterns [cit] . extraction of the stp patterns from st trajectories consists of three steps: grouping st trajectories based on weekly periodicities, computing the probabilities of a stay according to its periodicity group membership and generating stp patterns from the probabilities."
"a series of experiments are carried out to verify the scientificity of the weighted loss function. the experiment results are shown in figure 9 . the left represents the influence of the weight coefficient (w) on pixel accuracy, while the right shows a relationship between w and the testing accuracy. when setting w to 3, it can achieve the best pixel accuracy and testing accuracy."
"an fn is the result of combining an event net and an intensity net, i.e. an fn is a graph with four types of vertices: places, transitions, event handlers and intensity handlers. while the intensity net establishes the speeds at which actions are generated as a function of the marking, the event net specifies how the generated actions are executed and how a new marking is computed. thus, although fns are inspired by petri nets [cit], their structure is different, since in addition to places and transitions, fns have handlers which are connected to places and transitions by arcs and edges. fns offer both high modeling power and appealing analysis possibilities that aim to make use of all the information provided by the available uncertain parameters. namely, fns can accommodate uncertainties in the initial marking, in the marking change produced by the firing of the transitions, in the default speeds of transitions, and in the speed of transitions produced by the marking."
"this property suggests that (6a) is equivalent to a power synthesis problem if and only if the phase of the array pattern equals the phase of the desired pattern. hence, by considering (4)- (7), the original problem in (3) can be iteratively solved by minimizing, at the k-th iteration, the constrained function:"
"in this section, the results of fcn-8s, the proposed method, unet and deeplab v3 in pixel accuracy, testing accuracy, testing speed and training time are compared. as can be seen from table 1, our method achieves the best results in terms of pixel accuracy and testing accuracy. fcn-8s cannot measure the testing accuracy for the serious breakpoint, while the result of unet is not fine. deeplab v3 has a longer training time due to the more complex network. considering the inference time, the proposed method significantly outperforms the others by reducing the parameters of the depthwise separable convolution significantly, while the testing speed of fcn-8s, unet and deeplab v3 are too slow to meet the requirements of real-time detection in an industrial field. in a few words, our approach has the following advantages compared with other methods:"
"the service is offered through an instance of the ros:service class, which specifies the ros:mode of figure 1(a) . a minimal representation of a service exchanging the map between move_base and the map_server node would, for instance, be as follows: components capabilities. in figure 2 we show how ros components are mapped to ros:capabilities. the main insight here is that high-level capabilities are implemented in the robot through a set of ros nodes, messages, topics and services, which we can map and represent in a knowledge base to infer what the robot is able to do."
"algorithm for stp trajectory construction. in line 2, e s is added to s to represent the start of s, and in line 6, stay t i, with the smallest t s is selected to traverse in the ascending order of time and is removed from t i in the next line. from line 8 to 14, the algorithm attempts to find the matching patterns for t i, based on t-sim() among the candidate stp patterns by traversing the patterns in π d one by one in the chronological order of τ s ."
"through the experimentation based on a real-world dataset collected from eight people, it was found that the proposed methods outperform the conventional methods based on the markov chain in terms of prediction accuracy."
"in addition, the way messages are exchanged can also be exploited to identify capabilities, e.g. a publisher of a pose message identifies the current position of the robot, therefore evoking the ability of sensing itself (self perception), while a subscriber of pose expects a desired position to be send to the robot, evoking the ability of autonomous navigation. in order to represent how different capabilities can be evoked by a read or a published message, we use triples of the form ⟨ros:capability,ros:hasmodality,ros:modality⟩. a modality is either read or write, representing respectively capabilities giving information about the robot (e.g. sensor data) and capabilities expecting some inputs (e.g. navigation). in the remainder of this paper, we will refer to these as read capabilities and write capabilities."
"the pytorch framework is used to implement this multi-scale feature fusion network, and the model is trained on nvidia geforce 1080ti gpu with a batch of 4 for 1000 epochs. the adam optimizer is used with an initial learning rate of 0.001. the batchnormlayer decay factor is set to 0.9 for the exponential moving average. the loss function is weighted with the cross-entropy loss described above, and the weight coefficient of the positive sample is set to 3 through several experiments. after about 3 h of training with a five-fold cross-validation, the best model is saved and evaluated on 300 test images."
"to enrich the training data, the following data augmentation techniques are adopted: (i) flip horizontal; (ii) flip vertical. in the training process, the training data are processed by the feature extraction module for image edge feature extraction, and parameters are updated with backpropagation. in the test process, parameters of the best model obtained by the cross validation method are used to predict the conveyor belt edge image on test data. finally, the deviation detection module judges the deviation by calculating the conveyor offset."
"formally, the evaluation metrics for the proposed strategy are pixel accuracy (pa) and testing accuracy. in this paper, pa represents the ratio of the number of edge pixels predicted correctly to the total number of edge pixels in the label, while the testing accuracy is the product of camera resolution and average pixel width. lower testing accuracy represents a finer predicted edge. the camera resolution is measured as follows:"
"specifically, st trajectories are grouped according to the day of the week contained in t i to accommodate the weekly periodicity of user's movements. for each group of st trajectories, the probability of a stay is computed through examining whether or not a user has visited a place at a specific time based on the arrival and departure time. then, stp patterns are extracted by finding time segments that exceed a certain threshold in terms of the probability. detailed description of each step is given in the following."
"a number of different formalisms can be found in the literature that can, to some extend, incorporate uncertain parameters in their models. depending on the domain of their state variables, these formalisms can be roughly classified as those whose variables are integer numbers, and those whose variables are real numbers (hybrid approaches combine both types of variables)."
"the final step consists of using the defined ontology to create a system that can automatically extract high level capabilities by inspecting the ros setup of a robot. the system's architecture, shown in figure 3, relies on the following components:"
"and no event handler has fired. this corresponds to the state ( the graph in fig. 2b shows the potential evolutions of the net under the assumption that event handlers fire in discrete amounts, i.e. all markings and actions are integers. the components of the vectors of states in the graph correspond to the variables"
"all the trajectories in this paper have been obtained by the tool fnyzer (https:// bitbucket.org/julvez/fnyzer.git). this tool makes use of the modeling language pyomo [cit] and solvers, such as gurobi [cit] and cplex [cit], to solve the programming problems associated with the fns. the cpu time (intel i7, 2.00 ghz, 8 gib, ubuntu 14.04 lts) to solve one step of the mpc approach for the fns in figs. 6 and 10 was 1.81s and 5.93s respectively. the cpu time to solve the only programming problem associated with fig. 8 was 1.27s."
"among many sequential pattern mining algorithms that have been proposed in the past to discover frequent patterns from sequences, the gapped sequence mining algorithm has been known to provide satisfactory results in many applications [cit] . it extracts patterns with consideration of gap constraints when finding frequent subsequences to relax the consecutiveness requirement on the subsequences. we employ a gap-constrained sequential pattern mining algorithm, known as cspade (sequential pattern discovery using equivalence classes with constraints) [cit], to discover frequent subsequences from stp trajectories. it allows us to deal with irregular visits, as well as uncertainties in a mobile device log due to the presence of noisy data by using gap symbols. table 4a presents the result of applying the cspade algorithm to the stp trajectories in table 3b . the outputs of the cspade algorithm are frequent subsequences with gaps, as well as their confidence values, which are then used to generate gstp trajectories. the confidence of a sequence indicates the likelihood of occurrence of the last symbol in the sequence, given that all of the preceding symbols before the last one have been observed. table 3b, where s g denotes a set of sequences, each of which is an empty set or a sequence composed of events or patterns for representing a sequence of gap symbols. that is, subsequence of s 1, π 1, π 2, π 3, e f, can be obtained from π 1, s g, π 3, s g, e f by substituting the first s g with π 2 and the second s g with φ. similarly, the subsequence of s 3, π 1, π 3, e f, can be generated by replacing all of the s g 's with φ."
"the most popular modeling approaches in the continuous domain are based on differential equations [cit] . in particular, the relaxation of the integrality constraint in a discrete formalism usually leads to models that are governed by differential equations. for instance, the evolution of continuous petri nets [cit], which can be seen as a relaxation of petri nets [cit], is determined by a set of ordinary differential equations. another popular modeling formalism that can graphically represent systems in different domains and that can be easily converted to state space representation is bond graphs [cit] . however, it should be emphasised that a potential difficulty in the design of models based on differential equations is that exhaustive and accurate information about the system dynamics is required, i.e. uncertain parameters cannot be easily handled. note that the time trajectory of a system modeled by ordinary differential equations is continuous and deterministic. on the other hand, constraint-based models, which are popular in systems biology [cit], can incorporate uncertain dynamic information but their analysis capabilities are limited to the steady state."
"a final comment regarding the presented results concerns the mutual coupling effects, which, in the research field covered by the geometrical synthesis of antenna arrays, must be considered once the final positions have been estimated. in fact, the initial set of positions does not represent real elements, but only candidate ones. hence, when the selected elements change, also the coupling effects change. consequently, one should evaluate the coupling at each iteration. clearly, this is not a practical approach and hence the coupling effects may be considered at the end of the procedure. in this case, if the pattern distortion is not acceptable, the element excitations may be modified with any suitable algorithm for the synthesis of conformal arrays, as for example [49, 50, [cit] ."
"the publish/subscribe paradigm is represented as a class ros:ros topiccommunication in figure 1(b) a ros:servicecommunication is represented in figure 1 (c) as composed by the pair ⟨ros:requestmessage,ros:responsemessage⟩, respectively sent by instances of a ros:serviceclient and a ros:serviceserver."
"to make robots' capabilities available to users, we created a basic imperative programming language, in which the atomic blocks are invocations of the available robot capabilities (e.g. navigating to a certain coordinate). we included in this language also basic programming constructs, such as if-then-else, while-do and repeat statements, allowing the user to build condition-constrained behaviors. the parameters of a capability could also be used in the conditions, so to exploit any robot output to drive the program flow (e.g. moving forward until an artag is detected). such a language could be easily extended with common features of other existing languages (for loops, break and the use of variables); however, we considered this set of programming blocks sufficient for the purpose of our experiment. given a robot running on ros, the user was shown an interface, as in figure 4, including: (i) a left pane showing the robot capabilities as inferred by the ontology-based system, as well as the necessary control parameters; (ii) a right pane with the list of available programming blocks (right side) and (iii) a central pane for building the robot program, i.e. a sequence of blocks to be executed."
"the fns in fig. 8 models a dynamic system in which shared resources can be allocated to different production lines. such a net shows how the tokens of a given place can activate different processes (those places have several intensity edges) and can cooperate with active tokens of other places. namely, there are two types of resources, p a and p b, and three production lines, t 1, t 2 and t 3 . the production line associated with t 1 (t 2 ) uses the raw material modeled by the tokens in p 1 (p 3 ) and produces items modeled by the tokens in p 2 (p 4 ). the production line associated with t 3 produces tokens in p 5 and it is assumed that it requires no raw material (or equivalently, this raw material is inexhaustible). in order to operate, the production line associated with t 1 (t 3 ) requires the allocation of resources of type p a (p b ). the speed of these production lines, t 1 and t 3, is proportional to the number of tokens allocated fig. 8 fn modeling a resource allocation system with three production lines and two shared resources to them. the operation of production line t 2 requires the cooperation of both resources, p a and p b, i.e. tokens of both resources must synchronize in equal amounts to make t 2 work. the speed of t 2 is equal to the number of tokens of p a (or p b ) allocated to this production line."
"notation. throughout the paper the following notation is used: (·) t denotes the transpose operator, (·) * denotes the complex conjugate, j denotes the imaginary unit, and ∠x denotes the argument of x."
"where m low (r) and m up (r) are real positive functions representing, respectively, the lower and upper bound of the mask. the problem addressed in this paper is that of selecting, among the n candidate positions, the lowest number of array elements, their positions and excitations in such a way as to obtain a radiation pattern compliant with (2) . by consequence, the here addressed problem can be mathematically formulated in compact form as follows:"
"conveyor belt images are a special image data and differ hugely from natural images. in this paper, conveyor edge detection is transformed into pixel-level classification in image segmentation [cit] . fcns (fully convolutional networks) [cit], a classical full convolution network, replaced the last three full connection layers of vgg (visual geometry group) [cit] with convolution layers, which reduced parameters and kept the spatial information of the image. there are some advantages in fcns that are very suitable for processing conveyor images, so this paper takes the fcns as the basic architecture to study the deviation detection task of conveyor images. the resolution of the feature image is reduced because of several convolutions and poolings. fcns addresses this by adding links that combine the final prediction layer with lower layers with figure 1 . the overall architecture of the proposed algorithm. according to the specific task required of belt deviation detection, this paper develops an algorithm based on a multi-scale feature fusion network comprising of a feature extraction module and a deviation detection module. then, training data are fed into the feature extraction module and parameters are updated with backpropagation. in addition, parameters of the best model are used to predict results on test data. finally, the deviation detection module identifies and monitors the deviation fault."
"the event net of an fn is a graph with three different types of vertices: places, transitions and event handlers. while places and transitions are used to model the state and the processes of the system, respectively, event handlers are used to determine the quantities by which the state is changed when given processes occur. each place is associated with a state variable, and the value of that variable at a given instant is called marking, or number of tokens, in that place. similarly, each transition is associated with a process, and the number of times the process has taken place is the number of actions in the transition. each event handler connects a set of transitions to a set of places, and is associated with a set of linear inequalities that relates actions to marking. given a number of actions in the connected transitions, any solution of the set of linear inequalities can be used to update the number of tokens in the connected places. thus, the amount by which the marking changes is allowed to be nondeterministic. this feature of event nets allows the model to account for the different system evolutions that can arise as a consequence of uncertainty in the system. in fig. 1, the event net is composed of the places, transitions, event handlers and arcs and edges in green."
"inequality (3) guarantees that enough actions are available, (4) makes use of the matrices a k and b k to relate the number of executed actions to the marking changes, (5) guarantees that enough tokens are available in the input places to be consumed, (6) guarantees that the overall state change is not null. notice that the inequalities (4) allow the modeling of uncertainty in the marking changes produced by the execution of actions."
"exercise 4: object recognition. in the final exercise, users had to instruct the robot to perform autonomous navigation through several artags. in the s-variant, the robot had to patrol a set of rooms until an artag was detected. in the r-variant, the user had to implement a behavior for the drone to perform 3 different movement actions every time an artag was seen."
"a total of 14 users were involved in the evaluation, equally shared between the s-and the r-variant. those users were essentially members of our research lab with at least some basic programming skills (to make the exercise meaningful), but with no experience in either ros or robotics in general. as a starting point, users were first allowed to familiarize themselves with the interface, namely through clicking on the different sections to understand the general behavior of the tool. users were however prevented from reading the description of the capabilities. after this first step, they were asked to solve all four exercises one after the other. for every exercise, we measured the time from the end of the task description until the final execution of the program. table 2 shows the average time avg(t) required by the users to solve each exercise, along with the average number of programming blocks avg(pb) and the number of capabilities num(cap) required to solve the task."
"the net in fig. 4d models a choice in place p 1, i.e. each token in p 1 can be used either to produce an intensity within the interval [cit] in t 1, or synchronize with a token in p 2 to produce an intensity within the interval [cit] in t 2 ."
"the proposed framework constructs spatiotemporal (st) trajectories, each of which represents a sequence of stays in terms of place and time, from a limited amount of past visit data for each user. spatiotemporal-periodic (stp) patterns are then extracted from the user's st trajectories by the proposed stp extraction algorithm. the algorithm searches for stp patterns through considering both occurrence frequencies and associations with st trajectories with respect to time for effective recognition of irregular or new visits as stp patterns. in particular, we employ a smoothing function to deal with the noisy and missing data."
"first, mobile device logs are likely to contain much noise and missing data related to users' past visits, which is caused by various reasons, such as measurement errors, wireless connection problems or unpowered mobile devices. dealing with such noise and missing data is crucial, since they make it difficult to achieve accurate parameter estimation and rule generation, leading to unrealistic predictions of next places eventually. moreover, as the geographical granularity becomes finer, the impact of such error-prone data on the performance of a prediction method becomes more severe."
"formally, the evaluation metrics for the proposed strategy are pixel accuracy (pa) and testing accuracy. in this paper, pa represents the ratio of the number of edge pixels predicted correctly to the total number of edge pixels in the label, while the testing accuracy is the product of camera resolution and average pixel width. lower testing accuracy represents a finer predicted edge. the camera resolution is measured as follows:"
"accordingly, it appears that the proposed notion of stp trajectory facilitates accuracy enhancement through generalizing observations into patterns, as well as accommodating periodicities. in addition, incorporation of gaps into the pattern sequence by the gstp method was also successful for further increasing the accuracy. these together imply that the proposed framework was effective at predicting the user's next location."
"paper, conveyor edge detection is transformed into pixel-level classification in image segmentation [cit] . fcns (fully convolutional networks) [cit], a classical full convolution network, replaced the last three full connection layers of vgg (visual geometry group) [cit] with convolution layers, which reduced parameters and kept the spatial information of the image. there are some advantages in fcns that are very suitable for processing conveyor images, so this paper takes the fcns as the basic architecture to study the deviation detection task of conveyor images. the resolution of the feature image is reduced because of several convolutions and poolings. fcns addresses this by adding links that combine the final prediction layer with lower layers with"
"3) the analyzer, interfacing with the robot system to determine all the capabilities the robot processes. this component scans all the active topics in the robot system, and then associates to the relevant topics an appropriate capability based on the ontology based on the mappings described previously. 4) the server, acting as a bridge between the robot system implemented in ros and the outer world (namely, the users). the server invokes the analyzer when starting, in order to populate the knowledge base with the available capabilities and topics, and translates the high level capabilities to the correct ros topics every time it receives a request."
"this section introduces intensity nets, which can be denoted as p st nets, i.e. tokens in places p produce and consume intensities in transitions t through intensity handlers s. the intensity of a transition t j is the speed at which actions are produced in t j . in other words, the number of actions produced at t is given by the integral over time of the intensity of t j . intensity nets and event nets operate in a similar fashion. in fact, the changes in the intensities are produced by tokens in the intensity net in the same way that changes in the marking are produced by actions in the event net."
"where w 0 denotes the l 0 -norm of w. more precisely, w 0 counts the nonzero components of w, corresponding to the elements that will result physically active at the end of the synthesis process."
"a direct comparison with how the same users would have achieved the same tasks without the tool provided is not feasible and would turn out to be meaningless. however, it appears a straightforward assumption that, those users not being familiar with ros, the simple (in our tool) process to understand the different components of the robot, what they do and, crucially, how to invoke them, would require more than a few minutes by itself. ros is a complex framework, requiring hours of practice to master. in addition, analyzing the computational graph of the specific robot to understand which topics and services are being be used (i.e. what the tool does through the ontology) is far from an easy task. a number of ros nodes would need to be implemented from scratch to encapsulate the required functionalities, and managing the correct publishers and subscribers. lastly, the nodes would need to be deployed and integrated with the robot architecture. knowledge of specific packages (e.g. the move_base node for autonomous navigation) is also required by some of the exercises. in other words, while a direct comparison could not have been performed, there is little doubt that significantly more effort would have been required to enable our non-expert users to achieve the same results with ros, as it did with our tool."
"next, compared to the cases with coarse location granularity, there is a larger amount of irregular visits in the past history of a user in the considered problem, which makes it even more difficult for a prediction model to accurately identify the user's visiting patterns. for instance, various irregular visits, such as going shopping or to movies, are frequently found between the regular visits of going to and returning from work. as a result, if those irregular visits are simply ignored, prediction models often fail to capture the patterns hidden among them."
"subsequently, stp trajectories are built by mapping each st trajectory to an stp pattern that is most similar to the trajectory among the extracted stp patterns. basing on gapped sequence mining [cit], the proposed framework is able to identify user's sporadic visits in her/his daily life through constructing gapped stp (gstp) trajectories that allow gaps to accommodate irregular visits that cannot be specified in advance. the next place visited by a user is then predicted by the proposed prediction algorithm based on the user's current and recent visits. this paper is organized as follows: in section 2, the details of the proposed methods are described. in section 3, the data collection details and experimentation results are described. the conclusions are presented in section 4."
"the vertices of the net are connected by the edges in e v . each pair of vertices can be connected by at most one edge. the set e v is partitioned into two sets e p v and e t v, where e p v is a set of directed edges connecting places to event handlers and vice versa, and e t v is a set of undirected edges connecting transitions and event handlers. for simplicity, directed edges are referred as arcs, and undirected edges as edges. more formally:"
"the resolution of the feature image is reduced because of several convolutions and poolings. fcns addresses this by adding links that combine the final prediction layer with lower layers with finer strides. then, the stride 32, 16 or 8 predictions are upsampled back to the image, which corresponds to fcn-32s, fcn-16s and fcn-8s. considering that fcn-8s, which gets finer precision, only fuses the output of the last three stages of pooling by per-pixel adding, it still loses much detailed edge information for conveyor belt images. in this paper, a multi-scale feature fusion network, which builds on the fcns, is proposed for global and local feature extractions. as shown in figure 2, the output of each stage after pooling is upsampled by 2, 4, 8 and 16 times the original image. in addition, by changing the fusion strategy of adding pixels-by-pixels in fcns, the channel fusion method is used to fuse the low-level features with the rich location and detail information and the high-level features with stronger semantic information. therefore, the network can learn abundant multi-scale features and solve the problem of detail recovery in edge detection."
"capability taxonomy. based on the principles outlined above, we designed a basic taxonomy 11 including the mappings between ros components (especially messages) and specific capabilities. these were defined in a bottom-up fashion, by collecting the specific capabilities from the robotic platforms supported by ros 12 as well as from the most adopted ros libraries, then abstracting them iteratively onto capabilities of higher levels. due to space limitations, we only present here the three macro-classes we used in our work: sensing, which includes all the capabilities enabled by the robot sensors (vision, depth sensing, light sensing etc.); movement, comprising the activities related to changing position (tele-operation, bodypart movement, autonomous navigation etc.); and robotknowledge, which includes capabilities the robots might have to represent their surrounding environment (such as the map representation). while establishing the taxonomy of capabilities is naturally a constantly evolving work, we asked ros experts to verify that our current coverage is correct and reasonably complete."
"as shown in figure 8, the overall accuracy results of mc and mc-p were worse than those of stp and gstp. these poor performances yielded by the markov chain-based methods are due to their inability to address the irregularities of visits, which is the characteristic often observed in campus life. in particular, the performance results of mc-p imply that the periodicity alone cannot help with increasing the accuracy. figure 8 also shows that gstp slightly outperformed stp on average, while their performance variabilities barely differ. furthermore, it can be observed from figure 8 together with figures 6 and 7 that stp and gstp tend to provide more stable performances across the different days of the week than mc-p. the next places were far from being predictable for some subjects, owing to the high irregularity in visiting behaviors when the mc and mc-p were used, but the prediction performances were greatly improved for them when applying the proposed methods, stp and gstp. in particular, stp and gstp significantly outperformed mc and mc-p for subjects 3-5, as shown in figures 6 and 7 ."
"concerning (1), two main aspects should be taken into account. firstly, the geometry of the array, which may be linear, planar or even conformal, is usually specified by some shape/size constraints in the physical design. when the cs strategy is adopted, the application of these constraints allows one to identify the possible positions of the n elements as the candidate positions, while the final array will be composed by a very reduced number of elements, suitably chosen among these n candidates. the second aspect that should be considered is that also the pattern generated by (1) is always required to satisfy specific shape constraints, which can be properly modeled by a suitably defined mask. accordingly, the constraints on the pattern can, in principle, be expressed as:"
this operation enables to finally identify the iterative algorithm that solves the original synthesis problem formulated in (3) . the development of the algorithm is detailed in table 1 .
the remaining of the paper is organized as follows. section 2 formulates the addressed problem. section 3 presents the developed algorithm. section 4 discusses the numerical results. section 5 summarizes the most relevant conclusions.
"fns consist of two nets, an event net and an intensity net, that make an explicit distinction between the parts of the system involved in updating the marking in places, i.e. the event net, and the parts of the system involved in the determination of the speeds of transitions, i.e. the intensity net. both the event and the intensity net are tripartite graphs in which places and transitions are connected by event and intensity handlers, respectively. this way, handlers act as an intermediate layer between places and transitions, which results in a significant modeling power. for instance, a transition in an event net can consume tokens from different sets of places, and a place in an intensity net can regulate the speed of different transitions. the tripartite net structure of event and intensity nets has demonstrated to be useful to model partial observability and resource allocation. different types of system uncertainties can be accommodated by fns through sets of linear inequalities associated with places, transitions, event handlers and intensity handlers. namely, these inequalities allow the modeling of uncertainty in: a) the initial marking (8); b) the default intensities (15); c) the marking change produced by the execution of actions (4); and d) the intensity change produced by the active tokens (13). fns account for the potential system trajectories arising as a result of uncertainties by means of a set of constraints that represent necessary reachability conditions. the combination of these constraints with an objective function can be used to obtain a system trajectory that optimizes a given criterion. this approach was successfully used to compute trajectory bounds, for instance, in the presented linear system with uncertain parameters, or to obtain a control law in a system whose control action is modeled by a transition with uncertain default intensity."
"a series of experiments are carried out to verify the scientificity of the weighted loss function. the experiment results are shown in figure 9 . the left represents the influence of the weight coefficient ( w ) on pixel accuracy, while the right shows a relationship between w and the testing accuracy. when setting w to 3, it can achieve the best pixel accuracy and testing accuracy."
"a series of experiments are carried out to verify the scientificity of the weighted loss function. the experiment results are shown in figure 9 . the left represents the influence of the weight coefficient ( w ) on pixel accuracy, while the right shows a relationship between w and the testing accuracy. when setting w to 3, it can achieve the best pixel accuracy and testing accuracy."
"as a consequence, robotic systems are being approached by users with different backgrounds, who are often less interested in the low-level technological components making up the system (e.g. communication, synchronization, drivers) [cit], than they are in exploiting the capabilities offered by the robot at a higher level (e.g. autonomous navigation, vision, natural language generation). lacking the required technical knowledge to exploit such capabilities for their own purpose, these users are then restricted to using the system as an end-product, within the limits of usage anticipated by the robot's provider (e.g. remote control interfaces, and in some cases, restricted programming facilities/apis)."
"additional ros elements also play a role in the computational graph. being of less relevance to the purpose of this work, they have been omitted by our current implementation."
"in this section, we describe in detail how the proposed framework extracts gstp trajectories and predicts the user's next places through considering sequential, temporal and periodic characteristics of a mobile device log. figure 1 illustrates the overall training process of the proposed framework that consists of four steps to generate gstp trajectories. the training process proceeds as follows. first, an st trajectory, defined as a sequence of stays in which each stay is represented in terms of a place visited, and the arrival and departure time, as well as the day of week for the visit, is constructed from raw data. second, we extract stp patterns from st trajectories to capture periodic revisits by taking periodicity into consideration. the existence of an stp pattern for a user indicates that the user tends to periodically revisit a particular place at a specific time associated with the pattern."
"based on this example, establishing the mappings between a ros system and the capabilities offered by the corresponding robot (using the schema of figure 2 ) is equivalent to creating a set of rules whose premises are specific sets of ⟨ros:node,ros:message,ros:topic⟩, which allow to derive the presence of a capability. unfortunately, ros does not provide standards for naming topics or nodes. developers implement their own nodes and can choose a different name for the topic where the setpoints will be published. this lack of standardization makes it difficult to rely on topics and nodes to discriminate the robot capabilities in the ros computational graph. however, a major effort has been put into standardizing ros messages, and a wide variety of messages are now commonly used to represent most of the information used by ros-based systems. 10 messages have a clear semantics: for instance, geometry_msgs/pose will only provide information about the position of the robot in a 3d space. we can therefore encode this convention as a triple (rule) of the form ⟨ros:message,ros:evokes,ros:capability⟩."
"an increasing number of researchers have been paying attention to the detection of conveyor belt deviation, and it is a significant problem in coal mining [cit] . initial detection of conveyor belt deviation mainly relies on manual inspection, which is labor intensive and prone to errors and omissions. then, mechanical and photoelectric detection devices were introduced [cit] . mechanical detection is used to drive the linkage mechanism through the contact between the roller and belt, figure 1 . the overall architecture of the proposed algorithm. according to the specific task required of belt deviation detection, this paper develops an algorithm based on a multi-scale feature fusion network comprising of a feature extraction module and a deviation detection module. then, training data are fed into the feature extraction module and parameters are updated with backpropagation. in addition, parameters of the best model are used to predict results on test data. finally, the deviation detection module identifies and monitors the deviation fault."
s is constructed by replacing each stay in an st trajectory with the stp pattern that is most similar to the st trajectory while sequentially exploring each st trajectory in the ascending order of time. the similarity between a stay and an stp pattern is calculated based on overlap between the time segments of the stay and the pattern.
"ros however remains a low-level technical platform, providing a layer directly above the hardware components of a robot and enabling expert developers to more easily share and reuse specific modules. in other words, programming robots for high-level tasks with ros still requires a fine-grained understanding of the technical architecture of the robot, and advanced knowledge of robotics, computer programming, and of the ros framework itself. this particular issue is further emphasized considering that, while there are only a limited number of different types of high-level capabilities a robot might offer, there is a high variety of different low-level components to actually implement them."
"our work very much relates to the area of sensemaking, where meaningful representations are used to facilitate insight and subsequent intelligent actions [cit] . external knowledge in the form of ontologies has been used to achieve a wide range of tasks, such as semantic enrichment [cit], visualization [cit], text processing [cit], and enterprise knowledge management [cit] ."
"based on this survey result, it appears that the performance of the proposed method was satisfactory when a subject exhibited highly regular behaviors, leading to the average prediction accuracy of more than 0.7 for subjects 1 and 3. in contrast, when the visiting behavior of a subject was not very regular, the prediction performances of stp and gstp were low, as suggested by the results for subjects 6 and 8."
"exercise 3: condition-based halt. the third exercise required users to implement a sequence of actions with a termination condition. in the s-variant, the robot needed to perform a patrolling of three rooms of the office, stopping only once all the rooms had been visited at least twice. in the r-variant, the user had to instruct the drone to keep on turning on itself until an artag was seen, in which case it would land."
"the contour plot of the pattern derived using the proposed algorithm is shown in figure 6, while figure 7 and table 6 report the initial grid (red cross) with the finally active elements (blue circles) and the real excitations, respectively. for the correspondence between figure 7 and table 6, the active elements have been numbered starting from the x−axis counterclockwise from the outermost to the innermost ring. for readability reasons, figure 7 illustrates the numbers of the four elements that enable the reader to infer the order. the computational time required to achieve the here provided solution is higher with respect to the previous example (14,691 s, corresponding to eight iterations). this is due to the finer grid required to sample a so narrow beam in the u − v domain. this last example and the previous one prove that also 3d synthesis problems may be successfully dealt with by the conceived cs-based approach, considering both flat-top and pencil beam pattern requirements."
"as an additional point towards the validity of our claim that our tool reduces the effort required to exploit robots' capabilities and therefore make them more accessible, we asked an expert in robotics with extensive experience in ros to achieve the same task. once again, the objective here is not to compare the experts to the non-experts using two different frameworks, but to provide an intuitive understanding of the difficulty of realizing the tasks achieved by our users without our tool. in table 2, we therefore show for each task in each variant:"
let us introduce some of the basic features of fns by means of a simple chemical reaction network. assume that the reaction network is composed of the following two reactions:
"the pytorch framework is used to implement this multi-scale feature fusion network, and the model is trained on nvidia geforce 1080ti gpu with a batch of 4 for 1000 epochs. the adam optimizer is used with an initial learning rate of 0.001. the batchnormlayer decay factor is set to 0.9 for the exponential moving average. the loss function is weighted with the cross-entropy loss described above, and the weight coefficient of the positive sample is set to 3 through several experiments. after about 3 hours of training with a five-fold cross-validation, the best model is saved and evaluated on 300 test images."
"where f d (r) represents the desired main lobe function [cit] . now, it is worth to note that the usage of (6a) leads to a field synthesis problem and not to a power synthesis one. this implies that the available degrees of freedom are in part wasted to approximate the array phase pattern, which is usually not of interest. to overcome this issue, one can first recall the following property holding for two arbitrary complex numbers z 1 and z 2 :"
"the places, depicted as circles, model the different types of components or elements in the system, e.g. resources, products, items, etc. the transitions, depicted as rectangles, model the different types of operations, activities or processes in the system. such operations require time to be performed and have the potential to change, i.e. produce and consume, the amount of components, i.e. the marking. the event handlers, depicted as dots, model the different ways in which the transitions can change the marking."
"in this paper, we focus on simplifying the time-consuming process of configuring/programming a robot for specific tasks, and in enabling it for non-expert users of robotic platforms. our hypothesis is that this can be achieved through using an additional ontological representation of the high-level capabilities offered by a robotic platform on top of the existing ros middleware, as an abstraction and an intermediary to the actual technical realization of those capabilities within the system."
"although unet is helpful for precise localization in image segmentation after many times of up-sampling by bilinear interpolation and successive symmetric fusion, it cannot fully learn edge features because the positive and negative samples are imbalanced in the edge detection task in this paper, and the high-dimensional features are eliminated in the process of upward fusion layer by layer, which leads to rough detection results. deeplab v3 is generally used in complex image segmentation tasks with a pre-training model based on the resnet network and various scales of atrous convolution getting different sizes of field. the fusion of the multi-scale atrous convolution and the output after global average pooling is helpful for solving the problem of missing image edge details. however, for the simple edge detection dataset in this task, deeplab v3 is too complex, and atrous convolution cannot effectively extracts features for the conveyor belt occlusion part. as a result, no good experimental results can be obtained."
"when the camera is at an acute angle to the ground, as the model shown in figure 6, θ is the field of view of the camera, h is the camera installation height, β is the angle between the camera and the ground and l is the maximum distance that the camera can detect. the mathematical relationship between them is as follows:"
"to further explore the relationship between the regularity of movements and the prediction performance, we computed jaccard similarity [cit], which measures overlaps among the visited places by a subject for each day of the week, and employed it as a metric for assessing the regularity. figure 9 shows the result that contains 40 plots corresponding to five different days of the week for eight subjects and their resulting performances."
"besides, compared with unet, the deeplab series network is more popular in image segmentation tasks. deeplab v3 proposes a more general framework that replicates the last block of resnet and cascades it. an aspp (atrous spatial pyramid pooling) structure with a bn (batch normalization) layer is used to increase the receptive field and extract more abundant features without increasing the parameters. we train the deeplab v3 with the same training parameter settings and then test it. for the result of deeplab v3, we use the least square method for linear fitting. the results are presented in figure 8, column 6."
"as future work, we plan to apply our work to larger and more complex environments than a university campus, such as urban areas or travel sites with more participants, and to further enhance the proposed spatiotemporal-periodic patterns through developing more sophisticated similarity measures that can effectively accommodate diverse types of irregularities and the semantic meaning of places."
"it should be noted that solving convex quadratic programming problems is required by both methods above. given that the computational complexity required to solve such problems is polynomial, the proposed computational methods can be applied to large fns. the following subsections present some of the modeling, analysis and control capabilities of fns by modeling a linear system with uncertain parameters, a resource allocation system and a system with control actions."
"when the conveyor belt deviates, the deviation of the center line position of the two edges of the conveyor in the result and the label is used as the fault feature information. assuming (x ia, y ia ) is the coordinate of the starting point of the left edge in the result, while (x ib, y ib ) is the coordinate of the starting point of the right edge, and the abscissa of midpoint between two edge x a and x b can be expressed as:"
"a cs-based iterative procedure for the power synthesis of sparse arrays has been presented. the proposed algorithm relies on the solution of a sequence of socp problems with the aim of minimizing the number of radiators of an array composed by a large number of candidate elements. the constraints of the minimization problem have been formulated in such a way as to be convex, and to approximate a power pattern synthesis problem in the entire visible region. the constraints formulation, and, in particular, their iterative modification constitutes the original contribution, which allows one to improve the performance of the previously proposed cs-strategies for sparse array applications. different numerical examples involving linear and planar structures have been discussed, obtaining, in all cases, the matching of the pattern requirements, and, for the cases involving the comparison with the previous existing solutions, a lower final number of active elements."
"the fn in fig. 6 models a linear system with uncertain dynamics. more precisely, if we assume that all the tokens are forced to be active and all the actions to be executed, the rate at which the marking changes can be expressed as:"
"autonomous mobile agents and robotics in general are experiencing a new wave of interest thanks to the accelerating advancements in many areas, such as computer vision, artificial intelligence, data management and communication, sensors, and actuators [cit] . today, a considerable amount of efficient techniques for basic robotic tasks (perception, manipulation, navigation etc.), as well as hardware and software components, are available, along with an increasing number of cost-accessible robotic platforms in the market [cit] ."
"in this work, a real-time conveyor detection method based on the multi-scale feature fusion network is developed, which can realize remote on-line monitoring of conveyor belt operation and avoid the interference of a harsh and complex environment. the standard convolutions are replaced with the depthwise separable convolutions, which have 8x less computational cost. meanwhile, a new weighted loss function is designed to optimize the network, which improves the pixel accuracy from 53.32% to 78.92%. compared with the other three representative networks, the effectiveness of the proposed method is verified, while the processing speed of 13.4 fps can be achieved, and the real-time performance can meet the requirements of various production scenarios. the final test error is less than 3.2 mm, which meets the application requirements of the industrial field. in the future, we will continue to improve our algorithms for superior performance."
"motivated by the above remarks, we attempt to develop a novel framework that aims to predict a user's next place based on the user's past visiting behaviors through considering periodicity in addition to time and location. to address the three challenges mentioned above, the proposed framework maps the individual visit of a user to one of the visiting patterns by utilizing the pattern extraction algorithms and the pattern similarity function proposed in this research."
"there are two copies of resource type p a and one copy of resource type p b . assume that the goal is to compute how the resources must be allocated over time so that the objective functionm[p 2 ]+0.5m[p 4 ]+0.25m[p 5 ], wherem[p i ] denotes the average marking of p i, is maximized. in words, this objective function implies that the goal is to maximize the production of all items giving priority to the products of type p 2, then p 4 and finally p 5 ."
"while the previous work mainly focused on the problem of predicting the user's next locations in terms of cell ids [cit] or at the levels of intra-city or inter-cities [cit], the geographical granularity considered in this paper is at the level of people's daily lives (e.g., buildings). when applied to such a fine level of location granularity, the previous approaches suffer from one or more limitations due to the following unique characteristics of the next place prediction problem discussed in this paper."
"the comparison results for the proposed methods and the markov chain methods in terms of the accuracy metric are presented in figures 6 and 7, in which mc, mc-p, stp and gstp, respectively stand for (1) the markov chain method without periodicity consideration; (2) the markov chain method with periodicity consideration; (3) the prediction based on stp trajectories; and (4) the prediction based on gstp trajectories. while mc predicted the next locations by using all of the available st trajectory data without taking the day of week information into account, mc-p exploited the day of week information by selectively utilizing st trajectories grouped by weekly periodicities according to the day on which prediction was made. since mc prediction was performed on all of the trajectories in the training data, its accuracy results are the same across the day of the week, as shown in figures 6 and 7 ."
"this way, an fn is a continuous time model where time, denoted as τ, is the independent variable and all the state variables are nonnegative reals."
"conveyor belt images are a special image data and differ hugely from natural images. in this paper, conveyor edge detection is transformed into pixel-level classification in image segmentation [cit] . fcns (fully convolutional networks) [cit], a classical full convolution network, replaced the last three full connection layers of vgg (visual geometry group) [cit] with convolution layers, which reduced parameters and kept the spatial information of the image. there are some advantages in fcns that are very suitable for processing conveyor images, so this paper takes the fcns as the basic architecture to study the deviation detection task of conveyor images."
"as seen from figure 8, it is obvious that the conveyor edge detected by fcn-8s is a series of points, which can roughly describe the conveyor edge. however, there are many breakpoints and discontinuity, and the detection edge is blurred, which cannot meet the requirements of the task of conveyor belt edge detection. besides, the original unet result is rough with one end thick and the other thin, and the problem still exists after image erosion, which is that we cannot achieve the detection effect. in addition, the structure of deeplab v3 is complex with a long training time, while there are many breakpoints and rough edges in the result before post-processing. after linear fitting, the breakpoint problem is solved, but the rough edges still exist. we can see intuitively from the figure that in our method result, the conveyor edge is finer, the breakpoints are fewer and the boundary is clearer, which can reflect the conveyor belt edge well."
"step 3 updates the iteration, the pattern, the weights, and the phase of the mbr shape. this latter update constitutes the major novelty of the proposed approach."
"in this section, the experimental contents are discussed, and the proposed method with the traditional edge detection method and other deep learning methods are compared."
"to answer this question, we propose to develop a system able to understand the capabilities of a robot by relying on an ontology that abstracts from the capabilities of ros components, and then to use such a system to show how non-experts can access and instruct robots of different types and capabilities without previous experience in doing so. the main idea here is to abstract the ros layer using an ontological representation, where the ros components running on the robot are mapped onto capabilities of a higher level. for example, given a ros component connected to the robot motor and producing sensor data such as velocity or acceleration, we can derive that the robot is able to manage its speed, hence possessing the capability of movement."
"the time trajectories of the marking and the allocated resources are shown in fig. 9a and b respectively. four time periods with different resource allocations (or operation modes) can be distinguished in these figures. the first period, from time 0 to 1.25, allocates the two tokens of p a to s 1 . this gives a high yield in the production of the items in p 2 which has the highest priority. given that the two tokens of p a are used by s 1 during this first period, the token in p b cannot be used by s 2, and hence it is used by s 3 to produce the items in p 5, which has the lowest priority. during the second time period, from time 1.25 to 1.75, one token of p a is used by s 1, and the other token of p a is synchronized by s 2 with the token of p b to operate t 2 and produce the items in p 4, which has medium priority. as a result, the speed of t 1 (t 2 )(t 3 ) is 1(1)(0) during the second time period. at time 1.75, the marking of p 1 becomes 0, and hence, the active token of p a allocated to s 1 is released and becomes idle. thus, during the third period, from time 1.75 to 3.25, only t 2 is working. at time 3.25, the marking of p 3 becomes 0, and hence the two tokens of p a become idle. during the fourth period, from time 3.25 onward, only the token of p b is active, and is employed by s 3 to operate t 3 ."
"in this paper, we have shown how a layer of ontological knowledge can empower non-expert users to access robotic systems of different types and capabilities. we developed an ontology-based system for robot programming abstracting from the specific components of the robot operating system (ros), and showed how this allows non-experts to make robots achieve specific tasks without having any previous experience in robotics."
"owing to the recent exponential growth of location-aware services based on mobile devices, such as smart phones, smart watches and tablet pcs, predicting a user's next place becomes an important research topic in both academia and industry [cit] . this problem concentrates on predicting a place that will be visited by a user in advance before she/he arrives, on the basis of the user's past visiting behaviors inferred through utilizing sensors, such as global positioning system (gps) and wireless fidelity(wifi) sensor, that are commonly available in modern mobile devices."
"traditional edge detection is based on gray image, which detects the discontinuous gray scale of the image. where the gray value of a pixel transforms is the edge of the gray image. the canny algorithm and its variants are classical edge detection algorithms in the traditional machine learning field. we carry out the experiments with the canny algorithm and compare them with the results of the proposed method. as shown in figure 7a, all the edges in the image are detected with the canny algorithm. the covered part of the conveyor belt cannot be automatically repaired. however, it can be clearly seen from figure 7b, and the detection result of the proposed method has almost no breakpoint and avoids interference from other objects on the conveyor belt."
"in this section, we present the ontology-based system which extracts and abstracts the capabilities of robotic platforms. we start with a more detailed description of the ros framework."
"where j represents the pixel width of left edge and n − j represents the pixel width of right edge, then the abscissa of the center line of the left and right starting point are as follows:"
"as seen from figure 8, it is obvious that the conveyor edge detected by fcn-8s is a series of points, which can roughly describe the conveyor edge. however, there are many breakpoints and discontinuity, and the detection edge is blurred, which cannot meet the requirements of the task of"
"once understood the main principles of ros, the second step is to provide a formal representation of ros and its components. it is worth mentioning here that, although our ros representation is inspired by the event 8 and situation 9 ontology design patterns, we use the ontology as a mean to demonstrate our hypothesis, and it is therefore out of the scope of this work to assess its originality or robustness."
"is the desired mbr function updated, at the k-th iteration, according to the phase of the pattern synthesized at the (k − 1)-th iteration and to a suitable real function f 0 d (r) defining the mbr shape. notably, this latter formulation represents a second-order cone program (socp) problem, which can be solved by the use of freely available software routines, as, for example, the matlab-based cvx [cit] . it is also worth to observe that, although, at each iteration, (8a)-(8c) formally belongs to the class of field synthesis problems, the iterative modification in (9) of the constraint in (8b) actually leads to a power synthesis problem. importantly, note that it is this iterative modification of the mbr constraint that allows one to better exploit the degrees of freedom of the problem and thus to improve the final results. moreover, to refer all requirements in terms of mask specifications, one can define the functions identifying the accuracy and the mbr shape, respectively, as:"
"among several types of mobile devices, we adopted smartphones as data collection devices, since they are equipped with wifi sensors and frequently carried by users anywhere they go throughout their daily activities. for experimentation, we implemented an android mobile app that records the data pertaining to user's visits, such as timestamps and wifi signals, every minute. the mobile app was then distributed to eight students at seoul national university (snu), [cit] ."
"it consists of a collection of ros nodes and topics currently in execution on a specific platform. 2) the dynamic node, which dynamically creates publishers and subscribers using the information derived from the ontology. in the case of a request for a reading capability, the dynamic node creates a subscriber associated to the specific topic, and relays to the server the messages received from the robot. for writing capabilities, the parameters received from the server are structured in a message and then sent to the platform through an ad-hoc publisher."
where j m and k m are real matrices of appropriate size. note that the inequalities (8) (7) can be easily modified to take into account the relationships expressed by (8):
"the arcs are labeled with the event handler that is fired. remark that while the firing of v 1 produces a deterministic change (one token consumed from p 1 and one token produced in p 2 ), the state change produced by the firing of v 2 is nondeterministic (either one or two tokens can be produced in p 3 )."
"several directions can be taken as future work. we are looking into refining the taxonomy of robot capabilities, to allow both highlevel and fine-grained actions. in addition, we are also interested in testing our system with other types of robots, e.g. by employing robots with manipulators. another possible direction is to extend the system into a collaborative programming environment were multiple users can work together in programming a (set of) robots. finally, we intend to make the system reusable by providing the high-level capabilities in the form of public apis."
"formally, the evaluation metrics for the proposed strategy are pixel accuracy (pa) and testing accuracy. in this paper, pa represents the ratio of the number of edge pixels predicted correctly to the total number of edge pixels in the label, while the testing accuracy is the product of camera resolution and average pixel width. lower testing accuracy represents a finer predicted edge. the camera resolution is measured as follows:"
"when the camera is at an acute angle to the ground, as the model shown in figure 6, θ is the field of view of the camera, h is the camera installation height, β is the angle between the camera and the ground and l is the maximum distance that the camera can detect. the mathematical relationship between them is as follows:"
"ros is a collection of tools, libraries, and conventions that aim at simplifying the creation of complex and robust robot behaviors across a wide variety of robotic platforms. the main idea behind it is to free developers as much as possible from the burden of managing the communication between components, as well as promoting the decomposition of their functionalities. the core of any ros application is the computational graph, which consists of a network of all the data processes involved. main elements of this graph are nodes, messages, topics and services, described in the following."
"in an fn, the event net determines the way actions produce marking changes, and the intensity net determines the way tokens produce intensity changes. the inequalities associated with handlers allow the modeler to cover a range of relationships between \"actions and tokens\" and \"tokens and intensities\". thus, handlers can be seen as a flexible layer between places and transitions that offers the possibility to model uncertainties in both the way actions produce marking changes, and the way tokens produce intensity changes."
"the development of appropriate models is crucial for the design, analysis and control of dynamic systems. the usefulness of a model depends on both its capacity to capture the relevant features of the system, and its capacity for mathematical analysis. these capacities of the model largely rely on the adopted modeling formalism, i.e. on the set of modeling principles and rules that are used to build the model. the task of modeling is often hindered by the lack of detailed system information."
"all the exercises were successfully carried out by all users. as one can see, ex. 1 took slightly longer (especially in the s-variant), when compared with other more complex exercises. this can be attributed to the time users required to familiarize themselves with the capabilities of the robot they were working with, which they could not have known beforehand. the relatively high variance in the time taken for ex. 4 is due to this particular exercise having multiple solutions, some of which taking longer to implement than others. a key, straightforward conclusion from this table is that users of this tool, who had no experience of programming robots and no prior knowledge of the architecture of the robot they were manipulating, managed to successfully program such a robot to achieve tasks from the very simple ex. 1 to the more difficult ex. 4 in a matter of a few minutes. considering the inherent complexity of robot programming and of understanding not only what a robot can do (what capabilities it possesses), but also how to use it (how to invoke those capabilities), this can be considered a non-trivial achievement."
"mining operations require conveyor belts to move mined material, such as coal, from the working face over a long distance to the processing plant [cit] . as the main transportation equipment in the coal industry and other industries [cit], conveyor belts will deviate during operation because of uneven stress on the surface, which results in material spilling, property damage and even personnel injury. it has long been recognized that a belt condition monitoring system for early detection of unusual belt deviation is desirable. thus, it is essential to study the mechanism of conveyor belt deviation [cit], respond to the intelligent mine by focusing on monitoring and controlling [cit] and combine it with the new generation of information technology such as cloud computing, big data and artificial intelligence to propose a more intelligent method for conveyor belt deviation detection."
"milton keynes is a city in buckinghamshire, england, growing in attention not only for being an example of modern urban design, but also for being the fastest growing city in the uk (in terms of jobs, people and houses). the city engaged in a large \"future city\" program, at the center of which the mk:smart project 2, which has developed a state-of-the-art data acquisition and management infrastructure (the mk data hub 3 ) and an iot network with live sensors capturing many aspects of the functionalities of the city (energy and water consumption, transport data, satellite-acquired data, social and economic datasets, and crowdsourced data from social media or specialized applications). the mk data hub was built with the idea that a common facility to efficiently manage, integrate and re-deliver data from a variety of sources could be exploited by applications and services, reducing their development costs and enabling intelligent data management (mining, analytics, aggregation, alignment, linking) at the scale of the entire city. our current goal is to create applications where robots are also integrated, namely by developing scenarios where robots act both as data collectors [cit] and data consumers [cit] of the data hub. in a practical example, we are looking into exploiting teams of robots of different characteristics and capabilities (i.e. drones and ground robots) for the surveillance and maintenance of green spaces. the main problem we encountered here is that, without the required expertise especially in application development in ros, we can only exploit the few capabilities exposed by the drone interface (namely, tele-operated navigation and video recording), while more advanced abilities could have enabled our application to better exploit the drone (e.g. through programmed trajectories, or the ability to use the drone's sensors for surveying the area or even to perform object detection with artags 4 ). even with the required expertise in robotic application development and a sufficient understanding of the drone's technical architecture, the task of implementing an application accessing such capabilities at a low-level would still represent a major effort, even more so if needing to integrate several different robot architectures (different types of drones and ground robots)."
"in an event net, each place contains a number of tokens (or marking), and each transition contains a number of actions that represent the potential of the system to carry out the associated process. in contrast to tokens, actions require time to be produced (the production rate of actions is determined by the intensity net, see section 3). the state of an event net accounts not only for the marking and the number of actions, but also for the marking changes and the execution of actions:"
"next, in the stp trajectory construction step of figure 1, st trajectories are mapped into a sequence of the extracted stp patterns, named the stp trajectory, based on the similarity between an stp pattern and an element of an st trajectory. finally, gap-constrained sequential pattern mining is applied to the stp trajectories to construct a user's gstp trajectory that allows unobserved places in the user's stp trajectories. the generated gstp trajectories from the training process are then used for prediction of the next place when the user's most recent stp trajectory data are provided as test data. the detailed descriptions are presented in the following sections."
"algorithm 3 shows the detailed procedure for generating a set of stp trajectories from t d and π d, given a threshold for pattern similarity, θ . in algorithm 3, · is used for representing a sequence, and ⊕ denotes an operator for the concatenation of two sequences."
"we employ a wifi fingerprint-based localization method [cit] for extracting the places visited from a user's mobile device log. it is well known that this method has advantages over gps-based approaches when tracking and identifying people's movements in indoor environments, particularly in urban areas, and the method also provides several benefits in terms of energy efficiency, compared to the gps-based ones, as it utilizes wifi sensor data."
"unet is the cornerstone of medical image segmentation. in recent years, many effective methods in medical image field have been improved on unet. it consists of two parts: a feature extraction part that increases the receptive field to obtain context information, and an upsampling fusion structure that can achieve precision positioning. we use the dataset to train the unet network for 1000 epochs, and we save the best model to conduct experiments on the test images. then we post-process the result of unet by image erosion. the result images are shown in figure 7, column 5."
"when the camera is at an acute angle to the ground, as the model shown in figure 6, θ is the field of view of the camera, h is the camera installation height, β is the angle between the camera and the ground and l is the maximum distance that the camera can detect. the mathematical relationship between them is as follows:"
"as one can see, a ros:roscommunication consists in a ros:message routed via a ros:mode (consisting in a topic or a service communication) and a set of ros:communicationcomponents (publishers, subscribers, clients and servers), that instances of a ros:node use as a mean to communicate with each other. the class ros:package is used to identify the library to which the message belongs, since these are generally named in the form of /$package/$message. for example, the twist message published by move_base belongs to the package geometry_msgs."
"throughout the experiments, all participants were instructed to carry their mobile devices with them as much as possible to gather comprehensive data that can reflect their actual daily movements. the full dataset contains 714,448 wifi signal logs, and 19.85 wifi aps were detected on average for each observation. since the logs also include locations outside campus, only about 52 percent of logs were successfully mapped into meaningful places based on a localization method using the wifi fingerprinting database for campus buildings. furthermore, the first 42 days' logs out of 60 days were selected as training data for constructing the prediction model, and the rest was used for evaluating the model's performance. figure 4 shows an example of a subject's st trajectories retrieved from the collected data. in this figure, blocks of the same gray level indicate visits to the same place, and white backgrounds represent unknown locations. the horizontal axis corresponds to the time from 0:00 to 24:00 of a day, while the vertical axis represents the number of days from the beginning of the experimentation. that is, the horizontal block stands for the subject's stay at some place from the time at which the block begins until the time at which the block ends, and appearances of the blocks with the same gray level along with the vertical axis indicate that the subject visited the same place at similar time slots across the days. from figure 4, it can be observed that frequent revisits to the same place were usually made with weekly periodicities rather than daily due to the characteristics of campus life, and accordingly, we have extracted patterns based on the weekly periodicity. yet, there are many irregular or exceptional visits that can be attributed to noisy observations, errors during localization or participant's peculiarities, making the problem of next place prediction difficult. we address this difficulty by use of smoothing for constructing stp patterns and also by applying gapped sequence mining during the generation of gstp trajectories."
"as it can be immediately observed, the problem in (3) is in general nonlinear and nonconvex, thus extremely hard to solve. the cs approach is specifically useful for this kind of situations, but requires a proper reformulation of (3). to this aim, consider an iterative procedure, which, at the generic k-th iteration, solves a minimization problem whose objective function is given by the weighted l 1 -norm (or weighted manhattan distance) [cit] :"
"traditional edge detection is based on gray image, which detects the discontinuous gray scale of the image. where the gray value of a pixel transforms is the edge of the gray image. the canny algorithm and its variants are classical edge detection algorithms in the traditional machine learning field. we carry out the experiments with the canny algorithm and compare them with the results of the proposed method. as shown in figure 7a, all the edges in the image are detected with the canny algorithm. the covered part of the conveyor belt cannot be automatically repaired. however, it can be clearly seen from figure 7b, and the detection result of the proposed method has almost no breakpoint and avoids interference from other objects on the conveyor belt."
"to achieve this, the following steps were carried out: (i) studying and understanding the ros middleware, in order to be able to abstract the capabilities of ros components; (ii) formalizing the ros framework into an ontology, which generalizes from specific robotic platforms; and (iii) enclosing the ontology in a system to automatically understand a robot's capabilities."
"this comparison further show how programming a robot is made \"easier\" and, through abstracting capabilities from the technical aspects of their implementation, requires less complexity. our approach and the associated tool therefore represent a viable solution to enable non-expert users to exploit robots in ways that were before only accessible to expert ros programmers."
"the net in fig. 4c shows how the intensity of one transition, t 1, can be used to produce intensity in other transitions, t 2 and t 3 . for this net, the state equations (14) become"
"according to the above observations, this paper considers the synthesis of sparse antenna arrays for mmwave applications by developing an iterative algorithm based on the cs optimization. the algorithm is conceived to address the general case in which a fixed grid structure is not available, and so nonlinear problems characterized by complex formulations have to be managed, even when simple array geometries and not challenging radiation constraints are involved. in fact, the aim is to enable the 5g antenna designer to synthesize not only the array excitations (in amplitude and phase), but also the number and the location of the elements in the presence of requirements on the far-field pattern."
"the subjects were chosen in such a way that they have different majors; half of them are residents of a campus dormitory; and half of them take classes for more than 4 days a week, so that they can represent different campus lifestyles. as all of the participants were undergraduate students and the experiments were conducted during a semester, most activities they performed during the study period were related to typical campus life, including having a meal at a cafeteria, taking a class in a classroom, sleeping in a dormitory, doing homework in the library and doing exercise at a gym."
"four exercises of increasing difficulty were asked to be performed by each user, which corresponded to creating a program allowing a robot to achieve a specific task. in order to demonstrate that the ontology-based system could allow the abstraction of robot capabilities independently from the platform, we set up two variants of each exercise, a simulated one with a ground wheeled robot operating in an office environment (s-variant), and a real-world one with a drone flying in an indoor space (r-variant). table 1 presents the robot capabilities available in each setting 13 ."
the overall change in the state produced by several firings is the result of adding the changes produced by each firing. this leads to a set of equations that are satisfied by the states that can be reached from the initial state.
"the rest of the paper is organized as follows: section 2 introduces event nets and shows how a partially observable system can be modeled. intensity nets are presented in section 3. the combination of these nets leads to fns, which are defined in section 4. section 5 shows how fns can handle systems with uncertain parameters and analyze them. section 6 concludes the paper."
"similarly, the number of tokens in a place p i is equal to the initial number of tokens, which is denoted m 0 [p i ], minus the number of tokens consumed plus the number of tokens produced by the connected event handlers:"
"the fn in fig. 5 is composed of the event net in fig. 2a and the intensity net in fig. 4a . while the event net determines the marking changes produced by the firing of event handlers, the intensity net establishes the rate at which actions are created in t 1 . notice that the firing of v 2 implies the execution of actions in t 1, i.e. actions need to be produced in t 1 so that v 2 can fire. on the other hand, v 1 is not connected to any transitions and, thus, it can fire when there is a positive marking in p 1 . it should be noted that this is not equivalent to an immediate transition in petri nets, since the firing of t 2 is not forced to happen as soon as the marking of p 1 is positive, its firing can occur at any time at which the marking of p 1 is positive."
"on the other hand, stp is based on the stp trajectory data (e.g., table 3b ) for the prediction that was made by choosing the pattern or event that has the highest transition probability from a current pattern or event after computing the transition probabilities between patterns or events. finally, gstp trajectories (e.g., table 4b ) were used for predicting the next location with the gstp method."
"in order to demonstrate the effectiveness of the proposed framework, we have implemented two first-order markov chain-based methods that predict the next place by calculating the probabilities for all of the possible next places based on the transition probabilities among places and choosing the place with the highest probability. we remark that the same st trajectory data (like those in table 2) were used for both the proposed methods and the first-order markov chain methods to be fair with the presence of noisy data in the comparison."
"the user's movement pattern is represented as an stp trajectory that is generated from st trajectories by utilizing the extracted stp patterns for the user. we let s denote an stp trajectory. s is a sequence consisting of symbols, each of which corresponds to an stp pattern or event. it starts with event e s and ends with event e f, respectively indicating the start and finish of s. the set of s's generated for weekly periodicity d is denoted as s d ."
"exercise 1: single command. the first exercise required to implement a single movement behavior. in the s-variant, the user had to instruct the robot to move to a specific point in space corresponding to a room. similarly, in the r-variant, the user had to instruct the drone with a single movement command. exercise 2: command sequence. in the second exercise, users needed to instruct a robot with a sequence of two single actions. in the s-variant, the users needed to instruct the robot to navigate to two rooms of the office, one after the other. in the r-variant, the user had to instruct the drone with any motion command, then ending its movement with a landing command."
"the regularity score varied according to the subject, as well as the day of the week. the pearson correlation coefficient for the plots in figure 9 was 0.267, indicating a weak positive relationship between the regularity and the prediction accuracy of gstp, which suggests that the regularity alone cannot fully explain the prediction performance due to the gstp's ability of accommodating irregularities through the smoothing and gapped sequence mining. it is still interesting to note that we can observe more dots in the upper right corner of figure 9 for subjects 1 to 3, who reported the highest scores for their subjective regularity assessment than for the other subjects, and vice versa."
"let us consider again the example of move_base that publishes the twist messages on the /cmd_vel topic. by analyzing the computational graph while the robot is operating, one can notice that a communication component, published by move_base, is producing setpoints on /cmd_vel, and therefore derive that the robot is able to move, hence owning a capability such as capa:directional_move-ment."
"another common use case for summingbird is when we desire aggregations over large volumes of historical data, but also need up-to-date results. for example, suppose we want to keep track of impression counts for all tweets over the entire life of the service: this obviously entails processing large volumes of log data, which is more suited to batch processing-but at the same time, we want real-time counts so we can identify \"hot content\" with minimal latency. one obvious solution would be to run hourly hadoop jobs, and then \"fill in\" the latest hour with storm. now take the perspective of a querying client who wishes to consume the results of these analyses. the client would need to implement complex logic for merging results from the batch and online computations in a robust manner. for example, the client needs to ensure that messages are not double counted by both storm and hadoop, and also handle the opposite problem, when gaps appear between hadoop and storm coverage. the client must also handle various abnormal operating scenarios, the most common of which is batch processing delays. when the hadoop cluster is operating beyond capacity, jobs may not generate results in a timely fashion, in which case the client must continue gathering results from storm and wait for hadoop to \"catch up\". summingbird transparently handles these issues to provide an integrated view of the data to querying clients."
"we have recently been considering cases where commutativity may be relaxed. with respect to an error function e, one way to formalize this might be:"
"summingbird consumes and generates two types of data: streams, which are (potentially) infinite sequences of tuples, and snapshots, which represent the complete state of a dataset at some point in time."
"summingbird leverages the tolerance for errors in many analytical tasks by the use of probabilistic data structures, which are primarily based on hashing, and where the source of error often comes from hash collisions. the use of such data structures means that summingbird processes every input key-value pair, unlike a strategy based on sampling. of course, there is no reason why summingbird cannot also incorporate sampling, although proper sampling requires some knowledge of the underlying distribution. in our experience, probabilistic data structures can be used as \"black boxes\" by engineers who have no understanding of the underlying implementations. in contrast, proper sampling usually requires engineers to have a much more sophisticated knowledge of statistics. below, we describe a few useful commutative monoids:"
"in addition to the major pain of, essentially, writing everything twice (once for batch processing and once for online processing), there was no standard online processing framework at twitter until recently. the systems for counting events in real-time were responsible only for gathering signals and offered little support in helping a client manipulate and process them. over the past several years, the result has been a proliferation of custom one-off processing engines for various specialized tasks. a good example that illustrates all these issues is described in a previous paper about twitter's real-time related query suggestion architecture [cit] . in that paper, we shared the case study of how a batch-oriented system was first built, only to be replaced immediately by an online system that depended on a custom processing system. because the system was specifically built to implement a particular type of query-suggestion algorithm, it would be difficult to reuse the code for other related tasks."
"a minor detail here is worth noting: many log messages that are generated near the end of an hour appear in the directory for the next hour due to unavoidable latencies in the log pipeline. we address this issue by running the hadoop jobs across a moving window of two hours, but discarding events that do not belong to the relevant batch. the size of the moving window is configurable, but in practice, we have found that a negligible fraction of events arrive more than an hour late. the results of these summingbird jobs are materialized in hdfs files (at known locations) that hold the aggregated values for the relevant batch."
"although the goal of summingbird is to increase developer productivity as opposed to analytics performance, we see opportunities for faster job execution as well. our design follows a traditional separation of logical plan from physical plan, with many opportunities for query optimization during plan compilation. one simple example currently implemented is that in the absence of developer-specified constraints, the system tries to heuristically tune batch sizes in storm. another trivial example is to coerce the first set of bolts in a topology to be co-located with the spouts (sources), thus saving a serialization step and a network hop. the upshot is that, even now, summingbird jobs are often faster than hand-written storm topologies. this is a nascent aspect of the project, although the applicability of common 5 a trivial solution would be to retain a log of all events, but this is obviously not scalable."
"to enable efficient aggregations in both batch and online processing, summingbird takes advantage of commutative semigroups, a type of algebraic structure (more details below). this means that summingbird imposes constraints on the types of aggregations that can be performed in the \"reduce\", although in practice we have not found this to be overly restrictive in the types of analytical queries that are possible with the language."
"dataflow languages. summingbird is a high-level dataflow language for analytical processing and thus it shares common features with other languages such as pig, cascading/scalding, and dryadlinq [cit] . their overall approach is to provide developers with high-level primitives and userdefined functions (udfs) in a manner that is abstracted from the underlying execution engine. summingbird is reminiscent of sawzall [cit], which supports arbitrary map-side procedural code but has only a small set of aggregators on the reduce side. however, sawzall aggregators do not take explicit advantage of algebraic structures to help developers reason about correctness. in contrast to all these languages, which are suitable only for batch processing, summingbird can target execution on hadoop and storm without any modifications to the program logic."
"beyond these examples, much of the power of summingbird derives from monoid implementations of common probabilistic data structures. these types were motivated by the realization that in many analytics scenarios, exact counts are not necessary-one might argue that exact counts are not even desirable due to the noise inherent in human behavior, e.g., accidental clicks. in concrete terms, it doesn't matter much if a retweet counter displays 141 or 142. any count is \"good enough\" as long as the value is within, say, 1% of the true count; in fact, front-end designers might choose to round the value to the nearest ten anyway. we hasten to emphasize that for some analytics scenarios (e.g., for billing advertisers), there is no tolerance for error and exact counts are absolutely necessary-for those tasks, probabilistic data structures are not appropriate."
"in many ways, the current state of online analytics processing on high-volume event data is similar to the state of hadoop-based data warehousing several years ago. data infrastructure builders discovered that simple primitives such as selections, joins, aggregations were sufficient to encompass most use cases (at the time), which made simple domainspecific languages such as pig highly successful. of course, as time went on, the types of analyses data scientists wish to perform increased in sophistication to include advanced analytics such as data mining, machine learning, etc. in the same way, we see a co-evolution of needs and capabilities for online analytics: being able to handle \"the easy cases\" is an essential first step that summingbird solves, but we have no doubt that new needs will arise in the future."
"although values in summingbird must be at least commutative semigroups to enable efficient aggregation in both the online and batch contexts while maintaining correctness, this has not been a limitation in practice at twitter. we have implemented a number of types that capture a broad range of analytical tasks, described below."
"online data processing frameworks often have close relationships to publish-subscribe systems such as hedwig 8 or kafka [cit] and queuing systems such as rabbitmq, amazon's simple queue service, 9 or twitter's kestrel. 10 these systems often serve as sources from which online data processing systems consume data-for example, summingbird is able to read messages from kestrel queues and from kafka. these systems, however, focus on issues related to the physical transport and delivery of messages, as opposed to how the messages are processed by consumers. category theory and data analytics. we have seen a number of researchers explore the relationship between algebraic structures, category theory, and large-scale data analytics. for example, meijer and bierman [cit] showed that a single generalization of relational algebra over sets-namely, monads and monad comprehensions-forms the basis of a common query language for both sql and nosql. in the machine learning domain, izbicki [cit] described how online training, parallel training, and cross-validation of classifiers can be understood in terms of monoids and homomorphisms. from the distributed systems literature, work on convergent and commutative replicated data types (crdts) [cit], which are distributed data structures that are guaranteed to eventually converge on the same state under asynchronous replication, are enabled by certain algebraic structures. the application of crdts to stream processing has been illustrated in a prototype [cit] . like summingbird, these works share in the insight that abstract algebra provides a formal framework for thinking about and potentially resolving many thorny issues in distributed processing."
"in the online case when a single processor is consuming from a single message queue or in the batch case when a single processor is consuming a single file, semigroups are sufficient for correctness, i.e., we do not need commutativity, since input key-value pairs will be processed sequentially. note, however, that even in this simple scenario commutativity is needed to handle transient data glitches that are often encountered in online processing such as out-of-order message delivery."
"another useful monoid is the set of all hashmaps (i.e., associative array) that map from arbitrary keys to values that are monoids. similar to complex tuples, the associative operation is key-wise application of the operation with the empty hashmap as the identity element. if the operation is commutative, then so is the hashmap monoid. the common use case for this type is to compute histograms one more non-trivial example worth a passing mention is our monoid treatment of minhash [cit], a fast probabilistic algorithm for computing similarity between arbitrary objects. in this case, computes the minimum of two signatures (i.e., the \"min\" in minhash)."
"in practice, most of the value types used in analytical processing tasks at twitter are commutative monoids. the next section describes them in more detail, but here we discuss how the properties of associativity and commutativity relate to online and batch processing more generally."
"the basic idea behind hybrid processing is to periodically \"roll up\" aggregates using hadoop and to \"fill in\" results from real-time data using storm. while this general design pattern is decades old [cit], our architecture illustrates the role of semigroups in allowing summingbird to integrate batch and online results while transparently preserving correctness. we begin by highlighting two key features:"
"the nature of analytics within twitter has evolved over the past few years, perhaps leading trends in the broader community. this paper focuses on a specific pain point that arose from the need for both batch and online analytics, which previously resulted in duplicate and difficult-tomaintain code. we describe efforts to address this challenge and reflect on the impact within the organization. early on, we also made the decision to open source summingbird so that the community can benefit from our experiences and build on our efforts. in terms of exploiting the formal properties of algebraic structures to seamlessly integrate different modes of distributed processing, we have only begun to scratch the surface and hope to stimulate further work."
"for some given and δ that are specified by the algorithm designer. of course, errors are likely to compound as more operands are swapped, but in an online setting, monoids that obey this property will be resilient to a certain (quantifiable) amount of transient data glitches without needing to be fully commutative. nevertheless, this relaxed notion of commutativity is mostly a curiosity at this point, since the data structures that are commonly used in analytics tasks at twitter form commutative monoids, as we discuss next."
"a platform describes how summingbird workflows are instantiated in a specific execution framework. currently, summingbird provides platform implementations for storm, scalding, and an in-memory execution engine (primarily for testing purposes). scalding is twitter's scala api to cascading, an open-source framework for building dataflows that can be executed on hadoop. thus, summingbird is connected to hadoop for batch processing indirectly via scalding and cascading."
"in summingbird, a call to sumbykey triggers a reduce operation where key-value pairs from the mapper stage are grouped by keys and all values associated with the same key are aggregated. the results are typically materialized in a store such as a file on hdfs or an in-memory key-value store. to enable efficient aggregation and the integration of batch and online processing, values in summingbird are limited to certain algebraic structures. first, a few definitions: definition 1. a semigroup is a tuple comprised of a set m and an associative binary operation :"
"in both the online case and the batch processing case, we need commutativity to guarantee correctness as soon as we introduce partitioned input, that is, multiple processors consuming from a single message queue or mapping over multiple files on hdfs. this is in fact the common case, since there is no other easy way to build scale-out distributed processing systems. commutativity is needed because partial results from each partition may be aggregated in arbitrary order. 3 in the online processing case, commutativity lets us handle issues like out-of-order message delivery. for both online and batch processing, commutativity further enables certain optimizations such as combiners in mapreduce and partial aggregations in storm (as discussed above)."
"in general, summingbird users are able to take advantage of probabilistic data structures (bloom filters, hyperloglog counters, count-min sketches) with relative ease, since there is a fairly straightforward mapping between exact data structures and their probabilistic equivalents (see figure 1) . most engineers do take the effort to learn, at least at a high level, how the data structures work, but this is not an absolute pre-requisite for writing summingbird programs. however, the use of probabilistic data structures requires us to educate developers about their proper usage and limitationsin particular, setting parameters to achieve the desired compactness/accuracy tradeoff and understanding the effects of certain skewed distributions. to assist developers, we explicitly expose the approximation errors for structures where it is easy to do via an api that provides error bounds and the probability that the true value lies within the error bound. with these metadata exposed, errors can be monitored and the data structures can be retuned when necessary."
"optimizations in the data management literature is fairly evident. furthermore, the logical/physical plan separation allows us to explore different platforms beyond hadoop and storm-possibilities include spark [cit] and akka. 6"
"mapreduce, particularly the open-source hadoop implementation, has given the data management community a powerful \"hammer\" with which to tackle big data problems. higher-level dataflow abstractions such as pig [cit] and cascading provide data scientists with powerful tools to unlock insights from the petabytes residing in modern data warehouses. these solutions focus on high-throughput batch processing, an area that has received substantial attention over the past few years. 1 typically, batch analytics are performed on static or slowly changing datasets ranging from terabytes to petabytes. the standard approach for analyzing high-volume data streams (e.g., log data) is to run periodic batch jobs (e.g., hourly). batch processing frameworks (e.g., hadoop) primarily focus on job throughput and often have difficult handling latencysensitive jobs, e.g., from interactive analyses. there is often a need for low latency responses to potentially complex queries over high-volume, infinite streams of data: this demands online processing capabilities. although data management on streams is not new, how to best integrate batch and online processing in a production environment remains an open question. this is the challenge that we tackle in this paper."
"we begin by describing pain points that we and our colleagues have experienced over the past several years in managing batch and online computations at twitter. this provides broader context about the problem that summingbird tries to solve. our situation is hardly unique, and the analytical queries that we describe are common in domains ranging from social media to online retailing."
"need for batch/online hybrids. it is clear that hadoop and storm both have their place, but merely standardizing on the two processing frameworks does not solve the problem of a developer needing to write everything twice. it would be desirable to have an abstraction for expressing analytical queries that is agnostic to batch or online processing, and have a system automatically generate hadoop jobs or storm topologies as appropriate. summingbird does exactly this."
"bloom filters [cit] are compact probabilistic data structures for keeping track of set membership, i.e., whether an element is a member of a set or not. false positive matches are possible (e.g., a bloom filter might assert that an element is in the set when in reality it is not), but false negatives are not possible. one canonical use case for bloom filters is to keep track of users who have been exposed to a certain event (e.g., tweet, recommendation, etc.) in order to avoid duplicate impressions. for twitter, this involves keeping track of o(10 8 ) objects/day for o(10 8 ) users. the asymmetric error properties of bloom filters (i.e., no false negatives) means that a user will never be exposed to the same treatment twice. bloom filters provide accuracy/space tradeoffs: given a desired error rate and a given capacity (determined a priori by the developer based on different application scenarios), we can appropriately size the filter."
"periodically, a summingbird job on the scalding (hadoop) platform is triggered to compute aggregates on the next incremental source batch that has been deposited in hdfs. the process of physically launching these jobs is accomplished through mesos [cit], although conceptually it is no different from cron. the mapping from batch ids to physical hdfs paths can be deterministically computed since data are structured according to a yyyy/mm/dd/hh/ physical layout. the data import pipeline is engineered so that a directory does not appear until all the files contained in that directory have arrived [cit], so it is not possible to process partially-imported results."
"online analytics. over the years, twitter has developed several generations of scalable systems for counting events in real time. from these counters we can answer most of the questions described above, but the issue is that the systems are not well integrated into the rest of our analytics infrastructure. this creates substantial friction during the development cycle: a data scientist would use familiar hadoopbased tools for retrospective analyses or model construction. however, since a production system must run in an online environment, she must write a separate set of interfaces for gathering and processing real-time signals. to mitigate the pain of working with two separate code bases, the common development practice is to encapsulate core functionality in a library that is agnostic with respect to the processing model, and then separately \"hook\" the library into either a batch or an online execution framework. this, however, is an imperfect solution for a few reasons: it still requires the developer to build and maintain code that binds to different execution frameworks. in many cases, two sets of aggregation logics must be created due to the inherent differences between batch and online processing. moreover, when writing code that is supposedly agnostic to the processing model, it is easy to forget the constraints of the execution environment. for example, scaling out in hadoop is often as simple as increasing the number of reducers, but the ability to scale out in an online environment by splitting streams is more restrictive. thus, it is not uncommon to prototype a particular feature in hadoop and then discover that the implementation is too slow to run in an online production setting. as another example: in batch processing, it is possible to take advantage of disk storage if in-memory data structures grow too large, but in online processing this is usually not possible due to latency requirements. managing memory limitations is particularly important when trying to track large event spaces that follow zipfian distributions, due to the presence of long tails."
"stream-oriented databases have a long history [cit] . typically, users issue standing queries in a variant of sql with temporal extensions and results are returned via some sort of callback. one advantage of these systems is that they build on widespread familiarity with sql. in addition, these systems usually have built-in primitives representing various temporal constructs such as sliding windows, which makes large classes of queries easy to write (e.g., counting clicks and clickthrough frequencies). in the last few years, stream processing engines have received renewed interest. in addition to storm, other systems in this space include s4 [cit], samza, 7 blockmon [cit], timestream [cit], spark streaming [cit], millwheel [cit], and photon [cit] . summingbird is different from these systems in its integration of online and batch processing, its dependence on algebraic structures to allow developers to reason about the correctness of computations, and its use of approximate data structures."
"it might be tempting to eliminate hadoop and build an analytics platform entirely around online processing, but this is impractical for several reasons. online processing does not obviate the need to store the raw data: fault tolerant systems for distributed online processing are usually forced to choose between an exactly-once message delivery guarantee that is expensive and slow or looser guarantees (e.g., at-least-once, best-effort) that are simpler and faster. storm chooses the second option, which is why online processing still needs to be \"backed up\" by traditional batch processing. furthermore, to correct errors in the online processing pipeline or to handle upgraded capabilities (e.g., an improved language detector), it is often necessary to rerun analytics on historical data and recompute results. although it is possible to simply replay archived data through storm, this would be hugely inefficient. in practice, when replaying historical data through storm, we are at best a small factor faster than consuming the sources in real time, compared to the arbitrary scale out that we are able to achieve with hadoop."
"suppose we are interested in the number of user interactions with some object. this could be clicks on links, impressions of recommendations, numbers of logins on a particular client, etc. typically, we wish to know the raw counts as well as descriptive statistics such as mean, median, and percentile breakdowns. often, we want to perform cardinality estimation or focus on the heavy hitters. in most applications, exact answers are not necessary as long as the errors are bounded-as an example, for links that are clicked more than one thousand times, it is sufficient to know the click count to the nearest ten."
"summingbird's ability to integrate batch and offline analytics supports a hybrid processing model where we are able to efficiently and seamlessly provide access to aggregations across long time spans while maintaining up-to-date values with minimal latency. one common use case is to keep track of counts for a very large event space (e.g., all tweets) across a long time span (e.g., for the life of the service)."
"summingbird was developed as a solution to the pain points described in the previous section. although our discussion is couched within the twitter production environment, we believe that many other organizations face the same challenges. the goal of the project is to provide a mapreduce-like programming model that generalizes over both batch and online computations, thus allowing code to be written once and executed on multiple processing frameworks. furthermore, the programming model provides abstractions that allow results from batch and online processing to be integrated in a seamless manner. in concrete terms, summingbird is an open-source domain-specific language implemented in scala."
"in addition to the basic map and map-like computations described above, the producer api provides a few other additional features such as the ability to merge sources from two producers and to perform left joins (more details in section 5.1)."
"typically, the batch results key-value store is much larger and backed by durable storage, whereas the online results are kept in memory-resident key-value stores (e.g., memcached). to prevent memory overflow, keys are pruned based on a time-to-live (ttl) setting. the ttl is tuned such that, under normal operating circumstances (and within a \"margin of safety\"), there will be no \"gap\" between the batch and online results-that is, largest batch id in the batch results store will be greater than the smallest batch id in the online results store. however, during times of excessive load on the hadoop cluster or outages, gaps may appear when the online results coverage is not sufficient to fill in where the batch results end. in this case, a client lookup will fail."
"the batch results key-value store polls hdfs periodically for the appearance of newly-created stores, and when one appears, the contents are ingested. since the key-value pairs on hdfs capture results for only that source batch, the ingestion process requires applying the semigroup associative operator (i.e., ) to aggregate those key-value pairs with the current contents of the batch results store. however, instead of storing (k, v) pairs directly, the contents are transformed into (k, (batchid, v)) pairs-this data structure captures the value of a particular key up to and including the specified batch id. this transformation is performed \"behind the scenes\" without the developer's knowledge. note that folding the batch id into the value to form a tuple is necessary because our key-value store implementation does not have an atomic ingest feature, or otherwise it would be sufficient to store a global batch id for the entire store."
"overall, there are roughly a dozen teams at twitter who run production jobs on storm, and half of them use summingbird exclusively. many of the other teams were early adopters of storm, before summingbird was sufficiently mature, and are considering migrating to it. in the same way that pig or hive emerged a few years ago as the preferred interface for hadoop (as opposed to writing java programs), summingbird is becoming the standard tool to access online analytics capabilities within the company."
"one particular limitation we have encountered so far is the need for generic folds (that are available in functional programming languages): we wish to retain some state, which is altered by an incoming event to produce new state. unless the updates themselves form a semigroup, this use case cannot be efficiently handled by summingbird."
"the contribution of this work lies primarily in the design of summingbird (section 3) and the use of probabilistic data structures implemented as commutative semigroups for a broad range of analytical tasks (section 4). additional features such as left joins and a hybrid processing mode are 1 a clarification on terminology: the terms online, real-time, and streaming are sometimes used interchangeably, and often without precise definitions. to us, online means that data are processed as they are being generated, in contrast to batch. processing in real time suggests low latency responses, although online processing doesn't always mean real time, e.g., an expensive computation. the literature on streaming algorithms usually implies limited working space. this is often, but not always, the case in online processing; e.g., a system might have access to large distributed key-value stores for retaining partial results. also discussed (section 5). we share our experiences running summingbird in production at twitter, reflecting on strengths and future work (section 6). since summingbird executes analytical workflows on either hadoop or storm, its performance is no worse than programs that are directly written for each execution framework (but see additional discussion in section 6). in this respect, the contribution of our work lies not in increasing system performance, but rather in boosting productivity for developers by providing a unified framework for batch and online analytics."
"monoids can also be composed from other monoids in more complex data structures. for example, the set of tuples figure 3 : example of counting query frequency by hour using hashmaps (left) and count-min sketches (right)."
"we'd like to thank everyone at twitter who has contributed to summingbird, particularly, ashutosh singhal, wen-hao lue, bill darrow, joe nievelt, brian wallerstein, dmitriy ryaboy, and alex roetter."
"framework standardization. today, twitter has made progress in addressing the problem of the proliferation of one-off systems. hadoop remains the standard for batch analytics, although it is mostly accessed via higher-level abstractions such as scalding and pig. for online processing and real-time analytics, storm has emerged as the standard execution framework. hadoop requires no introduction, but here we provide a quick overview of storm."
"just as in mapreduce, the \"map\" phase of a summingbird program specifies per-record computations. these embarrassingly parallel computations are amenable to scale out in both batch and online execution. three types of per-record computations are provided for a producer:"
"as a concrete example, figure 3 shows two versions of a summingbird program to generate hourly counts of search queries. the left version provides an exact solution by keeping track of the query counts in a hashmap (which is a commutative monoid, as previously discussed). this solution will not scale because there is not enough memory to keep track of all unique queries and their counts. the version on the right replaces the hashmap with a count-min sketch: note that the program has exactly the same logic. extra lines are necessary to properly initialize the count-min sketch (type sketchmap), and the (implicit countmonoid: ...) statement declares that whatever is calling this code must have the monoid available in its implicit scope."
"the flatmap method takes a function that generates a list of values, applies the function to each object consumed from the source, and then flattens together all returned lists. the result is another producer. the word count example uses flatmap because the tokenizer generates a list of tokens for each input sentence."
"summingbird can perform left joins via the service abstraction. as described in section 3, a producer has a source which materializes the input; in addition, it can have a service, which can be viewed as a mapping from values of type t to values of type u. possible implementations for a service include gets from in-memory key-value stores, database queries, remote procedure calls, etc. in data warehousing parlance, a service would hold a dimension table to be joined with the fact table (i.e., the source)."
"a producer is an abstraction over the state of a data transformation in summingbird (i.e., a node in a dataflow graph). a producer has an associated source, which materializes objects of a particular type for processing. at the beginning of a program, in the online setting, sources are usually attached to message queues from which data are consumed in real time. in the batch setting, sources might read from files that are stored in hdfs. from a producer, a workflow can be constructed via transformations described below, each of which creates another producer."
"the complete architecture of summingbird running in hybrid mode is shown in figure 5, but we begin by introducing a few basic concepts. in addition to the summingbird program, we assume that the developer has defined two additional classes: the timeextractor extracts timestamps out of sources, and the batcher maps a timestamp onto an integer batch id. the batch id defines disjoint batches (e.g., hourly), the level of granularity at which aggregations are computed, and internally, summingbird will never partially aggregate across two different batches. in most cases, the implementations of these classes are trivial, as nearly all log events are annotated with a timestamp, which can be straightforwardly mapped (for example) to the nearest hour."
"the key insight of this work is that certain algebraic structures provide the theoretical foundation for seamlessly integrating batch and online processing. from this starting point, we have built summingbird, a data processing framework that supports both batch and online computations formulated in terms of these algebraic structures. summingbird provides a domain-specific language (dsl) implemented in scala for expressing analytical queries that transparently generates either hadoop jobs (batch computations) or storm topologies (online computations) without requiring any changes to the program logic. furthermore, summingbird can operate in a hybrid processing mode that transparently integrates batch and online results to efficiently generate up-to-date views over long time spans. although aggregations in summingbird are restricted to certain algebraic structures, in practice, we find that our language is sufficiently expressive to capture large classes of analytical queries in a production environment."
"let us now turn our attention to the client. the client library maintains connections to both the online and batch results store. all queries first go to the batch results store: by comparing the wall clock time and the batch id from the result, the system knows how \"far behind\" the value is. based on this, the client can figure out how many values need to be \"filled in\" from the online store, which is keyed by (k, batchid). it can then issue appropriate requests to the online results store. the final, up-to-date value is arrived at by aggregating all the partial values-once again, the validity of these operations is licensed by the fact that the values are (at least) semigroups. note that all this logic is handled by the client library, and thus downstream systems are presented with a simple key-value interface."
"as previously discussed in section 2, [cit], and it was open-sourced in september of that year. 4 the adoption of summingbird has grown over time, and here we provide a snapshot of its deployment status within twitter. currently, there are a few dozen summingbird jobs that are running in production, which account for roughly half of the jobs that depend on online analytics. a typical summingbird job might process 1-20 mb/s (around 100-250k events per second) in the online mode and execute hourly in batch mode, translating to several hundred hadoop jobs daily. the results of these jobs typically feed online dashboards for internal monitoring purposes or generate signals that serve as input to user-facing products."
"answering these questions on data stored in the hadoop data warehouse is straightforward using pig or any other comparable analytical tool. whatever is not provided as a language primitive can be easily implemented as a udf. this suffices for retrospective analysis and building offline data products (e.g., machine learning models), but what if we desire these answers in real time on live data, with subsecond latency?"
"although summingbird is capable of creating complex dag workflows, let us begin by focusing on the canonical use case. just like in standard mapreduce, a summingbird job consists of two phases: the \"map\" phase, where per-record computations are applied in parallel to generate an arbitrary number of intermediate key-value pairs, and the \"reduce\" phase, where values are aggregated by key. a simple word count example in summingbird is shown in figure 1 . in the flatmap we apply a tokenizer that breaks sentences from the source and for each token generates a key-value pair with the token as the key and one as the value. the keys are then aggregated with sumbykey (i.e., the \"reduce\") and materialized in store. a lot of complexity is hidden in sumbykey, as we detail shortly. to illustrate the difference between stores and sinks, if we insert a write(sink) before the call to sumbykey, the program will materialize (e.g., to an hdfs file or another message queue) each parsed token and the count one as a key-value pair. in contrast, if the write call is inserted after sumbykey, the program will materialize a stream of partial aggregations (i.e., deltas) for each word."
"we emphasize that the biggest pain point that summingbird tries to address is developer productivity, not runtime performance, since ultimately, what runs is either a \"vanilla\" hadoop job or storm topology. thus, the contributions of the language lie in the abstractions it introduces and its balance between simplicity and expressivity with respect to a broad range of analytical queries encountered at twitter."
"storm [cit], 2 now an apache project. a storm cluster executes user-submitted topologies (the equivalent of hadoop jobs). a topology represents a directed acyclic dataflow graph comprised of \"spouts\", which are sources of streams, and \"bolts\", which perform stream transformations. spouts are usually connected to message queues, from which they consume sequences of tuples. the execution framework is responsible for running topologies over a cluster, handling task placement, message routing, and ensuring robustness in the presence of failures. the framework provides options for either best-effort message delivery or at-least-once message delivery. bolts execute in parallel across nodes in the cluster, and storm provides \"groupings\" to specify connections between bolts. for example, a \"fields grouping\" performs a group-by to ensure that all messages with the same field are delivered to the same bolt."
"another related thread of research explores mapreduce extensions for incremental batch processing. examples include the continuous bulk processing (cbp) model [cit], incoop [cit], and hourglass [cit] . the focus of these systems is on techniques to minimize reprocessing of old data for reoccurring jobs on data that are being ingested continuously (e.g., log files). summingbird's solution to the same class of problems is the hybrid processing mode described in section 5.2, but in addition to efficient incremental batch aggregations (using hadoop), we are able to provide up-to-date results with minimal latency (using storm). other online and stream processing systems. online processing of data streams in real time is of course not a new problem. below we discuss other systems for online and stream processing beyond mapreduce extensions."
"summingbird in hybrid mode requires integration with other infrastructure at twitter: on the source end, we assume the existence of message queues that deliver event data in real-time and that the same data are also deposited onto hdfs (for example, see [cit] for a description of twitter's logging infrastructure). on the store end, we assume the existence of two separate key-value stores: one for the batch results, and the other for the online results (although the client library transparently handles results merging). twitter uses open-source software for most of these components, but even in cases where we rely on an internal implementation, comparable open-source equivalents are readily available."
"in summary, the three most commonly-used probabilistic data structures in summingbird are shown in table 1 with their exact equivalents. in our experience, the algebraic types discussed above appear to be sufficient for large classes of analytical queries that twitter data scientists issue on a daily basis."
"note that pre-shuffling and post-shuffling batching should be considered optimizations, since they do not affect result correctness (due to the commutative guarantees provided by the semigroup). it is more accurate to think of these as knobs that the developer can tune to control the tradeoff between latency and resource consumption. for example, if an application really demands updates for each key-value pair as it is encountered, batching can be turned off, and we would simply need to allocate sufficient resources in the store implementation to support the query load."
"hyperloglog counters [cit] are compact probabilistic data structures for cardinality estimation (i.e., size of a set). a canonical use case of hyperloglog counters is to keep track of the number of unique users who have performed a certain action, e.g., retweeted or favorited a tweet, clicked on a link, etc. for twitter, a naïve exact solution based on sets would be impractical for o(10 8 ) users, particularly for events with high cardinalities (e.g., retweets of celebrities' tweets or tweet impressions). these counters are also useful for computing graph statistics such as the size of a node's second-degree neighborhood (e.g., followers of followers [cit] ). a hyperloglog counter occupies o(log log n) space for cardinalities of up to n. these data structures are tunable within the (, δ) framework-that is, achieving (1 ± )-approximation with probability δ. the choice of parameters affects the constant factor in the size of the counters. [cit] are compact probabilistic data structures for keeping track of frequencies (i.e., counts) associated with events. a canonical use case of count-min sketches is to keep track of the number of times a query was issued to twitter search within a span of time. in general, count-min sketches can be used for building histograms of events. the data structure is based on hashing objects into a two dimension array of counts using a series of hash functions. given a desired error bound in the (, δ) model based on the application scenario, we can compute the size of the data structure that underlies the count-min sketch."
"in the general case, to guarantee the correctness of computations, values in summingbird must be at least commutative semigroups (and of course, they can be algebraic types with more properties such as commutative monoids). in the reduce phase of summingbird, sumbykey aggregates all values with the same key using the associative binary operation belonging to the semigroup ( ). the introduction of semigroups allows us to precisely define the semantics of sumbykey on an infinite stream of keyvalue pairs. formally, each aggregation is get(k) v, i.e., we incrementally apply the associative operation to the current value held in the store and each new value. in practice, however, this approach does not scale to the volumes of data that twitter processes. our solution is to buffer output key-value pairs after the group-by and perform aggregations in batches: buffer sizes are entirely user configurable, and a typical setting might be to process batches of 10k keyvalue pairs, but no less frequently than every 30 seconds. processing in small batches also allows us to take advantage of efficient multi-get and multi-put operations that are supported by many key-value stores. summingbird also provides options for buffering key-value pairs prior to network shuffling on storm and performing aggregations on the map side; this is entirely analogous to combiners in mapreduce and reduces network traffic."
"hadoop and batch analytics. a number of large hadoop clusters comprise the core of twitter's data warehouse. data are imported from a variety of sources, including structured databases (e.g., user profiles and the interest graph), unstructured text (e.g., tweets), and semi-structured interaction logs [cit] . for analytics and building data products, data scientists typically use a higher-level dataflow language such as pig or scalding (twitter's scala api to cascading). this is a mature production system, aspects of which we have previously described elsewhere [cit] ."
"in parallel with the batch jobs, the same summingbird program is continuously executed in a storm topology, and the results are deposited in an online results key-value store. instead of aggregating by key k, however, the system automatically builds a compound key (k, batchid) for performing the grouping. these represent the online partial results for each batch."
"summingbird jobs can generate two types of outputs: a store represents an abstract model of a key-value store. in the online setting, a store receives partial results that are combined with its present state, and might be backed by memcached, mysql, or hbase; reasonably low-latency reads and writes are a requirement. in the batch setting, a store contains a snapshot of the aggregated value for each of its keys, which is usually materialized to disk. a sink materializes (unaggregated) tuples from the producer, typically after some manipulation. sinks might populate another message queue for further processing (in the online case) or simply write data to disk (both online and batch processing)."
"note the summingbird program in figure 1 references only the source and store abstractions, and thus remains agnostic to the processing model (whether batch or online). to run the job on hadoop, we supply the scalding platform, as in figure 2 (left), which creates a hadoop job that processes input data on hdfs and writes results back to hdfs. alternatively, we can supply the storm platform, as in figure 2 (right), yielding a storm topology that processes tweets in real time and stores the counts in memcached. to precisely explain the semantics of a group-by on an infinite stream requires a discussion of algebraic structures, which we defer to section 3.2."
"we have found that most online analytics needs at twitter can be handled by summingbird, but admittedly, there is a large amount of self-selection in users. however, since the framework evolved out of several years of experience building and running disparate online analytics systems, we are fairly confident to have captured at least a large fraction of the cumulative needs of the organization."
"5 one specific example of this is online learning, say with a linear model using stochastic gradient descent [cit] . in the general case, a model learned online will be different from a model learned using a batch algorithm; even the order in which the training examples are presented to the online learner affects the final model parameters. thus, it is difficult to develop an approach that generalizes across online learning, batch learning, and distributed learning (over different partitions) while maintaining stability in the output models and theoretical guarantees. we are aware of recent work by izbicki [cit] in formulating standard machine learning constructs in terms of algebraic types, and it would be interesting future work to see how those results can be integrated into summingbird."
"due to the large variations of image content, using a single prior model cannot well describe natural images. rather than learning a uniform prior, we cluster the training samples and learn a prior model for each cluster. in this way, multiple nsp models can be learnt to fit different image contents adaptively, and hence content-aware prior models can be efficiently estimated for image restoration. moreover, mrf techniques can be adopted for robust parameter estimation along the nonlocal graph. our experiments on image denoising, deblurring and super-resolution demonstrated the effectiveness of the proposed nsp model. in summary, the contribution of our paper is twofold:"
"as a promising approach, graphics processing unit (gpu) is good at dealing with problems which can be expressed as data-level parallel computations. therefore, it is becoming the mainstream computation platform to manipulate computer graphics and image processing. 20 figure 1 illustrates the di®erence between cpu and gpu architectures. unlike traditional multicore cpus which are based on the multiple instruction, multiple data (mimd) design, the single instruction, multiple data (simd)-based gpu has a highly parallel structure which enables more e±cient computation than general-purpose cpus when handling large blocks of data in parallel. to facilitate the gpu programming, compute uni¯ed device architecture (cuda) provides a parallel computing platform and application programming interface (api) model, which can utilize gpu resources e±ciently. cuda o®ers a uni¯ed hardware and software solution for parallel computing on cuda-enabled gpus supporting the standard c programming language together with computing numerical libraries. to accelerate me processing, our approach adopts the cudabased gpu as the parallel computing platform."
"denotes the data structure that keeps the distance and brightness information of speci¯c pixels. here, l denotes the brightness level of pixels ranging from 0 to 25. our approach maps the traditional 256 brightness levels (i.e., 0-255) into 26 levels (i.e., 0-25) by dividing them by 10. in this way, we can signi¯cantly reduce the chance of bdm consistency violations as de¯ned in eq. (7). this is because the larger granularity of brightness sampling will lead to smaller brightness changes between adjacent captured images."
"1,2 as a promising means, me methods play an important role in the domain of environmental science. they not only can be employed to track ice°oes, 3 but also can be utilized for the purpose of weather prediction and ocean circulation analysis. 4, 5 moreover, me techniques have been considered as the key components in the domain of°uid mechanics, 6 medical images processing, 7 and image recovery. 8 when applying me, the motion¯elds are captured by cameras, and the images of the perspective projection onto the image plane are saved for the analysis. however, in this way, the images generally are not properly represented, since they only provide the brightness information of°uid°ows. it is common that brightness changes irregularly in image series, which can easily result in inaccurate me results. therefore, how to compute motion¯elds and how to recognize the image patterns have become major challenges in me. although in the past decade there are dozens of approaches proposed to address the above issues, most of them focus on the accuracy of motion¯elds rather than time-e±ciency. consequently, to achieve expected me accuracy, existing approaches require a huge number of computation e®orts to process complex images, which strongly limits the me application in practice."
"algorithm 4 shows the procedure for our gpu-based me algorithm. note that the bdms for both original and reference images are de¯ned as global data structures. their elements can be accessed during the following me stages. in this algorithm, step 1 calculates the bdms of each pixel for both the original image (i.e., img1) and reference image (i.e., img2)."
"this paper describes an approach to this challenge using ensembles of deep learning architectures. this approach, called random multimodel deep learning (rmdl), uses three different deep learning architectures: deep neural networks (dnn), convolutional neural netwroks (cnn), and recurrent neural networks (rnn). test results with a variety of data types demonstrate that this new approach is highly accurate, robust and efficient."
"to show the e±cacy of our me approach, we conducted our me algorithm on four typical scenarios, namely motions of ocean wave (i.e., video \\649dd10\"), river stream (i.e., video \\649i720\"), waving°ags (i.e., video \\646a210\"), and pumping fountain (i.e., \\6484d10\"). the video \\649dd10\" presents a scenario of ocean wave movement. this case involves a large number of drastic brightness changes due to the quick movement of sea water. the video \\649i720\" shows the movement of river stream which consists of slight brightness change. the video \\646a210\" presents the movement of waving°ag which is irregular due to strong winds. the video \\6484d10\" shows a scenario of pumping fountain. this case consists of lots of irregular°uid movements. the reason why we chose these four cases is because the sequences involve various irregular°uid movements and brightness changes, which can re°ect the e±cacy of our approach within most°uid motion cases. in fig. 2, the images on the left side are original images and the images on the right side are the ones obtained using our gpu-based approach. since the di®erence between the original image and reference image is small, we do not present the reference image in fig. 2 . in the right images, red arrows denote the motion vectors obtained from the initialization stage, whereas green arrows indicate the motion vectors generated in the optimization stage. from fig. 2, we can¯nd that our gpu-based method can achieve me results with higher accuracy than the other two approaches 23, 24 under di®erent kinds of°uid scenarios. for example, the result of \\649dd10\" demonstrates that our gpu-based method is capable of estimating motion of regions with drastic brightness changes. the result of \\649i720\" shows the e±cacy of our method when estimating regions with slight brightness changes."
"the rest of this paper is organized as follows: section 2 gives related work for feature extraction, other classification techniques, and deep learning for classification task; section 3 describes current techniques for classification tasks which are used as our baseline; section 4 describes random multimodel deep learning methods and the architecture for rmdl including section 4.1 shows feature extraction in rmdl, section 4.2 talks about overall view of rmdl; section 4.3 addresses the deep learning structure used in this model, section 4.4 discusses optimization problem; section 5.1 talks about evaluation of these techniques; section 5 shows the experimental results which includes the accuracy and performance of rmdl; and finally, section 6 presents discussion and conclusions of our work."
"in this section, we first conduct experiments with simulated gaussian blur kernel and uniform blur kernel, respectively, and then conduct experiments with real motion blur kernels. in the first experiment, we simulated the blur images by gaussian and uniform kernels, respectively, and then added gaussian noise to them (noise level: √ 2 and 2). we choose τ as 6 and gradient descent step length ζ as 0.6 in the deblurring experiment. we compare our method with several state-of-the-art deblurring methods, including fista [cit], sa-dct [cit], bm3d [cit] and csr [cit] . note that the recently developed csr has shown very strong deblurring capability. three images are used in this experiment, and the psnr results are listed in table 3 . we can see that our method outperforms all the competing algorithms in average. for examples of visual quality comparison and more details, please refer to the attached supplementary file."
"23,24 figure 3 shows the comparison results. we can observe that the me approaches 23, 24 cannot accurately detect the varying brightness changes. from examples shown in figs. 3(a)-3(c), the me results generated by refs. 23 and 24 indicate that large parts of the images are motionless, which do not re°ect the real°uid motions. however, our approach can accurately capture such complex scenarios. it is less likely to be a®ected by large displacements and drastic brightness variations. in fig. 3(d), there is a rock at the left-bottom corner of the image which causes irregular°uid motions in that area. unlike the reference methods (refs. 23 and 24), our approach can accurately express such irregular°uid motions."
"as a hot topic in computer vision, motion estimation for°uid°ows has been widely investigated. however, so far there is a lack of approaches that can accurately estimate°uid image regions with both slight and drastic brightness changes. furthermore, due to the enormous computation workload in accurate motion estimation, how to reduce the me time while guaranteeing expected me accuracy is becoming a major bottleneck in me research. to address the above issues, this paper presented a three-stage gpu-based method for e±cient motion estimation of°uid images. in our approach, the regional brightness of images is modeled using our proposed brightness distribution matrix. by denoising initial motion¯elds, our approach enables the early identi¯cation of inaccurate motion vectors. based on our proposed energy function, these inaccurate motion vectors can be tuned and optimized to re°ect the real°uid motions. since our approach requires more computation e®orts than the existing me methods, we developed a parallelized version of our approach based on gpu to accelerate the above me steps. experimental results show that our gpu-based approach not only can obtain better motion estimation results for complex°uid scenarios than the existing me methods, but also can be executed e±ciently in terms of runtime."
"feature extraction: feature extraction is a significant part of machine learning especially for text, image, and video data. text and many biomedical datasets are mostly unstructured data from which we need to generate a meaningful and structures for use by machine learning algorithms. as an early example, l. krueger et. al. [cit] introduced an effective method for feature extraction for text categorization. this feature extraction method is based on word counting to create a structure for statistical learning. even earlier work by h. luhn [cit] introduced weighted values for each word and then g. salton et. al. [cit] modified the weights of words by frequency counts called term frequency-inverse document frequency (tf-idf). the tf-idf vectors measure the number of times a word appears in the document weighted by the inverse frequency of the commonality of the word across documents. although, the tf-idf and word counting are simple and intuitive feature extraction methods, they do not capture relationships between words as sequences. recently, t. mikolov et. al. [cit] introduced an improved technique for feature extraction from text using the concept of embedding or placing the word into a vector space based on context. this approach to word embedding, called word2vec, solves the problem of representing contextual word relationships in a computable feature space. building on these ideas, j. pennington et. al. [cit] developed a learning vector space representation of the words called glove and deployed it in stanford nlp lab. the rmdl approach described in this paper uses glove for feature extraction from textual data."
"optimize(x, y,mf)begin if mf is denoised then for i from −6 to 6 do for j from −6 to 6 do 1. compute the image energy; image energy obtained in step 1 and smooth energy in step 2, which equals to total energy as de¯ned in eq. (12) . if total energy is less than the currently obtained min energy, we will set the motion vector of pðx; yþ using the current values of i and j, and reset the min energy. finally, the algorithm reports an optimized motion vector for pixel pðx; yþ."
"where w denotes the size of the sampling window and iðx; y; tþ denotes the brightness of pixel pðx; yþ at time t. generally, me approaches assume that the regional brightness has a constant value. nonetheless, slight brightness changes exist very frequently in real images. if we use formula (1) to present the regional brightness, the slight brightness may easily lead to the generation of incorrect motion vectors. in our approach, we calculate one bdm for each pixel in each frames. assuming that the current pixel at time t is at location pðx; yþ, we de¯ne the bdm of this pixel using a one-dimensional vector in the form of"
random multimodel deep learning is a novel technique that we can use in any kind of dataset for classification. an overview of this technique is shown in figure 2
"3.1.5 hierarchical deep learning for text classification (hdl-tex). this technique is used as one of our baselines for hierarchical labeled datasets. when documents are organized hierarchically, multi-class approaches are difficult to apply using traditional supervised learning methods. the hdltex [cit] introduced a new approach to hierarchical document classification that combines multiple deep learning approaches to produce hierarchical classification. the primary contribution of hdltex research is hierarchical classification of documents. a traditional multi-class classification technique can work well for a limited number of classes, but performance drops with increasing number of classes, as is present in hierarchically organized documents. hdltex solved this problem by creating architectures that specialize deep learning approaches for their level of the document hierarchy."
"step 4 optimizes the motion vectors that are identi¯ed during the denoise stage. finally, the me algorithm reports an optimized motion¯eld for the given images."
"the three basic deep learning architectures use different feature space methods as input layers. for instance, for feature extraction from text, dnn uses term frequency-inverse document frequency (tf-idf) [cit] . rdml searches across randomly generated hyperparameters for the number of hidden layers and nodes (desity) in each hidden layer in the dnn. cnn has been well designed for image classification. rmdl finds choices for hyperparameters in cnn using random feature maps and random numbers of hidden layers. cnn can be used for more than image data. the structures for cnn used by rmdl are 1d convolutional layer for text, 2d for images and 3d for video processings. rnn architectures are used primarily for text classification. rmdl uses two specific rnn structures: gated recurrent units (grus) and long short-term memory (lstm). the number of gru or lstm units and hidden layers used by the rdml are also the results of search over randomly generated hyperparameters."
"strategy p2: for each pixel, we assign a thread block with 256 threads. in order to calculate the distance function, each thread is responsible for the computation of distance function for one of the pixels within the searching window. table 2 presents the experimental results using the two proposed di®erent parallel strategies. from this table, we can¯nd that the execution times of strategy p2 are much smaller than those of strategy p1. this is mainly because solution p1 uses gpu to accomplish the comparison tasks which consist of many branch instructions. in this case, the gpu performance will be degraded due to branch divergence caused by branch instruction."
"in the training stage, we extract patches from the sample images and partition the n training patches into clusters by a standard gaussian-mixture clustering model which can be effectively solved by expectation-maximization algorithm. in each cluster, image patches share similar content, and we can assume that their nonlocal spectrums have similar ggd distribution. however, between different clusters, the nonlocal singular values have distinct distributions. fig. 1 has actually illustrated the content-awareness of nsp."
"among all the three stages of our sequential approach as described in sec. 3, there exist lots of independent calculation tasks which can be fully parallelized. for example, the motion vector calculations of pixels are independent which are quite timeconsuming. to accelerate our me algorithm, we choose cuda 25 as our computing platform based on gpu. when applying gpu to accelerate our me approach, we need to¯gure out how to e±ciently allocate such subtasks to gpu cores to enable su±cient data-level parallelism. since the initialization of motion¯elds and the optimization stage cost most of the calculation time, we only parallelized these two stages to achieve better me performance. the following two subsections will give the gpu implementation details of these two stages."
"several approaches have been proposed for adaptive parameter estimation in image restoration. [cit] sampled small regions around the objective location and estimated ggd parameters from high-order statistics by support vector regression. however, in our problem it will be time consuming to calculate the high-order statistics of each patch by svd. moreover, without knowing clean image in prior, robust parameter estimation is necessary for practical image restoration. since gradient-based statistics are not robust to noise, it is difficult to evaluate robustly the parameter from noisy images. instead, by exploiting nss in the proposed nsp model we could propose a fast parameter estimation approach which is robust to various kinds of image degradation."
"another neural network architecture that contributes in rmdl is recurrent neural networks (rnn). rnn assigns more weights to the previous data points of sequence. therefore, this technique is a powerful method for text, string and sequential data classification but also could be used for image classification as we did in this work. in rnn the neural net considers the information of previous nodes in a very sophisticated method which allows for better semantic analysis of structures of dataset. general formulation of this concept is given in equation 11 where x t is the state at time t and u t refers to the input at step t."
"the motion vector of a pixel is calculated using the coordinate displacement between the pixel in the current frame and the corresponding pixel in the reference image. since adjacent frames are almost the same and coordinate changes are mainly caused by the movements of objects or cameras, the brightness of one pixel should be similar to the regional brightness of its surrounding pixels. particle image velocimetry (piv) is widely adopted for°ow visualization and analysis. in piv, the brightness of a local region is described as"
"where n is the number of random models, and y i j is the output prediction of model for data point i in model j vote for finalŷ i . therefore,ŷ i is given as follows:"
"where z is a partition function to normalize density. considering the dependence between nonlocal patches and the content-aware parameter estimation, our proposed nsp model can be technically seen as a conditional random field. based on the specific block matching approach we adopt, the strictly equation can be ensured according to the independence between different nonlocal blocks (cliques). therefore, the estimated image x can be obtained by minimizing the log-posterior as follows:"
"for image classification, we have five baselines as follows: deep l2-svm [cit], maxout network [cit], binaryconnect [cit], pcanet-1 [cit], and gcforest [cit] . deep l2-svm: this technique is known as deep learning using linear support vector machines which simply softmax is replaced with linear svms [cit] . maxout network: i. goodfellow et. al. [cit] defined a simple novel model called maxout (named because its outputs' layer is a set of max of inputs' layer, and it is a natural companion to dropout). their design both facilitates optimization by using dropout, and also improves the accuracy of dropout's model."
"since motion estimation plays an important role in multimedia applications, various me algorithms have been intensively investigated. for example, horn and schunck 9 introduced the¯rst method for me of objects among image sequences. based on the brightness consistency-based constraints, their approach is widely used in the image sequences of solid. in ref. 10, lucas and kanade presented an me approach based on the pyramid model. the proposed approach not only can reduce the limitation of location window, but also can improve the accuracy of estimation. although this approach is e®ective in dealing with drastic brightness changes, the accuracy of this method decreases signi¯cantly when there are only slight brightness changes."
"in this paper, we use both contemporary and traditional techniques of document and image classification as our baselines. the baselines of image and text classification are different due to feature extraction and structure of model; thus, text and image classification's baselines are described separately in the following section."
"where e data ðu; vþ is derived from a given continuity equation and e regularization is generated from a speci¯c smoothness consistency constraint. 9 the parameter balances the in°uences between two force terms in the function. although the energy functions in this form are promising for me, they cannot estimate regions with drastic brightness changes accurately in most cases. since the pixels in the same region have similar motion vectors, under the guidance of neighboring motion vectors, the inaccurate motion vectors can be further improved. for the pixel pðx; yþ at"
"due to the movement of cameras and light sources, the brightness changes of°uid°o ws are very common, which can inevitably result in inaccurate me results. in the denoise stage, our approach tries to¯lter out inaccurate motion vectors from the motion¯eld obtained in the¯rst stage. note that pixels should have similar motion directions within a small region of°uid°ows. therefore, we assume that the motion vector of a pixel is inaccurate if it is quite di®erent from the motion vectors of other pixels in the same region. to improve the overall accuracy of motion¯elds, such kind of motion vectors should be identi¯ed for further improvements."
deep learning: neural networks derive their architecture as a relatively simply representation of the neurons in the human's brain. they are essentially weighte combinations of inputs the pass through multiple non-linear functions. neural networks use an iterative learning method known as back-propagation and an optimizer (such as stochastic gradient descent (sgd)).
"where l j is the length of the document j, and w i, j is the glove word embedding vectorization of word i in document j."
"in fact, the nonlocal self-similarity (nss) has been successfully exploited in image restoration [cit] . despite the wide use of nss, there lacks an in-depth analysis of the low-rank characteristics of nonlocal similar patches. in fig. 2, we plot the empirical distributions of the nonlocal singular values in five natural images. from fig. 1 and fig. 2, one can easily find that the nss is highly content dependent, spatially variant, and the nss induced nonlocal singular values are distributed with heavy-tails. based on these observations, in this paper we propose a novel natural image prior, namely nonlocal spectral prior (nsp), and apply it to image restoration tasks. in particular, we parameterize the heavy-tailed distribution of nonlocal singular values by generalized gaussian distribution (g-gd) 1 :"
"text classification techniques which are used as our baselines to evaluate our model are as follows: regular deep models such as recurrent neural networks (rnn), convolutional neural networks (cnn), and deep neural networks (dnn). also, we have used two different techniques of support vector machine (svm), naïve bayes classification (nbc), and finally hierarchical deep learning for text classification (hdltex) [cit] ."
the proposed approach has the ability to improve accuracy and efficiency of models and can be use across a wide range of data types and applications.
"compared with existing me approaches, our approach not only can enable the more accurate estimation of steady°uid°ows with slight motion, but also can be used to estimate regions with abrupt brightness changes. the rest of this paper is organized as follows. after introducing the related work in sec. 2, sec. 3 presents the major steps of our sequential me algorithm including initialization, denoising, and optimization. to accelerate the processing time, sec. 4 introduces the gpu version of our proposed me approach in details. based on the experimental results on a well-known benchmark, sec. 5 shows the e±cacy of our approach from the perspectives of accuracy and performance. finally, sec. 6 concludes the paper."
"note that we do not parallelize the bdm calculation using gpu, since there are lots of branches in the bdm calculation. the bdm construction is used for distancebased motion vector calculation as de¯ned in eqs. (7) and (8) . algorithm 2 presents the details of our parallelized method for distance-based motion vector calculation. in this algorithm, there are three inputs: x and y denoting the x-component and y-component of pixel pðx; yþ to be processed and mf denoting the motion¯eld of a speci¯c image. assume that the counterpart pixel of pðx; yþ in the reference image is located within the searching window. we can obtain the distance value for each pixel in the searching window and select the pixel with minimal value as the counterpart pixel. in this algorithm, step 1 launches the kernel function and loads the required"
"the joint use of nss and lrmr for image restoration is natural and very effective. by reforming the collected nonlocal similar patches into a 2d matrix, image restoration becomes essentially an lrmr problem. however, whether the low-rank prior models exist to characterize the nss for a wide range of natural images has been rarely discussed. in this paper, we investigate this problem in detail. by theoretical and empirical analysis, we find that the distributions of singular values of the matrices formed by nonlocal similar patches are heavy-tailed and can be parameterized as generalized gaussian distribution (ggd). based on this observation, a novel natural image prior model, namely nonlocal spectral prior (nsp) model, is proposed by learning the parameters of ggd from natural images. different from the widely used gradient prior models which exploit pixel-level high-order derivative statistics, the nsp model exploits structure selfsimilarities, and its parameter estimation is more robust to image degradation."
researchers from a variety of disciplines have produced work relevant to the approach described in this paper. we have organized this work into three areas: i) feature extraction; ii) classification methods and techniques (baseline and other related methods); and iii) deep learning for classification.
"as discussed before, due to the variation of image content, learning a uniform nsp model for all nonlocal matrices is not accurate and robust. therefore, multiple nsp models should be learnt and then applied adaptively based on image content. in this paper, we adopt vector quantization techniques to conduct multiple nsp model learning."
"the feature extraction is divided into two main parts for rmdl (text and image). text and sequential datasets are unstructured data, while the feature space is structured for image datasets."
"parallel strategies for gpu threads play an important role in gpu-based me performance optimization. since the initialization stage involves a large number of independent computations, proper parallel strategies should be applied to improve me performance. the following are two parallel strategy alternatives which assign subtasks of di®erent granularities to gpu threads."
"stacking svms is used as another baseline method for comparison with rmdl, but this technique is used only for hierarchical labeled datasets. the stacking svm provides an ensemble of individual svm classifiers and generally produces more accurate results than single-svm models [cit] ."
"in this paper we proposed a novel image prior, namely nonlocal spectral prior (nsp), by analyzing the heavy-tailed distribution of singular values of matrices constructed by nonlocal similar patches. the nsp builds a bridge between spectral analysis and image prior learning."
"in this section, experimental results are discussed including evaluation of method, experimental setup, and datasets. also, we discuss the hardware and frameworks which are used in rmdl; finally, a comparison between our empirical results and the baselines has been presented. moreover, losses and accuracies of this model for each individual rdl (in each epoch) is shown in figure 5 ."
step 2 assigns a thread block of 256 threads for computing bdmðx; yþ. note that the searching window of me in the initialization is di®erent from the sm introduced in eq. (3). the size of searching window is a two-dimensional 16 â 16 neighborhood.
"since motion estimation based on bdm involves huge number of independent distance function computations, we can resort to gpu platform to parallelize such computations in the initialization stage. we organize pixels of an image using a two-dimensional matrix and assign each pixel in the image with a thread block. after the bdm calculation of pixels, a thread is allocated in the searching window of the second reference image for the purpose of distance function calculation. in this way, each thread conducts the calculations of distance function and the pixel with the minimal distance is reckoned as the counterpart. during the initialization stage, a cuda kernel function is invoked to generate motion¯elds for each section. limited by the searching method itself, the boundary motion vectors of each section cannot be decided until the optimization stage. based on eq. (8), we can calculate the bdm of every pixel."
"step 6¯gures out the quadrant that has the most motion vectors. we use the function quad(mv) to obtain the quadrant index (i.e., 0, 1, 2, 3) of the motion vector mv. assuming that the kth quadrant has the most motion vectors, step 7 checks whether quad(mf(x; y)) equals to k, where mf(x; y) indicates the motion vector of pixel pðx; yþ. if they are not equal, we will set the°ag of mf(x; y) to true. the true°ag indicates that the achieved motion vector mfðx; yþ is not accurate and it needs to be corrected and optimized in the optimization stage as described in sec. 3.3."
this work is implemented in python using compute unified device architecture (cuda) which is a parallel computing platform and application programming interface (api) model created by nvidia.
note that motion vectors on the boundary of one subblock may have di®erent directions compared with other within the same subblock. should be set to a relatively small value in order to follow the motion trend in the subblock.
"therefore, in order to estimate the unknown image with high quality, we can apply the nsp model learned from natural images to the estimation of x:"
"when images are not divided, the calculation workload of bdms could be enormous. to enable parallel me processing, our approach divides images into multiple sections before transferring them into device memory during the initialization stage. one key issue in our approach is the granularity of the divided sections. since the size of the search window in our approach is 16 â 16, images should be divided into no more than 16 sections. this is because the size of each section is smaller than the search window if we divide images into more than 16 sections, which can lead to a large deviation of boundary motion vectors. during the optimization stage, images should be divided into more than four sections, which can facilitate the computations of energy function. moreover, the number of sections should be in the form of a square number. in other words, we only have two division choices for our approach, i.e., 9 and 16. table 1 presents the performance comparison results under di®erent division strategies. strategy d1 denotes the case of dividing images into nine sections, while the strategy d2 denotes the case of dividing images into 16 sections. the¯rst four rows of the table show the execution time details using the strategy d1, and the last four rows present the execution time details using the strategy d2. for each strategy, we investigate both the execution time for each stage and the overall me time. from this table, we can¯nd that strategy d2 outperforms strategy d1, since me using strategy d2 consumes less time. for example, strategy d2 only needs 47.33 s to get the me results of video \\649dd10\" while strategy d1 takes 51.09 s. this is because both strategies do not calculate the boundary motion vectors during the initialization stage. since strategy d2 divides images into more sections, the initialization calculation workload can be largely reduced. furthermore, the boundary motion vectors that are not initialized can be obtained rapidly in the optimization stage. therefore, the strategy d2 is a better choice for our approach. note that all the experimental results are obtained based on strategy d2."
"strategy p1: for each pixel, we assign a thread. in other words, each thread computes all the distances (using distance function) between the speci¯ed pixel and all the pixels in the corresponding searching window."
"to enable accurate modeling and e±cient calculation of motion¯elds for image sequences with the presence of both slight and drastic brightness changes, this paper proposes a novel three-stage me approach based on gpu. it makes three major contributions as follows:"
"to evaluate the e±cacy of our proposed approach, we conduct experiments on multiple series of real images. all the benchmarks are collected from a well-known dynamic texture library named dyntex, 22 which is a large database of high-quality videos. we developed a cuda-based me tool which implements all the three proposed stages based on cþþ programming language. our experiment was carried out on a windows machine with 3.30 ghz intel i5 cpu and nvidia gtx 645 gpu (with 576 cuda cores). to show that our approach outperforms existing methods in terms of me quality, sec. 5.1 makes comparisons with two state-of-the-art methods. 23,24 section 5.2 discusses the performance issues in order to demonstrate the performance of our gpu-based me approach."
"(ii) the second stage (i.e., denoise stage) scans the previous obtained motion¯eld to identify potential inaccurate vectors. (iii) based on our proposed energy function, the third stage (optimization stage) corrects and optimizes the inaccurate motion vectors identi¯ed during the denoise stage. the following subsections will give the details of our three-stage me method."
"recently, [cit] grouped the similar patches across spatial-temporal domain to form a low-rank matrix, and then presented a powerful nonlocal-based video denoising algorithm by using the recently developed low-rank matrix recovery (lrmr) technique [cit] implemented cartoon-texture separation by interpreting texture in low-rank patches. taking advantage of lowrank interpretation, their method can effectively separated noise from texture."
"in our sequential me approach, we calculate the results of distance function between an original pixel and all the candidate pixels within the searching window of the reference image. the candidate pixel with the minimal result of distance function is reckoned as the desired pixel. since each calculation process of each pixel is independent and the calculations share the same procedure, we can naturally parallelize this process using gpu."
"we evaluate the performance of the proposed nsp model for various image restoration tasks, including denoising, deblurring and super-resolution. for each task, we compare our method with state-of-the-art algorithms designed for that application. due to the limit of page length, we only list the psnr results and show one image for visual comparison in each task. more results and source codes are provided in our website: http : //www4.comp.polyu.edu.hk/ ∼ cslzhang/nsp.htm."
"all of the results shown in this paper are performed on central process units (cpu) and graphical process units (gpu). also, rmdl can be implemented using only gpu, cpu, or both. the processing units that has been used through this experiment was intel on xeon e5-2640 (2.6 ghz) with 12 cores and 64 gb memory (ddr3). also, we have used three graphical cards on our machine which are two nvidia geforce gtx 1080 ti and nvidia tesla k20c."
"(i) we propose a novel data structure named brightness distribution matrix (bdm), which can be used to accurately model the brightness of regions. during the initialization of motion¯elds, bdm can be used to e®ectively calculate neighboring pixels based on our de¯ned bdm consistency constraint. (ii) we introduce an e±cient energy function which can be used to correct and optimize inaccurate motion vectors identi¯ed during the denoise of initial motion¯elds. by using this function, we can obtain motion vectors in a more accurate way. (iii) we parallelize our proposed me algorithm based on gpu, which can reduce the overall me time."
"many image restoration problems such as denoising, super-resolution and deblurring are inherently ill-posed inverse problems. solving these low-level vision tasks often needs regularization to yield high-quality results. therefore, natural image prior models, which describe the 'true' statistics of natural image, play an important role in image restoration. the past decade has witnessed the rapid development on image prior modeling [cit], and these prior models can be categorized into several categories: gradient (derivative, edge) based [cit], filter-bank based [4, [cit] 13], transform based [5, [cit], etc."
"since the consistency constraint can be easily violated in real images, a distance function can be used to approximate the e®ects of consistency constraint. the distance function is in the following format:"
"the natural way to solve k-class problem is to construct a decision function of all k classes at once [cit] . another technique of multi-class classification using svm is all-against-one. in svm, many different methods are available for feature extraction such as word sequences feature extracting [cit], and term frequencyinverse document frequency (tf-idf)."
"motion estimation (me) of°uid°ows has been widely studied in many areas. for example, in the¯eld of pattern recognition, the motion¯elds derived by me techniques can be used to support facial expression recognition."
"although our approach is more accurate than existing approaches in dealing with images involving both drastic and slight brightness changes, it is based on bdm which requires much more computations. to improve the me performance, we propose gpu-based implementation of our me method. to fully utilize the potential of gpu to enable the data-level parallelism, our approach divides images into small sections. therefore, the performance of our tool mainly depends on the image division strategy and the subtask parallelization strategy. this subsection investigates the performance of our approach under di®erent strategies."
"categorization and classification with complex data such as images, documents, and video are central challenges in the data science community. recently, there has been an increasing body of work using deep learning structures and architectures for such problems. however, the majority of these deep architectures are designed for a specific type of data or domain. there is a need to develop more general information processing methods for classification and categorization across a broad range of data types. while many researchers have successfully used deep learning for classification problems (e.g., see [cit] ), the central problem remains as to which deep learning architecture (dnn, cnn, or rnn) and structure (how many nodes (units) and hidden layers) is more efficient for different types of data and applications. the favored approach to this problem is trial and error for the specific application and dataset."
"the classification task is an important problem to address in machine learning, given the growing number and size of datasets that need sophisticated classification. we propose a novel technique to solve the problem of choosing best technique and method out of many possible structures and architectures in deep learning. this paper introduces a new approach called rmdl (random multimodel deep learning) for the classification that combines multi deep learning approaches to produce random classification models. our evaluation on datasets obtained from the web of science (wos), reuters, mnist, cifar, imdb, and 20newsgroups shows that combinations of dnns, rnns and cnns with the parallel learning architecture, has consistently higher accuracy than those obtained by conventional approaches using naïve bayes, svm, or single deep learning model. these results show that deep learning methods can provide improvements for classification and that they provide flexibility to classify datasets by using majority vote."
"4.1.2 text and sequences feature extraction. in this paper we use several techniques of text feature extraction which are word embedding (glove and word2vec) and also tf-idf. in this paper, we use word vectorization techniques [cit] for extracting features; besides, we also can use n-gram representation as features for neural deep learning [cit] . for example, feature extraction in this model for the string \"in this paper we introduced this technique\" would be composed of the following:"
"two types of datasets (text and image) has been used to test and evaluate our approach performance. however, in theory the model has capability to solve classification problems with a variety of data including video, text, and images."
"where x is the unknown clean image, h is the downsampling operator, k is the blurring kernel, n is additive gaussian white noise and y is the degraded observation."
"the novelty of this work is in using multi random deep learning models including dnn, rnn, and cnn techniques for text and image classification. the method section of this paper is organized as follows: first we describe rmdl and we discuss three techniques of deep learning architectures (dnn, rnn, and cnn) which are trained in parallel. next, we talk about multi optimizer techniques that are used in different random models."
2 ) where ρ is a constant to control the decay rate w.r.t. the distance between x i and x j . eq. (4) is a standard mrf-map framework and many methods [cit] can be used to effectively solve it.
"multi-class svm. the original version of svm is used for binary classification, so for multi class we need to generate multimodel or msvm. one-vs-one is a technique for multi-class svm and needs to build n(n-1) classifiers."
"deep neural networks' structure is designed to learn by multi connection of layers that each layer only receives connection from previous and provides connections only to the next layer in hidden part. the input is a connection of feature space with first hidden layer for all random models. the output layer is number of classes for multi-class classification and only one output for binary classification. but our main contribution of this paper is that we have many training dnn for different purposes. in our techniques, we have multi-classes dnns where each learning models is generated randomly (number of nodes in each layer and also number of layers are completely random assigned). our implementation of deep neural networks (dnn) is discriminative trained model that uses standard back-propagation algorithm using sigmoid (equation 8), relu [cit]"
optimizer. sgd has been used as one of our optimizer that is shown in equation 22. it uses a momentum on re-scaled gradient which is shown in equation 23 for updating parameters. the other technique of optimizer that is used is rmsprop which does not do bias correction. this will be a significant problem while dealing with sparse gradient. [cit] .
"this section presents the sequential version of our me approach in detail. in order to obtain motion¯elds for image sequences, we develop an me approach which consists of three stages: (i) based on bdm, the initialization stage calculates neighboring pixels based on our de¯ned bdm consistency constraint. in this way, we can obtain primitive motion¯elds from images."
"step 4 identi¯es the pixel in the reference image with minimal distance. since the process of minimal value¯nding involves lots of control°ow branches which can easily cause branch divergence, our approach uses cpu rather than gpu to¯nd the pixel with minimal distance in the reference image. steps 5 and 6 set the motion vector for pðx; yþ. finally, the algorithm returns an initialized motion vector for pixel pðx; yþ."
"in order to get more accurate me results while keeping motion consistency and movement continuity, proper me optimization methods 23 should be adopted. for example, the active contour model has been adopted in many me variants. conforming to active contour models, energy functions 13, 16, 17, 19 have been widely used in optical°ows. most of existing energy functions are in the following form:"
"deep neural networks (dnn) are based on simple neural networks architectures but they contain multiple hidden layers. these networks have been widely used for classification. for example, d."
"to provide the objective performance of our gpu-based approach, table 3 shows the total execution times comparison among four me algorithms. the¯rst two rows of the table show the execution times using two state-of-the-art methods proposed in refs. 23 and 24, respectively. the third row shows the execution times using the sequential version of our me approach which is based on cpu. compared to the works in refs. 23 and 24, the total execution time of our sequential me method is quite long, though it can achieve me results with better accuracy. to reduce the overall me time of our approach, the fourth row gives the me execution time results using the parallel version of our me approach based on gpu. from this table, we can¯nd that our gpu-based approach can reduce the overall me time. compared to its sequential counterpart, the gpu-based approach can achieve an improvement of up to 7.06 times."
"image restoration aims to recover x from the degraded image y, given kernel k and the distribution of random noise n. in case k is unknown, it will be a blind image restoration problem and we could estimate k before estimating x, or estimate them alternately. in this work, we assume that k is known. the most popular approach for image restoration is to conduct maximum-a-posterior (map) estimation of x:"
"classification methods and techniques: over the last 50 years, many supervised learning classification techniques have been developed and implemented in software to accurately label data. for example, the researchers, k. [cit] and i. [cit] introduced the naïve bayes classifier (nbc) as a simple approach to the more general respresentation of the supervised learning classification problem. this approach has provided a useful technique for text classification and information retrieval applications. as with most supervised learning classification techniques, nbc takes an input vector of numeric or categorical data values and produce the probability for each possible output labels. this approach is fast and efficient for text classification, but nbc has important limitations. namely, the order of the sequences in text is not reflected on the output probability because for text analysis, naïve bayes uses a bag of words approach for feature extraction. because of its popularity, this paper uses nbc as one of the baseline methods for comparison with rmdl. another popular classification technique is support vector machines (svm), which has proven quite accurate over a wide variety of data. this technique constructs a set of hyper-planes in a transformed feature space. this transformation is not performed explicitly but rather through the kernal trick which allows the svm classifier to perform well with highly nonlinear relationships between the predictor and response variables in the data. a variety of approaches have been developed to further extend the basic methodology and obtain greater accuracy. c. yu et. al. [cit] introduced latent variables into the discriminative model as a new structure for svm, and s. tong et. al. [cit] added active learning using svm for text classification. for a large volume of data and datasets with a huge number of features (such as text), svm implementations are computationally complex. another technique that helps mediate the computational complexity of the svm for classification tasks is stochastic gradient descent classifier (sgdclassifier) [cit] which has been widely used in both text and image classification. sgdclassifier is an iterative model for large datasets. the model is trained based on the sgd optimizer iteratively."
"equation (8) shows the combination process based on the cosine similarity whereφ k is an aggregated topic, n is the number of similar topics, m is the number of models, t i is the number of topics in a model, ϕ (i,j ) is the φ distribution for topic t j in model m i, ϕ x is the xth φ distributions from the base model, and γ is the similarity threshold."
"in addition to updating the psf estimates (when so desired) the frc measures of the intermediate estimates were used to observe the progress of the rl deconvolution: the deconvolution was considered fully converged, when effective resolution reached its maximum value. we also defined an alternative threshold for stopping the iteration, based on the rate of change of the effective resolution (1µm/it) in our example. we compared our metric against two previously published ones τ 1 and η k [cit] ."
"the results for the presidential debate social media dataset can be seen in table 7 . again, in this domain the aggregated topic model outperformed nmf but not by as alleviate this problem by bringing similar terms from other topics in to replace noise which is not prevalent in other topics, leading to a higher coherence."
"to summarise, the proposed aggregated topic model method was tested in three experiments (experiments one and two served as a proof-of-concept and experiment three as a real world example). all experiments showed the aggregated topic model improved topic coherence by a statistically significant amount. this work proposes a novel solution for aggregating topic models that can improve the coherence of the topics produced. the experiments conducted demonstrate that the coherence has been improved through aggregating topic models. the experiments show that the coherence is improved after creating a number of models with different numbers of topics or different parameters and applying the aggregation technique. the experimental results provide an insight into a conjecture of the improvement that when models are created with different numbers of topics, they create a mix of general, as well as more focused, specific sets of topics as the number of topics increases. the advantage of this is that the aggregated models have more general topics which lead to the aggregated model being more representative of the corpus it was generated from as shown by the intrinsic coherence results. it is also observed that jensen-shannon divergence generally gives better results than cosine similarity. this could be because jensen-shannon divergence assesses if two distributions were drawn from the same underlying distribution rather than simply assessing similarity, as is the case with cosine similarity."
"research on the pmi has shown that it tends to have a high degree of correlation with a human evaluator's assessment of topic coherence, however, a normalized version of the pmi approach is able to produce an even higher level of correlation with human judgement (npmi) [cit] . it has been shown that for social media data, using a large collection of random tweets as external data and a modified version of pmi finds high coherence, however, if such a large collection of twitter data is not available then using wikipedia as external data with a pmi based approach is most closely aligned with human judgement [cit] . the use of the pmi with a general external corpus allows for calculating how frequently words in the topic occur together. because of a general corpus, it can be loosely seen as how a human would interpret the topics from everyday language."
"the results for the associated press dataset can be seen in table 6 . this table shows the intrinsic and extrinsic coherence for all topics generated by nmf, along with the mean coherence and mean aggregated topic model coherence. as it shows, intrinsically, both models performed quite low, however, the aggregated topic model was more coherent by almost double nmf's coherence. the most interesting result is how much more coherent the aggregated model was extrinsically compared to nmf, more than seven times more coherent. from empirical observation it appears that nmf gives higher weight to fewer words in topics compared to the lda models used to create the aggregated topic model. additionally, it appears as though nmf does not capture advanced lexical devices such as polysemy as good as lda. these could be contributing factors to the lower coherence score."
"we then subsequently calculate the number of reductions for the warps, smxes, and entire gpu for the target window, using the timing model described in section 4. note that in our attack model, we are attacking window-by-window, and the calculated number of reductions is only for the current window. while in section 4.2, the linear correlations are for the sum of the number of reductions across all windows. in our attack, we retrieve the private key window-by-window, where each window will have a lower correlation coefficient than the correlation across all h windows, as shown in figure 1 . in our experiments, the average pearson correlation of attacking one window is about 0.02 when using 8 messages in one rsa-1024 decryption running on an nvidia tesla k40 gpu, for 100k traces. the correlation value is much lower than that in figure 1 . however, it suffices to extract the correct key."
"in this work, we evaluate the vulnerability of an rsa gpu implementation to side-channel timing analysis. we build timing models for a sliding-window rsa implementation using montgomery multiplications. we consider a parallel implementation and evaluate it on an nvidia gpu. we characterize the execution behavior, capturing the execution time due to the complex interactions among threads. our proposed attack methods are designed based on the timing model. we obtain attack results on an nvidia k40 hardware for different key sizes and work across different levels of parallelism. our results show that an rsa implementation on a gpu is vulnerable to side-channel timing attacks, calling for both general countermeasures and gpu-specific countermeasures."
"in conclusion, the first group of operations on m temp are l squaring operations. after that, if the window is zero, the subsequent operations involve l x squarings; if the window is nonzero with a value of v x, the subsequent operations are multiplications with c v x and l squarings, as shown in figure 2 ."
"deconvolution can be performed in a single step, e.g. by wiener or tikhonov filtering [cit] -or then iteratively, e.g. by richardson-lucy (rl) [cit] . in this paper we use the wiener and rl algorithms."
"although methods such as bayesian optimisation can be used to optimise parameters, it is unnecessary for this task which, due to its nature, can be easily parallelised regardless suffering from the curse of dimensionality. this makes grid search a feasible option without overcomplicating the problem by using more complex methods."
"note, we do not include the extra reductions of the multiplication operation (line 9 of algorithm 1). given that the probability of executing a reduction depends on the value of c f i, as expressed in equation (3), there may be other windows with the same value of f i . this kind of dependency on an extra reduction in multiple windows will bias the attack result [cit], and an effective attack should not use the reductions associated with the multiplication."
"the main contributions of this work include: (1) a hierarchical timing model for an rsa implementation on a gpu with montgomery multiplication and sliding-window exponentiation, explicitly capturing various complex interactions in a massively parallel computing platform; (2) a successful correlation timing analysis attack based on the proposed timing model to extract the private key; (3) an effective error correction algorithm designed to detect and correct attack errors to improve the success rate; (4) a success rate analysis for success rate prediction with the quality of side-channel information obtained from the gpu."
"the aggregated topic model method has been evaluated in four experiments. the first two experiments makes use of a set of 2246 associated press articles that were used in the original lda paper. the third experiment uses a set of 300,000 [cit] election campaign. using these datasets a series of experiments were carried out by creating a series of topic models with varying parameters, such as α priors, β priors, and number of topics. using these models aggregated topic models were constructed for each experiment, and the effect of altering the similarity thresholds in constructing the aggregated model was observed. when calculating topic coherence, an external corpus − the full set of english wikipedia articles − was used, which is part of calculating coherence. the last experiment is a comparative study on the aggregated topic models with the non-negative matrix factorisation with the same setting as in the first two experiments."
", where f 1 (r, α, φ) and f 2 (r, α, φ) denote the voxels in two fourier transformed images that are located (i) at a given distance r i from the origin and (ii) within an orientation sector, defined by α and φ. we compared the sfsc measures against fpc [cit] as well as frc measures."
"the rest of the article is organized as follows: first, we discuss related work on gpu security in section 2. in section 3, we briefly introduce the rsa cipher, the optimizations, and a parallel implementation of rsa developed in cuda and run on an nvidia gpu [cit] . in section 4, we construct the timing model and propose a timing attack based on the model. we introduce the success rate analysis in section 5 and present both empirical and theoretical success rates of attacks with different key sizes. in section 6, we address the issues of attack error detection and correction. the experimental results are presented in section 7. in section 8, we propose effective countermeasures to protect rsa on gpus. conclusions are drawn in section 9."
"we plug in the distribution of r h − r i into equations (7) and (8), k i, j and κ i can be calculated for window length l h, and the success rate sr l h using equation (6) . note the value of k and κ are dependent on the correct window length l h . the success rates for attacking zero windows are plotted in figure 4(b) . results show that the success rate of attacking zero windows is much lower than nonzero windows with the same number of traces, indicating that our ability to distinguish zero windows is much lower than that of nonzero windows. in the attack of a nonzero window, the wrong guess of a window value will yield a totally incorrect number of reductions. for a zero window, if the window-length guess is close to the correct window length, the reduction number calculation is also very close. as a result, it is much harder to differentiate the correct window length from those close to it."
"typically resolution is estimated by measuring either the minimum resolvable distance between two adjacent structures in an image -as per the classical rayleigh/abbe/sparrow resolution definitionsor, alternatively it can be estimated from the full-width-half-maximum (fwhm) width of sub-resolution spatial structures. in order to perform either one of the two measurements, suitable structures need to be subjectively identified and manually measured; ideally, the measurements should be repeated at several positions to gain some statistical basis for the estimate. this task is both tedious as well as error-prone."
"we extend our previous work [cit] by adding the analysis of rsa with vlnw. attacking vlnw is more challenging than attacking clnw, because the length of the nonzero window is no longer constant. to accommodate for this issue, we do not separate attacking zero and nonzero windows anymore. instead, we combine the nonzero window with its preceding zero window as one attack unit as shown in figure 3 . if a nonzero window is preceded with another nonzero window, itself is an attack unit. in the attack, we first target the length of one attack unit, l x, which equals to the length of the nonzero window, adding the length of the preceding zero window, if it exists. then we target the value of the attack unit, v x, which is also the value of the nonzero window. by knowing the length and value of each attack unit, we can reconstruct the private key."
"in (1) p(w i, w j ) is the probability of two words cooccurring in a document, as seen in (2); p(w i ) and p(w j ) is the probability of word w i and word w j occurring in the document, respectively, as seen in (3). in (2) and (3) d ext represents the number of documents in the external corpus that contain either one or both words, depending on the calculation. the external corpus is a set of general documents which allow for the calculation of coherence based around general english usage."
"if the window is a nonzero window, we guess its value as v x (v x is an odd number). the following operations on m temp are l squaring operations and one multiplication by c v x . beyond the multiplication, we can still infer the operations by checking the next one or two windows. if the next window is nonzero, the operation beyond the multiplication is another l squarings, because the length of the next nonzero window is l. if the next window is zero, the window after the next window must be nonzero, and we have more than l squaring operations after the multiplication, consisting of the squarings of the next zero window and l squarings of the next nonzero window."
"in this section, we propose an attack method for the rsa gpu implementation based on the timing models we built in section 4. we assume the ciphertext message is random but known to the attacker. for each decryption run, the adversary only records the total execution time."
"to calculate r warp (i), we use a binary number, b msд (i, j), to record the extra reductions during execution, for the jth message in the ith exponent window. the length of b msд (i, j) is l(f i ), because in each window there are l(f i ) squaring operations, and each bit of b msд (i, j) is for the corresponding squaring. if the squaring of a message results in an extra reduction, this bit is 1; otherwise, it is 0. for each squaring operation, all the messages in a warp are synchronized (i.e., as long as there is one message resulting in an extra reduction, the warp (all the messages) encounters a delay). therefore, all b msд (i, j) should have bit-wise or operations over j, and the number of reductions in the warp, r warp (i) is the hamming weight of the bit-wise or result. the number of reductions across the warp for the entire decryption r warp is also the sum of r warp (i) over i."
"an example of an indirect extrinsic approach to calculating topic coherence is to assess the detection of intruder words in a topic [cit], this is essentially the inverse of previous work to detect the word that was most representative of a topic [cit] . this method works by finding association features for the top n words in a topic (intruder word inclusive) and then combining the results using svm rank (support vector machine) to discover the intruder words. it was discovered that this method achieves a high level of correlation with human evaluators. the disadvantage of this method is that it requires manual placing of intruder words in the topics."
"special countermeasures targeting gpu implementations can also be devised. for example, we can randomize the assignment of messages to threads and blocks such that the execution time cannot be correctly predicted by the attacker. with a randomized assignment countermeasure, the correlation of one block reduces from 0.54, as shown in figure 1(c), to −0.0073, as shown in figure 8 . there is little computational overhead introduced here, except for a random permutation of a sequence. the permutation incurs randomized memory accesses, which impact the data spacial locality adversely. security and performance can be balanced by varying the assignment granularity for randomization. we can also add more noise to the timing measurements with dummy random messages, at the cost of performance and throughput. if we insert one dummy random message for every three messages, the correlation between the number of reductions and the execution time of one warp is also reduced from 0.66 to 0.12, which also introduces, at most, 25% overhead. this approach is suitable in scenarios where the gpu is lightly loaded, so additional dummy messages can be computed on the unused resources."
a system called collaborative topic regression has also been proposed [cit] . the proposed system has been used with social trust ensembles to provide a method for social media content recommendation. this system uses a modified version of lda and takes into account social influence from a user's trusted friends for recommending content. it performs this function by using a similarity metric to assess friends' similar interests to recommend content.
"previously there has been research into using jensenshannon divergence and kolmogorov-smirnov divergence to assess the similarity of topic probability distributions [cit] within the topic model's φ distribution, which is the word distribution in each topic."
"while the resolution estimation certainly is an interesting application in itself, in our view, the true potential of frc/fsc is in much more practical tasks. in this paper we show several examples of advanced image restoration methods that leverage frc/fsc measures. we apply frc to perform image de-noising by frequency domain filtering. we propose novel blind linear and iterative image deconvolution 0 abbreviations: frc: fourier-ring-correlation: fsc: fourier-shell-correlation; sted: stimulated emission depletion; sfsc: sectioned fsc; fwhm: full-width-at-half-maximum; rl: richardson-lucy; wf: wiener filter; snr: signal-to-noise ratio methods that use frc/fsc measurements to estimate the effective point-spread-function (psf) of the microscope, directly from the images, with no need for prior knowledge of the instrument characteristics."
"the process for aggregating topic models can be seen in algorithm 1. in this algorithm k is the set of aggregated topics, t is the set of base topics, sw is the sliding window size, γ is the similarity threshold, andφ is an aggregated topic."
"because sub-sampling inevitably leads to loss of information, one might be keen to think that such a method is only feasible on significantly oversampled data, i.e. with sampling much higher than nyquist,"
"in (figure 4b) ) the same blind wiener filtering approach was applied to a much larger (deep) image of pollen recorded with a confocal microscope. only single image was available for analysis, so the diagonal splitting was used in the sfsc calculations. the deconvolution results show dramatic improvement of contrast and details -and axial haze is effectively reduced, as indicated by the depth coloring."
", where o k and o k+1 are the current and next object estimates, i is the original image, h is the psf and h * its mirrored version. in the equation, the pixel indexes have been omitted to allow a simple presentation of the algorithm."
"an rsa cipher uses a pair of keys, one public and one private. the public key (n, e) is used for encrypting the plaintext message m, and the private key, (n, d ) is used for decrypting the ciphertext c:"
"backward error correction is triggered when there are n consecutive window attacks with the correlation coefficients below c th . the attack rolls back to the first window of the n consecutive windows. this time, we choose the value for this window with the next highest correlation and then proceed. if this still results in an error and the window is attacked again, the next ranked guess should be chosen next. if all possible values for the window have been attempted and there is still an error, we move further backward to the previous window and reconsider other guesses."
"textual data is difficult to analyse due to their varied linguistic characteristics and semantics. a method that attempts to identify the underlying topical structure of textual data is topic models such as latent dirichlet allocation (lda) [cit] . topic models are a type of statistical and unsupervised model that can quickly discover the latent topics within large corpora of documents. when generating a topic model by lda, a number of parameters have to be set, which have effects on the output of topics. without prior knowledge of the corpus being modelled, setting a small number of topics will result in very broad topics that contain mostly common words and stopwords. however, when the number of topics is set too high, the models generated will be very granular and overfitted to the corpus. other parameters, such as α and β dirichlet prior values in lda, are also crucial in generating a good quality model [cit] ."
"topic coherence can be defined as how interpretable a topic is based on the degree of relevance between the words within the topic itself. the topic coherence measures used in this work aims to evaluate the quality of topics from a human-like perspective. considerable studies have been carried out in the evaluation of statistical topic models, however, it has been found that those methods are not always reflective of how a human views the topics [cit] . consequently, it was revealed that metrics based on word co-occurrences and a mutual information approach are more representative of how a human would approach topic evaluation [cit] . it is for this reason that we use these word co-occurrence and the mutual information approach in this work."
"in order to perform frc/fsc analysis on a single image, one needs to find a way to form statistically independent image subsets that share the same details, but different noise realizations, by some form of sub-sampling. as described in (figure 1a) ) we propose to do this by dividing a single image into four subsets, i.e., two image pairs. the first pair is formed by taking every pixel with (even, even) row/column indexes to form one sub-image and (odd, odd) indexes to the other. the second image pair is formed from pixels at (even, odd) and (odd, even) indexes. frc can be calculated from either one of the image pairs alone, but we noticed that averaging two measurements helps to deal with special spectral domain symmetries (figure s. 1). with 3d images (fsc) the same splitting method is used, except that in the axial direction (z) layers are summed pairwise to maintain image proportions; we get back to the 3d measurements later."
"in order to examine the overall coherence of the topics generated in the new aggregated model, we propose to use two intrinsic and extrinsic measures. the first coherence measure allows us to assess the coherent extent that topic models accurately represent the content of a corpus used to generate the topic models based on word co-occurrences (wco). the extrinsic measure allows for the examination of the generated topics against a corpus of general english documents to ensure that the topics are generally coherent in daily language, which is similar to how a human observing the topics would decide whether they are coherent or not. moreover, a statistical significance test has been calculated to examine how the aggregated model is statistically different from the base model."
"the cosine similarity has more flexibility in setting a similarity threshold and is also not invariant to shifts in input as opposed to measures such as pearson correlation coefficient which is invariant to input shifts. the upper and lower bounds of the cosine similarity are 1 for complete similarity and 0 for complete dissimilarity. the process for aggregating topics with the cosine similarity is described as follows. firstly, the user needs to define a set of parameters that will be used to generate the base models. a threshold for similarity will then have to be set using a grid search with training data sets to see which threshold can produce the most coherent topics on average. although this may be seen as computationally expensive, on modern hardware these parameter tuning processes using a subset of training data are relatively quick to conduct and easy to run in parallel. each topic from each of the other models is then compared to the base topics in a pairwise way in order to examine their similarity. if the cosine similarity of the two topics is above the threshold they will be then combined, which is to combine the similar topics via calculating the mean probability of each word in the φ distributions. it should be noted that the number of topics in the base model does not increase or decrease, nor does the number of words in the topic as the alphabet for each model is the same."
"the algorithmic version of the process for choosing the similarity threshold and sliding window size is visible in algorithm 2 where is the set of similarity thresholds to be tested, sw is the set of sliding window sizes to be tested, opt sw is the current optimal sliding window size, opt γ is the current optimal similarity threshold, max is used to track the maximum coherence found, t is the subset of the dataset used for testing,φ is an aggregated topic, and k is the set of aggregated topics."
"in the attack, we make a guess of the target window. based on the guess, we calculate the corresponding number of reductions. we compute the pearson correlation coefficient between the number of reductions and the observed timing information. to determine the correct value for a nonzero window or length for a zero window, we iterate over all possible guesses for the target window and pick the one with the largest correlation based on our attack results."
"for countermeasures against gpu timing attacks, [cit] introduced randomized coalescing techniques to mitigate the gpu coalescing attack [cit] . due to the degree of indeterminism introduced, the correlation between the execution time and memory accesses is reduced or eliminated. however, the timing leakage we target in our attack does not relay on coalescing memory accesses, which renders these countermeasures non-applicable here. however, there have been several countermeasures presented that protect a cpu from a side-channel timing attack while running rsa. they attempt to eliminate data-dependent variations associated with the reductions [cit] or mask the messages and exponent with random numbers [cit] . such countermeasures are general and are also applicable to gpu implementations. however, they will introduce a significant performance degradation, which is a major concern for gpu acceleration. we evaluate an always-reduce montgomery multiplication countermeasure on a gpu, which increases the execution time by 4.5%. we also compare the pearson correlation coefficient between the number of reductions and the execution time for one message and show the result in figure 7 . the correlation is significantly reduced from 0.79, as shown in figure 1(a), to 0.06, rendering the time leakage very small and hard to exploit."
"in the modern era of digital technology and with the advent of big data, there is unrivalled access to masses of data that would have been unimaginable in the past. one of the challenges faced is how to extract the underlying information from these masses of data. a particular interesting development of the internet has been the emergence of social networks, specifically microblogs. many internet users have been abandoning traditional methods of online communication such as blogs and newsgroups, in favour of social networks that enable microblogging, for instance, twitter. these microblog platforms have enabled millions of users to yaxin bi y.bi@ulster.ac.uk stuart j. blair blair-s4@email.ulster.ac.uk maurice d. mulvenna md.mulvenna@ulster.ac.uk 1 school of computing, ulster university at jordanstown, newtownabbey, co antrim, bt37 0qb, uk quickly and concisely express opinions about anything; from products to politics. for this reason these microblogs have become an invaluable source of information for many companies and institutions to gauge consumer opinion and help shape future product development or marketing campaigns."
"in order to help alleviate the problem of very general or very specific topics that could be generated using nonoptimal initial parameters, we propose a novel aggregating method for combining topic models. this method allows a user to define a set of different parameters and with them to generate multiple topic models. the topics that are found to be similar amongst these models are then aggregated."
"however, the security of rsa on a gpu has not received adequate attention. side-channel vulnerability (i.e., private key retrieval through side-channel analysis) is a major concern on cryptosystems. meanwhile, various side channels, including power, electromagnetic emanation, and timing, have been exploited for implementations of rsa on other computing platforms, such as cpus, fpgas, asics, and mcus. among these attacks, timing attacks have become a realistic cyber threat given that they are non-invasive and can be launched remotely. many timing attacks of rsa deployed on cpus are micro-architectural (cache-based) attacks [cit] . for example, in access-driven attacks such as prime+probe [cit] and flush+reload [cit], a spy process first sets the cache into a known state, the victim rsa process executes, and during the run the spy measures the timing of its memory accesses to infer the cache access pattern of the victim and recovers the secret key. we anticipate that applying such cache attacks on a gpu could be very challenging, if not impossible, due to the high degree of noise resulting from the massive parallel execution model of a gpu. these devices launch thousands of concurrent threads, so the victim would not be a single process, but a kernel that runs across threads on streaming multiprocessors (smxes). the cache access pattern would be a complex mix of many copies of the same kernel, and therefore the spy cannot easily attribute a particular cache access to a specific victim thread. each smx has its own sets of caches, so the only cache leakage possible is when the spy kernel and victim kernel are on the same smx. however, the victim kernels may be thread-switched out of the smx by the hardware scheduler, which attempts to maintain high utilization. this results in a significant amount of noise in the cache access pattern that the spy kernel observes."
"in this section, a guidance law is designed in consideration of the impact time and angle constraints. the objective of this guidance design is to achieve target interception with desired impact time and angle constraints. the stability analysis is provided as well."
"parameter selection: in our error-detection method, there are three parameters, d th, n, and c th . their values affect the effectiveness and convergence speed of the attack. for d th, if it is set too high, the forward error correction will be launched too often (false positive) and the attack will be impacted. if it is too low, the chance of overlooking an error is high (false negative). the error is only detected by the backward error correction, which is much slower than the forward one. so, a suitable d th should balance the speed of the attack and the error-detection rate. similarly both error-detection rate and attack speed will be affected by the n value. the choice of c th has a similar impact as n, but in the opposite direction. this means a larger c th is equal to choosing a smaller n . in our experiments, these values are chosen to maximize the success rate, with the attack speed in an acceptable range."
"with both wiener filtering and rl, psf estimate was generated on the basis of an frc measurement on the original image data: the frc resolution value was simply used as an fwhm value for a gaussian psf. with 3d images separate values were used for lateral and axial directions. in the adaptive rl algorithm we updated the psf during rl iteration, in which case a frc measure was taken after each iteration step and a new psf was generated according to the new fwhm width."
a method for modelling consensus topics across a range of contexts has been proposed [cit] . that method implements a co-regularisation framework to create pseudodocuments using a centroid-based regularisation framework to make topics from different contexts agree with each other based on a set of general topic distributions. it allows for context specific topics to be bridged together by the general topics. that method outperformed the standard lda in terms of topic coherence when modelling tweets.
"this experiment resulted in some noticeable difference between the base model topics' top words and the aggregated model's top words. a comparison between the base tables 1 and 2, respectively. in t 1 the aggregated model has additional words including \"nicaragua\" and \"contra\"; this supplements the words from the base model, \"united\" and \"states\". it would be logical to connect these words through the nicaraguan revolution, when the united states supported the contra forces in a campaign against the sandinista national liberation front. another major change can be seen in t 7 where the aggregated model contains more words about medical research and disease, whereas the base model includes some less relevant words such as \"children\", \"percent\" and \"space\". additionally, t 8 sees the addition of the words \"index\" and \"exchange\"; this makes it more obvious that this topic is about stock markets and finance. the aggregated model also allows for more subtle changes such as the addition of jackson in t 6, [cit] democratic presidential primaries."
another important area of further work is to present the base models and aggregated models to humans and have them to rank the topics based on human's perception. this will allow for examining the correlation of the coherence of the aggregated model with human opinion.
"in this study, the topic model that will be utilised when creating the base models in experiments is lda. lda is a generative probabilistic model that finds latent topics in a collection of documents by learning the relationship between words (w j ), documents (d i ), and topics (z j ). the data used by an lda model is input in bag-of-words form, word counts are preserved but the ordering of the words is lost. the only observed variable in the model are the words w j, everything else is a latent variable. the generative process for document d i assumes the following:"
"the iterators produce the sequence of 3d indexing structures that are needed to extract voxels on a given shell/section from the fourier space image for fsc calculation. the iterators were designed to be interchangeable -any one of them can be used in the fsc implementation to produce the desired behaviour. in addition to the regular fsc and sfsc iterators, we also implemented special ones to exclude pixels in certain orientations -this is necessary as microscope images often contain artefacts generated e.g. by the mechanical movement of the xyz -scanning apparatus; the artefact become visible in the frc/fsc measures as higher than normal resolution values. excluding pixels in the direction of the optical axis seems to be especially important, as the axial scanning steps (piezo) as well as possible interpolation artefacts otherwise compromise the resolution measure. with each iterator the width of the sections, the thickness of the shells (bin size), as well as the width of the possible exclusion area can be freely selected."
to perform the aggregation cosine similarity and jensenshannon (js) divergence will be used to assess the similarity of topics' φ distributions. the φ distribution (ϕ) is a t * v stochastic matrix where each row is a topic (t ) and each column is a non-zero real number probability for a word from the vocabulary (v ). both methods will then be evaluated for performance.
"the wiener deconvolution algorithm is based on inverse filtering, and takes advantage of the fact that a convolution operation in the spatial domain, becomes a multiplication in the frequency domain -and thus a relatively simple single-step deconvolution can be realised as follows:"
"we first describe the details of the attack of clnw. to calculate the number of reductions for the ith window for one message, we assume there are at least two windows after the target window. because the window could be zero or nonzero, we divide our guess into side-channel timing attack of rsa on a gpu 32:9 two groups, the length of zero windows and values of non-zero windows, which differs from previous work [cit], which only works bit-by-bit. if the window is zero, its length, l x, is varying, and we tally the number of l x squarings i of m temp first. for the sliding-window algorithm, a zero window is always followed by a nonzero window. therefore, the next window must be nonzero of length l, resulting in another l squaring in the next iteration, but without any multiplication in between due to the zero window. from another perspective, the operations can also be considered as l squaring operations, followed by l x squaring operations."
"when calculating the number of reductions, we exclude the first l squaring operations, because they are common to all guesses. for a zero window, we record the extra reductions of the next l x squarings in r zer o (l x, i, j) for each message, where i is the window index, and j is the message index. for a nonzero window, we record the extra reductions for the next l squaring operations in r nonzero (v x, i, j), where the value v x is used to compute the multiplications c v x ."
"the main goal of this research is to construct an aggregated topic model that produces topics with greater topic coherence than the base models used to create the aggregated topic model. in this sense, topic coherence refers to evaluation methods that analyse words in each topic to ensure that they would make sense together from a humanlike perspective. this is in contrast to other statistical methods of topic model evaluation that may not necessarily reflect human evaluation. when generating topic models, a resulting model may produce very granular topics or it could produce very general topics populated with words that are common across all documents, which depends on how parameters are set in topic modelling. this research shows the theoretical framework of aggregating models, which is to combine several topic models in an ensemblelike approach with the goal of increasing overall topic coherence. the work has significant practical impact as it can be used directly in any topic modelling systems to increase topic coherence, especially noisy domains, such as social media."
"the comparison between nmf and aggregated topic models demonstrate that the aggregated topic model outperforms nmf in terms of coherence both extrinsically and intrinsically on both datasets. both modelling methods performed quite similarly intrinsically showing that they both capture the underlying topical structure of datasets well, however, the extrinsic results are extremely different. the aggregated topic model strongly outperforms nmf extrinsically. this reveals how the aggregated topic model brings similar terms into a topic from other similar topics to displace potentially noisy terms, thereby increasing coherence extrinsically which demonstrates that the topic should be coherent in daily english language."
"the remainder of this research article is structured as follows. section 2 provides a literature review of work related to this area and an overview of topic coherence. in section 3, an overview of standard topic models, such as lda, is given. next, in section 4, the theoretical framework of aggregated topic models is introduced, including analysis of similarity measures, and the full algorithms for finding similar topics and combining these similar topics. section 5 contains the three experiments conducted, this includes initial parameters used and tuning the aggregated model for each of the experiments. in section 6 the results of these experiments are evaluated and the significance of each result is presented. finally, in section 7, the work is concluded and a discussion of the aggregated topic models is presented."
", where f 1 and f 2 are the fourier transforms of the two images and r i the ith frequency bin. the image resolution in frc/fsc is defined from the histogram, as a cut-off frequency at which the crosscorrelation value drops below a pre-set threshold value. advantages of the frc/fsc are that it is fully automatic, quantitative and depends both on sample and microscope characteristics -it is also less prone to subjective bias and measurement errors, although the choice of the appropriate resolution threshold criterion still requires some input from the researcher, as no single solution seems to be correct in all applications [cit] ."
"an important aspect of the proposed method for aggregating topic models is the choice of similarity threshold. the overall problem attempting to be solved can be viewed as optimising the semantic topic coherence by searching the optimal similarity threshold and sliding window size. the sliding window size is directly related to measuring coherence as it sets the window size for calculating word probabilities. for example, if the word co-occurrence probability for word w i and word w j is calculated using a sliding window of size 50 words, then as long as the words occur at least once in the 50 word window it will count as the words having co-occurred, irrelevant as to whether they are in different sentences or paragraphs. however, if a lower window size such as 10 is used, it is stricter as it limits where the words can co-occur. this allows for more confidence that the words actually occurred together in the same context."
"this experiment is interesting as its topics are less changes than experiment 1, but the few changes result in noticeable increases in topic coherence. this could be because some topics in the base model are quite specific, but are generalised more in the aggregated model."
"the data used to generate the models in the following experiments is a set of associated press articles used in the eponymous lda paper, and supplied with david blei's ldac package. 1 the corpus contains 2246 documents and 34977 tokens after removal of stopwords."
"the sliding-window exponentiation method for decryption is described in algorithm 1, regardless of how the windows are partitioned. there are h windows of exponent d. to begin processing, the odd powers c w of the method have to be pre-calculated once (lines 1-4). the computation loop iterates over the most significant window. during each window (iteration), there are a number of squares computed (line 7), which is equal to the bit length of the window, followed by conditional multiplication (only if the window is non-zero) with the multiplicand determined by the window value, f i (line 9)."
"testing d n,m (where n and m are the sizes of two distributions x and y, and f n and f m are the empirical distributions of the x and y values, respectively) allows for the evaluation of a null hypothesis that states that x and y are samples from the same underlying distribution. proving that the null hypothesis is true allows for the assumption that two topics are very similar. despite the usefulness of using a two-sample kolmogorov-smirnov test in this situation, it has been decided that it is not a viable method for finding similar topics. although this decision may seem contradictory to what has been discussed, some initial tests using the two-sample kolmogorov-smirnov test gave disappointing results due to the two-sample kolmogorovsmirnov test needing a critical value to be calculated. when this critical value was calculated for the experiments, it resulted in the need for an exceptionally high similarity value between the two distributions, whereas other methods allow for more flexibility in setting the similarity threshold."
"the effort to launch a successful attack on vlnw is more than a successful attack on clnw for two reasons: first, for each attack unit, we have to decide its length, which is prone to error, as we discuss in section 6. second, to decide the value of an attack unit, we calculate the reduction number from the q + 1 squarings, which is usually a small number. therefore, the correlation when using the correct and wrong values is harder to differentiate."
"another evaluation method for topic models is the empirical likelihood measure; that evaluation method is used in the popular topic modelling library, mallet. a different approach present in the mallet library is left-to-right evaluation that produces similar results to the empirical likelihood but is less prone to overestimation [cit] . additionally, an evaluation method of utilising the harmonic mean [cit] has been proposed but it has been discovered to drastically overestimate the probability of heldout documents [cit] . it should be noted that despite widespread use due to ease of implementation, that method has been a source of criticism, even from its original authors [cit] ."
"the jaccard index can be used to find similar topics by simply calculating the similarity coefficient between the top n words in two given topics. a high value from the result of the jaccard index indicates that there is indeed some similarity between the topics. however, the downside is that a threshold for similarity needs to be set via introspection as there is no foolproof method of statistically assessing a similarity threshold."
"the former method makes it possible to enforce a given shape for the resolution curve, whereas the latter is more robust, especially with very noisy data, into which it is hard to reliably fit a polynomial without some sort of pre-filtering or cropping. we almost exclusively use the smoothed splines as they produce nice looking frc/fsc curves almost with any kind of data."
"we begin by considering only a single message being decrypted by multiple threads (e.g., 16 for rsa-1024, which are within the same warp). since threads in a warp are synchronized, we can still view the decryption for one message as sequential execution. for each window of d, we calculate the reduction number, as shown in lines 7-10 in algorithm 1. the first for loop (lines 2-4) is ignored here, because it is independent of the secret key d."
"the idea of using similarity measures to compare aspects of topic models has been studied in the past years. cosine similarity has been used to measure the similarity among the top words in topics generated using two different methods [cit], while jensen-shannon divergence has been used to compare the similarity of topic distributions over documents [cit] . the work in this paper differs from those previous works by using the similar topics by combining them to produce new topics; the previous studies used the similarity measure as an evaluation tool. in combining the topics, new topics are generated that take aspects from different models; this results in a new set of topics that may be more coherent on the whole. taking aspects from different models refers to how words that appear in specific topics may be introduced into a general topic to change the underlying topic structure and increase that topic's coherence."
"studies have also been conducted to bring together two separate lda models in a two-fold method, one model for aspects and one for sentiment [cit] . that method was also extended further to allow for multiple aspects to be present in a sentence as the initial version assumed only one aspect per sentence [cit] . the extended two-fold model which used the jaccard index to identify the number of aspects in a document outperformed the original two-fold model, producing more relevant results for the documents modelled but at the expense of there being slightly more results produced."
"extrinsic coherence measures can be split into direct and indirect approaches. direct approaches require the use of an external corpus to calculate the observed coherence whereas indirect approaches will use some other test, such as identifying an intruder word in a topic. in all cases, extrinsic coherence is a useful measure of how coherent a topic would be in daily language without any context."
"we implement the rsa cipher on an nvidia k40 gpu, which has 15 smxes and 192 cuda cores on each smx, hosted on a workstation running ubuntu 14.04.05 lts with an intel e5-1603 processor. we record the timing information using the cpu clock that runs at 2.8ghz. we use the cpu instead of the more accurate gpu clock, because normally it is not accessible to the attacker, and we want to launch a realistic attack scenario."
"to verify our timing model, we run multiple experiments of rsa-1024 on an nvidia k40 gpu. we set the block size to 128, such that each block will have 4 warps. we run experiments and vary the message number for one decryption from 1, 2, 8, to 64 with clnw. we will have 1 message, 1 warp, 1 block (on one smx), and 8 blocks (on 8 smxes), respectively. for each experiment, we run the decryption 10k times with random messages, record the execution time of each decryption using the cpu's clock, and calculate the reduction number for the workload according to the timing model. our results are shown in figure 1, with the execution time normalized and correlation coefficients (ρ) of execution time and reduction number shown at the top. the figure shows we obtain a higher linear correlation between the measured execution time and calculated reduction number for (a) and (b) when we have one message and one warp running, respectively. the points in (c) and (d) spread out more for multiple warps and multiple smxes due to the approximation of the reduction number for one smx. we also characterize the timing channel for the vlnw implementation, and the correlation results are similar."
this section details the four experiments performed using the aggregated topic model. experiments one and two were experiments designed to show the feasibility of aggregated topic models and prove their effectiveness when different topic model parameters were changed. experiment three shows a real world application of the aggregated topic model. in this experiment we applied the aggregated topic model to tweets about the third presidential debate. the last experiment is to compare the aggregated model with the algorithm non-negative matrix factorisation with the same setting as in experiments 1 and 2.
"for the complete decryption with multiple smxes, we assume the smxes are running independently, and the time of one decryption is determined by the slowest smx. the number of reductions associated with one decryption on the gpu is r дpu, which is the maximum of all the smx reductions, r smx ."
"one-image frc/fsc measures. for traditional frc/fsc analysis two images of the very same region-of-interest and with independent noise realisations are needed. these two images can be obtained in a point-scanning microscope by using sequential imaging on a line-by-line base. compared to frameby-frame sequential imaging, line-by-line allows overcoming potential differences between the two images induced by focus drift, sample dynamics, or photobleaching. similarly, sequential imaging on a pixel-bypixel, pulse-by-pulse and photons-by-photons basis can further decrease the differences between the two images, but needs more sophisticated (e.g. time-resolved) instrumentation [cit] . on the other hand, frameby-frame sequential imaging is the only option for wide-field microscopy, but high frame-rate is needed to ensure the sameness of the two images, particularly with live samples. especially with the image processing tasks in mind, in which the requirement of two images introduces a unnecessary computational overhead, and to make the frc/fsc analysis compatible with any microscopy architecture, we developed a method to calculate frc/fsc from a single image (one-image frc/fsc)."
"intrinsic coherence measures show how well a topic represents the corpus from which it was modelled. intrinsic measures utilize word co-occurrences in documents from the corpus used to train the model [cit] . the feature of the intrinsic method allows us to better judge the coherence of a topic based on training documents. the scale of the measure is in a range of 0 − 1, if a measuring value is closer to 1, that means that the model has correctly identified words that co-occur frequently in documents as a topic, but this cannot guarantee that they make semantic sense or that they are interpretable by a human; it simply means that the topics represent the data known to be in the corpus [cit] . using this method allows for the identification of poor topics for which word intrusions tests do not account. given the top n words in a topic, the word co-occurrence can be calculated as seen in equation (6) [cit] ."
"in this experiment m 1 is the base model, m 2 − m 10 are the other models to be compared, andm is the aggregated model. any model can be the base model, the fact that m 1 was chosen in this experiment is arbitrary. also, if a different model was chosen as the base, the same topic similarity comparisons would be made; the only difference of using m 1 over the other models in this case is the number of topics in the final aggregated model. as fig. 3a shows, the aggregated model has an extrinsic pmi value of 0.75, this is much higher than any of the model used to create it. this shows that the aggregated model's topics are much more coherent based on general english language. the aggregated model also has the highest intrinsic coherence. this means the aggregated model's topics have been complemented with additional relevant topic words leading to topics that are more representative of the corpus."
"the concept of aggregated topic models has been validated in experiments 1 and 2, now it can be evaluated over social media data to replicate a real use case. in this experiment 2,000 gibbs sampling iterations were performed and a β dirichlet prior of 0.01 was used. ten different models were generated with each having a different number on 19th [cit], the third presidential debate between democratic nominee hillary clinton and republican nominee donald trump took place at the university of nevada, las vegas. the debate lasted 90 minutes and had six topics split roughly into 15 minute segments. the topics chosen by the chair were on debt and entitlements, immigration, economy, supreme court, foreign hot spots, and fitness to be president. this debate was the climax of lengthy campaigns which were not without scandal and dishonesty from both parties. the candidates provoked dissent and discord amongst the american population and this was reflected on twitter. during the debate 300,000 tweets were captured using various hashtags and keywords used by supporters of each nominee. these can be seen in table 3 ."
"for decryption, the ciphertext comes in a montgomerized form. the decryption goes through a series of montgomery multiplications using the sliding-window algorithm discussed in section 3.1. inside a montgomery multiplication, there is a conditional reduction operation dependent on the values of a and b. because the extra reduction will take more execution time, this data dependency creates a timing side-channel attack surface. from prior work [cit], if a and b are equal and random, which makes it a squaring operation, the probability of the extra reduction is:"
"the secret key d is extracted one window after another, starting from the most significant one, following the same order as decryption, as shown in algorithm 1. for the ith target window, the previous attack results are used to calculate the value of the intermediate variable m at the beginning of ith iteration. to avoid confusion with m in other steps in the algorithm, we denote it as m temp . for the first window, m temp is 1."
"where t d represents the desired impact time, t is the time elapsed after launch of the missile, t go denotes the time to go. the time to go estimation is given by:"
", where i(x, y, z), h(x, y, z) and o(x, y, z) are the measured image, the psf and the original sample object, respectively. by image deconvolution [cit] one attempts to revert the blurring effect of the microscope, and thus increase image contrast and effective resolution, by using the psf as a prior information."
another method for calculating topic coherence is referred to as distributional semantics [cit] . this method uses a similarity measure such as cosine similarity to evaluate a vector of the top n words in a topic and then weight this result using the pmi between the words. this method also introduces a variable to put more emphasis on word relationships with a higher pmi.
"the proposed aggregation technique shows that it outperforms standard topic models in topic coherence, but the method can still be improved, for example, by clustering or bagging the corpus into subsets of data and generating base models using these subsets, which could then be used for generate aggregated topic models. the topics generated from these subsets when aggregated could provide a good mix of general topics, as well as specific topics. this work could also be furthered by creating aggregated topics from different types of topic models."
"we run experiments with different key lengths of 512, 1,024, and 2,048. the number of threads launched in one decryption varies from 32 to 1,024, varying gpu utilization from a single warp, a single block, to multiple smxes. for each experiment, 100k decryption runs are carried out with random input messages. the error-correction parameters d th, n, and c th are set to 0.2, 8, and 0.01, respectively. the time used for one attack ranges from around 15mins for ras-512 with 32 threads, to 8h [cit] with 1,024 threads. the larger the key size and the number of threads, the longer the attack time due to the increased computational complexity, and a larger number of error corrections needed. table 1 shows the number of errors corrected by forward and backward error correction, and how many windows (least significant/right-most) are incorrect after the attack. in the results, we see the number of errors increases as the number of threads and the key size increase. the forward error correction works best with fewer threads and a smaller key size. the backward error correction works best when using a larger number of threads and for a longer key. the errors left in our experiment are very small (0-3 windows, 18 bits at most), and the full key can be easily recovered using a lattice attack [cit] or brute-force attack. for the largest error in our experiment, the brute-force attack space is 2 18 . it takes about 15mins to attack rsa-512 with 32 threads, and about 8h [cit] with 1,024 threads, due to the increase in key size and error corrections."
"frequency domain low-pass filtering. the frequency domain filtering was performed by first estimating the effective image resolution with frc/fsc, and then using it as a cut-off frequency for a low-pass fourier domain filter. three different types of fourier space filters were used in this work: (i) an ideal low-pass filter (ilpf), (ii) a butterworth low-pass filter and (iii) a gaussian low-pass filter."
", where r i is a polar distance from the center of the frequency domain filter (zero frequency) and r th is the distance at the cut-off frequency, obtained with frc/fsc. frequencies after the cut-off are simply clipped to zero."
"in this paper several novel blind image restoration methods were introduced that leverage frc/fsc resolution measurements in different ways. in frequency domain de-noising methods frc was used to find a cut-off frequency point for low-pass filtering. in deconvolution tasks, both linear (wiener) and iterative (rl), frc/fsc measurements were used to estimate the effective point-spread-function (psf), directly from the image data. there are several clear benefits of estimating the psf with frc/fsc."
"in this section, we introduce a hierarchical timing model for rsa decryption on a gpu, which accounts for various interactions during execution and is the basis for enabling our side-channel timing attack. note that we choose the extra reductions in the montgomery multiplication as the timing channel. the number of reductions depends on both the input message and each window of the secret exponent d. we start with a general linear timing model as follows:"
"forward error correction is triggered when the distinguishability d of a target window is below a threshold d th . we need to reconsider those values that were guessed and did not produce the highest correlation. we proceed with tentative attacks on the next window multiple times by setting the target window's value to other possible values, one after another. for each of those tentative attacks on the next window, we obtain a maximum correlation coefficient. to pick the right value for the target window, we choose the one resulting in the largest maximum correlation in the next window."
"the attack methods for the nonzero and zero windows are different. we derive the success rates of both methods. to simplify our analysis, we assume we know whether the window under attack is zero or nonzero. this is a valid assumption, because when the proceeding window is zero, the current window under attack is nonzero."
"based on adaptive sliding mode control algorithm, a guidance law with impact time and angle constraints is proposed in this paper. the mathematical model is given firstly. then the guidance law is proposed for intercepting the target with terminal constraints. by means of designed adaptive law, the gain of proposed guidance is adjusted with respect to the system state. the stability and convergence characteristic of proposed guidance law are also analyzed. the simulation results indicate that the proposed guidance law could meet the requirements in a satisfactory manner. extending the results to the cooperative guidance [cit] or containment control [cit] will be an interesting trial in future works."
"in addition to deconvolution and image denoising that was the focus of this paper, there are also several other image processing/analysis tasks that frc/sfsc could be applied to. in (figure s. 16 ) we entertain the idea of combining frc/fsc with other image quality assessment parameters, to produce quantitative measures of image quality for e.g. high-content screening applications; a similar method was recently proposed for assessing the quality of localization super-resolution microscopy image reconstructions [cit] ."
one such direct approach to gauging topic coherence is to utilise the pointwise mutual information (pmi) [cit] . this way is extrinsic in nature as it computes pmis over the top n words in a sliding window with an external corpus such as a wikipedia dump file (a plain text version of every article present on wikipedia). the calculation can be seen in (1).
"in iterative deconvolution (rl), frc measures were also leveraged to observe the progress and quality of the deconvolution process. no other such metric to our knowledge exists in the literature. the τ and η k, as well as many other measures that can be found in the literature, mainly quantify the convergence of the deconvolution algorithm, but can not really quantitatively analyse the quality of the deconvolution results, in absence of a ground truth image. it was shown that frc can be used to identify the deconvolution iteration at which the effective resolution is maximal, after which the algorithm starts to mainly fit noise."
"to conclude, frc is a rather valuable tool in evaluating deconvolution progress and results. [cit] . they essentially differ in the way that the fourier sphere is indexed to produce the directional resolution measures. the sfsc is especially tuned to observe variation of resolution, when rotating around a single axis, which makes it rather fast to calculate (few sections/volume) as well as robust (large number of voxels on every section)."
"the curve fitting has been implemented in two different ways: (i) a standard linear model fitting, with an exponential function of the order of n and (ii) a piece-wise (smoothed) splines based fitting method."
"in our attack, the length of a zero window is much harder to attack. because the number of extra reductions is roughly linear with the bit length of the zero window, the ability to differentiate provided by the correlation attack is fairly small. for example, when guessing the length, the correlations of the wrong length guesses is very close to that of correct ones, as shown in figure 6 for the case of attacking a zero window, where the length of the window is 4. if a wrong guess ends up with the highest correlation, the error can only be detected in later windows. we define a parameter distinguishability d to indicate how likely an error can happen. the value of d for one window is defined as the normalized difference between the largest correlation and the second-largest correlation value. the lower d is, the more likely an error will occur. based on these observations, we propose two error-correction methods: forward and backward methods."
"standard topic modelling was originated with lsa, however, in the context of an information retrieval task a standard lda is often referred as latent semantic indexing [cit] . lsa utilises a document-term matrix along with a singular value decomposition method to find similar documents. but it has a fundamental assumption that words, which frequently co-occur, are relevant to one another. two notable disadvantages of lsa are that the model is based on the bag-of-words representation and that it struggles with polysemy. that means that the order of words in documents is ignored and that it cannot distinguish between the different meanings of a single word. for example, crane can refer to both a bird as well as a piece of construction machinery. lda is capable of eliminating the polysemy difficulties through incorporating a probabilistic element to the model but it still has the disadvantage with the bag-of-words model, which is not able to capture sentence structures when creating the model [cit] ."
"another significant piece of study on combining topic models is to concern the utilisation of document-word tuples co-occurring in documents and their assigned topics [cit] . that method assumes an element vector t where each element is a topic assignment of a document-word tuple (d i, w i ), word w i from document d i . the corpus is then divided into a set of sub-corpora, where each subcorpus is represented by t i that can be merged into one vector t . that is then used to derive topic distributions over documents. that method has been introduced as both lda and latent semantic analysis (lsa) ensembles. the evaluation conduced on both real and synthetic data demonstrates that the lda ensemble outperforms the lsa in terms of perplexity, however, the performance of the lsa ensemble is better than that of the lda in terms of efficiency."
"the second experiment consists of creating ten models each with a different α dirichlet prior value a low α value results in very few (and in some cases, only one) topics being in the document."
"in terms of changes between the base model and the aggregated model, tables 4 and 5 show the topics before and after, respectively. as can be seen t 1 in the base model is mainly about trump and his affinity for \"putin\" and \"russia\", whereas in the aggregated model this topic also has the words \"woman\" and \"issues\", referring to the fig. 7 average intrinsic and extrinsic coherence for topics in the base models and aggregated model for social media data number of women who came out before the debate to allege that trump had made inappropriate advances on them. importantly, t 2 has the addition of the word \"amnesty\". this is associated with the word \"illegals\" in the base model topic and represents clinton's desire to grant amnesty to many illegal immigrants. topic t 10 also shows that the aggregated model has the ability to filter noise out of topics; in the base model the string \"skhbwg6aq3\" appears but is not present in the aggregated model."
"in this experiment, the same datasets used in experiments 1 and 2 have been used again with nmf, producing a new set of topics. as in the previous experiments, both intrinsic and extrinsic coherence are calculated. intrinsic uses the corpus the documents were generated from and extrinsic uses the english wikipedia. all other parameters for testing are also the same as previous experiments. for fairness, 10 topics were generated for each dataset as this is the same amount of topics that were previously generated using the aggregated topic model. [cit] iterations if approximate error did not converge. initialisation was performed using non-negative double singular value decomposition [cit] with a multiplicative updater solver."
"in this article, we extend our previous work [cit] including two contributions. first, we construct a new attack method with sliding window of variable length nonzero window optimization. previously, we only targeted constant length nonzero window optimization, which is a simpler case to attack. second, we build a success rate model to predict the effectiveness of the attack relative to the quality of the side-channel information."
"the experimental results reveal that the aggregating models increase the coherence of topics. figures 3a, b, 5a and b show that the model with the lowest number of topics or highest α prior (m 1 from both experiments) are normally the most coherent topic but after aggregation, the aggregated topic is the most coherent. this could be because m 1 is usually the most general model, therefore when evaluated extrinsically the words would have a high probability of cooccurring as they are not specific. what is also interesting is the fact that the aggregated model also has the highest intrinsic coherence, meaning that combining elements of more specific models into the general model allows for a greater representation of the modelled corpus. however, it was found that to maximise coherence the sliding window size had to be set to the size of the document being analysed. using the full document size is not detrimental to results as the average document length is 133.07 words, which is only 33.07 words more than the second highest average coherence sliding window size of 100."
"because sted is a bandwidth un-limited technique, the power spectrum in sted images often contains very high frequencies that unfortunately, are often hidden by noise. however, the simple blind wiener filtering as shown in (figure 4 a) ), is able to recover a surprisingly large amount of fine details. the axial haze is clearly reduced, and the effective resolution is drastically improved with previously blurred filaments, clearly visible in the results."
"the elementary objective of missile guidance is to attack the target effectively with minimum miss distance. for the reason that some developed targets nowadays are equipped with antimissile defense systems, conventional guidance law is hard to fulfill the mission generally. therefore, various terminal constraints should be considered for guidance design in some situations [cit], such as the impact angle control guidance (iacg) [cit] and impact time control guidance (itcg)."
"where n is the modulus, and e and d are the public and private exponents, respectively. the relationship between n, d, and e is that they have to satisfy constraints so the encryption and decryption are reverse operations to each other [cit] . the key length (the bit length of c, m, d, and n) can be 512, 1,024, 2,048, or longer, providing different levels of security. usually the public key exponent e is chosen to be a relatively small number with a small hamming weight, while the secret key exponent d is much larger and requires many more bits. therefore, the decryption process using the private key is 20 to 60 times slower than the encryption process, demanding acceleration for higher throughput [cit] ."
it is important to note that although the top n words in a topic may not appear to change much in some cases; the underlying φ distribution of the topics (topic-word distribution) will change after the aggregated model is formed.
"because of the anisotropic sampling that is typical to 3d fluorescence microscopy images, the frequency axes in sfsc and frc curves need to be corrected to compensate for it. in this paper this was achieved by multiplying the image pixel/voxel size by factor k(θ):"
"similar interchangeable iterator scheme is used in the frc implementation as well, the main difference is that the iterators work in polar coordinate system, and only one cross-correlation curve is calculated for every image. it may in some instances be of interest to exclude certain parts of the fourier rings as well, if a particular orientation of features is of interest, or if there are artefacts in a certain direction affecting the frc results."
", where n is the total number of pixels, n (u) is the number of pixels that are not currently converging and is the convergence epsilon, 0.05 in results shown this paper."
"the closest piece of previous study to our work is the self-aggregation process for short text in the model generation stage of a topic model [cit] . that process naturally integrates clusters of text before the topic modelling process begins. importantly, it has a fundamental assumption that each document only contains one latent topic. this allows for the creation of larger pseudo-documents to be used in the generative procedure of topic models."
"the two main approaches of handling the data when evaluating a model statistically are to either to train using some of the data and then get the probability of heldout documents given the latent topics discovered during training of the model; or to split each document in half and train the model with the first half of the document and then get the probability of the second half of the document given the latent topics discovered from training on the first half of the documents, this is known as document completion evaluation [cit] ."
"in terms of how the underlying topics changed in the aggregated model, there are not as many changes as in experiment 1. however, the few changes that occur improving topic coherence by a noticeable amount. for example, in m 1 there is a topic about mikhail gorbachev, the soviet union and the united states. in the aggregated model, this topic is supplemented with the words \"east\" and \"germany\", making the topic more clearly about the berlin wall and tensions between the west and east towards the end of the cold war. the other major difference between base model topics and aggregated topics is in one about finances. the base model contains units of money such as \"million\" and \"billion\"; as well as words to do with the workforce, such as \"workers\" and \"business\". the aggregated model's equivalent topic also contains the words \"industry\" and \"company\"."
"prior to generating a topic model by lda, a set of parameters need to be defined. on the one hand if setting a small number of topics, lda could produce very broad topics, whereas if the number of topics is set too large, the topics generated could be very granular and overfitted to the corpus. however on the other hand, if one of these parameters results in very different topics to other models' topics, they are unlikely to be combined with other topics, thereby making them not have an effect on the aggregated topics."
"although those methods are useful for getting an insight into how the model performed as a whole, in this work we do not focus on the statistical methods for evaluating topic models. we are more interested in the coherence of the latent topics that the models output. topic coherence measures are used to quantify the similarity degree of the latent topics discovered by a topic model from a human-like perspective to identify a high degree of semantic coherence of topic models. in simple terms, topic coherence assesses such a kind of topic extent through computing word cooccurrences and mutual information, which could reflect how people would perceive this problem. although there have been many obscure types of topic coherence measures and unifying topic coherence measures proposed [cit], this research focuses on the measure which are most popular among the literature in the field, which are detailed in the following section."
"to perform exponentiation of a large number, a simple binary method [cit] performs squaring and conditional multiplications based on each bit of the exponent. one optimization method is to perform sliding-window exponentiation, where the exponent d is decomposed into a series of zero and nonzero windows, f i of length l(f i ), and the exponentiation is processed window-by-window."
the success rate is dependent on l h . the final success rate of attacking zero windows is a weighted average of the attacking different window lengths:
"section ii states the problem. in section iii, the proposed guidance law is presented in consideration of the impact time and angle constraints. the convergence analysis is provided as well. in section iv, the simulation is carried out to verify the performance of proposed guidance law. conclusion is made in section v."
"where r is the number of extra reductions in the montgomery multiplications, v is the unit execution time of one reduction, and o represents other key-independent execution time factors that follow a distribution when the input message varies. for a sequential implementation, r will simply be the total number of extra reductions performed in each window. with multiple messages being decrypted in parallel by many threads on the gpu, there are complex interactions between the threads through competition for shared resources and the specific gpu programming model. we build a hierarchical model of r according to the interaction at different levels of parallelism on a gpu. we consider fine-grained thread (message) level interaction as well as interactions at a warp level, an sm level, and finally across the entire gpu."
"equation (4) shows how to find the ngd between two terms. equation (5) shows the process to find the average coherence for the top n terms in a topic, where k is the number of terms to be considered in the topic."
"there are two ways of constructing a sliding window: applying a constant length nonzero window (clnw) [cit] or a variable length nonzero window (vlnw), without incurring significant performance overhead [cit] . for clnw, the nonzero window length is fixed, while the zero window can have different lengths. for vlnw, both nonzero windows and zero windows have variable lengths."
"the results of the proof-of-concept experiment two were also interesting as despite having fewer changes in the aggregated model than the first experiment, there was a noticeable difference in coherence. this suggests that aggregation allows for more general topics, and that any form of generalisation results in a higher topic coherence."
"a few studies have been conducted on the application of classic ensemble methods to topic models. the boosting ensemble method has been applied to generating topic models with good results and generalisation ability as lda has the ability to map features to topic space rather than word space [cit] . another interesting work is to integrate a supervised component into lda, for instance, through defining one-to-one correspondence between lda's latent topics and classification labels [cit], or incorporating a supervised hidden markov model into lda, resulting in a further enhancement of the boosting method for topic models [cit] ."
"in order to show the effectiveness of aggregated topic models two initial experiments were performed. a number of variables need to be decided on before running a topic model, including the number of topics, the α dirichlet prior, and the β dirichlet prior. in these experiments, models with differing variables were created and their outputs aggregated to see if it can increase topic coherence."
"as in experiment one, this experiment denotes m 1 as the base model, m 2 − m 10 as other models for comparison, andm as the aggregated model. as fig. 3a shows, the aggregated model has an extrinsic pmi value of 0.7, which is much higher than any of the models used to create it. this also shows that the aggregated model's topics are much more coherent based on general english language. the aggregated model also has the highest intrinsic coherence. this means the aggregated model's topics have been complemented with additional relevant topic words leading to topics that are more representative of the corpus."
"the inherent drawback of the proposed attack, due to the iterative processing shown in section 4.3, is that the attack on the target window is highly dependent on attack results on the previous windows. if any of the previous guesses were wrong, the current attack may not succeed because of the calculation m temp and the reduction number will be wrong. as a result, the correlation coefficients after the first error will be much lower than those before, as can be observed in figure 5 . each point in figure 5 is the maximum correlation coefficient of one window attack when we iterate over all possible values. this non-robust feature can be utilized to detect errors if there exists n consecutive window attacks with low correlation coefficients."
"the time derivative of v 2 can be given bẏ in accordance with lemma 2, e θ will locally converges to zero in finite time:"
"firstly, no prior knowledge -even theoretical -is needed of the microscope or the sample. secondly, the psf generated via frc/fsc is always tailored to every given image. thirdly, the psf estimation with frc/fsc is a single-step process, although it can also be updated iteratively if necessary. this advantage specifically made it possible to perform linear blind wiener filtering in a very straightforward way, both in 2d and 3d. with larger images it might be of interest to divide the images into several smaller blocks [cit], to adapt the psf in the blind deconvolution to local changes of resolution ( figure s. 14) ."
"we also showed that this work can be used successfully in the social media domain. we demonstrated that it works well at increasing the topic coherence and adding additional words to topics which make them more coherent. additionally, the aggregated method has the feature of being able to filter out noise from topics. despite the experimental results showing an increase in coherence, it was not at a statistically significant level."
"to attack the ith unit, we assume the previous (i − 1) th units are already known. the intermediate value of m temp can be calculated at the beginning of the processing of the ith unit. we first guess the length of the unit l x by squaring m temp l x times and calculating the number of reductions for each message, the same as was done for clnw. the value of l x is determined by the correlation attack, similar to the zero window attack for clnw. next, we guess the value of the unit v x, by multiplying the m l x temp by c v x and then squaring the result q + 1 times and calculating the number of reductions. because the next attack unit has to be least q + 1 bits, we assume the computation would be correct if we have the correct guess of v x . by knowing l x and v x of the unit, we can recover the zero and nonzero windows and the private key."
"given the parallel execution environment on a gpu, it becomes more challenging to predict the execution time versus a single-threaded execution on a cpu. program execution includes both concurrent computation and blocking execution (e.g., for divergence in a warp). the execution time is not simply equal to the addition of all instructions anymore."
"to prevent the very general or very specific topics that could be generated using non-optimal initial parameters, we propose a novel method of aggregating topic models. with our method, a user needs to define a set of parameters, and multiple topic models are generated using these parameters; the topics that are found to be similar amongst these models are then aggregated. the main contribution of this proposed approach is that the aggregated models are able to increase topic coherence. this has the advantage of allowing granular topics, which might only be produced in a model with many topics, to have a presence in a topic model that has a small number of topics and is more representative to the corpus as a whole. the proposed method is also advantageous as it requires no changes to the underlying topic model's generative method. this makes implementation of this method more convenient."
"the main difference between the current work and the existing studies discussed in this section is that the method presented in this research is focused on the aggregation process of topics after the models have been generated. this is different from other methods that are often used in aggregating text by various factors, such as author or hashtag, before the model is generated in order to create larger documents [cit] . the advantage of the method described here is that this method does not rely on arbitrary manipulations of the model structures of underlying topic models or input data, thereby producing a less complex model structure that requires no additional context specific information such as seed words or part-of-speech tagging. the remainder of this section reviews how related approaches work in the context of an ensemble-like method and topic modelling."
"each experiment involved the use of lda with 2,000 iterations of gibbs sampling to generate the topic models, determination of the similarity threshold, and comparisons of how the aggregated model competes with the base models used to create the aggregated model. the topic coherence test makes use of the extrinsic pmi. the reference corpus used for the extrinsic test is the english wikipedia. this corpus has over five million articles and the average of article length in the wikipedia dump was 133.07 words. therefore it is a good reference of general english language for comparison. the assessment of the intrinsic coherence test has also been conducted to measure the degree to which topics capture the structure of the underlying corpus."
"in addition to new image processing methods, two major limitations in current state-of-the-art of frc/fsc were addressed, especially with practical fluorescence microscopy applications in mind. it was shown how frc/fsc can be calculated on single images, and how fsc can be used even in cases the optical resolution is anisotropic. in our examples, the anisotropy is evaluated in the axial direction only, but the measurement can of course be adapted for any orientation, or in the extreme case, expanded to to the full conical fsc implementation [cit] -however, with high computational cost, although conical fsc, as well as sfsc, could to an extent be parallelised to improve speed. these methods are general, and should be of use also outside the image restoration applications."
"1. the study introduces a novel method for aggregating multiple topic models in an ensemble-like way to create an aggregated topic model that contains topics with greater semantic coherence than individual models. 2. the aggregated model is constructed on the basis of cosine similarity and jensen-shannon divergence when exceeding a similarity threshold. 3. the experiment results show that the aggregated model outperforms standard topic models and non-negative matrix factorisation at a statistically significant level in terms of topic coherence. 4. [cit] american presidential debate scenario, demonstrating the competitiveness of the proposed method in real work applications."
"at its core, nmf approximates the dot product of w and h through iterations, resulting in the product of v . this iterative process is repeated until a specified amount of iterations is reached or the approximation error converges to a certain point. additionally, l 2 regularisation loss is used to prevent weights in the matrices from becoming too large."
"based on nonsingular adaptive sliding mode (nsasm) algorithm, a guidance law with impact time and angle constraints is proposed in this paper. in comparison with the existing researches, the main contributions of this paper can be summarized as follows."
"next, we consider running a warp using multiple messages in parallel. the number of messages in a warp depends on the key size. there are 4 messages in a warp for rsa-512, 2 for rsa-1024, and 1 [cit] . because threads in one warp are synchronized, the operations across different messages are also synchronized, (i.e., all the messages are processing the same window of size d at the same time). for one squaring operation in a window, if one or more messages have an extra reduction, the whole warp has to wait for it to finish, and the warp is considered to have an extra reduction. thus, the number of reductions in a warp for a window r warp (i) is not simply the sum of the reductions used for squaring for all the messages."
"there are two main ways to evaluate the quality of topic models. firstly, a statistical measure can be used to estimate the probability of a series of held out documents using a trained model. the disadvantage of evaluating models in that manner is that it does not account for the human interpretability of the topics generated. for instance, if a model overfits the data, the topics may not make much sense when a human examines them but the probability of held out documents may be quite high [cit] . a separate set of evaluation methods have also been proposed, which use the actual topics generated by the models and assess if the words within the topics belong together in a coherent manner."
"in order to achieve more accurate distance measurement value, a number of techniques for measuring the arrival of the echo are presented. in reference [cit] and reference [cit] cross correlation method and phase shift method were combined to estimate the value of tof, first calculated cross correlation function of the transmit and echo wave through cross correlation method to get the peak position of cross correlation function, and then detected phase shift of the transmit and echo wave through parabolic interpolation and phase shift method to decrease the measurement error of cross correlation method, so that more accurate tof value was obtained. in reference [cit], reference [cit] and reference [cit] tof value was estimated by using of parabola fitting method, which was, taking the starting line of ultrasonic echo envelope as part of a parabola, and performing parameter fitting of the starting line on the basis of curve fitting idea to achieve more accurate echo starting position. in reference [cit], different techniques and limitations of series of ultrasonic ranging methods were reviewed, and the accuracy and repeatability of the measurements were emphasized. on the basis of comparing time-domain methods with their frequencydomain equivalents, the use of hybrid models and biologically inspired methods were discussed. in reference [cit], in order to solve the problem that tof estimation of ultrasonic echo can only be obtained by numeric calculation in traditional narrow band model, a method based on laguerre model was proposed to calculate cramer-rao lower bound. the coherence between the variance of tof and the theoretic crlb obtained through laguerre model was verified by the simulation on hybrid exponential model polluted by gaussian white noise. in reference [cit], an inno-vative ultrasonic tof measurement method with narrow-band transducers was proposed. through introducing the received ultrasonic echo peak time sequences (ptss) of two slightly different frequencies, the relative tof can be more accurately identified than one wave period. then to describe the signal reception, a mathematical model was built, and from which the tof estimation algorithm was derived. after that a simulation model used to describe the characteristics of the ultrasonic transducer and the ultrasonic wave physical propagation was developed to verify the feasibility of the new algorithm. finally, an experimental system of measuring the relative tof over the known distance of 550-1450 mm was implemented to validate the feasibility. in reference [cit] and reference [cit], an ultrasonic tof estimation method based on kalman filter was proposed. the parameters of the theoretical model of two mixed echoes were estimated based on the ekf to accurately estimate the shape factors of echo envelope as well as locate its onset. simulation results showed that the proposed method had a high precision and well ability of noise reduction."
"according to the measured data in fig. 6 and table i, threshold method has the advantage of simple, practical and fast. however, threshold method also has the following disadvantage:"
"where amplitude a(t) is the envelope of given signal x(t), i.e. the modulation signal. the target echo can be quickly found by using peak method, and envelope model method considers the characteristics of the echo signal time domain. improved distance measurement algorithm based on combination of peak method and envelope model method is proposed, and the detailed algorithm is as follows:"
"currently the most popular distance measurement method is pulse-echo method [cit], which estimate time of flight (tof) from ultrasonic transmit to echo receive. the distance d between the ultrasonic probe and the measured object can be calculated as"
"however, the platforms and algorithms of above tof estimation methods are mainly applied to short range within 10 meters. in case of long range measurement, ultrasonic echo signal amplitude presents exponential attenuation with the distance increase in air because of atmospheric absorption loss, geometrical spreading loss and influence of temperature, humidity and atmospheric pressure. furthermore, echo signal of long rang measurement is very weak, it will be easily mixed with a large number of spurious echoes, electrical noise and acoustic noise, and then lead to low ultrasonic echo signal to noise ratio, thus make determining the exact moment of the echo arrival time more complex compared with short range measurement, which affects stability and accuracy of ultrasonic ranging algorithms."
"cross correlation method perform cross correlation calculation of echo signal and transmit signal, with the characteristics that ultrasonic transmitter signal correlation with echo signal but independent of most of the noise signal, then the cross correlation waveform peak arrival time is considered as the echo arrival time. cross correlation method is optimal algorithm under the condition of single path with additive white gaussian noise [cit] ."
"the composition structure block diagram of entire ultrasonic signal measurement system is shown in fig. 1 . it consists of ultrasonic transmitter module, ultrasonic receiver module, pxi data acquisition module, ultrasonic signal processing with pc module and other components. experimental setup of measurements is shown in fig. 2 . the working principle of the measurement system is, ultrasonic pulses emitted from microcontroller through high-power ultrasonic drive circuit, to motivate the ultrasonic transmitter transducer and convert electrical energy into sound energy and transmit. when encounter obstacles, receiving transducer receives the reflected ultrasonic echo signal and convert sound energy into electrical energy. the echo signal amplified by receiving circuit and the motivated transmit signal will be collected to the processing pc through the pxi 6122 acquisition card of ni company. pc host with labview software and matlab software platform will capture, store, display, analyze and process these signals."
threshold method algorithm is as follows (1) experimental data between 10m to 25m were analyzed with threshold method. the ranging results are shown in table i .
"(1) by performing cross correlation analysis of echo signal and transmit motivate signal, the cross correlation waveform between these two signals is obtained; the results of cross correlation method at a 20m distance are shown in fig. 7 ."
"tower crane has huge structure, and with the expansion of the scale of urban construction, the tower crane working area become narrow, tower crane arm may collide with surrounding obstacles, which will cause tower crane accident [cit] . the technical characteristic of both domestic and oversea tower crane safety warning system is locate itself, and only monitor tower crane its own state of motion, system control the rotation range on the basis of pre-defined permitted region and prohibited region to prevent tower crane collision accident [cit] . these schemes do not involve actively monitoring surrounding obstacles. compared with radar, laser sensors and vision sensors, ultrasonic sensors not only has better adaptability to illumination changes, smoke dust, electromagnetic interference and other harsh environments, but also has lower cost and better reliability. ultrasonic distance measurement technology has been successfully used in robot obstacle avoidance and vehicle reversing collision avoidance system [cit] . therefore, research focusing on the analysis of ultrasonic sensor technology in tower crane anti-collision system has great practical significance [cit] ."
"most of current ultrasonic distance measurement products on the market have a limited effective distance measurement range of about 10 meters. however, in order to apply ultrasonic sensor technology in tower crane anti-collision system, its warning range should be within 25 meters. therefore, the study of ultrasonic sensor long range distance measurement and target information acquire technology are the key and foundation of tower crane surrounding obstacles detection. in practical applications, due to ultrasonic distance measurement affected by environmental factors, the precision can be easily controlled to millimeter and centimeter level when performing a short range distance measurement, while for long range distance measurement, in order to achieve the same precision, system architecture and algorithm design need to be improved, accordingly, it will increase the complexity of the system architecture and algorithm."
"therefore, for long range distance measurement, this paper present to select appropriate long range ultrasonic sensors, design long range ultrasonic distance measurement system, perform ultrasonic signal acquisition and build mathematical model of the echo envelope, and then, from the perspective of signal processing, analyze accuracy and stability of traditional threshold method and cross correlation method，thus an improved long range measurement algorithm based on combination of peak method and envelope model method is proposed to achieve precise tof estimation of long range ultrasonic echo signal, which provides the foundation of subsequent research of information fusion and target recognition."
"echo signal at a distance of 20m is shown in fig. 5 (a) with a serious noise. in order to get accurate distance measurement, noise cancellation is needed. this paper uses an improved threshold filtering based on wavelet method to de-noise. echo signal after processing is shown in fig. 5(b) ."
"where c is propagation velocity of ultrasonic wave in the medium, it mainly related to the medium temperature. the relationship between ultrasonic wave propagation velocity in the air medium and the environment temperature can be expressed as"
"using the ultrasonic measurement system, select long range, transceiver split, air piezoelectric lhq22 as ultrasonic transducer, with transmitting transducer and receiving transducer is 1m positive, air as the transmission medium, data sampling frequency is 200khz, rectangular pulse is used as motivate signal and the pulse number is 10, the ultrasonic echo and its envelope are shown in fig. 3 . analysis from vibration theory, the ultrasonic echo signal mainly has two phases, rising phase and falling phase. rising phase is transducer perform forced vibration in the role of the ac motivate signal, the signal amplitude increasing. falling phase is after the motivate signal stop, receiving transducer perform damped oscillation attenuation vibration, the signal amplitude decreasing. then the received echo envelope is shown in fig. 4 . on the basis of transducers' mechanical -acoustic analogy relationship and kinematic mechanical properties [cit], mathematical model of the echo envelope signal can be deduced. the following introduction describes the process of establishing echo envelope rising phase mathematical model because it will be used in improved distance measurement algorithm of section iv d."
"for long range distance measurement, threshold method is easily affected by noise and threshold is difficult to pre-set, and cross correlation method has strong oscillation characteristics in the vicinity of the waveform peak, which cause lower accuracy and poorer stability, therefore improved algorithm is proposed in section d."
"the results of improved distance measurement algorithm at a 20m are shown in fig. 8 . fig. 8 and table iii, the improved distance measurement algorithm based on combination of peak method and envelope model method has higher accuracy and better stability than threshold method and cross correlation method, which is conducive to accurately detect the long range distance obstacle. this paper studies the long range ultrasonic measurement algorithms. a mathematical model of echo envelope was established on the basis of vibration theory and mechanics-acoustic analogy relationship of transducer. long range distance measurement results with traditional threshold method and cross correlation method were analyzed through experimental data. due to the poorer accuracy and worse stability problems of long range distance measurement with these traditional methods, the improved distance measurement algorithm based on combination of peak method and envelope model method is proposed. experimental results show that the improved distance measurement method can achieve more precise of tof estimation, and has higher accuracy and better stability than threshold method and cross correlation method, so that it provides algorithm support of long range distance measurement technology for the tower crane anti-collision system."
"as can be seen from the results, the distance is farer, the echo amplitude is smaller, the surrounding environmental noise is more serious, and there are false obstacle detection results when the signal to noise ratio is too small. in fig. 6, there are 6 false obstacle with threshold value was pre-set to 0.2 and 2 false obstacle with threshold value was pre-set to 0.26, and there is 1 false obstacle with threshold value was pre-set to 0.285. until the threshold value was pre-set to 0.3, the real obstacle was detected. the analysis results show that only when a threshold is pre-set greater than 0.3 then the real obstacle can be detected."
"where t is environment temperature. when environment temperature t is a certain value, the key of ultrasonic distance measurement technology is an accurate estimation of tof. the accuracy of ranging measurement mainly depends on the accurate determination of the arrival of the ultrasonic echo signal, and the algorithm to be used must be adapted with the requirements of the application respect to accuracy, sample frequency, complexity and other factors, etc. among tof estimation methods, the widely used methods are threshold method and cross correlation method. threshold method detects tof through a pre-set threshold, considering that the ultrasonic echo arrived when its amplitude exceeds the threshold. this method presents poor accuracy especially in case of attenuated echoes. cross correlation method presents higher accuracy but it needs higher sample frequency and storage capability, which makes it not suitable for realtime applications. thus from accuracy and real-time point of view, these two methods are applicable to short range measurement with lower accuracy and real-time requirements. while for long range distance measurement, the stability and precision of these methods will downgrade due to ultrasonic echo signal to noise ratio become low."
"on the basis of number of experiments, the position of the echo signal peak point can still be easily detected even if the low snr and small echo signal amplitude, which can avoid threshold method false detection with inappropriate threshold. with the measurement distance change, the time difference between time t p corresponding to the echo peak position and the time t 0 corresponding to the echo starting position is almost keep the same. this time difference is approximately 10 times of the motivate pulse, i.e., 250  s."
"in the experiments, the transmitter transducer and receiver transducer were placed on the same level and faced to the wall as the target. the experimental environment temperature was 9c. experiments were carried out in different distances at 10m, 15m, 20m and 25m. acquisition card sampling rate was set to 100 khz; ultrasonic transmitter circuit generated 10 ultrasonic burst of 22 khz frequency in each 200ms interval. to illustrate the distance measurement algorithm, data collection at a distance of 20m was used for analysis."
"by observing the echo waveform, the ultrasonic echo can be described as a relatively low frequency envelope of amplitude modulation wave signal, the envelopes of the echo waveform at different distances have better consistency, and the time difference between time corresponding to the echo peak position and the time corresponding to the echo starting position is almost keep the same. extract the envelope of echo signal, which is equivalent to increase 2 times of ultrasonic frequency, can improve time resolution."
"where  0 is density of medium and c 0 is speed of sound waves. when the time equal to t f, sound pressure reach maximum value p tm ，then rising phase ends. similarly, the process of establishing echo envelope falling phase mathematical model can be deduced."
"experimental data between 10m to 25m were analyzed with cross correlation method. the ranging results are shown in table ii . cross correlation method has strong oscillation characteristics in the vicinity of the waveform peak. there are a number of closest peak amplitudes in the vicinity of the correlation peak due to affected by ultrasonic signal center frequency, which will possibly cause a wrong estimation of tof and low accuracy. compared with threshold method, cross correlation method algorithm is more complex and poorer real time behavior."
"the principle of threshold method is when the received echo signal voltage is greater than the pre-set threshold value, the corresponding time is considered as the echo arrival time [cit] ."
echo signal after de-noise at a distance of 20m were analyzed using threshold method. the results with different threshold of threshold method are shown in fig. 6 . the detection result with a value of 1 means an obstacle is detected and the detection result with a value of 0 means that there is no obstruction.
"(1) considered the point when echo exceed the threshold as the starting point of an echo, the measured time is not the echo arrival time of leading edge, which will affect the accuracy of the measurement; (2) as the acoustic attenuation loss and diffusion in transmission, the echo attenuation increases with distance increase, then accurately pre-set threshold value is difficult; (3) echo signal amplitude of long range distance target is small, it is easy to be incorrectly triggering by noise, which result in false detection."
"therefore, using transducers' mechanical-acoustic analogy relationship, take account of air medium, obstacle reflection, voltage and sound wave generated by receiver chip is proportional and other factors etc, the final mathematical equation of echo envelope are as follows:"
"the first four buttons at the top of the preprocess section enable us to load data into weka, by importing it from any file in the default arff format supported by weka, or any other format accepted by weka for which a filter is implemented, such as excel comma separated value (.csv) file, a sql database, a url, etc., for preprocessing."
"since excel can produce and use csv files, these questions and their answers are entered in one sheet of an excel workbook file, so that each question and its corresponding answer is a record (row). our table consists of two columns with headings \"question\" and \"answer\". then the excel file was saved as comma delimited values csv file, by selecting the format \"csv (comma delimited value)\" from \"save as types\" in the save as dialog box."
"there are tremendous amounts of data on the internet, in many different formats and various templates, written using different applications. data collection is an important phase in many research projects; data is collected for many purposes to perform some required tasks such as to aid in research on statistical methods, or to train classifiers for machine learning experiments. collecting questions and answers in any domain can be an important resource to serve that domain, for example it can be used in a question answering system to answer online user questions. there are question and answer datasets available in many domains, for research and practical use; however there is no existing public resource specifically designed for quran questions and answers. there are existing sources which do include questions and answers from the quran, but these are scattered between different webpages and not integrated in one corpus. each of these corpora has its own format and style. there is a requirement to create a unified dataset corpus for quran questions and answers. collecting quran questions and answers in a unified corpus and converting it to text file format (.csv) that can be loaded into weka to do further analysis is our challenge. this paper aims to create a quran question and answer corpus dataset; this corpus should be useful in the application where quran questions and answers have to be searched. our main contribution can be summarized in the following points:"
"it will be useful to use the cross industry standard process for data mining (crisp-dm) to prepare the data for the analysis tool. the crisp-dm methodology is described in terms of a hierarchical process model, consisting of sets of tasks described at four levels of abstraction [cit] (from general to specific): phase, generic task, specialized task and process instance. at the top level, the data mining process is organized into six phases: (1) business understanding, (2) data understanding, (3) data preparation, (4) modeling, (5) evaluation, (6) deployment [cit] business understanding is to understand the project objectives and requirements from a business point of view, then convert this knowledge into a data mining problem definition, and then design a preliminary plan to achieve the objectives. data understanding begins with an initial data collection then continues with activities in order to be familiar with the data, to determine data quality problems, to discover first insights into the data or to discover interesting subsets to form hypotheses for hidden information. data preparation covers all tasks to build the final dataset from the initial raw data, such as table, record and attribute selection as well as transformation and cleaning of data for modeling tools. data preparation tasks can be performed multiple times, and not in specific order. modeling is to select and apply various data mining algorithms as modeling techniques and to calibrate their parameters to optimal values. evaluation is to evaluate the results against business objectives. deployment is to deploy the resulting model to be used by the customer whenever it is required."
"the most valuable resource that weka provides is the implementations of a wide range of data filtering tools, machine learning schemes, evaluation methods, and visualization tools. filters are used to preprocess the data; we can select filters from a menu and then adjust their parameters according to our requirements. weka also includes implementations of algorithms for learning classifiers, association rules, clustering data for which no class value is specified, and selecting relevant attributes in the data. also, there are many tools developed by third parties as add-ons. for example, weka was not designed for multi-relational data mining, but there is separate software for converting a collection of linked database tables into a single table that is suitable for processing using weka. another important area that is not covered by the algorithms included in weka is sequence modeling [cit], but third-party add-ons are being developed."
"to prepare a csv file to be loaded into weka the following has been done. a representative sample of questions and their answers were selected from the collected questions and answers dataset, to include representative samples covering the above methods and sources that were used to collect them, and the questions types, as well as the length of the answer. there are some questions that need a long explanation for their answers. the selected dataset required cleaning prior to data usage; therefore the data were cleaned by removing control characters, and resolving formatting problems concerned with some characters such as double quotes, single quotes, comma, apostrophe, etc."
"after loading the csv file into weka's explorer, this dataset were processed into vectors of word frequencies using the stringtowordvector filter, which converts a string attribute to a \"bag of words\", a vector that represents word occurrence frequencies. the stringtowordvector filter produces numeric attributes that represent the frequency of words in the value of each string attribute. the set of words (the new attribute set) is determined from the full set of values of all the strings in the full dataset. the list of all attributes, statistics and other parameters can be utilized as shown in fig.1 . there are 30 instances and 196 attributes in the \"quran question and answer\" sample relation file. the processed data can be further analyzed using different data mining techniques like, clustering, association rule mining, and visualization algorithms. in our model we use 4 clusters. fig. 2 shows the attributes which are clustered, the number of clusters, and the number of instances each cluster contains. clustering is used for data in which no class value is specified. in clustering, relevant attributes in the data are selected to decide the cluster. in some algorithms the number of clusters can be specified by setting a parameter in the object editor. for probabilistic clustering methods, weka measures the log-likelihood of the clusters on the training data: the larger this quantity, the better the model fits the data. increasing the number of clusters normally increases the likelihood. fig 3 shows weka cluster visualizer in which the attributes are clustered into 4 groups. the visualize panel helps to visualize a dataset itself. it displays a matrix with a two-dimensional scatter plot. fig. 2, it is shown that 18 instances were clustered in cluster 0, 1 instance in cluster 1, 8 instances in cluster 2, and 3 instances in cluster 3. from table 1, it is evident that cluster 2 has questions of \"how many\" with number answers, and in cluster 3 the questions contain some of the same words, for example the words \"name\", \"prophet\", \"mentioned\", and \"quran \" were found in the questions. cluster 1 has questions containing words that did not appear in any other question like the words \"islamic\", \"view\", and \"abortion\". cluster 1 contains the rest of the questions. these results can be used in further analysis."
"the waikato environment for knowledge analysis (weka) is computer software developed at the university of waikato in new zealand. weka is free java software available under the gnu general public license. it is a collection of machine learning algorithms to solve data mining problems. weka has become one of the most widely used data mining systems while it offers many powerful features [cit] . weka support many different data mining tasks such as data preprocessing and visualization, attribute selection, classification, prediction, model evaluation, clustering, and association rule mining. it is written in java and can be run with almost many computing platform. weka can be used to preprocess without writing any program code, and it comes with a graphical user interface to provide easily used tools for beginner users to identify hidden information from database and file systems in a simple way by using options and visual interfaces. there is a specific default .arff data file format that weka accepts. the data should be as a single flat file or relation; the data can be imported from a comma separated value (.csv) file, a database, a url etc. where each data point is described by a fixed number of attributes. weka supports numeric, nominal, date and string attributes types. weka can be used to learn more about the data by applying a learning method to a dataset and analyze its output, and it is also used to generate predictions on new instances by using learned models, as well as to apply several different learners and compare their performance in order to choose one for prediction. the desired learning method is selected from a menu. a common evaluation module is used to measure the performance of all classifiers."
"the weka graphical user interface (gui) chooser provides a starting point for launching weka's applications, this gui chooser consists of four buttons, to start applications [cit] . the main application is the explorer, which explores data with weka. the simple command line interface (simplecli) allows direct execution of weka commands for operating systems that do not provide their own command line interface. knowledgeflow supports the same functions as the explorer but with a drag-and-drop interface, and it provides a framework for incremental experiments in machine learning. the experimenter is used to carry out experiments and perform statistical tests between several learning schemes."
"to load data into weka, we have to put it into a format that weka understands. weka needs the data to be present in arff or xrff format in order to perform any tasks. when the format is incorrect while loading a certain file an error will occur; there are some reasons that caused that error for example wrong encoding file format or incompatible characters in the csv like a percentage sign (%), an apostrophe ('), incorrect endings, and, any extra commas, etc."
"the process of collecting data can be relatively simple according to the type of tools used to collect the data. data collection tools are used to collect information that can be used in many aspects such as evaluation of project performance. the collected data can also be reused for analysis purposes after its refinement and cleaning. there are several methods that can be used to collect data. the data collection methods should be good enough to collect useful data in order to have better evaluations for the research. selecting specific methods depend on the nature of the task, as well as the type of the required data. in this paper we selected three methods to collect quran questions and answers: (1) a web-based tool, created by a group of scholars interested in the islamic field, (2) eliciting questions and answers from islamic experts: a group from holy mosque in mecca who are leaders in the field of islamic studies, this group gives answers to questions from muslims who come to the holy mosque, and seek their expert advice, (3) some samples of questions and answers gathered from previous research."
"the task of creating an integrated quran question and answer corpus is an important issue, and we would like to encourage researchers to develop a quran question and answer corpus as a shared task, which aims at improving the state-ofart of online quran question answering systems. creating a corpus for data mining with weka to extract knowledge is becoming one of the key tasks for development issues and it plays a vital role for future planning and development. in this paper, we described the compilation of the quran question and answer collection, through harvesting data from websites, islamic experts, and existing research datasets. the merging and preparation of the corpus dataset involved removing control characters and solving the problem concerned with some characters, then creating a csv file format, and loading it into weka for further analysis."
"to load the csv file, from weka chooser gui we selected explorer application button, and then the preprocess panel, which is used to choose and modify the data being acted on. after that we chose the open file button to display a dialog box allowing browsing for our csv data file. from the dialog box we selected open button to load our file into weka. weka also enables us to load data from other locations by selecting the desired button. the open url button allows asking for a uniform resource locator webaddress where the data are stored. the open db button is used when we want to read data from a database. the generate button enables us to generate artificial data from a variety of datagenerators. using the open file button we can read files in a variety of formats: arff, csv, c4.5, or serialized instances format, these format have the extensions .arff, .csv, .data and .names, .bsi. weka has converters for some file formats such as spreadsheet files with extension .csv, c4.5's native file format with extensions .names and .data, etc., this list of formats can be extended by adding custom file converters to the weka core converters package. the appropriate converter is used based on the file extension. if weka cannot load the data, it tries to interpret it as arff. if that fails, it pops up the generic object editor box, which is used throughout weka for selecting and configuring an object. in this case the csvloader for .csv files is selected by default and the \"more\" button gives us more information about it."
data collection is an important part of many research projects. having enough data to learn from is of great significance for good performance. the issue of training data quantity and quality is key in machine learning research. increasing training set size can be more significant than developing better learning algorithms in terms of impact for improving object classification performance [cit] .
"the selection of such islamic data requires great care to give an accurate answer for a given question, which can be accepted religiously and universally. for instance, the answers should be evidenced as mentioned in holy quran as well as in hadith books. in this paper we propose to focus on frequently asked questions (faqs) using the above tools in order to collect the questions and answers. collecting questions and answers from authoritative and credible sources is an important issue; and low accuracies or wrong answers are not acceptable in the religious field especially in the domain of the holy quran. since there are not enough existing resources specifically designed for quran questions and answers, we propose to merge different data subsets to comprise the quran questions and answers dataset, as well as to have different questions from a range of different sources. four web resources were used as raw data sources for our questions and answers dataset."
"the explorer is the most used tool, and is composed of several panels to allow access to the main components of the workbench : (1) the preprocess panel which is used to import data, and preprocess this data by using filters to transform the data and prepare it according to specific criteria, (2) . the classify panel which allow applying classification and regression algorithms to a dataset, (3) the cluster panel enables access to the clustering techniques in weka, (4) the associate panel provides access to association rule learners that attempt to identify all important interrelationships between attributes in the data, (5) the select attributes panel gives algorithms for identifying the most predictive attributes in a dataset, (6) the visualize panel shows picture representations of data and results, such as a scatter plot matrix, where individual scatter plots can be selected and enlarged, and analyzed further using various selection operators."
"learning from data is categorized in two types: directed (supervised) and undirected (unsupervised) learning. classification, estimation and prediction are examples of supervised learning tasks, while association rules, clustering, and description and visualization are examples of unsupervised learning tasks. in unsupervised learning, no variable is singled out as the target; the goal is to establish some relationship among all variables. unsupervised learning attempts to find patterns without the use of a particular target field."
"the first web resource is the questions and answers generated by turntoislam.com [cit], which is widely acknowledged to be one of the best places to learn about islam, as it contains a huge library that covers many topics about islam in many languages. it answers questions, shares videos, polls, events and more. it aims at strengthening and uniting the nations and helping to show the beauty of islam to the world, as well as building a kind, friendly community, on islamic values. the second web resource is the questions and answers generated by islamic knowledge/come towards islam [cit] which is another widely-used web site, which contains monthly archives covering many topics concerned with islam such as questions and answers about the quran, understanding islam, islamic facts, holy quran chapters, teachings of the prophet muhammad, haram (forbidden) food and drinks, ramadan, women in islam and many more topics. [cit] . this web site also provides the quran text translated into many languages, as well as an islamic resource for reading and listening to the quran online with translation in various languages. the third web resources is all-quran [cit] web site, which aims to have the holy quran available to everyone in the world by having an easy way of audio streaming for a variety of quran reciters and audio translations. it contains a tab for islamic faq, in addition to further information about the holy quran. it has been a commercial-free website since it was created. the fourth web resource is the siasat [cit] web site, which also provides questions and answers about the holy quran. it is written in three languages: english, urdu, and hindi."
"as mentioned above, beside these four web resources, some questions and their answers were gathered from islamic experts at the holy mosque in mecca, and from previous research projects."
"data mining is defined as the process of discovering useful patterns in data. the process should be automatic or semiautomatic [cit] . data mining is the exploration and analysis of large quantities of data in order to discover valid, novel, useful, and understandable patterns in data. data mining is sometimes called knowledge discovery from databases. in knowledge discovery, what is retrieved is not explicit in the database. rather, it is implicit patterns. data mining finds these patterns and relationships using data analysis tools and techniques to build models. there are two main types of models in data mining. one is predictive models, which use data with known results to develop a model that can be used to explicitly predict values. another is descriptive models, which describe patterns in existing data. all the models are abstract representations of reality, and can be guides to understanding business and suggest actions. the two high-level primary goals of data mining, in practice, are prediction and description. the main tasks for data mining are: (1) classification: learning a function that maps (classifies) a data item (record or instance) into one of several predefined classes, (2) estimation: given some input data, coming up with a value for some unknown continuous variable, (3) prediction: same as classification and estimation except that the records are classified according to some future behavior or estimated future value, (4) association rules: determining which attribute or feature values typically go together in a data record or instance, also called dependency modeling, (5) clustering: segmenting a population of data records or instances into a number of subgroups or clusters, (6) description and visualization: representing the data using visualization techniques, for human inspection of patterns."
"the tv framework clearly promotes filled shapes as opposed to the stereo mismatch techniques. in fact, the total variation regularization favors piece-wise smooth candidates leading to foreground silhouettes suitable for further processing step to understand a scene. figure 6 illustrates the difference between the intensity-based approaches, the stereo mismatch approach presented in section 2, and our proposed approach. even if part of the scene has abrupt change of illumination, the foreground silhouette is still correctly segmented in figure 6 . any abrupt change of the background appearance can happen as long as the the change does not affect the 3d modeling of the scene. figure 7 shows the robustness of our algorithm when video is projected in the background."
"once foreground objects are present in the scene, the disparity d(0) does not correct anymore the mismatch over the objects' region. their silhouettes are correctly extracted if the objects have non-uniform color distribution. however, upon a uniform intensity, such approach extracts the contour of the foreground silhouettes instead of filled shapes, as required in further detection and tracking steps [cit] (see figure 3) . although pixels are not mapped correctly, they are compared with neighboring pixels having similar intensity value. objects with uniform regions induce intensity differences on boundaries only. a solution would consist in filling these contours, but as they are not stable and almost never closed, some heavy and slow processing should be applied. in addition, stereo mismatch method in practice induces a lot of noise due to poor calibration. therefore, we propose a total variation (tv) disparity estimation framework to extract foreground silhouettes based on the noisy stereo mismatch and hence promote filled silhouettes."
"medusa can deal with three faulty scenarios: i) with accidental faults, ii) with malicious faults, and iii) with cloud outages. initially, medusa will launch f 1 1 replicas of the job in distinct mapreduce runtimes, running in different clouds. when these jobs finish executing, medusa will validate the computation by comparing the digests of their outputs. medusa deals with accidental faults by reexecuting the same faulty job in the same clouds until it obtains equal results. the rationale for reexecuting in the same clouds is that accidental faults are inherently intermittent, so it is to expect they will eventually no longer affect the same job. this is in contrast with malicious faults or cloud outages, that require reexecutions in an extra cloud. in the former case, because one of the clouds cannot be trusted. in the latter, due to one of the clouds being no longer available. in any of these cases the framework reexecutes the faulty job in another cloud until it obtains f 1 1 equal results. in the malicious case, if the reexecution ends correctly it is possible not only to validate the results but also to find which cloud is compromised. the execution aborts if no final result is obtained and no more clouds are available."
"method 1: evaluating and selecting from a set of α's. this is a straightforward approach where different values of α (we considered 10 values of α spaced between 0.1 to 10) are table 1 . integral of copula differences for different dimensions. evaluated and the one with best predictive performance selected. the original training data is sub-divided into secondary training and secondary testing samples. the secondary training samples are used to create mrf models that are used to find prediction of secondary testing samples. this process is repeated for a set of possible values of α. the correlation coefficient between predicted secondary testing samples and original secondary testing samples are recorded and the α corresponding to highest correlation coefficient is selected (α s ). this α s is then used to create mrf model using the original training samples and tested on original testing samples. for our examples, we have applied 10 fold cv on the original data and thus for each fold of training data, we may select different α. however, for each specific fold, α will be fixed for all the trees generated. the above method increases the computational complexity due to the evaluation of multiple values of α. we next present another approach that attempts to reduce the evaluation of numerous values of α. our idea is to approximate the pareto frontier using straight lines and utilize the slope of the lines to design α. figs 1(b) and 2(b) shows that the pareto frontier can be approximated by two straight lines: one with slope greater than 1 and another with slope less than 1. consequently, the value of α can be approximated by the following equation."
"the correlation coefficients using 5 fold cross validation error estimation are illustrated for each drug set in table 5 . the corresponding mae and nrmse behaviors are illustrated in table 6 . for cmrf, results with scaling factor α selected using method-1 discussed earlier has been used. tables 5 and 6 shows that cmrf performed better than vmrf, kbmtl and rf in terms of correlation coefficients and nrmse for the related drug pairs s c1, s c2, and s c3 . when there is no relationship in the drug pair as in s u, univariate rf performs better than the multivariate approaches on an average. the scatter plots of predicted response vs original response for drug-set s c2 using rf, vmrf, cmrf are shown in fig 8."
"in text mining data structures tend to be more complex (e.g. phrase structures) than in bioinformatics so interoperability, which guarantees data type compatibility as implemented in u-compare, is required to create text mining workflows easily. with text mining workflows simplified, users would naturally seek integration of text mining workflows with other bioinformatics resources. taverna * to whom correspondence should be addressed. [cit] ) is a generic workflow construction system widely used in bioinformatics. we have developed mechanisms that allow users to embed any u-compare workflow in a taverna workflow. as u-compare is designed to facilitate the construction of text mining workflows, it is far simpler to expose a complete u-compare workflow to taverna than to construct the equivalent workflow from individual components within taverna, which lacks text mining specific data structures. it is therefore the easiest way to add text mining functionality to taverna."
replication is a common strategy to ensure the integrity and availability of distributed services in which individual components may fail due to crash faults or arbitrary faults.
"random forest (rf) regression refers to ensembles of regression trees [cit] where a set of t unpruned regression trees are generated based on bootstrap sampling from the original training data. for each node, the optimal node splitting feature is selected from a set of m features that are picked randomly from the total m features. for m ( m, the selection of the node splitting feature from a random set of features decreases the correlation between different trees and thus, the average response of multiple regression trees is expected to have lower variance than individual regression trees. larger m can improve the predictive capability of individual trees but can also increase the correlation between trees and void any gains from averaging multiple predictions. the bootstrap resampling of the data for training each tree also increases the variation between the trees."
"performance with arbitrary faults. we tested both solutions introducing arbitrary faults. the finegrained replication at the task level allows the system to react immediately when a fault happens in the map tasks, which explains why task replication was always the fastest solution in this case. when a fault happens in the reduce tasks the result is different. as in the case without faults, in the cb application, the reexecution of identity tasks makes the overall solution slower than job replication. in the ci application task replication was always faster than job replication, for the same reasons as before."
"an example run of the workflow, which sets 'pubmed_query' and 'workflowpath' parameters in the figure 1, and its result are given in supplementary material for the top 50 hits from the query 'saccharomyces and \"translation initiation\" '. the 'workflowpath' is set to a u-compare workflow given in the supplementary material, which runs 'uima sentence splitter' to detect sentence boundaries, 'abner' [cit] to detect protein named entities and 'eventmine' [cit] to detect interaction events. evaluations of the text mining tools are provided in u-compare [cit] . this example run detects 677 nonunique entities, which by exact string matching correspond to 371 unique entity names, and 254 non-unique binding events."
"for our experiments, we consider four sets of drug pairs where three of them have common primary targets and the remaining pair has no common target. we expect that the drug pairs with common primary targets will have some form of relationship among their sensitivities and cmrf should perform better than vmrf and both should perform better than rf approach. on the other hand, the drug pair without any common targets is expected to have minimal relationship among the drug sensitivities and thus rf is expected to outperform cmrf and vmrf. we also present results on 3 drug set and 138 drug set for gdsc as tables c, d and e in s1 file."
"we have developed two solutions that follow the generic scheme of figure 3 : medusa and chrysaor. their main difference is that they work at different levels of granularity: medusa deals with faults at the job level, whilst chrysaor deals with faults at the task level. sections \"replication in medusa\" and \"replication in chrysaor\" describe these solutions. equal, as digests are collision-resistant (no two different outputs can produce the same hash). otherwise, the proxy cannot identify which of the replicas is faulty and it will react accordingly; it only knows that there is a disagreement on the result."
"we have examined the variable importance measure for gdsc data using vmrf and cmrf in terms of protein interaction network enrichment analysis. in this section, we will primarily provide the detailed results for s c1 in gdsc. to avoid any bias due to feature selection in variable importance, we consider the full set of probe set ids without application of relieff for this analysis."
"we have evaluated our approach on three sequences: a scene made of two people walking (figure 2 to 5), a conference room ( figure 6 and 7), and an indoor office (figure 1 ). first, we have extracted foreground silhouettes given the temporal disparity variation, i.e. comparing the background disparity with the current estimated disparity. figure 4 presents the silhouettes extracted given such approach. it can be seen that the (a) original image (b) graph cuts [cit] (c) tv framework [cit] (d) our approach fig. 4 : foreground silhouettes obtained by temporal depth variation using two depth estimation techniques. for comparison purposes, we also present in (d) the output of our proposed tv disparity-based approach."
where μ(η p ) is the expected value of y(i) in node η p . thus the reduction in cost for partition γ at node η p is
"cloud computing has enabled computation of massive volumes of data that traditional database and software techniques had difficulty processing in an acceptable amount time. 5 one of the most popular distributed data-processing systems for analyzing big data in cloud environments is hadoop mapreduce, an open-source platform based on google's mapreduce paradigm. 6, 7 the popularity of this framework made the mapreduce model prevalently used for critical applications, such as medical research and finance, where outputting wrong results and service unavailability may be unacceptable. unfortunately, hadoop does not deal with arbitrary and malicious faults and does not scale the computation out to multiple clouds to deal with availability issues. in this article, we give an overview of our recent research on scaling out hadoop to multiple clouds for tolerating arbitrary faults, malicious faults, and cloud outages (unavailability of entire data centers). this contrasts with previous work on multicloud mapreduce (e.g., g-hadoop) that has not considered resilience, having focused exclusively on scalability of computation. 8 the design guidelines we propose include two additional goals to foster adoption: the overhead should be acceptable, and no changes to hadoop nor to the user's code should be required."
"estimating data processing time. the time for completing a given mapreduce job mainly depends on the following variables: i) the capacity of the cloud running this job; and ii) the configuration of the job. for example, a high level of parallelization (i.e., a large number of map and reduce tasks) for the same job in the same cloud, and having tasks accessing mostly local splits implies shorter data processing times. as such, estimating the data processing time involves having a scheduler that takes into consideration three types of features: job configuration; cloud capacity; and cloud overhead. we describe the representative features for each type in the following:"
"we first present a description of random forest regression followed by extension to multivariate random forest regression utilizing the covariance structure of the data. subsequently, the concept of copulas is introduced along with their application in designing node splits for multivariate regression trees."
we have presented a tv disparity-based approach to extract foreground silhouettes with stereo cameras in a manner robust to sudden changes of background appearance. the tv framework penalizes the sketchy-like solution made of contours only and hence promotes filled silhouettes as required in many applications. (b) illumination modeling [cit] (d) our approach (a) intensity difference [cit] (c) disparity mismatch [cit] fig. 7: illustration of the output of our proposed tv disparity based foreground extraction algorithm where part of the scene has sudden change of illumination.
"with i l (t) and i r (t) being respectively the left and right images of the stereo camera at time t. the wrapping operation,"
"the taverna u-compare workflow illustrated in figure 1 can be used to run any u-compare workflow, and does not require reworking for different text mining analyses. whilst developing a new text mining workflow does involve configuration within the u-compare environment, once developed, it can be deployed within the taverna activity described here by resetting the 'workflowpath' value to the location of the new workflow's descriptor, relative to a directory [user home]/.ucompare/taverna/classpath-root/."
our previous analysis of c provided a range of the integral difference between two copulas but was unable to provide a weight factor for combining d 1 and d 2 that is optimal in terms of predictive performance. we expect that the behavior of d 1 and d 2 will change significantly for different training datasets and thus we select α based on each training dataset. we next describe two techniques to select the weight factor α to achieve higher prediction accuracy.
"the u-compare taverna plugin works with taverna version 2.1.0. the user must specify two options: the u-compare workflow to embed and a post-processing beanshell script with proper i/o ports to appropriately reformat the output for further processes in taverna. this post-processing script is executed as the final uima component in the u-compare workflow. uima and u-compare apis can be used in the script. upon running the taverna workflow the u-compare application starts and runs the specified text mining workflow automatically, then shows u-compare guis such as statistics and visualizations of generated annotations. this plugin is implemented to download, install and update u-compare automatically. users are only required to install the plugin from the taverna's menu by inputting the plugin url. as running guis can be demanding when processing a large number of documents this plugin is mainly for testing and analyzing with results visualization. users can deploy two modes of workflow inputs to the u-compare taverna plugin. in the typical mode, the u-compare workflow takes a collection reader as input, generates a list of annotated documents as output. we have also implemented another mode to link specifically with taverna where the input is not the collection © the author(s) 2010. published by oxford university press. this is an open access article distributed under the terms of the creative commons attribution non-commercial license (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited."
"unlike medusa, chrysaor replicates tasks, not jobs. all map and reduce tasks produce a digest of their output and the proxy compares the digests of every set of replicas. this allows the identification of which tasks have produced different results and to reexecute them immediately after they finish, instead of having to wait for the end of the job and to reexecute it fully (as in medusa). faults in the map phase must be dealt slightly differently from faults in the reduce phase (this will be made clearer in the section \"fine-grained replication\")."
"in both vmrf and cmrf, the 50 top ranked probesets were generated separately. it should be noted that multiple probeset ids can map to a single gene symbol of a protein. this mapping was done in genome medicine database of japan (gemdbj) id conversion tool (https:// gemdbj.nibio.go.jp/dgdb/convertoperation.do). based on this mapping, we arrived at 58 top ranked proteins for vmrf and 70 top ranked proteins for cmrf. these proteins were provided as inputs to the string-db database (http://string-db.org/) for known protein-protein interactions. the protein-protein interaction (ppi) networks for top proteins using cmrf and vmrf are shown in figs 9 and 10 respectively. the enrichment analysis for both the networks are shown alongside each network. we observe that the network generated using cmrf is more enriched in connectivity than the network generated using vmrf. 18 interactions with a p-value of 0.132 were observed for the vmrf ppi network whereas a total of 35 interactions with a p-value of 0.00775 were observed for the cmrf network. moreover, the common target egfr is picked in the top 50 targets and is well connected to other targets of cmrf whereas egfr is not selected even in top 150 targets of vmrf. similarly, in drugset s c2 of gdsc (network not shown), there are 42 interactions with 51 proteins in cmrf and 25 interactions with 54 proteins in vmrf."
the gdsc gene expression and drug sensitivity dataset was downloaded from cancerrxgene. org [cit] . the dataset has 789 cell lines with gene expression data and 714 cell lines with drug response data. we considered the intersection of cell lines that had both drug response and gene expression data.
we have observed that multivariate random forests incorporating covariance (mahalanobis distance square) between output responses is more suitable for predicting output responses with linear relationships as compared to output responses with non-linear relationships. copula presents a methodology to capture the non-linear dependence relationships between multiple variables; and we anticipate that copula will be suitable for predicting output drug responses with non-linear relationships between them. we next present a synthetic example with non-linear relationship to investigate the performance of the proposed approach as compared to covariance based mrf design.
"devising a context-based scheduler that distributes replicated tasks across different clouds based on network throughput and computational power requires predicting which clouds (and which connections) will be the fastest. this prediction needs to consider both the historical performance as well as the current status of each cloud, allowing us to incorporate the heterogeneity of the clouds into the scheduling decision. as a consequence, our scheduler is split into two parts: one for estimating data transmission time and the other for estimating data processing time. we detail each metric in the following:"
"in summary, the main conclusions that can be drawn from our experiments are that task replication is favorable for workloads that are i) more computationally-intensive and ii) centered in map tasks. for (i), in this sort of application, the relative overhead of the logical job abstraction is low. for (ii), and independently of the nature of the application, a fault in a map is always handled more efficiently with chrysaor fine-grained replication scheme. importantly, in most mapreduce jobs the number (and size) of map tasks is much larger than the number (and size) of reduce tasks, which means that in the common case the benefits of a fine-grained solution will outweigh the overhead introduced to guarantee transparency."
note that the generation of the node cost function based on copulas presented in this paper can be considered as a generalization of the multivariate random forest framework based on the square of mahalanobis distances [cit] . the presented approach can be applied to any predictive modeling scenario with multiple interrelated output responses.
initially each cell line has 22277 features (probeset) as gene expressions. we have reduced it to 500 for each drug response using relieff [cit] and used a union of the 500 features in each of the four sets of drugs.
"in this article, we presented an approach to extend ensemble learning using regression trees to multivariate ensemble learning. we utilized the concept of copulas to represent the relationship between different drug sensitivities and incorporated them in the design of multivariate regression tree cost function. we designed the node cost function as a combination of (a) the sum of square of the differences from the mean and (b) a measure of the difference in the multivariate structure at the node compared to the original training data. the difference in the multivariate structure was captured as the integral of the absolute difference in the copulas observed at the node and the original training data. two approaches were presented based on enumeration and pareto frontier to design the weights of the two parts of the cost function. utilizing synthetic and biological data, we showed that the proposed copula based approach could increase the prediction accuracy as compared to univariate random forests or multivariate random forests based on covariance based node cost. as compared to rf, the gain in the correlation coefficient between predicted and experimental values was observed in scenarios where there exists a relationship between the drug pair sensitivities. the examples were also able to illustrate that cmrf is better suited for selecting the relevant features as compared to vmrf. the proposed methodology provides a novel technique to design multivariate regression trees for scenarios where there are nonlinear relationships between output responses. the presented research can be extended in multiple directions. one such direction will involve extending the concept of maintaining the multivariate structure in the design of weights of individual trees. another direction consists of analyzing the detailed bias and variance relationship of the proposed technique and designing confidence intervals for the predictions. supporting information s1 file. supporting information for article: a copula based approach for design of multivariate random forests for drug sensitivity prediction. rf, vmrf, cmrf results (5 fold cross validation) with and without prior feature selection ( table a) . results for ccle dataset drug sensitivity prediction for a drugset with 4 drugs in the form of correlation coefficients for rf, vmrf, cmrf and kbmtl approaches ( table b) . results for gdsc dataset drug sensitivity prediction for a drugset with 3 drugs in the form of correlation coefficients for rf, vmrf, cmrf and kbmtl approaches ( table c) . results for gdsc dataset drug sensitivity prediction for a drugset with 140 drugs in the form of correlation coefficients is shown (only 15 drugs that are common with ccle are shown in detail while the average represents the average of all 140 drugs) ( table d) . results for gdsc dataset drug sensitivity prediction for a drugset with 140 drugs in the form of nrmse is shown (only 15 drugs that are common with ccle are shown in detail while the average represents the average of all 140 drugs) ( table e) . results for ccle dataset"
"owing to the large and rapidly growing body of literature in the biological sciences text mining approaches are increasingly important for the extraction and collation of data. the collation of text mining data with bioinformatics databases is a separate topic [cit] which space precludes enlarging on here. yet many text mining methods are difficult to integrate with other bioinformatics tools as development tends to focus upon mining performance more than implementing accessible interfaces. u-compare [cit] ) is an all-in-one text mining system based on the uima (unstructured information management architecture) framework [cit] . u-compare mainly consists of two parts: the world-largest readyto-use uima component repository, and an all-in-one text mining platform."
"large-scale data processing jobs. 7 hadoop map reduce is an open implementation that appeared a few years later and that is currently the most adopted. 6 the term mapreduce denominates both a programming model and the corresponding runtime environment. as the name indicates, mapreduce involves two functions: map and reduce. the unit of execution is the job, which is typically broken into one phase that executes map tasks and another that executes reduce tasks (each task runs the map or reduce functions once). figure 1 shows a generic example of the execution of a job. the input data is split into files called splits. when a job starts running, each split is processed by the map function in a map task (map phase). then, the result of the tasks is partitioned, transferred, and sorted (shuffle & sort phase). in the end, the reduce tasks process the partitioned data using the reduce function (reduce phase). this simple model can express many real-world applications. 7 the main components of hadoop are the hadoop mapreduce and hadoop distributed file system (hdfs) (see figure 2 ). in hadoop, map reduce jobs are submitted to and managed by a central component called resource manager. the resource manager assigns map and reduce tasks to node managers, monitors these nodes, and tracks the progress of the job execution. the node manager is responsible for managing containers where tasks run. although the figure shows a single node manager, typically there are many of those, as they are the components that do the (large-scale) data processing. a hadoop mapreduce runtime works in a single data center. hdfs is the default file system for hadoop mapreduce. it stores files broken into blocks that are replicated in different servers (data nodes) for fault tolerance. hdfs can handle many servers for scalability."
"in this paper, we consider that mapreduce tasks, both map and reduce, can suffer arbitrary faults, often called byzantine faults. these tasks may for instance stop or produce wrong results. to deal with these faults we execute two or more replicas of each task. we assume that there are limits on the number of faulty replicas and clouds (including resource managers), and that there is a proxy that does not fail (details next)."
"u-compare and taverna focus upon different target domains, with the former utilizing a more strongly typed system specifically designed to handle the particular problems of text mining, while the latter offers a more generic solution for broader applications. by linking u-compare with taverna, we provide an easy way to create and embed complex text mining workflows within taverna."
"to capture the multivariate structure present in the output responses, we consider the use of copulas as they can deconstruct a multivariate distribution into its marginal distributions and underlying relationships that are represented by copula functions. we expect that the multivariate distribution of the sensitivities to a drug set will change based on the type of cell lines they are being applied to but the relationship structure separated from the marginal sensitivity distributions will remain similar. as an example, consider two drugs gefitinib and lapatinib that might have higher sensitivities when applied to breast cancer cell lines but lower sensitivities when applied to brain tumor cell lines. thus, the multivariate distribution representing the sensitivities to the two drugs will appear to be skewed towards higher values for breast cancer cell lines and skewed towards lower values for brain tumor cell lines. however, we might observe similar correlation coefficients between the sensitivities for the breast cancer cell lines and the brain tumor cell lines as the primary target of both the drugs (egfr) maintains the relationship. the correlation coefficient is one of the measures of the multivariate structure that will mostly capture linear relationships. however, incorporating the ability to separate the marginal distributions from the multivariate distribution will provide us with a more detailed representation of the underlying associations."
"in the era of cloud computing, hadoop mapreduce has emerged as a popular tool for processing big data in a distributed way. the mapreduce framework is prepared to tolerate crash faults by reexecuting tasks, but other faults that can affect the correctness of results are known to happen and will probably happen more regularly in the future. moreover, the design of mapreduce is targeted to a single data center (a single cloud), which makes this framework vulnerable to cloud outages, which are also common. based on our recent experience in building such systems, in this article, we present three techniques to assist in the design of multicloud resilient mapreduce systems. namely, minimizing the required replication; applying context-based job scheduling, based on cloud and network conditions; and performing fine-grained replication. put together, these techniques offer resilience at reasonable cost, and they are immediately deployable using existing, unmodified hadoop mapreduce solutions."
"for cmrf, results with scaling factor α selected using method-1 discussed earlier has been used. the robustness analysis of α using synthetic data is conducted using method-2 and is shown as tables h and i in s1 file. table 3 shows that cmrf outperformed (in terms of correlation coefficients) vmrf, rf and kbmtl for the related drug pairs s c1, s c2, s c3 whereas cmrf is outperformed by the other approaches for the unrelated drug pair s u . table 4 shows that cmrf outperforms vmrf, kbmtl and rf in terms of average nrmse for the related pairs of drugs s c 1, s c 2 and s c 3 . for the unrelated pair s u, univariate rf outperforms the"
"performance without faults. the approach followed by medusa of replicating jobs achieves slightly better performance when compared with task replication for the cb application. the reason is the main overhead introduced by the required logical job abstraction: the identity map tasks require additional computational time. this additional computational time came from the fact that the output produced by the map tasks are larger than the input data. overall, the characteristics of the cb application have brought a penalty to the new abstraction. interestingly, the results are inverse with the ci application. as the application is computationally intensive, the relative cost of the identity maps is less pronounced. in addition, one optimization allowed by the chrysaor design (namely, the generation of digests while the output is being produced) overcomes the logical job overhead."
"similar to medusa, when chrysaor is in the presence of a fault, it cannot identify which of the replica(s) is (or are) faulty. when dealing with accidental faults, chrysaor has the ability to reexecute the task for which there was no f 1 1 identical digests in the same clouds, until it obtains f 1 1 equal results."
"the performance of most multi-view people detection algorithms depends on the quality of the extracted foreground silhouettes. the latter are the backbone of more advanced systems to detect, track and analyze people behavior [cit] . foreground silhouettes are binary masks representing the connected pixels belonging to the foreground of the scene. static cameras can model the background of a scene by collecting statistics on every pixels. porikli presents some of the methods in a brief survey [cit] . typically, a decade ago, stauffer and grimson modeled each pixel as a mixture of gaussian with an on-line approximation for the update [cit] . a lot of efforts have been dedicated to enhance these algorithms to best model the temporal intensity variation [cit] . however, sudden changes of lighting conditions dramatically affect the performance of the extraction process. the latters are only robust to gradual changes of lighting conditions whereas sudden changes such as turning on and off the indoor lights, spot light effects in exhibitions, camera flashes are not well addressed. figure 1 illustrates the problem of any intensity-based background subtraction algorithm to extract foreground silhouettes when a sudden change occurs. recent work have addressed the sudden change of illumination issue (b) illumination modeling [cit] (c) our approach (a) intensity difference [cit] fig. 1: illustration of our tv disparity-based foreground extraction algorithm (right column) compared to a traditional intensity-based algorithm (2 nd and 3 rd column). the background is modeled at time t 0 . we turned the light off at frame t 0 + 2 and succeed to locate the foreground object although two opposite intensity variation occurred (turning down the light in the room and up in the corridor). [cit] but either suppose that the change is global or do not work in environments when videos are projected in the background. as a result, we propose to tackle this problem with stereo imaging. instead of classifying the image intensity as background or foreground, the estimated disparity map is processed to identify foreground regions. there have been many attempts to do segmentation using stereo cameras, mainly for background subtraction applications [cit] . however, stereo vision algorithms are only used as a post processing tool to enhance the foreground silhouettes extracted given the intensity variation of each camera. therefore, these algorithms fail again when sudden intensity changes occur."
"note that for a continuous feature with n samples, a total of n partitions needs to be checked. thus, the computational complexity of each node split is o(mn). during the tree generation process, a node with less than n size training samples is not partitioned any further."
"equation 3 is equivalent to the brightness consistency assumption. to compute the disparity maps, we suppose that the brightness does not change between the left and right images. this can be done without loss of generality by supposing identical calibration or applying histogram matching methods."
"as any workflow can also be called via the command line we provide special uima components, which reads input text and writes generated annotations via the standard i/o streams. using u-compare's command line mode, we created an example taverna workflow of a protein-protein interaction extraction, selected to show its usefulness in systems biology [cit] . this workflow outputs a possible interaction network from the literature associated with a pubmed query. extracted information is available in supplementary material. this example workflow is provided as a template for users to create their own workflow by imitating, reusing and modifying the interpretation part. since u-compare outputs results in a uniform format users only need to change the specific data types and their corresponding fields to create their own workflows, reusing most the template codes without modification. figure 1 shows a diagram of the example taverna workflow. a box labeled 'ucompare' corresponds to a beanshell script activity, calls u-compare via the command line. in this article, a mechanism to download, install and update the u-compare system is also implemented, so there is no need to explicitly install anything. in parts prior to 'ucompare', this workflow retrieves documents from pubmed, passes them to the u-compare activity, then interprets the results in parts following 'ucompare'."
"where λ is the covariance matrix, y(i) is the row vector (y(i, 1), á á á, y(i, r)) and μ(η p ) is the row vector denoting the mean of y(i) in node η p . the inverse covariance matrix (λ −1"
"loud computing has emerged as the paradigm for outsourcing computation. cloud service providers have been building massive data centers that are distributed over several geographical regions to efficiently meet the demand for this service. these data centers typically contain tens of thousands of commodity servers and use virtualization technology to do provisioning of computing resources. clouds are starting to be used together, forming multiclouds. 1 when the combination of clouds is created by users inconspicuously to the cloud providers, such multiclouds can be called clouds-of-clouds. 2 the purposes of using several clouds vary, but common goals are increasing performance and reducing costs. dependability problems in cloud services can cause great losses to its users and are becoming increasingly common. hardware components are prone to soft and hard failures that reduce their reliability and the availability of the cloud service, with impact on the software running atop. studies made at google and microsoft concluded that errors in the dram, chipset, and cpu of commodity servers are more prevalent than previously believed. 3, 4 therefore, fault tolerance in cloud computing platforms and applications is a crucial issue to the users, not to mention the cloud providers themselves."
"hereafter, the mrf approach that uses copula based node splitting criteria (based on eq 11) will be termed as cmrf and the mrf approach using covaraince based node splitting criteria (based on eq 7) will be termed as vmrf."
"estimating data transmission time. the data transmission time between two clouds depends on i) its geographical distance, ii) the network throughput, and iii) the size of the data to transfer. considering that the throughput varies with respect to the traffic load to other clouds (among other variables), the framework needs to periodically monitor the throughput for each pair of clouds in the system."
"note that we explained the difference between handling accidental faults and malicious faults as if the system was able to distinguish them, which is not the case. in practice, the system is configured with a threshold on the number of times it tries to handle faults as if they were accidental, then considers them malicious (i.e., starts using a new cloud)."
") is a precision matrix [cit] which is helpful to test conditional dependence between multiple random variables. the mahalanobis distance square normalizes the output responses by their standard deviations and in case of λ being diagonal, it represents the normalized euclidean distance. for bivariate case with covariance λ, the node cost is increased when the deviations of the two output responses from the mean responses are in opposite directions."
"estimated disparities are too noisy and not consistent across time. although the graph cuts and tv approach are only illustrated in figure 4, other local and global algorithms have also been tested and led to similar results. as mentioned previously, the performance of depth estimation algorithms depends on the content of the scene. the background disparity, d(0), corresponds to an empty scene with poorly textured and uniform regions, which is one of the most challenging scenes for a disparity estimation algorithm. therefore, the resulting disparity is noisy and leads to useless foreground silhouettes as illustrated in figure 4 . in contrast, using our tv disparity based approach, two people walking in the scene are correctly segmented as foreground (see figure 5 )."
"as described earlier, the regression tree generation process involves partition of a node into two branches based on optimizing a cost criterion. the node cost for univariate regression trees is given by eq 1 and the node cost for multivariate regression trees utilizing mahalanobis distance is shown in eq 7. the feature and threshold that results in maximum cost reduction for that node is selected for splitting. we next discuss the design of node cost function based on copulas to capture the output dependencies. we expect that the dependency structure among the samples in a node should be similar to the dependency structure observed in the original training data. consider the node η p with n p samples and let c denote the integral of the difference in the empirical copulas observed at node η p and the root node (this is same as the empirical copula for the training data). we design the node cost d c (η p ) for a copula based multivariate regression tree as follows:"
"hadoop was designed to be fault-tolerant as, with thousands of devices (computers, network switches, routers, and power units), component failures are necessarily frequent. hadoop tolerates faults using two techniques: i) monitoring and restarting tasks when servers, node managers, or the tasks crash; and ii) adding checksums to the files in hdfs to detect data corruption in disks. however, these mechanisms only work in a single cloud, cannot deal with cloud outages, and only tolerate crash faults-not arbitrary or malicious errors."
"we formulate the foreground extraction problem as a correspondence problem from the left image i l to a wrapped one, i w . the latter is obtained by applying the background disparity to the right image as in equation 2. the correspondence problem is solved using a tv-1 framework where the 1 norm penalize deviations from the brightness consistency assumption and a total variation regularization term penalize a sketch-like solution with contours and promote a solution with filled shapes. in other words, our approach to improve the foreground silhouette extraction is to replace the subtraction operator in the disparity mismatch by a tv-1 disparity computation. we first compute the background disparity between the left and right image, d(0). then, for every frame, the foreground image, f (t), is computed by measuring the disparity between the wrapped and the left image. to get a mask, a simple threshold is used on the output."
"14 that framework is a modified hadoop mapreduce that essentially replicates map and reduce tasks (i.e., runs several copies of each), then compares the results obtained by replicas to detect byzantine faults. the main challenge of this solution is the efficiency of replication. this is achieved by requiring only f 1 1 replicated tasks to tolerate f faults in case there are no faults, instead of the 3f 1 1 replicas the typical byzantine fault-tolerant state machine replication algorithms would require. 15 however, this solution works only in a single data center and as such does not tolerate cloud outages. figure 3 generically presents our approach of replicating mapreduce in a set of clouds, i.e., in a multicloud. the proxy is the central component to which the client submits a mapreduce job (note that this is transparent to the client-she does not need to be aware of the presence of a proxy). the proxy is an intermediate component between the user and the hadoop resource managers installed in different clouds. in this example, each cloud contains one hadoop runtime, and the proxy interacts with the several runtimes by sending them hadoop commands (e.g., instructing to start a job) over a secure channel. consider that f is the maximum number of task replicas and clouds that may fail (we normally consider these two thresholds separately, but we are simplifying here). when a job is submitted by the client, our solution involves executing f 1 1 replicas, one per cloud. in the figure, we consider f 5 1, so the proxy selects two clouds-a and b-to execute the replicas. during the execution, each task replica will produce an output and the respective digest (a collision-resistant hash, calculated, e.g., using sha-256). the digests will be compared to check if the result is correct: if there are f 1 1 equal outputs then that output is correct as at most f replicas may produce wrong results. otherwise, a new replica is executed. if a cloud stops responding a new one is selected (cloud c in the figure)."
"in this article, we discuss the appropriateness of copulas for capturing the multivariate structure in output responses and subsequently propose a cost function utilizing copulas for evaluating multivariate regression tree node splits. the cost function is a weighted combination of (a) the sum of squares of the differences between the node and mean responses and (b) the difference in the empirical copula observed at the node and the copula representing the training samples. we also demonstrate the suitability of the framework in variable selection where it provides higher importance to biologically relevant features as compared to competing approaches."
"let the t trees of the random forest be denoted by θ 1, á á á, θ t and let w i (x) denote the average weights over the forest i.e."
"when dealing with malicious faults or cloud outages, it is necessary to execute the tasks in an extra cloud, for the same reasons as above. if the system is dealing with a fault in a map task, chrysaor executes the faulty tasks in another cloud until it obtains f 1 1 equal results. if the reexecution of the map tasks has ended correctly, the solution has the capability to validate the results and find which cloud is compromised and exclude it from the rest of the execution. if a malicious fault or cloud outages have happened during the execution of the reduce tasks, it is necessary to run a new full job in a new cloud, and then validate the output. the execution aborts if no correct result is obtained and no more clouds are available to reexecute the tasks."
"we evaluated the two systems that form the basis of our multicloud resilient mapreduce designmedusa and chrysaor-experimentally, by running the two prototypes using several nodes in different regions of the amazon ec2 service. we ran several real-world applications (available with hadoop). in this article, we focus on the comparison of the two replication approaches-job replication (medusa) and task replication (chrysaor) -between themselves and with the original hadoop. we consider two applications: one communication-bound (cb) and the other computationally-intensive (ci). we present the results in table 1 . we invite the interested reader to obtain further information on the evaluation, including a detailed analysis on its several results, in the paper that proposed chrysaor. 13 comparison with the original hadoop. one of our goals was to have an acceptable performance overhead, so the table shows average times for the execution of the original hadoop in the clouds considered. our job and task replication solutions have an overhead, as seen in the table, but it is reasonably low (between 16% and 39%). an overhead was unavoidable, as we are doing more computation: this is the price to pay for the benefit of tolerating severe faults. the overhead is limited mainly due to the principle of just enough replication explained in the section \"just enough replication\"."
"performance with malicious faults. the conclusions of these experiments are similar to the previous case. as before, replication at the task level was always the fastest when tolerating faults in the map tasks. however, task replication was slower when dealing with faults at the reduce side in comparison with job replication when the job was not computationally-intensive, again due to the need to execute identity map tasks in the second logical job."
"disparity estimation at every time frame is computationally expensive. advanced high complexity algorithms cannot be used for real-time segmentation. conversely, stereo mismatch is a real-time alternative often proposed in the literature [cit] . in such a case, the background disparity is computed once at the beginning. then for every frame, the background disparity is used to wrap every right image on the left one to measure similarity. the foreground silhouettes are computed as follows:"
"note that the output responses are dependent on only 4 features out of the 10 possible input features. based on the relative weights, x 2 is the most weighted feature and should play a critical role while growing the trees at the beginning. note that y 1 and y 2 has a quadratic relationship."
"we first analyze the upperbound on the integral of the difference between two bivariate copulas and subsequently explore further multivariate copulas. based on frechet-hoeffding bounds [cit], any bivariate copula c(u, v) is bounded by the following:"
the paper is organized as follows: the methods section provides a description of the random forest framework with proposed extensions to copula based multivariate random forests including design of the node cost function and an illustrative example. the results section contains the performance of the proposed approach when applied to genomics of drug sensitivity in cancer database. the conclusions section presents the conclusions of the current study and discusses future directions.
mapreduce was originally designed by google for calculating web search indexes and running other w w w. co m p u t e r . o r g /c lo u d co m p u t i n g
"following our work in medusa, the previous section mainly considered replication of mapreduce jobs to obtain fault tolerance and availability. in that section, even if a single small task of a large job fails and produces a wrong output, then the whole job will produce a wrong output and will need to be reexecuted."
"to improve the efficiency of the system, it would be desirable to perform replication and reexecution at the task level. however, this fine-grained form of replication is a challenge, as it would require modifying the hadoop mapreduce source code. the main reason of the problem is that the execution of a job in hadoop mapreduce cannot be interrupted \"externally\". the need to modify hadoop is, however, problematic as it would require users to use our own version of the platform, hindering adoption (for instance, users could not use publicly available versions, such as amazon elastic mapreduce). we introduced a new abstraction in chrysaor to perform fine-grained replication without changing hadoop: the notion of a logical job. from the hadoop viewpoint, each logical job is a complete mapreduce job, but from the chrysaor viewpoint, there is one logical job to execute the map tasks and another one to execute the reduce tasks (see figure 5 ). moreover, if the replicas of a task produce different outputs, a new logical job is created to reexecute only that task. the use of logical jobs is transparent to the clients' applications, which request the execution of jobs as usual."
"the u-compare system itself is a stand-alone application. in this platform, users can create a workflow from the components in the repository, or any third-party uima components, in an easy dragand-drop manner and compare, evaluate and visualize the workflow results. the entire system can be started by a single mouse click in the u-compare website. workflows can also be executed via the command line without the gui based platform."
"the core techniques introduced in our design emanate from our experience in building two resilient multicloud mapreduce solutions-medusa (code available at https://bitbucket.org/pcosta_pt/ medusa) and chrysaor (code available at https:// bitbucket.org/pcosta_pt/chrysaor)-and one earlier solution that performs replication in a single cloud. [cit] we evaluated our solutions in a real testbed, considering several mapreduce applications, to assess the performance in different scenarios (see the \"evaluation\" section). the main result is that, by applying the proposed techniques it is possible to scale out hadoop mapreduce to multicloud environments to tolerate the above-mentioned classes of faults at reasonable costs, while requiring minimal modifications to the users' jobs, and in a way that is compatible with any hadoop map reduce version."
"when we are dealing with several cloud providers, we are facing heterogeneity in the server machines and the network. choosing the best clouds is critical to gain in performance. notice that we do not mean the \"best cloud\" per se, but the cloud with more resources available to the user at the moment of submitting a mapreduce job. naturally, if a job runs in a particular cloud with high computational power and is connected by high-bandwidth links, it ought to take relatively shorter time for the job to finish. on the contrary, if a cloud has low bandwidth links and low computational power, or if it is overloaded, it might take longer for the job to complete."
"since, the mahalanobis distance captures the distance of the sample point from the mean of the node along the principal component axes, it might be unable to capture nonlinear relationships that produces a closer to diagonal covariance matrix. thus, our objective is to introduce copulas to capture the nonlinear multivariate structure."
"to address these challenges, the design we propose for resilient mapreduce systems is based on three core ideas. the first consists of performing replication of the processing in a set of clouds. using replication may be considered expensive, but cloud outages are becoming so common that even cloud providers are exploring this approach (amazon recently launched the cross-region replication service). [cit] importantly, our solution minimizes the replication overhead (see the section \"just enough replication\"). the second idea is to leverage the diversity provided by a multicloud environment in the design of contextbased scheduling schemes that distribute the processing across clouds in such a way that performance is improved (see the section \"context-based scheduling\"). third, the solution should include fine-grained replication (at the task level) to achieve quick recovery in case of a fault. this can be achieved without modifying the hadoop source code by means of a new abstraction we propose: the logical job (see the section \"fine-grained replication\")."
"if the marginal cumulative distributions (f i (x)) are continuous, copula c is unique [cit] . some copulas can be parameterized using few parameters, for instance, the clayton copula [cit] for bivariate distribution is defined as follows using parameter ξ:"
"during the first logical job execution, each map task creates a digest of the map output. in the figure, the output data is represented as the squares exiting each map task. the digests will be fetched and compared by chrysaor to check if all map task replicas produced equal results. this is the case in the example (we are considering no faults), and so the second logical job is launched. the second logical job cannot start from the reduce tasks (a mapreduce job always starts from a map function). to solve this issue, we start this job with an identity map task, a simple task that outputs the input without modification. the second logical job will then read the data that was stored previously using identity map tasks, and perform the \"shuffle & sort\" phase before the reduce tasks start. each reduce task will produce the final output and the system will compare the results. in the example, as there are no faults, the results are equal and the job execution terminates successfully. if that was not the case, a new logical job would be created to reexecute only that task."
"two strategies can been exploited to extract foreground silhouettes without considering the temporal intensity variation: (i) temporal disparity variation, and (ii) stereo mismatch, where the disparity mismatch is calculated from an estimated background disparity [cit] . the first approach depends on the performance of the disparity estimation algorithms. although a lot of effort has been dedicated to find accurate algorithms to estimate disparity maps, the performances are often poor and not consistent across scenes. algorithms often perform poorly on uniform and non-textured (a) frame t 0 (b) block matching [cit] (c) graph cuts [cit] (d) tv framework [cit] (a) frame t 1 (b) block matching [cit] (c) graph cuts [cit] (d) tv framework [cit] regions (see figure 2 ). stereo mismatch algorithms compute the background disparity only. even if the background disparity is correctly estimated, the extracted foreground silhouettes are still very noisy and sketchy (see section 2) . in this paper we propose to use a total variation (tv) framework for the extraction of dense foreground disparities to fill the sketchy look of the stereo mismatch and hence extract foreground silhouettes regardless the temporal changes in background appearance."
one of the major challenges of virtualizing softwaredefined networks is the choice of the network abstraction model that will be used to abstract the physical infrastructure. there are currently two main approaches: i) the overlay network model and ii) the single router abstraction model.
"to the best of our knowledge, most network control languages use the overlay network model (fig. 1a) which consists in overlaying a virtual network of multiple switches on top of a shared physical infrastructure [cit] . virtual switches are very similar to standard switches in the physical infrastructure: they include lookup tables, ports and expose a set of basic forwarding actions. virtual switches can also map to one or more physical switches, and are connected to each other within a logical topology via virtual links."
"huba and freed have elegantly outlined and compared the criteria for teacher-centered and learner-centered paradigms (table 2) . 7 as huba and freed point out, student learning is the ultimate goal that the educators should focus on and, as a result, they must shift from a traditional teaching model to a learner-centered model. 7 in a learner-centered environment, students are more active in a teamwork manner than a teacher-centered paradigm. similarly, faculty are more engaged in accommodating and facilitating discussions among students, are encouraged to be innovative in teaching and assessment, integrate more disciplines into their teaching, and faculty and students assess student learning together. 7 on the other hand, in a teacher-centered paradigm, the focus is on how well a faculty teaches. questions such as how well organized and accurate the lectures are, how well the presenters maintain student interest, and how well the material is presented, are the focus of teacher-centered instruction."
"there are two kinds of mib variables: scalar values and table entries. scalars have a single value, such as the interface number shown above. for example, the ifnumber mib variable of a router is a single number that represents the total number of its interfaces. table values, on the other hand, provide the same pieces of information for different items, such as the traffic for each of a router's ports, or information about each of the tcp connections in a device."
"abstract-software defined networking (sdn) is a recent paradigm that aims to reshape the way we configure and manage today's networks. to fulfill this goal, sdn relies on control languages to programmatically express the desired network behavior, making it possible to quickly change and innovate within the network. as a consequence, having an expressive and powerful control language will unlock the full potential of this approach and enable new opportunities for developing network control applications. as such, numerous works addressed this issue, where the most recent ones have used network abstractions in order to spare administrators from dealing with the complex and dynamic nature of the physical infrastructure. however, we think that these languages rely on abstractions that are not the most appropriate ones for expressing modular and reusable control policies. in this paper, we present work in progress towards a new high-level, modular and flexible sdn control language. one novelty of this language is to integrate a network abstraction model that allows a clear separation between simple transport functions and richer network services. we believe that this approach will allow administrators to design and deploy control applications that can be easily maintained and reused."
"the mib can be thought of as a database of managed objects that the agent tracks. any sort of status or statistical information that can be accessed by the nms is defined in a mib. the smi provides a way to define managed objects, while the mib is the definition of the objects themselves. mib creates a collection of named objects, there types, and their relationships to each other in an entity to be managed. mib creates as set of objects defined for each entity similar to a database."
"the structure for the wireless mobile network can be configured depending on various applications. the optimal ids architecture for a mobile network will totally depend on the network infrastructure itself. in a flat infrastructure, all the nodes are to the same level of priorities and in multi-layered network infrastructure nodes may be separated into different clusters for communication."
"snmp mib variables are referenced by an oid, a sequence of digits and dots. this specifies the position of the variable in the mib tree. almost all the mib variables you see commercially will start with 1.3.6.1 (iso.org.dod.internet) and will then either take the proprietary limb of the tree (.4.1: private.enterprise) the standard limb (2.1: mgmt.mib). fig.2 gives the static implementation of the system. it shows how the various classes are related to each other using relationships like association and dependency. on larger scale system functions for this co-operative agent based wids can be divided into two main functions as intrusion detection and message passing to other nodes. intrusion detection includes local data collection and local detection of intrusion as sub functions. message passing contains secure communication channel and transfer of messages. fig.3 shows first main function of local intrusion detection. for this first data gets collected from mib forwarded to local mib agent and then to lids."
"as described previously, the first step consists in describing a virtual network that matches our high-level goals, thus abstracting all irrelevant information that are related to the physical infrastructure. the following extract is used to describe the virtual network showed in figure 4 (notice that not all links are represented in this extract):"
"all these limits make the controller's programing interfaces less productive and difficult to use in practice, since they do not allow to build modular control programs that can be easily maintained and reused."
"the most naive solution is to require each vehicle to broadcast the information of its two-hop neighbors in addition to its one-hop neighbors. the major drawback of this approach is that significant overheads will be introduced with longer packet length. therefore, to avoid such additional overheads, we use \"intermediate vehicles\" to detect the potential collisions between vehicles currently out of the two-hop range. since each vehicle is able to obtain the information of its two-hop neighbors from its one-hop neighbors, the intermediate vehicles are able to get knowledge of its two-hop neighbors ahead and two-hop neighbors behind. in this way, these intermediate vehicles can detect potential collisions between vehicles out of the two-hop range but within the three-or four-hop range who are reserving the same slot. this is the most essential observation for our proposed protocol."
"in the uk. in addition, pbl rapidly found its way into other disciplines, including education, law, business, and professional health sciences. 1, 2 in the years, over 60 medical schools adopted the pbl curriculum around the world. 1 today, the vast majority of medical schools in canada and approximately 80% of medical schools in the us have integrated pbl into their curricula. however, in half of the medical schools in the us, only a small portion (less than 10%) of their preclinical curriculum has been delivered in a pbl format. 3 [cit] s, the stakeholders at mcmaster university described pbl as a learning tool in which learners focused on a problem and used their previously gained knowledge in order to think rationally about solving the problem. in this manner, it was suggested that pbl would motivate students to participate in active intellectual processes at higher cognitive levels which ultimately would enhance student learning and knowledge retention. 4 during the implemented pbl, students were introduced to the learning strategies related to problem-solving, self-directed learning, and small-group tutorials, in which tutors played an important role in facilitating student group discussions."
"to satisfy these requirements, we put network virtualization at the very heart of our language. while there are many motivations to virtualize networks (e.g., isolation, customized network services), easing their management is probably the most important one [cit] . indeed, virtualization exposes logical abstractions (i.e, virtual networks) that are decoupled from the physical infrastructure. these abstractions provide just enough information to specify high-level goals, thus making control policies both easier to write, since only the desired behavior is expressed, and modular (subsequently reusable), since they are no more attached to a particular infrastructure. however, virtualization presents two major design challenges: the choice of the network abstraction model that will be used to abstract the physical infrastructure (i.e., the forwarding plane), and the technology needed (i.e., the network hypervisor) to map the logical state onto the underlying physical infrastructure [cit] . in this paper, we mainly address issues related to the first challenge."
"it is mentioned earlier that stand-alone ids does not work properly. so, co-operative and distributed intrusion detection system architecture should be implemented. for this there are ids agents running on each node. ids agent has complex design but by analyzing it properly and closely it can be viewed as having six different modules."
"intrinsic motivation has been identified as one of the driving factors that ignites student's interest in issues relevant to problems. when students are confronted with problems that they do not understand easily, they will actively seek information to solve the presented problems. 53 in line with the criterion for a learner-centered paradigm that states students are actively involved in their own learning, students must be motivated in order to be involved actively in their learning. attending presentations and watching a series of faculty notes, the trends in a teacher-centered paradigm, do not support student learning. this review suggests that there is an additional interface, ie, teamwork, which exists between pbl and learnercentered curricula. in teamwork, each team member brings a diverse set of knowledge, skills, experience, and expertise, not only to complement but also to support one another's strengths. 54 collaborative learning is a trend that is evident in a pbl curriculum in which students have a common goal to complete a clinical case assignment. [cit] emphasizes the role of teamwork in training medical residents. this standard states that \"residents must care for patients in an environment that maximizes effective communication. this must include the opportunity to work as a member of effective interprofessional teams that are appropriate to the delivery of care in the specialty\". 55 this statement stresses the importance of teamwork in the graduate medical education, which is consistent with the teamwork that pbl and learner-centered curricula accommodate. the teamwork objective, although it is not listed in the four objectives of barrow, corresponds to one of the learner-centered criteria ( table 2 ) that identifies the culture of learning as cooperative, collaborative, and supportive ( figure 1) ."
"we also consider the third case in which neither a nor b has passed the intersection. the distance between these two vehicles will be shortened before one of them passes the intersection. assuming a passes the intersection first, if a turns to the opposite direction of b's current driving direction of the same road segment, then the problem becomes road segment prediction. if a turns to other directions, then the problem becomes similar to the second case. thus, for this case, we only need to check whether the potential collision is active before a or b passes the intersection. we summarize all the cases in table i ."
"attack is an assault on system security that derives from an intelligent threat. it can be mainly classified as active attacks and passive attacks. active attacks are in the nature of eavesdropping on, or monitoring of, transmissions while passive attacks involves some modification of the data stream or creation of false stream. [cit] intrusion detection is the act of identifying intruders who attempt to compromise the integrity, confidentiality or availability of resource. it is used to secure the systems in the networks. [cit] there is a common misunderstanding that firewalls do the same thing of detecting and blocking of attack by shutting off everything and then turning back on only some wellchosen items. [cit] it just restricts access to the designated points. securing the computer networks with firewalls or using strong encryption algorithm keys are longer effective. this leads to the development of new architecture and mechanisms to protect wireless and mobile networks."
"first, some assumptions are made based on the basic tdma mac protocol that has been proposed in vanets. 1) every vehicle broadcasts a message at every frame, which includes its own location, speed, and moving direction. such vehicle information is required by most of the safety-related applications. this message also contains the fi about all the occupied slots by it and its one-hop neighbors. 2) every vehicle keeps the slot information about its onehop and two-hop neighbors, which are shared by its onehop neighbors. 3) each newly joining vehicle that has not obtained a slot and wants to get a slot needs to listen to the channel for one frame. then, they can randomly choose an available slot at the next frame for transmission. 4) each vehicle is equipped with a gps device that provides the information about its own location, moving direction, and speed. road information, such as road length, is also available. such information can be obtained from rsu broadcasting."
"1) the intermediate vehicle finds that one of the potentially colliding vehicles, which is running at the same direction, locates behind it. 2) meanwhile, the other potentially colliding vehicle, which is running at the opposite direction, locates ahead of it."
"for vehicles running along the same direction, they are likely to catch up with each other if the one behind has much faster speed. the distance between two vehicles may shorten to or be less than the two-hop communication range (2r) in a short time from now due to their speed difference. assuming vehicle a locates behind b and they are occupying the same slot, this potential collision is regarded as active if the distance between them can be reduced to 2r in a short duration of times, where r is the communication range of a vehicle. this is shown in the following:"
"we also evaluate the performances of the mac protocols under the four-way intersection scenario. similar to the twoway scenario, we measure their performances using different numbers of slots in a frame and traffic densities. four mac protocols are compared for the four-way traffic scenario. for the even-odd mac protocol, we regulate that vehicles moving to the east and north can use only even slots, whereas vehicles heading west and south can reserve only odd slots. moreover, we measure another mac protocol called the fourpart mac. in four-part mac, all the slots in each frame are evenly partitioned into four disjointed parts: one part for each direction. therefore, there will be no interference between vehicles running to the different directions."
"where v a and v b are the speeds of vehicles a and b, respectively. l b is the length of the road that b has not finished, and d is the current distance between a and b. t stands for a short duration time, which is used to check whether vehicles a and b can run into a two-hop range of each other within this short duration t . it is unnecessary for a potentially colliding vehicle to change its slot too early. if two potentially colliding vehicles will not encounter each other within time t, the potential collision can be removed later. k represents a short period of time that enables a potentially colliding vehicle to change its slot with high success probability. it is still possible that a potentially colliding vehicle can safely switch its slot to a new slot, but it gets into a potential collision with another threehop neighbor. if we set a larger k, the potentially colliding vehicle will have multiple chances to switch its slot and a higher probability of removing the collision can be achieved. on the other hand, if we set k to be too large, the original slot for the potentially colliding vehicle will be open for competition, and other vehicles may take over this slot. in this case, a new potential collision may appear right away, and the whole process needs to be done again. therefore, a smaller k can save resource and slot utilization. t equals to either k or the time before b leaves the road, depending on whichever is smaller. since t is a really short period of time (e.g., less than 1 s), we regarded v a and v b as constant within t, and their variability is less important. if a is faster than b and (1) is satisfied, this potential collision is considered active and has to be eliminated. otherwise, if v a is not larger than v b or (1) is not satisfied, this potential collision is currently harmless."
"in order to overcome the limitations of both models, we relied on a well-known idea within the network designer community, which is making an explicit distinction between the network edge and network core devices, as it is the case with mpls networks."
"the first type of components are hosts and networks which are used to represent sources and destinations of data flows. a host can represent a single end system (e.g., end host, application-level gateway or proprietary hardware appliance), while a network can represent a range of end systems. note that the use of these two components is not mandatory, but strongly encouraged as a way to make control policies easier to read and write. indeed, they allow administrators to manipulate identifiers that are meaningful to their high-level goals, instead of dealing with classical port numbers, network addresses and all sorts of other low-level parameters."
"the organizations invest in wireless networks as compare to traditional wired lans because of its low cost and relative ease of use. although the wired-ids are powerful systems, unfortunately they do little for the wireless world. the main difference between wired and wireless networks is their nature of transmission medium, different protocol specification in lower layer, different lower layer functionality of the intruders etc. [cit] the rapid proliferation of wireless networks and mobile computing applications has changed the landscape of network security. the nature of mobility creates new vulnerabilities due to open medium, dynamically changing network topology, co-operative algorithms, lack of centralized monitoring and management points that do not exist in a fixed wired network, and yet many of the proven security measures turn out to be inactive. this has led to the development of new architecture and mechanism to protect the wireless networks and mobile computing applications. [cit] the various approaches like ids using neural networks, artificial immune systems, manet based, clustering, system calls based, co-operative agent based etc. are developed and implemented so far."
"once network administrators have finished with the description of virtual devices, they will then just need to set-up the different virtual links in order to connect hosts or networks to edges, and edges to fabrics."
"using the pyretic language, that we have previously presented in section ii, to specify control policies upon an abstract topology is a challenging task, mainly because administrators need to use a complex transformation process which involves writing three auxiliary policies that make use of switches and links of the physical infrastructure."
"in order to avoid such difficulties, we have chosen to make a complete separation between control policies and the physical infrastructure. indeed, our programming approach is that network administrators provide two separate modules: the first one contains the principal control program, and the second one is a simple initialization module that gives information about the mapping between virtual units and switches present at the physical level. regarding the control program, it will be composed of two main parts: the first part deals with the design of the virtual network, and the second part contains control policies that will be applied over the virtual network, without any reference to the physical infrastructure. in the following, we describe in more detail each of these parts. figure 3 summarizes the key elements of the language."
"as its name suggests data is collected on each and every independent node. depending on this information, decision is taken for detecting intrusions. in this architecture, each node runs separate ids. no information or message is passed to each other. even though restricted by its limitations, it is more adaptable in situation when each node can run an ids on their own or have ids installed it is much more preferred for a flat network architecture which will unfortunately not suitable for wireless mobile network. [cit]"
"if an active potential collision is found, we need to prevent this collision from happening in the near future. one of the potentially colliding vehicles needs to give up its current reserved slot and switch to another available slot. since there may be multiple intermediate vehicles detecting the same potential collision, we need to select one of them to handle this potential collision. then, the selected intermediate vehicle has the responsibility to decide which one is the \"switching vehicle\" to release its current reserved time slot. finally, the switching vehicle needs to switch to another empty slot after receiving a switching notification from the responsible intermediate vehicle. recall that we focus only on the potential collisions detected between vehicles three hops away. a one-bit flag will be added into the broadcast fi of the responsible intermediate to indicate that a slot has an active potential collision. assuming slot i is currently occupied by the switching vehicle a, the responsible intermediate vehicle will broadcast its fi with an active flag on slot i. therefore, when vehicle a receives the fi from the intermediate vehicle, it finds its slot will conflict with another vehicle and can conclude that it has to change its slot. other intermediate vehicles found the same active potential collision could just broadcast their fis without adding an active flag on the same slot. after the switching vehicle changes to a new slot, it will update its fi and transmit using its new slot. vehicles that received such updated fi from a will update their fi. we can also allow a switching vehicle to use its original slot one more time to preannounce which slot it will switch to. this way, other vehicles that received such messages can avoid selecting the same slot. the contention collisions among multiple switching vehicles from different potential collisions and newly joining vehicles can be prevented."
"we first evaluate the performance of these three mac protocols under two-way traffic with balanced traffic densities. we focus on two metrics: packet delivery rate and total number of collisions. fig. 7 shows the results of the number of packet collisions and packet delivery rate with 200 vehicles generated for each direction. in fig. 7(a), every bar is separated into two parts by a black line. the part below the line stands for the number of encounter collisions, whereas the part above the line is the number of contention collisions. from the results, we can see that with the increment of the number of slots, all of the three mac protocols get better performance since more available slots decreases the collision probability. with 64 slots per frame, ptmac works better than adhoc mac and the even-odd mac protocol with 92.7% and 50% fewer collisions, respectively. this is because ptmac not only eliminates the collisions among vehicles from opposite directions but also avoids the collisions from the same direction. the delivery rate of ptmac also improves by 2.6% and 0.5% compared with that of adhoc mac and even-odd mac protocol, respectively. ptmac also has fewer contention collisions. the number of contention collisions is affected by the number of encounter collisions since the collided vehicles have to recompete for slots. therefore, reducing the number of encounter collisions is also helpful for decreasing the number of contention collisions. since the traffic density is pretty low in this case, the problem of packet collision is not severe. then, we increase the traffic density by generating 400 vehicles for each direction. the results about the number of collisions and packet delivery rate are shown in fig. 8 . with 64 slots per frame, ptmac has 90.6% and 29.7% fewer collisions than adhoc mac and even-odd mac protocol, respectively. the packet delivery rate of ptmac improves by about 5.8% and 0.4% compared with that of the adhoc mac and even-odd, respectively. we continue to increase the traffic density to 600 vehicles for each direction, and fig. 9 shows the results. the number of contention collisions of the even-odd protocol is sharply increased in this case. basically, the even-odd scheme has no great impact on the number of contention collisions among vehicles driving at different directions, since both the number of available slots and the number of competing vehicles have been halved. however, if a contention happens among vehicles in the same direction (among newly joining vehicles and recompeting vehicles), a higher probability of contention collision may occur since only half of the slots are available. higher traffic density means more newly joining vehicles are generated, and more encounter collisions happen among vehicles along the same direction. thus, more contention collisions are introduced for the even-odd scheme. on the other hand, ptmac has a little bit more encounter collisions than the even-odd mac protocol with 64 and 72 numbers of slots in a frame. this is because the total number of available slots is not enough. even if a potential collision has been identified, there is no other available slot to change to. in addition, it is likely that a switching vehicle switches to another slot but collides with another vehicle. however, even with 64 slots per frame under this dense traffic, the proposed ptmac still has better overall performance with 9.2% and 3.4% higher delivery rate than adhoc mac and even-odd protocol, respectively. when we increase the number of slots in a frame to 80 and 88 (i.e., enough number of slots is provided for vehicles to switch to when eliminating the potential collisions), ptmac has fewer encounter and contention collisions compared with the even-odd scheme."
the smi provides a way to define managed objects and their behavior. an agent has in its possession a list of the objects that it tracks. one such object is the operational status of a router interface. this list collectively defines information that nms can use to determine the overall health of the device on which the agent resides.
"we take an example of a four-way intersection scenario as shown in fig. 6 to explain our ptmac protocol. vehicles a and b are occupying the same slot i and are currently three hops away from each other. after the intermediate vehicles x and y detected this potential collision, they need to predict whether this collision is active or not. we consider the potential collision prediction in three possible cases based on the current locations of two potentially colliding vehicles a and b. the first case is that both a and b have passed the intersection. the second case is that one of them has passed the intersection, whereas the other one has not. the third case is that none of them has passed the intersection."
"to ensure driving safety, the 3-s rule is generally used, which suggests that a vehicle should stay 3 s behind the vehicle in front of it. for a highway scenario, we consider two-way traffic, table ii . to focus on the packet collisions, the simulation runs using an ideal physical channel, i.e., the packet will be successfully transmitted within the communication range, if there is no packet collision."
"the medline database was searched using different combinations of keywords (see table 1 ) [cit] . because the medline database provided only journals relevant to healthcare, it was important to include other educational journals. as a result, the following key words: \"problem based learning\", \"medical education\", \"traditional curriculum\", and \"self-directed learning\" [cit] which produced one article. papers were included if they met the criteria of pbl being implemented in a medical school and provided comparative data between pbl and traditional curricula."
"to obtain the vehicle information of the potentially colliding vehicle in two-hop distance, the intermediate vehicle needs to add a request to its broadcast message. other intermediate vehicles that require the same information do not need to send duplicate requests. the vehicle (must also be an intermediate vehicle) that hears such request and is a one-hop neighbor of the requested, potentially colliding vehicle and will add the requested vehicle information into its broadcast message. similarly, other vehicles, which receive the same request and find the required information that has already been broadcast, can ignore this request. once the requested vehicle information is received, the intermediate vehicle will begin the potential collision prediction. for example, in a potential collision that is predicted to happen is considered \"active.\" we classify the potential encounter collisions into two types: potential collisions among vehicles running at the same direction and among vehicles driving at the opposite directions. different methods will be used for these two types of potential collisions to predict if they are active or not."
"the fast-paced stream of medical information, a high expectation from the medical accreditation and national board examination agencies, an intensive trend of basic science learning followed by demanding clinical training, and diverse student learning have encouraged many faculty and stakeholders at medical schools to innovate new curricular designs to promote student learning. there are virtually three curricula that are implemented by most medical schools in the us, ie, lecture-based, problem-based, and a combination of both. while the lecture-based curriculum is one of the most widely used instructional techniques, its effectiveness has been criticized and questioned for many years. 41 on the other hand, pbl has been cited by mounting literature as an effective curriculum that actively engages students in their learning. 2, 9, 34 however, a few data have brought into question the effectiveness of pbl in improving student overall learning and knowledge and the improvement of patient's health, physicians' knowledge and performance, [cit] or have suggested that a pbl curriculum does not teach problem solving better than a traditional curriculum. 45 despite the fact that the pbl curriculum has been endorsed by the association of american medical colleges 46 and the world federation of medical education, 47 surprisingly, there are few published data providing conclusive evidence that a pbl curriculum fosters more qualified physicians than any other curricula."
"therefore, we can see that, when using even-odd mac, more contention collisions are introduced in the first case, whereas smaller contention collision probability is achieved in the second case. however, since more contentions are happening between newly joining vehicles and they are heading the same direction, the first case happens more frequently. the second case is more suitable for the collisions that happen between new joining vehicles and recompeting vehicles or among recompeting vehicles."
"ids agent can be viewed as mobile host shown in the above diagram. this includes lids, snmp agent, mobile agent and mib. lids, mobile agent and mib agent come under local detection engine, co-operative detection engine and local data collection module respectively."
"after explaining our ptmac under two-way traffic scenario, we extend it to a four-way intersection scenario. vehicles can drive at four possible directions: north, south, west, and east. we consider a traffic light model in which the intersection has a traffic light for controlling the traffic from all four directions. vehicles can go straight, turn right, turn left, or make a u-turn at a four-way intersection. more assumptions in addition to what we mentioned in section iii are made for four-way traffic."
"the novelty introduced by our proposal language is that unlike existing works, we rely on a new network abstraction model that we think is more appropriate for our language design requirements. indeed, since we consider virtualization as a cornerstone component, we must be mindful of the fact that the choice of the abstraction model will significantly impact the language's fundamental properties, namely: it's expressiveness, modularity and flexibility."
"since ptmac does not use the slot partition method, the way of computing the contention collision probability for ptmac is similar to adhoc mac. for even-odd mac protocol, the total number of available slots is halved for each direction. assuming the traffic densities are completely balanced for both directions, there will be n e /2 empty slots left for each direction. the contention collision can be analyzed in two cases. in the first case, if there are m number of competing vehicles from the same direction and n e /2 is greater than 1, the probability that one of them can reserve a slot successfully is computed as"
"pyretic abstract topologies may contain a mix of physical switches, and virtual ones that are overlayed over the physical infrastructure. we believe there is a better alternative that will best suit our language design requirements. in the next section, we discuss in detail abstraction models that are proposed in the literature, then we present our new approach."
"an ids is a software or hardware tool that monitors traffic on network looking for and logging threats. the purpose of ids is to monitor the computer networks, detect intrusions and alert the concern person. network based (nids) and host based (hids) are types of ids. in nids traffic flowing through network is analyzed. in hids activities on each individual computer are examined. [cit] there are two ways on which basis we can implement ids. the first one is signature based in which the attacks have unique signature that can be detected. known attacks can be detected by looking for these signatures. second approach is anomaly based in which a system develops a base line what it considers a normal traffic. any activity which is recorded beyond this traffic is considered as anomaly and alert is generated."
"clinical reasoning plays an important role in developing qualified physicians because it reinforces a cognitive process by which a clinical case is reviewed, analyzed, and explored to diagnose or suggest a therapeutic solution to a disease. 49 in other words, clinical reasoning skills develop when a student uses his/her knowledge effectively to review and address clinical issues in real-life contexts. barrows, in his study, indicated that a pbl curriculum promotes clinical reasoning skills. 9 in a teacher-centered paradigm, students often face \"black and white\" assignments that are also called \"well-defined\". a pattern of solving these problems does not prepare students for real-life problems that often are \"gray\" or \"ill-defined\". 7 on the other hand, in a learnercentered environment, students are challenged to deal with \"real-life\" problems to promote their knowledge and skills in areas of inquiry, reasoning, and problem solving 7 which ultimately will assist them in developing their clinical reasoning skill. promoting self-directed learning skills is a challenging process for faculty members and students. in a learner-centered paradigm, the role of the faculty is to coach and facilitate, 7 which ultimately encourages students to be self-guided and know how to use their knowledge and resources to complete a clinical case assignment. 8, 22 there is increasing evidence that, in a pbl curriculum, compared with a traditional curriculum, students are more self-directed and enthusiastic learners, which ultimately promotes graduates to be life-long learners. 9, 50, 51 in an interesting study, medical students' engagements in borrowing study material from libraries was compared between pbl students and traditional students. the results revealed that pbl students borrowed more material (67 books/student/year) than the traditional students (43 books/student/year). in addition, this difference was amplified during the clerkship, ie, 40 books/student/ year compared with 11 books/student/year. 52 the latter result indicated that students were enthusiastic learners, with a desire to explore and acquire more information in a self-directed manner."
"regarding the first restriction, it is well known that administrators often need to install multiple functions on their networks such as routing, monitoring, access control or load balancing. using programming interfaces of current controllers to implement these functions as independent and separate modules, that can later be composed to achieve high-level goals, is a complicated and error-prone process. indeed, to compose existing control modules, it is necessary to manually combine their logic in a totally new program in order to avoid overlapping problems between rules of different modules that apply on the same packet flows."
"on the other hand, the overlay network is a more modular approach, since the model allows network administrators to define multiple logical switches, on which they can install innetwork functions. these switches can be afterwards reused to, easily and quickly, construct sophisticated network control applications. however, we think that this model suffers from one major shortcoming, that is, unlike the platform as a service model, there is no distinction between in-network functions and packet transport functions, despite the fact that these two auxiliary policies solve two different problems. indeed, this shortcoming makes the definition of in-network functions more difficult, since their specification must consider issues related to packet transport across the virtual network (e.g., selecting the appropriate virtual path among several available)."
"here, we evaluate the performance of our proposed ptmac protocol. we use matlab and simulation of urban mobility (sumo) to construct a simulation environment in which both two-way and four-way traffic scenarios are considered. sumo is a traffic simulator to generate real-world mobility models, including road map, traffic light information, and vehicle's moving pattern. vehicles' speeds are adjusted based on the traffic condition and traffic light information when they are approaching an intersection. a mobility trace file that contains the position of each vehicle at any time is generated by sumo and input to matlab. matlab is used for building a vanet communication environment and for implementing the mac protocols. we compare our ptmac with adhoc mac [cit] and even-odd tdma protocol [cit] ."
"in another case, if there are a total of m competing vehicles within a two-hop communication range, half of them are running to the left, whereas half of them are driving to the right; the probability that one of them can reserve a slot successfully is computed as follows:"
"here, n e (a) is the number of empty slots, and a(e) expresses the empty slots from a's view. a(3) stands for the threehop neighbors of a, and they will encounter a within t (short duration). we call those neighbors as three-hop encounter neighbors. therefore, n (a(e) ∩ a(3)) expresses the number of slots that are empty from a's point of view and meanwhile are occupied by a's three-hop encounter neighbors. as long as the potential collision has not really happened, vehicle a still has chance to switch to elsewhere. if a has n sw number of chances to change its slot, the probability that vehicle a can eventually remove the potential collision is computed as"
"this section presents a simple use case in which we illustrate a preliminary version of our high-level network control language. the overall management goal of this use case is to configure an enterprise network in order to prevent external access to sensitive resources. the policy is that any user who is part of the enterprise's internal network can have access to all available resources (i.e., web server and computer cluster). on the contrary, external users can only have access to web resources and are not allowed to access the enterprise's cluster."
"based on the ieee 802.11p mac protocol, if the channel is sensed as idle, a vehicle starts the transmission directly. otherwise, it needs to randomly pick up a backoff value from the contention window (cw) and start a countdown procedure. transmission will begin when the backoff value reaches 0. if multiple vehicles within a two-hop communication range (two times communication range) try to access the channel simultaneously, a collision will happen and none of the packets can be received successfully. in this case, vehicles have to recompete for the channel to resend the packets. an exponential backoff scheme that extends the cw size for decreasing the possibility of contention collision is applied for unicast retransmission. however, as a contention-based scheme, csma/ca has the drawback of potentially unbounded channel access delay [cit] . if a vehicle has multiple packets, it has to contend for multiple times. furthermore, 802.11p is vulnerable to the hiddenterminal problem since it cannot use the request-to-send/ clear-to-send mechanism for packet broadcasting [cit] . in this case, the packet collision cannot even be detected right away. no exponential backoff scheme can be used for broadcasting, and the probability of packet collision is potentially high [cit] ."
"the first simulation scenario is a highway with two-way traffic. vehicles are running at different speeds within different maximum speeds. a vehicle can catch up with and pass over other vehicles if its speed is faster. we measure the performances using different traffic densities. in total, 200, 400, and 600 vehicles are generated for each direction during the simulation time of 600 s. we also investigate the impact of unbalanced traffic densities for different directions on these three mac protocols. the second simulation scenario is an intersection with four-way traffic. there are three lanes for each direction. the right lane is for vehicles turning right, the middle lane is for vehicles going straight, and the left lane is used for vehicles turning left or making a u-turn. the number of vehicles that has been generated for each direction is varied from 150 to 200 vehicles within a simulation time of 600 s. for both scenarios, the packet size is assumed as 400 b and the data rate is 6 mb/s."
"security is important in any environment. as large information is available on the network and it is possible to share this data through it, it should be secure. it is somewhat defined in wired network but in wireless there is great challenge of different attacks. people and organizations have been protecting their data from harmful activities using rules that identify and block such things. however current and future threats require development of more adaptive defensive tool."
"having described the virtual network, the next step is the specification of the control policy. the subsequent piece of code represents the in-network function that will be installed on the ingress edge. this function configures an edge so that it classifies incoming internal flows as \"trusted\" and the external ones as \"unreliable\". once the classification has been done, the edge will simply forward flows to the fabric in order to be transported to their right destination. in the context of this example, the gateway is only designed to analyze unreliable flows. we therefore use the following transport function to configure the fabric so that unreliable flows are transported to the filtering gateway, while trusted ones are directly transported to the egress edge. for the gateway's configuration, we define the below innetwork function that performs two actions. the first one is to discard all flows that want to reach the enterprise's cluster, since only unreliable flows are redirected to the gateway. the second one is to reclassify all web flows as \"trusted\" flows, since they are allowed to access the enterprise's web server."
"till now little research has been done in area of wireless ids. because of its structural and behavioral differences, ids designed for wired networks is not that applicable to the wireless network. [cit] in case of wireless networks communication is done through an open air environment and the medium is not well protected .so it is impossible to monitor network traffic at bottlenecks. it is necessary to do monitoring at each and every network node. but it is inefficient due to high network bandwidth requirement and increased power resources that are not easily available. [cit] ad hoc wireless networks are very dynamic in structure, giving rise to apparently random communication patterns, thus making it challenging to build a reliable behavioral model. misuse detection requires maintenance of an extensive database of attack signatures, which in the case of ad hoc network would have to be replicated among all the hosts. [cit] this will result in an extended initial setup time and decrease in useful computational power of each host."
"we have made an important observation that most of the encounter collisions can be predicted and potentially avoided based on vehicles' moving patterns and traffic condition. therefore, instead of using the slot partition method, we propose a novel mac protocol that takes advantage of prediction to remove potential collisions. our ptmac protocol is described under a two-way traffic scenario here, and it will be extended to four-way intersections later. for both two-way and four-way scenarios, there are three steps that need to be processed in the ptmac protocol: potential collision detection, potential collision prediction, and potential collision elimination. a potential collision needs to be detected first based on the slot information. then, we can predict whether this potential collision will really happen in the future based on the real-time traffic condition and vehicles information. finally, we reschedule the slots to eliminate this potential collision. detailed descriptions of these three steps will be provided later. notice that the collisions we mentioned here mean encounter collisions, so as the following collisions mentioned in this paper, unless we point out that it is a contention collision. recall that in a tdma-based protocol, each vehicle will first contend for an empty slot in a frame. it will continuously use this slot if it successfully transmitted the first time. a contention collision happens if multiple vehicles within a two-hop communication range contend for the same slot. an encounter collision is caused by two vehicles approaching each other while using the same slot in a frame."
"in addition, we study the influence of unbalanced traffic densities on these three mac protocols. here, we define a new parameter called a traffic balance rate (tbr). it is computed as the ratio of the number of vehicles in the direction with sparser traffic to the number of vehicles in the direction with denser traffic. therefore, tbr equals to 1 when the exact number of vehicles is generated for each direction during the simulation. a small tbr means a scenario with severely unbalanced traffic densities. we fix the total number of vehicles that are generated through the simulation as 800 and measure the packet delivery rates by using different tbrs. fig. 10(a)-(d) represent the packet delivery rates with 64, 72, 80, and 88 number of slots in a frame, respectively. the performances of adhoc mac and ptmac are not greatly affected and degraded by the unbalanced traffic densities since these two protocols do not use a slot partition method. on the other hand, the even-odd protocol shows its sensitivity to a small tbr with a low packet delivery rate, particularly for smaller numbers of slots in a frame. with 64 slots in a frame, the performance of the even-odd protocol is worse than the adhoc mac when tbr is set as 1/7 or 1/3. thus, vehicles in the direction with heavier density will suffer a high probability of contention collision, even if there are many empty slots left in another direction. with 64 slots and 1/7 tbr, ptmac has higher delivery rate of 6.4% and 12.5% compared with adhoc mac and even-odd protocol, respectively. in addition, for even-odd, if a vehicle finds that all the slots assigned for its direction have been occupied, it will not have a chance to access the channel even if there are still empty slots left for the other direction. it cannot begin the slot contention until someone in its direction release the slots. in this case, the slots of the sparse traffic density side will be wasted, and contentions will be considered failed. these failed contentions are unnecessary and can be fully prevented if the number of available slots can be well adapted. both ptmac and adhoc mac do not suffer such unnecessary failures since vehicles freely select any available slots for channel contention."
"contrary to filters, actions are primitives that can change packets value or location. they are applied on sets of packets that are returned by installed filters. the simplest action is drop which discards a packet received on one of the edges input port. the forward action allows to move, within the same edge, a packet from an input port to an output port. the modify action is used to update one or more of the packet's header fields. lastly, the tag action allows to attach a label onto incoming packets, considering that labels are the unique information that a fabric will use to identify a packet."
"notice that, if the traffic density is very low, an intermediate vehicle may not exist between the two vehicles with a potential collision. for example, in fig. 3 vehicle, the potential collision cannot be detected. in this case, the ptmac protocol performs similar to adhoc mac. vehicles that get encounter collision will recontend for an available slot to transmit the packet. meanwhile, the packet collision will not become a problem in such sparse traffic condition."
"the second restriction is that network administrators are obliged to specify their control policies directly upon the global view of the physical infrastructure that is provided by the controller. the drawbacks with this approach is that administrators need, on the one hand, to deal with a large amount of informations that are irrelevant to their high-level goals (e.g. even in the case of specifying an access control policy, administrators must also consider issues related to packet forwarding between intermediate nodes) and, on the other hand, they have to constantly adapt their policies to changes that may occur in the physical infrastructure (e.g. discovery of a new path, link or device failure)."
"we start from the first step of our ptmac protocol: how to detect a potential encounter collision. typically, two vehicles within their communication range (i.e., one-hop distance) using the same slot will cause a transmission collision. however, in a broadcast environment, a collision will happen if these two vehicles are within two times of their transmission range (i.e., two-hop distance) since a vehicle in between these two will not receive either broadcasting sent by these two vehicles. to detect a potential collision before it actually happens, we intend to identify any two vehicles using the same slot that are out of the two-hop communication range from each other. that is, a vehicle needs to know the slot usage information of other vehicles that are beyond the two-hop distance."
rection at the coming intersection before passing the intersection. this information can come from the turning left/right signal or from the gps device base on a predetermined route. 2) the location of an intersection and the phases of its traffic lights are provided by rsu broadcasting.
"finally, we believe that decomposing control policies into transport and in-network functions will enable network administrators to write control programs which are much easier to understand, reason about and maintain."
"advances in medical education and practice is an international, peerreviewed, open access journal that aims to present and publish research on medical education covering medical, dental, nursing and allied healthcare professional education. the journal covers undergraduate education, postgraduate training and continuing medical education including emerging trends and innovative models linking education, research, and healthcare services. the manuscript management system is completely online and includes a very quick and fair peer-review system. visit http://www.dovepress.com/testimonials.php to read real quotes from published authors."
"in this paper, we present work in progress towards the definition of a new high-level control language for sdn platforms. the goal is to design a control interface that overcomes the previously presented deficiencies. in order to do so, we designed our language so that it satisfies the following key principals:"
"it is worth mentioning that this review has a number of limitations. [cit] on wards). secondly, whilst the literature search yielded compelling data that pbl supports student learning, the impact of pbl on improving physician interaction and communication with patients and other healthcare providers has not been researched to any significant extent. lastly, the inconsistency in the development and implementation of different pbl formats among medical schools made this review a challenging task to provide a pattern of consistent measures of the effectiveness of pbl."
"it is important to stress that none of the previous innetwork functions consider packet transport issues. indeed, all focus only on their high-level goal (i.e., classifying in ingress, filtering in gateway and delivering in egress), and at the end, functions just send data flows to the fabric which ensures the transportation to the right destination."
"in this type every single node plays an important and critical role. each node contributes individually or on entire network for the process of detection. it scans for any sign of intruder. using six different modules such as local data collection, local intrusion detection, cooperative message passing, secure communication etc. the above explained approach can be achieved. the ids also triggers response if intrusion is detected. the isolated ids agents are entirely linked together to form the ids system defending the mobile wireless network. [cit]"
"the ptmac protocol still processes with the three steps for a four-way intersection scenario. the steps of potential collision detection and potential collision elimination are similar to what we described in the two-way traffic scenario. we will focus on explaining the most different part: potential collision prediction under a four-way scenario. similar to the two-way scenario, the intermediate will first request for the information about the potentially colliding vehicle that is two hops away and then begin the prediction. we further separate the potential collision prediction into two parts: road segment prediction and intersection prediction. the road segment prediction concerns the collisions between vehicles running on the same road segment, either heading in the same direction or the opposite directions. the intersection prediction pays attention to the potential collisions among vehicles driving on different road segments while approaching to or leaving the intersection. since the road segment prediction is the same as what we have explained in a two-way traffic scenario, we concentrate on describing the intersection prediction that is used to check whether two vehicles are currently out of the two-hop range and reserving the same slot will encounter each other."
"the various values that can be retrieved from a mib are called mib variables. these variables are defined in the mib for a device. each mib variable is named by an object identifier (oid), which usually has a name in the form of numbers separated by periods (\".\"), like this: 1.3.6.1.xxxx.x.x.x.x... e.g. the mib-ii has a variable that indicates the number of interfaces (ports) in a router. it's called the \"ifnumber\", and its oid is 1.3.6.1.2.1.2.1.0. network monitoring tools will query a device for the mib variables and display the results. when a device receives a snmp get-request for this ifnumber oid, it will respond with the count of interfaces."
"in our proposed ptmac protocol, it is likely that a detected potential collision cannot be successfully removed under heavy traffic density. one possible situation is that there is no other empty slot for the switching vehicle to switch to. another possible situation is that the switching vehicle switches its slot to a new slot, but this new slot incurs a new potential collision with another vehicle. therefore, under increased traffic density, this vehicle may have to keep changing its slot until it finds a slot without potential conflict with others or the collision really happens. assuming that vehicle a is detected having a potential collision with vehicle b, the probability that this potential collision can be removed at the next frame is expressed as"
"currently, we are working on the design and the technical development of a network hypervisor that will support the control language we presented. in addition to the main control module, which contains virtual network and control policies declaration, the network hypervisor will rely on a mapping module consisting of initialization information and mapping instructions linking virtual network components to real physical elements. the definition of this module will largely depend on one side, on the physical infrastructure (network topology, host deployment, etc.) and on the other, on the administrator's virtual network design choices."
"the third and last type of components are fabrics which represent the network's raw forwarding capacities. the fabric's primary purpose is packet transport. it exposes only a minimal set of forwarding primitives and uses a specific addressing mechanism that is much simpler than the one used by edges (i.e., using a unique label instead of several header fields). in normal cases, all edges in the virtual network will be connected to a unique fabric. however, in some specific cases, virtual networks can include more than one fabric according to the network administrator's high level goals. indeed, it is important to note that two fabrics within the same model will map to two separate collections of physical switches. this design choice allows us to capture specific network policies such as expressing an explicit physical backup path for critical data flows."
"teamwork is a trend that is clearly evident in the pbl curriculum in which student groups develop a common goal to complete a clinical case assignment. it has been suggested that a pbl curricular implementation can accommodate the needs for developing teamwork skills, 37 and pbl medical graduates, compared with traditional medical graduates, have learned better communication and teamwork skills. 38 because the success of each individual is tied to the success of the team, students are motivated to help each other, which in turn promotes cooperative learning. 39 in addition, students who work on teamwork assignments achieve a high understanding of complex and difficult problems that is often challenging to achieve individually. 40 one apparent pbl objective, although not listed in the four objectives of barrow, is teamwork. teamwork is also one of the learner-centered criteria (table 2) 7 which states that the culture of learning is cooperative, collaborative, and supportive (figure 1 ) or, in other words, the culture is noncompetitive. in both pbl and a learner-centered paradigm, team members feel their contributions are appreciated and valued, that they make collective decisions, and focus on common goals. as michaelsen and sweet point out, teamwork provides an environment in which team members progress well, achieve a depth of understanding, identify their strengths and weaknesses, and the group develops into a self-managed learning team."
"additional recent proposals introduced modern features that allow to build more realistic and sophisticated control programs. indeed, languages such as procera [cit] and netcore [cit] offer the possibility to query traffic history, as well as the controller's state, thereby enabling network administrators to construct dynamic policies that can automatically react to conditions like authentication or bandwidth use."
"an agent has a list of objects that it is tracking. this list contains all the information that network management system (nms) can use to determine health of the device on which this agent resides. these objects that agent tracks are managed in mib defined above. the objects in mib are categorized under 10 different groups as system, interface, address, translation, ip, icmp, tcp, udp, egp, transmission and snmp.the information from mib variables can be read by using languages like java."
initial requirement will be the installation of snmp on all the nodes present in ad-hoc network. snmp agent will extract the information from mib variables. mib includes network related information. thus approach is network based ids. this information will be analyzed by lids agent using either misuse based detection module or anomaly based detection module.
"the second type of components are edges which are general processing devices placed at the border of the virtual network in order to support in-network functions installation. edges can either play the role of host-network interfaces or the role of middleboxes. indeed, following the approach we propose, ingress edges will receive incoming data flows, inspect packet's headers to identify which in-network function is to be considered, and redirect flows either to an egress edge for delivery to the destination or to an intermediate edge for potential further treatment. in addition, it is important to stress that edges are purely logical entities that can map to one or more switches in the physical infrastructure."
"fabrics expose two main primitives that are catch and carry. the first primitive captures an incoming flow on one of the fabric's ports. data flows are identified based on a label that has been inserted beforehand by an edge. the second instruction carry transports a flow from an input port to an output port, it also allows to specify some forwarding requirements such as maximum delay to guarantee or minimum bandwidth to offer."
"based on the report by the u.s. department of transportation (usdot) [cit], vehicle information, such as speed, position, and moving direction, is required and should be broadcast by every vehicle periodically to support the safety-related applications in vanets. therefore, we make an important observation that most of the encounter collisions can be predicted and potentially avoided based on such vehicle information. we design a new prediction-based tdma mac protocol (ptmac) to reduce the possibility of encounter collisions. to the best of our knowledge, ptmac is the first protocol that is designed for both two-way traffic and four-way intersections. our main contributions in this paper can be summarized as follows."
"as a reminder, there are two types of collisions: contention collision and encounter collision. we first investigate the contention collisions probability of the three mac protocols. n denotes the total number of slots in each frame. n e and n r stand for the number of empty slots and reserved slots, respectively. m is defined as the number of new joining vehicles within two-hop communication range. they currently do not occupy slots but are trying to compete for slots. here, we consider only the case that m is larger than one. otherwise, no contention collision will happen. for the basic adhoc mac protocol, if n e is greater than 1, the probability that a vehicle among these m competitors can reserve a slot successfully is computed as"
"finally, we drew inspiration from pyretic work in order to provide our language with composition operators that enable network administrators to easily combine, in a parallel (+) or a sequential ( ) way, edge and fabric policies."
"technically speaking, these mapping instructions will mainly consist in associative arrays binding each virtual unit (i.e., edge, fabric, host or network) as well as their parameters to their respective physical counterparts of the underlying infrastructure. associating network addresses to hosts and virtual networks, or mapping an edge's ports to physical ones (knowing that these physical ports may belong to different physical switches) are examples of such mapping instructions. these mapping rules will be reused afterwards by the network hypervisor's runtime in order to generate a policy for the physical infrastructure that is semantically equivalent to the one applied over the virtual network. it will then be the hypervisor's responsibility to enforce the policy on the underlying network."
uml sequence diagram in fig.4 shows further main function of message passing to other neighboring nodes in the network. responsibility of this transfer of message is given to ma. lids forwards message to ma which in turn passes it to other nodes.
"we are presently implementing our high-level network control language as a domain-specific language embedded in python. to map the logical state of the virtual network onto the physical infrastructure, the prototype relies on the pox controller, an open source development platform for python-based sdn control applications. in the current state of work, the initialization module will be manually specified by network administrators, but our long term goal is to be able to automatically generate part of this module, by relaying in particular on topology information returned by the controller."
"figs. 11-13 represent the results under different traffic densities. from the simulation results, we can see that ptmac works the best with the least number of collisions and highest delivery rate. since adhoc mac allows a vehicle to contend for any empty slot without considering the vehicles' mobility nature, it is suitable for only one-way traffic, and its performance is severely affected by the huge number of encounter collisions under such a four-way intersection scenario. meanwhile, both even-odd and four-part macs do not have obvious improvements for this four-way intersection scenario. they even perform worse when traffic density becomes heavier. more encounter collisions happen among vehicles at the same direction in this four-way intersection since a vehicle ahead may need to stop and wait for the red signal, so it is easy to be caught up by other vehicles behind. such collisions cannot be handled by even-odd and four-part macs. moreover, even-odd mac cannot avoid the contention collisions that happen near the intersection between vehicles using the same set of slots (such as vehicles heading north and east that both use the even slots). for the four-part mac, although no contention collision will happen between vehicles that are originally driving at different directions, vehicles may change their directions at the intersection. furthermore, the quartered number of available slots not only increases the probability of contention collisions but also causes more encounter collisions between vehicles running in the same direction."
"ad-hoc networks, vanets have the unique characteristics of high node mobility, dynamic topology changes, and strict delay constraints. these issues must be considered in developing medium access control (mac) protocols for vanets to support both safety-and nonsafety-related applications."
"if both conditions are satisfied, the intermediate vehicle knows that the two potentially colliding vehicles are approaching each other. otherwise, this potential collision can be ignored. assuming the dsrc communication range is 300 m and vehicle speeds are 30 m/s (67 mi/h) on a highway, for a three-hop potential collision, the time to shorten the distance between two potentially colliding vehicles to a two-hop range is about 5 s. therefore, there is plenty of time for a potentially colliding vehicle to change its slot since every vehicle needs to broadcast its information at least every 100 ms. similar to the same direction collision, only if the distance between a and b can be reduced to 2r in a short duration of time t, the potential collision is active. this is shown in"
"in a learner-centered environment, students establish their reasoning strategies by communicating their knowledge to address emerging issues in real-life contexts. 7 quellmalz and hoskyn have established many different reasoning strategies that can be applied in a learner-centered paradigm. a few of these reasoning strategies include comparing, error analysis, constructing support, analyzing perspectives, decision-making, investigation, experimental inquiry, problem-solving, and invention. 21 a closer look at these reasoning strategies and the pbl curriculum indicates that the pbl curriculum accommodates many of these reasoning strategies during student group discussions to encourage students to use their clinical reasoning more effectively. 9, 17, 19 self-directed learning skills self-directed learning skills demonstrate that students are self-guided and know how to use their knowledge and resources to complete problems or assignments."
the last in-network function simply configures the egress edge in a manner that it forwards web requests to the web server and forwards computation requests to the computer cluster.
"in the first case, both a and b have already passed the intersection and are driving away from the intersection. if a and b have turned to different directions, they are running farther away from each other, and no collision will happen. if a and b have turned to the same direction, the prediction problem becomes the road segment prediction. the second possible case is that a has already passed the intersection, but b has not. b will turn to the same or different direction as a's. if b did not encounter a before it passes the intersection and turns to the same direction as a, the problem will become road segment prediction again. on the other hand, if b did not encounter a before it passes the intersection and turns to a different direction as a, their distance will become larger, and they have no more chances to encounter each other. thus, we only need to check whether the potential collision is active before b passes the intersection."
"when an intermediate vehicle detects a potential collision between two other vehicles, e.g., vehicles a and b, it needs to predict whether a and b will \"encounter\" each other, and the potential collision will really happen. in this paper, the \"encounter\" means that two vehicles come into two-hop communication range of each other. the predictions that can be done based on the vehicle information include the locations, speeds, and moving directions of these two potentially colliding vehicles (in term of transmission). since every vehicle periodically broadcasts its vehicle information to meet the requirement of safety-related applications, the intermediate vehicle has the vehicles information of one of the potentially colliding vehicles, which is its one-hop neighbor. however, it has no knowledge about the other potentially colliding vehicle, which is its twohop neighbor."
"in this paper, we propose the ptmac protocol to decrease the number of packet collisions, particularly for encounter collisions. potential collisions among vehicles that are currently out of the two-hop communication range can be detected by intermediate vehicles, predicted, and then eliminated before they really occur. our simulations show the effectiveness of the proposed protocol. since no slot partition is used, unbalanced traffic densities will not degrade the performance of ptmac. unlike a few existing mac protocols that work only for oneway or two-way traffic scenarios, ptmac is also suitable for handling four-way traffic."
"this paper described the design of a new high-level language for \"programming\" software-defined networks. we used network virtualization as a main feature in order to spare administrators the trouble of dealing with the myriad of irrelevant information that are related to the physical infrastructure, thus complying with the sdn promise to make network programming easier. the novelty of this language lies in the use of a new abstract model that explicitly identifies two kinds of virtual units: i) fabrics to abstract packet transport functions and ii) edges to support, on top of host-network interfaces, richer in-network functions (firewall, load balancing, caching, etc.). we think that this model offers the proper level of abstraction, by providing just enough information, to clearly specify the network's desired behavior according to the traffic's type. moreover, this network abstraction model covers our language design requirements, namely its expressiveness, modularity and flexibility."
"in addition to the interfaces between a pbl paradigm and a learner-centered paradigm identified above, huba and freed have developed eight hallmarks for a learner-centered paradigm. 56 many of these hallmarks are also supported by the pbl curriculum, ie, learners are actively involved and receive feedback, apply knowledge to enduring and emerging issues and problems, and integrate discipline-based knowledge and general skills."
"using two types of virtual devices, namely edge and fabric, implies having two distinct instruction sets. indeed, this will allow the two components to evolve separately, focusing on their specific problems."
"notice here that vehicles a and b have different neighbors and slot allocations. n e (a) and n e (b) stand for the numbers of empty slots from the view of a and b, respectively. n e (a ∩ b) expresses the number of empty slots from both a and b's views. notice here that, for even-odd mac protocol, no encounter collision happens between vehicles running at opposite directions. however, it cannot avoid the encounter collisions from the same direction. since the number of available slots is halved in even-odd protocol, the probability of the encounter collision from vehicles at the same direction is increased."
"in addition to the tdma-based mac protocol, spacedivision multiple-access (sdma)-based schemes are also considered [cit] . the basic idea of sdma is to divide the road into separated cells, and each cell has its own assigned time slot. however, the network utilization is potentially low when the traffic is sparse. it is a waste of bandwidth to assign slots to the cells with no vehicle. the fairness may also become a problem for different traffic densities on different cells. therefore, we focus on the tdma-based protocol in this paper."
"we take fig. 4 as an example, and the original slot arrangement is shown as fig. 5(a) . vehicles a and b are occupying the same slot 3. this potential collision will be detected by both the intermediate vehicles i 1 and i 2 . since i 1 has not received a notification about the detected potential collision, it will become the responsible intermediate vehicle who needs to broadcast a notification about this potential collision. as i 1 's one-hop neighbor, vehicle a will be selected as the \"switching vehicle\". meanwhile, since the slot of vehicle i 2 is behind the slot of i 1, i 2 is able to hear the switch notification from i 1 and does not need to broadcast a duplicate notification. as shown in fig. 5(b), when vehicle a receives the notification, it will randomly switch to another available slot. after a switches to a new slot, all its neighbors will update their fi about a. therefore, the potential collision between vehicles a and b can be eliminated before it happens."
"the question, then, is which model to choose for our network control language, taking into account that the abstract model must ensure the expressiveness, the modularity and the flexibility of the language."
proposing advanced programming interfaces for sdn controllers has already been the subject of numerous research projects. in this section we briefly present the most important ones and their main contributions.
"considering the above discussion, using edges and fabrics will allow us to overcome the limitations of both previous models. indeed, using fabrics enables network administrators to abstract packet transport issues, thereby allowing them to focus solely on the definition of complex in-network functions. by contrast, the possibility to use multiple edges allows, on one side, to maintain the language's flexibility and, on the other side, to decouple and distinguish in-network functions, thus facilitating their test, debug and, more especially, their reuse."
"the above studies match well with a learner-centered environment that encourages students to construct knowledge through active learning, communication, critical thinking, and problem solving. 7 structuring knowledge can assist students in using their knowledge more effectively in order to solve ill-defined medical cases. for instance, it has been suggested that the reason some students do not remember facts and concepts of a discipline or do not know when to use their knowledge, compared with experts (faculty) who do remember their knowledge, is that faculty's knowledge is well structured and organized in their memories. 15 this indicates assignments and problems that assist students in structuring their knowledge can promote the quality of student thinking and learning."
"therefore, we can see that, with higher traffic density, a vehicle may need to switch its slot multiple times to avoid the encounter collision. this is also the reason that t should not be too small. otherwise, a only has one or two chances to switch its slot, which may cause failed collision elimination, particularly under a heavy-traffic-density scenario."
"for potential collision detection, vehicle information is unnecessary, and the detection can be completed only by slot information. therefore, there is typically no additional overhead for potential collision detection in ptmac. on the other hand, vehicle information, such as speed, position, and moving direction, will be helpful for potential collision prediction. once a potential collision is detected, such vehicle information will be requested and used for potential collision prediction. in this case, very small overhead is introduced for collision prediction in ptmac since only the potentially colliding vehicle's information will be transmitted upon the detected collision and request. more details will be discussed in the following."
"considering a real traffic environment, a few distributed tdma mac protocols have been proposed for a two-way traffic scenario [cit] . there are two types of collisions. the first type is contention collision, which happens between newly joining vehicles who are trying to reserve the same available slot within a two-hop communication range. the newly joining vehicles are those that have not reserved a slot and intend to transmit packets. another type of collision is encounter collision, which happens between vehicles that are currently occupying the same time slot. they are originally out of the two-hop range but approach and encounter each other later. although some slot partition methods, such as even-odd [cit], are proposed for eliminating the encounter collisions between vehicles running at opposite directions, the slot utilization becomes low when the traffic density is high in one direction while low in another. moreover, they cannot eliminate the encounter collisions among vehicles from the same direction. furthermore, none of the previously proposed mac protocols work well at the four-way intersections."
"finally, we call the main function that allows, on one side, to pass arguments to transport and in-network functions and, on the other, to orchestrate their execution in order to obtain the overall desired network behavior. here we pass the identifier of the corresponding virtual device on which each function will apply. after execution of this main program, the policy resulting from the combination of the four functions is processed and enforced onto the physical infrastructure by a runtime system, which we are currently prototyping using the python language and the pox controller [cit] ."
"where t equals to the k in (1). when an intermediate vehicle finds that two potentially colliding vehicles are approaching each other and (2) is satisfied, it regards this potential collision as active."
"computers keep track of information present in routers like information about packets, number of bytes and errors that are transmitted and received through each interface. all this information is kept in a database called mib. for management tasks snmp uses this mib along with structure of management information (smi)."
"in order to allow network administrators to easily and clearly design their virtual networks, we have chosen a fully declarative approach. thus, building a virtual network would only imply describing virtual devices and the connections (i.e., virtual links) that exist between them. we distinguish three types of components, depending on their role in the virtual network."
"the third and last group of edge primitives are queries. like actions, queries are applied on filters. we distinguish two main kinds of queries depending on the type of information they return. the first kind is composed of packet count and byte count which, as their name suggests, allow to periodically poll packet and byte counters that are associated to filters. the second kind of query is packet which allows to poll entire raw packets. in addition to providing the ability to conduct network monitoring, queries enable network administrators to construct dynamic policies by allowing them to associate queries to callback functions that are executed each time a raw data is collected or a timer has elapsed."
"unfortunately, current sdn controllers provide low-level programming interfaces that have several limits, the most restrictive ones being: i) the inability to write separate modules that compose and ii) the obligation to directly deal with the complex and dynamic nature of the physical infrastructure."
"in a learner-centered paradigm, students are actively involved in their own learning because they are motivated. in a study conducted by cheang, a series of surveys was developed to determine the effect of the learner-centered paradigm in enhancing students' intrinsic motivation to complete a pharmacy course. the results of this study indicated that goal orientation, control of learning, and self-efficacy, which were a few domains of motivation, were significantly improved and assisted students in developing learning skills and self-awareness. 35 similarly, spencer and phipps, in their learner-centered study of a drug literature evaluation course, suggested that students had more control of their learning environment and had multiple opportunities to demonstrate their learning."
"an encounter collision is caused by two vehicles that are currently reserving the same slot and out of the two-hop range but will encounter each other in the near future. assume that there are two newly joining vehicles a and b, and they are trying to reserve their slots. if we know that they will encounter in the future (e.g., driving at opposite directions and approaching each other), the probability that a and b will select the same slot (an encounter collision will happen) is computed as"
"another problem is monolithic ids design. each node must have an ids client and should take part in global detection process. to get rid of this problem modular ids should be implemented using mobile agents. by this we have many advantage of increase in fault tolerance, reduced communication cost, and improved performance of whole network and scalability. [cit]"
"the remainder of this paper is organized as follows: in section ii, existing works are briefly presented. in section iii, we discuss network abstraction models that are currently used by existing control languages, then we describe our new approach. section iv gives an overview of our language's key elements. an illustration program is exposed through a toy example in section v. finally, we conclude and shortly present ongoing work."
"edges are more complex devices than fabrics, and hence expose a richer set of instructions. edge primitives are divided into three main groups : filters, actions and queries."
"the remainder of this paper is organized as follows. section ii provides an overview of the related work. we introduce our proposed ptmac protocol under two-way scenarios in section iii and extend it for four-way intersections in section iv. performance analysis of adhoc mac, even-odd mac, and ptmac is provided in section v. in section vi, we evaluate the performance of our ptmac protocol and compare it with other tdma-based mac protocols, and section vii gives a conclusion of this paper."
"they demonstrated that the implementation of pbl significantly improved student skills in clinical knowledge, attitude, and practice. a majority of their students stated that pbl enhances self-directed learning. 27 [cit] in a medical school in india, the investigator indicated that the implementation of a pbl curriculum, alongside their traditional didactic curriculum, improved students' motivation in self-directed learning and benefited student learning by relating a clinical condition to a basic science mechanism. 28 in addition, in a review conducted by norman and schmidt, it was suggested that pbl enhanced student self-directed learning and made students more enthusiastic learners. 5 as a result of the role of the faculty as facilitator, rather than primary information giver, self-directed learning is strongly promoted in a learner-centered model. it has been suggested that when the faculty's role is to guide and coach, they provide an environment for students to discuss, explore the available resources, and use their own knowledge to make an informed judgment."
"despite a variation in the lengths of pbl curricula (at a course level versus at a program level) presenting a challenge in this study in terms of providing a pattern of consistent measures of the effectiveness of pbl, one can create an interface between pbl and a learner-centered paradigm. during the last 40 years, pbl has represented a major shift in educational practice, particularly in medical schools, and is one of the most studied and researched curricula in higher education. 34 [cit] s, as a product of the joint task force on student learning appointed by the american association for higher education, the american college personnel association, and the national association of student personnel administrators, many universities and colleges have departed from a teacher-centered paradigm and moved toward a learner-centered paradigm. 7, 48 as suggested in the results section of this review, in a learner-centered paradigm, students construct knowledge through active learning, communication, critical thinking, and problem solving, which are also skills accommodated by a pbl curriculum. structuring of knowledge and clinical context is a challenging task for medical students and often requires the faculty's intervention to facilitate this skill. in a pbl curricular activity, collaborative learning is encouraged to conduct effective discussions, integrate new information, and apply prior knowledge which, in turn, provides an environment where students can construct knowledge."
"all networks are vulnerable to different attacks. regardless of whether the network is wired or wireless, network security and integrity should always be preserved. as it said that wired ids are not capable of taking care of things in wireless there is strong demand of effective ids for wireless networks. due to dynamic nature of wireless networks it is a challenging topic of research. we have shown that architecture for wireless ids should be distributed and cooperative in nature. so, in the proposed system using information from mib variables intrusion will get detected and mobile agents will alert other nodes by passing the message. it works in co-operative way."
"for all the three mac protocols, the contention collisions are not only caused by the newly joining vehicles but also from the vehicles that have suffered encounter collisions and have to recompete for new slots. therefore, reducing the number of encounter collisions is also helpful for decreasing the number of contention collisions."
"in complex network environment it is very difficult task to manage all the devices like routers, switches and servers. they should be up and perform optimally. snmp helps to do this. it is application level protocol and set of rules that allows computer to get statistics from another computer across the network. this is a standard for managing internet protocol (ip) devices. here a few manager stations control a set of agents. the manager station is a host that runs the snmp client on it and agent, which is a managed station, is a router that runs the snmp server program on it."
"if the intrusion is detected by lids, then it will generate alarm locally and then message will be passed to all the nodes present in ad-hoc network using mobile agent. whenever any such type of message arrives at any node, then corresponding lids will generate alarm or will display warning message."
"2) opposite direction potential collision prediction: for vehicles running at opposite directions, potential collisions may be detected between vehicles that are running toward each other or farther away from each other. an example is shown in fig. 4"
"contrasting with adhoc mac, even-odd, and four-part protocols, ptmac performs better with 48.1%, 44.7%, and 47.9% fewer collisions, respectively, when we set 64 slots in a frame and 150 vehicles for each direction. in the same environment, the packet delivery rate of ptmac improves by about 8.6%, 7.4%, and 8.5% compared with that of adhoc mac, even-odd, and four-part protocols, respectively. when we increase the traffic density to 175 vehicles for each direction, ptmac has 10.9%, 10.7%, and 10.8% higher delivery rate compared with that of adhoc mac, even-odd, and fourpart protocols, respectively. for heavier traffic density with 200 vehicles for each direction, the efficiency of ptmac is weakened with a smaller number of slots in each frame since the number of slots is not enough. however, it still has 5.5%, 10.5%, and 8.8% higher delivery rate than that of adhoc, even-odd, and four-part protocols, respectively, for 64 slots per frame."
"finally, it is worth mentioning that tutors have a critical role in the pbl process. a tutor's way of coaching can change the interface between a pbl curriculum and a learnercentered paradigm. in order to maintain the interface between these two curricula, special attention needs to be paid to train tutors in a pbl process. for instance, when tutors are confronted with problems in group work, such as students who do not actively participate in group discussions or do not contribute to achieve the goals of the study, those tutors who implement solutions to problems or cases are characterized as teachers in a teacher-centered paradigm rather than facilitators in a learner-centered paradigm. 57 the information presented in this review shows that there is adequate support to create an interface between the objectives of pbl and a learner-centered paradigm. in addition, the faculty's role as a facilitator, involving students in evaluating student learning, and intertwining teaching and assessment, are other components of a learner-centered paradigm that often are visible in a pbl curriculum. these parallel matches indicate that implementing a pbl curriculum can facilitate a smooth transition from a teacher-centered paradigm to a learner-centered paradigm. [cit] has influenced medical education in the us and canada in a positive way. 58 this report emphasized the importance of applying alternative instructional methods that stressed active learning for medical students. indeed, what flexner was referring to was similar to what pbl and learner-centered curricula refer to regarding replacing student's passive learning with an active learning process. however, it is important to stress that a theoretical wellmatched alignment between a pbl objective and a learnercentered criterion does not justify an interface between a pbl curriculum and a learner-centered paradigm. in other words, there must be existing data to support a coherent curricular interface."
"to overcome the shortcomings of ieee 802.11p, timedivision multiple-access (tdma)-based mac protocols have been proposed to facilitate efficient transmission in vanets 0018-9545 © 2016 ieee. personal use is permitted, but republication/redistribution requires ieee permission."
"although the carrier sense multiple access/collision avoidance (csma/ca)-based ieee 802.11p [cit] has been approved as the standard mac protocol, it has the problem of high collision probability if the traffic density is high, particularly for packet broadcasting [cit] . broadcasting plays an important role on propagating safety-related messages, such as vehicle accident warning and road condition alerting. in addition to the event-driven messages, wireless access in vehicular environments (wave) [cit] also develops additional layer protocol, including the basic safety messages (bsms) and wave basic service advertisements (wsas) [cit] . the bsm contains critical vehicle's status information, such as its location and speed. to support most of the applications and make sure that the potential dangers can be detected on time, every vehicle is required to broadcast and exchange bsms periodically, i.e., at least once in every 100 ms [cit] . wsas are also needed to be periodically broadcast by rsus or vehicles to mostly support nonsafety services."
"as mentioned earlier, the biggest advantage of using the platform as a service model is that it allows network administrators to focus only on the expression of in-network functions that they plan to install on their network. however, we think that, from a network programming language point of view, this model suffers from a big disadvantage: it forces network administrators to put different in-network functions within the same router, thereby the resulting application will be a monolithic program in which the logic of different innetwork functions are inexorably intertwined, making them difficult to test, debug, maintain and reuse. moreover, forcing network administrators to always use a single router as an abstract topology can significantly affect one of the language's fundamental characteristics, namely its flexibility. for instance, using this model clearly makes it difficult to define middlebox functions (e.g., firewall, deep packet inspection) or to represent a network that contains multiple administrative boundaries."
"intrusion detection systems are burglar alarms of computer security field. here other than alerting the host system on which ids is installed, this system forwards information of detected intruders to other nodes in the network. the aim is to warn other nodes prior about the intruder so that they can take precaution. fig.1 system design fig.1 shows overall system design of co-operative multi-agent based wireless ids using mibs. ids agent is present on each system. internal of this ids agent can be shown as above diagram which has four modules in it."
"networks have become increasingly dynamic, complex and hard to control due to major evolutions in computing environments, such as the exponential growth of mobile devices, desktop and server virtualization, the wide adoption of cloud computing or the advent of \"big data\". administrators are therefore looking for more flexible networks that can quickly adapt to the evolving needs of today's enterprises, carriers, and end users. software defined networking (sdn) is the latest attempt in order to respond to this lack of flexibility of current network architectures. in order to do so, sdn decouples the control plane (which decides how to handle packet flows) from the data plane (which forwards packet flows according to decisions taken by the control plane), and centralizes it in a logical and programmable entity called controller [cit] . by using this logical and central point of control, network administrators are able to quickly define and change network behavior by simply (re)programming the controller using the provided programming interfaces. communications between the control plane and the data plane are then enforced via an open and a well-defined protocol such as openflow, which is currently the most accepted standard [cit] ."
"the process of potential collision detection can be described as: based on the message containing the fi received from other vehicles (one-hop neighbors), every vehicle needs to check whether any two of its one-hop or two-hop neighbors are occupying the same time slot. every vehicle learns the information of its two-hop neighbors from its one-hop neighbors. therefore, a potential collision can be detected between two vehicles at most four hops away. since each vehicle tries to avoid reserving the same slot with other vehicles within a twohop range, a potential collision can only be detected before it happens between two vehicles that are three hops or four hops away. however, since two vehicles with four-hop potential collision are still far away from each other and will be safe for a time, we only need to concern the potential collisions detection for vehicles that are between two to three hops distance. for example, figs. 2 and 3 show the potential collisions that are detected among vehicles at the same direction and opposite directions, respectively. in both cases, vehicles a and b are currently out of the two-hop range but are within the three-hop range. they are occupying the same slots i, but they cannot find this potential collision by themselves. instead, the intermediate vehicles x and y have the slot information about both a and b; therefore, they are able to detect this potential collision between a and b."
"solutions constructed by hyperplane initialization have better evaluations than those found by randomly assigning solutions. for small enumerable instances, these initial solutions were shown to have variable assignments consistent with the correct assignment for backbone variables in the majority of cases."
"the simulation runtime is set to a five-year period to cover seasonal variations in incident appearance and weather. moreover, long-term effects such as uav crashes can be observed and analyzed."
"the software of choice is anylogic c (version 6.7), a software tool developed by xj technologies c . anylogic offers a true agent-based framework embedded in a classical discrete-event environment. the schumann, scanlan, and fangohr java-based software allows multi-core evaluation of design points, enabling exploration of large design spaces and long life-cycles."
"genome-wide approaches often generate low-level data on thousands of genes or genomic regions, the interpretation of which becomes much more valuable when integrated with previously published studies on individual loci."
"despite the obvious advantages of using an operational simulation to improve design, the uncertainties of modeling an operational environment for aerospace products are much greater than those of a well-known factory floor. the time frames involved are several magnitudes larger because the simulation models a future product in its future environment, often a decade or more in advance for aerospace products [cit] . these uncertainties grow exponentially with product design time. civil uavs currently have a much shorter design times. therefore, designing uavs offers a unique opportunity to explore operational simulations for larger aerospace products. further uncertainty comes from outside environmental factors such as business competitors or market changes. moreover, interactions of the product with its environment must be anticipated, often lacking trustworthy data. consequently, the model described here cannot hope to give an accurate estimate of performance of the final product. instead, it is envisaged to guide designers and support their decisions by informing about trends, theoretical optimums and unforeseen consequences of design decisions. however, this must not distract conducting sensitivity analysis for inferior data, otherwise even such a \"proof-of-concept\" tool can be misleading."
"the search space of a maxsat instance with n variables and m clauses corresponds to a n-dimensional hypercube. if we 'fix' the truth values of j variables to 1 or 0, the search space is reduced to a (n − j)-dimensional hyperplane."
"we analyzed 14 [cit] challenge (http://maxsat.ia.udl.cat/); the problems range in size from 247,943 to 2,766,036 variables. as shown in table 1 real world industrial problems have fewer walsh coefficients on average than random max-3sat problems. as the update cost during search is a function of the number of walsh coefficients, the update cost for industrial maxsat problems will be much faster than updates on uniform random problems."
"it is possible to calculate a \"value\" from a specific uav design using the outputs above. more importantly, the designer must be able to compare different designs and explore design spaces. for this, three-dimensional surface plots can be helpful. however, surfaces must be smooth to be able to deduct trends and find optimal designs. otherwise, neither automated design optimization nor manual design analysis is feasible."
"the operational simulation presented here is a proof-of-concept tool implemented into the uav design suite developed by the decode-team. we note that absolute values of the outputs have significant uncertainty. a wide range of assumptions, both data-and procedure-wise create uncertainty. in an industry setting, this level of uncertainty must be overcome by rigorous data mining and model building. however, this additional work would not change the trends of the output plots significantly. it has been shown that much can be learned from these trends alone and they can be used to steer design decisions where rigorous model building is not possible."
"the walsh function, bitcount(i ∧ x), counts how many 1-bits are in the bit string produced by the logical operation i ∧ x. if the bit flip in string x does not interact with a bit in string i, it has no impact on walsh coefficient contribution ψi(x)wi. if the bit flip in x does interact with a bit in string i, it flips the sign generated by ψi(x) since bitcount changes by exactly 1."
"this work has demonstrated the effectiveness of using hyperplane averages to initialize the search. although our results are on maxsat, this method can be applied to any pseudo-boolean function, such as nk-landscapes or spin glass problems. our future work will explore other methods of guiding local search based on the variable interaction information contained within the walsh coefficients."
the calculation of hyperplane averages requires walsh coefficients; we show how the coefficients can also be used to implement a constant time neighborhood update in local search. we use this update to develop a constant time next descent local search algorithm. this algorithm helps assess the effect of hyperplane initial-ization on two factors: the first local optimum encountered and the final solution after n bit flips.
"to compare the proportion of articles with dna sequences to the proportion of articles accompanied by a genbank submission, we estimated the number of pmc-oa articles that have a nonhigh-throughput genbank record. overall, we found that 6.7% (10 378/153 513) of pmc-oa articles are linkable to a genbank submission (supplementary file 5), and that this number of articles has remained relatively low over time (fig. 1) . as expected, we can extract nucleotides from the full text of the majority of pmc-oa articles with a genbank submission (76.5%, 7937/10 378). surprisingly, 77.2% (26 891/34 828) of articles with extractable nucleotides in their full text are not linkable to a genbank submission. this result implies that the majority of sequences extracted from full-text articles have not been submitted to any nucleotide data bank."
"for publications where authors report dna sequences directly, these problems could be solved if all published sequences were systematically sent to primary sequence databases such as genbank [cit] . however, in the post-genomic era fewer articles report primary dna sequences directly and instead only report primers used for polymerase chain reaction (pcr)-based techniques that are designed from published genome sequences. furthermore, in contrast to longer dna sequences, journals generally do not require deposition of short primer sequences in databases, and the minimum sequence length required for a genbank submission is 50 bp (http://www.ncbi.nlm.nih.gov/ sitemap/samplerecord.html). as such, many dna sequences that could provide unique tags to link articles to specific genes and genomes remain locked in the biological literature."
"the resulting blat matches from genomes and transcripts were then filtered to obtain the best matching species, genes and genomic regions. for each extracted sequence, only the highest scoring hits were retained. hits to common plasmids and sequencing vectors were removed (using data from ncbi univec). in order to disambiguate sequences matching several different organisms equally well, we extracted all mentions of organism names from the articles using default settings of linnaeus [cit] . if the full text contained organism names detected by linnaeus, only blat matches for these genomes were kept. if no organism mentions were found, the matches were limited to human and major model organisms. if there was no best match among these genomes, all remaining matches were retained. the best genome was determined as the one with the highest number of matching sequences at the gene or genomic level. to account for conserved sequences that may hit highly similar genomes (e.g. chimpanzee and human), the best genome for species that had the same number of best hits was decided by ranking genomes based on the species with the higher number of publications in entrez gene."
"analysis is based on surface plots that compare 60 different uav designs by their variation in permissible payload and landing speed (see sample plot in figure 8 ). every design is based on the same uav configuration (figure 4 ). increasing the permissible payload changes the uav design such that larger wings are necessary and maximum flight speed reduce slightly. however, a better camera can be used. increasing the landing speed acts as a design variable that correlates with the general speed of the uav. if landing speeds can be higher, wing size can be reduced because less lift is required during landing. this, in turn, allows higher flight speeds because smaller wings create less drag. also, overall fuel consumption is reduced."
"the first case study above informed the designer that a better or faster uav (more payload or higher landing speeds) will accumulate more takeoffs. this must be taken into account during part design of the undercarriage which must become more robust. this, in principle, will increase the overall weight of the aircraft which will reduce general flight speeds again. this would cause a reduction in takeoffs. it is beyond the scope of this paper to investigate the quantitative nature of this negative feedback loop."
"if the uav fails to identify the incident, it will continue its search in an ever-expanding square pattern until it runs out of fuel. the uav will return to refuel at its base station. subsequently, it will launch again to find the incident, possibly crossing it a second time. if, during an expanding-square pattern, the uav covered a search area larger than 40 by 40 kilometers, it will initiate a new pattern at a different start location. this imitates real search behavior where a maximum search area is defined and covered multiple times if necessary. the uav only makes an impact on search operations if it finds the incident the first time. otherwise, either other lifeboats searching together with the uav will have found the incident or too much time will have passed to rescue the incident alive."
"to find a trade-off between better performance and the number of uavs purchased in total. this is where the value function becomes useful: it weighs product performance factors (mass, speed) against operational performance (number of saved lives, number of crashed uavs) and returns a simple (monetary) number to help decide if it is better to use a well-equipped uav that often crashes or an inferior uav that lasts longer. results are currently analyzed and will be published separately."
"in total, text2genome identified 12 655 genes in 9935 articles, the majority of which (6907, 70%) are from the human and mouse genomes. for sequences mapping to human and mouse, the most frequently hit genes are actb and gapdh (fig. 4), which are well-known control loci for rt-pcr experiments. other genes with sequences frequently reported in the literature are from heavily investigated loci involved in immunity (tnf, il6, ifng) or cancer (p53, myc). detected genes are spread over a substantial proportion of human and mouse genomes. for example, articles linked to the human genome cover 21.1% of the 19 814 human and 13.4% of the 20 192 mouse gene models that are listed in ensembl 56 and entrez genes."
"permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. gecco '13 for all k-bounded pseudo-boolean optimization (pbo) problems, we can convert the evaluation functions into a polynomial form in o(n) time. this allows us to quickly and exactly compute low order hyperplane averages. we can then explicitly determine which combination of variable assignments will lead to the highest overall combined hyperplane average. by using this information to initialize search we achieve two results: 1) search starts at a solution that must be better than average, reducing the number of steps needed to reach a local optimum, and 2) the initial solution is in a region of the search space that is also better than average relative to other regions. thus, not only is the initial solution better than average, solutions that are nearby in hamming space must also be better than average."
"a higher payload increases the fuel burn because the aircraft becomes heavier and requires more power (i.e. fuel) during flight. in addition, increasing the payload while keeping the landing speed constant requires larger wings to produce the required lift. this, in turn, produces more drag which must be overcome by burning more fuel. however, there is a balancing effect to this: the operational simulation also evaluates the effect of a higher payload, namely that uavs find incidents earlier. the uavs have fewer false alarms (far) and miss the incident less often upon crossing it (mdr). therefore, the uav spends significantly less time in the air, leading to a reduction in fuel consumption while increasing the number of incidents found alive. the simulation combines these two effects and informs the designer that the former is more acute in the chosen scenario."
"to address the effectiveness on application problems, we selected a subset of the 52 [cit] challenge. the number of variables (n) and number of clauses (m) are shown in the top rows of table 2 . these problems were used in a circuit-board testing method utilizing maxsat solvers [cit] . these problems therefore represent circuits converted to a boolean formula in conjunctive normal form as previously described."
"for the wider modeling community, there are two new developments exemplified here: on the one hand, an optimum design can be chosen based on a global value calculation that takes into account a number of life-cycle considerations (fuel usage, maintenance issues, crashes...). on the other hand, the operational scenario itself can be optimized: is it better to use many small and cheap uavs in a swarm or one large, highly capable but expensive uav? is it better to station uavs at every lifeboat station or just at strategic points of interest? it is this combination of information that can distinguish a good from an outstanding design. although discussion has been limited to uavs in this paper, the general design approach can be extended to many other areas such as electric cars. this paper introduced the usage of an agent-based operational simulation to support design of uavs for sar missions. it has been shown that an operational simulation can be used to create novel knowledge about the behavior of an aerospace design in its intended operational environment. this knowledge can challenge engineering intuition because complex agent interactions over the life-cycle of the product create unexpected performance outputs. it has been shown that it is possible to explain such results by consulting the simulation operation methods."
"finally, we ran our search using both initializations on the instances along with several algorithms from ubcsat. walshsat-n with hyperplane initialization consistently found better solutions than search initialized with random solutions. surprisingly walshsat-n was able to outperform the ubcsat algorithms in the vast majority of cases. this includes adaptg2wsat which was shown to be the best performing algorithm in ubcsat on industrial instances [cit] . walshsat-n was also much faster than these algorithms, but this is largely due to the use of next-improving moves instead of greedy best-improving moves used by adaptg2wsat and gsat."
"we analyzed the distribution of species and genes in our dataset to provide an overview of the taxonomic and genomic data extracted. as expected, we found that sequences in full-text articles most frequently map to the human and mouse genomes, as well as other organisms used in basic or agricultural genetics (fig. 3) . trends in species identified using sequences in full text are largely consistent with the relative proportion of species in pmc-oa articles with genbank records and the relative proportion of species mentions that can be extracted by linnaeus [cit] . only 2 of the top 10 species mentioned in the entire set of medline abstracts [cit] are missing from the top 10 text2genome best species list: hiv and dog."
"not only are the local optima found by hyperplane initialized search better than those found by random initialized search, but they are also found in less bit flips. thus the initial solutions constructed by hyperplane voting are closer to better local optima than random solutions."
"we find that the first local optima found by next descent search when initialized with hyperplane averages have better evaluations than those found by randomly initialized search. these results are consistent across both real world and random instances of maxsat. furthermore, the first local optima are found more quickly and are closer to an optimal solution on an industrial maxsat problem."
"this reduction yields four clauses that reference exactly the same three variables. it is simple to prove by enumeration of cases that all \"if and only if\" expressions over three variables will yield exactly four literal clauses unless the expression reduces to a simpler form."
"dna sequences extracted from text do not themselves contain information about the species or gene from which they were obtained, but instead must be mapped to other annotated sequences in order to propagate this meta-information to the articles in which they were found. therefore, we searched all sequences extracted from text against all genome and transcript sequences in the ensembl and ensemblgenomes databases and resolved the best matching species, gene and genomic region (see section 2 and supplementary file 1 for details). an example of a genome browser view of the text2genome mappings for a region of the mouse genome containing the tumor necrosis factor (tnf) gene is shown in figure 2 . overall, 79.3% of articles with sequences (27 632/34 828) lead to a single best species prediction that is based on hits to a best matching genomic region (99.3%, 27 452/27 632) and/or a best matching gene (40.0%, 9935/27 632). roughly one-third of all article-species associations are based on both a best matching genomic region and a best matching gene hit (35.3%, 9755/27 632), indicating that many sequences extracted from articles map to intronic and intergenic regions. in total, these articles generate 247 007 unique associations between articles and genomic regions (supplementary file 6) and 23 388 unique gene-article associations (supplementary file 7) . the ∼ 21% of articles with sequences that do not lead to a best genome prediction at all arise from sequences in repeated regions, from cloning/sequencing vectors, or species not currently represented in ensembl or ensemblgenomes."
"when a uav searches for a missing person, it may occasionally happen that the image analysis software carried on board mistakes a wave crest or any other item floating on the sea for the head of a person or for the life raft in question. to model this, we define the false alarm rate (far) as the fraction of images that will report a false positive, i.e. those that will report an incident where there is none."
"large uniform random maxsat problems have surprisingly few duplicate nonlinear interactions. the maximum number of nonlinear terms in max-3sat is four. randomly generated maxsat problems with 100,000 variables typically have 3.999 unique walsh coefficients per clause; this number approaches 4.0 as n increases."
"we found that the first local optima found by next descent search when initialized by hyperplane averages had better evaluations than those found by search initialized with random solutions. furthermore, the hyperplane initialized search required less bit flips to find the first local optimum. comparing the hamming distance of local optima to a globally optimal solution on industrial instances, we found that search initialized with hyperplane averages found local optima closer to the global optimum."
whitley [cit] translates this special case result into an o(1) bound on the average case computation. whitley and chen extend that result to cover nk-landscapes [cit] . this proof depends on holding bits that appear in a large number of clauses tabu after they are flipped to amortize cost; this can be particularly important on industrial maxsat problems.
"to evaluate the accuracy of text2genome species and gene mappings, we used as a reference the set of articles where the original authors submitted sequences to genbank. we chose not to use data from entrez gene as a gold standard since it is part of our pipeline, and since entrez gene may curate species or genes that are mentioned in an article but for which no sequences are reported. the text2genome-inferred gene or species was considered to be correct if it matched any of the genbank-derived information for an article. we attempted to filter out articles from this evaluation set that reported either (i) high-throughput sequencing results cutoff refers to the number of predictions allowed for text2genome or gnat predictions and the genbank evaluation set. n refers to the number of predicted species-article or gene-article associations for each method and for the genbank evaluation set. tp, fp, and fn refer to true positives, false positives and false negatives, respectively. precision is defined as tp/(tp + fp) and recall is defined as tp/(tp + fn)."
"once an incident occurs, lifeboats and the uav dash out to the \"initial search position\", a point somewhere near the incident symbolizing the last known position or best position guess. upon arrival, the agents start an \"expanding square pattern\" search, an internationally agreed search pattern suitable for incidents with relatively good knowledge of position [cit] . this pattern can be seen in figure 5 ."
we have investigated the use of hyperplane averages as a means to initialize stochastic local search for maxsat. this method uses configurations of variables that correspond to hyperplanes with good averages to construct a probability distribution over all variables.
"although hyperplane initialization finds better solutions in less bit flips, this could be detrimental to the overall search if the local optima are further away from a globally optimal solution. is hyperplane initialization guiding the search closer to globally optimal solutions or away from them? to answer this question, we calculated the hamming distance of the local optima found by hyperplane initialized search to a known globally optimal solution on the i2c-25 industrial instance. the hamming distances and the evaluation differences are shown in figure 3 for instance i2c-25. the local optima found by hyperplane initialized search are closer to the global optima than those found by random initialized search. thus, in this case hyperplane initialization does lead the search to better parts of the space."
"we find that 96% of species-article and 91% of gene-article associations predicted using text2genome match those based on genbank submissions from articles discussing a single species or gene. when compared with a state-of-the-art text-mining method that attempts to associate articles to species or genes by named entity recognition, text2genome exhibits much higher performance than gnat for species (72%) or gene (51%) prediction. thus, if researchers are looking for genes specifically investigated at the molecular level in an article, our results indicate that dna sequences in text provide a richer source of information than gene names. it is important to point out that our evaluation of these systems is benchmarked against genes from associated genbank sequence submissions spanning a wide range or organisms. since many more genes are mentioned in the literature than are actually studied experimentally and since gnat only recognizes genes for a limited set of species, the performance of gnat on our genbank evaluation set may be reduced relative to benchmarks performed on gene names [cit] ."
"hyperplane voting can improve the initial solution and provide an estimation of the backbone variables on random instances of max-3sat, but how do the solutions produced by hyperplane voting influence subsequent search? we investigate the effect of hy-perplane voting on two factors: the first local optimum encountered and the final solution after n bit flips. in these experiments, we have two sets of benchmark problems: random and industrial instances."
"the early theoretical analysis of genetic algorithms emphasized the potential for populations to explicitly estimate hyperplane averages and to use this information to guide search [cit] . while this line of research has been criticized [cit], a similar idea is at the foundation of estimation of distribution algorithms: information about the interaction between variables can be used to guide search [cit] ."
"note that the reduction in fuel burn is only valid within the current design regime of medium flight speeds and aircraft sizes. at some point, drag would increase and fuel usage would rise again when flying faster."
"as discussed in section 1, the operational simulation informs the designer about the performance of his design as part of a suite of softwares. hence, the output of the simulation is fed into subsequent analysis programs. the outputs must be unambiguous, clear and informative. three sets of outputs have been identified to be able to calculate the \"value\" of a design easily: scenario outputs, uav outputs and performance outputs."
"however, designing for a higher landing speed allows the uav to generally fly faster. therefore, a faster uav returns home from a mission earlier than a slower uav. occasionally, it is notified of a subsequent incident that needs to be searched for and launches for this subsequent mission. the slower uav, on the other hand, returns to its base later by which time the subsequent incident has been found already by other vessels. therefore, faster uavs generate more takeoffs than slower uavs."
"precision was defined as the number of species-article or gene-article predictions by text2genome that matched at least one species-article or gene-article association defined by the genbank record, as a proportion of the total number of text2genome predictions. recall was defined as the number of species-article or gene-article predictions by text2genome that matched the species-article or gene-article associations defined by the genbank record, as a proportion of the total number of associations defined in the sample of genbank records."
"in max-3sat, given a clause vi containing the variables p, q, and r, there are eight possible assignments of these variables: 000, 001, 010, 011, 100, 101, 110, and 111. we compute the averages of the eight hyperplanes formed by fixing p, q and r to each of these partial assignments and leaving the remaining variables free. this process is repeated for each clause in the instance. thus we compute eight hyperplane averages for each clause for a total of 8m hyperplane averages. we then use the hyperplane with the best average from each clause to calculate a probability distribution over all n variables as follows."
"text2genome: mapping articles to genes and genomes fig. 2 . example of text2genome mappings for the mouse tnf region. exons for refseq gene models of tnf and lymphotoxin a (lta) are shown as grey rectangles below, and chained blat hits from text2genome mappings are shown as black rectangles above. note that the majority of mapped papers contain pairs of sequences that are consistent with being pcr primers. the two larger mapped sequences come from original publications reporting tnf and lta primary sequences [cit] ."
"the pattern depends on the size of the scanned area which depends on camera capability, cruise height and weather. generally, the scanned area is larger for heavier (i.e. better) on-board camera systems, higher cruise heights, larger incidents (lost yachts are easier to spot than a drowning \"head in the water\") and better weather conditions. the larger the scanned area, the sooner the uav will find a casualty. lifeboats follow the same pattern but their pattern scan sizes depends on the size of the crew on board, weather and the type of the incident only."
biologists using quantitative rt-pcr [cit] to measure transcript levels need to select control genes and find validated primers and cycling conditions before conducting their experiments. as many of the sequences in our database map to genes that are commonly used in rt-pcr experiments (fig. 4)
"in this paper, we focus on maxsat for three reasons. first, local search algorithms for maxsat have been widely studied for more than 20 years. thus, improving local search algorithms for maxsat is very challenging. second, there are many well studied and widely available benchmark problems as well as real world problems that have been reduced to maxsat problems. third, other methods for initializing search have been studied for maxsat. in particular, other researchers have attempted to identify \"backbone variables\" as a way to initialize search [cit] ."
"only the most westward station \"lyme regis\" houses one uav. however, this uav will be called for search support for any incident along the south coast to force the uav to cover long distances before it can actually support searches. this is desirable to be able to distinguish very similar designs in the case"
"in an age of rapidly increasing amounts of dna sequence data and published literature, finding peer-reviewed experimental results for a sequence of interest is more time consuming than ever. here, we show that dna sequences in full-text articles provide a rich source of 'unique identifiers' that can be automatically extracted and mapped to genomic data in order to link articles to species, genes and genomic regions. we confirm recent findings that a substantial number of oa articles in pmc contain extractable dna sequences [cit] b), and provide the first quantitative estimate of the proportion of pmc-oa articles with dna sequences (∼ 22%), the majority of which we show are short sequences that are not found in genbank. our study is also unique in that it presents the first attempt to apply sequence extraction techniques at a large scale to all types of both full text and supplementary data files, and in fact may be the first systematic application of text mining to supplementary files in any domain. our observation that the majority of nucleotides in the pmc-oa corpus were extracted from supplementary files underscores the increasing reliance of authors to deposit important information contained in these files [cit], as well as the importance of using these resources for biological data mining and requiring ancillary research data to be persistently stored together with the main publication [cit] . future work will be necessary to determine if the quality of data from full text differs in any way from that obtained in supplementary files."
"in addition to providing bidirectional links between articles and genes or species, text2genome allows accessing the biomedical literature using the powerful tools of modern genome browsers. in this manner, text2genome joins a limited number of other hybrid text-mining/genome bioinformatics systems that provide mechanisms to interpret the biomedical literature via genome browsers, such as posmed [cit] and littrack (http://littrack.chop.edu/cgi-bin/hgtracks). however, since posmed and littrack rely on gene name recognition methods and therefore can only map articles to the gene level, genomic coordinates must be inferred indirectly by these systems, whether they are appropriate or not. by mapping at the dna sequence level itself, text2genome can directly identify the exact set of nucleotides in a genome sequence that is analyzed in a study. this distinguishing feature of our system is critical for researchers studying non-genic sequences such as cis-regulatory regions or mirna binding sites. database curators in these and other areas could use our system to aid in the prioritization and extraction of experimental data from papers."
"intuitively, an engineer would expect that in order to obtain better performance (i.e. save more lives) higher costs (i.e. burn more fuel) are inevitable. at some point, it becomes unfeasible to increase performance further because costs rise exponentially. however, comparing figure 10 and 11 reveals that this is only partly true for this scenario. saving the maximum amount of lives does not require the maximum amount of fuel. this is a key finding because the reason to employ uavs in sar is to save as many lives as possible."
"we now describe a method of exploiting hyperplane averages to construct solutions to max-sat that we call hyperplane voting. while we use max-3sat to describe the method, hyperplane voting can be applied to any max-sat problem."
"hits on the best genome were fused into 'chains'if they were located closer than 50 kb; hits on transcripts from the best genome were chained if they matched the same gene. when a sequence was a member of several chains (e.g. caused by matches to segmental duplications), the hit was retained only for the chain with the maximum number of other matching sequences. genes were predicted to be hit only if they matched at least two text-extracted sequences. if two genes passed this threshold and were hit by exactly the same sequences, only the gene with the largest number of publications in entrez gene was retained."
"in the simulation model, rescue stations and lifeboats are represented through agents. lifeboat agents vary in dash and search speeds according to their type. lifeboats are distributed among the stations as in reality. they dispatch upon receiving an emergency signal from incident-agents and start a search operation (see section 2.4)."
it has been shown how an operational simulation can reveal and explain unexpected behavior to the designer. the following lists some lessons that a designer could learn from the results.
"once the uav actually crosses the incident in question, it can either recognize it and notify the human operator at the ground station or it can pass the incident without noticing. therefore we introduce the missed detection rate (mdr) which indicates how often, on average, per incident crossed the uav will not notice the incident and thus not notify the authorities."
"to demonstrate the utility of text2genome for research in genetics and genomics, we highlight possible use cases in the following two examples. in addition to these examples, the dataset of sequences extracted from articles by text2genome should also be useful in many other contexts, not least for annotators of various biological databases [cit] ."
"to assess the longer term impact of the initialization, the algorithms are run for n bit flips, well past the first local optimum. again, the solutions found by hyperplane initialized search are always significantly better than those found by randomly initialized search. when our walsh-based next descent search algorithm is initialized with hyperplane averages, it outperforms several stateof-the-art stochastic local search algorithms for maxsat."
genbank records were used to generate a benchmark set of links between articles and species or genes. the pubmed document id and organism of a submission were parsed directly from genbank records using the chronologically first article associated with the record (i.e. the last or second-to-last 'reference' entry). genbank accession numbers in full-text articles were identified using the following set of regular expressions:
"there are two ways to obtain maxsat problems: they can be randomly generated, or they can be produced by reducing another problem to a maxsat problem. assume we wish to reduce the following boolean satisfiability problem [cit] ) into a max-3sat problem."
"scenario outputs relate to quantifiable data from agents that are part of the wider scenario selected by the user. they are expendable according to user needs, i.e. if a scenario includes other agents such as helicopters, appropriate outputs will be generated:"
"the last issue is cost: how much time is required to execute n bit flips by each of the tested algorithms? table 6 shows means and standard deviations of the time in seconds of each of the 10 runs for each algorithm in the right-hand columns. the left column of table 6 times shows the initialization time required to walsh coefficients and hyperplane averages. the median time required to compute the coefficients on industrial instances is 4.06 seconds. not only does hyperplane initialized walshsat-n find better solutions in most cases, but it is generally faster than the algorithms in ubcsat, in some cases by an order of magnitude or more."
"of course stochastic local search algorithms are typically run beyond the first local optimum. to evaluate the effect of hyperplane voting on longer runs, we ran both our hyperplane and random initialized walshsat-n for n bit flips, where n is the number of variables in the instance. we did 50 runs per instance and recorded the evaluation of the final solution found by each run. to determine how well walshsat-n is doing relative to other algorithms, we also ran several algorithms from the ubcsat search package [cit] : gsat, irots, and adaptg2wsat."
"as our walsh-based search algorithm is very similar to gsat, we expected that it would find similar quality solutions when initialized with random solutions and that it would outperform gsat when initialized with hyperplane voting. table 5 confirms this. surprisingly, we see that our simple walsh-based next descent method outperforms both irots and adaptg2wsat when initialized with hyperplane voting. this result highlights the power of hyperplane initialization: our search algorithm is able to outperform more sophisticated stochastic local search algorithms when leveraging hyperplane information."
"finally, the speedup induced by exploiting walsh coefficients is greater for industrial problems, as opposed to randomly generated problems. industrial problems tend to have a very high rate of co-occurring variables due to the algorithms used to convert general satisfiability expressions into cnf-sat. these co-occurrences translate into overlapping walsh coefficients and more compact walsh polynomials, which translates into faster updates per move."
"in reality, image analysis software on board the uav constantly processes the images taken. it is tuned such that it recognizes the incident type. in such a case, the image in question will be sent to a manned ground station where an operator can agree or disagree with the software's findings. if he agrees, the nearest lifeboat will be sent to the uav position and the uav returns home. otherwise, the uav is asked to reduce height and cross the spot in question again to take a close-up photo for the operator. in in order to analyze differences between uavs, design decisions must exert an influence upon the performance output of the operational simulation. one way to achieve this is by correlating the search performance of a uav with its payload mass. payload mass correlates with the quality of the camera system on board, i.e. a uav that is designed to carry more payload can house a heavier camera system. this model assumes that a heavier camera system has better quality in terms of spotting incidents. [cit], there are two influences that must be simulated when recreating camera-driven search for humans: the false alarm rate (far) and the missed detection rate (mdr)."
"in general, our filtering steps are designed to achieve high precision, which can result in no prediction for either genes or genomic features. for instance, genomic features but no genes are predicted if sequence map to noncoding regulatory dna (promoters and enhancers). conversely, genes but no genomic features are predicted if sequences are designed to span exon-exon boundaries such as morpholinos or primers for quantitative rt-pcr."
"each rescue station has a given number of incidents occurring each year based on historic values (see figure 2 ). this value is used to calculate random incident occurrence events. once an incident occurs, its distance to the rescue station is drawn randomly from the distribution seen in figure 3 . additionally, a random bearing is assigned. if this position is not on any landmass, the incident is created and other agents are notified about it."
"a core concept of decode is the use of an operational simulation to design and optimize uavs for their intended missions. in the traditional sense, operational simulations are highly detailed and realistic models of factory floors that support short-term \"live\" production planning [cit] . during the last two decades, usage has spread to transportation management (demitz, hübschen, [cit] ) and supply chain simulation. decode extends the use of an operational simulation to aerospace design."
"it should also be noted that a fast descent search with constant time updates could also be applied to the ubcsat algorithms to make them faster during the first phase. however, the second phase still accounts for the vast majority of these runs. although it is likely possible to further optimize the second stage, it would require a non-trivial amount of engineering to optimize the algorithms and it is not clear if this could be accomplished without significantly changing their behavior."
"we show how the walsh transform can be used to efficiently calculate the hyperplane averages of maxsat instances and then describe a method of using these averages to construct a solution. this method consistently produces solutions with better evaluations than those constructed with a uniform random distribution, the standard practice for local search algorithms. we conjecture that this method also finds solutions closer to the backbone. we empirically test this conjecture on random instances of max-3sat and find that our hyperplane construction algorithm sets variables in the backbone to their correct assignments in the majority of cases."
we evaluated the use of hyperplane initialization on local search on both industrial and random instances of maxsat. we examined two factors of the search: the first local optimum encountered by the search and the final solution after n bit flips.
"even with curated or automatically generated links between articles and genes, the exact genomic sequences referred to in an article currently can only be determined by human interpretation of the full text. furthermore, specific questions such as 'which transcript was cloned?', 'which exon was amplified?' or 'where in the genome is a particular mutation found?' can take considerable time for an individual researcher to answer, often requiring laborintensive manual interaction between the literature and genomic databases."
"similarly, more payload means that the uav spots incidents quicker on average (it misses them less often on overflight due to the mdr and has less in-flight investigations due to the far). the same argument holds in this case: the heavier uav sometimes returns home earlier and may start a subsequent mission. figure 10 shows that a generally faster uav (higher landing speed) saves more lives. similarly but at a greater rate, a uav with more payload saves more lives. figure 11 shows that having a generally faster uav leads to a reduced total fuel consumption. using a heavier uav with better camera equipment requires a modest amount of additional fuel. a higher landing speed allows the designer to employ smaller wings which save weight and reduce drag during flight. this reduces the specific fuel consumption and explains why higher landing speeds result in less fuel used. simultaneously, higher landing speeds allow the uav to fly faster which results in more incidents being found earlier and alive."
"why is this important? if four clauses share exactly the same variables x1, x2 and x3 then the walsh coefficients w1,2, w2,3, w1,3 and w1,2,3 will capture all of the nonlinear interactions from all four clauses simultaneously using just these four numbers. this, in effect, compresses the representation back to a size that reflects the underlying intermediate form."
"note that if sj (x) is negative, then flipping bit j must yield an improving move. thus, sj(x) can be used as a proxy for f (yj ) because f (x) is constant as j is varied."
"all extracted sequences, blat matches and genome-gene associations generated by text2genome are stored in a mysql database. custom python cgi scripts render data into html pages and act as a light-weight distributed annotation system (das; [cit] ) server, making it possible to overlay the matches onto the ensembl genome browser and provide metadata including links to the corresponding articles. an additional script exports the same data in browser extensible data (bed) format, allowing visualization and filtering of chained blat matches on the ucsc genome browser. extracted sequences, predicted genes, browser tracks and additional metadata, such as gene names recognized in full-text articles by gnat [cit], can be searched, downloaded and browsed at http://www.text2genome.org. source code for the text extraction, mapping and display are available as a set of python 2.4 scripts that can be downloaded from http://sourceforge.net/projects/text2genome/."
"immersing a design into its intended (virtual) environment adds complexity to the design process. even for relatively simple designs, unforeseen outputs can radically change the final product. this paper studies how the use of an operational simulation for the design of a maritime sar uav creates unexpected results. two case studies explaining those counter-intuitive outputs are presented. the rest of the paper is structured as follows: section 2 details the operational simulation developed for decode. it presents agent behaviors, outputs and the specific scenario for the case studies. section 3 shows how results for this research were obtained. section 4 explains two case studies taking into account agent interactions and environmental constraints. subsequently, section 5 discusses the implications of the results for decode and the wider simulation community."
"pmc-oa [cit] . pmcid-pmid associations were obtained from the pmc ftp server (ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/pmc-ids.csv.gz). when available, we used xml files, or the ascii text version of the pmc-oa article based on optical character recognition (ocr) of the original pdf. for pmc-oa articles where neither xml nor text files were available, pdftotext (http:// www.foolabs.com/xpdf/) was used to convert the pdf file to ascii text. additionally, we processed text (or converted text) from supplementary files of the following document types: html, csv, txt, xml, doc, xls, ppt and pdf."
"in decode, the designer creates a uav design and forwards design parameters to the operational simulation. these parameters include payload, landing speed, fuel tank capacity and a number of aerodynamic coefficients to calculate flight performance for that specific design. each design is simulated over the fiveyear period for 150 replications. results are averaged and represented as dots in the surface charts. confidence is consistently above 95%. random variables include:"
the following two sections introduce and explain typical user cases where an operational simulation creates new and unexpected insights into the design. results are based on the scenario setup described above. figure 9 shows the variation in the total number of takeoffs the uav conducts over the five-year period of the simulation. it can be seen that a generally faster uav (higher landing speed) leads to a modest figure 9 : the total number of takeoffs over five years as a function of payload and landing speed.
"ever growing product complexity strains organizational decision-making structures established decades ago. companies still use relatively traditional processes and design tools that are guided by a chief engineer whose experience and intuition lead him to design decisions based on analysis from different disciplines [cit] . however, contemporary product complexity and project scales render this method more and more obsolete and chief engineers are forced to negotiate consensus-driven decisions by large number of specialist teams. decision quality hinges on communication and information clarity. even with ideal information quality, decision trade-offs are often focused on short-term success, neglecting operational information on long-term cost and environmental impact."
"this would be converted to a binary \"parse\" tree (which might double the number of variables). an intermediate form is then produced which \"and's\" together expressions over triples of variables of the form y1 ⇐⇒ y2 ∧ ¬x2."
"a common challenge encountered by many biomedical researchers is to obtain a summary of the relevant literature pertaining to a particular gene or genomic region. [cit] articles added to medline on a daily basis (http://www.nlm.nih.gov/ bsd/index_stats_comp.html), it is increasingly difficult to keep up with the rapid pace of publication outside ones immediate domain of expertise. the problem of finding relevant articles for a particular locus is becoming more acute as researchers increasingly adopt high-throughput genomic technologies (microarrays, genomewide association studies, high-throughput sequencing, etc.). these * to whom correspondence should be addressed."
"during each image capture, a random decision to spot a false incident will be taken based on the far. therefore, a low far leads to quicker spotting of the actual incident as each false positive causes a 2-minute delay for subsequent investigations as outlined above. [cit] and engineering judgment, the far for persons floating in water varies with payload mass as in figure 6 . note that uavs designed by decode will not exceed the payload range between 0.5 and 7 kilograms."
"the case studies in this paper are conducted within a maritime search-and-rescue (sar) scenario. the effectiveness of locating people and vessels out at sea in possibly life-threatening situations can be significantly improved by additional camera vision from the sky and has been identified as a key application area for civil uavs [cit] . practical matters such as certification, safety and camera capabilities will not be discussed here because the research is concerned with proof-of-concept capabilities at this stage. the specific mission scenario for decode is based around the south coast of the uk, as in figure 1 . in reality, there are 11 rescue stations equipped with various types of lifeboats in this area, covering an area of approximately 7800 square kilometers. once an incident is reported to the authorities, they dispatch one or all lifeboats of the nearest station. currently, no uavs are part of the sar-activities in the region. this simulation will model the impact of introducing uavs."
"chip-seq experiments providing information on the binding of transcription factors to thousands of loci can only be properly interpreted when calibrated against positive control data. by intersecting chip-seq regions with genomic regions annotated by text2genome, one can automatically find articles that have previously studied chip-seq regions. [cit] conducted chip-seq against p300, a common cofactor in many transcriptional complexes, with the aim of predicting enhancers in the forebrain, midbrain and hindbrain of mouse embryos. we intersected the 5119 p300-bound fragments in this dataset with mouse text2genome genomic regions, and found a region upstream of gene lmo1 that is bound by p300 [cit] . these authors have shown using chip-pcr that lmo1 is expressed in the mouse forebrain and that its upstream region is bound by the transcription factor arx in neuroblastoma cell lines. [cit] and confirms that this p300 bound region is actively bound by a transcription factor in mouse neuronal cells. to enable streamlined automation of this type of analysis using the ucsc table browser ( [cit] ), we provide text2genome data as bed tracks for selected assemblies in the ucsc genome database (fig. 2) ."
"most acutely, this lack is observed during the early design phase where design decisions weigh most heavily [cit] . early development is limited by budget and time constraints as much as by lacking knowledge about how the product is best designed for its intended market. whereas designers routinely analyze design changes with regards to performance changes, the link to operational variations is not commonly possible. in other words, it is easy to answer \"how much faster can i fly by reducing the wing span by 10%?\" but much more difficult to investigate \"how much money do i save over 5 years operating an aircraft with a 10% smaller wing span\"? an operational simulation can support such queries by linking design quality and performance to operational constraints. one can argue that the latter question is of superior importance because it is meaningful to management and customers. the first question is only important for other engineers. currently, aerospace companies have a good understanding of answering the former question while struggling to even approach the latter question with confidence. these observations led to the work reported here, which is carried out as part of the decode (decision environment for complex design evaluation) research project."
"increase in the number of takeoffs whereas an increase in permissible payload leads to a strong increase in the number of takeoffs. the absolute values indicate that a uav has to takeoff between 2800 and 2950 times over five years in order to support sar around the south coast of the uk (equates to an average of 1.5 take-offs per day). intuitively, an aircraft engineer would expect no influence of the design upon the number of takeoffs. surely, a faster uav would have to fly as many missions as a slower uav? why would a heavier uav with a better camera need to take off more often than a lighter uav?"
"we ran the ubcsat algorithms, using the default parameters, for the same number of bit flips and runs as walshsat-n. the mean and standard deviations of the final solutions are shown in table 5 ."
"the challenge of linking articles to genes is partially solved for a limited number of model organisms, where dedicated teams of curators scan the literature and link publications to gene records in individual model organism databases such as flybase [cit], or through federated multi-organism databases such as entrez gene [cit] . however, these collections are not comprehensive and for the majority of species, including human, efforts to curate gene-article associations remain incomplete. in principle, automatic linking of articles to genes could be achieved by developing text-mining tools that detect gene names or identifiers in abstracts or full-text articles. however, gene names are not consistently used and are often not unique and developing accurate methods to resolve and disambiguate gene names in text and link them to database identifiers remains an active area of research [cit] ."
"repeatmasked genomes and gene transcripts were obtained from ensembl release 56 [cit] and ensemblgenomes release 3 [cit] . the taxa represented in these genomes include 68 animals, 134 bacteria, 10 fungi, 8 plants and and 4 protists, totalling 224 organisms with a ncbi taxon id."
"to obtain the most likely ensembl gene identifier for each genbank record, blat was used to map each genbank sequence to ensembl/ensemblgenomes transcripts, keeping only the best matching gene id. as non-high-throughput divisions still can contain submissions with several thousand sequences, we filtered this set to retain sequences from small-scale analyses only and therefore removed articles that submitted more than 100 sequences. the resulting table contains articles identified by their pubmed id and the genomes and predicted genes that were submitted to genbank, represented by their ncbi taxonomy id and the ensembl gene id (supplementary file 2) ."
"this capability is useful to design and optimize a uav for sar missions. extending the operational simulation will enable designing uavs for other applications such as pipe-line monitoring, volcano ash analysis or forest fire detection."
"where wi is a real-valued weight known as a walsh coefficient and ψi is a walsh function. the index i and vector x can be represented as binary strings, and standard binary operations can be applied. the walsh function"
"the results presented here mean that uav designs can be tailored towards a specific desired operational scenario. for the decode project, this means that it is possible to optimize the design for its intended operational environment to a degree that has not been achieved before. the simulation will be used to analyze a range of design configurations (not just the one used for this study) and their design spaces. it will be possible to choose the best configuration and optimize its design for best overall value."
"here, we address the question of whether annotation of all genomes with sequences from the available open access (oa) biomedical literature is a realizable and practical goal. we show that the automated extraction and mapping of dna sequences from more than 150 000 oa full-text articles in pubmed central (pmc) is indeed possible and present a software implementation to achieve this aim called 'text2genome'. we map extracted sequences to 224 genomes and provide easily searchable results in the form of both a web interface and genome annotation tracks for the ensembl [cit] and ucsc genome browsers [cit] . we demonstrate that we can associate articles with relevant genes and genomes by evaluating text2genome results on the subset of articles that also have genbank records. finally, we provide example use cases to demonstrate potential applications of our approach, by intersecting text2genome mappings with chip-seq data, and by querying for articles that report quantitative reverse transcriptase (rt)-pcr experiments for a given genomic locus. our work provides a unique and timely resource for interpreting both biomedical literature and genomic data, and will help aid discovery across many domains of the life sciences."
"for both text2genome and gnat, system performance is related to the number of predictions made per paper. the effects of multiple predictions are greater for genes relative to species for both systems, and influence precision and recall differentially for text2genome and gnat. the difficulty that both systems have for gene prediction in documents that discuss many genes is consistent with the fact that human annotators do not always agree when asked to curate genes in articles [69-91% depending on the dataset [cit] ]. despite these differences, there is a substantial degree of overlap between gnat and text2genome gene-article mappings for some species such as human, suggesting that future full-text mining systems could fruitfully integrate sequence extraction together with named entity recognition to predict gene-article associations [cit] ."
"(e.g. expressed sequence tag projects) by excluding articles with more than 100 submitted sequences, or (ii) genome-scale sequence contigs, by limiting the length of the genbank sequence to 1 mb. this resulted in a dataset with the species and the best matching gene for 4800 articles based on genbank submissions (supplementary file 2). as with genbank submissions, the number of predicted species and genes can vary for text2genome predictions. by limiting the number of text2genome species or gene predictions for a given article, we observed that all performance measures with the exception of species precision decrease with increasing numbers of predictions per article (table 1 ). the most easily interpretable understanding of the true performance of text2genome can be obtained when documents are limited to those with one predicted species/gene and one reference species/gene. in this case, each false positive prediction creates an associated false negative and precision equals recall, and the accuracy of species prediction is 96%, while the accuracy of gene prediction is 91%. when we allow greater than one prediction per article, recall becomes lower than precision, reflecting the fact that not all sequences in a genbank submission are reported in the full text, confirming the intended role of nucleotide databases as repositories that complement the main publication. at all cutoffs, species predictions are better in terms of precision and recall than gene predictions."
"a wide range of important optimization problems are naturally expressed as k-bounded pbo problems. in computing, this includes hardware verification, combinatorial auctions, design debugging, software testing and graph coloring [cit] as well as classic np-hard problems such as maxsat, vertex cover, maximum cut, and maximum independent set [cit] . in biology, nk-landscapes have been developed as a general model for interacting sets of components (alleles, proteins, amino-acids) with applications in rnafolding [cit] and the study of viruses [cit] . in physics, ising spin glasses correspond to pbo problems [cit] ."
"again, the mdr is assumed to be correlated with payload mass such that a heavy-payload uav spots an incident with a higher probability. a possible model for this is shown in figure 7 ."
"if an element of s does not change, the bit flip cannot yield a new improving move. therefore we can identify any new improving move in o(1) time. for next descent, we simply maintain a list of improving moves. as improving moves are created or destroyed by updates to the vector s, the list is updated as well. then, we can select any improving move randomly from the list."
"furthermore, there is the hidden cost of converting industrial problems, such as the circuit board debugging problems used here, from their original sat formulation to cnf-sat. the walsh coefficients can be more efficiently computed from the intermediate form than from the cnf-sat form, making it unnecessary to convert to cnf-sat. table 5 : means and standard deviations of evaluations of solutions found after n bit flips by several algorithms. our walsh based algorithm (left two columns) was initialized using hyperplane averages (hp) and random solutions (rnd)."
"in the best change to the evaluation, whether it be improving or disimproving. like gsat, our walsh-based next descent will take an improving move if one exists, otherwise it will take an equal move and in the last case it will select a disimproving move. unlike gsat, our algorithm randomly selects from each of these cases and is not guaranteed to always take the best move. irots, or iterated robust tabu search [cit] competition. it will always take the move that satisfies the largest number of unsatisfied clauses with respect to the current solution, given that the move is not tabu. irots also incorporates a perturbation stage if a new best improving move has not been found for some number of bit flips. adaptg2wsat [cit] was chosen as it has been shown to be the best performing algorithm in ubcsat on industrial instances [cit] . it has performed exceptionally well in several sat competitions [cit] . adaptg2wsat works in two phases. the first phase is a steepest descent search that lasts until a local optimum is reached. the second phase uses a heuristic based on the novelty family of algorithms to choose the bit to flip, although the steepest descent search is invoked if improving moves are found."
"only ∼ 1% of all medline articles are available at the moment for full-text mining in the oa section of pmc. if we were to mine the full text and supplementary files of all 16.5 [cit] to the present, we would expect to harvest sequences from ∼ 3 million articles using the text2genome approach. as we work toward this goal, we hope that the results presented here encourage other free-access and subscription-model publishers to permit the extraction and mapping of dna sequences within their articles, to the mutual benefit of researchers, database curators and publishers alike."
"in the general case, the walsh polynomials associated with random boolean functions are exponentially large. however, this ignores the fact that most evaluation functions are decomposable."
"the second case study demonstrated that the uav should have a large payload and a large landing speed. however, this will mean that more uavs crash during landing because their kinetic energy grows linearly with extra weight and exponentially with higher speeds. either the designer chooses to strengthen the undercarriage or a higher number of landing crashes (i.e. new uav purchase) is deemed acceptable. for the case study above, the number of uavs crashed and re-purchased is shown in figure 12 . it is vital figure 12 : the total number of uavs purchased as a function of payload and landing speed."
"to be more concise, assume we just want to convert the boolean expression x2 ∧ ¬x3 to max-3sat and we use the intermediate form x1 ⇐⇒ x2 ∧¬x3 where x1 is an introduced variable. using a truth table [cit] ) to extract the dnf formulas and then applying demorgan's laws to obtain the cnf formulas yields the following literal clauses."
gsat [cit] was chosen as it is a well-studied algorithm for sat and maxsat that is very similar to our walsh-based next descent. the main difference is that gsat always takes the move that results select j with uniform probability from improvingmoves. 8 else 9
where k is a big constant. the total cost of global wires across groups is directly related to the number of groups in which each signal is present and can be estimated as follows:
"the data derived is in the form of plain text, these data must be converted into a sequential database format. the reason behind such a conversion is to facilitate easy and quick retrieval of desired information for the next session of requests through queries. now the cloud server contains virtual machines that are ready to be allocated to client requests based on the provenance data shown in fig.2 . when the request is made, rather than allocating the first available virtual machine, the cloud server refers the information about the request and matches its requirements with the available provenance data and then allocates the most desirable virtual machine, thereby significantly reducing resource fragmentation. one of the main characteristics that require citation is that all these data retrieval processes occurs to and fro the data tier and the cloud server process the request dynamically in the back-end without the client side being acknowledged of such an occurrence."
"the column vectors of u l span the concept space of words and the columns of v l span the concept space of documents. the i th row of x, t i, describes the i th word. consequently, the i th row of u l is the description of the i th word in the concept space with l concepts and we refer to it ast i . in fact, each of the l dimensions in the low dimensional vectort i shows the projection of the words along one of the concepts. it is expected that synonym words are close in the concept space."
"for the synthesis of standard cells, it is often the case that the total number of tracks for active area is defined a priori, whereas the height of the n and p strips can be adaptable as long as the sum of both heights does not exceed the available tracks for diffusion."
"the minimization of global wires can be incorporated in the cost function as a new term weighted with a small constant (γ), that is min : area + γ · costwires."
"currently, fixed coding dominates and streaming servers will offer either a single stream resolution or a selection of predefined resolutions from which to choose, sometimes known as simulcast [cit] . simulcast typically requires excessive storage while using single resolution may result in unsuitable video quality for some devices. more importantly, both approaches lack flexibility of adapting to network conditions. additionally, both approaches may result in inefficient usage of system bandwidth as the number of users increase."
"typically, an svc stream consists of m layers, and to increase resilience to network loss this stream is encoded as an mdc of n descriptions. hence, the mdc representation of each group of frames (gof) can now be viewed as a matrix of m times n sections. in keeping with the relationship between the receipt of an additional mdc description and the incremental increase in stream quality, it is most natural that m and n be equal. hence, each layer i has (n-i) redundant sections. this redundancy translates to an increased transmission cost and a higher consumption of device computation and battery resources. the following subsection explains how sdc can reduce these drawbacks, followed by a four-layer video example."
"the initial design of sdc is such that only one d s shall be transmitted per gof and that the byte size of the d s is comparative to the byte size of the d c . future work will investigate attaining further benefits from other design alternatives for the d s . iii) a redundancy description -d r : the role of this description is to reduce the effects of network loss. the d r is either formed by utilizing fec (to extend the original sections) or by employing network coding (to combine the previously transmitted descriptions), as highlighted later in the paper. it is identical to the d c in that it contains one section from each layer in a gof, but is similar to d s as only one d r can be transmitted per gof. eq (2) defines the byte size and section allocation for a redundancy description."
"the model assumes that transistors with different sizes cannot be chained, 4 as it was shown in fig. 2 . diffusion breaks are required to separate transistors with different sizes and the separation gap is denoted by the constant diffsizegap. sets of transistors with the same size cannot always be chained due to the nonexistence of eulerian paths. in this case, the diffusion breaks may have a different gap, denoted by the constant samesizegap. when using isolation gates, as in fig. 2(a), samesizegap will be 1."
"the milp model generates transistor legs with different sizes. in the case of multiple-height cells, it also assigns the legs to one of the rows of the cell. at an abstract level and using the nomenclature from section v-b, every solution assigns legs to the g r,s graphs of the cell. every graph g r,s represents a group of transistors with the same size."
"with the increasing popularity of computational photography brought by light field [cit] cameras such as lytro [cit] and raytrix, simple and intuitive editing of light field images is becoming a feature of high interest for general users. light field editing can be combined with the traditional refocusing feature, allowing a user to include or remove objects from the scene or change the color, contrast or other features of the scene objects."
"the simulation design implemented a layer id and gof id, such that each datagram is allocated to a specific layer/description in a specific gof. this sign allows the simulation to track the datagrams between server and client. datagrams arrive at the client in the order they are sent from the server but if datagrams arrived out of order, then the gof id and layer id are utilized to determine what is decodable by the client."
"extensive simulations are run to determine the mean percentage value for viewable layers in svc, mdc, sdc and sdc-nc. to coincide with the length of the yuv media streams, a stream size of 300 frames was chosen for both the four and six layer simulations, thus creating an averaged trace file for each streaming model, which is then analyzed to determine the highest viewable layer per frame, as a valuable metric for assessing qop."
"an example is shown in fig. 3, where the pull-down netlist of a nand3 gate is depicted and each transistor has two legs. by enforcing the legs of the same transistor to be contiguous, a chain such as the one shown in the top can be obtained. this chain has a diffusion break since no eulerian path can be found. 2 however, no diffusion break is necessary if some of the transistors are allowed to have separated legs, as shown in the chain at the bottom."
"a non-eulerian graph can become eulerian by adding extra edges. this process is called eulerization. eulerizing a connected graph with a minimum number of edges is simple: it is sufficient to add edges between pairs of odd nodes until all nodes become even. in case of semi-eulerization, one less edge is to be added so that two odd nodes may still remain. for graphs describing transistor netlists, semieulerization represents the process of adding diffusion breaks in the transistor chains."
"an important feature of the approach is that the algorithm does not even deliver any specific transistor ordering. instead, it generates a netlist for which an area-optimal transistor ordering is guaranteed to exist. in general, different solutions with the same area may exist, thus giving the opportunity for transistor placement tools to explore the best one in terms of routability."
"communication in service meshes generates a lot of traffic and data that needs to be controlled and captured e.g. to distribute access control and usage policies, and observe and collect telemetry, traces and metrics. the traffic control decision is usually made for the whole service mesh as illustrated in figure 3 . there are four traffic control options:"
"due to the inter-dependence of layers/descriptions in adaptive streaming, it is of benefit to the receiver to ascertain without delay which specific descriptions have been received during transmission. hence, the device can infer what has been lost and what is ultimately required to maximize stream quality -all remaining descriptions or just a subset. sdc is ideally positioned to benefit from this type of inference, as its architecture consists of a combination of prioritized (d s and d r−nc ), equal importance (d c ) and redundancy (d r ) descriptions, and it offers the potential of maximum stream quality with only a subset of descriptions. with this benefit in mind, sdc can be further improved by optimizing the transmission of its descriptions."
"as it was mentioned in the previous section, the milp model may deliver a solution in which the area cost given by (6) does not coincide with the real cost of the solution when considering all diffusion breaks."
"let us introduce two new variables, s n and s p, that represent the maximum size of n and p transistors, respectively. since we have a fixed number of tracks for active area, we add a constraint on the total number of tracks used by the diffusions, that is"
"multiple-height cells are typically organized by interleaving n and p rows in such a way that the vdd and gnd rails can be shared internally. for example, a triple-height cell can be laid out with adjacent n and p rows organized as follows: nppnnp. fig. 1(b) and (c) depicts double-height cells with the structure nppn."
"in addition to handling cross-service communication, service meshes often intercept incoming traffic, usually called ingress traffic. the decision for managed ingress communication is usually made for the service mesh as a whole. the ingress traffic then needs to be routed to the containers orchestrated in the mesh. of course, we might choose not to manage ingress communication but this is a risky and dangerous option since it might expose the service mesh to malicious or overloaded traffic. this option may be adopted in case of a private service mesh, but such meshes seem to be very rare. the typical design option chosen is a front proxy which is used by the control plane to intercept ingress traffic as shown in figure 2 . an api gateway [cit], a common microservice pattern with the goal to provide a common api for a number of services, can be"
"in the example, the maximum size for p and n devices is four and three tracks, respectively. double-height cells can also be designed as shown in fig. 1(b) . the two p strips can potentially be merged to extend the maximum size of the p devices, as shown in fig. 1(c) . hybrid approaches having segments with two p strips and segments with one merged strip are also possible in double-height cells. the three layouts shown in fig. 1 are area-optimal in each category (single height, double height, and double height with p-diffusion merging)."
"1) the spice netlists have been parsed and functionally equivalent transistors have been merged (unfolded) into one larger transistor with a size equivalent to the sum of sizes of the original transistors. 2) a horizontal pitch p of 130 nm has been defined for each track of active area. with this pitch, most transistors in the small cells end up by taking 5 p tracks and 3 n tracks. 7 the minimum and maximum number of tracks for each transistor has been calculated as follows:"
"would be forced to take the value 1. since usesize(s) will be a variable minimized by the cost function, its value will be 0 when no transistor uses any leg of size s."
"an overview of our method is illustrated in fig. 1 . note that the epis in a light field connect together the subaperture views from the same row (or column) of the light field, accounting for only horizontal (or only vertical) disparities. our proposed epipolar plane diffusion allows to propagate an editing from the center view of a light field row (or column), to the other views of same row (or column). there are different possibilities for the propagation order, here we arbitrarily first compute a propagation through the central column of the light field, and in the sequence we compute a row-by-row tensor driven diffusion."
"communication in service meshes usually uses encryption based generated keys and certificates; if not used, the service mesh might be exposed to malicious traffic and manipulations, unless a key and certificate management service outside of the service mesh can or must be used. a simple option is using api keys [cit] and local key management. the alternative is to introduce a central certificate authority, residing in the control plane, that takes care of storing and distributing security keys and certificates to the data plane. this option is more secure than the other options and creates in large installations less maintenance overhead for managing various api keys in the clients and service proxies, but it is also more complex than e.g. api keys. once authentication is handled, authorization needs to be considered. this can be achieved by setting up access control in the control plane or in the data plane. if we choose not to control access after authentication, then services are exposed to unintentional and unwanted modifications. security is the most important driver in this decision; a solution on the data plane supports more fine-grained control but is more complex than a solution on the control plane. using encryption in service meshes, usually based on mutual tls, has to be handled at both ends; not using encryption means security is endangered. there are three decision options for tls termination: either we offer tls termination directly in the service, at the front proxy, or -the most common option -in the data plane. the first option brings boilerplate code to the service which might also decrease its performance. the second option is only viable if the service mesh is in a private environment in which internal unencrypted communication is an option (or another encryption than the one used for communication with clients)."
"to scale and achieve redundancy, service meshes can be expanded and form multi-clustered service meshes, leading to the selection of the option multicluster support in the decision illustrated in figure 4 ."
"given the gaps required to separate transistors with different sizes, area-optimal layouts have a tendency to group (and chain) transistors with the same size in the same active area. let us call local wires the ones used to connect terminals of the same signal within the same group of transistors represented by graph g r,s ."
"1. the user sends the request to the cloud controller/cloud broker. 2. cloud controller/cloud broker process the user request. 3. all the requests are arranged, that is deciding which request to be processed first by using heuristic scheduling algorithm. 4. initially place the virtual machine based on the size or beyond the size whichever is near requested by the user. 5. according to the provenance data, cloud controller/cloud broker refers the data from global provenance data base and selects the most suitable virtual machine. 6. now, already placed request is reconfigured to the newly found most suitable virtual machine using best fit algorithm. 7. while reconfiguration takes place, vm migration algorithm migrates vm in the most suitable server for processing."
"digital object identifier 10.1109/tcad.2013.2269680 metal layers routed on gridded layouts [cit] . in this context, cell design is a problem that is moved from the continuous domain (any type of shape, any location) to the discrete domain (only rectangular shapes on a coarse grid with fixed pitch). thus, cell synthesis becomes a combinatorial problem in which eda algorithms can do a much better job than manual design. area is a critical resource that still needs to be minimized for cost-efficient manufacturability. the height of a cell depends on the number of tracks used for the active area, whereas the width is determined by the number of devices and the diffusion breaks inserted to isolate transistor chains."
"psnr [cit], a widely used pixel-difference based objective metric, is utilized to assess the quality of yuv video streams and our results reference the y-psnr luminance values. the steps that we adhere to for h.264 psnr evaluation are as follows. first, we create a multilayered h.264 video using jsvm v9.19 [cit] . in our evaluation, we create crew.264 from the raw 4cif 30fps yuv data stream crew.yuv, which consists of a crew of astronauts waving while walking down a corridor, obtained from the leibniz universität hannover video library [cit] . next to determine the per frame svc layer value, the transmission of this video over lossy links is simulated using ns-2."
"assuming that every graph g s is connected, 6 euler's theory provides the number of edges (semi-eulerization cost) that need to be added to create an eulerian path (see theorem 2)"
"we perform experiments on the kth action database [cit] and the fifteen scene database [cit] . kth action database is one of the largest and most challenging datasets for human action recognition. it consists of 6 actions -boxing, hand clapping, hand waving, jogging, running and walking -performed by 25 subjects under 4 different scenarios -outdoors, outdoors with scale changing, outdoors with different cloth and indoors with lighting variations. there are a total of almost 1200 video clips in this database. we use the leave-one-out cross validation technique to test the performance, i.e., each time we train with videos of 24 persons and use the videos pertaining to the remaining person for testing and report the average of the recognition results."
"provenance virtual machine placement algorithm uses initial resource allocation scheme and virtual machine migration algorithm while the client is utilizing the resource. for example, when the cloud controller or cloud broker receives the request, it has to find the virtual machine and place the request. but the virtual machine is oversized than the request. cloud broker then refers the global provenance database, request requires a less amount of resource than it has currently been allocated to, the server immediately detects the suitable virtual machine and migrate virtual machine to another virtual machine that will suit the requirements. such migration occurs without the knowledge of the customer side. this concept is known as dynamic reconfiguration. the"
"proof-of-concept implementation our proof-of-concept implementation is based on our existing modeling tool implementation codeablemodels 1, a python implementation for precisely specifying meta-models, models, and model instances in code with an intuitive and lightweight interface. we implemented all models described in this paper together with automated constraint checkers and plantuml code generators to generate graphical visualizations of all meta-models and models."
"if a signal is present in k transistor groups, there will be at least k − 1 global wires across these groups, i.e., the number of edges of the spanning tree connecting the groups."
"it is a data base used to collect event log of the each physical server at particular period once. then, that information will be transmitted to the global provenance database."
"since epis are piecewise smooth and have no complex texture content, tensor driven diffusion is naturally suited for inpainting the epis as an efficient technique to obtain a coherent edit propagation."
"the method consists in calculating a hash value of the solution and eliminating all solutions that have the same hash value. the hash function is calculated as a linear combination of the nonzero variables of the solution using a set of coefficients, that is"
"to incorporate the double-height diffusions, the milp model must be slightly modified. a similar approach as the one presented in section v-b for multiple-height cells must be used. however, adjacent rows are not totally independent since they can allocate large transistors. in the model, we will assume we have two single-height rows that we will represent as (top) and ⊥ (bottom). we will also use the symbol ⊥ to denote the double-height row that can be used by merging the top and bottom rows."
"this paper presented scalable description coding (sdc), a novel approach for the transmission of redefined mdc-encoded video. sdc shows a noticeable superiority in comparison to mdc in terms of bandwidth requirements, without sacrificing user-perceived quality. we believe that sdc is especially significant for mobile networks where bandwidth over-the-air and in the backhaul continue to be insufficient to satisfy the growing demand of video applications. sdc yields upwards of 25% bandwidth savings over mdc and also delivers higher quality streams for longer periods of time. future work includes the comparative evaluation of sdc in a real media testbed, as well as determining the benefits offered by sdc to other versions of mdc [cit] ."
the following constraints are defined to calculate the length of the transistor chains after folding based on the existence of eulerian paths and the eulerization cost.
"the main functionality that has to be performed under such a scheme is to maintain historical data of each virtual machine and retrieve the respective data as and when further requests are made. historical data is generated by the cloud server by continuous monitoring of the cloud environment. whenever a request is made, the server must derive resource usage information such as time, load etc, and maintain a data tier which stores the derived data."
"to avoid an excessive computational cost, a maximum number of iterations is allowed (line 10). if this number is exceeded, the returned solution cannot be guaranteed to be optimal."
1) all the variables of the model (see table ii 3) the rest of constraints (2)-(5) must be instantiated for each row of the layout.
"recolorization consists in modifying the color palette of an image, either manually or based on color transfer algorithms which rely on example images. here we are interested in local recolorization, where color editing occurs inside a mask ω m ."
"while we believe generalizability of our results beyond the knowledge sources we have studied is possible to a large extent, our results are limited to those sources and to a lesser extent to very similar service mesh architectures. most of the sources were public web sources; there might be inhouse practices not reported to the public by practitioners not covered here. some of the sources were from the technology vendors, which might have introduced bias; but this is mitigated to a certain extent as we considered sources from most major service mesh vendors. our results are only valid in our set scope; we do not claim any form of completeness. possible misinterpretations or biases of the author team cannot be fully excluded and might have influenced our results. we aimed to mitigate this threat by our own in-depth experience and by carefully crosschecking among the sources in many iterations."
"we propose a new method to eliminate a solution that only requires one new binary variable and two constraints. however, the method may also eliminate other optimal solutions, but hopefully with very low probability."
"transistor folding is one of the subproblems in the design flow of standard cells. 1-d gdrs enforce active areas to be rectangular, thus reducing the chances to find area-efficient transistor chains for netlists with multiple transistor sizes. this constraint originates a new formulation of the folding problem that can be efficiently solved algorithmically."
"the remainder of the paper is organized as follows. section ii gives background information on svc, mdc and existing research on mdc optimization. section iii presents sdc, followed by two optimization mechanisms in section iv. section v is dedicated to evaluation while section vi concludes the paper."
the second conclusion is that the balanced heuristic is superior to the greedy heuristic. the main reason is because fig. 9 . symbolic layouts for cell sdffr x1 obtained from the netlists generated by the balanced (top) and optimal (bottom) methods.
"minimum-area cells are synthesized by finding good transistor orderings that allow to maximize diffusion sharing. the algorithms proposed to find these orderings are tightly related to the theory of finding eulerian paths in undirected graphs. several theoretical results and algorithms have been proposed to find optimal transistor orderings, either considering fixed transistor netlists [cit] or allowing transistor reordering of series-parallel graphs while preserving the functionality of the cells [cit] . large transistors may exceed the maximum allowable size in a standard cell. this problem is solved by breaking large transistors into smaller ones (legs). for example, a transistor that needs seven tracks of active area may be implemented with three legs of 3+2+2 or 3+3+1 tracks. the strategy of creating multiple legs of the same transistor is called transistor folding. fig. 1 depicts the feol layers of three different implementations of an and2 gate using multiple legs to implement large devices. table i reports the characteristics of the devices. the second column specifies an interval of sizes (tracks) allowed for each device. for example, device p1 can have a size between eight and 10 tracks."
"finally, the algorithm can also incorporate terms in the cost function that, still guaranteeing area optimality, can deliver solutions that have better routability properties. this paper is organized as follows. section ii presents a graph model for the problem and reviews the relevant euler's graph theory. section iii proposes the mixed-integer linear programming (milp) formulation of the problem. a strategy to generate multiple solutions is discussed in section iv. several extensions of the model are presented in section v. a strategy to deliver routability-aware solutions is proposed in section vi. section vii describes two heuristics that are compared with the milp model. finally, the impact of the proposed methods on area and routability is evaluated in section viii. section ix concludes this paper."
"to manage cross-service and ingress communication, the control plane and data plane need to know where each packet should be headed to or routed; routing is usually configured on the control plane and enacted by the proxies on the data plane. in addition to such basic routing, the service mesh often offers custom routing options which can be based on url path, host header, api version or other application-level rules for control over the routing in the mesh. such routing rules can be dynamically changed, increasing the flexibility of the architecture. custom routing can in follow-on decisions be used for extra tasks, a prominent one is to support continuous experimentation techniques such as staged rollouts, a/b testing and canary deployment (or not). the latter can help for more controlled deployments to production, which helps to minimize deployment risks."
"all the previous approaches work with the assumption that differently sized transistors can share diffusions, i.e., a 2-d design style. additionally, the methods are restricted to the placement of pairs of p and n transistors that must be aligned vertically to share the same polysilicon stick. the placement of transistor pairs also involves some area overhead (see, e.g., [6, sec."
"several works in the literature aims to reduce the data redundancy in mdc. one approach is to remove the bl from the mdc description and place it in its own description [cit] . other ideas propose to separate the bl to generate two descriptions [cit] or reduce the number of higher layers to suit the requesting devices [cit] . in this paper, we focus on reducing the higher layer replication which in turn reduces the number of descriptions required and this reduction offers greater opportunity for better use of available bandwidth."
"the previous model can be extended for multiple-height cells. a common case is the synthesis of double-height cells, as shown in the example of fig. 1(b) . as for the synthesis of single-height cells, the problem can be solved independently for p and n devices."
"if a signal belongs to various g r,s, there will be wires across different transistors groups. let us call them global wires. this is illustrated in fig. 8, where the shadowed boxes represent active areas allocating transistors with the same size, i.e., active area corresponds to a graph g r,s . in the picture, the p and n transistors are allocated in the p and n rows, respectively. the numbers on top of the picture represent transistor sizes (e.g., number of tracks). the dots represents the terminals of one signal and the dashed and solid lines represent local and global wires for that signal."
"the balanced heuristic tries to minimize the number of different sizes used for each transistor. this tends to reduce the costly diffusion gaps between differently sized transistors. another reason is that it also tries to generate an odd number of instances of each transistor, thus preserving the evenness of the degree of the nodes in the transistor graph."
"as an example, the case [cit] could have been implemented with three different balanced distributions: 1) 4 + 4 + 3+3+3; 2) 4+4+4+3+3; or 3) 4+4+4+4+3. the distribution 4 + 4 + 4 + 3 + 3 is preferred to preserve the evenness of legs with maximum size."
the milp proposed in section iii can solve the folding problem for single-height cells. the model can be solved independently for the p and n devices and the width of the cell will be determined by the maximum width of the two rows. it is also easy to formulate an integrated model to solve both problems simultaneously.
"multiple-height cells are usually laid out to improve the routability of complex cells. by having a more balanced aspect ratio, congested channels of signals that go across long cells are avoided."
"in heterogeneous environments like service meshes, multi-protocol support is required. it helps to have a unified api interface that can be used by services using different protocols, which increases interoperability and extensibility of the service mesh. this can be offered by data plane proxies or on the front proxy, where the latter option offers less fine-grained support and is suitable if the mesh uses only one protocol inside. of course, we might choose not to use this api interface and relieve the service mesh from the resulting processing overhead. then, we need to add boilerplate code to services to support different protocols or suffer from interoperability issues."
"finally, the reader may realize that the previous model can be easily generalized to accept rows with a different number of tracks of active area."
"the light field inpainting application is particularly challenging, as the disparities between the views are not determined for the pixel coordinates which are lying on the inpainting mask ω m . it follows that the disparity map should be inpainted for a correct propagation of the inpainted region from the central view to the peripheric views."
"for evaluating the provenance based scheduling algorithm, the cloud infrastructure was utilized. these experiments have been carried using cloud simulator. the provenance based vm placement algorithm is simulated with 3 datacenters and 14 resource request. the parameter has to be set for cloudlet, virtual machine and data center with some metrics."
"a simple approach for editing a light field image can be obtained with an edit propagation, where first a particular subaperture view is edited (most likely the center one) and then a coherent propagation of this edit is performed through the other views [cit] . this problem is particularly challenging for the task of inpainting, as the disparity field is unknown under the occludding mask."
"we also adopt a basic error concealment strategy based on frame duplication to compensate for non-decodable frames. a more advanced error concealment could be used to increase the psnr values for the decoded streams, but is outside the scope of this paper."
"by employing idd-based sdc (i-sdc), and by implementing the transmission schedule is as outlined in section iii, the device is able to disregard the final description should all previous description be received. table iii shows that i-sdc would attain significant processing savings over sdc, a 35% reduction, by eliminating the need for decoding the redundant sdc description, d r . as can be seen, the current implementation of i-sdc while decreasing resource usage on the device, is not reducing network consumption, as the final description is only being dropped after being received at the device. future work will consider idd transmission options to reduce network costs. it is important to note that while sdc-nc and i-sdc are separate optimization techniques, they combine quite naturally, to further increase sdc performance."
"where s is a constant that now represents the maximum number of tracks for diffusions. we now have to make sure that no transistor exceeds the maximum allowable size. for that, we can add new constraints on the variables that represent the usage of each size"
"the synthesis of multiple-height cells with double-height diffusions can be extended to incorporate an adaptable number of tracks, as was discussed in section v-a. for example, for a double-height cell, the constraint (7) could be extended as follows: where s is the maximum total height of the cell. additional constraints on the usesize variables should be included in a similar way, as was discussed in section v-a. this scheme can be easily extended for any arbitrary number of rows."
"the physical server is logically divided into multiple logical machines using the concept of virtualization. it shares the physical components such as os, user interface, memory, apps, etc., from the physical server."
"microservices are a recent approach for designing service architectures that evolved from established practices in service-oriented architectures [cit] . as microservices, especially in large-scale systems, introduce many challenges and high complexity in terms of manageability and interoperability, service meshes [cit] have been introduced as an infrastructure for managing the communication of containerized microservices and perform many related tasks. for this, they usually use a network of lightweight proxies or sidecars that handle all the communication burden [cit] . as a result, the coupling between microservices and of microservices to the infrastructure services can get drastically reduced. this also eases establishing interoperability between microservices developed in different programming languages and with different technologies. the proxies or sidecars form a data plane that is typically managed by a control plane [cit] ."
"a second wave of inpainting methods appeared after the work of criminisi et. al [cit], which was influenced by patch-based texture synthesis [cit] . patch-based approaches are able to inpaint textures but can fail to correctly reconstruct geometry, which motivated some works to combine patch-based and pde-based inpainting [cit] ."
"vm placement is mapping of virtual machine to physical machine. this can automatically happen based on on-demand requirements. in usual scenarios, whenever a request is made for a resource in a cloud computing server, the server performs allocation based on fcfs scheme, i.e., the first detected space that is sufficient for the request, is allocated to the request irrespective of its size. this scheme generates a large amount of fragmentation, which means that, there exists a large amount of unused resource for each request. one way of overcoming such a drawback is to use provenance based virtual machine placement which keeps track of the amount of resource that has to be allocated for a particular request, based on historical data of the virtual machine that is making the request. therefore, under such virtual machine placement scheme, whenever a request is made by a user or client to the cloud server for resources, allocation is performed based on the virtual machine's historical data and not on fcfs scheme but on the virtual machine space which is perfectly suitable for the request."
t he scaling of transistor dimensions and the manufacturing challenges involved in the subwavelength optical lithography impose severe constraints on the layout patterns that can be reliably printed on the wafers.
transistor folding is a combinatorial problem that cannot be simply reduced to minimizing the total number of devices of the cell. the existence of eulerian paths in the transistor strips and the diffusion breaks required to isolate blocks with different size are crucial in defining the transistor folding strategy for each cell.
"cloud computing has some imperative research issues related to resource management, performance, data, bandwidth and security. this proposed graft focus primarily on resource management. resource management itself has significant problems that need to be resolved. resource allocation is one of the research challenges in source management. solving problems related to vm placement, vm consolidation and load balancing will result in the solution for resource allocation. this paper proposes the technique for solving the vm placement problem. the fig. 1 explains the general architecture of the cloud infrastructure. to support it industries for managing scientific resources and to save the energy and cost, mapping the virtual machine to physical machine is required. selecting a proper host and placing it in the correct virtual machine is virtual machine placement problem. for mapping virtual machine to physical machine, the suitable algorithm is required."
"load balancing decision service meshes, especially at scale, have to handle tremendous traffic loads which might overload services, increase their latency and decrease their availability. in order to avoid such a situation and maintain scalability, services are replicated and load is distributed over these instances by both the control plane and data plane using a load balancing algorithm. load balancing can also be based on geographical location, especially in the case of service mesh expansion described in section 4.4. if load balancing is used, the typical option is load balancing on the service proxies. an alternative which offers balancing loads for the whole ingress traffic is load balancing on the front proxy; this option offers less fine grained control over the load balancing than e.g. to balance per service cluster. both solutions can also be combined, offering the benefits of both solutions but also increasing the complexity."
"generally, virtual machine placement are of two types: they are initial virtual machine placement (static) [cit] and dynamic virtual machine placement. allocating the vm according to the number of user jobs to available physical resources is a big predicament. virtual machine that are freed or de-allocated because of their completion of tasks or if their lifetime has expired or if they are not completely used by previously allocated vms are allocated for new requests. below objectives are considered for performance improvement and evaluation. they are resource wastage, utilization ratio of resource, total completion time, reliability, power consumption and revenue acquisition. according to the number of objectives considered for performance evaluation, algorithm is classified into single objective algorithm and multi-objective algorithm."
"in this paper, we present scalable description coding (sdc), a technique that enhances mdc-fec, henceforth referred to as mdc, with a novel transmission scheme to achieve lower data rates without sacrificing user-perceived quality. sdc operates by redefining the mdc description prior to transmission, to reduce the required bandwidth. compared to mdc, sdc improves the user-perceived quality with lower bandwidth usage levels, while offering increased robustness against packet loss. our analysis quantifies the data rate reductions, showing that in some instances the sdc data rates are on par with those of svc. furthermore, we propose several optimizations to sdc, including sdc with network coding [cit], that further improves sdc performance."
"it is important to clarify that once the highest layer (e.g. layer six) is decodable this does not mandate the client to view the highest layer but permits the client to view any layer between the highest sdc decodable layer (layer six) and penultimate decodable layer (layer three), i.e the client can view any layer between four and six."
"within each group, the number of allocated slots is equal to the number of transistors plus the semi-eulerization cost (corollary 1). the diffusion breaks for equally sized transistors have a cost of samesizegap slots (typically one slot when using isolation gates)."
"this paper presents an exact algorithm that guarantees an area-minimal layout for the transistor folding problem considering different layout parameters: single-and multipleheight cells, parametrized diffusion breaks, flexible transistor sizes, and adaptable diffusion tracks."
"graphs with multiple connected components are not eulerian. in general, each graph can have eulerian connected components (eccs) and non-eulerian connected components (nccs) depending on the property of individually being eulerian. the eulerization cost of a graph with multiple components must also account for the cost of connecting the graph."
"the second goal aims at preserving the evenness of the nodes in the graphs. by folding one transistor into an odd number of legs with the same size, the evenness of the degree of the source/drain nodes is not modified. bearing in mind that the original transistor netlists have a tendency to have good eulerian properties, this approach contributes to maintain them."
"the last column of table vii reports the cpu time required to deliver the optimal solution when including wire optimization, which is the most complex model instance of the problem. on average, each instance took about 4 s. however, the worst case was observed for cell sdffrs x1 (164.91 s)."
"where ecc(g) is the number of eccs of g. 5 the max operator is required to prevent a negative cost in case the graph is eulerian. corollary 1: let g be a non-eulerian graph. the semieulerization cost of g is the graph in fig. 5 (b) has four connected components and six odd nodes. however, two of the connected components are eulerian (second and fourth from the left). in this case, extra edges must be used as bridges to connect these components."
this section analyzes the area cost for the adoption of a gridded 1-d style with regard to the usage of a gridded 2-d style. the only difference between both styles is the area cost of the diffusion breaks between differently sized transistors.
"one benefit from this manner of d s creation, is that the value of the layer, with only one section added to the d s, is the combined number of d c and d r required by sdc. thus allowing us to rewrite eq (1) as eq (4) ."
the first conclusion is that the strategy used for transistor folding can have a significant impact in area. the balanced and greedy heuristics are local strategies that lack a global view of the graph in terms of diffusion chains between different transistors.
"an important aspect to evaluate is the computational complexity of this problem. milp is np-hard, but the instances of the problem evaluated in this paper can be solved in affordable cpu times."
"this section describes various experiments performed to evaluate the milp model and heuristics presented in this paper. the experimental setup is first described and the results are later reported. finally, the impact in area, routability, and computational complexity are discussed."
this section presents two heuristics for transistor folding as an alternative to the milp model. the comparison of the results delivered by the milp model and the two heuristics will contribute to emphasize the importance of a good transistor folding.
"the main objective is to minimize the total energy consumption and reduce resource wastage [cit] by running of pms, which indicates the resource utilization and reduce expenditure. according to the multiple dimensionalities of physical resources, there exists wastage of resources, which results in the use of multidimensional resources from the imbalanced condition. the characterization of the multi-dimensional resource usage states of pms presented a multi-dimensional space partition model and also gave a virtual machine placement algorithm eagle, which can balance the multidimensional resources utility, decrease the number of running pms, and thus lower the energy consumption."
"we have performed in this paper a qualitative study in which we have studied service mesh established practices and proposed a formally defined add model. in total based on our findings, we modeled 14 architectural design decisions with 47 decision outcomes and 77 decision drivers. in our uncertainty reduction estimations we were able to indicate that the knowledge organization in our add model can lead to a significant reduction of uncertainty. we plan in our future work to combine our add model with other aspects of microservice design and devops practices, and empirically validate a theory based on the preliminary uncertainty reduction estimations. we also plan to validate our add model using real life case studies with field practitioners."
"for every size s, the existence of an eulerian path in g s and the cost of eulerizing g s can be calculated by knowing the number of odd nodes. for this, we introduce two sets of variables, i(n, s) (integer) and o(n, s) (binary), to calculate the parity of the degree of each node. thus"
"in order to communicate in a service mesh, services need to locate others based on information like ip address and port number. of course, this might be simply hard-coded in each of these services. if a service changes its address, fails or is unavailable for other reasons like congestion, then it becomes not reachable anymore. then, there is a huge problem since all services code needs to be changed and the mesh needs to be restarted which impacts negatively availability. to resolve this issue, the control plane and data plane use service discovery system usually provided by platforms like kubernetes for example. an alternative is using a central lookup service [cit] . the distributed service discovery option requires a consistency protocol and caching of discovery information, i.e. it is more complex. however, the lookup is local, thus it offers better performance. without service discovery, the manageability, changeabiltiy, and evolvability of the service mesh would severely suffer."
"svc separates each stream into a base layer (bl) and one or more enhancement layers (el) as illustrated in figure 1a . the bl represents a low resolution version of the stream, normally produced from the key frames, while the els contain higher quality information. by receiving the bl and a relevant el, a higher quality stream resolution can be decoded. a major limitation in svc is that a higher layer can only be decoded when all lower layers on which it depends are received. it may be the case that the higher layers depend on a subset of the lower layers according to the dimensions being scaled -scaling could be temporal (frame rate), spatial (image dimensions) or quality (signal to noise ratio) [cit] . note that the bl is always required for svc media decoding. mdc creates n independent descriptions representing stream vertical slices of equal importance using layer partitioning and fec, as illustrated in figure 1b [cit] . layer partitioning divides each svc enhancement layer into a number of equal sections, based on its layer order value, e.g., layer two is subdivided into two equal byte sized sections. these sections are then extended across four descriptions using fec such that receiving any two descriptions would allow for correct decoding of layer two. note that each description contains a copy of the bl, such that the reception of any description would enable decoding of the bl. more generally, m descriptions are required for decoding the m th layer. in figure 1b, the minimum number of sections required to decode and view all streams is shown in dark shade, with the redundancy shown in light shade. clearly, the quantity of redundant data in mdc is inversely proportional to the level of the layer in svc, with the bl containing the highest overhead."
"an undirected graph has an eulerian path if and only if at most two nodes have odd degree, and if all of its nodes with nonzero degree belong to a single connected component. if there are two nodes with odd degree, these nodes must be the endpoints of any eulerian path."
"the bag-of-words(bow) framework has been shown to be useful in various computer vision applications like object recognition [cit], scene recognition [cit] and action recognition [cit] . the framework builds a visual vocabulary by vector quantization of raw features extracted from an image or video. the vector quantization essentially involves clustering of the raw features by k-means and choosing a cluster's mean as the codebook or visual word. an unknown image or video is classified by a suitable classifier, according to its histogram of visual words. it is well known that the choice of the number of clusters in k-means clustering determines the discriminative ability of the visual vocabulary that is generated [cit] . however, the more important drawback of k-means clustering is that it is based on the appearance of the image or video as represented in the raw features, as opposed to being based on the semantic relations between features. utilizing the semantics inherent in visual content improves image/video categorization and understanding."
cloud broker is a third party who supports the cloud users for their business needs. it is an intermediary between users and available cloud services.
"in addition to studying and answering these research questions, we have estimated the decision making uncertainty in the resulting add design space, calculated the uncertainty left after applying the guidance of our add model, and compared the two. our model shows a potential to substantially reduce the uncertainty not only by documenting established practices, but also by organizing the knowledge in a model."
we can observe that the ⊥ variables representing the legs and breaks of the double-height diffusions equally contribute to the area of the top and bottom rows.
"in this section, we discuss how our method can be used for light field editing applications, in particular recolorization and inpainting. both applications are based on the idea of first editing the central light field view, either manually or using available algorithms and then propagating this edit to the remaining light field views. we consider a local editing, where a specific region of the central light field view is edited, this region is given by a mask ω m ⊂ ω."
"as shown in fig. 1(c), adjacent rows with the same polarity can be extended and merged to allocate larger legs. in this example, the active area for p transistors occupies four tracks. however, by merging two adjacent p rows, transistors up to ten tracks can be implemented. fig. 7 shows a possible structure for the active area of standard cells with double-height diffusions. the layout is organized by putting all double-height blocks of transistors at the left of the cell and the single-height blocks at the right. as in the basic layout model, differently sized blocks will be separated by diffsizegap tracks. this structure guarantees area optimality but does not prevent the placement tools to find another ordering with better routability."
"as explained above, the control plane and data plane provide numerous central services and proxy tasks, and many of those are achieved jointly. the decisions on central services and proxy tasks are usually made for the service mesh itself but can in many cases be changed for individual services or service clusters from in addition, we have found evidence for decisions that are needed for the basic functioning of the service mesh such as policy distribution, which we have not included in our catalog, as the user does not have to make a decision about them. based on the services and tasks listed below we found evidence for many possible follow-on decisions such as support for rate limits, quotas, circuit breaking, retries, timeouts, fault injection, dashboards, analytics, and so on. we did not include those in our add model either, as the possible list of such higher-level services is excessive and will likely grow over time."
"the same is true in principle for the decisions made without our add model, but as the decision d is here split into multiple separate decision nodes ndec d and without the add model no information on which combinations are possible is present, we need to consider any possible combination in ndec d, i.e., the size of the powerset of the decision nodes: table 2 shows the results of the uncertainty reduction estimation. it can be seen that the number of decisions to be considered ndec can be in total reduced from 33 to 14, with an average improvement of 49.76% when using our add model. as all decisions have multiple criteria and when not using our add model no decision are pre-decided, the improvement for criteria assessments is higher: on average a 86.70% improvement is possible. finally, the possible decision outcomes is improved from 96 to 47, with an average 32.59% improvement."
"in contrast with generative methods that do not make use of category labels, our method trains a classifier using the histograms from the training set. moreover, in our method the number of topics can be changed as opposed to the unsupervised framework where this number is fixed to be the number of classes. this will allow us to analyse the semantic relations in more detail and consider as much topics as appropriate. on the other hand, plsa is able to handle polysemy which is so effective in cases when different categories share the same topics e.g. walking, jogging and running, all have similar movements of the legs. we have tested our proposed method on the kth human action database [cit] and also on the fifteen scene dataset [cit] with promising results. into concept space. we first extract features from patches (cuboids) in the images (videos). the initial vocabulary is constructed by performing k-means clustering on the extracted features and choosing the cluster centers as the codewords. the feature vectors are quantized based on the initial codebook to form the word-image (or video) matrix which describes the occurrences of words in images/videos. the codewords are then embedded into the concept space by latent semantic models -we demonstrate the embedding both by lsa as well as by plsa. finally, the embedded codewords in the concept space are again clustered using k-means to obtain the desired semantic visual vocabulary."
"the discrepancy is originated by the difference of the semi-eulerization cost for connected and disconnected graphs, formally modeled by the difference between theorem 2 and corollary 1. more precisely, the milp model does not take into account the number of eccs of each graph g s ."
we propose to enhance the cost function of the milp model with a term that aims at minimizing the local and global wiring cost in the layout. the experimental results will show that this minimization has a positive impact on routing the cell.
"a crucial question in gt is when to stop this process; here, theoretical saturation [cit] has attained widespread acceptance in qualitative research: we stopped our analysis when 5 to 7 additional knowledge sources did not add anything new to our understanding of the research topic. as a result of this very conservative operationalization of theoretical saturation, we studied a rather large number of knowledge sources in depth (40 in total, summarized in table 1 ), whereas most qualitative research often saturates with a much lower number of knowledge sources. our search for knowledge sources was based on popular search engines (e.g., google, bing), social network platforms used by practitioners (e.g., twitter, medium), and technology portals like infoq and dzone."
"the parameter setting of the cloudlet is based on the length of a task and the total number of task. virtual machine parameters are the total number of vms, mips, vm memory, bandwidth and number of pes required. data center parameters are number of datacenter, number of host and vm scheduler. table 1 shows the execution time of the vm placement algorithm with parameters like cloudlet id, execution time, start time of the task, finish time of the task execution. the execution time of the simulation work is shown in fig. 4 ."
"the transmission scheme for an sdc stream is comprised of one or more d c, one d s and one d r, transmitted in that order."
"although solution (c) is the one that uses the largest transistor sizes for arcs c-d and e-f, it turns out to be the most area efficient. this example clearly illustrates the impact of a good folding strategy in the cell area. this example also illustrates how different legs of the same transistor can be placed separately in the layout."
"ii) a scalable description -d s : the role of this description is the delivery of the higher layer sections of the stream. it is formed by combining several sections from the enhancement layers in an iterative downward fashion and as such contains no redundant replication. initially, one section from the highest layer is added to the scalable description, s n, n, this iteration is the base case. subsequent iterations shall only commence, iff the byte size of one section from the next layer down, s n −1, n −1, plus one additional section from every layer added so far, s n −1, n, is less than or equal to the byte size of the d c . in this manner, every iteration above the base case will reduce the number of d c required to decode the stream by one. eq (3) defines the section allocation for the scalable description."
"for this purpose, the milp model has been modified in such a way that the diversity of transistor sizes is ignored when calculating the eulerization cost of each solution. the details of this modification are not explained, but the reader can easily devise them by assuming that no breaks are used for differently sized transistors."
"in this section, we assess the performance of sdc and sdc-nc and compare it with both svc and mdc. the theoretical improvements in reduced device processing from the use of idd were quantified in the previous section, but in the simulations we chose not to implement idd because the benefits offered by sdc for bandwidth reduction and increased stream quality are the focus of this paper. in our evaluation, we consider four and six layered streams transmitted over a lossy network with different loss rates. the lossy medium is simulated using network simulator 2 (ns-2) [cit] and a 300 frame layered video is obtained using the jsvm framework [cit] . figure 4 provides an overview of the adaptive streaming topology that our evaluation simulates. sdc-nc sdc mdc svc sdc-nc sdc mdc svc layer 6 n/a n/a n/a n/a 95.7% 89.2% 52.7% 51.2% layer 5 n/a n/a n/a n/a 0% 0% 1. methodology ns-2 is used to simulate a two node pair (server-client) communicating over udp with a duplex unreliable link corresponding to the path between a server and a client. an ns-2 errormodel is utilized to simulate a percentage of packet loss. constant bit rate (cbr) is used to simulate transmission of the video trace, with the size of the datagram for svc being 300 bytes, with mdc and sdc at 625 bytes. a streaming model is defined, such that the datagrams need to arrive within a specific timeframe, so as not to reduce the perceived quality of the stream. svc and mdc require four datagrams per gof, with sdc requiring three datagrams, thus decreasing the delivery rate of sdc."
"both cells have the same number of p and n devices. however, the heuristic approach does not consider the global combination of diffusion sizes to reduce the gaps between diffusion breaks and to create more internal eulerian paths, thus resulting in a larger cell."
"gupta and hayes [cit] identify the interdependence between transistor folding and diffusion sharing. they propose an integer linear programming model for multiple-height cells. however, folding and transistor placement are solved independently, assuming that each transistor is folded with the minimum number of legs allowed by the cell height. this approach also assumes that the legs of each transistor are placed contiguously in the layout. for these reasons, this strategy does not guarantee a minimum-area layout."
"local recolorization is obtained in a straightforward manner with our proposed epipolar plane diffusion, based on the procedures described in section 3. results can be seen in section 5."
"the results show a clear impact of the cost function on the final wire length. out of 127 cells, the milp model delivered different solutions for 26 cells. in most of them, there was a clear improvement of hw, which contributes to a better routability and efficiency of the cell. interestingly, many of the optimized cells were sequential. this is understandable given the fact that wire optimization has more impact on gates with complex nonseries/parallel structures. most of the conventional static cmos cells with series/parallel structures (nand, nor, aoi, oai) and with small transistor sizes (x1 or x2) do not show differences in the final netlists after transistor folding. fig. 10 depicts the two layouts for one of the cells (clkgatetst x4) after transistor placement. in this case, the optimized layout has one less n device and one less global wire. this contributed to a better reorganization of active areas to reduce the wiring cost. the figure clearly demonstrates the reduction in wire length when using the wire optimization term in the cost function."
"another interesting aspect is that milp solvers usually offer a timeout option that allows to deliver the best solution obtained when the timeout expires. with this option, an order of magnitude can often be reduced while still obtaining a probably optimal solution."
"by means of disparity interpolation, we can enforce the disparities inside the inpainting mask to be similar to the disparities surrounding the mask. however, if we naively interpolate the unknown disparities based on the nearest known disparities, depth boundaries may be violated, and disparity values from different depth layers may be mixed together in the interpolation."
"therefore, the proposed system focuses on the framework design of the provenance based vm placement algorithm, which gives the solution to the virtual machine allocation problem and also improve the throughput and resource wastage."
"we first explain the model for one row of transistors. in section v, the model will be extended to support multiple rows and multiple-height cells."
an important observation is that gurobi [cit] contains very sophisticated heuristics to solve milp efficiently. similar experiments were done using glpk [cit] with a cpu time between one and two orders of magnitude longer.
". in this paper, it is assumed that the central view i c has been edited beforehand and the goal is to obtain angular coherence between the edited central viewî c and the remaining views."
"resulting diffusion : remaining row-by-row edit propagations : first edit propagation on center column figure 1 : overview of the proposed method. in the green box, it is illustrated a light field with an inpainted central view. we illustrate the original light field after a first edit propagation, which is performed through the center column of views (red arrows). remaining edit propagations are performed row-by-row (blue arrows). the remaining boxes illustrate the steps of epipolar plane diffusion: structure tensor computation (eq. 1) where we obtain dominant structure tensors (eq. 2); from dominant structure tensors we compute a spatial regularization (eq. 3) and tensor inpainting (eq. 6); and finally a tensor driven diffusion (eq. 4) on epis. the output views are reconstructed from the resulting diffusions on the epis. diffusion follows the orientation of the epi structure tensors."
"solution for virtual machine problem have been given in many paper by various authors [cit] . most of the papers enlightened the virtual machine placement problem as bin-packing problem. there are some bin-packing algorithms such as first fit decreasing and best fit decreasing and its variation are mostly used in literatures. along with this, vm placement problem is classified based on its data usages and vm's dependency. they are classified into three types namely 1. prediction based that refers to the particular duration data. 2. provenance based that refers the full history of data. 3. affinity aware vm placement which is based on the dependency among the vm's. this paper, proposes the algorithm based on provenance based vm placement algorithm and all the above revealed algorithms are used for provenance based vm placement problem. they are heuristic scheduling algorithm, provenance based vm placement algorithm and vm migration algorithm."
we will consider the folding problem for arbitrary transistor netlists (not necessarily static cmos) in which the rows for p and n devices can be optimized independently.
the milp model with the constraints (1)- (5) and the cost function (6) delivers an area-optimal folding solution under the assumption that the value of breaks(s) from constraint (4) corresponds to the semi-eulerization cost of the resulting graph (see theorem 3).
"following our study results, we identified 14 adds for service meshes described in detail below. service meshes are usually used together with a container orchestrator such as kubernetes or docker swarm. that is, the services in the mesh, the central services of the service mesh, and service mesh proxies are usually containerized and the containers are orchestrated. very often service meshes are used to deal with heterogeneous technology stacks. that is, a major goal is that microservices can be written as http servers with any programming language or technology, and without modification these services get containerized and managed in a mesh, including high-level services like service discovery, load balancing, circuit breaking, and so on. in the first four sections, we describe adds that characterize a service mesh as a whole. the remaining section describes adds that can be made for specific components of a service mesh."
"the design of scalable description coding (sdc) aspires to reduce the transmission cost of mdc by reallocating a subset of the enhanced layer sections to a new scalable description prior to transmission. hence, sdc decreases the number of n descriptions required to decode a media stream, while maintaining the level of stream quality received by the user. as previously the byte size of layer j s i, j"
"we obtain the initial vocabulary by performing k-means clustering on the extracted visual features. this initial codebook forms a reasonable-sized set representing all features, but they are not semantically clustered, i.e., the features in a cluster may convey different concepts. thus, the formed histograms will not be semantically discriminative for classification. therefore, we need a space in which semantically related words are adjacent. in order to find such a space, we use latent semantic models that find the underlying latent semantics given the occurrence matrix of word-image (or video). these models are the well known lsa and plsa, which we briefly review in the following sections. we use tf-idf instead of the normal count in the occurrence matrix for a higher efficiency."
all the experiments have been performed in a quad-core cpu runnning at 2.67 ghz and 8 gb of memory. gurobi [cit] has been used as the milp solver. gurobi can efficiently exploit the architecture of multicore cpus when solving complex milp problems.
"as stated previously, a service mesh is composed of a set of networked proxies or sidecars that handle the communication between microservices [cit] . the decision regarding managed communication across the services in a service mesh is made for the service endpoints of these microservices, as illustrated in figure 1 . not using managed cross-service communication is a decision option for each service endpoint but please note that this essentially means to not follow a service the service proxy option has the benefit to make it easier to protect the service from malicious or overloaded traffic by achieving access control, tls termination, rate limiting and quotas, circuit breaking, load balancing, and other tasks; this is discussed in more depth in section 4.5. also, the independence of the service from its proxy increases the extensibility and maintainability of the service, which is, as a result, not aware of the network at large and only knows about its local proxy. however, this option might produce additional communication overheads and congestions. the major benefit of choosing an api-based service integration over a service proxy is that it makes the service mesh less complex and there is less communication overhead. however, doing so limits its extensibility and interoperability. the option not to manage crossservice communication basically means that all benefits of service mesh are not achievable."
"the redundancy description stated, mdc utilizes layer partitioning to create numerous sections per layer and fec to extend the original section over numerous descriptions. eq (1) defines the maximum stream quality of an mdc gof to be the sum of all original sections for all layers. it also defines the per layer partitioning ratio by generating the number of original sections per layer."
"we propose incorporating network coding (nc) [cit] into sdc to improve the overall performance. in sdc-nc, the redundancy description, d r, as outlined in section iii, is re-placed by a network coded (nc) description, d r−nc, which is the exclusive disjunction, or xor, symbolized by ⊕, of all the leading descriptions for a given gof, d c ⊕ d s in our example. as with most nc implementations, should the byte size of the d s be smaller than a d c, then the d s shall be padded with trailing 0s [cit], to maintain a description of equal size and balance the impact of the nc mechanism mechanism. by utilizing sdc-nc, individual devices can recover from the loss of any description (scalable or complete) by receiving the nc description. in this manner, if any two descriptions are correctly received by the client device in the four-layer example, the full quality stream can be decoded."
"algorithm iv generates a solution for transistor folding by iteratively solving different milp models. when a solution is found in which the estimated cost and the real cost coincide (line 8), the solution is guaranteed to be optimal."
"where usesize n (s) and usesize p (s) are the binary variables that represent the presence of legs of size s in the p and n rows, respectively."
"the proposed milp model aims at minimizing the area of the cell and delivers a solution that guarantees (by euler's theory) the existence of a transistor alignment with the area calculated by the model. still, there may be multiple folding solutions with the same optimal area cost. an interesting question is: among all the area-optimal solutions, can the milp model provide one with good routability properties? in this section, we propose an optimization criterion that has a direct impact on wire congestion."
"fig. 6 depicts a layout architecture for transistor netlists that guarantees minimal area. since transistors with different sizes cannot be chained, groups of equally sized transistors are created. each group corresponds to one of the g i graphs for folding. this scheme minimizes the diffusion breaks between different sizes that have a cost of diffsizegap slots."
"to overcome this issue, we assume that inside the inpainting mask, disparity clusters are likely to have shape and boundaries similar to the known color clusters. in practice, we consider a segmentation of the edited central viewî c into slic superpixels [cit], where each superpixel account for a color cluster onî c . superpixels are an adequate image partitioning for this problem since their boundaries tend to respect object boundaries in the image. hence, the main purpose of superpixel segmentation is to enforce the disparity interpolation to respect depth boundaries in the light field."
"with few services, we can choose not to collect them centrally. at large scale, this might make control and management tasks complex and central features such as dashboard or metrics are hard to impossible to build. some telemetry might also be needed anyway for the functioning of the service mesh itself."
"this paper provides a novel and efficient solution to this problem, where we address an angularly coherent propagation of local edits through an approach we call epipolar plane diffusion, as it performs an anisotropic diffusion on epipolar plane images. anisotropic diffusion has been extensively applied to solve image editing problems such as inpainting, due to properly modeling piecewise smooth image content. however, diffusion approaches met a diminished usage in inpainting due to their limited capability of reconstructing image textures."
"the generation of a cell layout also depends on the subsequent steps in the synthesis flow: transistor placement and routing. some cells may require a highly congested layout (e.g., flip-flops, full adders, or simple cells with multiple inputs) and the routability of the cell may depend on subtle variations of the folding and placement solutions. the availability of different solutions may contribute to increase the probability of finding routable solutions with optimum area."
"-centralized control plane -a central component, called the control plane, controls traffic of a service mesh. it is responsible of managing and configuring sidecars in addition to distributing access control and usage policies, observing and collecting telemetry, traces and metrics, in addition to numerous other services like service discovery, as described in section 4.5. -distributed control plane -each service of a service mesh has its own cache that is efficiently updated from the rest of the services. this helps to enforce policies and collect telemetry at the edge. -front proxy -the proxy is responsible for intercepting incoming traffic from outside the service mesh as described in section 4.2. it might also be extended to handle traffic control at the entry point of the service mesh. this option can potentially be combined with the two previous options (for that reasons, the decision is marked with a stereotype that indicates that multiple answers can be selected). -finally no dedicated traffic control can be used as well."
we propose an extension of the model that supports this flexibility. the extension is proposed for the case of synthesizing single-height cells. the extension to multiple-height cells is briefly discussed in section v-c.
"4) a new variable (area) and r constraints must be added to calculate the maximum area of all rows. for each row r, the following constraints will be added:"
"according to several authors, the 1-d design style with gridded design rules (gdrs) is one of the principal trends toward addressing the manufacturing issues in future process technologies [cit] . in 1-d gdrs, layout is composed of grating patterns with rectangular shapes located on a grid with fixed pitch."
the folding problem consists of finding an equivalent implementation of the netlist with multiple transistor legs that cannot exceed a maximum size s and can be implemented with minimum area using an optimal transistor chaining.
"transistor folding has been applied to the 45-nm nangate standard cell library [cit], which contains 127 cells. the original cells already have large transistors that have been folded to fit in the active area. the procedure applied to obtain the netlists for transistor folding is as follows."
"in a nutshell, our approach imposes angular coherency for light field editing by inpainting the epipolar plane images. inpainting is modeled as an anisotropic diffusion on epis, guided by structure tensors."
there have been several attempts at to incorporate semantics into the bow model so that a more discriminative visual vocabulary is realized. [cit] . the copyright of this document resides with its authors. it may be distributed unchanged freely in print or electronic forms.
"1. resource usage information such as time, load, etc., is collected from the virtual machines of cloud environment. 2. the information is in the form of unstructured and is converted into a sequential database format (structured). 3. then, it is stored in the local database of the server. 4. it is transferred to the global provenance database which is available in cloud data tier at the particular time period. 5. frequently, these data are analyzed and set the frequently used attribute."
"unfortunately so far no dedicated architectural guidance exists on how to design and architect microservices in a service mesh environment apart from practitioner blogs, industry white papers, experience reports, system documentations, and similar informal literature (sometimes called gray literature). this includes that so far there is no guidance for users of service mesh technologies or even their implementors to select the right design and technology options based on their respective properties. very often it is even difficult to understand what all the possible design options, their possible combination, and their impacts on relevant quality properties and other decision drivers are. as a result, there is substantial uncertainty in architecting microservices in a service mesh environment, which can only be addressed by gaining extensive personal experience or gathering the architectural knowledge from the diverse, often incomplete, and often inconsistent existing practitioner-oriented knowledge sources."
theorem 3 (eulerization cost [cit] ): let g be a noneulerian graph and v o (g) the subset of odd nodes. the eulerization cost of g is
the next section proposes an algorithmic approach to guarantee optimality even in the case that there is a discrepancy between breaks(s) and the semi-eulerization cost.
"the paper is organized as follows: section 2 discusses previous work related to light field editing, inpainting and anisotropic diffusion; sec. 3 presents our epipolar plane diffusion method; sec. 4 discusses the application of our method to light field recolorization and inpainting; sec. 5 demonstrates the efficiency of the proposed algorithm through experiments with synthetic and real light field images. finally, sec. 6 concludes the paper with final considerations and suggestions of future work."
"both heuristics are myopic, in the sense that the distribution of legs for each transistor does not depend on the other transistors in the same netlist. the difference between the heuristics comes from the way sizes are distributed among the legs."
"the 1-d design style is becoming a major trend in current nanometric technologies and will be unavoidable in the future. layouts with regular patterns are becoming a viable alternative to handcrafted layouts for semicustom design. when severe manufacturability constraints are imposed, the design of a standard cell is progressively evolving from an art to a combinatorial problem. in this context, design automation is playing a predominant role."
the remaining part of this paper is organized as follows: section 2 describes the related work. section 3 provides a description of the provenance based vm placement algorithms and its framework. section 4 details the evaluation of the algorithm. section 5 closes the paper with conclusions.
"a row may also contain blocks of devices with different size. with 1-d gdrs, no diffusion sharing with differently sized transistors is allowed, since this would imply nonrectangular shapes for the active area, as shown in fig. 2(b) . diffusion strips must have equally sized transistors and breaks must be introduced between devices with different size. the length of these breaks must be a multiple of the technological pitch, as shown in fig. 2(c) . depending on the technological pitch, breaks between differently sized transistors may occupy one or two polysilicon slots."
"cloud computing is a recently evolved computing terminology or metaphor based on utility and consumption of computing resources. cloud computing is not a personal computer or an individual application server instead it is a distributed worldwide computation server. most of the it companies are using cloud environment for their application development, deployment and to store their information. who are cloud providers? companies that have their own data centers are known as cloud providers. cloud providers offer services of cloud computing. according to several service models, these services are known as delivery models. mostly used cloud delivery models are software as a service (saas), platform as a service (paas), and infrastructure as a service (iaas). this research paper will be based on iaas delivery model. infrastructure as a service delivers the resources of cloud computing such as computing, storage and networking on demand. due to its elastic nature, cloud environment has numerous issues and challenges. the fundamental problem is the placement of virtual machine. by using virtualization concept, physical machine is logically divided into multiple parts using virtual machine monitor (vmm). these parts are called as virtual machine. virtual machine is a self-contained operating system that behaves as a separate computer. virtual machine plays a significant role in cloud infrastructure."
"cloud computing is one of recent advancement in technology since it combines the advantages of both grid computing and ubiquitous computing. one of the major challenges in a cloud computing environment is the identification of the vm and placement of job. this paper aims at proposing a solution to this problem using the provenance data. a framework for provenance based vm placement algorithm has been proposed in this paper. the digital print the system leaves in the process of vm placement is utilized by the provenance algorithm in predicting the suitable vm. the placement of the vm is one of the key attribute in enhancing the performance of the system. the algorithm is aimed at optimizing the vm placement to improve the system performance. the proposed framework explains the methodology used to collect the provenance data and using the data in effectively placing the virtual machine. the results are verified in a simulated environment using cloudsim. in future, this work can be extended by applying optimization algorithm in this framework and test the performance of the system."
"the adoption of the 1-d design style implies a new tradeoff between area and performance. on one hand, the diversity of transistor sizes has a negative impact in area due to the overhead introduced by the diffusion breaks. on the other hand, the diversity of sizes is convenient to have a larger space of solutions for gate sizing."
"we note that our unoptimized implementation in python takes in average one minute to propagate an inpainting to all light field views (lytro 1.0 image), but our algorithm could greatly benefit from parallel processing. we invite the reader to access our project website (http://oriel.github.io/epipolar_plane_diffusion.html), where we provide more results obtained with our method. in the zoomed areas that the refocusing using the baseline method (orange circles) incorrectly mixes \"in focus\" and \"out of focus\" pixels, while the light field inpainted with our method (note the yellow zoom circles) results in consistent refocusing. the inpainting mask contour is represented in red color."
"in highly versatile environment such as service meshes, services go up and down unexpectedly which decreases availability. to overcome this issue, periodic health checks on services can be applied and e.g. mitigation strategies like service restarts can be applied. health checks are usually performed by the service proxy and a central service collecting the information. alternatively, simple health checks like pinging service proxies can also be done solely on the central service, but then more complex health checking is not possible. of course another decision option is to not perform health checks."
"in figure 2 we illustrate the super-pixel based eigenvector interpolation. it can be noted that our superpixel-guided interpolation correctly preserves most shapes in the disparity map, while an interpolation without any constraint tends to violate depth boundaries."
"processing, for which our approach could be used to guarantee angular coherence for other tasks such as segmentation and super-resolution. for that, occlusions should be carefully handled in the epipolar plane diffusion. finally, we believe this paper may inspire new approaches for using tensor diffusion with light field images. for instance, 4d structure tensors could be employed to obtain simutaneously spatial and angular coherence in light field reconstruction problems."
"i) a complete description -d c : the primary role of this description is the delivery of the lower layer sections of the stream. it is identical to an mdc description in that it contains one section from each layer in a gof and as such contains a percentage of redundant replication. similar to mdc, one or more d c can be transmitted per gof. eq (2) defines the byte size and section allocation for a complete description."
"an example realizing the api-based service integration is connect proxy used in consul that is implemented using language-specific libraries that are used directly in microservices code. service proxy and sidecar proxies are more frequently supported; examples are envoy proxy [cit] in the istio service mesh [cit], kong proxy, nginx proxy and linkerd proxy. most such service proxy technologies can be deployed as a sidecar or a service proxy running in a different environment (e.g., different server or vm); they usually also offer the option to be used as a front proxy as discussed in the next section."
"the remainder of this paper is organized as follows: in section 2 we compare to the related work. section 3 explains the research method we have applied in our study. then section 4 explains a precise specification of the service mesh design decisions resulting from our study. the uncertainty estimation is discussed in section 5, followed by a discussion in section 6 and conclusions in section 7."
"video coding techniques represent a means to mitigate the aforementioned limitations. more specifically, scalable methods allow devices to dynamically modify their stream quality by adjusting the percentage of received data. thus a device might commence by selecting to receive the highest resolution it can support, but subsequently alter the quality in response to changes in the achievable network throughput. scalable video coding (svc) [cit], an extension to the h.264/mpeg-4 part 10 or avc (advanced video coding) compression standard, achieves streaming scalability by partitioning a video stream into several subsets, commonly called layers. the cumulative receipt of each additional layer offers a progressively higher quality [cit] . a serious limitation in svc is that the loss of a lower layer hinders the device from decoding the dependent higher layer. hence, multiple description coding (mdc) offers an improved error resiliency to packet loss, but at the cost of increased data rates due to redundancy techniques, such as forward error correction (fec), as in mdc-fec [cit] ."
"indeed, any impact in area and/or performance has a corresponding impact in power. the exploration of this tradeoff is something that should be further investigated in the future."
"global provenance database collects the log information of the local database. the log information is in unstructured format. it converts into structured format for easy access. this structured information will be analyzed frequently and the parameters supported for placing the virtual machine are calculated. for example, set the levels of usage of virtual machine for identifying frequently used virtual machine."
"incremental datagram delivery (idd) is proposed as a dynamic transmission protocol, which is based on an incremental delay in the initial transmission of each description in a gof, with higher priority descriptions being offered transmission precedence. thus increasing the time available for retransmission of the important descriptions and for the receiving device to infer its ongoing requirements based on the descriptions received so far. in this manner, idd offers a prioritized transmission scheme which can offer benefits by reducing the occurrence of transmission obstacles such as delivery latency, datagram re-ordering, and by increasing qos."
"the milp model for transistor folding has been also executed with the cost function for wire optimization (see section vi). the transistor placement tool has been used to find an area-optimal layout with minimum horizontal wire length (hw), measured as the total length of horizontal wires to connect the transistor pins. table vi reports the set of cells in which the milp model provides a different solution when wire optimization is incorporated in the cost function. column tr reports the number of transistors in the cell that corresponds to the term costlocalwires in (9) . column gw reports the value of costglobalwires in the same equation. finally, hw reports the horizontal wire length after placement."
"provenance based virtual machine placement in cloud computing provides cloud servers as an efficient way of allocating resources to a user or client request based on historical data. this historical data is unique to each client request and each data is formed as a result of gradual monitoring of data usage of the client such as amount of resource required, duration of usage of resource etc."
"the method presented in this paper has an important feature: it can guarantee area optimality without calculating the exact location of the devices. with this approach, folding and placement can be decoupled without sacrificing area, which is essential to provide automation with affordable computational cost. he is currently a professor with the department of software, universitat politècnica de catalunya. [cit], he was a visiting scholar at the university of california, berkeley. he has coauthored numerous research papers and has been invited to present tutorials at various conferences. his current research interests include formal methods and computer-aided design of very large scale integration systems with special emphasis on asynchronous circuits, concurrent systems, and logic synthesis."
"to avoid the unlikely situation in which a cut also eliminates some optimal solution, a hybrid approach can be used combining the cuts presented in section iv-a (to find an optimal solution) with the cuts presented in this section (to generate a diversity of similar solutions)."
"first, we assess the instantaneous video quality using different schemes as highlighted in table iv and figure 5 . table iv provides the percentage of viewing time at the different possible quality levels for the compared algorithms at a packet loss rate of 10%. sdc shows a better performance in comparison to svc and mdc. the network coding component in sdc-nc result in a further improvement of approximately 6% of the highest quality viewing time, thus providing an increased qop for the user. it is important to note that the marked change between the viewable layers of sdc can be quite high, especially as the number of layers increases, i.e. in the six layer example the transitions from highest quality to the next available quality was from layer six to layer three in sdc and from layer six to layer two in sdc-nc. also there is a noticeable percentage of stream transition to the lowest level in sdc-nc and sdc that is not evident in mdc (as mdc combines all received description, per gof, which increases higher layer decoding), but it is felt that the jump from higher layer to lower layer happens so infrequently that it can be easily and efficiently addressed, such as with a simple retransmission scheme. figure 5 presents a two second snapshot of the data from table iv and illustrates the viewable layers during the snapshot interval for the considered schemes. additionally, the figure also illustrates the frequency of transitions between different video layers. clearly, the figure shows that sdc and sdc-nc have far fewer transitions in comparison to mdc and svc, which has the worst performance. the figure also illustrates that sdc-nc results in higher performance gains as the medium becomes more lossy. hence, the inclusion of the network coding component would be of great benefit to less reliable mediums such as wireless technologies."
"as discussed in section v-b, the milp model can be adapted to handle multiple-height cells. in this section, we estimate the impact of wire optimization in double-height cells. table vii reports results on wire optimization for cells using active areas organized as n-p-p-n strips with a maximum number of 3-5-5-3 tracks, respectively. the table reports the number of transistors (tr) and the estimation of global wires (gw) in the solution delivered by the milp model with and without wire optimization. 9 the number of cells affected by the optimization is much larger than for single-height cells. 10 (88 out of 127). the reason is because the amount of solutions with the same area is larger for double-height cells, since devices are allocated in a larger set of active areas with different sizes distributed in the top and bottom rows of the cell. the most relevant information in the table is that the number of global wires is reduced from 1493 down to 1198 (almost 20%), which may have a positive impact in the routability of the cell."
"for simplicity, we will formulate a model for the doubleheight p diffusion in a cell organized with rows nppn. the reader will soon realize that the model can be easily extended for other templates."
"while nc significantly improves the resiliency of sdc to errors and losses, it is important to note that while the quality of the decodable stream generally increases, the number of scenarios in which no frame can be decoded also increases. in this manner, the nc description can also be seen as a prioritized description, such that receiving only that description increases frame loss and as a result decreases stream quality, but as can seen in the evaluation in section v this loss rarely occurs."
"finally, a new constraint is added to cut all solutions that have an estimated area greater than or equal to the real area of the last solution (line 13)."
"it is interesting to observe that the support for multipleheight cells not only calculates the folding for transistors but also partitions and assigns the legs to the rows of the layout. therefore, the model also guarantees the existence of a partition with the target area. however, this partition may not be unique and the subsequent synthesis tools for transistor chaining may find a different one with the same folding configuration, but possibly assigning the legs to different rows."
"collect telemetry, traces, and metrics decision to observe telemetry, traces and metrics in a service mesh, they first need to be collected. otherwise, we have to access each of the services and upload this data manually. this is usually done by a control plane service collecting data from data plane proxies."
"assuming that l is the minimum number of legs required to fold the transistor, according to (10), the rules to fold a transistor t are the following."
"the most obvious benefit of using a centralized control plane is its simplicity and ease of administration. however, especially when using one single control plane, it produces a single point of failure and is hard to scale. also, it might cause traffic congestion which increases latency. centralized control planes provide policies to the service proxy on how to perform routing, load balancing and access control. in that case, the next optional decision to take is related to service mesh expansion. istio service mesh, for example, which is based on centralized control plane supports service mesh expansion in a multi-cluster environment. on the other hand, a distributed control plane is highly scalable and there is no single point of failure. however, this option is the most complex and thus risky option. using this option, traffic may be forwarded to either a service proxy or directly to a service via api-based service integration as described in section 4.1. consul for example, which implements distributed control plane, consists of a set of client agents that are efficiently updated from server agents to control traffic at the edge. an example using front proxy, described in source s10 in table 1, uses envoy which can be extended to become an api gateway, which then can do traffic control for the service mesh by integrating with istio. the front proxy solution does not have the fine-grained control offered by the other options, as it is only applied in a central place. it is also a single point of failure and is negative for congestion and latency, but is a simple and non-complex solution. if used together with one of the other options, it increases the complexity of these options even further, but enables more fine-grained control for the ingress traffic and thus can reduce the overall traffic in the mesh. these options lead us to the next optional decision regarding distributing traffic control and other tasks among control plane and data plane (see section 4.5 for a list of these follow-on decisions, not shown in figure 3 for brevity). figure 3 shows these decision options and their relations."
"then jsvm is used to extract each layer from the original crew.264 and reconstruct a yuv stream for each layer. note that we also up-sample lower resolution streams for psnr calculation using the normative up-sampling jsvm method \"downconvertstatic\", which is based on a set of integer-based 4-taps filters derived from the lanczos-3 filter. jsvm does not contain a dependable mechanism to simulate packet loss, so to simulate this loss, we developed our own code, \"modp-snr.exe\", based on the jsvm code base, which creates a modified yuv stream, mod crew.yuv, from the layered yuv streams based on the svc layer values per frame in the ns-2 generated trace file. finally jsvm calculates the corresponding psnr."
"finally, the cost function must take into account that the area of the double-height diffusions must be accounted in the two p rows simultaneously. the following constraints guarantee that area takes the maximum area of both rows:"
"in fig. 4, it can be seen that the edit propagation obtained with the baseline fails to correctly propagate the edit on the central view, resulting in a refocusing with noticeable inconsistency. note that the inpainting mask is still visible after refocusing, where foreground and background areincoherently mixed. on the other hand, our method allows for a coherent edit propagation with correct refocusing at foreground and background."
"we have studied knowledge on established practices in service mesh architectures, relations among those practices, and decision drivers to answer our research questions rq1-3, respectively, with multiple iterations of open coding, axial coding, and constant comparison to first codify the knowledge in informal codes and then in a reusable add model. precise impacts on decision drivers of design solutions and their combinations were documented as well; for space reasons we only summarized those in the text and did not show them in detailed tables. in addition, we estimated in section 5 the uncertainty reduction achievable through the organization of knowledge in our add model. we may conclude that our add model (and similar models) has the potential to lead to substantial uncertainty reduction in all evaluation variables due to the additional organization it provides and pre-selections it makes. for individual decisions, mastering and keeping in short term memory the necessary knowledge for design decision making seems very hard for the case without the add model (see numbers in table 2 ), but quite feasible in case of our add model. our model also helps to maintain an overview of the decisions ndec ⊕ and criteria assessments ncri ⊕ in the combinations of contexts. only the number of possible decision outcomes for the combination of multiple decisions seem challenging to handle, both in the ndo ⊕ and ndo case. that is, despite all benefits of our approach, the uncertainty estimations show that a limitation of the approach is that when multiple decisions need to be combined in a context, maintaining an overview of possible outcomes and their impacts remains a challenge -even if a substantial uncertainty reduction and guidance is provided as in our add model. further research and tool support is needed to address this challenge. as our numbers are only rough estimates, further research is needed to harden them and confirm them in empirical studies, maybe based on a theory developed based on such preliminary estimations."
"finally, extra edges must be added to bridge blocks of transistors with different sizes. the number of required bridges is equal to the number of different sizes minus one (see fig. 6 )"
"considering these drawbacks, we propose a method for action and scene recognition based on a semantic visual vocabulary that uses latent aspect models to embed visual words into a rich semantic space which we call the concept space. using lsa (latent semantic analysis) or its probabilistic version plsa, the synonym words which convey the same meanings are embedded close to each other so that they can be clustered together into the same semantic cluster. the distance in the proposed concept space is practically based on the meanings of the words and thus, they represent the semantic relations. consequently, the formed histograms based on these semantic clusters are efficient and discriminative for recognition."
"this is a euler-friendly heuristic with two main goals. 1) for large transistors, use only legs with size s whenever possible. if not possible, use only legs with size s and s − 1. 2) whenever possible, use an odd number of legs with size s. the first goal reduces the diversity of sizes used for transistor folding, thus reducing the area penalty associated with the gaps between differently sized transistors."
"in this paper, we have proposed a novel approach for using semantic relations in bow framework. we have used the latent aspect models such as lsa and plsa to map the visual words into a semantic space. under the lsa framework, this mapping is done by low rank decomposition of the word-document occurrence matrix using svd. by using plsa, the topic-specific distributions of words are considered the projection of the words along different concepts. the distances in the proposed concept space reveal the semantic relations. clustering is done in the concept space to capture the semantic structures. we tested our method on two complex datasets of human actions and scenes. our results confirm that the proposed features are efficient and discriminative for scene and action recognition. also, our method performs better compared to some similar methods for constructing semantic vocabularies."
"to better appreciate the benefits of sdc we provide an example in which we assume that each svc layer is 300 bytes. this number is chosen purely to simplify the example by allowing each layer/description to be transmitted within the size of a typical un-fragmented ip datagram. by assuming a value of 300 bytes per layer, this yields an svc gof of 1,200 bytes -4 layers x 300 bytes. note that in a case where the layers are not the same byte size, but incrementally larger/smaller, the same mechanism is employed but additional performance gains may be attained, due to the changes in section size. let the corresponding mdc representation have four descriptions, each consisting of 625 bytes (300 + 150 + 100 + 75), determined using eq 2, thus totaling 2,500 bytes, 650 x 4, for the mdc gof. clearly, to improve the stream reliability, mdc adds significant overhead to svc, providing a sufficient motivation for sdc to offer efficiencies by reducing bandwidth demands but without sacrificing user-perceived quality."
"in this work, we revisit anisotropic diffusion, showing that tensor driven diffusion is an appropriate model to be applied to epipolar plane images (epi) derived from light fields."
"fluorescence microscopy is widely used in biology to observe selected molecules and subcellular structures of interest. however, the resolution of conventional techniques (e.g., widefield, confocal) is limited by the diffraction phenomenon. since the early 90's, this limitation has been overcome by super-resolution techniques, achieving unprecedented nanoscale resolution. single molecule localization microscopy (smlm) techniques 1,2 rely on photoactivatable fluorescent probes together with psf engineering and dedicated localization algorithms. other methods, such as structured-illumination microscopy (sim) 3, 4, use standard staining but specific excitation processes which move high-frequency content into the observable region of the microscope. a numerical reconstruction from a set of sim acquisitions with varying illuminations can double the resolution of conventional systems."
"the helper may select imperfect thresholds that do not fall in this range. however, it is still possible to achieve a high detection accuracy. for example, if the threshold is set to 0.02, the detection rate is as high as 91.2 percent, the false positive rate is still 0, which are obtained from fig. 19 ."
"decision tree algorithm when applied on dataset 1, with or without normalization, was able to classify the data with an accuracy of 97.1%. dataset 2, with or without normalization, was able to classify the data with an accuracy of 95.12%."
"non-linear representations of data could be trained using a deep learning approach. deep learning solves the problems of neural networks, which are mainly composed of vanishing gradients and may suffer from overfitting related problems. this approach overcomes such problems due to various activation functions, using huge amounts of data and handling dropouts. deep learning combines multiple neural networks with non-linear activation functions."
"to detect motion camouflage, we propose to utilize a passive helper at the receiver side and observe the difference of the two channel estimates at this helper side."
"thus, when the attacker causes the receiver to observe the same channel estimation results for the first and second transmissions, the two channel estimation results at the helper side are also the same. therefore, the virtual multipath attack in ofdm systems cannot be detected by the previously proposed regular defense, which just observes the difference of two channel estimates at the helper side for two transmissions with different training sequences. however, we identify alternative ways to close the loophole of the regular defense and defend against virtual multipath attacks in ofdm systems. we first categorize two typical objectives of attackers to confuse the location distinction:"
"the location distinction system consists of a transmitter and a receiver. both are equipped with radio interfaces that can transmit and receive wireless signals. the receiver aims to verify whether or not the transmitter has changed location. towards this goal, the receiver estimates the channel impulse response from a wireless signal received from the transmitter, and then compares it with the previous estimation results to generate a decision. to constantly enforce the location distinction, the receiver periodically sends an inquiry to the transmitter, and the transmitter responds to the inquiry by sending wireless signals back to the receiver."
"where f is the residual matrix of y. pls combined with the armax (autoregressive moving average with exogenous inputs) time series model generates dynamic pls (dpls). dpls uses the past values of the pcs along with the relationships between the source variables and the target variables. thus, the required data set for dpls could be obtained even when there are external disturbances or set-point changes. the difference between the estimated value from the dpls system and the measured one is called residual."
"the problem with the above equation is that x t x is singular. this could be because the number of independent variables and the number of observations vary from one another. there is also a possibility of collinearity among variables. a check for normality and influential elements is done using a q-q plot and cook's distance respectively. cook's distance is used to measure the influence due to leverage and outliers. here, leverage indicates the influence on the prediction if the observed value is increased by a unit. cook's distance is calculated as:"
"we perform real-world experimental evaluation on the universal software radio peripherals (usrps). experimental results show that an attacker, by using the virtual multipath channel as camouflage, can fool a target to believe any desired channel characteristic with successful probability of 95.0 percent. however, our defense can discover this attack with probability more than 91.2 percent and the false alarm rate can be reduced to 0 with a carefully chosen detection threshold. the experimental results suggest the discovered attack is a real threat to existing location distinction schemes using the spatial uncorrelation property, and demonstrate the success of the defense approach. our contributions are summarized as follows."
"the multipath effects of different wireless links are different, and so are the channel impulse responses [cit] . due to this reason, a channel impulse response has been utilized to provide location distinction [cit] . specifically, to determine if the transmitter has changed its location, the receiver estimates the channel impulse response of a newly received signal and compares it with the previous estimation result. the location change is detected if the difference between the newly estimated channel impulse response and the previous one exceeds a certain threshold."
"as an initial validation, we simulate the virtual multipath attack using the crawdad data set [cit], which contains over 9,300 real channel impulse responses measured in an indoor environment with obstacles (e.g., offices and furniture) and scatters (e.g., windows and doors)."
"in case of a very powerful attacker, who is able to set up a collaborator transmitter that is co-located with the receiver's helper (i.e., at the exact physical location of the receiver's helper), h 12 and h 22 may be obtained from the wireless signals sent by the collaborator transmitter. nevertheless, the defense methods can be easily extended to deal with these attacks by increasing the number of helpers at the receiver."
"in this experiment, we evaluate the performance of the proposed defense approach in terms of detection and false positive rates. we have 90 pairs of locations to place the attacker and the helper. from each pair of the locations, we can obtain the corresponding distances d helper and d rec ."
"we identified a new attack against existing location distinction approaches built on the spatial uncorrelation property of wireless channels. by launching such attacks, the attacker can create virtual multipath channels to deteriorate the location distinction capability of a target receiver. to defend against this attack, we proposed a detection technique that utilizes a helper receiver to identify the existence of virtual channels. we also explored virtual multipath attacks and corresponding defenses in ofdm systems. we performed real-world evaluation on the usrp platform running gnuradio. the experimental results demonstrated both the feasibility of the virtual multipath attack and the effectiveness of the defense approach."
"song fang received the bs degree from the south china university of technology, guangzhou, china, and the ms degree from the beijing university of posts and telecommunications, beijing, china. he is currently working toward the phd degree in computer science at the university of south florida, tampa, fl. his research interests are in the area of network security and system security."
"as discussed, a wireless signal usually propagates in the air along multiple paths due to reflection, diffraction, and scattering. a receiver then receives multiple copies of the signal from different paths, each of which has a different delay due to the path it traverses. the received signal is the sum of these time delayed copies. each path imposes a response (e.g., delay and attenuation) on the signal traveling along it [cit], and the superposition of all responses between two nodes is referred to as a channel impulse response [cit] . wireless channels can be characterized by channel impulse responses."
"the accurate detection of faults is vital for fault reduction in the system. this detection is performed by monitoring the residuals using the cusum [cit] approach. the cusum method provides an out-of-control signal if the sums of the larger values of the sample readings cross a predefined threshold. minimum jump size and threshold size are considered as parameters for the cusum method. 60 datasets with 30 variables each were given as input to the system. among these 30 variables, twelve are selected as target variables by dpls. this reduction of variables occurs since certain ones have no impact on faults. the relation of many faults to certain variables cause fault detection by cusum to be unsuccessful. thus, certain classifiers are used to identify the fault. according to the results, dpls with the cusum approach performs equal well when compared to neural network based systems in detecting faults."
"the presence of different valves in the desalination plant, along with other heterogeneous instrumentation could enlarge the number of features to be studied. the process of learning with all the features could be performed using various algorithms like multiple linear regression discussed above, but it would be efficient to extract only the salient features and impart them as inputs to various algorithms. the process of feature extraction could be performed using all possible regression approaches or automatic methods. all possible regression approach used adjusted r 2, akaike information criterion (aic) and bayesian information criterion (bic) values to select features. automatic methods in regression include stepwise regression with either forward or backward approaches."
"the effects of different features was tested using regsubsets in the leaps library. the outcomes of adjusted r 2 and that of bic differ significantly, which prevented us from reaching a decisive conclusion. figure 7 shows the inclusion of a variable using black and exclusion of a variable using white. observing the top-most row which provides the minimum adjusted r 2 or bic, it is obvious that the variables selected using different criteria are different. to explore further, stepwise regression was used."
". however, such a quantitative estimation is made possible from a set of tirf acquisitions for different incident angles α (ma-tirf) by developing dedicated reconstruction algorithms. we solved this inverse problem (i.e., estimating f in (1) from multi-angle acquisitions"
"another recent work that is closely relevant with the proposed defense approach is securearray [cit] . this work utilizes the physical angle-of-arrive (aoa) of a multi-antenna access point to enforce user authentication. our proposed defense technique uses channel impulse responses observed by multiple antennas to protect location distinction systems. however, our defense targets attacks against location distinction systems built on the spatial uncorrelation property of wireless channels, whereas securearray is designed to combat spoofing attacks that attempt to impersonate legitimate wi-fi clients. both approaches apply to different application domains."
"the given datasets 1 and 2 are standardized using equation 1. the mean squared error is high for dataset 2 but the p-values are all acceptable given the level of significance. in dataset 1, 92.55% of the variations of observations are explained by the equation of regression, with or without normalization. meanwhile, this value drops to 78.63% without normalization and still down to 53.65% for dataset 2 with normalization. the reason behind this substantial variation of observations was explored further. residuals are to be checked whether they are normally distributed and have equal variance. the normal distribution is verified using a q-q plot as shown in figure 3 and 4."
"the attacker may also bring a second transmitter to confuse the receiver. fig. 6 shows such a scenario. we refer to the attacker's second transmitter as the attacker's helper. let h 11, h 12, h 21, h 22 denote the channel impulse responses between the attacker and the receiver, the attacker and the receiver's helper, the attacker's helper and the receiver, and the attacker's helper and the receiver's helper, respectively. to successfully launch the virtual channel attacks without being detected, the attacker must generate the same channel impulse response at the receiver's helper for both transmissions. let h help denote such a channel impulse response. further let h a denote the one that the attacker expects to generate at the receiver for both transmissions. the attacker needs to make the following equation hold:"
"we show an example of the defense approach using the real measured channel data from the crawdad data set. we randomly pick three nodes from the data set, and they are used as the attacker (node 14), the receiver (node 3), and the helper (node 32), respectively. we also randomly pick one channel impulse response (between nodes 4 and 9) from the data set, and it is used as the fake channel impulse response that the attacker would like to fool the receiver. let h, h help, and h a denote the channel impulse responses between the attacker and the receiver, the attacker and the helper, and the fake one chosen by the attacker."
"where y t, y − t, and y + t are the received symbols at time (t), the received symbols before time (t), and the received symbols after time (t), respectively [cit] . if we take the logarithm of (7), we obtain"
"random variable x probability density function (pdf) codes have not been considered on plc systems. furthermore, it is well known that orthogonal frequency-division multiplexing (ofdm) is a powerful tool to combat the frequency selectivity and it is highly resistant to the effect of impulsive noise by spreading the noise signal energy simultaneously over sub-carriers [cit] . in addition to the use of ofdm, non-linear blanking and clipping operations are also applied to mitigate the effects of impulsive noise samples with large amplitudes [cit] . the ber performance of non-binary turbo codes defined in a galois field gf(4) is investigated for an ofdm-plc system on realistic power-line channels and compared with comparable binary turbo codes."
"the system is developed under labview. lasers, piezo and galvanometers are driven by daq (ni-6731 and ni-6733, national instruments, nanterre, france) which store the full experiment design for an acquisition. rotation rate of the galvanometers (for conical illumination) is adapted to impose a tunable number of rotations per image based on the exposure. delay time between images imposed by the daq is reduced to 1.5 ms based on the devices specificities. this lag is then exploited to transfer the data from the camera and move the galva to their subsequent position. for reconstruction, the acquisition of 10 angles in 1 s or less is usually required. the filter wheel is the slowest element with 40 ms between adjacent filters. cells and culture conditions. primary cultured bovine aortic endothelial (bae) cells have been described previously 25, 34 . cells were maintained in dmem (invitrogen, cergy pontoise, france) supplemented with 5% foetal calf serum (fcs) in a humidified incubator at 37 c with 5% co2."
"first, a pre-condition to launch mimicry attacks is the knowledge of the real channel impulse response between alice and bob (thus they assume the existence of a spy node). however, a virtual multipath attacker can still launch attacks without this knowledge. moreover, if the attacker knows the real channel impulse response, she can make the receiver believe a specific channel impulse response. therefore, virtual multipath attacks have a broader attack impact and less prerequisites. in addition, we extend the virtual multipath attacks and the defense to mimo and ofdm systems. it should be possible to extend mimicry attacks to these systems as well, because the attacker can directly manipulate the training signals for ofdm and mimo systems with the knowledge of all channel information. however, mimicry attacks require to place a spy node close to the receiver. thus, it becomes much more difficult to launch mimicry attacks when the receiver is equipped with a mimo system, because the attacker has to place one spy node for each antenna to know the channel."
"different normalization approaches like normalization, standardization and transformation are applied separately on the dataset in order to study their impacts. standardization of the given variable is performed as: we identify the correlation matrix for all the given independent variables as:"
"where x is the training symbol for channel estimation. based on equation (9), we have h . thus, to make the receiver believe that she is at location 2 while she is actually at location 1, the attacker needs to make equation (8) hold. otherwise, the receiver can utilize the ratio of the estimated channel at the receiver to that at the receiver's helper to detect the immobility camouflage, as the ratio remains the same when the attacker is stationary (i.e., normal case) and changes when the attacker moves (i.e., immobility camouflage case). we now explore how this ratio differs in the normal case and an attack case through a real world experiment."
"with multiple hidden layers, we cannot maintain the number of nodes per hidden layer to be 1 because of connectivity problems. increasing the number of hidden layer nodes to 10 reduces the error rate. but, it is clear that the normalization of data helps in improving the error rate. adding many hidden layers just increases the time taken to complete the task. thus, it could easily be concluded not to go beyond three hidden layers. the poor performance of the deep learning in this training might be due to lack of substantial data, as it is known that big data is required for deep learning to train the system better. dataset 1 has very minimal data and thus causes deep learning to perform relatively poorly. the performance on dataset 2 with a more substantial amount of data is comparable with a few other techniques considered in this research."
"we can see that in normal situation, the real channels between the attack and the receiver when the attacker is at location 1 and location 2 are quite different, and the euclidean distance between them is 0.5290. however, when the attacker launches the virtual multipath attack at location 2, the estimated channel at the receiver is quite close to the real channel between the attacker and the receiver when the attacker is at location 1, and the euclidean distance between the two channels turns to as small as 0.0964. therefore, the attacker is able to effectively make the receiver believe that she is at location 1 while she is actually at location 2."
"we assume that the transmitter is malicious and aims to hide her location change or impersonate movements while she is actually static. to achieve this objective, the transmitter attempts to mislead the receiver through creating a virtual multipath channel, which can fool the receiver to estimate a fake wireless channel impulse response chosen by the transmitter. we assume that the malicious transmitter knows the training sequence used for the channel estimation."
the impact of a selected independent variable over the dependent variable is not considered in pca. principal component regression (pcr) is used to relate the selected principal components with the dependent variable (y). the coefficients can be represented as
"to demonstrate the virtual multipath channel, we first explain the multipath effect, which is the fundamental reason for the spatial uncorrelation property. wireless signals normally propagate in the air through multiple paths due to obstacle reflection, diffraction, and scattering [cit] . therefore, for wireless signals sent from different locations, the receiver can observe different channel characteristics from these signals, because they experience different multipaths and accordingly undergo different channel effects (e.g., power attenuation, phase shifting, and delay). to fool a receiver, the attacker needs to create an \"artificial channel\" that can exhibit a multipath propagation feature similar to the real-world multipath."
"1. motion camouflage: the attacker is moving but she aims to deceive the receiver about the moving activities. towards this end, the attacker makes the receiver believe that she is stationary by causing the estimated channel at the receiver to appear unchanged. 2. immobility camouflage: when the attacker is stationary, she wants to make the receiver believe that she moves to a new location by changing the estimated channel at the receiver. the typical example targeting this objective is the sybil attack, in which the attacker pretends to change her location and therefore identity while she indeed just changes the channel between herself and the receiver, as the receiver will observe differing channels between transmitters in different locations. in practice, the two objectives may happen alternatively. for attacks against ofdm systems, we propose a corresponding defense strategy for each attack goal."
"figs. 17 and 18 show the channel impulse responses estimated using x 1 and x 2 at the receiver and the helper, when the attacker and the helper are placed at locations 2 and 8, respectively. we can see that the virtual multipath attack leads to a much larger distance at the helper than the receiver, i.e.,"
"the accuracy of the decision tree algorithm remained similar on both normalized and non-normalized data. in multiple linear regression, the variance explained by the regression dropped for dataset 2. normalizing dataset 2 and applying multiple linear regression still caused the r 2 value to drop further. this was reflected in the high mean squared error."
"ls, where r h is the multipath channel correlation matrix (i.e., the statistical expectation of hh h ) and s 2 n is the variance of the noise [cit], both assumed prior knowledge. if the correlation matrix r h and noise variance s 2 n are both known, lmmse is used; otherwise, ls is used. we here focus on the ls estimator, because for location distinction schemes in a realistic environment, precise channel correlation statistics and noise knowledge are difficult to obtain due to the time-variant property of wireless channels and potential movements of wireless nodes."
"the simulation result demonstrates the theoretical feasibility of the virtual multipath attack. in section 7, we reveal the practical impact of such attacks with real world experiments."
"we examine three example attacks: (1) injecting a randomly chosen channel impulse response into the receiver, (2) reproducing a same channel impulse response in the crawdad data set; and (3) mimicking another location while hiding the true location. for all three attacks, we place the transmitter at location 2 shown in fig. 9 ."
"we give an example to illustrate how the attacker can create such a channel. fig. 1a shows a simple real multipath scenario, where a signal sent by the transmitter travels on two paths, i.e., the reflection path and the direct path. at time t 0, the receiver starts to receive the signal copy that travels on the direct path. the reflection path is longer than the direct path, and thus at a later time t 0 þ d t, the receivers receives the aggregation of the signal copy from the direct path and the one from the reflection path. now consider the scenario in fig. 1b : there is only one direct path between the attacker (i.e., a dishonest transmitter) and the receiver, but the attacker wants to make the receiver believe that two paths exist similar to the real multipath propagation shown in fig. 1a . to this end, the attacker sends the signal alone first. after duration d t, she superimposes a fresh signal copy onto the one already in transmission. the attacker scales both the original signal and the time-delayed copy by attenuation factors w 1 and w 2 to mimic the signal amplitude attenuation caused by real paths. consequently, the receiver observes an aggregation of one signal plus a time-delayed copy, with each undergoing a certain amplitude attenuation, and thus thinks that they are caused by the multipath effect."
"it is observed from figures 3 and 4 that normalization of the dataset causes the normality to be affected. thus, causing the drop in the variations of equations explained by the regression line. thus, we could conclude that normalization could not be expected to improve the adjusted r-square value always, as shown in table 2 ."
"the dataset was gathered and compiled using two methods: (1) data was collected from the system while specifically adding a fault to a certain valve; (2) data was collected from a simulated system using simulink. the first dataset contains 552 samples and the second 55004. the second dataset was created for the purpose of testing the behaviour of the selected methods with large datasets as it was not possible to collect more data from the real system. additionally, two extra features were removed as we could not accurately replicate them in the generated data. the presence of different independent variables with varied range could cause an impact on the various approaches studied. thus, normalization of the dataset is performed along the column so as to have a common level being maintained for all the rows of the dataset."
"(2) kolmogorov-smirnov test: the resultant p-value indicates that the distribution is normal. cook's distance was measured for both the normalized and non-normalized datasets. there was no specific indicator of influential elements identified by cook's distance, as shown in figures 5 and 6."
"the example scenarios where virtual multipath attacks may exist include: (1) movement detection: an attacker may hide its movement by creating a static virtual channel impulse response at the receiver, e.g., a wireless sensor can be moved from the monitoring area but the movement is not detected; (2) detection of sybil attacks: an attacker may bypass the detection of sybil attack by pretending identities that are originated from different locations; (3) authentication: the attacker may impersonate another wireless transmitter. this attack scenario requires the attacker to know the channel impulse response between the target transmitter and the receiver, and thus imposes some limitations to the attacker. however, since the virtual multipath channel attacks can produce any channel estimation results at the receiver, such attacks are still a threat to existing channel fingerprinting based authentication schemes; (4) in addition to the attack scenarios, on the other hand, the attacks can be further utilized to enhance the wireless security. for example, the virtual channels can be used to provide a rich set of shared keys between two wireless devices, or enable anonymous communications by protecting location privacy of wireless users via virtual channel camouflage."
"otherwise, the linear system lacks necessary coefficients to generate solutions. however, the acquisition of h 12 and h 22 will impose difficulty for the attacker, because the receiver's helper can be designed passive, i.e., it receives wireless signals but doesn't actively send out wireless signals to the channel. due to the close proximity, the receiver can communicate with its helper through the cable connection or internal circuit. a passive helper of the receiver eliminates the chance for the attacker to extract the channel impulse responses based on heard wireless signals."
"we repeated the simulation using all data in the crawdad data set. fig. 4 plots the empirical empirical cumulative distribution functions (cdfs) of the euclidean distance d real between the the chosen channel and the real channel response, as well as that of the euclidean distance d est between the chosen one and the channel impulse response estimated under the attack. we can see that the probability that d est is smaller than d real is high. in particular, 95.13 percent of d est is less than 0.2295, whereas only 1:59 percent of d real is less than this value. thus, if the receiver uses 0.2295 as the detection threshold to verify channel impulse responses, the receiver will get a mis-detection rate of 0.9513 and a false alarm rate of 0.9841 (i.e., 1 -0.0159)."
"water desalination is a vital industry especially, in environments that are facing lack of pure water. there is very limited research in the area of automatic fault detection in water purification plants, mainly due to the lack of datasets that include data from various faulty states. this research studies the process of identifying faults in water desalination. two different datasets were used with very similar features. one with the real values collected from the plant and one with generated values. the second dataset was created for the purpose of testing the behavior of the selected method with larger data. these two datasets were analyzed using different approaches such as decision trees, multiple linear regression, stepwise regression, principal component analysis, partial least squares regression and deep learning, both with and without normalization. multiple linear regression was found to be inappropriate for the second dataset. further, salient features from the given dataset were selected with pca. based on the analysis, five principal components are fo0und to be significant features to achieve 97.77% accuracy for both datasets with or without normalization. deep learning in the form of deep belief network was tried with varied number of hidden layers and hidden nodes per layer. the outcome of deep learning was not in accordance with our expectations in dataset 1, mainly due to lack of sufficient data. meanwhile, the performance of deep learning for dataset 2 is comparable with the other considered techniques. overall, the decision trees algorithm outperforms others considering the trade-off between processing time and accuracy."
"three-dimensional reconstructed volumes are visualized using a color-coded depth representation. the image intensity in each z-slice of the volume is multiplied by the corresponding element of a isolum color map 40, as shown in the supplementary fig. 1 . the resulting volume is then averaged along the axial direction to produce a color-coded depth map of the sample. this representation is particularly well suited for the visualization of such thin volumes."
"for the normalized data described in table 8, it could easily be observed that pc1, pc2, pc3, pc4 and pc8 are correlated better with the dependent variable."
"as mentioned earlier, the helper may use a threshold to enforce the detection. if d helper is larger than the threshold, then the attack is assumed. in general, detection and false positive rate are two performance metrics associated with a detection method. the detection rate is the probability that d helper is larger than the threshold when there is indeed an attack. the false positive rate is the probability that d helper is larger than the threshold when there is no attack."
"in the normal case when no virtual multipath attack occurs, when the estimated channel at the receiver is unchanged (i.e., the receiver is actually stationary), the estimated channel at the receiver's helper should maintain the same for both channel estimations, i.e., h"
"similar to the discussion of motion camouflage, we illustrate the defense against immobility camouflage using fig. 11, where the attacker is stationary while she aims to make the receiver believe that she moves. suppose that the attacker is stationary at location 1. let h r a and h h a denote the estimated channel at the receiver and that at the receiver's helper respectively when the attacker launches the attack. then, the attacker manipulates the transmitted symbol x a1 so that the following equation holds"
"we point out that the virtual multipath attack discovered in this paper doesn't target traditional localization systems using aoa, toa, rss, etc. thus, complementary analysis and measures are necessary to protect these systems. besides, in our future work, we will consider extending existing location distinction algorithms so that they can be adaptive to a more dynamic environment."
"total internal reflection fluorescence (tirf) microscopy offers a unique optical sectioning of areas adjacent to the glass coverslip. this technique, introduced by axelrod in the 80's 5, relies on an evanescent excitation produced in the total internal reflection regime. the fast decay of the evanescent field in the axial direction limits the observed region to a thin layer of a few hundred nanometers. this property makes tirf microscopy ideally suited to the observation of biological activities near the cell membrane 6 . moreover, an important advantage is that tirf microscopy does not require any specific dyes, psf engineering, or complex excitation processes, but it only requires a tilted illumination beam. this leads to high-quality (low out-of-focus signal and high signal-to-noise ratio) and fast live imaging of cell/substrate interactions."
"by adding delayed signals together, a virtual multipath attacker introduces inter-symbol interference to its transmission signals. we note that such signals are decodable at the receiver. it is common for a receiver to receive signals with inter-symbol interference due to the wireless multipath effect. a receiver normally uses channel estimation results to learn multipath channel conditions [cit] . the estimated channel impulse response is then used in the demodulation process to compensate the multipath effect and convert the self-interference signal into a meaningful message."
"second, both attacks differ in technical design methodology. the essential way of mimicry attacks is to manipulate the training signal such that the receiver believes an impersonated channel impulse response. such a manipulation at the training signal level fools the receiver to accept an incorrect channel estimate, but the data payload after the training signal still goes through the real channel. as a result, the receiver will use an incorrect channel estimate to compensate the real channel effect, leading to incorrect packet decoding. in contrast, the virtual multipath attack uses a delay-and-sum process (with chosen weights) to create a virtual channel and pass all the data (e.g., training sequence and data payload) to be transmitted through this virtual channel. the receiver then not only gets a faked channel impulse response, but also uses it to successfully decode the entire data payload. hence, the design methodology of virtual channel attacks ensures more stealthiness and consistency to fool the receiver."
"in this paper, non-binary turbo codes are proposed [cit] for improved error correction in the presence of impulsive noise and multipath. to the best of our knowledge, non-binary turbo *wael abd-alaziz is sponsored by the iraqi ministry of higher education and scientific research to study his ph.d."
"l ocation distinction in wireless networks aims to detect a wireless user's location change, movement or facilitate location-based authentication. enforcing location distinction is important for many wireless applications [cit] . for example, wireless sensor networks are usually utilized to monitor a target area by sensing the physical or environmental conditions (e.g., temperature, sound, and pressure). administrators of the sensor networks would like to enforce location distinction to prevent an unauthorized person from moving the sensors away from the area of interest. wireless networks are vulnerable to sybil attacks due to the broadcast nature of the wireless medium [cit] . here, an adversary forges a significant amount of fake user identities to fool a networked system. location distinction can tell whether or not all identities are originated from the same location, and thus detect such attacks. active radio frequency identification (rfid) tags are often used in warehouses for tracking inventory and maintaining the physical security. it has been assumed that \"location distinction is critical to provide a warning and to be able to focus resources (e.g., security, cameras, and personnel) on moving objects\" [cit] ."
"virtual multipath attackers are able to make the receiver believe any channel characteristic the attacker chooses. at the receiver, it seems that there is no way to tell whether the signal goes through real or virtual multipath scenario. hence, existing location distinction methods built upon distinguishing locations from channel characteristics (e.g., [cit] will be easily defeated by virtual multipath attacks. the intuition behind our defense strategy is that nobody can craft one key to open two different doors. in other words, if a receiver cannot tell whether there is an attack or not, maybe a second receiver can. as a result, the proposed approach makes use of an auxiliary receiver or antenna, which we refer to as a helper. the helper is placed more than half a wavelength away from the receiver to ensure a distinct channel characteristic. we let the receiver use two different training sequences x 1 and x 2 to estimate the channel impulse response alternatively. without loss of generality, we assume that the receiver uses x 1 to estimate the channel from the first transmission, and uses x 2 to estimate the channel from the second transmission."
"theoretically, the attacker can set an arbitrarily small delay (e.g., 1 nanosecond) to create a much richer virtual multipath effect at the receiver. however, modern channel estimation algorithms estimate only resolvable paths whose inter-arrival durations are no less than one symbol duration, and it has been shown that using the estimation of resolvable paths is sufficient to compensate the channel effect for signal demodulation. thus, at the receiver's point of view, the channel consists of multiple resolvable paths. this means that it is sufficient to set the delay in virtual channel generation to be one symbol duration (e.g., just generate resolvable paths) to fool the receiver's view on the channel. even if the attacker reduces the delay to generate a more fine-grained virtual multipath channel, the receiver can still observe the resolvable paths and the corresponding channel impulse response. thus, decreasing the delay can only add implementation complexity to the attacker, but will not cause more impact of the attack at the receiver. on the other hand, if attacker utilize a larger delay (e.g., larger than the symbol duration), the receiver may not observe enough multipath effect under the virtual multipath attacks and thus the attack impact is limited. therefore, it is reasonable to set the delay to be one symbol duration to balance the attack effect and complexity."
"to enhance the regression analysis, variable selection was performed using all possible regression approaches. criteria like adjusted r 2 and bic gave different results based on variable selection, making us to try stepwise regression for variable selection. stepwise regression identified five variables that are significant in dataset 1 but variable selection by forward and backward stepwise regression was not conclusive for dataset 2. this concern was further studied using pca. pca was able to achieve 99% and 97.77% accuracy using only five principal components. the outcome of pca was applied to pcr regression along with an f-test to study its validity. in order to handle multicollinearity, plsr was applied both on normalized and non-normalized datasets. it was observed that the number of principal components required to achieve 95% accuracy induced the variance increase from 4 to 6 for the normalized dataset. furthermore, deep learning was applied on the same datasets. deep learning with 1 to 5 hidden layers, with 1 or 25 or 100 nodes each, was trained and tested. the error rate was reduced when the number of hidden layers was increased to 10 but further increases in hidden layer numbers did not help with accuracy while increasing the computation time. similarly, adding more than three hidden layers did not enhance the accuracy."
"similarly, the virtual multipath attack can also easily defeat any method verifying that nodes are from different locations based on examining the difference of their channel impulse responses (e.g., [cit] ."
"as long as the attacker passes the training and the information payload through the same virtual channel as shown in fig. 2, the received signal at the receiver will go through the same combined channel effect of virtual and realistic channels. in this regard, although the receiver obtains the estimation of a fake channel impulse response, such an estimation result still represents the combined channel effect that the data goes through. therefore, the receiver will successfully decode the original message using this estimation result. the only impact of virtual multipath attacks is that the receiver is fooled by fake channel impulse responses."
"orthogonal frequency-division multiplexing (ofdm) is a popular wireless communication scheme that encodes the digital signal using multiple sub-carrier frequencies. these sub-carriers are normally narrow-band (e.g., 802.11 a/g physical layer advocates an ofdm sub-carrier bandwidth less than 0.5 mhz). thus, ofdm systems are robust against channel fading caused by the multipath effect. for an ofdm system, the channel estimation is done by estimating the channel impulse response of each sub-carrier. due to the lack of the multipath fading, the channel estimation result of each sub-carrier is a complex number rather than a vector, and the final channel estimation output of an ofdm system is formed by these complex numbers. in this section, we explore virtual multipath attacks and corresponding defenses in ofdm systems."
"the example in fig. 1b assumes that there exists only one direct path between the attacker and the receiver (i.e., no multipath effect is considered). in practice, the attacker's crafted multipath signal is affected by the real multipath effect as well, and she should have a way to deal with the impact of this real multipath. our research reveals that the attacker can easily achieve this goal by reverse-engineering existing wireless channel estimation algorithms and performing linear transformations on the original signal."
"similar to pcr, plsr is also said to handle multicollinearity efficiently. plsr was applied both on normalized and non-normalized data of both datasets. the proportion of variance shown in table 9, was obtained for the nonnormalized dataset 1. the proportion of variance shown in table 10, was obtained for normalized dataset 1."
"for the non-normalized data described in table 7, it could easily be observed that pc1, pc2, pc3 and pc5 are correlated better with the dependent variable."
"the pls approach is used to maximize the covariance between input variable x and output variable y. here, x is the pcs obtained using pca. similar to eq. 1, y is decomposed into"
"we point out that under this circumstance it is still feasible to detect virtual multipath attacks as long as the receiver has more helpers than the attacker. a significant advantage of the receiver over the attacker is that the receiver just needs to find contradiction to detect the attack; while the attacker has to know all channel information for signal manipulation to make sure no contradiction is found. in particular, when the receiver adds one more passive helper, it actually reduces the attack situation to the normal case. in order to beat the defense, the attacker must meet all the following requirements at the same time to beat the receiver: (1) add one helper, (2) add one spy node at the exact location of the receiver's new helper to know the channel information, (3) synchronize herself and all her helpers to transmit the manipulated signal at the physical-layer symbol level. hence, the attacker has much more costs to beat the receiver with more passive helpers."
"we demonstrated the potential of the proposed ma-tirf framework (system + calibration + reconstruction) for the study of fibronectin assembly beneath endothelial cells. co-localization experiments in which endothelial cells were co-stained with alexa-561-and alexa-488-coupled phalloidin, confirmed the ability of the approach to provide coherent axial information from multiple independent channels. this was corrobored by results from others experiments revealing the relative depth of the main actors of cell adhesion and fibronectin matrix assembly with nanometric precision. importantly, the reconstruction of intra-and extra-cellular stainings of the α5β1 integrin showed an axial separation in the order of magnitude of the previously estimated length of integrin in an extended-open conformational state. furthermore this shift was consistently observed in different experiments. altogether, these results strengthen the significance of ma-tirf microscopy, and the proposed framework, for shedding light on the process of fibronectin fibrillogenesis as well as other dynamic phenomena near the cell membrane."
"(1) shapiro-wilk normality test: due to restriction on the sample size (3 to 5000), this test is applied only on dataset 1. this confirms that the sample is obtained from a population that follows a normal distribution."
"microscopy setup. the microscope has been developed based on a ti-e inverted microscope base (nikon) equipped with a 100x/1.49o objective with a fully customized reflexion path. the laser bench ( fig. 2a green part) has been equipped with 405 nm diode laser (oxxius, lannion, france), 491 nm dpss laser (cobolt, solna, sweden), 561 nm dpss laser (oxxius) and 640 nm diode laser (vortran laser technology inc., sacramento, ca usa). the aotf (aa optoelectronic, orsay, france) controls the laser power for dpss while the two diodes are directly driven. laser lines are combined taking into account their respective polarization. the laser bench is coupled to the illumination device with a polarization-maintaining optical fiber to optimize the homogeneity of illumination (note that direct coupling is enable for high power purpose). the illumination device is built around 2 galvanometers (harvard apparatus, les ulis, france) conjugated with the sample. a first beam expander (4x) let to fill the mirrors while the second one (3x) let to fill the tube lens (200 mm na/0.15). fluorescence signal is selected by a quad band dichroic filter for excitation and a filter wheel (sutter instruments, novato ca) for emission. acquisition is achieved on an emccd (ultra from andor, belfast, northern ireland). for bfp imaging, a second emccd (ixon2 from andor) is used."
". if an attack based on immobility camouflage occurs, the attacker aims to make the receiver believe that she is at two different locations. we introduce a new metric, called ratio proximity and denoted with h, to demonstrate how close the two ratios are. in order to make h range between 0 and 1, we divide the minimum valued ratio by the maximum valued ratio."
"existing schemes in general compare the difference between the receiver's current estimated channel and previous reference channel with a threshold to check a location change [cit] . since our attacker can inject any random channel impulse response into the receiver with a very high accuracy, the performance of existing location distinction schemes can be significantly degraded by the virtual multipath attack. for example, given a threshold set less than 0.5 for location change detection in our system, when the attack is launched, the receiver will think that the transmitter moves because all the differences between the estimated channel in the presence of the attack and the reference channel (attack-free channel) exceed the threshold of 0.5. however, the estimated channel and the real channel are actually measured at the same location, and thus the location distinction false alarm rate is raised to 100 percent under the virtual multipath attack."
"normalization needs to be performed when joining the actual faulty data with the simulated, non-faulty data of the amalgamated data set, as seen in table 1 ."
"fig . 3 plots the real channel impulse response h between the attacker and the receiver, the chosen channel impulse response h a that the attacker wants to emulate, and the channel impulse response h r estimated by the receiver. we can observe that h a is very close to h r under the virtual multipath attack."
"we assume that the channel impulse response is stable in a short period of time (e.g., a packet duration), which is a common assumption for designing wireless communications. we further assume that the malicious transmitter knows the actual channel impulse response between herself and the receiver. this can be achieved by estimating the channel impulse response from the wireless signals (e.g., location distinction inquiries) emitted by the receiver."
"three-dimensional reconstruction. the image formation process through a tirf microscope follows the model despite its high optical sectioning capability at the vicinity of the cell membrane, tirf microscopy does not provide quantitative axial information on the three-dimensional density of fluorophores"
"the correlation between various principal components and the original dependent variable is showed in the table 5 . it could easily be observed from table 5 that pc1, pc2, pc3 and pc5 are correlated better with the dependent variable."
"in the third example, the attacker performs actual location mimicking, mimicking location 1 from location 2 shown in fig. 9 . the attacker first records the real channel impulse response between herself and the receiver when she is at location 1, and then mimics this obtained channel impulse response when it moves to location 2. fig. 15 plots the real channel impulse responses between the transmitter and the receiver when the transmitter is at location 1 and 2 respectively, as well as the estimated channel impulse response at the receiver when the attacker performs the attack."
"estimating channel impulse responses is a must-have function for most modern wireless systems [cit] . note that the signal propagation paths are unresolvable (i.e., each multipath component signal can not be extracted from the composite signal) if the differences between the arrival times of the signals traveling on these paths are much smaller than the symbol duration, which is the transmission time of a wireless physical-layer unit [cit] . hence, existing channel estimation algorithms assume a resolvable multipath, i.e., the arrival times of signal copies traveling on different paths are larger than the symbol duration. channel impulse responses are usually estimated using training sequences [cit] . specifically, the transmitter sends a training sequence (i.e., a sequence of bits) over the wireless channel, while the receiver uses the same training sequence and the corresponding received signal samples to estimate the channel impulse response. the training sequence can be pre-shared [cit] or reconstructed from the received signal [cit] ."
"the majority of studies adressing the structure and dynamic regulation of integrins and integrin-based cell adhesions have been performed using cells plated on coverslips coated with immobilized ligands (e.g. plasma fibronectin or fibronectin fragments containing the cell-binding rgd sequence). the fact that cells produce their own matrix proteins and assemble a pericellular matrix, that is fibrillar in the case of fibronectin, is generally overlooked. however, this process is of particular importance in endothelial cells in which basally-directed secretion and assembly of cell-produced fibronectin is required for vascular remodeling during development and in diseased states [cit] . as illustrated in the spinning disk images of fig. 6a,b, the topologies of adsorbed plasma fibronectin ( fig. 6a ) and fibrillar cell-derived fibronectin ( fig. 6b ) beneath endothelial cells are drastically different. distances between the coverslip and the ventral surface of cells spread on these substrates vary as well. using our ma-tirf system, not only can the organization of fibronectin beneath cells be visualized, but the thickness of the matrix deposited by cells and the z-position of associated cellular proteins can be measured. we previously showed that ilk is required for the maturation of α5β1-based focal adhesions into fibrillar matrix-forming adhesions associated with the assembly of secreted fibronectin 25, 34 . thus 4 h after plating cells on adsorbed fibronectin, ilk was mostly localized in focal adhesions less than 100 nm from the coverslip (figs 5 and 6a ). however, after 48 h in cells on endogenously-produced fibronectin, both ilk and α5β1 integrin localized in fibrillar adhesions. these α5β1-based matrix-assembly structures were higher than 350 nm above the coverslip, as determined by α5β1 staining (fig. 6c ). the observed difference in depth of the integrin between 4 h and 48 h, schematized in fig. 6b (bottom), is related to the thickness of the fibronectin matrix. the correlative histogram of fig. 6c reveals a parallel shift of approximately 70 nm between intra-and extra-cellular α5β1 epitopes (corresponding to their consistent difference in altitude at 4 h and 48 h). this difference is of the same order of magnitude as the vertical separation observed between intra-and extra-cellular stainings of integrin α5β1 in cells plated on immobilized fibronectin. altogether these results highlight the importance of providing extended depth measurements for the study of adhesion events at the ventral surface of cells and the advantage of our ma-tirf, with respect to classical tirf microscopy."
"one of the earlier works considering the multi-stage flash (msf) water desalination process was the development of its steady-state mathematical model [cit] . the relationships between various parameters like thermal performance ratio, specific heat transfer surface area, etc were determined. assuming constant heat transfer surface area per stage, the impact of the variation of water's physical properties like temperate, salinity, etc are analyzed. the model was found to conform to data obtained from six msf desalination plants."
"an adjusted r-squared result of 0.9255 was obtained when using all of the principal components as input to pcr regression. thus, the adjusted r-square obtained is the same as the one achieved in multiple linear regression. at the same time, the principal components pc8 and pc10 have higher pvalue compared to the level of significance. considering only pc1, pc2, pc3 and pc5 as independent variables in linear the comparison between all pcs based regression schemes and the selected pcs based regression was studied using anova. the selected pcs based regression gave an f-value of 17.931 and a very small p-value with 547 degrees of freedom."
"thus, if the successive estimated channel impulse responses show dramatic changes in a short time at the helper, the helper then triggers an alert at the receiver regarding the existence of potential virtual multipath attacks. in practice, the helper may use a threshold to enforce the detection. if jjĥ help 1 àĥ help 2 jj is larger than the threshold, then the attack is assumed. the threshold can be selected based on the empirical studies to achieve an optimized detection accuracy. in section 7.4, we show an example of the threshold selection. note that in the defense system, the helper and the receiver can switch their roles, i.e., if the attacker attempts to fool the helper instead of the receiver, the receiver will estimate two different channel impulse responses and therefore detect such an attack."
"where γ t is the probability transition metric between s 0 and s 1, α t is the forward recursion, β t is the backward recursion."
"the receiver can normally use one passive helper, i.e., a secret wireless tap, to detect the attacks. the exception happens when the attacker knows all channel information from her and her helpers to the receiver's passive helper (by placing a spy node co-located with or extremely close to the receiver's helper), which is in fact a very harsh requirement for the attacker."
"in the second example, an attacker aims to generate a channel impulse response in our office building such that the generated channel impulse response is exactly the same as one in the crawdad data set, which was collected in an office building in the university of utah. we note our usrp system is different from the crawdad measurement system, sigtek model st-515, which has a much higher bandwidth (40 mhz) than the usrp (10 mhz). therefore, the crawdad measurement system can observe richer multipaths. nevertheless, even with a relatively low-end usrp, we can still duplicate the resolvable paths in a channel impulse response measured in the crawdad data set. specifically, we select one channel impulse response (between nodes 14 and 43) from the crawdad data set and we plot it as \"crawdad channel\" in fig. 14. we can see that this channel impulse response carries three peaks and thus exhibits three resolvable multipaths. we launch the virtual multipath attack to make a replica of the same three resolvable multipaths observed at the receiver in our experiment, which is shown as \"crafted channel\" in fig. 14 . the attack's crafted channel impulse response of the resolvable multipaths closely matches the crawdad channel response and their euclidean distance is as small as 0.0036."
"deep learning was used to test the learning curve of the given datasets, in both normalized and non-normalized form. deep belief network (dbn) provided by deepnet package of r language was used for purpose. the hidden nodes were given as 1 or 25 or 100 nodes each in one to five hidden layers. the number of hidden nodes in each hidden layer is maintained as constant. the outcome of deep learning on different datasets are given in table 11 ."
"in our study, however, we discover a new attack against all existing location distinction approaches built on the spatial uncorrelation property of wireless channels. by launching such an attack, the adversary can generate any chosen wireless channel characteristics at a target receiver to deteriorate the location distinction capability of the receiver. the key idea of the discovered attack is to create a virtual multipath channel as undetectable camouflage to make the receiver believe a specified channel characteristic chosen by the attacker."
"however, equation (8) rarely holds in practice as the real channel impulse response is uncontrollable and unpredictable. a real world experiment is presented to demonstrate this in section 6.2.2. therefore, when the receiver realizes that its two successive estimated channels are the same, it should discern one of two possible reasons: either the attacker's location is not changed, or the attacker wants to achieve motion camouflage. meanwhile, if a difference between the two channel estimates can be observed at the receiver's helper side, the virtual multipath attack aiming to achieve motion camoufage is discovered."
"first we show an attack with intent to generate a random channel impulse response. fig. 13 plots the real channel impulse response between the transmitter and the receiver, the channel impulse response chosen by the attacker, and the estimated channel impulse response at the receiver. the y-axis and the x-axis indicate the power gain and the relevant path respectively. we can see that the chosen channel impulse response and the estimated one are very similar to each other, but both of them significantly deviate from the real channel. the euclidean distance between the chosen channel and the real channel is 0.3025, whereas that between the chosen channel and the estimated channel is as small as 0.0686."
"in this research, the flow of operation is performed as described in figure 1 . raw data, pre-recorded from the desalination system, is obtained and used as input to the feature selection system. the important features are selected from the raw data. with these salient features, the raw data is labelled as normal or faulty data using an expert system. this combined normal and faulty data is passed to the learning system, which learns using decision trees, regression or other approaches described below. then, the test data is fed into the learned system and the data is classified as normal or faulty data. the impact of collecting normal samples along with faulty samples cause lots of issues. within a given dataset, the number of different faulty samples and normal samples used might not reflect the actual composition of data in a specific period of time."
"the defense approach functions based on a critical observation that the attacker cannot fool both the receiver and the helper at the same time. thus, in our feasibility evaluation, we would like to examine how the channel estimation results of the receiver and the helper differ from each other, so that such an inconsistency can reveal the existence of the virtual multipath attack. towards this goal, we perform the following experiment."
"where x a1, x h1, x a2, and x h2 are the actual signals to be transmitted by the attacker and her helper for the first and second transmissions. to break the proposed defense, the attacker must solve them from equation (6). this implies"
"including all the principal components into the regression as pcr, resulted with an adjusted r-squared value of 0.5365. thus, the adjusted r-square obtained is the same as the one achieved in multiple linear regression. all the principal components have achieved the required p-value. using pc1, pc2, pc3 and pc5 as principal components, the adjusted r-square value drops to 0.4256. the comparison between all pcs based regression and the selected pcs based regression was studied using anova. the results for the selected pcs based regression gave an f-value of 2990.1 and a very small p-value with 49999 degrees of freedom."
". similarly, when the attacker moves from location 1 to location 2, we can obtain the estimated channel h h a 2 at the helper under the attack"
"the summary of all of the above processes is given in table 12 . in order to compare the performance of various approaches, the given datasets are tested with and without normalization of the data. furthermore, the error rate, accuracy, adjusted r 2 and processing time are analysed. better accuracy is expected from any process given an acceptable execution time. this duration is vital when the processing of the test data is to be in real time. adjusted r 2 provides information regarding the variation in the system."
"principal component analysis is mainly used to reduce the dimensionality of the given problem. the basic concept behind the pca is the use of orthogonality of vectors and to obtain the vectors considering the variance of projections. this causes,"
"after identifying the salient principal components involved, it was determined that there is no one-to-one relationship between a principal component and the original independent variables. thus, reducing the principal component does not reduce the considered independent variables and thus, all the independent variables are considered in the pca."
"ideally, a successful attacker should have a large value of d real (indicating that the attacker's chosen channel significantly differs from the real channel) and a small value of d est (indicating that the attacker's chosen channel is close to the receiver's estimated channel)."
"in this section, we describe how to create a virtual multipath channel to defeat location distinction algorithms. the attacker can launch two types of attacks. in a basic attack, the attacker can use any weights to craft a virtual multipath signal. this will fool the receiver to obtain random, incorrect estimates of the channel impulse response. in an advanced attack, with the knowledge of the real channel impulse response between herself and the receiver, the attacker is able to compute exact weights that make the receiver estimate the chosen channel impulse responses specified by the attacker. in the following discussion, we focus on the advanced attack due to the more misleading nature of such attacks."
"in order to reduce the number of independent variables involved in regression, stepwise regression was applied on both the datasets. forward as well as backward stepwise regression are used to reduce the variables involved in regression. based on the selected variables, adjusted r-square and mean square error were calculated, as shown in table 3 . based on the observations provided in table 3, the selection of five variables (tih, toh, toc, poh, qh and qc) for dataset 1 are sufficient to match the required level of significance. but, dataset 2 has still issues in achieving the required level of significance, so principal component regression is considered to solve this issue."
"considering the significance level of 0.05, certain independent variables have higher p-values. the observation from these two datasets, shown in table 1, indicate that dataset 2 has only 78.63% variance according to the regression equation. the presence of a fewer number of independent variables and more observations have caused the p-value of only one element to increase. upon observing the datasets, it is clear that the values of dataset 1 have substantial differences in their mean values. the values of qh and qc in dataset 1 are higher and thus their coefficients are 9.475e-05 and -2.960e-04 respectively, causing very little impact."
"the physical layer channel estimation can be processed in either frequency (e.g., [cit] or time domain (e.g., [cit] ), which are inter-convertible due to the linear relation between the two domains. in the following, we describe the channel estimation method in the time domain."
"a fast and flexible ma-tirf pipeline. one of the main novelties of the proposed approach concerns the joint reconstruction and deconvolution which improves the lateral quality (deconvolution) while reconstructing super-resolved information in the axial direction. the achieved improvement can be appreciated in the height map shown in fig. 1, which depicts actin fibers at the basal surface of cultured endothelial cells. the actin filaments are considerably more contrasted when the joint estimation algorithm is deployed."
"location distinction using wireless physical layer information has been extensively studied during the past several years (e.g., [cit] . scientists have discovered that wireless channel characteristics become uncorrelated every half carrier wavelength over distance (spatial uncorrelation property) [cit] . this property has been widely explored and adopted to enforce location distinction of wireless devices (e.g., [cit] . specifically, changes of wireless channel characteristics have been utilized to identify location changes of a wireless transmitter."
". note that though the attacker is actually at the new location (i.e., location 2), the channel estimation result that the attacker would like to generate at the receiver is still h r a so that the receiver believes that the attacker is stationary (i.e., remaining at location 1)."
"considering only 5 principal components, we could achieve 97.77% percent of the variations. it is worth mentioning that using only the first principal component achieves only 37.3% variance. the correlation between various principal components and the original dependent variable, both for normalized and non-normalized data, is showed in tables 7 and 8 ."
"to examine the overall attack impact, we perform the following experiment. for each location in fig. 9, we estimate the channel impulse responses during a short time window (around 10 -30 seconds). for each estimates, we perform 100 trials, and in each trial we randomly generate a length-5 vector whose elements range between 0 and 1. this vector is used as the attacker's chosen channel impulse response. we then launch the virtual multipath attack and record the euclidean distance d real between the chosen channel impulse response and the pervious channel impulse response estimated in the absence of the attacks (i.e., the real channel response), and also record the euclidean distance d est between the chosen one and the channel impulse response estimated under the attacks. we repeat the same experiment for the other nine locations."
"the paper is organized as follows: section ii-a introduces middleton class a distributions. in section iii, the encoding and decoding of non-binary turbo codes are presented. in section iv the system model components are explained in detail. in section v, the simulation results are shown. finally, section vi offers our conclusions."
"we discover that for both transmissions, at the receiver, the virtual channel created by a malicious transmitter (i.e., the attacker) can result in the same estimated channel impulse responses (equal to the one chosen by the attacker). however, at the helper, the virtual channel leads to different estimated channel impulse responses. we summarize the defense approach in fig. 5 . the reason that the attacker cannot fool both the receiver and the helper is detailed next."
"calibration and model validation. relevant reconstructions are made possible only with a proper calibration of the incident angles used during the acquisition. to that end, we deployed an approach that we previously proposed in a preliminary communication 31 . it involves estimating the relation linking the tension applied to the galvanometric mirror and the incident angle, from the recording of the objective back focal plane. the latter exhibits a ring whose radius is directly related to the incident angle 16, 21, 31 ."
"for this case, since the attacker changes the channel estimation result generated at the receiver, based on equation (7), the estimated channel at the receiver's helper changes correspondingly. thus, merely observing the difference of two channel estimates at the receiver's helper side is not feasible to distinguish immobility camouflage. instead, we still propose to use a passive helper at the receiver side but observe the ratio between the estimated channels at the receiver and at the receiver's helper to identify immobility camouflage."
"where x k is the data after the mapping process, n is the number of sub-carriers and t s is the active symbol interval. due to the effect of multipath, the receiver will receive many copies of the original signal with different delays. at the receiver, after adding the middleton class a noise, the received signal will be processed by the blanking or clipping operation to reduce the effect of impulsive noise. blanking is a non-linear process that is used to reduce the impulsive noise effect on the received signal y and this block is also shown in fig. 4 . after blanking, the received signal is given as"
"to defend against this attack, we propose a detection technique utilizing an auxiliary receiver (or antenna) at a different location to identify the virtual multipath channels and the fake channel characteristics. specifically, the attacker must craft its transmitting signal to make the target receiver believe a particular channel characteristic. however, we show that this crafted signal exhibits inconsistent channel characteristics to the auxiliary receiver. based on this result, we create a defense scheme that does not require the receivers to have any prior knowledge about the real channel characteristics between themselves and the transmitter."
"to illustrate the defense against motion camouflage, we use fig. 10 as an example, where the attacker is previously at location 1 and then moves to location 2, and she wants to make the receiver believe that she is stationary. let h r l i and h h l i denote the actual channel characteristic between the attacker and the receiver and that between the attacker and the helper when the attacker is at location i, respectively. let h r a denote the fake channel estimation results that the attacker would like to generate at the receiver. based on equation (7), we can obtain the estimated channel h h a 1 at the helper when the attacker is at location 1 as h"
we build a prototype channel measurement system to demonstrate the impact of the identified attack and the effectiveness of the proposed defense. our prototype is implemented on top of usrps [cit] . the software toolkit is gnuradio [cit] .
"1) standardize the independent variables, as specified in eqn. 1. 2) transform the data by multiplying the it (nx p matrix; n observations with p independent variables) with the eigenvectors (p x p matrix)."
"despite the ease for an attacker to extend virtual multipath attacks to ofdm systems, as described above, there are no straightforward ways to extend the previously discussed detection approach to these systems, because the channel estimation of an ofdm system is significantly different from that of a traditional communication system. let h r i and h h i denote the actual channel characteristic between the attacker and the receiver and between the attacker and the helper, respectively. let x i 1 and x i 2 denote the ith element of the first and second training sequences. denote the fake channel estimation results that the attacker would like to generate at the ith sub-carrier of the receiver and the helper. the conditions for the attacker to launch the attack without being detected are summarized as"
"the plc channel exhibits severe impulsive nature, ofdm can spread the noise over all sub-carriers during the fft process. hence the noise after ofdm demodulation can be regarded as gaussian but with a difference variance and the channel log-likelihood ratio (llr) can be calculated as"
"algorithm implementation and hardware ressources. the proposed reconstruction method has been inplemented with matlab (the mathworks inc., [cit] ) within the framework of the globalbioim library 39 . reconstructions have been performed on a dell alienware computer (intel core i7-7820x processor). gpu reconstructions were made using a nvidia (geforce gtx 1080 ti) graphic card."
"assume that the receiver is not aware that the original training sequence has been manipulated by the attacker. he thinks that the length of the training sequence is m, the number of paths is l, and hence the number of corresponding received symbols should be m þ l à 1. the receiver then uses the first received m þ l à 1 symbols to calculate the channel impulse response. let y 0 a denote the vector formed by these symbols and we can represent y thus, using matrix operations, we have"
"we discover that multipath propagation can be artificially made in a lab environment, and create a technique that can successfully generate virtual multipath channels. based on the virtual multipath channel, we identify a new type of attack that can defeat all existing location distinction algorithms using the spatial uncorrelated property of wireless channels. we create a defense technique to detect such attacks and protect location distinction systems. we specifically explore such attacks in ofdm systems and craft corresponding defenses according to the objective of attackers. we implement real-world prototypes to examine the practical impact of the attacks and the effectiveness of the proposed defense method."
"where t is the orthogonal score and p is load principal components are obtained from the given independent variables and they are used for classification. pca can do the classification considering the multicollinearity among independent variables which is one of the shortcomings in the linear regression discussed above. thus, the correlated independent variables are converted to non-correlated principal components. at the same time, we could also reduce the number of variables involved. principal component analysis is performed using the following steps:"
"overall, it can be observed that decision trees provided better accuracy than other algorithms in this application, given our data sets. even though the time required by regression is shorter for both datasets, it yields poorer accuracy, especially for dataset 2. considering the trade-off between processing time and accuracy, decision trees outperform all others."
"to launch virtual multipath attacks, the attacker requires to sum all delayed signal components with weights, as shown in fig. 2 . this delay-and-sum process can be easily implemented using software (e.g., designing a delay-and-sum c++ module in gnu radio for usrp) or hardware (e.g., using flip-flop components to delay signals and using accumulators to sum all signal components in fpga). such an architecture does not significantly incur software or hardware complexity."
"there are various optimal and sub-optimal algorithms used to decode binary turbo codes on the additive white gaussian noise (awgn) channel [cit] . in this paper, the max-log-map algorithm is employed to decode non-binary turbo codes on impulsive middleton class a noise channels. basically, this algorithm finds the maximum probability input symbol by estimating the probability of each trellis edge that corresponds to one of the q inputs. the probability of the state transitions from the state s 0 at time t − 1 to state s 1 at time t is given as"
"to compare localization of different dyes in a single sample, it is also crucial for the setup to be achromatic. to test this, cells were stained with a mixture of phalloidin coupled to alexa-561 or alexa-488. data were acquired and reconstructed independently. the results are strikingly similar in the two channels, as shown by the color-coded depth representations (fig. 3c ). this accurate co-localization is corroborated by the 2d-histogram ( fig. 3d ), indicating the relative depth between phalloidin in the two channels, which shows an almost perfect diagonal pattern. finally, as expected, actin fibers were in close contact with the coverslip at focal adhesions near the termini of stress fibers (white arrows), whereas their nanometric height increased towards the center of the cell. phalloidin confirmed the robustness of the method (cross-correlation showed global drift of less than 8 nm) and its ability to allow precise comparison of the axial location of different stainings. to highlight this, we labeled integrin α5β1, a transmembrane receptor for fibronectin, with different fluorophores on the extracellular and intracellular domains of the protein. staining was performed in endothelial cells on immobilized fibronectin with antibodies that recognize an epitope in the ectodomain formed by α and β subunits of the heterodimeric integrin (anti-α5β1) and antibodies against the cytoplasmic domain of the α5 subunit (anti-α5). by classical tirf microscopy it is not possible to discriminate between stainings of intra-and extra-cellular epitopes of α5β1 localised in focal adhesions (yellow structures shown in the composite of fig. 4a ). in contrast, intracellular anti-α5 staining (in green) can be clearly distinguished from extracellular anti-α5β1 staining (in red) in the axial section depicted in fig. 4b (left), corresponding to the reconstructed ma-tirf stack (400 nm depth stack with 20 nm axial steps), along the arrow in fig. 4a . in ma-tirf microscopy, as opposed to confocal microscopy, axial localization represents an absolute value with respect to the coverslip. thus, the axial height of α5 staining was found to be 50 nm to 100 nm greater than that of α5β1. this result is in agreement with estimated dimensions reported for intact ligand-bound integrins in an extended-open conformation 32, [and references therein] as schematized in fig. 4b (right) . the integrin is in close proximity to the substrate-bearing coverslip when clustered in focal adhesions (asterisks, fig. 4b ) and beneath actin fibers that flank the nucleus. vertical localization of external and internal epitopes of the integrin across the ventral surface of cells can be visualized in the depth maps shown in fig. 4c,d, with colors indicating the z coordinates (0-200 nm) relative to the coverslip surface. axial positions of actin-rich structures are highest at the periphery of cells and lowest at the tips of stress fibers linked to integrin α5β1 in adhesion plaques (fig. 4e, 0-350 nm depth). we next determined the axial position of another component of cell-matrix adhesions, ilk, involved in linkage of the β1 integrin cytoplamic tail to the actin cytoskeleton, in cells plated on immobilized fibronectin (fig. 5 ). it is noteworthy that the adsorbed integrin ligand forms a thin coat with mean altitude less than 20 nm (95%+/−1% of the signal is in the layer 0-20 nm, fig. 5a ). z-coordinates ilk ranged from 50 nm to 200 nm and displayed a split distribution, above or below 100 nm. it is clear from the color-coded altitude maps that the lower set of molecules corresponds to the presence of ilk in the rim of adhesive structures (called the focal adhesion belt 33 ) closest to the fibronectin-coated coverslip (yellow and orange staining, fig. 5b ) and more centrally located adhesions (orange staining, fig. 5b ). the upper population of ilk molecules between 100-200 nm above the coverslip corresponds to diffuse or vesicular staining (pink and blue, fig. 5b ) and thus displayed a greater variation in z median values. the broadest signal in the z axis with the highest vertical distribution was obtained for f-actin, with an average height greater than 300 nm across the cell (fig. 5a )."
"in such situations with highly correlated data, the variance is increased and it is indicated as multi-collinearity in regression literature [cit] . the presence of multi-collinearity in our dataset is studied using pca and pls."
"the helper and the receiver estimate the channel impulse responses from two successive transmissions, then calculate the euclidean distance between both estimates. let d helper and d rec denote the distances computed by the helper and the receiver, respectively. as analyzed in section 5.1, d helper should be much larger than d rec ."
"a similar pattern of having cumulative variance of more than 1 was observed for both the normalized and nonnormalized versions of dataset 2, but the number of principal components required to achieve 95% explained variance (r 2 ) increased from 4 to 6, when the dataset was normalized. the impact of normalization on reducing the number of principal components is not consistent."
"given more than one independent variable (x) and one dependent variable (y), a linear relation between them could be generated using multiple linear regression. this relationship is given by"
"we generate two 64-bit training sequences x 1 and x 2 . for the first and the second transmissions, we compute the weight vectors w 1 and w 2, so that the corresponding virtual channels will result in estimated channel impulse responses that are equal to h a at the receiver. as discussed earlier, these weight vectors should be computed based on h, h a, x 1, and x 2 . fig. 7 shows the channel estimation outcomes at the receiver for the first and the second transmissions, respectively. we can see that both estimated channel impulse responses are consistent with each other. the euclidean distance between them is 0.1127. we also calculate the channel estimation results at the helper. as shown in fig. 8, these channel estimates significantly differ from each other. the euclidean distance between them is as high as 0.5701, which is out of the normal range of variation of the channel impulse responses. thus, the virtual multipath attack is detected."
"principal component analysis is applied on the datasets, with and without normalization. pca considered 10 components with different combinations of the given 10 independent variables in dataset 1, shown in table 4 . considering only 5 principal components, we could achieve 99% percent of the variations."
"as we still desire a solution with multiple zero rows, the group sparsity constraint can be enforced on α. apparently, the zero rows of x can be indicated by those of α. now we can reformulate eq. (7) into a large scale version as follows:"
"circle) is compared to the weight vectors of the trained som map (the squares of figure 1 ), the algorithm defines the closest unit according to the following equation:"
"finally, we analyzed quantitatively the experiments with artificial data, with an average classification accuracy defined by a dataset with 10 6 objects (which were used to generate the decision boundary of figure 4) . the results are shown in the conclusion for the experiments using artificial dataset is that the use of nn is more effective than nn when the separation class has high confusion and that for this, the performance accuracy has not been abruptly reduced. we also note in this qualitative analysis that the use of som as prototype generation method does not significantly degrade the accuracy performance. the use of artificial datasets can make qualitative and quantitative analyses between the classifiers. the next experiment has the objective of expanding the previous study [cit] through analysis with other performance measures, such as kappa, impact of dataset reduction on the accuracy, and performance of classification time. for these new experiments, 12 new public datasets were used that are benchmarking in prototype generation approach [cit] ."
"the informativity calculation has a high computational cost because, in addition to comparing the object under analysis with the objects of the training set (first part in (7)), the algorithm still requires a comparison between the training set objects (second part in (7)). in order to reduce the computational effort, [cit] suggest having the execution of the nn algorithm before executing the nn to define a reduced dataset with most similar objects, according to the euclidean metric. however, the nn algorithm has the disadvantages presented in the introduction (the need to store the training set, noise sensibility, etc.) and its use before the nn can affect the performance in the classification of objects that are in a border region, as illustrated in figure 2 (c)."
"where x * is the reconstruction coefficients. as shown in fig.2 (a), a normal event (the up one) is likely to generate sparse reconstruction coefficients x *, while an abnormal event (the bottom one) is dissimilar to any of the normal bases, thus generates a dense representation. to quantify the normalness, we propose a novel sparse reconstruction cost (src) based on the l 1 minimization, as"
"therefore, the primary objective of the classifier addressed in this paper was the maintenance of the accuracy of the nn and the reduction of the classification time in a classification process, thus concluding that the use of the objects represented by the winning neuron and adjacent neurons was effective in the analytical aspects by not degrading the performance of nn. the results presented in table 3 : classification results for all algorithms represented by mean and standard deviation of accuracy and kappa measures. another important conclusion in analysis of the classification experiments, mainly using artificial dataset, and also in benchmarking dataset where the accuracy performance was worst, the nn approach presents more significant accuracy results when the objects of different classes are not well separated, with high mixture in the border region."
"ii. to increase computational efficiency, a novel dictionary selection model based on group sparsity has been designed to generate a minimal size of bases set and prune noise training samples. moreover, the lower rank constraint is considered to handle the large scale problem caused by large scale training samples."
"to handle different abnormal events, lae or gae, we propose several type of bases with different spatio-temporal structures, whose representations by the normalized mhof is illustrated in fig.5 . for gae, we select the spatial bases that can cover the whole frame. for lae, we extract temporal or spatio-temporal bases that contain spatio-temporal contextual information, like the 3d markov random field [cit], and spatial topology structure can replace co-occurrance matrix. in general, our design of the local and global features is very flexible and other alternatives are certainly possible. moreover, several features can be concatenated to build a more advanced description."
"the classification method proposed herein explores two important characteristics of the som: vector quantization and topological ordering [cit] . for better understanding these features, consider the representation of figure 1 with input patterns objects (filled circles) used for training a som map and the weight vectors of each neuron (squares) after the training phase. in this figure, each weight vector represents a microcluster of input patterns, which is a quantization characteristic. the relationship between the weight vectors can be interpreted as a boundary, which can be understood as a voronoi region, as exemplified by the shaded area in figure 1 . in operational aspects of use, this can be considered in a classification process in which the strategy, introduced and explored herein, means to establish a two-step process. in the first step, when a test sample x (see figure 1, the unfilled u x t figure 1 : the border between the weight vectors (squares) can be interpreted as a voronoi region (shaded area). thus, the input patterns object (filled circles) belongs to a voronoi region."
"in order to contribute to the [cit], in this paper, the nn has been substituted by som because of quantization vector and maintenance topological of raw dataset. in other words, a som map is trained with the dataset and, after this, the objects of this set are associated with the nearest (or winning) neurons. and, thus, each neuron of the map or prototype represents an object subset. now, in a classification process, the object is compared with the map prototypes, where the winner is elected. the objects mapped in this winning neuron and adjacent neurons are retrieved and presented to then have the execution of nn."
"the approach utilized by the som nn classifier explores the concept of quantization, topology maintenance, and informativity. as already mentioned, an informative object allows the correct prediction of an unknown object, even in boundary not well defined. when talking about information, we cannot have information quality without first significantly measuring this. information quality is one of the determining keys for the quality of the decisions and actions that are made according to it [cit] . it is exactly what the som nn classifier proposes to do; in other words, before predicting the class of the unknown object, it measures the information of the training set objects before making the classification decision. in order to understand the som nn combination, consider a som trained with the objects from figure 2 (a) without using the class information (shaded color). the prototypes adjusted resulting in trained som map (weight vectors) are represented in figure 3(a) . the result of the som can be generally understood as being a summary of the training set, through a set of prototypes that have a voronoi region, with the number of prototypes being smaller than that of the training set, in the following example: the twelve objects were summarized into four prototypes. the number of prototypes is a parameter that refers to the number of neurons of the som map. now consider the new object classification submitted to process that was presented in figure 2(b) . also consider the prototype set being utilized in a first comparison, instead of the training set. in this case, for the classification process, in initial phase a comparison will be done between the object under analysis and the set of prototypes, as illustrated in figure 3(a) . repeating the process that takes place in training the som for the selection of the winning neuron, made using the euclidean distance, the nearest neuron is selected (winner or best match) to the object under analysis. from this process where the nearest prototype is known and that, on the other 6 computational intelligence and neuroscience (c) nn: the object under analysis will be compared with the objects represented by the nearest or best match prototype hand, it is possible to know which training set objects are represented by the prototype, see figure 3 (b) where each prototypes has a voronoi region. thus, the reduced training set objects are retrieved to start the classification phase with the nn. finally, the classification will be done with a reduced set, as shown in figure 3(c) ."
"in summary, the last step in algorithm 1, the use of classifier algorithm, is executed with nn. this process is called here as som nn classifier."
"dataset. the objective of experiments using artificial dataset was to compare the performance of nn, nn, som nn, and som nn classifiers in situations where there are wellseparated classes (figure 4(a) ), classes partially overlapped (figure 4(b) ), and a large number of classes overlapped (figure 4(c) )."
"the simulation results are as shown in fig.6 and some statistic results are provided in tab.1. thus, without considering the size of b, we can conclude that both of eq. (7) and eq. (9) have nearly the same performance."
"hence, as is the nearest unit, we know the list with input patterns indices that should be queried, that is, bmul . illustratively, consider that weight vector belongs to a voronoi region; see figure 1, the shaded area, which has a bmul list with the indices of input patterns known (filled circle). also in this figure, the unlabeled sample x (unfilled circle) belongs to the region covered by unit (shaded area); that is, in the second step of the classification process, the nn algorithm is performed with a reduced set of objects."
"the same time experiment discussed above was repeated for som nn and som nn (figure 8(b) ). the behavior of the results in this experiment is similar to that discussed for figure 8(a) . this can be interpreted in two ways. the first is that the above analysis can be applied for these results and, more importantly, that the objects selected by the reduced set som prototypes can maintain the characteristics of the raw database. however, it should be noted in the result analysis that the time classification scale (horizontal axis) ranges from 0 to 100 seconds. in the earlier results, the scale ranged from 0 to 350 seconds. nonetheless, the importance of this result is that the trend remains exponential, 2 with 0.7. it is noteworthy that, in the result time shown for som (som nn and som nn), the training time is included."
"as shown in fig.2 (b), for the frame-level abnormal event detection, the normal frame has a small reconstruction cost, while the abnormal frame usually generates a large reconstruction cost. therefore, the src can be adopted as an anomaly measurement for such a one-class classification problem."
"some readers may ask why we use group sparsity, and whether sparsity is really effective or not. to answer these questions, we define a similar dictionary selection model using frobenius norm to compare with our dictionary selection model using group sparsity, i.e. l 2,1, as below:"
"figures 4(a)(c and d), 4(b)(c and d), and 4(c)(c and d) are the results using som as the generation prototype approach. that is, the decision boundary was generated without using all objects of the database but, instead, based on objects distributed in prototypes of the trained som map. in this qualitative analysis, the most important to note is that the preservation of the decision boundary was maintained in all experiments, without significant changes."
"silva and del-moral-hernandez [cit] presented combination methods that use the winning neuron and topological maintain concepts of the self-organizing maps (som) neural network to define a reduced subset of objects of the training set that are highly similar to the object that is under analysis for classification [cit] . this object subset is retrieved and then utilized by the nn to execute the classification task. in other words, the som executes a preprocessing for the nn classifier, recovering the similar objects from the winning neuron and from the adjacent neighbors of the som map [cit] ."
"the rest of this paper is organized as follows: section 2 gives the related work. section 3 provides an overview of our algorithm. section 4 presents the implementation details of our algorithm, including basis definition, dictionary selection, weighted l 1 minimization and self-update procedure. for dictionary selection, we compare the large scale version with the traditional one [cit] in section 5. then, section 6 reports our experimental results and comparisons with state-of-the-art methods to justify the performance of our algorithm. finally, section 7 concludes the paper."
"this paper introduces a new classifier named som nn, which is based on the combination of self-organizing maps (som) and informative nearest neighbors ( nn). the nn classifier is costly in computational terms, because in a classification process the informativity is not calculated only by the object under classification analysis, but also considering the other objects of the training set. [cit] suggested the use of nn algorithm (with the best value experimentally found as being 7) before nn to minimize the high computational cost, that is, using 7-nn to find a reduced subset for the classification process with the informative nearest neighbor algorithm."
"where true positive (tp) is the correctly labeled abnormal events; false negative (fn) is incorrectly labeled normal events; false positive (fp) is incorrectly labeled abnormal events; and true negative (tn) is correctly labeled abnormal events. for pixel-level and frame-level, we choice different thresholds and compute the tpr and fpr accordingly to generate the roc curve."
the following section presents a proposal that combines the som with the nn algorithm to build a process that will be named som nn. this section will also show the advantages of the som nn over the nn.
"computational intelligence and neuroscience analyzing qualitatively, starting by figures 4(a)(a), 4(b)(a), and 4(c)(a), nn results, we can note that the boundary separation degrades from the moment that the classes start the overlapping. in the worst case, we can observe that a high overlap (figure 4(c) ) is clearly one of the nn disadvantages, because it makes the decision boundary considering all objects as having the same importance. for the nn results (figures 4(a)(b), 4(b)(b), and 4(c)(b)), it is clear that the border of separation is softer, even when the class overlap increases. this is because the separation was defined by informative representation of the objects from the same class. this fact is most evident in the last experiment (figure 4(c) ), where we can observe that the boundary separation is created to preserve the predominant class in the border region."
"for these three dataset of gae and lae, we find that our improved version using lsds gets similar result as our previous one [cit], this is because both of them select the most efficient bases to construct the dictionary and use them for sparse reconstruction. however, our lsds can handle large scale training data, eer rd auc sf [cit] 31% 21% 17.9% mppca [cit] 40% 18% which is crucial in practical applications. all experiments are run on the computer with 2gb ram and 2.6ghz cpu. the average computation time is 0.8 s/frames for gae, 3.8 s/frame for ucsd dataset, and 4.6 s/frame for the subway dataset."
"this section details how to determine a testing sample y to be normal or not. as we mentioned in the previous subsection, the features of a normal frame can be linearly constructed by only a few bases in the dictionary b while an abnormal frame cannot. a natural idea is to pursue a sparse representation and then use the reconstruction cost to judge the testing sample. in order to advance the accuracy of prediction, two more factors are considered here:"
"much progresses in video surveillance have been achieved in recent years for some key areas, such as background model [cit], object tracking [cit], pedestrian detection [cit], action recognition [cit], crowd counting [cit] and traffic monitoring [cit] . abnormal event detection, as a key application in video surveillance, has also provoked great interests. depending on the specific scene, the abnormal event detection can be classified into those in crowded scenes and uncrowded scenes."
"for that reason, in a classification task with nn or ( nn as will be introduced in the next section) combined with som, the use of the objects represented only as bmul list results in a substantially reduced classification process time but can reduce the accuracy rate. thus, we explored the second important feature of som, the topological ordering of the training dataset objects. in other words, in addition to the bmul list, the lists of adjacent neurons in the som map grid are also consulted."
"as verified in this section, we formalized a strategy to select input pattern objects to be used as references in a classification task and to speed the time of nn algorithm. the next section introduces the nn algorithm which is less sensible to parameter and for this works better than nn in datasets with overlapped classes (boundary not well defined)."
"for a global analysis, in the more advantageous in database with more than 180 objects (from sonar dataset). this result can be observed at the upper end, where the consumption of classification time is high (vowel dataset), and the use of som can reduce by more than 3 times the nn and nn classification time."
"otherwise, for a hexagonal lattice topology, we have to consider six adjacent units and so on. in previous studies using som with nn [cit], we compared the two the reduced dataset and the unknown sample are used by a classifier ( nn or nn) that return the object class."
"by analyzing in detail the result of the time classification algorithms nn and nn in figure 8(a), it is observed that, to a certain number of objects, around 180 (datasets appendicitis to wine), the classification time is almost linear. from this point the tendency curve is not clear. the reason is that there are an increasing number of objects in these other databases and also a variation in the number of attributes. this means that the classification time depends not only on the number of objects but also on the number of attributes, for example, the balance database (625 objects) and dermatology (366 objects), whose last dataset has a smaller number of objects and consumes more time. another interesting case to mention is observed between the base mov libras and vowel. the former has almost half the number of objects and nearly ninefold more attributes than the latter but both consumed an equivalent time in the classification process. another point to consider in the graph is that, for every experiment, the classification time of nn is higher than nn. this result was expected because, as mentioned earlier, nn is computationally more costly due to the fact that nn is run before it as a preprocessing step and, thus, it finds the closest informative object. although it seems to be an obvious result, the experiments confirm their reliability. finally, for a general idea of the time, a tendency line was added to the results and the best adjustment was an exponential trend, with pearson coefficient above 0.7, which is considered a high value. as it is difficult to find a relationship between the numbers of objects and attributes to explain the process timing, the trend is more indicative about the number of objects. thus, for this experiment, the classification time is more sensitive to the number of objects."
"as a final conclusion, the nn is an algorithm with accuracy performance better than nn. but the classification time is a bottleneck for the algorithm, which is minimized using som as a prototype generation technique. thus, the 14 computational intelligence and neuroscience som nn classifier is proposed here which is specialized to solve problems where the border region is not well defined in a tolerable time."
"to handle both lae and gae, the definition of training basis y can be quite flexible, e.g., an image patch, a spatio-temporal video subvolume, or a normal : the red/green color corresponds to abnormal/normal frame, respectively. it shows that the s w of abnormal frame is greater than normal ones, and we can identify abnormal events accordingly. image frame. it thus provides a general way of representing different types of abnormal events. moreover, we propose a new dictionary selection method to reduce the size of the basis of φ for an efficient reconstruction of y. the weight of each new training sample is also learned to indicate its normalness, i.e., the occurrence frequency. these weights form a weight matrix w which serves as a prior term in the l 1 minimization."
this section will present the dataset utilized in the experiments and the parameterization of the classifiers utilized for the comparison with our som nn proposal. the experiments consist in using an artificial dataset for qualitative and quantitative analysis and with public dataset used as benchmarking in the literature to evaluate the efficacy of the algorithm proposed.
"appendix.appendix a gives the derivation to this theorem. note that one can fix β and optimize α for multiple times, but the practical experiments indicate that one time is optimal."
"thus, the class of the nearest (or informative instances as will be explained in the next section) is used to label the unknown sample x . this framework combination was initially called som nn (and here will be introduced the som nn classifier)."
"the visit of adjacent units depends on the grid initially set at the som training phase. for the som trained with rectangular lattice topology, the units of the four adjacent units should be considered. thus, the query list bmul query for the unknown pattern x is defined as"
"in practical aspects, the accuracy and kappa measures are equivalent in terms of performance. for purposes of simplification, only the accuracy will be considered in the extended discussion of the result analysis."
"in summary, the conventional algorithm nn (or nn) compares the unknown sample with all the instances of the dataset; here, the comparison is limited to a selection of the objects; that is, the comparison is restricted to a small number of instances from the training dataset. the main implementation steps are described as a pseudo-code in algorithm 1."
"in this paper, we propose a new criterion, sparse reconstruction cost (src), for abnormal event detection in the crowded scene. whether a testing sample is abnormal or not is determined by its sparse reconstruction cost, through a weighted linear reconstruction of the over-completed normal bases. our proposed dictionary selection method supports a robust estimation of the dictionary with minimal size; and with the help of the low rank constraint, it can not only deal with large scale training samples, but also require less memory cost than our previous work [cit] . thanks to the flexibility in designing the basis, our method can easily handle both local abnormal events (lae) and global abnormal events (gae). by incrementally updating the dictionary, our method also supports online event detection. the experiments on three benchmark datasets show favorable results when compared with the state-of-the-art methods. in fact, our algorithm provides a general solution for outlier detection; and can also be applied to other applications, such as event/action recognition."
"inspired by use of pg [cit], we introduce a hybrid approach, where in a first step there is the som, which has the quantization vector and topological maintenance as important features for using it as a preprocessing in order to present to the classifier algorithm a reduced set of objects, highly similar to the unknown object that is being investigated. next, the nn algorithm will attribute a class to the unknown object based on the most informative objects of selected set. for the initial exploratory experiments, we observed important results of accuracy and time in classification process [cit] ."
"as a final analysis of the accuracy performance, in order to show that the degradation with the use of som has little impact on the final performance, the comparison of the same pair of classifier presented above is performed using a radar chart ( figure 6 ). in this graph, the external values (polar scale) indicate the dataset number of 1 to 21, and the internal values show the accuracy performance, starting from 0.6 to 1.0. the ideal result would be to have the graph contour in 1.0. in this study, the main results are obtained for the overlapped lines, representing an equivalent result for contrasted classifiers. combining the results of the accuracy performance (statistical and chart), we can consider that nn has, in the vast majority of studies, a superiority in the classification performance when compared to nn. from this result, it is interesting to note that the nn superiority occurs mainly in datasets with performance below 90% as follows: 7, 9, 11, 12, 14, 15, 16, 18, 19, and 20. on the other hand, the result is lower in experiments with datasets 2 and 21, that is, where the accuracy performance is close to 1 (100%). therefore, the results also suggest that nn has superior results in datasets in which the decision boundary is not well separated."
", 2, · · ·, k}, such that the set b can be well reconstructed by b and the size of b is as small as possible. a simple idea is to pick up candidates randomly or uniformly to build the dictionary. apparently, this cannot make full use of all candidates in b. also it is risky to miss important candidates or include the noisy ones, which will greatly affect the reconstruction. to solve this problem, we present a principled method to select the dictionary. our idea is that we should select an optimal subset of b as the dictionary, such that the rest of the candidates can be well reconstructed using it. more formally, we formulate the problem as follows:"
"where r(x, y) and θ (x, y) are the motion energy and motion direction of motion vector at (x, y) respectively. therefore, our mhof not only describes the motion information as traditional hof, but also preserves the spatial contextual information. actually depending on the specific applications, we can define much more scales mhof, but for us, two scales are enough."
"this is a convex but nonsmooth optimization problem, as in our previous work [cit] . denote f 0 (α) as the smooth part 1 2 bαβ t − b 2 f . we employ the proximal method to solve it by the following updating procedure:"
"however, note that the input patterns object stored in the dataset (filled circles), which are the closest to the object being classified x (unfilled circle), belong to neighboring voronoi regions and are consequently represented in other lists; see figure 1, circle with a dotted line."
"to understand the nn algorithm in practical terms, consider the dataset utilized in figure 2 (a), where it is represented by shaded circles, to the extent that the shades (dark and light) represent the two classes of the set. now consider figure 2(b), which has the same training objects with the addition of an object without class represented by a circle without shade. now, consider in figure 2 (c) the contours in training objects and test object, representing the classification process executed utilizing the traditional nn, with euclidean distance and value being equal to 5. in this process, the majority class of the nearest neighbors is the one that is represented by dark shading. and, therefore, the decision-making process is made by this class. however, the object under analysis has as its nearest neighbor an object of the training set that belongs to the class with light shading and this, on the other hand, also has as its neighbor another object of the same class. therefore, utilizing the nn algorithm, the informativity takes into consideration not only the majority class but also the nearest objects and the concordance that the other objects of the training set have with the nearest object. in conclusion, in the case of the nn, the classification would be made by the class represented by the lightest shading thus, the concept of informative nearest neighbor has the following definitions. within the nearest neighbors, the object that is nearest to the object that is being classified, which is distant from other objects of different classes, is considered the most informative object, such that its class is attributed to the unknown object. on the other hand, the object that has a different class from the most informative object is considered least informative. an object is also considered least informative within the nearest neighbors when it has the same class as the most informative object and is nearest to other objects of different classes."
"the receiver operating characteristic (roc) curve is used to measure the accuracy for multiple threshold values. the roc is consisted of true positive rate (tpr) and false positive rate (fpr), of which tpr determines a classifier or a diagnostic test performance on classifying positive instances correctly among all positive samples available during the test, and fpr, on the other hand, defines how many incorrect positive results occur among all negative samples available during the test. these measures are given by the formulas in eq. (22) the rocs for the detection of abnormal frames in the umn dataset. we compare different evaluation measurements for abnormal event detection, namely weighted src with large scale dictionary selection model (lsds), weighted src, src without weight, sparse with concentration function and sparse with entropy measurement, and also the other two methods, social force [cit] and optical flow [cit] . our method outperforms the others."
"in order to provide a qualitative analysis with visualization of the border decision-making area and a quantitative analysis in terms of classifier accuracy, three datasets were generated with the following features: 300 objects, two attributes, two classes, and a balanced number of objects per class. for all datasets, the objects were distributed with the same mean but with difference in the standard deviation value, in order to force an overlapping of classes. thus, each dataset represents distinct situations on the border of classes: no, low, and high confusion. in order to evaluate the efficacy of the algorithm proposed and compare it with others from the literature, 21 public databases were chosen (repository of the university of california, irvine, uci) that are used as benchmarking for prototype generation approaches. table 1 summarizes the properties of each benchmarking dataset in number of objects (obj), number of attributes (att), and number of classes (cla). for all databases, the attributes are numerical. the separation of these datasets in training and test set were done with the use of the 5-fold cross validation."
"neighbors. some data classification approaches based on nearest neighbor, in addition to defining a given range of values to find the nearest neighbors, also utilize new distance metrics, such as the informative nearest neighbor [cit] . in other words, they utilize in the analysis of a new object of unknown class a measure that quantifies which training set object is most informative."
"for the pg methods, prototypes are used by classifiers instead of raw datasets, or they are used to generate an artificial dataset. data generation can be interesting in some cases to eliminate data noise or to solve dataset with unbalanced class. since the possibilities of usage are diversified, the literature presents different methods, approaches, and algorithms. this was the reason for [cit] to propose a pg taxonomy that is used to enhance nn drawbacks, which was defined as a hierarchical way of three levels (generation mechanisms, resulting generation set, and type of reduction), and also review the all algorithms of the pg from the literature (see [cit] for a detailed explanation)."
"the experiments were implemented using the r language, version 3.1.2, with rstudio ide version 0.98 and using a conventional computer with windows 10, i7 with 8 gb ram. the experimental results are presented in the following section."
"neighborhood topologies (rectangular and hexagonal) and the results were equivalent. for this reason, the rectangular lattice topology was chosen in this work. finally, in the second step of the classification method proposed here, the reduced objects set belonging to bmul query (4) is used to find the nearest neighbors ( nn). note that the set of objects extracted from the query lists, that is, x bmul query, is part of the set of input patterns objects used for the som training; that is,"
"since the training phase has been completed, each input pattern object from the training set has to be grouped to the closest neuron. the idea in this approach of using som as a pg technique is that the index of each instance x is a part of the nearest neuron list. thus, the list of each neuron is here called the best matching unit list (bmul), formally defined as"
"for the som training, a dataset is chosen and divided into two distinct sets. the training set is used to train the som which is here called x train . the other set is used to test the trained som (x test ). after this dataset division, we start the training som. formally, an object is randomly selected from x train during a training, defined as during the learning process, the input pattern is randomly selected from the training set and it is compared with the weights vector of the map, initially initialized randomly. the comparison between x and w is usually made through euclidean distance. the shortest distance indicates the closest neuron, which will have its weight vector w, updated to get close to the selected input pattern x . formally, neuron is defined as follows:"
"iii. by using different types of bases, we provide a unified solution to detect both local and global abnormal events in crowded scene. our method can also be extended to online event detection by an incremental self-update mechanism."
"analyzing the results from table 5 is possible to note in the first two lines of table that nn showed to be better than nn in most of cases (66.7% and 52.4%, combined with som). the use of som in the classification process (last two lines of the table) has been shown to be slightly better or worse in some cases. the som performance with nn is improved (52.4%) and with nn there is a little degradation (47.7%). however, an important result that should be emphasized (last row of the table) is that the use of som with nn maintains or improves the performance in most databases (52.4%)."
"we evaluate our method in three different abnormal event detection datasets, including the umn dataset [cit], the ucsd dataset [cit], and the subway dataset [cit] . the main contributions are as below:"
end if 9: end for abnormal (outlier) sample is the point with a lower probability. we can estimate the normal sample by maximizing the posteriori as follows:
"thus, due to the preprocessing made by the som to the nn algorithm, the computational effort as a whole to find the informative nearest neighbor is much smaller, which results in a significant reduction in the classification time when compared to the classification time of the nn without preprocessing."
"to overcome the drawbacks above, there are in the literature different approaches such as similarity measure alternative to the euclidean distance to minimize misclassification in boundaries region [cit], methods to avoid searching the whole space of training set [cit], and dataset summarization to find representative objects of training set [cit] . for the dataset summarization approach, there are two main strategies to reduce the dataset volume: one of them based on instance selection and the other based on prototypes. for the 2 computational intelligence and neuroscience approaches based on pattern (or instance) selection, the aim is to find a representative and reduced set of objects from the training dataset, which has the same or higher classification accuracy of a raw dataset [8, [cit] . the strategies based on prototype, on the other hand, are defined in two approaches: prototype selection (ps) [cit] and prototype generation (pg) [13, [cit] . the approaches are equivalent; both can be used to identify an optimal subset of representative prototypes, discarding noise, and redundancy. the difference is that pg can also be used to generate and to replace the raw dataset by an artificial dataset. the use of prototypes or reduced training objects that are represented by prototypes minimizes some of nn drawbacks previously mentioned as the exhaustive comparison of all training dataset."
"since the nn algorithm has no model, an exhaustive comparison of the unlabeled sample with all the labeled and stored objects in the database is necessary, which increases the computational time of the process. in addition to this weakness of algorithm, the decision boundaries are defined by the instances stored in the training set and, for this, the algorithm has low tolerance to noise; that is, all training dataset objects are considered relevant patterns. finally, the optimal choice of depends upon the dataset mainly when the object analyzed is in a boundary region, making this parameter to be tuned according to the application [cit] ."
"the next results to be analyzed are the time consumed in the classification process. the results are shown in figure 8, and, for interpretation purposes, the databases are arranged in the vertical axis and are organized in ascending order of number of objects. in vertical axis, each dataset is described by name, number of objects, and number of attributes (described in table 3 ). the time shown on the horizontal axis is measured in seconds."
"the combination using the som neural networks approach with the nn explores the main characteristics that define the potential of a data classifier, which are storage reduction, noise tolerance, generalization accuracy, and time requirements [cit] . to the contrary of the nn that preprocesses the data utilizing the nn algorithm that has a high computational cost, the algorithm proposed in this work reduces the data representation through the som. in addition, with the use of the som, the classification time of the som nn is expected to be shorter when compared with the nn, which results in less memory use, maximizing the classifier's performance in terms of classification time."
"the som nn approach will be compared with nn, nn, and som nn. the classifiers parameterizations are represented in table 2 . the som parametrization is the same for som nn and som nn."
"the second equation is obtained by swapping \"max\" and \"min\". since the function is convex with respect to x and concave with respect to y, this swapping does not change the problem by the von neumann minimax theorem. letting"
"for training, only normal videos are required. to detect abnormal events from normal training samples, we collect the feature from training video frames to generate the normal feature pool b, where each sample in b is normal feature. different features are designed for lae or gae (sec.4.1). as the normal feature pool b is redundant and contains noisy features, an optimal subset b with minimal size, is selected from b as training dictionary (we call each feature of the selected dictionary as basis), and the weight of each basis of the dictionary is also initialized (sec.4.2). for testing, we also extract the same feature as in training, then each testing sample y can be a sparse linear combination of the training dictionary by weighted l 1 minimization, and whether y is to normal or not (e.g. the green/red point in fig.3 ) is determined by the linear reconstruction cost (src) (sec.4.3), i.e., normal feature can be efficiently spare represented by training dictionary with lower cost, on the contrary, the abnormal bases will be constructed with greater cost or even cannot be constructed, as shown in fig.2 . moreover, our system can also selfupdate incrementally, which will be explained in sec.4.4. the algorithm is shown in alg.2."
"the closest weights vector w and their neighbors are updated using the kohonen algorithm [cit] . however, the topological neighborhood is defined so that the farther away the neuron from w, the lower the intensity for the neighborhood to be updated. the intensity of the neighborhood function is defined in relation to the training time. in other words, in initial times, the level has high value and, according to the next iterations, it is reduced at each iteration. see kohonen [cit] for a complete explanation of the training rule of the som map."
"the next analysis consists in verifying the som efficiency in reducing input objects. for this, the reduction and accuracy percentage of each dataset performance is checked. the results are shown in figure 7 . interestingly, in both results, som nn and som nn, there are three regions very well defined in the accuracy reduction experiment. the first datasets have an average of 150.5 objects, with the second averaging 215 and the last averaging 694.5 objects. that is, the reduction varies with the number of objects. therefore, the results of som nn (figure 7(a) ) and som nn (figure 7(b) ) show that the more objects in dataset, the higher the reduction rate."
"to detect both lae and gae, we propose a general solution using sparse representation, as illustrated in fig.3 . the flowchart of our algorithm is shown in fig.4 ."
"dataset. this section shows the experiments and results for datasets introduced in table 1 . the results are analyzed using the following measures as performances: accuracy, kappa, hypothesis test, rate of dataset reduction, and classification time. table 3 shows all the classification results for the paper experiments. in this table, the accuracy and kappa measures are shown in terms of average and standard deviation. the other results are also discussed in this section."
"high-dimensional feature is usually preferred to better represent the event. however, to fit a good probability model, the required number of training data increases exponentially approximate o(d 2 ) with the feature dimension d, it is unrealistic to collect enough training data for density estimation in practice. thus, for most state-of-the-art methods, there is an unsolved problem between event representation using high-dimensional feature and model complexity. for example, for our global abnormal detection, there are only 400 training samples with the dimension of 320. with such a limited number of training samples, it is difficult to even fit a gaussian model robustly."
"kohonen self-organizing map (som) is a type of neural network that consists of neurons located on a regular lowdimensional grid, usually two-dimensional (2d). typically, the lattice of the 2d grid is either hexagonal or rectangular [cit] . the som learning or training process is an iterative algorithm which aims to represent a distribution of the input pattern objects in that regular grid of neurons. the similar input patterns are associated in the same neurons or in the adjacent neurons of the grid."
"the main task of a data classifier is to predict the class of an object that is under analysis. the simplest procedure for data classification tasks is the nearest neighbor ( nn) algorithm. the algorithm strategy for classification comprises three operations: (i) an unlabeled sample is compared to dataset training through a similarity measure; (ii) the labeled objects are sorted in order of similarity to the unlabeled sample; and finally, (iii) the classification occurs giving the unlabeled sample the majority class of the nearest neighbors objects. because of its simplified algorithm (three basic operations steps), and reduced number of parameters (similarity measure and the number of nearest neighbor), this instancebased learning algorithm is widely used in the data mining community as a benchmarking algorithm [cit] ."
"the relation existing between a computational theory and the algorithmic level can be regarded as the relation between a function (in a mathematical sense) that is computable and a specific algorithm for calculating its values. the aim of a computational theory is to single out a function that models the cognitive phenomenon to be studied. within the framework of a computational approach, such a function must be effectively computable. however, at the level of the computational theory, no assumption is made about the nature of the algorithms and their implementation. [cit] this way, once the function is identified, we can focus on the properties of that function and leave aside considerations on algorithms and their implementation. but when we consider mathematical problems, we are already working within the paradigm of such functions. if we dismiss the algorithmic and implementational levels, we are left with explaining the task of taking a mathematical decision problem as the input and providing the correct answer (yes or no; true or false) as the output. when studying the complexity of this cognitive task, it is understandable to characterize it in terms of the computational complexity of the problem in question. this way, the number of computational steps required to reach a solution is seen as characterizing the complexity of the cognitive task. in the cognitive science literature, this is called problem complexity and it is often considered to be the central variable in the research on mathematical cognition [cit] (ashcraft, 1995 [cit] ) ."
"the computational complexity approach to mathematical problem solving, and computational-level explanation based on it, is unable to account for the enculturated nature of problem solving processes. this is where the present approach has an advantage. by including the algorithmic level in the explanations, we can study which problem solving methods are cognitively beneficial for human problem solvers within a particular culture. we can also study how these methods vary interculturally. perhaps for many mathematical problems there is relatively little cultural variation. with some problems, it may make sense to speak of universal, humanly optimal problem solving algorithms. in some cases, these may also coincide with computational optimal algorithms. however, we should not assume that this is generally the case. that is why humanly optimal algorithms must be considered conceptually separate from computationally optimal algorithms. for the same reason, cognitive complexity should be considered to be conceptually separate from computational complexity. because of the expected cultural variation, humanly optimal algorithms-and hence cognitive complexity of mathematical problem solving-should be considered in cultural contexts. with these conceptual differences in place, our approach can account for the enculturated nature of mathematical practices."
"lda seeks to find directions along which the classes are best separated. on the other side, pca is based on the data covariance which characterizes the scatter of the entire data. although one might think that lda should always outperform pca (since it deals directly with class separation), empirical evidence suggests otherwise [cit] . for instance, lda will fail when the discriminatory information is not in the mean but rather in the variance of the data. here, a modulation recognition performance comparison shows that lda slightly outperforms pca in the poor recognition region, and the performance of the two algorithms rapidly converges as the snr goes high. anyway, we will use pca due to its simplicity and direct implementation."
"however, it could be that all three cases are relevant in a limited manner, since the advantage of the fast algorithms only starts to show with operations that involve very large numbers. the karatsuba algorithm, for example-which used to be the fastest multiplication algorithm before the schönhage-strassen algorithm-only starts to outperform the standard algorithm when the integers are hundreds of bits long [cit] . it is unfeasible that a human problem solver (as characterized above) would ever perform this kind of multiplication, and it could thus still be the case that for all the relevant multiplications-those that human beings could in principle engage in-the standard algorithm is also a computationally optimal one, and not merely humanly optimal. thus we should perhaps look elsewhere for more widely relevant humanly optimal algorithms which are not computationally optimal."
"pca constructs a low-dimensional representation of the data (extracted features) that describes as much of the variance in that data as possible. pca is mathematically defined as an orthogonal linear transformation that transforms the data to a new space such that the greatest variance by any projection of the data comes to lie on the first dimension (called the first principal component), the second greatest variance on the second dimension, and so on [cit] . this moves as much of the variance as possible into the first few dimensions. the values in the remaining dimensions, therefore, tend to be highly correlated and may be dropped with minimal loss of information. pca is the simplest of the true eigenvector-based multivariate analyses."
"even though there is quite some variation in how cognitive complexity is understood in psychology, the account of john and mary can be seen as representative of the traditional psychological understanding of the term. importantly, cognitive complexity is seen as a property of the process of the individual cognizing subject [cit] . but of course this approach to cognitive complexity is not limited to determining individual differences in cognitive processing. we can also ask more general questions about cognitive tasks. even with individual differences, it is clear that human beings have great inter-individual similarity in their cognitive processes, which makes it feasible to study the complexity of those tasks, rather than individual cognitive processes."
"that our algorithm has a robust performance regardless of the channel model used. finally in figure 12, we present the frp as a function of the mobile speed (x) in a gsm system considering the channel model of rural area (rax, 6 taps) as introduced in the 3gpp gsm/edge channel model. it is clear from these results that the algorithm performance is better over fading channels than static ones. these results confirm the special capabilities of wavelet analysis relative to conventional analysis."
"where f c is the carrier frequency, θ c is the carrier initial phase, and s(t) is the baseband complex envelope of the signal s(t), defined by"
"( from these simple examples, it is already easy to see the potential fruitfulness of the computational complexity approach. clearly the case of three numbers appears to be a cognitively more complex problem, as seen from the fact that the pseudo program for solving it consists of six lines, compared to the three lines of the two-number problem."
"however, the empirical data strongly suggests that whenever human cognizers see number symbols, they automatically process them as magnitudes, and the effect is present in everyone familiar with number symbols. with this type of general cognitive tendency, we can longer dismiss the suboptimal algorithm as dealing with performance. instead, our cognitive competence with integers includes the distance effect, and if we want to model the cognitive capacity accurately, also the distance effect must be included in the model."
"in what follows, the continuous wavelet transform of the normalized signal will be taken into consideration. knowing that the amplitude of the normalized signal is constant and from (8), it is clear that the signal normalization will only affect the wavelet transform of nonconstant envelope modulations (i.e., ask and qam), and will not affect wavelet transform of constant envelope ones (i.e., fsk, msk, and psk). note that there will be distinct peaks in the wavelet transform of the signal and that of the normalized one resulting from phase changes at the times where the haar wavelet covers a symbol change. in what follows, we consider the magnitude of the wavelet transforms for different modulation schemes. given the complex envelope of qam signal"
"certainly there is nothing in the distance effect that suggests that it could not be computationally modeled, and such models have indeed been proposed [cit] . but the modeling of it goes against much of what we would expect from an algorithm that takes numerical distance into account. for a computer algorithm to count the distance between two numbers, we should expect the reverse of the distance effect: the smaller the difference, the faster it is to compute. thus the suboptimal problem solving algorithms due to the distance effect seem to be a genuinely human (and possibly animal) 22 phenomenon whose modeling requires moving beyond the computational-level approach and including the algorithmic level of explanation."
"the basic service provisioning triangle, relevant to this work, consists of the call session control function (cscf) entities, providing session control, service triggering and aaa functionalities, the home subscriber server (hss), or the extended user profile server function (upsf), representing the subscriber profile database and an extended aaa and mobility server, and the application server (as), hosting the service logic and providing the convergent service delivery environment. other entities are also defined for the ims (e.g., media server functionalities, interworking, and gateway functionalities, etc.)."
"in general referred to as the quality, there are basically two approaches to defining, measuring and assessing the success of meeting a specific set of requirements or an expected behavior. the measure of performance from the network perspective is known as the quality of service (qos) and involves a range of qos mechanisms that are implemented for the purpose of meeting the defined conditions in the network. typically, qos metrics include network operation parameters (i.e., bandwidth, packet loss, delay, and jitter). on the other hand, the measure of performance as perceived from the end user is known as the quality of experience (qoe) and addresses the overall satisfaction of the end user and the ability to meet their expectations. while the qos is rather objective approach to assessing the success of performing within a specified network subsection, the qoe is subjective, measured on an endto-end basis, and involves human-related criteria, based on which certain descriptive indexes of performance are set. some examples of qoe metrics are the mean opinion score (mos), degraded seconds, errored seconds, unavailable seconds, etc."
"there are numerous recommendations and guidelines on how to ensure the appropriate ip network level performance objectives (etsi ts 185.001). however, for complete service delivery, a systematic qoe and qos assurance is required (itu-t rec. y.1291) that spans through all layers of the solutions and approaches the issue of end user's satisfaction from the services viewpoint rather than from the network viewpoint. moreover, the notion of multiple separate interconnected domains enforces dynamically changing conditions that imply the usage of dynamic quality assurance mechanisms."
"further challenges arise from this concept. the complexity and the performance requirements of the rather complex signaling procedures are an issue that would present a substantial load to the entire environment, and the required level of intelligence needed to perform the quality negotiation and enforcement with the respective security issues is challenging. further standardization efforts would be required to resolve the interconnection and interworking of the traversed access and transport domains as well as with the servicelayer mechanisms. once the various proposals are harmonized and the standardization is completed, the ngn services can be considered as a collection of specialized services implemented with the already available functionalities of the ngn environment in a standardized fashion and with ensured operator-grade quality."
"where t s is the symbol duration and channel is the channel function which includes the channel effect on the signal. for additive white gaussian noise (awgn) channel, the received waveform is described as"
"the term \"quality of service\" sums up all quality features of a communication as perceived by a user for a specific service. in order to achieve the end-to-end qos, it is necessary to maintain a level of qos all along the path from the source te (terminal equipment) to the destination te crossing various administrative domains. in the context of ims services, the involved domains will be ngn bearer service domain, external ip domain, ims domain and/or other umts bearer service domains. the 3gpp proposes the use of diffserv to support qos in the underlying ip networks. furthermore, the provisioning of qos is performed by the pbm framework standardized by the ietf [cit] ."
"after pre-processing and features subset selection, the training process is triggered. the initiated feed-forward neural network is trained using rprop algorithm. finally, the test phase is launched and the performance is evaluated through confusion matrix and false recognition probability. some authors try to explain their results through receiver operating characteristic (roc) which is more suitable for decision-theoretic approaches where thresholds normally classify modulation schemes."
"one would expect that the task is so simple that in both cases it takes equally long to get the answer. however, the data shows that even for adult subjects, the (4 5) pairing takes considerably longer than (4 9). this is called the distance effect: the greater the numerical distance between the two numbers, the quicker we are in solving the problem. the distance effect is usually explained in terms of our automatic tendency to process numerical symbols as quantities. when we process quantities we use the so-called approximate number system which is an estimation system that becomes less accurate as the distance between numbers becomes smaller (and the magnitude of numbers becomes larger) [cit] (dehaene /2011 . 20 interestingly, we cannot get rid of the distance effect even if we are trained to solve these types of problems. the effect is also a remarkably strong one, as seen from the following example. let us have the same task with the these numbers:"
"the distance effect has important consequences for the topic at hand. the kind of problems we have been dealing with in this example may not be particularly interesting as mathematical problems, but they show how we automatically assign meaning to number symbols and words."
"throughout these considerations, we should keep in mind the pragmatic character of the complexity classes. for small inputs, np or exp-hard problems can be perfectly solvable for human beings, whereas p-hard problems are unsolvable for large enough inputs. the complexity classes, as well as the tractable cognition thesis based on them, should be seen more as guidelines for evaluating complexity than strict results. thus the p-cognition thesis should not be understood to claim that nphard functions, for example, cannot be used to model cognitive capacities. rather, the thesis implies that for each such model, ultimately-perhaps for larger inputthere must be a p-hard function that models the capacity more accurately."
"it could of course turn out to be the case that the distance effect does not play a role in human arithmetical calculations. it could merely be \"noise\" that disturbs arithmetical calculations by making us process numerical symbols as magnitudes. the arithmetical calculations themselves could follow algorithms that are computationally optimal. this is a possibility that should be empirically pursued. what we do know is that even for mathematically highly educated subjects, the total cognitive process of being presented with an arithmetical problem and solving it is not algorithmically optimal. when we study the human competence in solving arithmetical problems, this needs to be taken into account."
"18 among these heuristic and didactic processes, drawing diagrams is probably the most prevalent. [cit] were first published, the study of heuristics and didactic methods of mathematical problem solving has been an important topic in mathematics education. starting from perhaps polya's most commonly applied rule \"if you are having difficulty understanding a problem, try drawing a picture\", some didactic rules have become the standard way in which mathematics is taught. the great usefulness of pictures and diagrams has been confirmed in an enormous number of studies (see, e.g., [cit] . yet applying already that simple rule of polya takes us away from the computational complexity approach to characterizing human problem solving. generally speaking, computers do not benefit from visualizing problems and adding this sort of didactic process to a problem solving computer program makes it less optimal, thus (from the computational perspective) needlessly increasing the complexity of the solution. 19 for human problem solvers, however, the matter is quite different. the computationally superfluous pictures, analogues and such can be absolutely crucial for finding the solution."
"in cognitive science, however, that has not been seen as a satisfactory state of affairs. since cognitive scientists are interested in the general features of human cognition, they have understandably largely disregarded the kind of different cognitive 3 in this paper, we will work in the paradigm of decision problems. in computer science, many other types of problems are also studied, such as counting, search, function and optimization problems [cit] . the focus on decision problems is not meant to suggest that other types of problems are not relevant for computational complexity. algorithms for solving optimization problems, for example, may well provide better models for some human cognitive tasks. there are two main reasons for focusing on decision problems. first, research on decision problems is the most developed field in complexity theory. second, the discussion in cognitive science and its philosophy has focused mostly on decision problems when it comes to computational modeling. it should also be noted that these distinctions between problems are somewhat arbitrary, as many problems can be framed in different ways."
"we are faster in determining that the numbers are not the same as with: this is also the case when numerals were use instead of number symbols (two nine takes shorter than two three) [cit] . for some reason, we cannot treat number symbols or numerals without thinking of them as quantities and reverting to the approximate number system."
these protocols can be combined to provide various levels of qos. the common types of qos that various vendors may claim to support are as follows:
"in fact this approach can already be seen in the actual computational modeling of human cognitive capacities. [cit], for example, has noted that the distinction between marr's three levels is relatively insignificant in modern computational modeling:"
"on the network level, management of ip addressing scheme within the access networks and authentication of the access sessions. the following key functionalities are provided through the nass:"
"modulation classifiers are generally divided into two categories. the first category is based on decision-theoretic approach while the second on pattern recognition [cit] . the decision-theoretic approach is a probabilistic solution based on a priori knowledge of probability functions and certain hypotheses [cit] . on the other hand, the pattern recognition approach is based on extracting some basic characteristics of the signal called features [cit] . this approach is generally divided into two subsystems: the features extraction subsystem and the classifier subsystem [cit] . however, the second approach is more robust and easier to implement if the proper features set is chosen."
"the remainder of the paper is organized as follows. section 2 defines the mathematical model of the proposed problem and presents cwt calculations of different considered digitally modulated signals. section 3 describes the process of feature extraction using the continuous wavelet transform. section 4 focuses on features set pre-processing and subset selection, besides the structure of the artificial neural network and the learning algorithm. the results, algorithm performance analysis, and a comparative study with some existing recognition algorithms are presented in section 5. conclusions and perspectives of the research work are presented in section 6."
"while this computational-level approach has clear advantages, i submit that there should be limits to how strong and wide the application of the a priori computational methodology should be. as fruitful as the computational complexity paradigm may be, we should not dismiss the possibility that human competence in mathematical problem solving may indeed differ in important and systematic ways from the optimal algorithms studied in the computational complexity approach. in the rest of this paper, i will argue that by including considerations on the algorithmic level, we can get a more informative framework for studying the actual human problem solving capacity. furthermore, i will show that the algorithmic-level approach does not move the discussion from competence to performance. instead, we get a theoretical framework that is better-equipped for explaining human competence by including considerations of the algorithms that are cognitively optimal for human reasoners. 13 13 [cit], who proposed amending the marr classification by adding a level \"1.5\" of cognitive explanation (1 being the computational level and 2 the algorithmic). he argued that the purely extensional computational approach corresponds neither to chomsky's competence nor even to the scientific practice that marr intended. if we are concerned only with the output and the input, we are left with a multitude of characterizations of the functions involved. for example, if we think of functions simply as cartesian sets of ordered pairs (i.e. purely extensionally), we learn nothing about the intensional intricacies of the function. but surely such purely syntactic pairing of inputs with outputs (with, e.g., a look-up table) is a inferior characterization (cognitively) than a description of a function that implies how it is cognitively computed. in this way, the characterization of the function may already point us toward the algorithms used in cognitive processing."
"that practical difficulty notwithstanding, however, heuristic and didactic processes like diagrams seem to give us a clear case in which the humanly optimal algorithms may differ from the optimal algorithms, as studied in a purely computational approach to mathematical problems. consequently, heuristic and didactic processes are also a clear case in point that we should not work exclusively on marr's computational level. in the computational complexity approach to mathematical problem solving on the computational level, such computationally suboptimal tools are by definition ignored. but given the great use that human problem solvers have for such cognitive tools, it becomes obvious that we should also be interested in the actual algorithms that human cognizers use. indeed, to understand the human problem solving capacity as well as possible, it is the humanly optimal algorithms that we should be looking for-as suboptimal as they may be computationally."
"but this pragmatic nature of the complexity classes notwithstanding, the acceptance of the tractable cognition thesis has very strong implications for philosophy of mathematics and beyond. indeed, if mathematical problems beyond a certain complexity are thought to be unsolvable, this is a strong conclusion generally for philosophy of mind and epistemology. it imposes explicit limits on what our cognitive capacities can achieve, as well on the class of problems whose truth-values we can know."
"above i have argued that diagrams and other heuristic and didactic methods contribute to humanly optimal algorithms. but there are also ways in which we use suboptimal algorithms which actually make solving problems unnecessarily hard for us. diagrams concern a relatively sophisticated level of mathematical thinking, but we can see computationally suboptimal algorithms in use already on the very basic, unconscious level of treating mathematical concepts."
"above we have arrived at an important conclusion: the computational complexity of a mathematical problem cannot be equated with the complexity of a humanly optimal algorithm for solving it. as a direct consequence, the complexity of the cognitive task of solving a mathematical problem is not necessarily the same as the computational complexity of the problem. in sect. 2, marr's computational level of explanation was characterized in terms of identifying a function that models a cognitive capacity in terms of its input and output. due to the considerations above, we have established that such modeling does not always give the best characterization of the cognitive capacity in question. we should also consider the algorithms that human cognizers use to compute the values of the function."
"whereas that method may be heuristically less complex for children acquainted with it-at least with numbers where the digits are small like in fig. 6 -for children not acquainted with such visual methods this is usually not the case. in addition to such cultural differences, there are also individual differences in how helpful we find different heuristic methods. more visually inclined students, for example, are likely to find diagrams and other visual methods more helpful, and students with diverse learning abilities generally benefit from different heuristic and didactic tools [cit] . 26 in this way, many mathematical practices are enculturated. enculturation refers to the transformative process in which interactions with the surrounding culture influence the way cognitive practices develop [cit] . [cit] calls \"learning driven plasticity\", new cognitive capacities can be acquired due to the neural plasticity of the human brain, which allows for both structural and functional changes [cit] . in learning mathematics, the methods and practices of the surrounding culture thus play an important role in how the cognitive mathematical capacities 24 the name \"streetlight effect\" comes from the old joke in which a drunk looks for his keys in the better visibility of a streetlight, instead of the place where he lost them. 25 the origin of the name for this method, which has recently received a lot of attention on the internet, is unknown, but most often it is referred to as the japanese multiplication method. this use also seems to adopted in academic contexts. see, e.g., http://www.cemc.uwate rloo.ca/event s/mathc ircle s/2013-14/fall/ junio r6_multi plica tion_nov5.pdf. 26 the usefulness of visual heuristic methods can also be present without actual visual input, as seen in the \"mental abacus\" used in many especially asian cultures for calculations [cit] . develop. this theoretical framework can help explain the different methods that human problem solvers use [cit] (pantsar, 2019 ."
"with this brief account, we have arrived at three different aspects of cognitive complexity: the complexity of individual cognitive processes, the complexity of general cognitive tasks, and the complexity of problems. there is an intuitive way in which the three aspects are connected: the more complex the problem, the more complex the cognitive tasks required, and thus the more complex the individual cognitive processes. equally intuitively, the converse chain of implications from the individual cognitive processes to the problem does not necessarily hold. it is possible for a problem to be simple and yet for the individual human processing of it to be highly complex. this way, to study the complexity of mathematical problem solving, it would seem that we need to work on all three aspects."
"in studying enculturated mathematical competence, it is clear that we should use as much information about the computational complexity of mathematical problems as we can. while the cultural and individual differences are important, much of the complexity of the cognitive task of solving a mathematical problem can indeed be traced to the computational complexity of that problem. in this way, the approach here is not meant to question the value of the established fruitful methods of studying complexity. instead, i want to suggest that the established methodology can be augmented by including considerations of the actual, sometimes suboptimal, enculturated algorithms that human problem solvers use."
"styles that make humans use needlessly complex cognitive processes. [cit] presented a fundamental distinction in linguistics (in particular syntax) between competence and performance, which has later been widely transferred to the study of cognitive phenomena. in studying the nature of cognitive tasks, we are not primarily interested in the actual performance of the cognizers. rather, we want to study an idealized version of his/her cognitive abilities, i.e., what we would expect from a fully competent user of that cognitive capacity."
"if the partial derivative is positive (i.e., increasing error), the weight is decreased by its update-value. if the derivative is negative, the update-value is added. to summarize, the basic principle of rprop is the direct adaptation of the weight update-value. in contrast to learning rate-based algorithms, rprop modifies the size of the weight update directly based on resilient updatevalues. as a result, the adaptation effort is not blurred by unforeseeable gradient behavior. due to the clarity and simplicity of the learning rule, there is only a slight expense in computation compared with ordinary backpropagation. besides fast convergence, one of the main advantages of rprop lies in the fact that no choice of parameters and initial values is needed at all to obtain optimal or at least nearly optimal convergence times [cit] . also, rprop is known by its high performance on pattern recognition problems."
"however, when seen in the current setting of the computational complexity approach to modeling cognitive processes in mathematical problem solving, there is a potentially serious difficulty involved. in the study of computational complexity we are (in this regard) mostly interested in optimal algorithms. if we characterize the complexity of the cognitive task of solving a mathematical problem through the complexity of the problem, we are implicitly assuming that the competence of human reasoners can be characterized by a computationally optimal algorithm."
"now we would certainly expect the solution to take an equally long time. after all, we can clearly grasp an optimal algorithm for solving the problem, which would be to first compare whether there is a difference in the first digit and only consider the second digit if no such difference exists. yet also in this case there is a clear difference, the pair (71 65) takes more time than the pair (79 65) [cit] ."
"however, acknowledging the need for a wider methodological paradigm is only the first step. as important as expanding the study of cognitive complexity to include algorithmic aspects is, there are many understandable difficulties involved. most importantly, the characteristics of advanced mathematical cognition can be very difficult to study. this is the case especially on the level of implementation, but also in terms of identifying the algorithms that humans use in problem solving (see, e.g., [cit] . perhaps in their minds the test subjects make cognitive moves that do not show up as behavioral patterns, nor are the test subjects able to identify them introspectively [cit] . the design of pertinent experiments is an enormous challenge and even with ideal circumstances for testing we cannot be entirely sure that we have accurately identified the algorithmic procedures. while that does not mean that we cannot gather reliable data at all, it does bring in the kind of inexactness that is a bad fit with the exact notions involved in the study of computational complexity (see, e.g., [cit] ) ."
"the adaptive update-value δ i j for rprop algorithm was introduced as the only factor that determines the size of the weight update. δ i j evolves during the learning process based on the local behavior of the error function e, according to the following learning rule:"
"if our aim is to model human cognitive tasks accurately, we must include considerations on didactic processes such as constructing and interpreting diagrams. the problem is that such matters can be hard to study. the cognitive processes involved in solving even a moderately difficult mathematical problem are not easy to identify with brain scanning methods, thus making explanations on the implementational level problematic. on the algorithmic level, we can measure problem solving time for simple problems, and thus compare the cognitive efficiency of different algorithms used to reach the solution. for more difficult problems of the type that mathematicians actually deal with, however, also this method can be tricky. as the problems become more complex, it becomes increasingly difficult to trace all the parts of the algorithm that the problem solver uses. often we have to rely either on observing mathematical practice or the testimony of the mathematicians. both of these methods have significant drawbacks. observing mathematical practice is likely to include many stages of thought process which are difficult, if not impossible, to detect. questionnaires and interviews of mathematicians are also potentially problematic. not only are there the usual problems with unreliability of introspective inquiry, but the kind of didactic, heuristic and intuitive aspects of mathematical thinking that we would wish to identify are often likely to be unconscious and difficult to get reliable data on."
"in this study, the properties of the continuous wavelet transform are used to extract the necessary features for eurasip journal on advances in signal processing 3 modulation recognition. the main reason for this choice is due to the capability of this transform to locate, in time and frequency, the instantaneous characteristics of a signal. more simply, the wavelet transform has the special feature of multiresolution analysis (mra). in the same manner as fourier transform can be defined as being a projection on the basis of complex exponentials, the wavelet transform is introduced as projection of the signal on the basis of scaled and time-shifted versions of the original wavelet (so-called mother wavelet) in order to study its local characteristics [cit] . the importance of wavelet analysis is its scale-time view of a signal which is different from the time-frequency view and leads to mra."
"from the considerations above, it becomes clear that heuristic algorithms do not allow us to drop the p-cognition thesis. we may discuss the details, but it seems that we should adopt some form of the tractable cognition thesis. based on the physiological limitations of our brains (as well as our computing tools), algorithms above some complexity level cannot be feasibly implemented by us. consequently, there must be some level of complexity after which problems are no longer computable for us. this is an extremely important point and the way we can analyze it in terms of complexity classes shows the great fruitfulness of the computational complexity approach to modeling cognitive tasks. a vast class of functions can be feasibly dismissed as potential models of human cognitive tasks due to their prohibitive complexity. equally importantly, considering the topic of this paper, a vast class of mathematical problems can be deemed to be unfeasible for humans to solve efficiently."
"in particular, admr has gained a great attention in military applications, such as communication intelligence (comint), electronic support measures (esm), spectrum surveillance, threat evaluation, and interference identification. also recent and rapid developments in software-defined radio (sdr) have given admr more importance in civil applications, since the flexibility of sdr is based on perfect recognition of the modulation scheme of the desired signal."
"one strategy that humans constantly use in mathematical problem solving are different types of heuristic, or didactic, processes. these should not be confused with the heuristic algorithms we discussed earlier. whereas heuristic algorithms in computer science provide partial or approximate solutions, humans use heuristic processes also in processes that lead to exact solutions."
"within the ngn, the policy-based quality assurance qos and qoe seems to be the reasonable approach. the characteristics of the ngn environment present challenges to quality assurance for several reasons, such as general support of mobility, access agnosticism, multidomain environment, best-effort technologies, etc. while mechanisms and technologies for the transport-layer core and access quality assurance are well defined, the issues of interconnection and interworking need to be resolved in order to achieve dynamic service-aware end-to-end user-perceived quality experience. the proposal presented here is an approach that engages all layers of the environment via parameterization, profiling, negotiation, and arbitrating mechanisms, pursuing the end-to-end controllability of the quality of the respective communications. qos is a vital component of any network. qos is even more critical for converged networks. as soon as your network is required to support traffic that is sensitive to delay or packet loss, qos must be present to provide the assurances that these data flows are delivered with timeliness without dropping packets. adding bandwidth to your network might appear to be a cheaper solution, but the unpredictable nature of network traffic flows can result in momentary congestion. if qos is not present, voip and video traffic will suffer from excessive delay and packet loss rendering them ineffective."
"while my focus is on mathematical problem solving, the approach applies also to other domains of cognitive modeling. this paper targets mathematical cognition partly because it works as a case study of modeling cognitive tasks. however, an important reason for the focus on mathematical problem solving is that there is a well-established paradigm in place for characterizing mathematical problems in terms of their computational complexity. finally, given the importance of mathematics in our culture, i believe that there is a great need to treat mathematical problem solving explicitly when it comes to modeling cognitive capacities. this should include philosophical questions concerning that research."
"or approximate solutions, or structures and strategies for solutions-and this process may take less time than an optimal algorithm. but we cannot have full step-by-step solutions that are faster than an optimal solution. however, this does not mean that special human cognitive characteristics (what we might call \"insight\") do not play an important role in mathematical problem solving. here i want to distinguish between optimal algorithms and humanly optimal algorithms. the key idea is that the solution that is most easily accessible to human cognizers may not always be a computationally optimal one."
"the turing machine is closely connected to the computational-level approach in cognitive science. it can be seen as providing a theoretical framework that connects the study of computational complexity and computational-level explanations in cognitive science. marr's work on the computational level was influenced by newell and simon (1976, 1980, who argued that cognitive science should focus on functional explanations of what they called physical symbol systems, i.e., general classes of systems capable of manipulating symbols. marr then developed this idea into the computational level of explanation. for newell and simon, the concept of physical symbol system is specified as an \"instance of a universal [turing] machine\" [cit] . this way, the computational level of explanation has been from the very beginning in close relation to turing machines and therefore also the study of computational complexity."
"importantly, this result can be established without basing it on any experiments on reversi and chess players. although there is obviously a great deal of details concerning the neuronal activity and the actual algorithms that are manifested in solving such problems, the computational-level explanations are informative without involving the other two levels. this way, focusing on the computational-level has a great practical advantage: whereas the algorithmic and implementational levels may be extremely difficult to study, research on computational complexity has an established and highly fruitful methodology when it comes to mathematical (as well as many other) problems."
"the qoe and qos assurance procedures involve vertically the entire ngn environment. on the service layer, the service control and service entities, and profile repositories are engaged, while on the transport layer the user traffic is appropriately handled using various mechanisms (e.g., congestion avoidance, packet marking, queuing and scheduling, traffic classification, policing, and shaping). the resource and admission control entities enforce the arbitrating functionalities that bridge the service and the transport layers. while the entire system is indirectly involved in the qoe and qos assurance, these functionalities directly enforce the dynamic service-aware admission control and resource reservation, as follows."
"in the past, much work has been conducted on modulation identification. the identification techniques, which had been employed to extract the signal features necessary for digital modulation recognition, include spectral-based feature set [cit], higher order cumulants (hoc) [cit], constellation shape [cit], and wavelets transforms [cit] . with their efficient performance in pattern recognition problems (e.g., modulation classification), many studies have proposed the application of artificial neural networks (anns) as classifiers [cit] ."
"the wavelet transforms were calculated, and the median filter was applied to extract the features set. then, preprocessing and features subset selection of 100 realizations of each modulation type/order is performed as a preparation of ann training. the performance of the classifier was examined for 300 realizations of each modulation type/order, and the results are presented using the confusion matrix and false recognition probability (frp)."
"the main purpose of the mother wavelet is to provide a source function to generate ψ a,τ (t), which are simply the translated and scaled versions of the mother wavelet, known as baby wavelets, as follows [cit] :"
"we note that the scaling factor of the cwt has a great effect on the final performance of the classifier. through extensive simulations, the optimum scaling factor was found to be 10 samples. extensive simulations show that the optimal ann structure to be used for this algorithm is a two hidden layers network (excluding the input and the output layer), where the first layer consists of 10 nodes and the second of 15 nodes."
"this approach leaves us with two aspects: the complexity of cognitive tasks in general and the complexity of problems. intriguingly, as we will see, in the subject of mathematical problem solving these two levels can come to be equated when focusing on the computational level of explanation in cognitive science. the complexity of decision problems is a widely researched topic in theoretical computer science and in recent times, it has also become influential in the discussion on modeling cognitive tasks in cognitive science (see, e.g., [cit] . in what is called the computational level of explanation, cognitive tasks are understood as employing cognitive capacities to transfer input states (e.g., perceptions) into output states (e.g., decisions) [cit] . in the widely-used distinction by marr (1977 marr (, 1982, three levels of explanation of cognitive tasks are identified: the computational level, the algorithmic level, and the implementation level (table 1 ). in the context of mathematical problems, the three levels can be understood, respectively, as the computational characterization of the problem, the algorithm used to solve the problem, and the neuronal action involved in solving it. marr's distinction has become widely accepted as the basic framework for studying cognitive capacities (see, e.g., [cit] . [cit] argued that for maximal progress in cognitive science, the focus should be on the computational level. his view (p. 27) was that by studying the computational level, we also get a better understanding of the algorithmic and implementational levels. [cit], agents choose actions they know to lead to their goals. [cit] has argued that through the evolutionary process human competence in cognitive tasks has been optimized, thus explaining why studying the computational level will also provide explanations on the algorithmic and implementational levels. the evolutionary process and rationality are thought to ensure that human cognitive acts are generally optimized for the task and thus the focus can be on the computational level of explanation. as cognitive scientists have developed the computational modeling of cognitive tasks, the focus has indeed moved to the computational level more than to the algorithmic and implementation levels (see, e.g., [cit] . 4 in this paper, i call the approach that combines computational-level explanations with results from computational complexity theory the computational complexity approach to cognitive complexity. in many ways, it is an understandable development. first, it is a practical fact that computational-level modeling is a field in which great progress has been made. we know much less about the actual algorithms used in cognitive tasks, let alone the neuronal activity that correlates with them. second, the results from computational complexity are often easily applicable to the study of cognitive complexity. as we will see, there are classes of problems which have been shown to take prohibitive time to solve. when applied to cognitive complexity, such results immediately rule out many algorithms as explanations of cognitive tasks. third, in focusing on the computational level, there is a natural line of development in terms of abstraction. when making the chomskyan move from performance to competence, the purpose is to get rid of the inter-individual variations in order to establish the general nature of a cognitive ability. in applying the results from the mathematical study of computational complexity, we are taking one abstraction step further and talk about the computational properties of the mathematical functions we believe to model cognitive tasks. this way, if we model a cognitive task accurately, we can determine its complexity objectively. perhaps the human performance on algorithmic and implementational levels is less than optimal, but the focus should be on modeling the cognitive task itself, not the full variety of its practical implementations."
"the policy decision function represents the mediation layer between the service provisioning domain and the network resource-provisioning domain, providing an appropriate level of abstraction of the resource processing technologies to the service execution technologies. the policy decision function issues a request for resource authorization and reservation, indicating the qos characteristics (negotiated with the service provisioning domain). the resource control function is in charge of the permission control mechanisms and informs the policy decision function of the successful resource allocation."
"the classification process basically consists of two phases: training phase and testing phase. a training set is used in supervised training to present the proper network behavior, where each input to the network is introduced with its corresponding correct target. as the inputs are applied to the network, the network outputs are compared to the targets. the learning rule is then used to adjust weights and biases of the network in order to move the network outputs closer to the targets until the network convergence. the training algorithm is mostly defined by the learning rule, that is, the weights update in each training epoch. there are a number of efficient training algorithms for anns. among the most famous is the backpropagation algorithm (bp). an alternative is bp with momentum and learning rate to speed up the training. the weight values are updated by a simple gradient descent algorithm"
"the ip based transport platform spans through core and various types of fixed and mobile access networks. it operates under the control of the arbitrator functionalities. the key objective of this group of functionalities is to provide ip connectivity for the purpose of accessing the service-layer functionalities. at this level, the qos is ensured by using the corresponding mechanisms for the transportation of the media and the reservation, quality, and security accomplishment, which are outside of the scope of ngn. note that the presented generic ngn model comprises core functionalities that represent the enabling infrastructure for session handling, service triggering, admission control, user management, and quality assurance, whereas additional functionalities are required for specific features, e.g., application-related issues, management, real-time streaming support, access termination, etc.,"
"in particular, i will study the question of cognitive complexity from two directions. in the first part of the paper, i will study what we can achieve with the computational complexity approach to cognitive complexity. i will show that this is an important research paradigm because we can establish explicit complexity measures in it, which we can then use to discuss the possible characteristics of cognitive processes. however, i will also argue that this approach alone is not sufficient for studying cognitive complexity. in the second part of the paper, i will show that the reason for this is that it fails to take into consideration the plurality of processes used by human cognizers when solving a mathematical problem. the method of assessing the computational complexity of mathematical problems is closely related to the idea of optimal algorithms, i.e., algorithms requiring a minimum of computational resources for solving the problem. here i will argue, however, that human cognizers use problem solving algorithms that may in fact be computationally suboptimal. 1 i will show, among other examples, that diagrams and other heuristic and didactic tools play an important role in mathematical problem solving, yet they add to the complexity of the problem solving algorithm. on these grounds, i will argue that we should distinguish between computationally optimal and what i will call humanly optimal algorithms. 2 the first part of this paper (sects. 2-5) studies the approach based on computational complexity. in the first section, i will present the modern computationallevel paradigm for studying cognitive complexity, which will then be given a more detailed treatment in sect. 3. in sects. 4 and 5, i will present the possibilities and the limits of this paradigm in explaining cognitive phenomena. in the second part of the paper (sects. 6, 7), i expand the methodology beyond the computational complexity approach. in sect. 6, i aim to show how the human problem solving algorithms may be different from the optimal algorithms studied in the computational complexity approach. finally, in sect. 7, i discuss the notion of humanly optimal algorithm, concluding that rather than looking for a uniform notion of mathematical competence, we should expect both cross-cultural and intra-cultural variation in the problem solving methods."
"when we study human mathematical problem solving abilities, it is quite obvious that we will encounter a great deal of suboptimal performance. humans make errors and even when correct, they may use algorithms that are less than optimal. in the tradition of chomsky and marr, such use of suboptimal algorithms is deemed to be variation in performance and as such irrelevant for studying competence. since that paradigm aims to explain general human competence when it comes to mathematical problem solving, in that respect it is not important that people do not always reach the full competence, nor that even competent problem-solvers are not always completely error-free and optimal."
"it is essential to focus on the fact that in our algorithm the different modulated signals are digitalized in rf or if stages (the carrier frequency is unknown) with respect to sdr principles. the recognition is done without any priori signal information, and our algorithm shows robustness over fading channels."
"taking time as the measure, one of the most important complexity classes is called p and it is defined as the class of decision problems that can be solved by a deterministic turing machine in polynomial time [cit] . an algorithm (i.e., a turing machine) is said to run for polynomial time if its running time has an upper bound of a polynomial function of the size of the input for the algorithm. this means that if the size of the input is n, the running time has an upper bound of some function n k for some constant k. another important complexity class is called exp (or exptime) and it is the class of decision problems that are solvable by a deterministic turing machine in exponential time. an algorithm runs for exponential time if its running time has a lower bound of some exponential function of the size of the input, i.e., for input size of n, the running time has a lower bound of some function 2 p(n) where p(n) is some polynomial function of n. p and exp form an important pair of complexity classes for two reasons. first, according to the widely accepted cobham's (or cobham-edmonds) thesis, p is generally seen as the class of problems that can be feasibly solved by a computer. second, it has been proven that the complexity class exp is strictly greater than p. algorithms for solving problems in p are called efficient, or tractable. algorithms for solving problems in exp (that are not in p), on the other hand, are inefficient or intractable. as we will see, this distinction is very important when we consider the computational complexity of functions that model cognitive tasks. for computer science, the distinction is crucial. a simple example of an efficient algorithm to define the computational complexity of functions in terms of optimal algorithms for solving them (i thank an anonymous reviewer for bringing up this point)."
"cognitive complexity as a scientific term does not have a fixed cross-disciplinary meaning. if we take the dictionary definition of \"cognitive\" as referring to conscious or conscious mental processes such as remembering, reasoning and knowing (among others), cognitive complexity can be understood as the measure of simplicity of such activity. however, both the relevant activity and its simplicity can be understood in different ways."
the proposed classifier has shown an excellent performance over awgn channel even at low snr. table 2 values shows that the full-class recognition gives the lowest percentage for psk signals.
the network attachment subsystem (nass) has also been considered for the ngn-based environment within the standardization efforts of etsi tispan ngn and itu for the purpose of consistent and controlled registration and attachment of the end users accessing the ngn services through various access networks. the nass is responsible for the registration procedures within the access domain and the initialization of the end user's terminal equipment when accessing the
"lda is a supervised technique that attempts to maximize the linear separability between data points (features) belonging to different classes (targeted modulation schemes) [cit] . it does so by taking into consideration the scatter between-classes besides the scatter within-classes, that is, finds a linear transform so that the between-classes variance is maximized, and the within-classes variance is minimized. the within-classes scatter s w and the between-classes scatter s b are defined as"
"what all such approaches have in common, however, is that they do not focus on the human aspect of mathematical problem solving. as we have seen, we can do a great deal of important work purely theoretically in the computational approach to cognitive modeling. but obviously there are limits to this: at some point we need to study the actual human cognitive capacities in order to find out which functions can model them. to assess the computational complexity of different cognitive tasks, we obviously first need to model those cognitive tasks as mathematical functions. the interesting thing about mathematical problem solving, however, is that we often seem to be able assess the complexity in a purely a priori manner. by studying the two simple problems presented in sect. 3, for example, it is quite understandable to deem the three-number case as cognitively more complex than the two-number case, without ever conducting empirical research on how human reasoners in practice solve the problems."
"ann usually consists of several layers. each layer is composed of several neurons or nodes. the connections among each node and the other nodes are characterized by weights. the output of each node is the output of a transfer function which its input is the summed weighted activity of all node connections. each ann has at least one hidden layer besides the input and the output layers. there are two known architectures of anns: the feed-forward neural networks and the feedback ones. there are several popular feed-forward neural network architectures such as multi-layer perceptrons (mlps), radial basis function (rbf) networks, and selforganizing maps (soms). we had chosen mlp feed-forward networks in our work because of their simplicity and effective implementations; also they are extensively used in pattern recognition and data classification problems."
"however, this is not to suggest that the computational level is preferred merely due to pragmatic considerations. [cit] have stressed that there is greater explanatory power in understanding the computational characteristics of a particular cognitive task than in explaining the algorithmic and physical manifestations of the task. indeed, marr argued that the computational level of explanation is in fact crucial when we try to explain cognitive tasks on the algorithmic and implementational levels:"
"the performance of our algorithm has been evaluated in the case of full-class recognition when the snr is not lower than 4 db over different fading channels. the examined channel models were derived for the standards and specifications: gsm/edge channel models (3gpp ts 45.005 v7.9.0 (2007-2)) [cit], cost 207 channel models [cit], and itu-r 3g channel models (itu-r m.1225 (1997-2)) [cit] ."
"when it comes to turing machines, there are two main ways of measuring the complexity of algorithms: the time and the space it takes to run an algorithm. since the turing machine is an abstract model, time and space are not measured in seconds or bits. instead, they are measured as a function of the size of the input. this has proven to be a highly fruitful approach and as perhaps the most important result, we can divide computational problems into complexity classes."
"12 alternatively, complexity for particular cases can also be measured in terms of computational steps, as in the example in the beginning of sect. 3 of this paper. all these approaches have their difficulties, but a wider research paradigm with a plurality of measures of complexity can give us tools to introduce more informative distinctions between the complexity of mathematical problems."
"the ip multimedia subsystem (ims), defined by the third generation partnership project (3gpp) and later adopted by the etsi tispan, has become recognized as the core session control, service triggering, and aaa framework for the delivery of convergent multimedia services within an efficient service delivery environment. initially it has been proposed as the control subsection of the universal mobile telecommunications services (umts) environment, however further expansions have been completed to meet the fixed domain requirements and to address a wider system concept. nevertheless, both proposals pursue access agnosticism and general user mobility. logical structuring is clearly defined; session control, user and application data, gateway control and gateways and service environment all reside in clearly separated entities. interconnection amongst these segments and towards outer world is achieved through open standardized interfaces based on sip and diameter protocols and different types of interface technologies."
"one common method for this is modeling the cognitive tasks computationally, i.e., identifying a mathematical function that models the particular cognitive task. to assess the complexity of the task, we then assess the computational resources it takes for an algorithm to compute the function. when it comes to mathematical problem solving, this approach gives rise to a clear research paradigm. in theoretical computer science, the complexity of mathematical problems is studied in terms of their complexity, which is characterized by the resources (time or space) required for running algorithms that solve the problems. it is thus understandable to characterize the complexity of the cognitive task of solving a mathematical problem in terms of the complexity of that problem. in this paper, however, i will show that this approach is flawed and we need distinct concepts of cognitive complexity and computational complexity."
"let us suppose that x is the input data (extracted features). pca attempts to find the linear transformation w which maximizes w t cov (x−x) w, where cov (x−x) is the covariance matrix of the zero-mean data. it can be shown that w is formed of the first d principal eigenvectors (i.e., principal components) corresponding to the greatest d eigenvalues of the covariance matrix. the selected features are given by"
"in order to have transparently decoupled service and transport layer, specialized arbitrator functionalities are needed to implement the inter-layer communications and transport control logic. the network attachment subsystem (nass) is needed that enables the end users admission to the ngn ecosystem and the ngn services, and sustains transport-layer profiles. the resource and admission control subsystem (racs) performs policy-based resource allocation and appropriate qos assurance."
"the measure of system performance represents one of the basic evaluation criteria of a successful network, solution or a service from nearly all viewpoints: deployment, operation, and customer satisfaction."
"(iii) the statistical properties including the mean, the variance, and higher order moments (hom) of wavelet transforms are different from modulation scheme to another. these statistical properties also differ depending on the order of modulation, since the frequency, amplitude, and other signal properties may change depending on the modulation order."
"the large number of extracted features causes that some among them share the same information content. this will lead to a dimensionality problem. the obvious solution is the features selection, that is, reducing the dimension by selecting some features and discarding the rest. a features space with a smaller dimension will allow more accurate classification (regardless the classifier) due to data organization and projecting data to another space in which the discrimination is more obvious. the output of the features selection process is the input of the feed-forward neural network. then, features selection also affects the neural network convergence and allows speeding its learning process and reducing its size. among several possible features selection algorithms, we will investigate principal component analysis (pca) and linear discriminate analysis (lda)."
"so how can the complexity of the cognitive task of problem solving be assessed? in mathematics education, this is often a question tested in practice. low performance by students in a particular problem is easy to interpret as showing high cognitive complexity of the task of solving it. however, from a theoretical perspective, this practical approach is unsatisfactory, as it does not necessarily distinguish between the cognitive processes involved. from the perspective of cognitive science, we are primarily interested in identifying those processes, after which we can assess their complexity."
"let us examine the effect of the number of received symbols, n s, on the algorithm performance. the results of this investigation are shown in figure 10, where the frp for several recognition cases is shown at a prescribed n s . similar to the definition of snr min, we define n min as the minimum n s value for which frp is less than 1%. we found that n min for inter-class recognition (case i) is 100 symbols, for full-class recognition is 100 symbols, for intraclass fsk recognition is 75 symbols, and for intra-class qam recognition is 50 symbols."
"although the case of gcd is not clear, it reminds us of an important point: we should not treat all mathematical problems in the complexity class p as being the same in terms of complexity. crucially, even if a problem were solvable in polynomial time, it could be in practice beyond the human capacity to solve it. all the p-cognition thesis gives us is an upper bound for human problem solving capacity. but within p there are problems with very different computational complexities. when studying the complexity of human mathematical problem solving ability, we should be interested in those differences. if it is possible to identify within p complexity measures that characterize mathematical problems in a more fine-grained manner, we would get important information also for identifying the cognitive tasks involved in the problem solving process."
"it should be noted here that taking didactic processes into consideration does not in any way imply that the cognitive task of mathematical problem solving could not be computationally modeled. rather, the argument here is that instead of focusing only on the input (the problem) and the output (the solution), we must be prepared to take into account all the relevant cognitive processes involved in mathematical problem solving. some of these processes concern individual performance and should not be included in accounts of mathematical competence. but some processes, like those including diagrammatic reasoning, are likely to be generally applied by human problem solvers (at least those with a shared cultural background) and cannot be dismissed as dealing with performance. when it comes to modeling mathematical competence computationally, we need to identify these latter processes before we can know what we are modeling."
"the communications are no longer limited to the choice of voice, data, or video: their multimedia nature presumes an enhanced end user's experience engaging various services and contents within a single convergent session. commonly understood as the next generation networks (ngn), a composite environment of proven telecommunications and internetoriented mechanisms is established, enabling agile service creation, access agnosticism, and global mobility of end users. the ngn environment is based on the internet protocol (ip) transport platform and adopts a model of a transparently separated service provisioning platform above a heterogeneous transport and access platform, employing various technologies to accomplish the ip connectivity. unlike legacy solutions, the ngn tends to be access agnostic; from the functional viewpoint it consists of subsystems-logical groupings of entities that perform precisely defined functionalities-which originate from both fixed and wireless domains and promote unlimited choice of access possibilities (e.g., fixed-dsl, cable-or wireless -umts, wimax, wifi). the key objective of the ngn environment is to converge and turn to advantage the benefits of the two communications worlds by combining the controllability, reliability, and quality of telecom with the flexibility, ease of operation, creativeness, and end users' involvement of the internet."
"let us start by establishing a general sense in which one cognitive process can be more complex than another. think of two people reasoning about the weather. the sky is red during sunrise and john remembers the saying \"red sky at morning, sailors take warning\", thus concluding that he should not go boating. mary, on the other hand, reasons that since the sun is in the eastern horizon, the red color must mean that there is clear sky in the east which allows sunlight to color the clouds in the west. since winds in that particular region (e.g., the atlantic) tend to be from west to east, mary concludes that more clouds and perhaps storms are coming and thus it is better to stay ashore. this way, both john and mary make the same observation and end up with the same action, yet it is clear that mary's reasoning is more complex."
"it should be noted that while it may seem that cognitive complexity of mathematical problem solving can be studied independently of all empirical data, the matter may not be that simple. it is certainly true that the complexity of the cognitive task of solving a problem is often assessed based on the computational complexity of the problem, which can give the impression that the assessment is purely a priori. however, it seems feasible that this methodology would not be used if it were in conflict with empirical data. there are at least two ways in which the seemingly a priori 11 there are complexity classes that are known to be strictly smaller than p, such as, a class of circuit complexity. but these classes are generally very weak and thus of little use in distinguishing between mathematical problems."
"presenting and calculating the wavelet transform of digitally modulated signals using different modulation schemes will clarify the role of wavelet analysis in feature extraction procedure. the wavelet analysis concept will be studied using only one family of wavelets (haar wavelet). all the results and figures of cwt presented in this section are obtained using the haar wavelet. nevertheless, in our simulations we will extend our results to other families including daubechies, morlet, meyer, symlet, and coiflet."
"one cannot specify algorithms without at least some considerations of possible implementations, and what is to be considered \"computation\" (i.e., what can be computed) relies on algorithms, especially the notion of algorithmic complexity, and so on. therefore, one often has to consider computation, algorithms, and implementation together somehow (especially in relation to cognition). [cit] this developing modern paradigm fits well with the theoretical considerations of this paper. as we have seen, the computational approach can be needlessly limiting and may preclude finding important data about the algorithms used by human cognizers. without including the algorithmic level, we may not be able to identify the cognitive phenomena we want to model in the first place. this way, dismissing the algorithmic level leads to problems also on the computational level."
"as a naive initial description, we can calculate the steps it takes to get the correct answer. given the input a, b, we go through the process characterized by the following algorithm:"
"it is of course possible that diagrams and other didactic methods are not indispensable for all humans in solving any particular mathematical problem. even if diagrams were generally useful, it is possible that all mathematical problems could be solved by some human reasoners without them. in that case we could in principle find for any problem a solution in which the human cognizer uses a computationally optimal algorithm. indispensability, however, is too strong a condition for the current purpose. what we want to know is whether there are problems in which human reasoners generally benefit from diagrams or other didactic and heuristic tools. with many problems, it seems highly probable that this is the case. if we are looking to establish what the human competence in certain problem solving task is, as indeed is the purpose in the paradigm established by chomsky and marr, it seems clear that we should be interested in this general trait, rather than possible outliers."
"this paper addresses a question of great theoretical interest in philosophy, psychology and cognitive science: how should we assess the complexity of cognitive tasks? although the analysis carried out in this paper is applicable to cognitive tasks in general, here i am particularly interested in mathematical problem solving as a case study. in addition to the theoretical aspect, although not usually framed in these terms, this is also a practical question that mathematics educators face constantly. in designing exams, for example, the teacher has to use some kind of (perhaps implicit) measure for evaluating problems in terms of the complexity of the cognitive task of solving them. the required cognitive tasks cannot be too complex, but they should be complex enough in order to assess the knowledge and skills relevant to the mathematical concepts."
"based on the above considerations, it is unlikely that these kinds of simple mechanical operations are where we can find the kinds of humanly optimal algorithms we are after. for that, we should look at more complex mathematical tasks, in which the human cognitive task could differ importantly from an optimal computational solution. unfortunately, however, the more complex a mathematical task is, the more problematic it becomes to study the cognitive processes that are involved in solving it. still, mathematical practice gives us reason to believe that mathematical problem solving generally is not a case of applying an optimal algorithm. instead of merely constructing step-by-step solutions, in mathematics a wide array of different cognitive resources are used. as analyzed by, e.g., [cit], it is clear that problem solving is not a straight-forward process where different algorithms are tested until the correct one is iteratively hit upon. in schoenfeld's analysis, mathematical problem solving draws from four factors: resources, heuristics, control and belief systems. while control (i.e., resource management) and belief systems (i.e., mathematical world view) form interesting research questions, here i am more interested in the first two. how do human cognizers typically use their body of knowledge about mathematics to solve a particular problem? and crucially to the matter at hand, can analyzing human problem solving from this perspective provide insight into cognitive complexity that goes beyond the computational complexity approach?"
"it should also be noted that mathematical competence is a much wider phenomenon than simple problem solving. throughout this paper the focus has been on problem solving, and even that only in a very limited paradigm set by turing machines, i.e., step-by-step algorithmic solutions. actual mathematical problem solving is of course a much more complicated phenomenon in which there are many important socially and culturally determined aspects, such as clarity of expressions and proof structures. in addition, there are numerous epistemic considerations involved in mathematical cognition, whereas in this paper we have only focused on problem solving. from verifying the solutions to communicating and explaining them, enculturated mathematical competence includes various dimensions which have not been analyzed here. this is not an oversight, as i believe that research of mathematical practice is crucial in determining what enculturated mathematical competences are on higher levels of mathematics. rather than dismissing the importance of such considerations, my purpose in this paper has been to show that we need a notion of humanly optimal algorithms and enculturated competence already when we seek to explain mathematical cognition in the limited paradigm of problem solving algorithms. in order for us to be able to study mathematics as a human phenomenon in all its richness, we may need to expand the paradigm even more."
"one important aspect of developing this notion of enculturated competence is that we should also consider the question of human-computer interaction in mathematical problem solving. to present just one example, as computer-assisted problem solving becomes more prevalent, it makes sense to move the focus to the cognitive and computational complexity involved in the computer-assisted process. it is easy to predict that if mathematics relies increasingly on problem solving (or solutionchecking) computer programs, many of the human heuristic and didactic tools, such as diagrams, may no longer play the same kind of role as they currently do. however, in this scenario we get new interesting questions concerning cognitive complexity, including considerations on the understandability of computer programs. increasing human-computer interaction will undoubtedly change the way mathematics is practiced and learned, but it seems unrealistic to assume that the heuristic and didactic elements important for human understanding will disappear. in this way, although the study of cognitive complexity may evolve into new directions, there is no reason to believe that the cognitive complexity of human mathematical problem solving will become completely reducible to the computational complexity of the problems. instead, what will emerge is a new enculturated notion of mathematical competence, one involving algorithms with their own particular characteristics. in that scenario, it is important to have a research paradigm that does not focus only on computational complexity and the computational level of explanation."
"in the computational-level approach of chomsky and marr, such a result would be dismissed due to it concerning performance rather than competence. but what if we acquired extensive data and found out that humans generally use a suboptimal algorithm like a* to solve the problem? in that case there would clearly be a unique algorithm to describe the human competence, yet it would not correspond to an optimal algorithm. such an example would show, contra marr, that the explanation cannot focus mainly on the computational level. as we have seen, in the computational complexity approach optimal algorithms are used to model human cognitive competence. but if human performance generally follows a suboptimal algorithm, how can we retain this approach?"
"an improvement can be carried out to our work by using an algorithm that can automatically optimize the neural network size by balancing the minimum size and the good performance, since it is harder to manually search the optimal size. there are several techniques that help to approach the optimal size; some of them starts with huge network size and try to prune it toward the optimal size [cit], others start with small network size and try to increase it toward the optimal size [cit], and some works combine both the pruning and the growing algorithms [cit] ."
"a good qos system supports standards so that each network component interacts in a heterogeneous networking environment comprised of different vendor's equipment. as a network administrator you may not always be in control of the type of equipment that will be included in your network. as a result of acquisitions, you may find yourself faced with an integration scenario that will be much easier to address if your networking equipment supports standard protocols that allow qos functionality to be mapped between the various layers resulting in effective, heterogeneous networking."
"in this paper, we consider the resilient backpropagation algorithm (rprop) [cit] . basically, rprop performs a direct adaptation of the weight update based on local gradient information. only the sign of the partial derivative is used to perform both learning and adaptation. in doing so, the size of the partial derivative does not influence the weight update."
"simulation results in table 3 show that when the snr is not lower than 3 db, the percentage of correct interclass recognition of ask, fsk, msk, psk, and qam modulations (case i) is higher than 99%. for lower snr values, our results show that the inter-class recognition gives the lowest percentage for psk and fsk, but the inter-class modulation recognition will remain robust for lower snr values for qam and ask signals. we note that, reducing the modulation pool used in simulations to qam, ask, and fsk (case ii) shows a high percentage of correct interclass modulation recognition for lower snr value (−2 db), as shown in table 4 ."
"the turing machine has become the established paradigm for studying algorithmic, mechanical procedures. in the study of complexity in theoretical computer science, researchers are not interested in the computing capacities of particular computers. instead, they want to study the inherent complexities of different tasks free from the limitations of physical computers. under this approach, the complexity of a mathematical problem can be characterized by the complexity of the least complex turing machine (i.e., the algorithm run by a turing machine that takes a minimum of resources) that solves the problem. such an algorithm is called optimal. here it should be noted that for practical purposes, optimality may include considerations of various aspects, such as stability, verifiability, simplicity of coding, etc. the context of turing machines here is a purely theoretical one in which such matters are ignored, important as they are in practice. it should also be noted that optimal algorithms are not unique. in theoretical computer science, an algorithm is called (asymptotically) optimal if it never performs more than a constant factor worse than the best possible algorithm. there can thus many (even an infinite number of) optimal algorithms. finally, although the method of characterizing complexity of a problem in terms of an optimal algorithm for solving it is commonplace, we know from blum's speedup theorem [cit]"
"as fruitful as the computational approach to both cognitive modeling and mathematical problem solving has proven to be, it is clear that the limits it imposes are in practice rather weak. even by accepting the p-cognition thesis we are left with an enormous class of functions that can potentially model human cognitive capacities, namely all functions computable in polynomial time. finding the greatest common divisor (gcd) of two integers, for example, is known (in its decision form) to be in the complexity class p, yet a sequential solution for it can take a prohibitive amount of time. it could be the case that the gcd could be solved faster by parallel processing, yet there is no known way of parallelizing the computation effectively. 10 the algorithm for solving gcd may thus be seen as unfeasible as a model of human cognitive capacities."
"in general, separate resource control functions exist for the core network and for each type of access network, taking into account specific characteristics and management policy. in the process of the resource allocation it consults the network attachment subsystem (nass) for the access and transport-layer qos profile. other functionalities of the racs are the border gateway functions and the resource control enforcement functions that perform the gate control, packet marking, resource allocation, network address translation, policing and usage metering, etc. in general, the resource control functions act as the local policy decision points in terms of subscriber access admission control and resource handling control, whereas the policy decision function represents the final policy decision point."
"these practical difficulties of studying mathematical cognition should not be downplayed, yet at the same time they should not be thought of as an argument against engaging in the algorithmic level of explanation. from the perspective of computational complexity, it may be asked why we should be interested in modeling suboptimal algorithms in the first place, whether they concern diagrams and other didactic and heuristic tools, or unconscious processes like those underlying the distance effect. after all, the computational complexity approach has an established fruitful methodology, and including algorithmic considerations only makes the problems more complicated and difficult to study. however, the fruitfulness and established methodology of the computational complexity approach should not lead to a streetlight effect in which the ease of explanation becomes a determining factor in establishing the research paradigm. 24 however, aside from such practical worries, there is also a potentially important theoretical problem in algorithmic-level explanations. in the previous section, i proposed that we can consider an algorithm humanly optimal if it is generally beneficial for human problem solvers. but as was also mentioned in that section, there is no reason to believe that there are unique humanly optimal algorithms for mathematical problems. indeed, since mathematical practices (including symbols, tools, etc.) vary in different cultures, it seems clear that we should be looking for optimal algorithms in culturally specific contexts. for example, let us consider the \"japanese\" visual multiplication method (fig. 6), in which orthogonal lines are drawn for ones, tens and hundreds, etc. and the result is reached by counting the intersections of the lines."
"in their work, they have used a wavelet transform to extract 2 eurasip journal on advances in signal processing the transient characteristics in a digital modulated signal. it has been shown that when the signal-to-noise ratio (snr) is greater than 5 db, the percentage of correct identification is about 97%."
"similarly, i want to introduce the term \"humanly optimal algorithm\" as a characterization that we can use as a tool in modeling the problem solving strategies that human cognizers use, rather than something that could be universally defined for particular mathematical problems. as will be discussed in sect. 7, what is humanly optimal depends on how our problem solving processes are culturally determined. as such, the general use of the term \"humanly optimal algorithm\" should be understood as a tool that can be used to introduce a new approach to modeling human problem solving processes and characterizing their complexity. the important idea is that humanly optimal algorithms are not necessarily computationally optimal, but cognitively optimal for human cognitive agents with specific learning trajectories. i will argue that this approach allows for more accurate characterizations than the computational-level approach focusing on computational complexity. in the final section of this paper, we will see in more detail how the concept of humanly optimal algorithm can be used in pursuing more accurate models of human problem solving processes, and therefore also more accurate characterizations of their cognitive complexity. but first, let us take a look at some examples of the ways in which human problem solving processes can be computationally suboptimal."
"however, there are notable mobile characteristics that should be considered that affect the performance of the system as a whole and condition the quality-related issues. for the purpose of quality assurance procedures within the ims-based ngn environment, the profile entity is important, incorporating relevant subscriber, service and content information. the hss/upsf entity of the ims subsystem sustains the service-layer profile repository, as depicted on figure 2 . figure 2 . the key quality-related ims entities and the service-layer profile repository information"
"the ngn qos mechanisms are technology dependent and extend vertically across transport layer and transport control functionalities of the service layer. on the other hand, ngn qoe mechanisms are technology independent and involve service control and application functionalities as well as the mapping of these to transport-layer quality assurance. only overall integrity and orchestration of all functionalities in all subsystems and layers brings systematic quality assurance in all aspects of service delivery. based on these prerequisites, the following approach is generally recognized for the ngn environment. the procedure of quality assurance occurs in two stages. first, dynamic negotiation is conducted to set the initial communications parameters in the session set-up procedure. afterwards, further renegotiations are possible, initiated either by the end user, network, or services."
"in fact, in many cases, the diagram does not advance the formal solution at all. in such cases, the function of the diagram is purely didactic (fig. 5) . what the diagram does in such cases is assist us in finding the formal solution. for a problem solving computer program, this would mean that the initial representation of a mathematical problem is changed into another mode of representation, but ultimately the solution must be presented in terms of the initial representation. the solution via this process is generally computationally more complex than an optimal algorithm for solving the problem. in the case of bolzano's theorem, for example, it seems clear that a theorem-proving computer program would not benefit from changing into a visual representation."
"in computer science, an algorithm refers generally to a finitely describable welldefined procedure which takes an input and after a finite number of steps produces some output and halts. from a modern perspective, the turing machine gave a highly intuitive characterization of algorithms: essentially, they are the kind of procedures that can be run by digital computers. in this paper, we are interested in algorithms from two perspectives. first, in computational complexity theory, we are interested in optimal algorithms for solving a mathematical problem. second, from the cognitive perspective, we are interested in algorithms that model human cognitive capacities. as was characterized in sect. 2, in the computational complexity approach to modeling cognitive tasks those two approaches are merged."
"cascade-correlation algorithm (cca) attempts to automatically choose the optimal network size [cit] . instead of just adjusting the weights in a network of fixed topology, cca begins with a minimal network, and then automatically adds new hidden nodes one by one, creating a multi-layer structure. for each new hidden node, cca attempts to maximize the magnitude of the correlation between the new node's output and the residual error signal which cca is trying to eliminate."
"it should be noted that there are some differing voices on this matter. np-hard functions have been suggested as models at least for visual search [cit] and analogical reasoning [cit] . in addition, some researchers have suggested that humans are able to solve the np-hard euclidean traveling salesperson problem near-optimally in a short time [cit] . however, we should not confuse the actual use of np-hard functions as models with them being accurate models by the strict criteria applied here. neither should we believe that near-optimal performance in np-hard tasks implies that the cognitive processing can only be modeled with np-complete functions. it is conceivable that in many problems we can reach very good solutions with an approximate algorithm, but these should not be confused with proper exact solutions. [cit] suggest that human performance is often not even nearly optimal in the euclidean traveling salesperson problem."
"however, it could also turn out that in order to have maximally useful computational characterizations of mathematical problems for the study of cognitive complexity, we need to introduce more fine-grained measures of complexity than provided by complexity classes like nc, l, p, np and exp. this can be done by studying the functions that are upper bounds for the running time of an algorithm that solves a problem. or in a pragmatic approach, we can measure the actual running times of algorithms on physical computers. finally, we could also introduce another notion of complexity, such as kolmogorov complexity, to work as the relevant unit of measure."
"the the generic ngn architecture and its functionalities are represented in figure 1 . a twolayer model is adopted, logically decoupling the transport from the service control functionalities and the services. four principal groups of functionalities within the ngn architecture can be identified, as follows:"
"the upper-most entities of the service layer represent various general or dedicated application servers (as), where service logic is hosted and operated. additionally, the developer-friendly interface functionalities and secure gateway functionalities for third party service provisioning are enabled. the openness and the support for various technologies result in considerable complexity of this ngn segment, and the blended service offering requires mutual engagement and coherent functioning of many application servers simultaneously, therefore orchestration application servers are needed. in this segment, session control, service triggering, and authentication, authorization, and accounting mechanism (aaa) are implemented. service-layer profiles are sustained here, incoming requests are routed to the appropriate entities and services are triggered. recently, the ip multimedia subsystem (ims) (3gpp ts 23.228, 2006; etsi es 282.007, 2006) has become the recognized standard for service-layer functionalities and is today incorporated into the majority of recommendations. for this reason, the remainder of this paper assumes the ims as the core of the service layer. the ims provides the core session control, service triggering, and authentication and authorization mechanisms for the ngn environment."
"the inherent nature of the ims as the core session control subsystem is global mobility of end users, services and the ability of these to be independent of the selected access domain and terminal equipment. the ims-based ngn environment is applicable to both fixed and mobile domains regardless of the initial mobile origin of the ims subsystem."
"the process of quality assurance in the ngn environment is a challenging task due to several factors. the ip-based next generation environment, originating from internet domain, is best effort and therefore requires several additional mechanisms to meet the appropriate quality and availability levels. the issue is even intensified due to an extensive range of different media-rich services, which presents a challenge to resource allocation in terms of diverse performance needs (e.g., real-time or near-real-time delivery, priority treatment). in the ngn environment a single session operates across many conceptually and technologically unfamiliar networks, operated by different operators; moreover, the operators do not have full control over the environment as in the legacy telecommunications solutions and each end user is increasingly involved in the shaping of the operation of the environment through the usage of intelligent end user's devices and service personalization."
"due to limits in space, it is not possible to present the details of these algorithms, but in all three cases, human problem solvers do not generally use the faster algorithm to solve the task. thus all three examples appear to give us clear cases where algorithms with lower time-complexities are not used by human problem solvers. this would seem to go against the basic paradigm of the computational complexity approach to mathematical problem solving: for some mathematical problems, the lowest-complexity algorithms cannot feasibly characterize the cognitive processes of human problem solvers."
"this way, what may seem like a priori assessment of cognitive abilities may in fact be at least partly based on empirical considerations. although the empirical data may not be explicitly acknowledged, using computational complexity to explain the complexity of cognitive tasks may be empirically justified. this empirical aspect notwithstanding, however, in practice the exclusive focus on the computational level of explanation can make the approach largely a priori. in a purely computationallevel approach it is natural to assume that human competence can be modeled by optimal algorithms for solving mathematical problems, rather than studying empirically what kind of problem solving algorithms actual human reasoners use."
"however, there is no compelling argument why this should always be the case. the evolutionary argument for the optimization of cognitive tasks, for example, while perhaps appealing in many cases, seems to be a bad fit for mathematical problem solving. on an evolutionary scale, the emergence of mathematics appears to be too recent an event to allow for the optimization of problem solving strategies [cit] . while an accurate timing of the emergence of the first finger counting and other body part systems is impossible, the earliest known written systems of numerals are from around 3000 [cit] bc [cit] . of course mathematical cognition may apply cognitive patterns that have been evolutionarily developed for other purposes, but with this relatively recent history it seems quite plausible that our mathematical problem solving ability can include aspects that are suboptimal. indeed, in sect. 6 i will argue that such suboptimal algorithms play an important role in actual human problem solving processes."
"also, the different studies considered different modulation pools and different simulation configurations which will result in different and incomparable performances. some algorithms need a priori information of the signal, for example, carrier frequency [cit], frequency offset [cit], and channel information [cit] ."
"in the rest of this paper i will argue that indeed we cannot, and we must take into account also marr's level 2, the algorithmic level of explanation. 17 we will start by looking at three cases in which the humanly optimal algorithms for mathematical problem solving are not the same as the algorithms with the lowest complexity, and assess the computational approach in each of them."
"the problem of modulation recognition will be investigated with three scenarios: (i) inter-class recognition (identify the type of modulation only), (ii) intra-class recognition (identify the order of known type of modulation), and (iii) full-class recognition (identify the type and order of the modulation at the same time), as shown in figure 8 ."
"all this is important for us for two reasons. first, since the conjecture ≠ is generally accepted, the complexity class np is de facto the lowest complexity class of problems which are thought to be computationally intractable. second, the class of np-complete problems includes many famous mathematical problems, such as graph coloring and the traveling salesman problem (in its decision version). it also includes many familiar games, such as rubik's cube (finding an optimal solution) and sudoku (generalized). in short, many of the kind of problem solving tasks that human beings engage themselves in are np-complete."
"the resource control function derives and installs the layer 3 and layer 2 traffic policy, indicating the traffic control handling (e.g., gate control, packet marking, etc.). in the process of granting the resources the network qos parameters of the layer 3 and layer 2 are mapped to the respective policy. the operation of the racs is generally application agnostic but supports traffic control for the purpose of application delivery with uni-/bidirectional, a-/symmetric, uni-/multicast, up-/downstream traffic patterns."
", for example, does not even contain integer multiplication [cit] ). 12 kolmogorov complexity refers to the length of the shortest computer program that has an informative object, such as a string of symbols, as its output. for example, the string of symbols \"aaaaaaaaaaaaaaaaaaaa\" has a lower kolmogorov complexity than the string of symbols \"keehfydo38d-krislero29s\". both strings are 20 symbols long, but whereas the second string cannot be described with a shorter string, the former can. for instance, the english description \"20 times a\" is 10 symbols long. however, kolmogorov complexity is not without problems. while it may seem like a straight-forward concept, it has turned out that determining the kolmogorov complexity of even short strings of symbols is a highly difficult task [cit] ). assessment of cognitive complexity can in fact be based on empirical results. first, all the data we have about cognitive capacities in general can be used to justify the assessment of the complexity of mathematical tasks. the limits of working memory, for example, impose limits on our problem solving capacity, as do physical limitations in using external tools. we do not need to empirically study each mathematical problem if we can establish that a certain class of problems puts more demands on working memory than another class. second, the general method of using computational complexity to characterize the complexity of the problem solving tasks only works because in many cases we know the human competence to be modeled reasonably well by computationally optimal algorithms. empirical data on the competence in the addition of (small) natural numbers, for example, implies that an optimal algorithm can be used to characterize the complexity of the human cognitive task [cit] ."
"diffserv provides a scalable aggregate approach to categorize into different classes that are subjected to a specific treatment, known as phb (per hop behavior). ietf defines three main groups of classes: ef (expedited forwarding), af (assured forwarding) and be (best effort). the ef class aims to provide low loss, low delay and low jitter guaranteed services. the af class gives different forwarding assurances in terms of loss, delay and jitter. it is composed of a set of policy enforcement point (pep), a policy decision point (pdp) and a policy repository component. the pep component is a policy decision enforcer located in the network and system equipments. the pdp is a decision-making component that governs the logic of the overall management system based on the high level directives of the administrator/operator based on the agreed sla (service level agreement) with his customers."
"an algorithm is likely to be understood more readily by understanding the nature of the problem being solved than by examining the mechanism (and the hardware) in which it is embodied. [cit] seen in our present mathematical context, the implications of this approach are clear. if we want to examine the way human beings solve mathematical problems, we should focus on understanding the computational nature of those problems. this way, the computational complexity approach to studying mathematical problems can be explicitly formulated as the primary route to explaining human mathematical problem solving competence."
"so far our results are based on haar wavelet. now we examine the proposed algorithm using different wavelet families seeking the optimal wavelet filter to be used. in particular, we provide in table 9 the total recognition the confusion matrix shows a high percentage of correct fsk intra-class recognition when snr is not lower than 2 db. the confusion matrix shows a high percentage of correct psk intra-class recognition when snr is not lower than 4 db."
"the network size includes the number of hidden layers and the number of nodes in each hidden layer. the network size is an important parameter that affects the generalization capability of ann. of course, the network size depends on the complexity of the underlying scenario where it is directly related to network training speed and recognition precision. in this paper the network size has been chosen through intensive simulations."
"in this paper we focus on the continuous wavelet transform (cwt) to extract the classification features. one of the reasons for this choice is due to the capability of the transform to precisely introduce the properties of the signal in time and frequency [cit] . the extracted features are higher order statistical moments (hom) of the continuous wavelet transform. our proposed classifier is a multi-layer feed-forward neural network trained using the resilient backpropagation learning algorithm (rprop). principal component analysis-(pca-) based features selection is used to select the best subset from the combined hom features subsets. this classifier has the capability of recognizing the m-ary amplitude shift keying (m-ask), m-ary frequency shift keying (m-fsk), minimum shift keying (msk), mary phase shift keying (m-psk), and m-ary quadratic amplitude modulation (m-qam) signals and the order of the identified modulation. the performance of the proposed algorithm is examined based on the confusion matrix and false recognition probability (frp). the awgn channel is considered when developing the mathematical model and through most of the results. some additional simulations are carried to examine the performance of our algorithm over several fading channel models to assess the performance of our algorithm in a more realistic channel."
"the comparison among different modulation recognition algorithms is not straightforward. this is mainly because of the fact that there are no available standard digital modulation databases. hence, different works have applied their algorithms to cases of their own choosing [cit] ."
"this approach seems to be unproblematic when it comes to some mathematical problems. human competence in the addition of integers, for example, seems to use an optimal algorithm, and there is also evidence that this is the case in certain logical tasks [cit], section 5) . however, in what follows, i will argue that there is no reason to believe that this is generally the case. in fact, i will argue that there are several ways in which the human problem solving cognitive capacity differs essentially from the optimal algorithms for solving mathematical problems. 14 there have been some arguments (e.g., [cit] ) to the effect that human mathematical ability could actually rise above the power of any algorithms, by a special intuition or mathematical insight. i consider those arguments to be dubious and will not focus on them here. i believe that mathematical intuition and insight are important subjects and they no doubt play an important role in mathematical problem solving. however, based on the definition of optimal algorithms, i cannot see how they could lead to super-optimal solutions to mathematical problems. if they did, we would presumably be able to model this \"insightful\" solution, which would then turn out to be an optimal solution."
blind signal interception applications have a great importance in the domain of wireless communications. developing more effective automatic digital modulation recognition (admr) algorithms is an essential step in the interception process. these algorithms yield to an automatic classifier of the different waveforms and modulation schemes used in telecommunication systems (2g/3g and 4g).
"according to the above observations, we propose a feature extraction procedure as follows. the cwt can extract features from a digitally modulated signal. these features can be collected by examining the statistical properties of wavelet transforms of both the signal and its normalized one. since median filtering affects the statistical properties, these properties will be calculated with and without applying filtering. based on our simulations, we noted that moments of order higher than five will not improve the overall performance of our algorithm. therefore, in what follows, we consider moments of order up to five to calculate the hom of wavelet transforms. figure 6 shows the processing chain of features extraction. as shown, the digitalized received signal is first normalized then the cwt of the received signal and the normalized one are obtained where the first subset of features will be the hom (up to 5). a median filter is then applied to cut off the peaks in the corresponding wavelet transforms. finally the hom of these two filtered transforms will form the other features subset. this large number of features may contain redundant information about the signal. however, these features will surely have the necessary information to distinguish between different modulations. in order to select a smaller number of features a subset selection algorithm is proposed."
"the proposed algorithm was verified and validated for various orders of digital modulation types including ask, psk, msk, fsk, and qam. table 1 shows the parameters used for simulations. testing signals of 100 symbols are used as input messages for different values of snr and channel effects (awgn channel is used unless otherwise mentioned)."
"most of the existing works in the literature had examined their methods over awgn channel. here, we also developed our mathematical model and tested our algorithm over this channel. it is clear that it be will more realistic to examine the proposed algorithm performance over fading channels."
"therefore, in the marrian three-level distinction between explanations in cognitive science, at least some cognitive tasks are not best characterized purely in terms of the computational-level approach. moreover, the entire distinction between the computational and algorithmic levels becomes problematic: without including the algorithmic level we cannot identify the phenomena we aim to model on 22 the distance effect itself is a general characteristic of the approximate number system and it has been detected in various nonhuman animals, including rats, pigeons, dolphins and apes [cit] 2011, p. 16 ). however, most nonhuman animals cannot grasp symbolic representations of quantities, so it is difficult to see the distance effect as causing suboptimal problem solving strategies. recently there have been experiments on monkeys that suggest an ability to make addition of symbolically represented magnitudes [cit] ). in such case, it would not be unfeasible to think that the distance effect can cause the monkey to use a suboptimal problem solving algorithm. the computational level. in principle, there is of course nothing to prevent determining the complexity of the algorithms that human beings use in their cognitive tasks purely computationally. however, with too much focus on the computational approach, how can we find out what the human algorithms are? if the algorithms that most accurately model the problem solving strategies of human cognizers are not necessarily computationally optimal ones, we should already include considerations on algorithms-and possibly also their implementations-in the initial research question."
"the resource and admission control subsystem (racs) has become generally recognized as the subsection of the ngn responsible for the policy control, resource reservation and admission control. standardization efforts of etsi tispan ngn and itu-t have addressed the issue of policy-based admittance of the end user to the resources based on a rather complex service-aware procedure of negotiation. the proposals vary in the defined entities and logical organization but are conceptually similar and extend horizontally cross access and core domain and vertically across service and transport layers. as depicted in figure 3, the generic racs comprises:"
"the functionalities are provided through several logical entities. among these, the functionality responsible for session description and transport layer profile maintenance is actively involved in the quality assurance procedure (referred to as the nass database-nass db). it communicates with the racs subsystem to relay the relevant transport-layer access, session and subscription information, involved in the quality assurance procedures. an example of the information model of the nass is represented in figure 4 ."
"15 under the computational complexity approach-when possible-the complexity of mathematical problems is characterized by optimal algorithms. such algorithms do not need to be unique, but by definition, every other algorithm for solving the problem takes equally long or longer to run than an optimal algorithm (or performs at best a constant factor better). 16 with human insight, we may be able to provide feasible hypotheses, partial 16 or it requires as much as or more space, or it has equal or higher kolmogorov complexity, etc."
"in the first part of this paper, i will evaluate the computational complexity approach to the cognitive complexity of mathematical problem solving. in the next two sections, we will see how useful it can be in limiting the space of possible functions that can work as characterizations of human cognitive capacities. in addition, much of the information we can gather from the research on computational complexity of mathematical problems can be used to assess the complexity of the cognitive tasks involved in solving them. overall, there is nothing necessarily wrong with the general guideline that the more complex a problem is computationally, the more complex it is for humans to solve it."
"14 an upshot of this approach is that we can also better understand why human competence sometimes can be modeled by an optimal algorithm and sometimes not. 15 of course this argument fails if we believe that human insight cannot be even in principle computationally modeled, as penrose does."
"ann is an emulation of biological neural system. ann is configured through a learning process for a specific application, such as pattern recognition. anns with their remarkable ability to derive meaning from complicated or imprecise data can be used to extract patterns that are too complex to be noticed by other computer techniques."
"at this point, one might ask whether this cultural and individual variation is not about performance while we are interested in competence? but based on all the considerations we have been through in this paper, it is impossible to retain this distinction in the sense that there is one human competence in mathematical problem solving. mathematical problem solving is an ability with great cultural and individual variation and a single notion of mathematical competence seems thus misplaced. the distinction between performance and competence is important to make, but instead of having a uniform notion of an optimal algorithm (and one corresponding notion of competence) we should look for general patterns in performance in order to establish a wider, more fine-grained conception of enculturated competence. with fig. 6 the \"japanese\" visual multiplication method the focus on enculturated competence, we can have a theoretical framework that is more sensitive to particular culturally determined mathematical practices, such as constructing and interpreting diagrams."
"by extending the work of hong and ho [cit], from (1)- (3), (6), and (7), the magnitude of continuous wavelet transform is given by"
"we presented a wavelet-based algorithm for automatic modulation recognition. the proposed algorithm is capable of recognizing different modulation schemes with high accuracy at low snrs. our classifier has high full-class modulation recognition performance when the snr is not lower than 4 db. we found that the percentage of correct inter-class recognition for msk, fsk, ask, psk, and qam is high when the snr is not lower than 3 db. also, the percentage of the correct intra-class recognition of modulation order was found to be high for low snrs, and the minimum value of snr for which the high percentage of intra-class recognition is still reachable depends on the modulation type, and could reach a very low value (−6 db for intra-class qam recognition). in addition, we have shown that our algorithm offers excellent performance over both awgn and several fading channel models."
"in this paper, we proposed interference-aware transmit power control (ia-tpc) and power headroom report-based ia-tpc algorithms for lte-a heterogeneous networks with the aim of achieving zero-outage for the macrocells users. with the ia-tpc algorithm, we showed that the proposed algorithm not only ensures zero-outage for the macrocells users but also increases the macrocells users' average throughput, even above the received interference threshold level, at the cost of an increased outage ratio for femtocells users. further, with the phr-based ia-tpc algorithm, we showed that, along with the zero-outage for macrocells users, the outage ratio for femtocells users can also be reduced by properly updating the transmit power by taking the current channel condition into consideration. the performances of the proposed algorithms were evaluated using simulations in the matlab environment. the results show that the proposed algorithms perform significantly better than the conventional transmit power update method. in their future consideration, the authors' focus is on flexible physical resource blocks (prbs) allocation based on the user's power capability and channel condition. the significance of flexible prbs allocation is twofold: (i) efficient resource allocation and (ii) decreasing harmful interference, which will eventually result in throughput enhancements and a decrease in outage ratio."
"the effective interference for user is the ratio of total interference caused to user and the path gain to its serving (f)enb, denoted by, and it can be expressed numerically as"
"to extend the applicability of the markov chain model to the atmospheric ocdma communications, we should consider that the basic assumption for modeling the acquisition procedure as a discrete markov chain is that the decision variables in different code shifts must be statistically independent [cit] . in wireless optical links, temporal correlation of fading process introduces a memory to the system, leading to statistical dependence of fading samples inside a temporal span identified by the coherence time of fading process (t f ). since the decision variables are related to the fading level, depending on the value of t f compared to dwell time (t s ), the decision variables in consecutive code shifts may become correlated. this fact makes it impossible to model the acquisition procedure as a markov chain in a fading channel with arbitrary coherence time. however, we can consider two extreme cases, for which, we can still use this analytical tool:"
"where 2 is additive white gaussian noise power received at the receiver of user . in addition, is the total interference caused to user at its receiver and is given by"
"the proceeding sections of this paper are organized as follows. in section ii, system model is described and channel model and synchronization procedure are discussed. in section iii, the applicability of markov chain model to the atmospheric ocdma communications is discussed. in section iv, probabilities of false alarm and detection of correlator and chip level detector are computed. the numerical results are presented in section v and in section vi, we conclude the paper."
"where l h is the number of interfering users in hth marked chip. since the photon counts of different chips are independent and fading level is considered to be constant in a dwell time as discussed in section iii, the mean and variance of decision variable in wrong code shifts will be"
"for the previously discussed model, the cdfs of the average throughput of mues and fues are simulated for the three algorithms, ia-tpc, phr, and fpc, and the results are plotted in figures 2 and 3 . from the throughput cdf curves, the following information can be deduced:"
"in the next step, we should average the probability functions with respect to the number of interfered chips. by averaging with respect to r, lower bound of p fa is expressed as"
"if the instantaneous effective interference is equal to or higher than the maximum tolerable effective interference, then case 1 infers that there may exists one or more mues which are in outage if the power is updated with the fpc algorithm. for the given scenario, we have to show that the power vector p(̂) updated with the proposed algorithm is a valid power vector p(̂) and that the following equalities and inequalities hold for case 1:"
"the structures of active and passive correlators are depicted in fig. 4(a)-(b) . the performance of passive correlator which is a mf is the same as active correlator meaning that all the expressions derived here for p fa and p d can be used for both structures; however, mf benefits from rapidity and can compute the correlation in a chip time duration compared to a bit time in the active correlator. we prefer to explain the analysis of the performance of correlator considering the active structure. in the active correlator structure, the received signal from photodetector is multiplied by the shift of ooc under test; then the output of multiplier is integrated over a bit time duration and the result is compared with a predefined threshold. we note that multiplying the received signal with different code shifts in the active correlator is equivalent to changing the sampling chip in mf."
"interference at enb. the total received interference at the serving enb can be calculated by considering the interference from fues located in the serving macrocell region and mues from the neighboring cell. thus, the total interference received at the serving enb in a subframe is formally defined and obtained as"
"in case i, we can directly use the markov chain model. first we average the probability functions of p d and p fa with respect to fading; next, we use them in gain of branches in (5), (7), (8) and finally we should do the algebra in (10) to obtain the mean of acquisition time, e(t acq ). for case ii, it is not possible to use the markov chain model directly because different decision variables are completely correlated."
"unlike the existing literature, in this paper, we propose ia-tpc and phr-based ia-tpc with reduced signaling overhead and computational complexity for hetnet environments. the objective of this work is to support the maximal number of fues subject to mue protection. to the best of our knowledge, this is the first work that guarantees the protection of mues while combating the harmful interference in hetnet environment using power control technique."
"moreover, the proposed ia-tpc method uses a dynamic approach for setting the interference threshold. when there are many mues with high target sinr requirements and/or with poor channel conditions, the total tolerable received interference threshold is set to a very small value. alternately, when there is a small number of mues with low sinr requirements and/or the channel condition is good, the total tolerable received interference threshold can be set to a relatively large value."
"the rest of this paper is organized as follows. the next section provides a literature overview of the related work. section 3 presents the system model and states the problem. in section 4, based on the received interference at the enb, we introduce the proposed ia-tpc and phr-based ia-tpc algorithms. section 5 discusses the signaling overhead, valid power existence, and outage ratio for the proposed algorithms. in section 6, the performance evaluation is discussed in detail and section 7 concludes the paper."
"we consider the case of data transmission from user i in sub-network no. 1 to user j in sub-network no. 2 through the wireless link. the transmitted signal is impaired by the channel. we denote the transmitted signal by s j (t) and the impaired received signal by s j (t). the subscript j indicates that the transmitted signal was multiplied by ooc code of user j showing the destination of the signal. the received multiple access signal in front end optics of the wireless receiver of sub-network no. 2, denoted by r(t) in fig. 2(a), will be"
"to minimize the outage ratio of fues subject to the zero-outage ratio of mues, the uplink power of all users must be dynamically controlled in the following conditions."
"in this paper, we address the problem of intertier interference in hetnets through an interference-aware transmit power control (ia-tpc) method. having obtained the interference limit of each user, we intend to develop an adaptive power control algorithm that should consider the total effective received interference at the enb while allocating transmit power to all users with the aim of having all macro users reach their target sinr with a minimum outage ratio for femtocells users (fues). the existing interference management schemes do not assure the protection of mues from the harmful interference caused to mues by fues, which results in the outage of some mues. still, these schemes can be used by fues provided that the interference caused by them to mues does not exceed a given threshold. in particular, if fues limit their transmit power level so that the total interference caused to mues does not exceed a given threshold, each mue is able to reach its target sinr and fues can minimize their outage ratio. the major contributions of this work can be summarized as follows."
"in this section, we evaluate the performances of the proposed algorithms based on system-level simulations. we used the hetnet deployment as given in the 3gpp lte specifications in our simulations. the simulations are carried out using matlab. the in-depth details of the simulation model, evaluation criterion, and simulation results are discussed in the following subsections."
"in testing a h 0 cell, if the number of chips detected as one, exceeds th, false alar2[]\\m occurs. these chips can be among the interfered or non-interfered chips. if we assume that l users have interfered in r chips out of w chips, lower bound for p fa is obtained as follows"
the proposed transmit power control algorithm takes the total received interference at the serving enb that can be tolerated by all associated mues while allocating power to the user terminal. the power update equation of the proposed power control method is given by
"(iv) finally, we have compared the performance of the proposed algorithms with the conventional fractional power control (fpc) algorithm [cit] and the outage performance shows the significance of the proposed algorithms."
"the phr can either be positive or negative. a positive phr indicates that the actual transmit power was lower than the maximum terminal transmit power. however, a negative phr indicates that the transmit power was limited by the maximum power constraint, that is, . to calculate the transmit power for the next subframe based on the phr, we define two terms, the instantaneous phr difference and average phr difference denoted by and, respectively, which can be obtained as"
"in this paper, we discussed how to extend the applicability of markov chain model to the acquisition algorithm analysis in the atmospheric ocdma communications. it enabled us to analytically evaluate the performance of the code acquisition system in optical fading channel. we validated our approach by comparing the analytical results with those obtained from simulation. we analyzed the performance of correlator and chip level detector in acquisition system. by discussing the effects of different parameters such as fading and background light on the acquisition procedure, we showed that cld outperforms correlator. furthermore, we evaluated the performance of various two stage acquisition receivers employing different combinations of active correlator, mf and cld in search and verification stages and showed that mf/cld structure outperforms all the other combinations of these three detectors. numerical results show significant reduction in the acquisition time and the required transmitted power for synchronization in the atmospheric ocdma communications by using our proposed mf/cld structure instead of formerly used active correlator structure."
"for any set of values of system parameters, there is an optimum threshold where e(n acq ) is minimum. for example, in variance of fading of 0.5, based on curves corresponding to simulations and \"cor fading\", the optimum nth should be set around 0.05. if we wrongly use the expressions for \"ind fading\" to analyze the performance, then optimum nth will be around 0.09. we observe if the correlated optical fading statistics is not considered in system analysis, it results in wrong threshold setting using the corresponding analysis. by comparing the simulation curve for the two nth of 0.05 and 0.09, we observe a large increase in the acquisition time by using the wrong optimum nth. this fact emphasizes the importance of considering the effect of correlated fading statistics in threshold setting in acquisition system analysis. we also emphasize that threshold setting should be sufficiently accurate for ooc-ocdma systems otherwise it results into a penalty in acquisition time."
"a more reasonable idea compared to the single dwell acquisition is to use multiple dwell or two stage acquisition consisting of search and verification stages. we use two stage acquisition structures in our receiver, as depicted in fig. 2(b) . in a two stage acquisition scheme, after a decision variable exceeds the threshold in a code shift in search stage, system goes to verification stage where using a verification strategy, it tests the accepted shift again. the aim of using a verification stage is to ensure that the correct shift was selected in the search stage. despite the radio cmda systems using pseudo noise (pn) codes where partial correlation with different lengths of pn code is being used in different dwells of verification [cit], we have to use full code length correlation in verification stage due to the properties of oocs [cit] . we use majority verification with full length correlation over a bit time. the use of algorithms such as multiple bit observation which can give diversity gain will not lead to a better performance because our system is line of sight and there is only one path between the laser of transmitter and aperture of receiver in our system. in majority verification, the selected shift is tested for a times. if in at least b tests out of a tests, the decision variable exceeds the threshold, the acquisition is confirmed. we can define probabilities for false alarm (p fav ) and detection (p dv ) in verification stage as well, which mean to confirm a wrong shift or the correct shift, respectively. the detectors used in the two stages can be different. we evaluate the performance of all combinations of the three different detectors of active correlator, mf and cld in search and verification stages of our acquisition receiver to find the one leading to minimum acquisition time in wireless ocdma communications. we will describe their structures and compute their p fa and p d in atmospheric channel in section iv."
"(i) first, the mue throughput of the phr algorithm significantly outperforms the ia-tpc and fpc algorithms as can be seen from figure 2 . this is mainly because the phr algorithm updates the transmit power in accordance with instantaneous channel conditions, thereby reducing the interference results and increasing the average throughput. additionally, the ia-tpc algorithm also outperforms the conventional fpc algorithm for mues. although for the mue case the ia-tpc is reduced to fpc due to less interference from fues, the average throughput of the ia-tpc algorithm is more than that of the conventional algorithm."
"in this section, we present numerical results for the effect of various impairments on acquisition procedure and discuss on the performance of different acquisition schemes. system parameters and their corresponding values are listed in table i . we introduce two new parameters named as the mean number of acquisition bits, e(n acq ), and normalized threshold, (nth). e(n acq ) is defined as the mean acquisition time divided by bit time. nth is defined for the correlator structure as"
"in our simulations, we evaluate the performances of the proposed algorithms summarized in (15) and (17), termed ia-tpc and phr, in comparison to the conventional fpc algorithm specified in (7) . in the simulations, we used a cumulative distribution function (cdf) of the mue and fue average throughput and a cdf of the received interference at the enb in order to compare the three algorithms. further, we compare the average throughput of mues and fues relative to the increasing interference situation. in addition, the transmit power allocated to mues and fues is compared based on the proposed and conventional power update algorithms. finally, the outage ratio, defined in (5) and (6), is used to evaluate the outage performances of the proposed algorithms."
"where p fas and t s are the probability of false alarm and dwell time in search stage, respectively. the dwell time of system must be one bit duration (t b ) for active correlator and cld or one chip duration (t c ) for passive correlator in order to obtain all the pulses of ooc. t v is the time required for the verification. since majority verification is used, the verification time equals to at s . p fav is the probability of confirming a wrong code shift in the verification stage which can be computed as"
"in the above expression, ( ) is the uplink power for user in a given subframe and pl dl is the downlink path loss as estimated by the user. the term denotes the maximum transmit power, indicates the instantaneous bandwidth measured in terms of physical resource blocks (prbs), and ∝ is the partial path loss compensation factor that takes a value less than one. the parameter ( ) is the desired target received power that takes the target sinr and instantaneous interference level into account and thus varies with time:"
"o ver the past two decades, various optical code division multiple access (ocdma) techniques have received increasing attention for their potential applications in high speed and high capacity optical access networks. ocdma via optical orthogonal codes (oocs) was first introduced for fiber-optic networks using intensity modulation and direct detection (im/dd) technique [cit] . later, it was employed in im/dd atmospheric or free space optical (fso) communications [cit] . atmospheric optical systems can provide high bit rate and cost effective links which also benefit from the security in data transmission and the lack of licensing requirements [cit] . different applications are being investigated for fso communications [cit] . a novel idea is a hybrid fiber-optic/fso ocdma network which can deliver different multimedia and data services to the users, efficiently and at a low cost. fig. 1 illustrates a hybrid network with fiberoptic cdma sub-networks in different locations connected to each other via atmospheric ocdma links. the free space transmitters and receivers can be mounted on the top of the buildings. by using wireless optical transceivers, different fiber-optic networks are grouped together to form a larger ocdma network. one of the essential requirements to succeed in developing high speed hybrid ocdma networks is to introduce fast and efficient synchronization systems. the code synchronization system generally consists of an acquisition subsystem that synchronizes the desired receiving code and its locally generated replica in the receiver with a specified accuracy. then using a tracking circuitry the two codes are precisely synchronized."
"long term evolution (lte), developed by the third generation partnership project (3gpp), is an orthogonal frequencydivision multiplexing access-(ofdma-) based mobile broadband wireless communication technology. in release 10, 3gpp introduced the advanced features of lte, referred to as lte-advanced (lte-a) [cit] . considering the need for increased data traffic, lte-a is targeting low latency and high peak data rates at reduced costs. to achieve these targets, 3gpp has been focusing on various aspects of lte-a which include utilization of higher frequency bands, enhanced mimo, cooperative multipoint communication, carrier aggregation, and deployment of heterogeneous networks (hetnets) [cit] . beside others, the concept of hetnets has attracted considerable attention due to its high potential to enhance system capacity and coverage [cit] . thus, hetnets, utilizing a diverse set of base stations, are considered good contenders for providing improvement in the spectral efficiency per unit area."
"n and l are the wave number, wavelength of optical transmitter, refractive index structure constant and the length of the link, respectively. the minimum is taken to simply approximate the saturation of log-normal model [cit] ."
"p ds and p dv are the probability of detecting the correct shift in search mode and confirming the detection of correct shift in the verification mode, respectively; p dv can be computed as"
"the simulation model is developed according to the standard lte network with an ultra-dense hetnet environment as shown in figure 1 . the model consists of 7 enbs where csg-based femtocells are randomly deployed that reuse the same carrier frequency to create a hetnet environment. the service area of each enb and fenb is considered to have a 500 m intersite distance and 20 m radius, respectively. each mue/fue is equipped with a single antenna while the user speed is assumed to be 3 km/h. we have assumed full buffer traffic in our simulations. the channel is considered to follow the claussen model [cit], which incorporates path loss and shadowing in urban environments. moreover, in the proposed algorithms we have set a 100 ms slot length (i.e., after every 10 frames) for the power update function. the transmission time interval (tti) for each subframe is 1 ms as given in the lte specifications. however, this time duration is too small for the purpose of power updates because the channel conditions and interference situation changes much slower as compared to the tti. other simulation parameters are set in accordance with the lte standard and are summarized in table 1 ."
"in this section, first we discuss the total interference received at the enb that will be used in our power control algorithms. later in this section, we propose two interference-aware uplink power control algorithms for heterogeneous networks, keeping the constraint where the outage ratio for mues is zero with the minimum outage ratio for fues. in our proposed algorithms we impose a new limit on the transmit power of fues based on the total received interference at the enb and the terminal's phr in order to keep the interference caused by fues below a given threshold."
"to acquire the code, the uncertainty region of τ j which is within [0, ft c ) will be divided into f cells of length t c, each corresponding to a chip shifted version of ooc. for the serial search algorithm, system serially searches different shifts to find the cell (shift) within which the actual value of τ j is located. in testing each shift, a decision variable is obtained through a correlation process. then, it is compared with a threshold. the time required to test each shift is called \"dwell time\". acquisition system can be either single dwell or multiple dwell. in a single dwell acquisition which consists of only a search stage, if the decision variable exceeds the threshold in a shift, that shift is introduced as the correct shift; otherwise it is rejected and the next shift is tested. the decision variable may exceed the threshold in any of the wrong code shifts (named as h 0 cells) or the correct code shift (named as h 1 cell) with the probabilities of false alarm (p fas ) and detection (p ds ), respectively. if a wrong code shift was selected, it takes a considerable time of t p for the synchronization system to return to the search algorithm and continue the search. t p is called penalty time."
"to evaluate and compare the performance of different acquisition receivers, we need to compute the time required to acquire the code, denoted by t acq . since t acq is a random variable, its statistical characteristics must be considered in the performance evaluation. in the next section, we use markov chain model and obtain a closed form expression for mean of t acq of multiple dwell serial search algorithm in atmospheric ocdma communications."
"the comparison between the simulation and analytical results is done at bitrate of 100 mbps. although wireless optical channels are designed for higher bitrates [cit], this bit rate is chosen to study the validity of our analytical expressions for a wide range of bitrates selected by network designers. the expressions given in section iii are based on the comparison between e(t acq ) min and coherence time where we argued that coherence time is much larger than e(t acq ) min . based on that, we assumed that fading level is constant over the whole acquisition procedure. lower bitrate results in to a larger e(t acq ) min . coherence time can be on the order of several milliseconds where we have set it to 2ms. a low bit rate and a small coherence time in our simulation will let us examine how accurate our assumption of \"constant fading level over acquisition time\" is. it is obvious that if our analytical expressions predict the performance correctly for 100 mbps, they can surely predict the performance for higher bitrates. we observe excellent agreement between the results obtained from the analytical expression associated with the completely correlated fading statistics and the simulation results which validates the assumption we made in section iii in our analytical approach. by comparing the minimum of e(n acq ) for the two cases of correlated and uncorrelated optical fading statistics during the acquisition procedure, we observe that correlated fading statistics results into increase of acquisition time. this increase of acquisition time becomes more significant in larger variances of fading."
"where p fav,1 is the probability of false alarm in one single test in the verification stage. t v is the time required for the verification. since majority verification is used, the verification time equals to at s . two other gain of branches in the diagram of fig. 3 (a) are h d (z) and h m (z), corresponding to detection and miss-detection of the correct shift, respectively. they are expressed as"
"(iii) secondly, for the cell edge users (i.e., below 5% user throughput cdf in the graph), the ia-tpc and phr give almost the same performance, while fpc gives a better performance, specifically in the fue case. the reason for this is that along the cell edge, the instantaneous interference becomes larger than the threshold interference resulting in outage for fues. hence, the throughput of fues is decreased whereas that of the mues is protected. figure 4 shows the cdf of the average received interference at the enb. for average users (i.e., at 50% of the cdf), the curves show that the received interference for the phr and ia-tpc algorithms is almost 17% and 8% less than the conventional fpc algorithm. this is because the proposed algorithms consider the received interference at the enb while allocating transmit power to the terminals. the phr algorithm outperforms the fpc and ia-tpc algorithm due to the aforementioned reason. next, we show the average throughput of an mue/fue against the varying interference. in our simulations, we set the threshold interference equal to 5 db. in figure 5, the average throughput of an mue is plotted against the interference. at the threshold value (i.e., 5 db), the mue average throughput is rapidly decreased in the case of the fpc algorithm, while, for the proposed algorithms, the rate of decrease in average throughput for the mue is slow. in order to understand this scenario, recall that when the interference level is increased above the threshold, the proposed algorithms drastically decrease the transmit power of fues (fue may go into outage), thereby maintaining the mue throughput. on the other hand, as shown in figure 6, when the interference increases above the threshold level, the average throughput of fues decreases rapidly in the case of the proposed algorithms as compared to the fpc algorithm. the reason for this is that the transmit power of the fues is significantly decreased by the proposed algorithms (fues may go into outage) in order to protect the mues. further, figures 7 and 8 show the allocated transmit power to the mue and fue at the varying interference levels. in the case of mues, as given in figure 7, the transmit power is increased gradually with increased interference. the ia-tpc and fpc algorithms assign the same power to the user terminal while the phr allots less power since it considers instantaneous changes in the channel condition while allocating power. instead, the fue transmit power is decreased drastically by the proposed algorithms when the interference level goes beyond the threshold level, as shown by the red and green curves in figure 8 . however, the fpc algorithm allocates more transmit power to mitigate the interference. hence, it can be anticipated that mue protection is guaranteed at the cost of fue outage. finally, in figure 9 we describe the average outage ratio of an mue and fue versus the target sinr for the proposed and conventional algorithms. note that, in the case of mues, both of the proposed algorithms outperform the conventional fpc by ensuring zero-outage for the mue at the cost of increased outage for fues. as can be seen from the green and red star curves, the mue outage is zero for the ia-tpc and phr algorithms while the blue curve shows some outage ratio for the mue. however, in the case of fues, the outage ratio for fpc is less than that of the phr and ia-tpc. moreover, the outage ratio of the fue using the phr is less than the outage ratio of the fue using ia-tpc since in the phr, the fues cause more interference (due to channel gain) to mues and accordingly reduce their transmit power more, thereby resulting in a lower outage ratio."
"where f (x) is the pdf of fading level. to evaluate the mean of acquisition time, p fa and p d conditioned on fading level must be computed. in the next section, we compute them for the correlator and cld structures."
"(ii) secondly, we prove that there always exists a valid power vector for the proposed algorithms; that is, in every interference scenario, mues achieve their target sinr."
"(iii) third, we have shown that with the proposed algorithms the average throughput of mues can be significantly improved while a slight improvement in the throughput of fues can be obtained by properly adjusting the transmit power using proposed algorithms."
"(ii) however, for fue throughput, as shown in figure 3, the conventional fpc performs slightly better than the proposed algorithms. to understand this scenario, recall that, in the proposed algorithm, when the received interference is increased beyond the threshold, some fues may be gradually removed from the network, resulting in decreased throughput."
"(i) we define the problem of transmit power control in the hetnet environment in order to maximize the throughput by attaining a zero-outage constraint for mues with as minimum an amount of outage as possible for fues. to achieve this goal, we present two algorithms, ia-tpc and power headroom report (phr) based ia-tpc. in the proposed algorithms, the transmit power of all users is adjusted by properly following the effective target sinr. if the effective received interference at an enb is above a given threshold, the fues adaptively decrease their transmit power to protect mues. further, in the phrbased ia-tpc algorithm, the transmit power is additionally adjusted by the phr factor that considers the current channel condition, which results in reducing the outage ratio for fues."
"lower and upper bound of p d can be computed in a similar fashion to p f a, using (28), (29) by substituting p 0 and p 1 with p 2 and p 3, respectively."
"path choosing strategy considers channel condition and the remaining energy of each node. channel condition is constant once the sensors are implemented. it is determined by the distance between two nodes and whether it is inbody channel or external channel. remaining energy alters constantly and decides flexibility of the algorithm. assign weight to these two parameters and the weight determines the adaptability of the network. the more the energy of the parameter weight is, the more the nodes are able to react to the change of the environment. it could balance the energy consumption within the relay nodes, rebuilding the routes, however, consuming extra energy as well. so it is critical to select the proper weight. in what follows in the paper, weight for channel condition is always considered as 1, and weight for energy consumption is referred to as matrix ."
"in this paper, we study optimal stationary sampling for transmission of measurements of a stochastic process from a source encoder to a source decoder through a costly communication channel. we measure information transferred over a time interval by the change in the decoder's entropy regarding the state of the process given the transmitted measurements. in our setting, the encoder employs a sampler to control the information flow in the channel. this study has a broad range of applications including surveillance and reconnaissance, planetary explorations, wireless wearables, teleoperation, and many other examples of cyber-physical systems."
"atypical wban architecture usually includes a small network around the body (about 1-2 meters) and a gateway (sink) bridging to other network types that can be another node with some routing and data aggregate features. beyond the sensors around the body, a whole network is the mechanism for communications by intra-wban and inter-wban that can be an internet or intranet network, and wban also contains applications with gui for medical or other healthcare personnel. in the real time application of wban, time-effectiveness is important because services for users must be provided in a timely manner for such mission-critical applications as life-saving. energy efficiency is essential because a sensor node's energy resources are limited and easily exhausted which is the existing limitation of wban. for these reasons, energy efficiency and sink location as the core of the entire network probably are two key elements to enhance the performance of the wban."
dra make the lifetime of the nodes relevant but not in a desired way. introducing the weight prolongs the lifetime of the whole network significantly with some sacrifice of the longest lifetime of some nodes which are of less reference value when most nodes are dead in a wban where sensors collect different kinds of data and work integrated. it is feasible to make the compromise.
"dra enable collaboration among the nodes and sacrifice the longest lifetime nodes to compensate the nodes which die sooner. figure 3(a) shows the remaining energy of each node within wban when applying dra. the curves are related to each other to a certain extent. but death points range from 5000 seconds to more than 35000 seconds, which means that energy consumption is not ideally balanced. nodes near the central node die sooner than others because of the forwarding tasks. these nodes' death in turn makes it more expensive to transmit data from further nodes to central node. so it is crucial to prolong the lifetime of the network to reserve these nodes properly."
"the first method that we use to compute the optimal stationary policy is approximate value iteration (described in algorithm 1). in approximate value iteration, we generate a sequence of functions j k (i) which are approximations of the functions t k j(i). in each iteration, the improved function j k+1 (i) is obtained based on the function j k (i). the algorithm stops when the maximum difference between two successive functions is less than a specific tolerance."
"definition 3 (information from decoder's perspective): information is the change in the entropy of the state of the process given the decoder's information set i k (δ k ), i.e.,"
"to simulate the node working on or around or inside human body, 14 nodes (central node as node 1) are set on the same surface as shown in figure 2 . and the corresponding coordinates are shown in table 2 ."
"nonuniform sampling [cit] and its important sub-class event-driven (event-triggered) sampling [cit] for estimation have received early attention in the literature. kushner [cit] studies the optimal measurement control problem for linear systems subject to a constraint on the number of measurements. [cit] look at the optimal measurement control problem subject to measurement cost, and propose dynamic programming (dp) and the gradient method as computational procedures.åström and bernhardsson [cit] show that event-driven sampling can outperform periodic sampling with respect to the estimation error of a scalar linear system under a sampling rate constraint. [cit] study optimal event-driven sampling as a stopping time problem for a scalar system under a finite transmission budget constraint. molin and hirche [cit] investigate the optimal design for event-driven sampling in a scalar system with communication cost by considering a two-player problem. furthermore, [cit] study optimal closed-loop sampling in which the effect of the cost-to-go is taken into account."
in this section some simulation result is presented to testify the performance of the given routing algorithm. some simulation parameters are given in table 1 .
"the outline of this paper is as follows. problem formulation is presented in section ii. in section iii, we derive the optimal stationary sampling policy, propose two computational methods for calculating this policy, and introduce two triggering mechanisms. we illustrate numerical and simulation results in section iv. finally, concluding remarks are made in section v."
"definition 6 (value of information): the value of information (voi) is the maximum value that the observer would be willing to pay for the sampling and the transmission of a measurement y k at time k, i.e.,"
"is a gaussian distribution given the system's model introduced before. the conditional distribution evolves in time due to the system dynamics, and is updated at times k s due to measurements."
"sensors nodes could be a wearable device or an implanted device, so it is not possible to recharge very often. the central node, however, is usually a hand device as mentioned before. compared to sensor nodes, it has infinite energy and carries on the main signal processing."
"to address the solution to enhance the energy efficiency of wban made up of the vital signs sensors and the sink node, in this paper, we propose a routing algorithm to properly arrange the forward route in the network to balance energy consumption of each sensor with the consideration about the network topology structure. different from the classical routing algorithm working with the traditional sensor network to archive the minimum hops, the reported algorithm here is constructed according to the classical dijkstra algorithm to determine the route matching the restrictions of minimization of the maximum energy where the optimization is processed to archive the maximum lifetime of the whole network."
"four major parameters of sensitivity (sn), specificity (sp), accuracy (acc) and mathews correlation coefficient (mcc) were selected to access the prediction performance. the accuracy can express the percent of global prediction between the positive and negative datasets. the sensitivity denotes the percentage of positive datasets which were predicted correctly. similarly, the specificity denotes the percentage of negative datasets. the mcc, a comprehensive evaluation parameter, accounts for the true and false positives and negatives. in addition, it is often identified as a balanced measure when the classes possess pretty different sizes. fig. s1 comparison of average pwaa scores in glycation and non-glycation samples."
"in this section, we present numerical and simulation results for a simple unstable system. example 1: consider a system with the following linear scalar dynamics"
"where k − s denotes time k s before the estimate and the fim are updated. following bayes' rule [cit], the estimate and the fim at time k s are updated aŝ"
"we showed that by using algorithm 1 or algorithm 2, we can find the value function and the optimal sampling policy. here, we will link the optimal policy to the value of information in our system, and we will propose two triggering mechanisms that can generate the optimal policy."
"a cellphone can also provide interface for the wearer to check the health indicator when needed. sensors nodes collect anthropometric indicators such as egg, cgg, pulse, temperature, and blood sugar and other information such as motion and gps and transmit the data to the sink for primary processing of way to professionals. a typical wban is shown in figure 1 ."
"(1) each node is still and keeps transmitting data to central node. (2) central node has infinite energy while sensor nodes have the same finite initial energy. (3) after a round of transmitting the package, sensor nodes are aware of their own remaining energy. (4) all the sensor nodes have equal computing and signal processing capabilities."
"wban is a solution to personalization and ubiquitous calculation in area of medical system and entertainment, which is under the picture of rapid development of internet of things. achievements in intergraded circuits and sensor technology have laid foundation for wban. ieee 802 established a task group 6 for standardization of low-power, small-scale wearable medical devices network. a wban usually consists of a central node (could be a cellphone or pda) and several sensor nodes. central node is the sink within the network and also functions as the gateway to public network through wlan or cellular network."
node 1 is the central node and other nodes sent 1024 kbsized packages to node 1 every second. it is clear to see that node 12 should have the largest sending power and die much sooner than the node which could access node 1 at a low transmitting power. it is easy to infer that energy levels of each node decrease irrelevantly. and the nodes die in accordance with the distance from central node. the network fails when certain node acquires important data like egg for patients with epilepsy.
"algorithm. wban is essentially a wireless sensor network, which is supposed to be self-organized and can survive the situation where topology changes and data is usually transmitted through relay nodes to central node. aside of the features, wban has some other issues to care about. firstly, energy for the nodes in wban is usually supplied by batteries so effective energy strategy is highly recommended. secondly, sensors can be either in the human body or on the surface of human body and the channel around or inside human body has a distinct property. thirdly, each sensor within a wban collects different kinds of data, thus leading to different transmission rate and throughput. on the other hand, traditional routing strategy tends to lead to a minimal hop route from each node to central node. and the nodes which are close to central node are more likely to be chosen to be a relay node, in which way their battery would be dying soon, therefore leading to degradation or even failure of the overall performance of the network. according to the reasons mentioned above, it is urgent to put forward a low-power, energy-saving, and robust routing strategy. this paper proposes a dynamic routing algorithm (dra) based on multipath choosing strategy to meet these requirements."
"however, g(i, δ + ) is convex by definition (17), and by the inductive hypothesis t k j(i) is convex. hence, t k+1 j(i) is convex. according to theorem 1-i for any initial function t 0 j(i), we have"
"dijkstra algorithm is a typical minimal path routing algorithm. it solved the problem to discover the shortest path from one single node to all other nodes within the whole network. in traditional ad hoc network where message transmission is uncertain, performance deteriorates rapidly as the scale of the network increases. complexity is proportional to 2, where refers to the number of codes. in wban, nevertheless, data flow is from each sensor node to central node and it is usually a small-scaled network. so the complexity of dijkstra algorithm would be simplified to a large extent. use dijkstra algorithm to determine the route which meets the restrictions of maximization of the minimum energy."
"where i k is the fisher information matrix (fim) [cit] . following the kolmogorov forward equation [cit], the estimate and the fim in the interval (k s−1, k"
"wban is a solution to modern medical problem as its continuous data acquisition and processing. it provided seamless care for elderly and precaution for chronic diseases. strictly restrained energy, however, is the limitation for commercial utility. dra and improved dra proposed in this paper solve the energy-saving and balance issue on the network level. the algorithm is to preserve the node at hot spot so that, with proper coefficient applied, network lifetime is significantly extended. the theoretical analysis and simulation result indicated that the performance of"
"in this paper, we developed a framework for obtaining the optimal stationary sampling policy based on the fim dynamics for transmission of measurements of a stochastic process from an encoder to a decoder through a costly communication channel. we proposed two computational methods, i.e., value iteration and policy iteration, to compute the optimal policy. we showed that there exists a triggering mechanism based on the value of information that generates the optimal policy. in addition, we showed that due to the monotonic property of the value of information, there exists a triggering mechanism based on the fim threshold that specifies the optimal policy for scalar systems."
. description of our benchmark datasets and other benchmark datasets (dataset d is our dataset) table s2 . the statistics of glycation datasets in this study table s3 . the results of training sets by svm under different parameters for xgboost. fig. s1 comparison of average pwaa scores in glycation and non-glycation samples.
"where weights w i are nonnegative. given that j(i i ) is the value function of a grid state i i, the value function of a non-grid state i is approximated bỹ"
many organizations and companies are committed to commercial utilization of wban system because of the huge potential seen in the area. these researches provided variety of solutions and introduce the technology to all walks of life.
"(5) use dijkstra algorithm to derive one minimum energy consumption path to the central node, during which mark the nodes that are chosen as a relay node so that every node is just used only once."
"even though human body is always not still, the comparative position of each node usually stays unchanged. that is to say, the far-away node, egg sensor, for instance, is not likely international journal of distributed sensor networks 3 to move to the space between cgg sensor and the central node. so although sensors are moving, the topology between them is abiding. so it is plausible to hypothesize as follows:"
"(1) network establishment: initialize all nodes within the network. generate the coordinates of node denoted by (, ). obviously this method could find the most energy-saving path. but it is not necessarily the optimal outcome which results in the longest lifetime of the whole network. in simulation process, sometimes it turns out to be that some branches worked nicely while others had a poor result. this is because there are no parameters that balance between different routes. to solve this problem, node protection strategy is proposed. that is to say, once the energy level of a relay node is below a certain threshold, it stops forwarding data and broadcasts the information to other nodes and its original son node is to find another relay node or transmit its data directly to central node directly."
"in traditional routing strategy, minimum hop is the primary parameter. so some spots have a great probability to be chosen as a relay. in wban, sensor nodes are strictly energyrestrained and hot spots would die from much forward packages. these nodes are usually around the central node and loss of their cooperation would degrade the overall performance severely. also, because each node around human body carries different sensors to collect different data, death of certain nodes would undermine the function of the system. to solve these problems, a dynamic routing algorithm based on multipath choosing strategy is proposed. the principle of the algorithm is to establish several paths between source node and destiny node. optimize the selection strategy according to the channel condition and remaining energy of the possible relay nodes. to achieve the longest lifetime of the whole network, the node with the minimum amount of energy is primarily considered. maximize the minimum remaining energy to choose the optimal path to process data transmission. compared with static routing strategy, the optimization could enhance the lifetime of the whole network."
"2.2. improved dra. as discussed above, the reported dra algorithm estimates the transmission energy cost to establish the energy consumption matrix and then bring the dijkstra algorithm involved to optimize the path to minimize the sensor location related energy cost. however, as many studies mentioned characteristics of wban are dynamically associated with the distribution of nodes and varied topologies. precisely, because of it, the dra algorithm is quite unstable due to the lack of considering a global variable to balance the whole network energy consumption."
"in recent years, the aging population has been receiving more and more attention; medical care problems of the elderly have become the focus of attention. the advent of wireless body area network can be a good solution to this problem. wireless body area network (wban) enables remote medical monitoring and diagnostics, saving a lot of manpower and resources for the medical care of the elderly, and the elderly do not have to stay in the hospital, which greatly expanded the space to improve the medical resources for the elderly utilization."
"according to the wban architecture, some nodes next to central node take so much forwarding tasks as relay in the data transmission and this situation will consume their energy soon, which in turn deteriorate the cooperation communication of the whole network because these nodes are always at hot spots. to solve this issue, the improved dra algorithm introduces a weight matrix for the energy consumption matrix. since the dijkstra algorithm is performed to choose the route limited by the minimum energy, which will evenly increase the energy cost of individual node according to the transmission work, this situation will run the hot spot quickly. the weight matrix along with the protection strategy when node energy level is low balances the energy consumption of nodes near or far from the central node. detailed process of improved dra is as follows:"
"t where x i is its ith component. we write x t to denote the transpose of the vector x. the identity matrix with dimension n is denoted by i n . we write δ kk to denote the kronecker delta function. we write p(x) to denote the probability distribution of the stochastic variable x. the normal distribution with mean µ and covariance σ is denoted by n (µ, σ). for matrices a and b, we write a 0 and b 0 to mean that a and b are positive definite and positive semi-definite, respectively."
"the current study investigates whether and to what extent resolve helped language learners with emotion word use. we examined learner emotion word use in the preand post-writings as well as analyzed their reflections of resolve experience. to reflect learner performance accurately, the results of the two judges' evaluations on each issue were reported respectively."
"it is worth noting that for the participants whose english proficiency was extremely high or extremely low, the benefit of resolve was less obvious. it is not surprising that advanced learners had sufficient competence to make appropriate lexical choices. on the other hand, the low-achieving learners' limited vocabulary was likely to make emotion word learning much more demanding [cit] . overall, less proficient learners significantly benefitted from the proposed system, resolve, in emotion word learning compared with the highly proficient learners, which was expected."
"to the best of our knowledge, this is the first study considering the application of sentiment analysis to assisting language learning. it is worth mentioning that although the focus of the current study is emotion vocabulary learning, the resolve system can be easily adapted for other vocabulary or synonym learning. while the preliminary findings supported that the developed system was beneficial to emotion word use, there are limitations in this study that could be improved. for example, a greater number of participants should be involved, which could help us take a closer look at potential learner difficulties in the use of emotion words. also, tool comparison (e.g., online thesauri) could be conducted to examine their individual impacts on learners."
"previous studies suggest that cultural and linguistic differences are two major causes leading to learning difficulties [cit] . emotion vocabulary in any given language is unique and reflective of the cultural context surrounding the language [cit] . communities have different ''affective reactions to specific events'' [cit] . for example, chinese culture discourages strong emotional expressions [cit] . on the other hand, linguistic differences are widely prevalent between languages [cit] in terms of the number of emotion words and semantic equivalence."
"the above mentioned differences reflect the complexities of emotion word use, which confuse learners and hinder their recognizing or capturing the meanings of emotion words [cit] . as a result, learners find it difficult to use emotion words appropriately in the target language. bearing this in mind, we developed resolve to help learners gain a better command of english emotion words. different from existing reference tools, the suggested synonyms are contextually appropriate rather than in alphabetical order. also, usage scenarios and example sentences of individual emotion synonyms are provided. such information, illustrating real-world language use [cit], is crucial for learning to differentiate the nuance of emotion words."
"in recent years, sentiment analysis or opinion mining is an increasingly important area in artificial intelligence or natural language processing [cit] . emotion vocabulary or sentiment lexicons are crucial linguistic resources for classifying or detecting the sentiments or emotions of user reviews [cit] ."
"pre-test: at the beginning of the study, the participants took an english proficiency test [cit] . afterwards, they were evenly divided into six groups according to their proficiency test scores. then the six categories were randomly assigned to the six groups. all the participants completed a writing task based on the second scenario of the assigned emotion categories."
"to have a greater understanding of the opinions of different students, the feedback of learners with different proficiency levels was analyzed (table 4) . a two-sample t test was carried out, and the results showed no significant difference between the two groups' responses. learners' perceptions on each survey question were discussed below."
emotion words have been demonstrated to be acquired more slowly compared with both concrete and abstract words [cit] . it indicates that mastering emotion vocabulary is not an easy task for language learners. the possible causes are subject to detailed discussion in the following section.
"the resolve system was developed adopting machine learning techniques because of its strength of learning from data and making predictions, which is an effective approach to sentiment analysis or opinion mining. figure 1 shows the resolve framework. we first extracted approximately 2.8 million example sentences from vocabulary.com and calculated the co-occurrences of all 3,785 emotion words and these example sentences. the purpose of this was to determine the appropriateness of the emotion words for the example sentences. at runtime, the synonyms of the target emotion word in the learner context are collected from wordnet synset. next, each sentence in the learner context is compared with the sentences in vocabulary.com using the edit distance algorithm, which quantifies the similarity between two word sequences as the number of steps it takes to transform one into the other. as a result, those sentences in the dictionary that share the highest similarity with those in the learner writings are then selected. based on the co-occurrence of the collected synonymous emotion words and the selected sentences from the dictionary, the sorting algorithm ranks the synonyms. as a result, the suggested emotion words are volume 6, 2018 figure 2. resolve system interface."
"post-test: in the post-test, all the participants were to complete another writing task on the second scenario of their assigned emotion categories. they also filled in the survey questionnaire."
"the second panel shows participant perceptions on system usefulness, including system function (item 3) and suggested information (item 4-6). first, the tool usability scored quite high. in particular, both groups were satisfied with being able to consult synonyms and usage information directly without switching visual focus between webpages while composing their writing. the results supported previous research on the split-attention effect [cit] : learning in an integrated environment reduces mental effort and improves learning results. participants' feedback showed that our design met their needs. second, the information provided by resolve included the ranked emotion words (item 4), the number of synonymous emotion words (item 5), and the usage information (item 6). both groups appreciated the suggested emotion words as well as the usage information. less proficient learners particularly acknowledged that the usage information (i.e., scenario descriptions, definitions and example sentences), benefit them in learning to use emotion synonyms appropriate to their contexts. interestingly, both groups did not seem to share similar views on the number of suggested synonyms, even though there was no significant difference between them. less proficient learners appreciated five suggestions whereas highly proficient participants expected more suggestions. some participants suggested that eight synonyms would yield adequate choices for better wording."
"mastering emotion words is a challenging task for language learners, so emotion vocabulary should be explicitly taught and consciously learned. the resolve system is applicable for use in classroom teaching and activities. the approach proposed in this study could be replicated in the classroom. instructors could create various scenarios for specific emotion categories and have their students compose writings via consulting resolve. afterwards, instructors elaborate on how the words students produce could be used in various scenarios. such pedagogical activities enable learners to develop a clearer understanding of as well as stronger command of emotion word use."
"treatment: during the three-week treatment phase, each participant was required to complete a total of nine writings for three scenarios of the assigned emotion category on the resolve system. precisely, three writings for one assigned scenario had to be completed per week. the participants were allowed to decide the events relevant to the given scenarios. after reviewing the synonymous emotion words and the corresponding usage information, the learner could either select one of the suggested emotion words or keep his original one to describe his feeling. note that in order to maintain the quality of learning, the interval between any two writings from the same participant was at least 24 hours."
"to address this issue, this study applied the sentiment analysis techniques to assisting language learning. we developed resolve (ranking emotional synonyms for language learners' vocabulary expansion) [cit], a context-aware emotion synonym suggestion system. taking advantage of machine learning capabilities of learning from and making predictions on data, the resolve system is able to suggest appropriate emotion synonyms based on learners' contextual information. importantly, each suggested emotion synonym comes along with usage information, including word definitions, scenario descriptions, and example sentences."
"regarding the approaches to sentiment analysis, machine learning is one of the most powerful approaches because of its capability of learning from the training data and then making predictions or decisions [cit] . for this reason, machine learning techniques have been widely used to detect or predict people's opinions, attitudes and emotions towards certain topics [cit], an early work, apply machine learning techniques to determine the polarity of product reviews and movie reviews. afterwards, a growing number of more sophisticated machine learning approaches have been proposed towards sentiment analysis tasks [cit] ."
"researchers have been concerned about the vocabulary size of language learners as well as how they use emotion words. for example, rintell [cit] analyzes personal experience narratives about emotional events from six native speakers of english and eight intermediate esl students and suggests that language learners use more direct and explicit statements, which are less elaborate than those of native speakers. moreover, learners did not use figurative language, which occurs regularly in the narratives of native speakers. kaneko [cit] compares a learner corpus (including chinese, japanese, and french learners) and a native speaker corpus, and reveals that the range of the emotion vocabulary of language learners is not as wide as that of native speakers. besides, learners use fewer attributive adjectives of emotion words in comparison to native speakers. pavlenko and driagina [cit] compare the oral narratives between native speakers of russian and advanced american learners of russian, and conclude that although advanced learners had rich emotion vocabularies, they still showed difficulty in semantic choices, which significantly differed from native speakers."
"a writing task and a survey questionnaire were developed to evaluate whether learners benefitted from the proposed system. the purpose of this study is to investigate learners' general performance in emotion word use. thus, ekman's six emotion categories [cit] was adopted, the most used characterization in many previous studies, while designing the writing task. for each category, we designed three individual scenarios, which were determined based on the results of a pilot study. the scenarios, described in the participants' first language to avoid misunderstandings, were to illustrate the emotion category instead of suggesting a specific emotion word. table 1 exemplifies scenarios for anger and disgust. the participants were allowed to decide their own events relevant to the given scenarios. also, they were to express a consistent emotion through the whole writing in at least 120 words and use a specific emotion word to describe their feelings."
"context-aware instead of being listed in an alphabetical order. it is worth noting that given more detailed contextual clues, the algorithm can effectively specify the emotion. however, longer contexts and more synonyms would increase computational complexity."
"over the past several decades, there has been limited research conducted on emotion vocabulary learning. issues addressed primarily involve learners' command of emotion vocabulary and differences of emotion word"
"thirty-six volunteers were recruited online to participate in this experiment. they confirmed their willingness to participate in this study by signing a consent form. because three participants were unable to fully participate in all of the experimental activities, data of a total of 33 participants were valid for analysis. these participants, 24 females and nine males, were aged between 23 and 38 (mean age 28.4). they were all mandarin-speaking efl learners with educational backgrounds of undergraduate or higher. eight of them majored in english in college whereas 25 of them were non-english majors. the participants had at least six years of formal english instruction and were estimated as upper intermediate learners of english, as measured by the proficiency test they took at the beginning of the experiment. a non-experimental pre-and post-test design was conducted. grounded in the statistical analyses, the findings are presented below."
this exploratory study aims to promote contextuallyappropriate emotion vocabulary learning. the resolve system was developed to serve this purpose. three research questions guide this study:
"to evaluate the appropriateness of learners' emotion word use, a 7-point grading scale ranging from 0 (the least appropriate) to 6 (the most appropriate) was used. two nativespeaker judges, trained before the pilot study, were given the participants' pre-and post-writings as well as the lists of synonyms of the target emotion words they produced, which were extracted from the corpus. the judges gave scores to each emotion synonym on the lists based on the participants' contexts. note that the judges had no information about the target emotion words participants produced in both the pre-and post-writings, which avoided scoring bias. participants' word choice rather than grammatical accuracy was the focus in the evaluation. if two or more words were considered equally appropriate, they were given equal scores. a weighted cohen's kappa value of 0.68 indicated substantial agreement between the judges."
"this paper is structured as follows. we first give a brief overview of the role of and the approach to emotion vocabulary in sentiment analysis. also, studies of emotion vocabulary learning are discussed. subsequently, the development of resolve is described. this is followed by an evaluation study, in which the results are discussed."
"emotion vocabulary has been studied in various disciplines, such as psychology [cit], linguistics, and computational linguistics [cit] . in particular, emotion vocabulary has been an essential component in sentiment analysis or opinion mining, tasks of great interest within the field of artificial intelligence combining natural language processing, machine learning and psychology [cit] over the past two decades."
"to prevent the context-switching problem, the resolve interface ( figure 2) consists of a writing area on the left and a reference area on the right. once the learner finishes writing, he may highlight the target emotion word surprising and click the synonym look up button. on receiving this request, the system initializes four zones on the right: synonyms for surprising, explanation of how the word can be used, definition, and usage examples. the listed emotion synonyms are amazing, surprising, astonishing, shocking, and startling, which are ranked based on the given context. this synonym ranking suggests the extent of their appropriateness. in this case amazing is deemed more appropriate than startling. note that the suggested synonyms may include the target word."
"the first research question investigated whether learners benefitted from the proposed tool support. to answer this question, we compared learners' scores of their emotion word use in the pre-and post-writings. besides the gain scores, we also reported learners' error reduction ratios (err) in order to avoid analytical bias. the error reduction ratio (err) quantifies the learner's error reduction from the pre-test to the full mark, which is shown in equation 1."
"taken together, the judges shared similar opinions with respect to the performance of the highly proficient learners whereas there were discrepancies for the less proficient learners. to investigate these differences, we scrutinized the individual performances of the less proficient participants. discrepancies were found in four less proficient participants, so the data of these four participants were particularly examined. table 3 shows the breakdown of the participants' emotion words and their scores in the pre-and post-tests."
"next, we investigated whether resolve was of great benefit to different learners ( table 2, second panel). to do so, the participants were divided into two groups based on their scores of the proficiency test, which was conducted in the first week. a total of 15 participants scored above or equal to the grand average (67.45), so they were classified as highly proficient. the other 18 participants with below-average scores were classified as less proficient. the average scores of the highly and less proficient groups were 80.93 and 56.22 out of 100.00 respectively. the learning effectiveness of the highly and less proficient students is visualized in figure 3 . both figures showed that after the treatment, less proficient students made more noticeable improvement as compared with the highly proficient students. in fact, less proficient students achieved almost the same or even higher levels of performance in the emotion wording task than their counterparts."
"however, emotion vocabulary has received little attention in second or foreign language learning [cit] . emotion words are either absent in pedagogical materials [cit] or are not emphasized in classrooms [cit] . little exposure leads to language learners' limited vocabulary size [cit] . as a result, learners opt for general terms or superordinates (e.g., ''happy'') rather than specific terms or hyponyms (e.g., ''thrilled'') to describe their feelings [cit] . on the other hand, the existing reference tools, such as dictionaries or thesauri, appear not directly or effectively to help learners discriminate the nuances of synonymous emotion words. the suggested emotion words may be accompanied by definitions but seldom carry explanations of how they could be used [cit] . as a result, even though learners learn the definitions of synonymous emotion words, they may still be unable to determine the appropriate word to ''fit the concept being expressed'' [cit] ."
"despite its important role, emotion words have received little attention in second or foreign language learning. to assist language learners with the expansion of as well as with appropriate use of emotion words, we developed the resolve system. in this study, wordnet-affect and wordnet were the two main lexical resources adopted to build the corpus mainly because wordnet-affect, despite a small number of words, provides six emotion categories [cit], whereas wordnet provides synonyms. sentiwordnet was not considered because more than 90 percent of the words are objective words [cit] that are not directly useful for our categorization of emotion words. meanwhile, in view of its ability to make predictions, machine learning techniques were adopted to suggest context-aware emotion words for language learners."
"to answer this research question, a seven-item reflection questionnaire was designed to elicit participant perceptions on resolve in terms of learner needs, system usefulness, and its contribution to language learning. moreover, participants' open-ended responses were collected to elicit the reasons underlying their answers to each closed-form question."
"specifically, from the perspective of judge 1, these four participants showed improvements whereas from judge 2's view, no one made progress except participant 1. although both judges agreed on participant 1's improvement, they were not consistent about the appropriateness of his emotion word use. we then take a further look at the performance of the other three participants from the evaluation of judge 2. participant 2 and participant 4 received full marks in the preand post-tests, leaving no room for improvement. turning to participant 3, the zero point in both pre-and post-tests showed that his emotion words were the least appropriate. for this reason, the views of both judges differed greatly. researchers such as dewaele and pavlenko [cit] suggest that the use of emotion words is related to individual experience; this could explain the judges' divergent views."
"the first panel illustrates the participant learning backgrounds: their writing behavior (item 1) and their needs for tool support (item 2). the scores of these two items were quite high (all above 3.80/5.00). both groups reported that they intended to restate their feelings. to have a better understanding of the behavior of the two groups, we scrutinized the emotion words produced by the participants and found that eight highly and 16 less proficient participants used different emotion words in their pre-and post-writings. it is likely that highly proficient participants had greater confidence in their emotion word use than the less proficient ones. meanwhile, participants' strong demand for tool support indicated that learners needed help to be engaged in emotion word learning."
"the second research question attempted to explore how the improvements of highly and less proficient learners differed. a second order polynomial curve was used to fit these (proficiency level, improved score) data points: the trend is shown in figure 4 . from the first judge's evaluation (left subfigure), a negative relationship can be seen between the learners' language proficiency and their improved scores. in other words, the lower the proficiency level, the more improvement achieved by the participant. the second judge's view, however, was somewhat different. the right figure reveals a downward trend for highly proficient learners and a slightly upward trend for less proficient learners. this means that for the highly proficient group, the higher the proficiency level, the smaller the gain; however, the less proficient learners did not follow this pattern."
"where max is the full mark (six in our scoring scheme), post and pre are the scores of post-and pre-tests, respectively. this can be interpreted as a measure of the room for improvement. note that errors in this study indicate inappropriate emotion word use. taking this study as an example, two learners both improved their scores by 1 point (out of 6): one learner's score increased from 4 to 5; the other from 5 to 6. their improvements were 20.0% ((5−4)/5) and 16.7% ((6−5)/6) respectively. at first sight, the learner who scored higher at first did not make as significant progress as the lower-scoring learner. indeed, to a certain extent, it can be hard for learners with better initial performance to make considerable improvements. however, the err shows different results. the improvement of the initially lower-scoring learner is 50.0% ((5−4)/(6−4)) while that of the initially higher-scoring learner is 100.0% ((6−5)/(6−5)). because err provides us with an alternative view on learners' improvement, we included err to examine participants' performances. the two judges' scorings were reported individually, which helped us learn the different views they held on learners' emotion word use. we first compared learners' average scores in the pre-and post-tests. the first row of table 2 shows that all learners volume 6, 2018 this indicates that the participants made improvement on emotion word use after the three-week treatment."
1. can the proposed system improve learners' emotion word use? 2. how does learner performance differ in terms of the level of proficiency? 3. what are learner perceptions of context-aware emotion vocabulary learning?
"the provided information could help learners to expand the size and knowledge of emotion vocabulary and further to facilitate appropriate use of emotion words in their writings. it is worth mentioning that although the focus of the current study is on emotion vocabulary learning, the resolve system can be easily adapted for other context-aware vocabulary or synonym learning."
"in addition, a seven-item reflection questionnaire was developed to elicit participant opinions on resolve. the first two questions sought information on participant writing behavior and demands for tool support. the next four questions explored participant views on the system's experiences in terms of system usability, the quality and number of suggested emotion synonyms, and usage information. the last question elicited participant perceptions on context-aware emotion vocabulary learning. all the questions used the five-point likert agreement scale ranging from 1 (strongly disagree) to 5 (strongly agree)."
"although a large vocabulary of emotion terms exists in any language, the number of words available to describe emotions differs greatly between languages [cit] . for example, altarriba [cit] finds that there are more than 100 words in the affective lexicon in english, whereas there exist fewer than 10 words in the affective lexicon in the malaysian language. the fact that different sizes of emotion lexicons in languages indicates that languages ''may lack lexical items for some emotions'' [cit] . in other words, differences in the number of emotion words in languages [cit] make it difficult to reach semantic equivalences [cit] . while lacking translation equivalents across languages, language learners would find ''alternative linguistic means when rendering these and other untranslatable emotion words'' in the other language [cit] . on the other hand, even though there exist translation equivalents between languages, they semantically appropriate [cit] . another case in point is that there are also times when various emotion words can be used to describe a certain feeling; however, these words may not be identical or interchangeable in all contexts [cit] ."
"if the learner selects one of the synonyms, such as astonishing, the system returns the corresponding usage information, including the descriptions of the specific scenarios, definitions, and example sentences. such information illustrates how the emotion word could be used, which enables learners to differentiate nuances of emotion words and select appropriate emotion words to fit their contexts. in this way, the suggested synonyms are customized for individual learners."
"if we exclude the markedly different data, a similar trend can be seen in the judge evaluations as that seen in figure 5 . that is, for the less proficient group, the lower the proficiency level, the greater the improvement. in contrast, for highly proficient participants, the higher the proficiency level, the smaller their gains. this could be because they had confidence in their command of emotion words, as evidenced by the fact that seven out of 15 participants used the same emotion words in their pre-and post-writings, which scored full marks. on the other hand, 16 out of 18 participants used different emotion words in their writings. their improved scores showed that their attempts were successful."
"several issues are worth noting. first, the second scenario was determined to avoid the effect of short-term memory mainly because the writing task of scenario two was performed in the third week, which was not too close to either the pre-test week or the post-test week. second, the events they described in the pre-and post-tests were identical. the purpose was to compare the appropriateness of the emotion words the participants produced. third, the participants were unable to refer back to their previous writings, which discouraged them from copying the previous writings. note that on both the pre-and post-tests, the participants completed their writings using the resolve interface but with the suggestion of synonymous emotion words withheld."
"regarding the corpus, 1,000 emotion words from wordnet-affect were first collected. next, to increase coverage, 2,785 synonyms of the above emotion words were included from wordnet synset [cit] and the merriam webster dictionary (http://www.learnersdictionary.com/3000-words/topic/emotions). then, following classification of the emotion words used in wordnet affect, all 3,785 emotion words were categorized into six major emotion categories: anger, disgust, fear, joy, sadness, and surprise [cit], which is the most used characterization in many studies."
"emotion vocabulary has been an important concern in several disciplines. they were especially widely used in sentiment detection or prediction. however, it has not received particular attention in foreign language teaching and learning [cit] . in view of the paucity of research, we developed resolve, a context-based emotion synonym suggestion system. developed using machine learning techniques, resolve is capable of suggesting contextually appropriate emotion synonyms. the usage information is also provided to reinforce learner's word use. the tool effectiveness was assessed using a writing task and a reflection questionnaire. the results demonstrated that both the suggested emotion synonyms and the corresponding usage information are beneficial to learners' use of emotion vocabulary. notably, less proficient participants showed greater improvements. moreover, analysis of the questionnaire responses showed that the participants appreciated the tool support in learning appropriate emotion word use."
a pilot test was conducted three months before the experiment to identify the feasibility and effectiveness of the operational procedure. a group of 19 efl college students enrolled in an english writing course participated in a five-week experiment involving the resolve system. we thereafter revised the experimental design based on these preliminary results.
"the model of this paper integrates the data view. patient data comprises of name (or id), age, disease and urgency of the case. it is determined and entered one after the other in several activities during the process execution. the patient data can be viewed as a dataflow that is handed over from one activity to another one. during transportation name and age of the patient should be determined. after reaching the hospital, an exact diagnosis takes place during the check of the patient. to help a doctor finding the correct diagnosis a list of diseases and respective symptoms could be presented by an information system."
"as an optimization for memory encryption, the vmm does not encrypt domain u's uninitialized pages when those pages are mapped on domain 0 (fig. 3 (c) ). we define as uninitialized a page in which any data has not been written since domain u is created. such uninitialized pages do not need to be encrypted because they contain no valid data. this reduces the overhead of memory encryption. for example, most of the domain u's pages are initialized by domain 0 when domain u is booted and resumed."
"one of the future work is to reduce the overhead of memory decryption. in the current implementation, pages mapped on the management vm are synchronously decrypted on unmapping. to overlap the decryption with i/o, the vmm should decrypt unmapped pages asynchronously. another future work is to enable vm introspection from the management vm. to introspect the user vms, the management vm has to access user vms' memory without encryption. we are planning to implement the support for vm introspection based on permission by the users. supporting full-virtualization is also necessary for applying vmcrypt to real iaas clouds. the vmm has to identify the memory regions used for framebuffers and dma as unencrypted pages, which is done by cloudvisor [cit] ."
"we examined the number of extra pages allocated for an encrypted view. when domain 0 maps domain u's pages, the vmm allocates new pages in domain 0 for encrypted replication. at worst, the number of the allocated pages could be equal to the total number of domain u's pages. however, the maximum number of extra pages was 1024 while we were migrating domain u. this is because the migration program mapped 1024 pages at once and unmapped all of them after the memory manipulation. 1024 pages are 4 mb of memory and not so large."
"to allocate the bitmap inside domain u's memory, the vmm reserves a memory region using the e820 facility of bios. since e820 is used to report the memory map to guest operating systems, the vmm inserts a reserved area into that memory map when it creates domain u. the encryption bitmap is copied to the reserved area in domain u's memory when those pages are mapped to domain 0. to preserve the integrity, the embedded bitmap is copied back on unmapping only while domain u is constructing, e.g., for vm migration. its details are described c ⃝ 2012 information processing society of japan in section 4.5."
"the grant table is a mechanism for sharing memory pages between domain u and domain 0. for example, i/o ring buffers are shared between the frontend drivers in domain u and the backend drivers in domain 0. with the grant table, domain 0 has to read and write the memory pages permitted by domain u. the vmm can identify all the shared pages by checking the grant table. domain 0 can map only the pages that domain u explicitly permits to access. such pages may include sensitive information, but domain u should encrypt it with encrypted file systems and vpn, as described in section 2."
the final property of interest is how robust the evolved cppns are to changes in resolution. a key benefit of cppn-neat over other evolutionary methods is that the evolved encoding are capable of producing structures at different figure 4 : mean running time in seconds with standard error bars for the two experiments that evolve structures at the highest resolution with the full set of cppn inputs. evolutionary trials in experiment 5 which always evaluate structures at the highest resolution take significantly longer than those of experiment 7 which evaluate structures at a reduced resolution for the first 100 generations.
"since the public key is signed with tc's private key, host a's vmm can verify it. the tc's public key is embedded into the vmm in advance to prevent man-in-the-middle attacks. next, host a's vmm encrypts the session key of the target vm with b's public key and transfers it to host b. host b's vmm decrypts it using b's private key and shares the session key for the migrating vm with host a's vmm. as such, session keys are passed to only hosts authorized by the tc."
"there are several reasons why it is desirable to have a growth procedure such as this. merely querying cppns over a sampling of three-dimensional space may lead to disconnected objects. even if all but one of these objects are thrown out much computational resources will have been wasted querying these regions of space. additionally imposing a grid over space to determine which points to query restricts the sort of structures that may be produced when compared to the \"point cloud\" method discussed above. for example curved structures can be constructed using the point cloud method, but only coarsely approximated on a grid. additionally, having a growth procedure allows for providing the cppn with knowledge about the structure's growth trajectory and environment which prove to be beneficial (see below) and which may more easily allow for environmental influences to act on this growth trajectory in more complex ways in the future."
"also noteworthy in this figure is that, when using the full set of cppn inputs, on average there is no significant difference in the best fitness in the final generation when comparing between experiments that always evaluate structures at the highest resolution and those that spend the first half of their generations evaluating structures at a lower resolution (comparing experiment 1 with experiment 3 and experiment 5 with experiment 7). this is important, because as shown in fig. 4 the evolutionary trials that spend their first 100 generations evaluating structures at a lower resolution run in significantly less time than those that always evaluate structures at the full resolution. this makes intuitive sense, because evaluating at the lower resolution requires fewer queries of the cppn and allows for faster physical simulations due to the lower complexity of the structure being evaluated."
we developed also a workflow runtime plugin which was presented in this paper. it uses the tool realizing parts of the uml action semantics for executing the workflow model based on the assl language [cit] . the required data objects are presented to the user during the workflow execution. mutual dependencies can be specified between data and the control flow specification. such constraints are observed by use and missing data entries are identified during workflow execution. the user is directed to the corresponding objects that violate the ocl invariants.
"another useful and significant input to the design process is knowledge about platform-specific constraints of rbbs. these constraints originate from the fact that rbbs in embedded systems are often optimised to exploit a particular feature of hardware components on which they are implemented. thus, they provide an acceptable quality with respect to efps while consuming a small amount of limited resources. we argue that these constraints need to be documented and accounted for in order to support integration of rbbs into embedded systems. for example, preissig [cit] reports the results of performance analysis of a data encryption standard (des) implementation optimised for the memory architecture of the used chip. similarly, other implementations of des rely on the presence of a certain instruction set to accelerate permutation operations [cit] . however, techniques for a systematic categorisation of such platform-related knowledge are currently missing. thus, to leverage the results of the rbbs' performance analysis along with constraints on platform components, both extrafunctional domain experts and embedded system engineers need a consistent framework to capture (and store) these outcomes and to reuse them for later design decisions respectively."
"the intuition is that while each component might operate in an interval that is a subset of its operational condition, the composition with another component that disallows that subset dictates that the first component is prohibited from operating in that condition. the presence of these constraints for components guides a system engineer to implement a mechanism (e.g. cooling system) that ensures that the corresponding components sustain the allowed env. conditions (e.g. to keep within a given temperature range). the liu magicdraw plugin generates these env. constraints automatically from given environment conditions attached to individual components."
"since the information on these ring buffers is stored in the start info page, the vmm can identify it easily. even if the attackers specify arbitrary pages as the ring buffers, domain u uses them only for console and xenstore, not for the other purposes. domain u may write sensitive information to the console, and so the contents should be encrypted between domain u and its user. such a mechanism for secure console is beyond the scope of this paper."
"to examine the overhead of constructing an encrypted view, we measured the time needed for mapping and unmapping a memory page of domain u on domain 0. we performed this experiment for writable mapping, read-only mapping, and the mapping of uninitialized memory. vmcrypt encrypts and decrypts a writable page, only encrypts a read-only page, and only decrypts an uninitialized page, respectively. we repeated memory mapping and unmapping 100000 times. figure 6 shows the mean time."
"robots that operate in outdoor or other unstructured environments such as the home or office would be of great social utility. but, to date, the vast majority of robots currently in use operate only in structured environments such as factories. if robots are to make the migration from factories into our everyday lives they will need to be adaptive; that is, they must exhibit intelligent behavior."
the literature provides many metamodels for business process modeling. some of them are used for conceptual modeling to define elements of the workflow language and their interrelations [cit] . these metamodels can be further used to implement modeling tools [cit] . the approach presented in this paper uses a uml metamodel along with the tool use [cit] . use checks static properties of the workflow models during the modeling process by observation of ocl invariants. the modeler gets quick feedback as identified problems and the involved modeling elements are immediately indicated.
"furthermore, an adapted use object diagram could be implemented as another plugin for use to provide a user interface that only allows valid adaptive changes."
modeling the organizational aspect is the core of this subsection. the organizational chart of figure 11 activities are either assigned to roles or to orgunits in figure 11 (b). they are assigned to persons at runtime by (assl) allocation procedures. the person has to hold the role to accomplish the allocation. roles and persons are parts of the hierarchical organizational model.
"this layer in fig. 4 consists of vendor component ontologies, where a vendor is a provider of platform components available for construction of execution platforms for embedded systems (e.g. texas instruments). each ontology encapsulates description of platform components that belong to some vendor. the resource ontology of the expert layer serves as a language to describe these components."
"in the query above, the ?a and ?b symbols denote variables. they are replaced by components of a system platform and components of an rbb platform (annotated with the \"gemrequiredcomponent\" stereotype) respectively. in our case study (see section 5.1), these are omap3530 and c64x+ for the aes rbb, and omap3530 and tms320c6211 for the des rbb. since tms320c64x+ has a c64x+ processor as its part, the query returns true. in contrast, no path is found between tms320c6211 and tms320c64x+ for the des rbb. thus, we conclude that the particular implementation of the des algorithm is not logically compatible with the current design of a system platform that is based on the omap3530 board, while aes can be selected as a rbb to provide secure communication for the tsm device."
"the vmm detects memory mapping and unmapping of domain u by monitoring the modification to the page tables of domain 0. the page tables are maintained by the vmm and protected to prevent illegal memory accesses. to modify a page table entry (pte), domain 0 has to issue a hypercall to the vmm. if domain 0 attempts to modify its pte directly, a page fault occurs and the vmm emulates the modification. in either case, the vmm can check the modification to ptes. when a new pte includes an mfn allocated to domain u, the vmm can notice that the modification is for memory mapping. when an old pte includes an mfn belonging to domain u, the modification is for memory unmapping."
"during the execution of the workflow the time aspect is captured by the workflow plugin and is stored as meta-data of the process instance. after or even during the execution the use tool provides an interface to state ocl queries for process mining purposes. thus, use provides a powerful mechanism to explore properties of the execution data of the workflows."
"we avoid using the ontology uml profile [cit] that allows designing ontologies as uml models since this requires indepth understanding of the underlying ontologies from an engineer. our goal is to exploit advantages of ontology technologies (e.g. querying services), but to allow an engineer to operate only with terms of a considered extra-functional domain."
"due to the privileges of the management vm, sensitive information in the user vms may leak via the management vm. from users' point of view, the administrators in iaas are not always trustworthy [cit] . lazy administrators may allow outside attackers to intrude in the management vm. worse, administrators themselves may be malicious and perform insider attacks. such attackers can easily steal sensitive information from the user vms. however, it is difficult to prohibit the access to vms' memory for the management vm because the administrators have to manage the user vms, e.g., vm migration, by accessing their memory. this inherently leads to information leakage from the user vms. tadokoro@csg.is.titech.ac.jp this paper proposes vmcrypt, which is a system that preserves the data secrecy of the user vms' memory. vmcrypt provides a dual memory view: a normal view and an encrypted view. a user vm can run normally with a normal view whereas the management vm cannot steal information through an encrypted view. these two views coexist so that both vms concurrently access the user vms' memory. to allow the management vm to inspect memory regions of the user vms for vm management, vmcrypt exceptionally gives a normal view to the management vm only for several regions. it automatically identifies such regions by monitoring the interaction between the management vm, the user vms, and the virtual machine monitor (vmm). the information on the identified regions is cached during the life cycle of a user vm to preserve the compatibility with the existing management software."
"the process for the workflow instantiation is as follows: the runtime plugin gets the models from the design time plugin as an assl file as presented in the tool chain of figure 12 . the user only needs to load the desired assl runtime file with the included process instantiation procedures. afterwards the possible instantiation procedures are listed and the user has to select one, which is consequently instantiated. as a result the process instance appears in the workflow plugin for execution."
"the vmm does not decrypt the received memory pages on memory unmapping, but just before domain u starts running. until the encryption bitmap is restored, the vmm cannot determine whether each page should be decrypted or not. this delay of decryption does not cause any problems. during live migration, domain u at the destination host neither runs nor accesses any pages. domain 0 can access shared pages such as the page tables because those pages are transferred without encryption from the source host. when domain 0 issues the first unpause hypercall to start domain u, the vmm extracts the embedded bitmap from domain u's memory. then it copies back the bitmap to the encryption bitmap in the vmm. after the bitmap extraction, the vmm decrypts memory pages of domain u on the basis of the encryption bitmap."
"to assist an embedded system engineer in reusing performance evaluation data captured by a domain expert, we exploit ontology technologies (see sect. 3.1). in particu- lar, the profile in fig. 2 is transformed into an ontology called the core evaluation ontology (the owl-file is available at our web-page 7 ). each refinement of this profile for an extra-functional domain (e.g. the one depicted in fig. 3 ) is transformed into a separate ontology called [domain name] evaluation ontology that imports the core evaluation ontology enriching it with additional concepts from this domain, i.e. concrete toe parameters, domain, and resource metrics. since description of actual performance evaluation results is essentially an instance of the profile, we transform it into axioms on individuals and call it [domain name] evaluation record. for example, a class annotated with the toe stereotype becomes an individual of the toe class in the core ontology."
"this subsection discusses the data aspect in the workflow metamodel. use is a tool that is used for validating uml data models at an early design stage before they are realized in a database system during a system implementation [cit] . data model integration seems to be very promising, because an integrated workflow model can describe data in connection with the control flow properties and vice versa. further on, these models can be executed in the workflow plugin so that the integrated models can be validated which will be subject of discussion in subsection iv b."
"note that our profile is not restricted to capture results obtained only when gqam or its refinements are used. for example, results presented by preissig [cit], that are obtained through a traditional approach, can also be described by gem (not shown in this paper). however, gqam can facilitate this task, since the mapping identified in table 1 can be automated as a transformation directly feeding relevant data into gem."
"in vmcrypt, the management vm could interfere with the vmm through the alteration of unencrypted pages of the user vms. for example, when the vmm traverses the p2m table, it might crash if the table was altered by the attackers so that it includes non-existing memory pages. to prevent this, our vmm carefully checks that the accessed pages belong to the target vm."
"the vmm does not encrypt the contents of unencrypted pages, which domain 0 can access for the vm management. when do- main 0 maps such pages, the vmm makes domain 0 share the pages with domain u. domain 0 can read information from the shared pages at any time, but it can update the pages only before domain u starts running. this prevents domain 0 from interfering with running domain u, e.g., by altering the page tables. this limitation does not disable most of vm management because domain 0 only sets up the shared pages at the creation time of domain u. when domain 0 unmaps the pages, the vmm simply ceases to share them. this mechanism is the same as the traditional one for memory mapping between domain 0 and domain u."
"next, we measured the times for suspension, resumption, and migration, using vmcrypt with aes. the resumption and migration times were 1.8 and 2.3 times longer than those in the vanilla xen, respectively. these overheads come from cryptographic operations of aes. interestingly, the overhead for suspension was only 7 % because the overhead of aes was hidden by disk i/o."
"note that multiplicity of the toe, approach, and workloadevents association ends are equal to one. each time a new approach (likewise workload or toe) is used a new evaluation should be defined. during one evaluation of a toe several evaluation platforms can be used. thus, several metrics sets can be obtained as indicated by the multiplicity of the metrics association end. in turn each set of metrics is uniquely associated with one evaluation platform (see measuredon)."
"vmcrypt provides a dual memory view for each user vm: a normal view and an encrypted view, as illustrated in fig. 1 . a normal view is a view of unencrypted memory and is used by a user vm. this view enables software running inside a user vm to access its memory as usual. in contrast, an encrypted view is a view of encrypted memory and provided to the management vm. management software running in the management vm can access only encrypted data via this view, so that the attackers in the management vm cannot steal any useful information inside the user vms. nevertheless, the iaas administrators in the management vm can manage the user vms through the encrypted views. for example, migrating a user vm is achieved by transferring the encrypted view of the vm's memory to the destination."
"third, we measured the number of re-transferred pages, which increases due to the re-transfer of the embedded bitmap. on average, 29.5 extra pages were re-transferred, but this is negligible when compared to the total number of pages of domain u."
"in the metamodel of figure 1 the class activity is the central part. a process contains several flowobjects that can be flowoperators, groups or activities. for practical reasons it is sufficient to connect a process object with at least one flowobject that connects all other flowobjects transitively. the operations getflowobjects() and getactivities() calculate that transitive closure. they are part of the metamodel but not explicitly shown in figure 1 ."
"this paper presents a method for using cppn-neat along with a novel growth procedure to evolve three dimensional structures appropriate for a specific task, presents some advantages of cppn-neat over other methods that may be used for evolving three-dimensional physical structures, and discusses how this method can be extended to co-evolve actuated body plans and control strategies."
"after completion of all evolutionary trials, statistics are computed for each experimental setup. specifically of interest is how fit the evolved structures are, how long it takes for each evolutionary trial to complete, and how robust evolved cppns are to changes in the resolution at which they are queried. fig. 3 shows the mean best fitnesses from the final generation for each of the eight experiments. the first thing to notice in this figure is that in three of the four pairs of experiments the structures evolved using the full set of cppn inputs (shown in red) on average achieve significantly higher fitness when compared with the structures evolved in the equivalent experiment using the restricted set. moreover in the fourth pair of experiments (experiments 5 and 6) on average there is no significant difference in performance between those evolved with the full set of cppn inputs and those evolved with the restricted set. this means that including the additional inputs never degrades performance of the evolved structures and more often than not leads to an improvement in performance."
"the vmm identifies the start info page by monitoring virtual cpu registers of domain u. this identification is done when domain 0 issues the first unpause hypercall for the domain u. this hypercall is used to start running domain u. to pass the start info page to domain u, domain 0 sets the virtual address of the start info page in domain u to the rsi register at boot time. the vmm translates the virtual address to the corresponding mfn. even if the attackers alter that register, they cannot steal useful information. they could make the vmm recognize an arbitrary page as the start info page. at this time, however, domain u will not be able to boot because an arbitrary page does not contain the correct information for booting. in addition, domain u has no sensitive information yet at the boot time."
"compositional pattern producing networks (cppns) are a form of artificial neural network (ann) where each internal node can have an activation function drawn from a diverse set of functions instead of being limited to a sigmoid function as is the case with classical anns. this function set includes functions that are repetitive such as sine or cosine as well as symmetric functions such as gaussian, thus easily allowing for motifs seen in natural systems: symmetry, repetition, and repetition with variation."
"in figure 12 the development and validation process is pictured. the diagrams used in use and the plugins are listed and related to the activities within the development process. on the left hand side of figure 12 an activity diagram is pictured. for every activity, a diagram, plugin or tool is linked within the tool chain. the arrows sequenzing the activities and tools represent object flows that are outputs of the previous activity or tool and input for the next activity or tool in the chain. the figure illustrates the order in which the diagrams and plugins are applied in use."
"for the page cache, we attempted an attack to obtain shadow passwords from the page cache in domain u's memory. we logged in domain u as root, so that the contents of /etc/shadow were stored in the page cache. then we searched the memory dump of the domain u for the string \"root:$\", with which a root password begins. vmcrypt disabled such a password search whereas we could find a root password when the page cache was not encrypted by vmcrypt."
this subsection is about integration of organizational aspects into the workflow metamodel. in use an organizational model can be integrated quite easily. figure 8 shows the organizational metamodel with the integration of the activity class of the workflow metamodel. it expresses the allocation of the role to the activity at design time and the person to activity at runtime.
"to select for this property, an evolved virtual object is placed in a physical simulator 1 for a set amount of time. the fitness of this object is then calculated by finding the point of the object nearest to the origin of the space (where the object was touching the ground) in terms of the planar euclidean distance. this distance becomes the fitness of the object and therefore of the cppn that produced the object, which cppn-neat attempts to maximize."
"there are even more benefits to uml workflow metamodels with respect to dynamic properties. they provide means to define execution semantics. ocl invariants are used for system states and pre-and postconditions for operations. they describe the causal or temporal relationships between the modeling elements. during execution of the workflow model, the execution semantics is interpreted and disallowed flows of a process are forbidden. furthermore, enabled activities can be identified and a worklist visualization is possible."
"the approach is validated using a case study that is also used as a running example and presented in sect. 2. sect. 3 gives the background. thereafter, sect. 4 explains the proposed uml profile to record results of the rbb performance analysis. we present our method for the model-based compatibility analysis in sect. 5. sect. 6 shows results of scalability and performance estimations for our approach. the paper ends with a summary of related works followed by conclusions. fig. 1 depicts a smart grid metering infrastructure developed by the company mixedmode within the european secfutur project [cit] . it is built of a set of metering devices, database servers, client applications, and communication infrastructure. the main goal of this system is to measure energy consumption at households and associate it with client data for billing purposes."
in this paper a uml metamodel based approach for workflow modeling is presented. this technique is powerful which is demonstrated by checking it against the workflow patterns. the original 20 workflow patterns can be expressed and some of them can be expressed easier than in popular languages like bpmn or epcs like for example the interleave parallel routing pattern [cit] .
"queries like the first two are implemented directly as a set of sparql queries [cit] . however, the task of compatibility analysis requires more sophisticated support. hence, in the rest of this paper, we present the developed method for cross-domain model-based compatibility analysis, where one domain is an extra-functional domain and the other one is an application domain. to demonstrate introduced concepts and methods, we use the security domain as an example of efps."
the rest of this article is structured as follows. in section ii the metamodel is introduced. ocl is used to express the semantics of the metamodel elements. in section iii the metamodel is applied to model an example workflow with the use tool and with its abstract syntax. data and organizational aspects of the workflow are also captured in the model. in section iv the execution of the workflow models is presented. running processes and activities are shown to the user in a specialized workflow view. section v discusses related work while section vi concludes the paper.
"specifically, the work presented in this paper makes use of a recently introduced abstraction of development known as compositional pattern producing networks (cppns) [cit] to grow three-dimensional physical structures. in this paper, these cppns are evolved using cppn-neat [cit] to produce physical structures capable of conserving momentum to achieve maximum displacement due to gravity. cppns are used here because they are a form of indirect encoding that have been shown able to capture geometric symmetries appropriate to the system being evolved, are capable of reproducing outputs at multiple resolutions [cit], and have shown promise in producing neural network control policies for legged robots [cit] . the combination of these features makes it likely that evolving cppns will prove to be a more promising approach to realizing intelligent agents than other approaches."
"with vmcrypt, we performed live migration, which transfers the memory image without stopping domain u and reduces the downtime of domain u. for comparison, we also performed migration with ssl. xen provides the mechanism of migrating domain u via an ssl connection. although ssl can prevent information leakage on the network, domain 0 at both hosts can still steal information. transferred data on the ssl connection is decrypted at domain 0. xen used aes with a key size of 256 bits for encryption."
"in this paper, we proposed vmcrypt, which preserves the data secrecy of user vms' memory in iaas clouds. vmcrypt provides a dual memory view concurrently and prevents sensitive information from leaking to the management vm. it also allows the administrators to use the existing management software including live migration. to support the existing management software for para-virtualized operating systems, vmcrypt automatically identifies unencrypted memory regions and caches that information during the life cycle of user vms. we have implemented vmcrypt in the xen vmm and confirmed that the administrators could manage user vms as usual but could not steal the sensitive information. in particular, the downtime due to live migration was comparable with the vanilla xen and still less than one second."
"but the adaptive changes of the workflow instance by deletions and state modifications of activities generate some problems. certain changes are not reasonable but still allowed by the tool, for example if historical data is deleted in the workflow instance. furthermore, an activity state change could be conducted although it is not allowed by the life cycle specification of figure 2 . in general activity objects must not be deleted or changed within the object diagram in use. if an activity should not be executed during the process execution it shall be skipped instead of deleting it in the object diagram. in contrast, adding activities should not cause any problems during the process execution."
"we examined the overhead of remote attestation, which is performed only when a host is booted. first, we measured the time needed for the measurement of the vmm using trustedgrub 1.1.5 [cit] . trustedgrub is a boot loader that calculates the sha-1 hash value of not only the vmm but also the kernel in domain 0 and stores the value in the tpm. we compared the reboot time for trustedgrub with that for grub legacy. the time for the measurement was 0.5 sec."
"the bottom layer in fig. 4 is the engineer layer. at this level, system and security engineers use the ontologies created at the expert and vendor layers to model the adopted platform and components used for evaluation of rbbs. in particular, an engineer uses existing components provided by vendors and stored in vendor ontologies, whereas the resource ontology is used if such a component is not known or not present in vendor ontologies. thereafter, a security engineer creates a security evaluation record (i.e. an instance of the security evaluation ontology) where the gemplatform concept refers to the rbb platform specification (see fig. 4 )."
"the encryption bitmap in the vmm is also embedded into domain u's memory. when domain u is migrated, the embedded bitmap is automatically transferred to the destination as well as the domain u's memory itself. this allows domain 0 to use the existing management software as is. through the embedded bitmap, the vmm at the destination can extract information on unencrypted pages for the migrated domain u. without the encryption bitmap, the vmm could not identify unencrypted pages when domain u is restored."
"domain 0 first maps the shared info page of domain u to obtain information on the vm and the p2m table. then it transfers the p2m table to the destination host. at this time, it canonicalizes the entries so that the table does not depend on host-specific memory allocation. specifically, it replaces mfns in the entries with the corresponding pfns. next, domain 0 maps all the pages of domain u and transfers their contents to the destination in turn. when domain 0 transfers pages used for page tables, it canonicalizes the ptes in the tables. in live migration, domain 0 repeatedly transfers dirty pages, which are modified by domain u during migration. finally, it stops domain u and transfers the remaining dirty pages and other states."
"in our approach, a workflow plugin is implemented for the use tool that presents the activities and the corresponding execution states in an appropriate way. the user can interact with the workflow plugin. related data objects are presented to the user so that scenarios can be played through by the user interacting with the tool. thus, the integrated workflow and data models can be tested before system implementation and the workflow models can be validated at an early design state."
"(rbb). for example, such rbbs can encapsulate security functions, e.g. encryption and digital signature, that provide such efps as confidentiality and integrity of data respectively. moreover, rbb encapsulation of functions needed to satisfy efps allows domain specialists to study rbbs before their integration into different embedded devices that are complex hardware-software systems. note that not all efps can be implemented in the form of rbbs, but such efps are outside of the scope of this work."
"we have implemented vmcrypt in xen [cit] and vmcrypt currently supports para-virtualized linux as guest operating systems. our experimental results show that vmcrypt allows the administrators to manage the user vms securely and correctly. with vmcrypt, the administrators in the management vm failed to find cryptographic keys and passwords from the user vm's memory. nevertheless, they could boot, suspend, resume, and migrate user vms as usual. they could perform even live migration [cit] for running user vms. the execution performance of vm management was degraded mainly by the overheads of cryptographic operations. however, the downtime due to live migration was still less than one second and its overhead was 13%."
"alteration of the page tables is also detected by the embedded hash value. just before domain u starts running, the vmm canonicalizes the ptes, calculates the hash value, and compares it with the embedded one. replay attacks are not impossible, but it is difficult to steal useful information with only old page tables. in addition, the pages used for replayed page tables have to be marked as unencrypted in the encryption bitmap, which cannot be compromised."
one major purpose of this paper is to demonstrate that cppn-neat coupled with the growth procedure just presented is capable of evolving three-dimensional physical structures with desirable dynamic properties. this is a necessary capability of any evolutionary algorithm to be used for coevolving robot morphology and control. in particular the property selected for in this work is the maximum displacement of an object due to gravity from a starting position where part of the object begins in contact with the ground.
"initially, needed data has to be modeled in a uml class diagram. as shown in table ii uml class diagrams are used for data modeling at design time. during this step of the design process, a data model may be already validated with use by creating snapshots as object diagrams and stating ocl queries (see [cit] ). the data model for the data aspect of the emergency process is shown in figure 9 . class patientdata provides needed data within the attributes. it is a subclass of dataflowobject which in turn is a part of the workflow metamodel. thus, patientdata may be used as a dataflow object within the workflow model. class disease represents possible diseases with their associated symptoms, as stored in the hospital's information system. patientdata has an attribute that links to the particular disease which is identified during the initial diagnosis."
the object diagram in use is showing the workflow instance at runtime in which the execution data is captured. this can also be used for inspection but the data is not presented in an easily readable way. it seems to be more promising to use process mining with ocl queries to get certain properties of a workflow execution. an example ocl term is given in the following. this query lists the activities ordered by elapsed execution time each activity has needed. any number of further queries can be stated and evaluated in the ocl evaluation window [cit] .
this section presents a brief description of cppns and the cppn-neat evolutionary algorithm. this is followed by a description of the methods used for generating threedimensional physical structures from evolved genotypes. following this a description is presented of the fitness function used for evaluating these structures.
"in figure 4 the mergeoperator is used in combination with the xordecision activity (wcp4) in a certain structure [cit] . it ensures that all pred activities are in state done or skipped before the succ activities, or groups of activities, can start. if the pred object is a group all included activities have to be done or skipped. merges typically appear in such composite structures. this is essential in wcp7 and emphasized by its name: structured synchronizing merge. it requires to have a decision directly prior each merge. decisions are expressed through decision activities that have to be finished before a merge is possible. the non-selected activities and groups are immediately skipped after the decision activity is done [cit] . with this characteristic the merge can express wcp5 and wcp7. the wcp9 is expressed by discriminator. it uses a similar composite structure as the one in figure 4, but the invariant is somewhat different to the one of merge. it allows succ activities connected to the discriminator object to start after one pred object is finished. figure 5 is showing the wcp8 which is expressed by the multimerge object. multimerge is a subclass of iterationgroup that allows iterative executions of the included activities one after another by calling the nextiteration() operation. to define the semantics of the wcp8, two ocl invariants are used. one says that the number of iterations is not allowed to exceed the number of executed pred activities or groups. the other invariant states that all succ activities are not allowed to be started before the number of iterations are equal to the number of executed pred objects. a group is classified as executed if at least one included activity is done and no other activity is waiting, running or failed. as discussed before, an invariant assures the static property that only activities or groups are allowed to be connected as pred objects to the multimerge object. in figure 6 (a) the wcp10 is expressed. new iteration cycles can be initiated by calling nextiteration() of iterationgroup at runtime. an ocl precondition states that all included activities are skipped, done or canceled. the included activities are reset to waiting and execution data of the previous iteration is stored through the association archive. in this archive every iteration cycle is expressed as a group object that itself is connected to the executed activity instances with the assigned execution data as timestamps in the attributes start and finish. in figure 6 (b) wcp18 is expressed by using the exceeded association. this association may have side effects on linked activities or groups. if an activity with the role until starts, all activities with the role before that are not already started or finished will skip. these side effects are implemented in the assl start procedure that is used for the execution of the process models at runtime. if, in the example of figure 6 (b), the activity d is started while the activities of group it1 and the activity c are still waiting they are skipped. if the current iteration cycle of it1 is running that iteration can finish but no further iterations can be initiated. this integrity constraint is expressed in the precondition of the nextiteration() operation in the class iterationgroup."
the above definitions for computing the compatibility relation and their pairwise imposed constraints allow us to reason about env. conditions of assemblies based on constraints for its constituent components.
"we assume that iaas providers themselves are trusted, as widely accepted [cit] . in iaas, trusted senior administrators should be responsible for the maintenance of vmms and the hardware. they would not be lazy or malicious, unlike average iaas administrators that manage the user vms in the management vm. in real iaas clouds, average administrators may manage vmms and the hardware as well, but senior administrators should finally examine the correctness. therefore, we assume that vmms are well maintained and have no vulnerabilities that are compromised by the attackers. also, we do not consider physical attacks such as the cold boot attack [cit] because server rooms should be strictly protected in data centers."
"a session key for a user vm has the following life cycle. the vmm generates a new session key whenever a user vm is created whereas it discards the key at the destruction time of the vm. when a user vm is migrated, the vmm transfers the encrypted session key from a source host to a destination host, as proposed in tccp [cit] . figure 2 shows the procedure. first, host a's vmm obtains the public key of host b's vmm securely from the tc."
"vmcrypt automatically identifies such unencrypted memory regions by monitoring the interaction between the management vm, the user vms, and the vmm. for example, a shared memory region is passed from the management vm to the user vm c ⃝ 2012 information processing society of japan via the vmm. the memory regions for the page tables are registered to the vmm. as such, the vmm can recognize all the unencrypted memory regions. the information on the identified memory regions is cached during the life cycle of a user vm, including vm migration, to preserve the compatibility with the existing management software. with the cache, the vmm can restore the user vm's memory correctly even at another host."
"business process modeling gets more and more important with the increasing complexity and automation of business processes in companies and organizations. business process models are used to document, restructure and optimize the processes. furthermore, requirements for software and computer services that support the business processes are captured in these models. nowadays flow oriented languages are frequently used for business process modeling like event-driven process chains (epc), uml activity diagrams and bpmn. these are languages based on petri net token semantics which may restrict developers too much since they are following the principle \"all executions paths are forbidden if they are not allowed in the model\" [cit] . besides, the well-accepted workflow patterns are driven by petri net token semantics. in contrast, declarative workflow models have a flexible background driven by design principles. that means, all execution paths are allowed if they are not explicitly forbidden. this declarative view is followed in this paper."
"in our work, we recognise the need to reuse the performance evaluation results of rbbs for their integration into embedded system designs when considering efps already at the early development phases. thus, we present an infrastructure that unifies uml profiling, ontologies, and transformation techniques to support the domain engineer in describing these results and an embedded system engineer in using this knowledge. additionally, we elaborate the model-based compatibility analysis as a basis to reuse the performance analysis results across domains. we developed the liu magicdraw plugin to support the above tasks. the case study of smart metering devices is used to illustrate our proposal."
"xen distinguishes machine memory and pseudo-physical memory to virtualize memory. machine memory is the entire memory installed in the machine and consists of a set of machine page frames. it is reserved for the vmm, allocated to domains, or unallocated. each machine page frame has a number called the machine frame number (mfn), which is consecutively numbered from 0. pseudo-physical memory is a per-domain abstraction and allows a guest operating system to consider the allocated physical pages as contiguous. for each machine page frame, a pseudo-physical frame number (pfn) is consecutively numbered from 0. the vmm maintains a machine-to-physical (m2p) table for the mapping from mfns to pfns. for the inverse mapping, a physical-to-machine (p2m) table is maintained by each domain."
"next, we measured the downtimes due to live migration of domain u. here, the downtime is the time from when domain u is stopped at the source host until it is restarted at the destination host. as shown in figure 7 (b), the downtime was also proportional to the memory size of domain u. in vmcrypt with aes, it was still less than one second even when the memory size was 4 gb. it was 13 % longer than that in the vanilla xen."
"when vmcrypt used the null cipher, the execution time increased by 3 µs in comparison with the vanilla xen. this overhead comes from examining the encryption bitmap and replicating a page. the optimization for encrypted replication was not effective because the memory copies were completed only on the cpu cache. when vmcrypt performed both encryption and decryption of aes, the execution time was 81 µs, which was 7.7 times longer than that in the vanilla xen. when the optimization was enabled, the execution time was reduced to less than 60 % of the non-optimized case. this shows that our optimization for reducing cryptographic operations is effective."
a number of data items is created during the emergency process. for example a nurse has to document if she gives medication to the patient. kind and amount of medicine are important for the medication protocol. to avoid applying an overdose it can be helpful to present this protocol to her during the execution of the corresponding activity.
"embedded computing has a ubiquitous presence in almost every facet of our life. as a result, the focus of embedded system engineers is increasingly moving from realising only application functions to providing extra-functional properties (efps) for them. safety and security are among these efps that highly impact the design of embedded systems, sometimes originated from certification or legal requirements. often efps have a cross-cutting nature that impact systems in many dimensions. functions that enforce these properties can, in turn, be encapsulated into reusable units built of software and hardware components and used across many applications. in our work, a reusable unit that provides functions to satisfy efps is called reusable building block crts '13 vancouver, canada copyright is held by the authors."
"in the case study outlined in sect. 2, tsm devices are built on the omap3530 board from fig. 5 . since this component will have been described and stored in the vendor ontology a system engineer only needs to load the corresponding ontology and use this component as a part of the tsm model. the liu magicdraw plugin supports this functionality."
"next, we execute 50 runs of the transformation and measure the execution time for each run (see table 2, row 9). the hardware used is a system with 2.8 ghz intel core i7 and 8 gb of ram running mac os. in our case, the transformation time does not vary substantially for small and medium mcus, while a one second increase is observed for the sophisticated 32-bit mcus. this increase can be explained by naturally larger, in comparison with 8-bit and 16-bit mcus, complexity of 32-bit mcus in both number of elements and their attributes."
"model-based compatibility analysis allows automatic accounting for platform constraints while selecting a set of rbbs to be used for a system design. recall, that an extra-functional domain engineer provides these constraints by annotating elements of a marte model, which describes an evaluation platform, with the gemrequiredcomponent stereotype (see fig. 2 ). the core of our method is a set of ontologies and sparql queries. they are designed to infer whether rbbs and an adopted platform for the system are compatible with respect to formulated platform constraints for an rbb and platform declarations of the system being configured. we develop the liu (linköping university) magicdraw 8 plugin to support transformation of marte models into ontologies and to execute compatibility analysis. sect. 5.1 explains the developed ontology hierarchy exemplified using the security domain knowledge, and sect. 5.2 defines the notion of compatibility."
the assignment to activities is achieved by the abstract class bindingobject and the association allocation. similar to the aris method [cit] activities can be assigned to organizational units or rather departments with the association class alloc_unit. the selection of the persons from the department is specified in the attribute alloctype. three types are given by the metamodel within the enumeration allocationtype. anyperson selects a idle person of that department to execute the task. allroles selects for every role assigned to the department a person at runtime to the activity execution. the last value allpersons would allocate all persons of the department to the task execution. an example will follow in subsection iii c. in this section we demonstrate how the metamodel is used to model an actual workflow with respect to data and organizational aspects. firstly the workflow is explained using natural language. afterwards the metamodels introduced in section ii will be applied to create the necessary models. in the last subsection a development process and a tool chain will be presented.
"during the execution of the workflow, model elements can be adaptively changed in the workflow instance. adaptation patterns [cit] are supported on the process instance level. changes cannot take effect from the instance level back to the process model or \"type level\" (see [cit] )."
"domain disaggregation [cit] moves management functions in the management vm to another vm called domb. this architecture reduces the privileges of the management vm and prevents the attackers in the management vm from stealing sensitive information in the user vms' memory. for example, most of the code for building vms is run in domb. however, the administrators have to be still trusted because they also manage domb as well as the management vm. in addition, domain disaggregation needs a large modification to the existing management software so as to use the functions provided by domb."
"overshadow [cit] provides a dual memory view using the vmm. the basic idea is similar to vmcrypt, but overshadow provides a normal view for applications and an encrypted view for the guest operating system. the aim of overshadow is to prevent information leakage from applications to the operating system. furthermore, sp 3 [cit] can control views among applications as well. in contrast, vmcrypt prevents information leakage from the user vms to the management vm. particularly, the mechanisms for enabling live migration are specific to vms."
"we have implemented vmcrypt in xen 4.0.1 [cit] . in xen, a user vm is domain u and the management vm is domain 0. in the current implementation, vmcrypt supports para-virtualized linux for the x86-64 architecture as guest operating systems. vmcrypt mainly depends on xen in how the vmm identifies unencrypted memory regions. if we implement this method in the vmm, vmcrypt can be applied to full-virtualization in xen and the other virtualized systems such as vmware esx."
"since gem is defined as a general uml profile, a domain expert can refine some of its concepts to tailor it to a certain domain. in particular, an expert needs to defined data types for toeparameter, domainmetrics, and resourcemetrics. fig. 3 shows such a refinement for the security domain as an example where a cipher rbb is considered."
"the marte gqam sub-profile (mentioned in sect. 3) significantly helps the domain experts to design their evaluations. in order to demonstrate how our profile can be used to capture performance evaluation results modelled in gqam, we identify the correspondence between the stereotypes and tags of gem and gqam shown in table 1 ."
"the feedback loop is a significant step of any analysis as indicated with the step \"give feedback to designers\" by petriu [cit] . the gem profile supports this step enriching it with three additional principles: encapsulation of efp functions into rbbs; separation of a system designer and (extra-functional) domain expert roles; and reuse of the evaluation results."
we implement the compatibility analysis as queries to the vendor ontologies searching for potentially hidden relations. this allows accounting and reusing the expert knowledge captured by vendors as opposed to querying unrelated models using a tool such as emf query 15 .
"as mentioned in sect. 2, the data transmitted between a tsm and tsmc must be protected against confidentiality threats. we consider two candidate rbbs, namely the aes 11 www.owlapi.sourceforge.net 12 www.eclipse.org/acceleo/ (advanced encryption standard) 13 and des (data encryption standard) [cit] implementations by texas instruments, which can provide this efp. aes rbb requires the use of the c64x+ processor while the des rbb requires the use of the tms320c6211 chip. in the next section, we explain how the suggested architecture of ontologies enables selection of rbbs based on platform compatibility analysis."
"the trusted vmm maintains two types of cryptographic keys: session keys and a pair of public and private keys. a session key is maintained for each user vm. it is used to construct an encrypted view from a normal one of user vm's memory and vice versa. a pair of public and private keys is used to encrypt and decrypt the session keys. the public key is securely registered to the tc. since the session keys and the private key are kept inside the trusted vmm, they are not stolen by the attackers in the management vm. also, they are not extracted from the binary file of the vmm stored in a disk. the session keys are dynamically generated at runtime. the private key is sealed by the tpm and only the securely-booted vmm can unseal it."
"owl represents an ontology as a set of axioms. these axioms describe classes, their relations, and individuals. an owl class declares the concept of a domain and can be refined by sub-classes. owl individuals are instances of the owl classes. owl supports two types of relations. an owl object property defines a relation between two individuals, where one of them plays the role of a domain, and another one the role of a value range. an owl datatype property serves to introduce relations between an individual (domain) and the xml schema datatypes (range) known as xsd 1 . the owl language supports the importing feature. it allows relating different owl ontologies using the owl:import statement. to design and manage an ontology, one can use tools such as protégé 2 . in our work, we exploit the java owl apis 3 since the rest of our tool chain is java-based."
"in order to capture the essence of evolving non-trivial physical structures the current work tackles the problem of evolving solid objects with these important properties. while not incorporating any actuation and therefore not robots, strictly speaking, the structures evolved in this work provide a stepping stone for the eventual evolution of fully articulated robots controlled by closed loop, neural network control policies. this approach is not without precedent in the field of evolutionary robotics. funes and pollack [cit] first demonstrated evolving solid structures such as bridges, scaffolds, and crane arms constructed of lego bricks before moving on to the evolution of actuated robots [cit] ."
"the actual measurement is done by trusted sensor modules (tsms) consisting of a computing platform and physical sensors. the acquired data is transferred via a local bus from each tsm to a trusted sensor module collector (tsmc) and then eventually sent to an operator server via a generalpurpose network. tsmc and tsm are functional modules that are implemented on the same physical platform. in our previous works [cit], we presented a technique to systematically extract assets that need protection using functional models. also, we illustrated a security engineering process figure 1 : smart metering application [cit] that bridges the gap between the security experts and the engineers specialised in embedded systems with this networked scenario [cit] . this paper takes the process further by selection of suitable rbbs for encryption and decryption functions, as examples of means to satisfy extra-functional properties. we will use the case study in the forthcoming sections to illustrate new concepts when they are introduced in the paper."
we assume that the management vm can be compromised by outside attackers or abused by iaas administrators. such attackers could take the root privilege of the operating system in the management vm and even alter the operating system kernel. this means that the management vm is removed from the trusted computing base in terms of confidentiality.
"the user can conduct adaptive changes in the object diagram view of use. that is already known from the design time shown in figure 10 . activities, data or organizational aspects can be added, deleted or changed by manipulating the workflow execution data or objects of the data or organizational model."
"data and organizational aspects can be modeled. an abstract syntax is provided with the uml tool use for workflow-, data-and organizational modeling. the workflow models can be captured by the developed workflow design time plugin for use. the model is persistently stored in assl-files for later reuse at runtime."
these problems are avoided in the approach presented here by having an integrated process and data model within the use object diagram. the integration of the data views into the workflow models is identified with the workflow data patterns [cit] .
"enqueue return false figure 2 : grow structure pseudo code. the growth procedure starts with a root node at the origin (line 3). then, as long as there are nodes in the queue to consider it takes the node at the front of the queue, casts a point cloud around it and considers adding a node at each point in turn (lines 5-18). a node is added at a given point if all of the following hold: it does not conflict with a previously added node, the cppn outputs a value above the threshold tmatter when queried at that point, the point is within the bounding cube, and the maximum number of nodes m has not been reached (lines 19-23)."
"resolutions, and as mentioned above if these structures are going to be physically fabricated it is important that they not be too sensitive to changes in resolution. fig. 5 demonstrates how some of the evolved structures have similar dynamics and achieve similar performance when grown at different resolutions. this figure shows the dynamics of a single structure that achieves the best fitness in one of the evolutionary trials in experiment 5 first as it was evolved, then regrown at a lower resolution and finally regrown once more at a higher resolution. it is noteworthy how in all three cases a mass distribution is preserved that allows the structure to first fall onto its heavier end, carry its momentum through a horizontal rotation and fall once again away from its starting position."
"we confirmed that vmcrypt prevented information leakage from domain u's memory. first, we attempted an attack to find aes shared keys used by openssh processes. we logged in domain u to make the ssh server generate an aes key, obtained the memory dump of the domain u, and used the aeskeyfind tool [cit] to find aes keys from the memory dump. second, we attempted an attack to find rsa private keys generated by openssl. we generated an x.509 certificate signing request by executing the openssl command in domain u and used the rsakeyfind tool [cit] . we could find several keys without vmcrypt, but vmcrypt could successfully prevent such tools from finding keys."
"the secure runtime environment (sre) [cit] preserves the confidentiality of user vms for para-virtualized operating systems. like vmcrypt, sre encrypts memory pages except several pages necessary for vm management when the management vm accesses them. sre supports boot, suspend, and resume of user vms. however, it cannot enable live migration because it provides an encrypted view to the management vm only when a user vm is not running. moreover, sre needs to modify the existing management software so that the vmm can identify all the unencrypted pages. for example, the resume program has to notify the vmm of the pages used for the p2m table because the vmm cannot identify them at the resume time. in vmware vsphere hypervisor (esxi) [cit], the vmm itself includes the management functions and the system administrators perform vm management by sending commands directly to the vmm. unlike vmware esx, the management vm called the service console is not used by default. lacking the service console makes it difficult to steal information from the user vms' memory. however, this architecture lowers the flexibility of the vm management. it is not easy for the administrators to use their own custom management software."
"gemtoe, gemapproach, gemworkloadevent can be used to annotate either a complex or a very simple model of a corresponding constituent. the level of detail does not play a significant role for the gem profile. nevertheless, the richer these models are the more informed decisions can be made by an embedded system engineer when selecting rbbs. all parameters and metrics types (toeparameter, appproachparameter, workloadparameter, domainmetrics, and resourcemetrics) are defined as uml data types the attributes of which can be of the marte nfp types or other basic types (not shown in fig. 2 )."
class medicationdosage is used to protocol the medication during an emergency process. for every new medication a new object of the class should be instantiated. attributes of such newly created object must be filled after the creationactivity has been finished. this can be expressed by an ocl invariant similar to the one presented in the following.
"the two views in a dual memory view are provided concurrently to enable live migration. in other words, an encrypted view coexists with a normal one for each user vm. therefore, the management vm can access the memory of a running user vm. live migration requires that the management vm transfers the memory of a user vm while the user vm accesses its memory. if vmcrypt directly encrypted the memory of the user vm by overwriting it, the running software in the user vm would fail when reading encrypted data."
"vmcrypt increases the size of the vmm, resulting in a larger tcb. this may make the whole system more vulnerable, but the increased code size is 12500 lines, including 6500 lines of code for aes. this size is only 5 % of the original vmm."
the intuition is that env. conditions of a platform component a adopted for an embedded system and a component b required by an rbb are compatible if corresponding interval values of env. conditions of the same type are overlapping. these components are also compatible if there are no common types of conditions that are defined for both components.
cppns have several properties desirable for generating robot morphologies. it is directly evident that geometry is a key aspect of any artifact existing in a physical or simulated physical environment. providing the evolutionary process with information about this geometry should be useful in evolving functional structures. additionally the ability to operate at multiple resolutions should allow for the rapid evolution of coarse grained structures composed of a small number of large components followed by re-querying the generating cppn to produce qualitatively similar structures composed of a greater number of smaller components without needing to evolve cppns for these higher resolution morphologies from scratch.
"ontologies allow querying of the declared knowledge by means of ontology reasoners with the help of the sparql query-ing language 4 . sparql 1.1 is a standard query language to execute data queries on top of owl. it supports yes/noquestions (the ask query), a selection which matches a desired pattern (the select query), filtering (the filter modifier), sorting (the order modifier), etc. to execute the sparql queries, one can load an ontology into the protégé tool and use its sparql plugin 5 . we use java apis provided by a widely accepted jena 6 framework."
"gemevaluation is a central concept of the gem profile. it relates four sub-concepts that indicate constituents of any evaluation procedure. first, gemtoe (target of evaluation) refers to an rbb (gemtoe rbb ) or its functions (gemtoe function) that are under performance evaluation. a gemtoe can be characterised by a set of parameters introduced by the toeparam tag (i.e. a property of the stereotype), e.g. a key size and cipher mode for an encryption rbb. any toe executes on some evaluation platforms reflected by the gemplatform stereotype. we use a uml class model annotated with stereotypes from the hrm marte package to describe this platform. an execution platform can describe resources that take a variety of forms, e.g. hardware, software, or logical resources. in this presentation, we consider only hardware components. however, the general concept is scalable to include other forms of resources for analysis. the gemrequiredcomponent stereotype is used to introduce those components of an evaluation platform that are significant for a considered toe to obtain the captured performance results. for example, it can be a particular instruction set exploited by a toe implementation."
"as an optimization, the vmm decrypts memory pages as early as possible to reduce the downtime of live migration. if all pages are decrypted at the final stage, the downtime becomes long because domain u at the source host is stopped at this stage. the vmm extracts the embedded bitmap just after domain 0 receives all the pages used for the bitmap. after that, the vmm can decrypt memory pages on the basis of the restored encrypted bitmap. however, the encryption bitmap may not be consistent because it can be updated during live migration. for example, pages to be encrypted may be mapped without encryption before the encryption bitmap is correctly updated. this may lead to information leakage from domain u. to prevent this inconsistency, the vmm maintains the decryption record, which is used to record whether each page of domain u has been decrypted or not, as shown in figure 5 . its bit is set when the corresponding page is decrypted. when domain 0 unmaps a page of domain u, the vmm sets the corresponding bit of the decryption record if the vmm decrypts it on the basis of the encryption bitmap. when domain 0 maps the page later, the vmm determines whether the page should be encrypted or not, using the decryption record instead of the encryption bitmap. with the decryption record, the vmm can give a consistent memory view to domain 0. it is guaranteed that decrypted pages are necessarily encrypted when domain 0 maps them. at the final stage, the vmm adjusts the encryption of all the pages on the basis of the consistent encryption bitmap. note that unencrypted pages may be incorrectly decrypted but they can be restored by encryption in case of aes-xts at least."
"we focus on microcontrollers (mcus) provided by some of the most common vendors (renesas, texas instruments, fujitsu, atmel, and microchip technology). we estimate potential complexity of corresponding marte models and the size of corresponding ontologies as an amount of generated axioms. three classes of embedded systems and mcus commonly used for their design [cit] are considered: small scale (8-bit mcus), medium scale (16-bit mcus), and sophisticated systems (32-bit and arm-based mcus). then, we study how many models are currently available on the market for each vendor (see table 2 ). the data has been extracted from the official internet resources of the vendors. to estimate the potential number of generated axioms, we select five commonly used mcus of each class, create their marte models, and execute their transformations. this study shows that the simplest 8-bit mcus consists on aver-age of 68 axioms, while the most sophisticated 32-bit mcus generate 133 axioms (see table 2, row 7). row 8 shows the number of produced axioms when all models are added into ontologies. finally, we compare these numbers with scalability studies of the owl apis and jena technologies [cit] . in particular, horridge and bechhofer [cit] show that owl apis can easily handle ontologies that contain 1 651 533 axioms consuming 831 mb. as a result, we conclude that the used technologies (owl apis and marte) allow handling ontologies for a significant number of vendors in a potential real world deployment. this capacity allows loading multiple vendors' ontologies to execute compatibility analysis. additionally, some techniques for swapping ontologies in memory can be implemented to handle even bigger datasets."
the use of performance analysis at the early design phase is a subject of active research. [cit] apply an approach for performance analysis of rbbs represented as aspects. [cit] present a toolkit for performance evaluation where rbbs conform to a specific component model. these methods use models as a means to input required data into performance tools. our work complements these approaches since it enables reuse of outcomes of the rbbs performance evaluation conducted by efps domain experts.
"in this implementation, normal and encrypted views are synchronized only on memory mapping and unmapping by domain 0 for efficiency. we call this lazy synchronization. if domain u is stopped, the encrypted view is the latest because domain u does not modify the normal view. however, the encrypted view may become obsolete if domain u is running. for example, live migration is performed without stopping domain u. this inconsistency between memory views is usually acceptable because domain 0 cannot access domain u's memory consistently even when the memory pages are shared between domain 0 and domain u, as traditionally performed. management software in domain 0 should already consider this."
"we measured the impact of vmcrypt on the performance of domain u. vmcrypt does not perform encryption or decryption unless domain 0 maps or unmaps pages of domain u, but the vmm checks several operations even when domain u modifies page tables. we ran two benchmarks, lmbench [cit] and unixbench [cit], on domain u in the vanilla xen and vmcrypt. figure 9 shows the performance degradation in lmbench due to vmcrypt. context switching in vmcrypt was 1.8 % slower than in the vanilla xen, but the overall average of performance degradation in vmcrypt was 0.7 %. for unixbench, the overall average of performance degradation in vmcrypt was also 0.7 %."
"the organizational aspects are also tested during the workflow execution. the assl allocation procedure which is derived from the design time allocation model of figure 11 (b) tries to find an idle person for executing the activity. if someone is found, then the person is assigned to the activity by the allocation link. if none is found the procedure stops with no result. an invariant states that no person can execute two activities in parallel. this should apply for most situations in real life. nevertheless, the metamodel could be extended to allow activities that can be executed by one person in parallel with certain other activities. this is not considered in the current metamodel."
"were correct then it would be intuitive how to design such structures. however, such body plans are non-intuitive to design because they have subtle dependencies on mass distribution and structural curvatures, making automated methods including evolutionary algorithms good candidates for designing these artifacts."
"unlike disks and networks, memory is crucial because it is difficult for the guest operating systems to encrypt their own memory. without hardware support, neither operating systems nor applications can run with encrypted memory. there are various types of sensitive information in memory [cit] . clear-text passwords and cryptographic keys can be extracted from the user vms' memory. such information resides in buffers in the kernel or processes temporarily. in addition, the memory usually contains the buffer cache, which is used for maintaining copies of the data of file systems in memory. through the analysis of the buffer cache, the attackers can read the data blocks of specific files if the blocks are on the buffer cache. the encryption of the file systems in the user vms is not sufficient because the buffer cache cannot be encrypted."
"next, we attempted the verification of the measurement by the tc, but openpts [cit] did not work in our experimental environment. according to the literature [cit], the verification process takes about one second. the time should depend on the network performance."
task tree models that represent hierarchy oriented workflow models can be tested and executed within the modeling environment ctte [cit] . this is similar to the method presented here. but ctte provides less integration of data and organizational aspects and does not allow workflow models to be adaptively changed at runtime.
"we measured the time needed for migrating domain u between two hosts. we changed the allocated memory size of domain u from 26 mb to 4 gb. the results are shown in figure 7 (a). the migration time was proportional to the memory size of migrated domain u. the overhead of vmcrypt with the null cipher was only 1 %. with aes, however, the migration time was 1.7 times longer than that in the vanilla xen. these results mean that encryption and decryption degraded the performance largely. note that the performance in vmcrypt with aes is almost the same as that in the vanilla xen with ssl. the reason why the vanilla xen with ssl is slightly slower than vmcrypt with aes is that ssl checks the data integrity as well."
"so far, we have used the smart metering network to illustrate the compatibility analysis and knowledge management ideas supported by our methods and tool. this section proceeds to show that our approach is scalable to domains with large data sets. we design experiments to estimate the size of resulting vendor ontologies as well as the execution time for the transformation of marte models into an ontology."
class diagramdataflowobjects are integrated in the workflow metamodel with inheritance and specially named attributes links to the workflow model like pictured in figure 7 runtime
"the seq association is used in several other contexts, for example with the flowoperators shown as subclasses in figure 1 . the class and is used to express wcp2 as demonstrated in figure 3 . an ocl invariant of the class and defines that all pred objects have to be in the state done or skipped if one of the succ activities is allowed to be in the state running. an ocl invariant for static design time properties is defined which ensures all objects connected with a flowoperator to either be activities or groups. thus flowoperator cannot be connected to flowoperators by the seq association. a similar static property is needed with the wcp8 in which all pred objects have to be activities or groups but not flow operators. the expression of that pattern in a workflow model is shown in figure 5 ."
"we performed experiments to measure the overheads of vmcrypt and confirm that vmcrypt prevents information leakage via domain 0. we used two pcs, each of which has one intel xeon processor 2.67 ghz with 8 cores, 12 gb of memory, a 1 tb of sata hdd, and a gigabit ethernet nic. we ran modified xen 4.0.1 for the x86-64 architecture on these pcs. for domain 0, we allocated 6 gb of memory and ran linux 2.6.32-5-xen-amd64. for domain u, we allocated 1 gb of memory if not specified in each experiment and ran para-virtualized linux 2.6.32.27. these pcs were connected with a gigabit ethernet switch."
"therefore, the confidentiality of the user vms' memory should be preserved while the iaas administrators perform vm management. a simple solution to prevent information leakage is to disable the management functions of the management vm, but this would not be acceptable in iaas. at least, live migration is indispensable for the reasons of load balancing and power saving with negligible downtime. in particular, it is challenging to support para-virtualized guest operating systems because the management vm has to inspect and modify the user vms' memory. although full virtualization is being used, para-virtualization is still useful in terms of efficiency. in addition, any modification to the existing management software should not be required because it is not realistic to modify various management software."
"next, we examined how live migration in vmcrypt affected the performance of a web server running in domain u. we ran the lighttpd web server [cit] in domain u and measured its throughput with apachebench [cit] in a client host. the client host had one intel core 2 quad processor 2.83 ghz, 8 gb of memory, and a gigabit ethernet nic. the server and client hosts were connected with a gigabit ethernet switch. figure 10 shows that the throughput in vmcrypt with aes was higher than that in the vanilla xen. this is because live migration in the vanilla xen occupies the network bandwidth and the web server cannot use the network sufficiently. however, the throughput in vmcrypt with aes is lower than that in the vanilla xen with ssl. in vmcrypt, the c ⃝ 2012 information processing society of japan sender transfers 4 mb of the memory image at once and causes bursty network traffic intermittently. this bursty traffic affects the throughput of the web server. with ssl, in contrast, the traffic is not bursty because the sender slowly transfers the data while encrypting."
"in further work, we will enhance our compatibility technique considering other marte packages (e.g. those to model the software components). additionally, we will explore other criteria and strategies to reuse the rbbs performance analysis results, validating them on smart metering devices and other case studies given by industrial partners of the secfutur project [cit] ."
"the definition of environmental compatibility is built upon the env condition type from the hrm package (see figure 14 .72 in marte [cit] ) which defines five condition types: temperature, humidity, vibration, shock, and altitude. environmental (env.) condition of each type has a value range. an engineer needs to annotate the components with the \"hwcomponent\" stereotype and define the \"r conditions\" 13 www.ti.com/tool/c64xpluscrypto tag to assign env. conditions to a component. we use the following terms and functions to define environmental compatibility:"
"thus, gem provides a means for a domain expert to capture the results of rbb performance evaluation in order to support embedded system engineers in selecting a suitable rbb, and to enable automated analysis of the captured data."
resource management is another important aspect. in our approach resources are allocated during the workflow execution. the shortness of particular resources may be identified in the system animation.
"values for name and age have already been entered during the execution of the abulancedelivery activity, as was assured by the data integrity constraint presented in subsection iii b. the attribute disease is set to the corresponding disease2 object. moreover, the urgency attribute is entered. the checkpatientcondition activity is a decision activity. the criterion has to be selected together with the corresponding guard. this is done by the user during the execution of the decision activity within the window shown beneath the use object properties view in figure 13 . decision modeling is similar to epcs, where there is the rule that only activities have the competence to make decisions [cit] . uml activity diagrams or yawl handle decision modeling slightly different. in these languages the choice operators are data driven and automatically executed by the (workflow) system."
"ontologies of the expert layer are maintained by experts of the embedded and extra-functional domains. it contains three ontologies. the first two, i.e. nfptype ontology and resource ontology, are obtained as transformation of marte packages dedicated for resource modelling. the third ontology is the refinement of the core evaluation ontology for the security domain already explained in sect. 4. 8 magicdraw is adopted as an integrating environment within the european secfutur project [cit] ."
"vmcrypt prevents the sensitive information of the user vms' memory from leaking via the management vm, using the trusted vmm. nevertheless, the iaas administrators can manage the user vms with the management vm, including live migration, as before vmcrypt is introduced. vmcrypt supports para-virtualized guest operating systems and allows the iaas administrators to use the existing management software."
"to enable the iaas administrators to manage the user vms with para-virtualization, vmcrypt exceptionally provides a normal view to the management vm only for several memory regions. for such regions, the management vm can directly access unencrypted data in a user vm. examples of such regions are memory shared between vms and the page tables inside a user vm. for para-virtualized guest operating systems, the management vm needs to read and write data in the shared memory to exchange information with a user vm. the page tables have to be modified by the management vm during vm migration. the details of such unencrypted memory regions are listed in section 4.4."
data and workflow modeling uses several different diagram types in this approach. data modeling employs uml class diagrams while workflow modeling uses uml object diagrams to express the abstract syntax of the workflow language. table ii gives an overview on modeling concepts and their associated diagram types.
the rest of this paper is organized as follows. section 2 clarifies administrator-related information leakage in iaas clouds and discusses related work. section 3 presents vmcrypt to prevent information leakage from the user vms. section 4 describes the implementation details in xen. section 5 shows our experimental results for vm management with vmcrypt. section 6 concludes the paper.
"in this section the workflow runtime plugin is presented which provides an environment for workflow instantiation and execution. the plugin presents the workflow instance in an appropriate way to the user so that she can interact with activities work items and related data in an appropriate way. validation of dynamic control flow properties and related data integration are conducted here. furthermore, organizational resource aspects can be tested and process mining can be done on the executed processes."
"regarding the layout of the models, it is obvious that integrated workflow models are hard to read, because of the number of objects and links that might inevitably tend to overlap. this is not solely a problem of workflow models in use though its abstract syntax might amplify the effect. but use offers support by providing a filter mechanism for displaying the desired aspects of a model [cit] . thus, the designer can inquire relevant model elements exclusively for the control flow, organizational or data aspects of the workflow model."
"restricting the process execution by adding execution constraints can cause problems, too. but they are recognized by the use tool as constraint violations. ocl invariants are permanently checked by use so that constraint violations are instantly displayed by the ocl invariant view [cit] . anyway, the workflow runtime plugin will recognize any changes in the object diagram and it will instantly update its view on the workflow instance model."
"however, the iaas administrators are not always trustworthy from iaas users' point of view [cit] . it is not rare that one cloud consists of several data centers around the world. the users often cannot know which data centers their vms run in because the vms may be migrated between data centers. the users cannot exactly know who are the administrators of their vms. if a lazy administrator manages virtualized systems in iaas, some management vms may have vulnerabilities in software or configurations. in this case, vulnerable management vms may be penetrated by outside attackers. if an administrator cannot fix those vulnerabilities for some reasons such as no patches, he is considered lazy as a result. if an iaas administrator himself is malicious, he can perform insider attacks with high privileges. we refer to both outside attackers and malicious iaas administrators simply as attackers."
"the marte foundations define a set of concepts required to model non-functional properties (nfps in particular, the generic quantitative analysis modeling (gqam) package defines a set of general components while its extensions refine it to support schedulability and performance analysis. while these extensions facilitate the application of analysis techniques to embedded systems, there are no extensions for storing alternative analysis results for their later reuse, whereas our profile is designed to provide these facilities."
"vmcrypt allows domain 0 to inspect domain u's memory pages necessary for migration, such as the shared info page, the p2m table, and the page tables. such shared pages are not encrypted while the other pages are encrypted by the vmm when domain 0 maps them. thanks to a dual memory view provided by vmcrypt, domain 0 can concurrently access encrypted pages while domain u accesses the original pages. when domain 0 unmaps pages, the vmm does not decrypt them for performance c ⃝ 2012 information processing society of japan because any pages are not modified by domain 0."
"for boot, domain 0 can construct domain u as usual because the vmm does not encrypt any memory pages. it loads the kernel to domain u's memory and sets up the initial page table, the start info page, and so on. when domain u starts running, the vmm checks the integrity of domain u's kernel and the other data structure, as described in previous work [cit] . in addition, vmcrypt supports the suspension and resumption of domain u, which are similar to operations at the source and destination hosts in vm migration, respectively. c ⃝ 2012 information processing society of japan"
it is also possible to test and compare different organizational models and configurations. such comparison can be achieved by instantiating multiple workflows which are then tested in varying contexts.
one pair of ring buffers is used to achieve the console of domain u. domain 0 needs to read text outputs from and write key inputs to the console. the other pair is used for domain u to access a storage system called xenstore in domain 0. domain 0 and domain u exchange information on vm configurations through xenstore.
"more work will be needed to extend the growth procedure to allow for the inclusion of arbitrary numbers of sensors on each cell, but the current results along with previous experiments using cppns suggest that additional cppn outputs and/or input flags will provide the necessary mechanisms for extending the current framework in that direction. additionally, since the hyperneat variant of cppn-neat has shown success in the evolution of anns it is reasonable to expect that co-evolving neural network control policies along with the morphology should be possible. the authors intend to tackle these problems in future work."
note that not all operations that change object states have to be assigned to its class. for example the class iterationgroup can initiate another iteration with the operation nextiteration(). this resets all included activities to the state waiting and stores the execution data of the last iteration to a new group linked via the association archive. class activity itself does not directly provide an accessor for resetting its instances' state.
"in this situation, sensitive information in user vms can leak via the management vm. using the abilities of the management vm, the attackers inside it can steal the whole contents of the memory, disks, and networks of the user vms on the same physical machine. for example, the attackers can dump the whole memory of the user vms into their disks. they can mount the disks of the user vms and read all the files. network sniffers running in the management vm can easily capture packets to/from the user vms. fortunately, disks and networks could be encrypted by the guest operating systems themselves in the user vms to protect their contents. the user vms can use encrypted file systems like windows efs and virtual private networks (vpns)."
"the attacks against the embedded bitmap cannot succeed in information leakage. corruption of the bitmap is detected by the hash value embedded into domain u's memory. replay attacks are useless because domain 0 is always given the same memory view as that at the source host. even if a page is decrypted using an old encryption bitmap, it is necessarily encrypted again on the basis of the same bitmap when domain 0 maps it."
"the nfptype ontology of the expert layer contains a set of types and their relations needed to characterise a hardware/software component, e.g. data rate (mbps, kbps, etc.) and frequency (hz, khz, etc.). this ontology is derived from the measurementunits, marte datatypes, and basic nfp types sub-packages of marte that enable specification of non-functional properties (see chapter d.2 in marte [cit] ). the resource ontology contains concepts needed to describe platform components. it is derived from the marte hrm package described in chapter 14.2 [cit] . both ontologies can be accessed on our web page 9 ."
"in this paper, we focus on the attempts to steal sensitive information from the user vms' memory. information leakage from other resources such as cpu registers, storage, networks, and covert channels is out of scope. in real iaas clouds, these resources should be protected against the management vm as well. since there are several studies for protecting them, we can incorporate those with vmcrypt. for example, the protection of cpu registers has been proposed in sre [cit] . storage and networks can be encrypted by the guest operating systems themselves, as described in section 2, or by the vmm [cit] . mitigating the risk of covert channels is discussed in the literature [cit] . also, we do not consider the other types of attacks against the user vms from the management vm, such as integrity attacks and denialof-service attacks."
"first, we measured the time needed for building domain u. the time we measured was from when we started creating domain u until the vmm completed the unpause hypercall. the overhead due to vmcrypt was only 1 % when 4 gb of memory was allocated. this is because the vmm does not encrypt or decrypt at boot time."
"the workflow instances can be analyzed and adaptively changed at runtime. the same applies to the snapshots of the organizational and data model. time aspects are captured by the workflow plugin and are stored during workflow execution. thereafter, this data can be analyzed in a log window of the workflow plugin, by a uml sequence diagram or by ocl mining queries. so, important properties of the executed process instances can be discovered."
"this work is a first step in a research trajectory that aims to co-evolve articulated body plans and control policies. the results presented here are promising in this endeavor in that cppn-neat is able to find non-intuitive solutions that capture the powerful yet subtle relationship between physical structure and function. adding in articulation will involve extending the growth procedure presented here to additionally query evolved cppns for cell connectivity information. for example when adding a new cell to the morphology instead of always doing so in a rigid manner the cppn can be queried with information regarding the two cells' geometric positions to produce another value used to determine whether the cells should be connected rigidly or with a rotational joint, and if they are to be connected with a joint, properties of this joint may also be determined from the cppn output."
"compared to a petri net-like modeling language, our work is an alternative possibility to express the workflow patterns in a declarative way with a metamodel based modeling approach using uml and ocl. in our view, this is a new direction in the context of workflow languages. there are several graph-or block oriented languages like uml activity diagrams [cit] or bpel [cit] that are checked against the workflow patterns, but no language uses a declarative foundation to express them and to check declaratively formulated properties."
as described in subsection iii b the workflow plugin is used to visualize the execution of the workflow models. in the scenario of figure 13 the process model of figure 10 is interpreted. the activity checkpatientcondition is started and correlated data is presented to the user within the workflow plugin. the use class extent view is shown on top. it lists all the queried data objects and their attribute values. with the getalldiseases:dataread object in the workflow model of figure 10 all the stored diseases are queried. three were found and are listed in that data window.
"finally, gemmetrics defines a set of metrics adopted for the performance evaluation. these metrics can be of two categories. the first category (resourcemetrics) describes the resource footprint created by toe, e.g. execution time. the second category (domainmetrics) refers to the obtained indicators that characterise the quality of efps, e.g. the security level provided by a toe. the presence of these two categories reflects the fact that for selection of suitable rbbs both resource footprint and quality of service indices play a significant role."
"the cppn takes as input the cartesian coordinates (x, y, z) of the point in question as well as a constant bias input. additionally information is provided as input about the growth trajectory itself: two angles φ1 and θ1 describe the direction from the parent cell that this point is located and an additional two angles (φ2 and θ2) provide directional information about the parent cell relative to its own parent cell. further knowledge of the growth trajectory is provided via an input that receives the number of cells in the growth tree separating this point from the root (the cell's depth, d). additional input is provided by a value representing the radius of the cells being considered for addition (r) thus informing the cppn about the resolution at which the structure is being grown. resolution will be dependent on the environment in which the physical structure exists, therefore this value may be considered an environmental input."
"dataflow objects are connected via links of the association flow to activities that need the corresponding data object. the integrated model is pictured in figure 10 . emergencyprocess is linked with an activity and a group of activities. adjustmedication is independent to any other process fragments. remaining activities are correlated to the process by calculating the transitive closure as described in subsection ii a. after delivery is done, the patient is checked. this is expressed with the seq link between these objects. depending on the decision made in checkpatientcondition either the normalsurgery or emergencysurgery take place. patientdata is a central data flow object in the model. it is related to the delivery activities and checkpatient. that activity has a dataread object which inquires all diseases from the database of the information system to assist the doctor finding the correct diagnosis. this database is represented by data objects in the uml object diagram. the adjustmedication activity is an iteration that creates a new medicationdosage object with a datacreate object. additionally, past medicationdosage objects, created by the same activity, are inquired by the dataread object. past medications are presented to the user so that she can avoid medication overdoses. the following ocl invariant expresses a data integrity constraint in combination with workflow data. it states: if the transportation of the patient is done the name and age of the patient must be filled."
"evolutionary robotics [cit], in which evolutionary algorithms are employed to optimize the control policy of a robot, has provided one framework for overcoming the limitations of human intuition in designing robust, non-linear, control strategies. however, the majority of work in evolutionary robotics has done only that: optimize control strategy for a human designed or bio-mimicked robot morphology. this methodology has severe limitations: fixing a robot's morphology places limits and biases on the kinds of action that the robot can perform, and therefore also on the more complex behaviors that those actions may eventually support. for example, a robot with legs can only exhibit legged locomotion while a wheeled robot with a rigid gripper can only move over even terrain and grasp objects with a fixed radius."
"humans are much better at designing physical systems than they are at designing intelligent control systems: complex powered machinery has been in existence for over 150 years, whereas it is safe to say that no truly intelligent autonomous machine has ever been built by a human (p. 22) [cit] ."
another possibility to integrate data to workflow models is by using the dataflowobject and the flow association. in the data model an inheritance relationship expresses that objects of a special data class are used to be passed from one activity to another.
"there the examples are domain 0 in xen [cit] and the service console in vmware esx [cit] . with the management vms, the iaas administrators can access the memory, disks, and networks of user vms."
"also, the vmm embeds the hash value of the page tables into domain u's memory to detect the alteration during migration. for example, if restored domain u uses altered page tables, the domain u might store sensitive information to unencrypted pages. when domain u is finally stopped at the source host, the vmm canonicalizes all the entries and calculates its hash value."
"to improve the recognition of unreasonable changes that are not already recognized, some pre-and postconditions can be assigned to another operation adaptivechange() added to the class process of the metamodel of figure 1 . before applying adaptive changes, the operation must be entered in use [cit], then the process instance is modified and afterwards the operation is exited. with ocl the workflow instance state before the adaptive change can be compared with the state after the change so that unreasonable changes can be identified."
"additionally, this work demonstrates that it is possible to improve run time performance without significantly degrading the quality of evolved structures by using a lower resolution at the start of an evolutionary trial followed by increasing this resolution partway through. first evaluating structures at a lower resolution allows the evolutionary process to more quickly search the space of possible solutions before switching to a higher resolution to more further refine the shape and mass distribution of the structures."
the start info page is used for passing information from domain 0 to domain u when domain u starts running. this page contains information necessary for booting a guest operating system such as the allocated memory size. domain 0 needs to set up this page when domain u is booted or restored.
"in sect. 3.1, we make a brief introduction to ontology technologies used in our work. next, the marte profile used for platform modelling is described in sect. 3.2."
"as an optimization for memory decryption, the vmm does not decrypt replicated pages when domain u's pages are mapped on domain 0 in a read-only manner (fig. 3 (b) ). since domain 0 cannot modify read-only pages, the contents do not need to be written back to the original pages in domain u. when domain 0 unmaps such pages, the vmm simply releases them. this optimization is enabled by a dual memory view that concurrently exists because unencrypted contents are still preserved in the original pages. this reduces the overhead of memory decryption. for example, all of the domain u's pages are just read on suspending domain u."
"in para-virtualized operating systems, a page table is a table for translating virtual addresses into mfns, not pfns. when domain 0 migrates domain u, it needs to rewrite all the ptes in the page tables of the domain u, using the mfns at the destination host. the vmm identifies the pages used for the page tables in domain u through a hypercall. the pages are always registered to the vmm. if a page is no longer used for a page table, it is unregistered from the vmm. the attackers cannot register arbitrary pages as page tables because the vmm checks the validity of all the ptes. in addition, the vmm disallows domain 0 to modify the page tables after domain u starts running."
"in the model of figure 11 (b) the activities are assigned to corresponding roles. for example checkpatientcondition is executed by a surgeon. adjustmedication is assigned to medicinedepartment. the allocation strategy is indicated by the anyperson object assigned to that link. thus, any person of that department can execute that task."
"an ontology represents knowledge in a particular domain as a set of concepts and their relations [cit] . this knowledge is formalised as a logic-based system and described by a knowledge representation language. in particular, we use the web ontology language (owl2) [cit] which is a commonly used language for creation of large ontologies."
expressed with cancelprocess -all waiting or running activities will be canceled if operation cancel() is invoked. the further execution of the process is not possible.
we define a profile for capturing performance evaluation results called generic evaluation model (gem). this profile is depicted in fig. 2 and explained in this section.
"in order to provide the description of available components, a vendor needs to perform the following steps. first, a vendor uses the magicdraw tool 10 to create models of platform components. these models are uml class models annotated with stereotypes from the hrm package. second, a vendor launches the liu magicdraw plugin to transform the created 9 www.ida.liu.se/~marva/ontologies/ 10 www.magicdraw.com marte models into an ontology. we use the java owl apis 11 and acceleo 12 tools to realise this transformation."
"in live migration, domain 0 transfers the memory image of running domain u from a source host to a destination host. vmcrypt encrypts the memory contents while it allows domain 0 to access necessary information in domain u's memory."
"we exploit the marte [cit] profile and ontology technologies to support (a) extra-functional domain experts in capturing the results of performance analysis for rbbs and (b) embedded system engineers in selecting the right set of rbbs based on automated analysis of the resource constraints. for this purpose, we use model transformation techniques to bridge the marte profile and specific ontologies. these contributions are detailed in this paper as follows:"
"toeparam cipher says that a cipher mechanism can be characterised by its key size and cipher mode. quality of service metrics for a cipher (see dm cipher ) are resistance to attacker's capabilities in terms of skill, motivation, and duration of the attack. finally, a pair of resource metrics defined for this rbb are used memory and data rate (see rm cipher )."
"there are several studies for preventing information leakage via the management vm. cloudvisor [cit] runs a security monitor underneath the vmm using nested virtualization. it encrypts all the memory pages of the user vms whenever the management vm accesses them. therefore, it is difficult to support para-virtualization because cloudvisor does not allow the management vm to access pages necessary for vm management in user vms, e.g., the p2m table in xen. the security monitor cannot recognize such pages by hardware events such as vm exit. although cloudvisor supports vm migration for full virtualization, there are no performance data. in addition, cloudvisor introduces extra overheads to user vms due to an additional virtualization layer. it does not trust even the vmm, but we believe that we can also trust the vmm if we can trust the security monitor via remote attestation."
"to construct an encrypted view from a normal view of domain u's memory, the vmm encrypts the contents of memory pages of domain u when domain 0 maps those pages. domain 0 has c ⃝ 2012 information processing society of japan to map memory pages on its address space to access domain u's memory. to make two memory views coexist, the vmm replicates pages of domain u and maps them on domain 0, as illustrated in fig. 3 (a) . for this encrypted replication, the vmm allocates pages in domain 0 and copies encrypted contents to them. when domain 0 unmaps domain u's pages, the vmm decrypts the contents, writes them back to the original pages in domain u, and releases the pages allocated in domain 0."
the vmm automatically identifies the unencrypted pages to exceptionally deal with them. figure 4 shows the data that domain 0 needs to access. this identification is specific to the paravirtualization in xen.
"through the experiments, vmcrypt used aes-xts with a key size of 256 bits for the encryption of domain u's memory. we have implemented the aes-xts support in the vmm but not yet fully optimized. the performance can be improved by using aes-ni, a set of special instructions for aes. to exclude the overhead of cryptographic operations, we also used the null cipher, which did not encrypt or decrypt data. for comparison, we conducted the experiments in the vanilla xen."
"however, there are ways to overcome these limitations. evolutionary algorithms may be used to optimize robot morphology as well as the control policy. sims [cit] first introduced an evolutionary framework in which both the morphology and control of simulated machines were optimized in virtual environments to produce adaptive behavior. this work was followed by other studies [cit] in which aspects of the machine's morphology and control were evolved in virtual environments. this approach has the advantage of discovering body plans appropriate for the machine's task environment rather than being artifacts of human design biases or copies of animal body plans only appropriate for that animal's ecological niche."
"this paper demonstrates that cppn-neat is capable of evolving three dimensional physical structures with nontrivial dynamical properties. moreover a case has been made for why including additional inputs which recursively provide the cppn with information about the growth trajectory and environment (those in the full set) is beneficial when using cppn-neat to evolve such structures. specifically, structures evolved using these inputs on average perform either equivalently or significantly better when compared with those grown from cppns that are limited to the basic cartesian inputs in the restricted set. moreover including these inputs results in cppns that are either as a robust or more so to changes in growth resolution relative to cppns taking only the restricted set of inputs."
"second, gemapproach denotes the description of the used evaluation approach. the kind tag defines the type of the evaluation method, e.g. simulation, emulation, or analytical analysis. these types are represented as a uml enumeration, i.e. approachkind. an approach can be parameterised using the approachparam tag."
"before the workflow is executed in the workflow plugin, the snapshot of the data model should be prepared in the use object diagram. in the example workflow of this paper the diseases that should be stored in the hospital information system are created before the process simulation starts."
"during or after the execution of a workflow, its instance data can be evaluated. there are several ways to analyze this data. first of all, the workflow plugin itself is logging the instructions taken from the user. it builds a list in which the call events like start, finish, skip, cancel and fail are assigned with timestamps and the activity."
"at the destination host, domain 0 creates a new domain u and reconstructs its memory using the received memory image. when it receives a memory page, it allocates a new page for domain u, maps it, and writes the contents to it. when domain 0 receives pages used for page tables, it uncanonicalizes the ptes of the tables, according to the memory allocation at the destination host. specifically, it replaces pfns in the ptes with the corresponding mfns. domain 0 repeats this as long as memory pages are transferred from the source host. when domain 0 has received all data from the source, it sets up the start info and shared info pages. then it uncanonicalizes the p2m table and finally starts domain u."
"cloud computing, providing services via the internet, is being widely accepted. among various types of services, infrastructure as a service (iaas) like amazon ec2 [cit] provides users with virtual machines (vms), which are hosted in data centers. the users can migrate hardware, software, and data that they have possessed to iaas clouds and use the necessary amount of vms on demand. in iaas, the users manage their vms from remote sites via the internet while the system administrators in iaas manage the user vms using privileged vms called the management vm. for example, they may migrate user vms to other hosts in clouds when necessary for load balancing or power saving."
"to cache unencrypted pages that have been identified once, the vmm maintains the encryption bitmap, as shown in figure 4 . each bit corresponds to each machine page frame consecutively and all the bits are set at first. the bit is cleared if the corresponding page is identified as unencrypted. for the attempt to map domain u's pages, the vmm determines which view it provides to domain 0 by referring to this bitmap. the encryption bitmap is necessary because the vmm cannot always determine whether a specified page should be encrypted or not when the page is mapped on domain 0. the vmm identifies unencrypted pages at the appropriate times as described in section 4.4 and constructs the encryption bitmap."
"according to proponents of embodied artificial intelligence such intelligent behavior arises out of the coupled dynamics between an agent's body, brain and environment [cit] . one extension of this concept is that the complexity of an agent's controller and morphology must match the complexity of the task or tasks that it is required to perform. however, when extending this idea to more complex agents in more complex environments it is not clear how to distribute responsibility for different behaviors across the agent's controller and morphology. for example, if all a robot needs to do is follow a light source over flat terrain wheels and a direct sensory motor mapping would be an appropriate solution [cit], but if the robot must be able to navigate over a variety of terrains and perform more complicated tasks a more complex control strategy and/or morphology are required. this issue of scaling has been one of the major obstacles in developing robots capable of robust and adaptive behavior in unstructured environments."
"to allow only the trusted vmm to be run, vmcrypt performs remote attestation with a trusted server called the trusted coordinator (tc). the tc runs in a trusted third party outside iaas clouds. remote attestation certifies the authenticity of the vmm by tamper-resistant hardware such as the trusted platform module (tpm) [cit] . it measures the vmm by calculating its hash value, sends the signed measurement to the tc, and verifies its integrity. according to our threat model, trusted senior administrators in iaas configure the vmm correctly for remote attestation. average iaas administrators that may not be trusted cannot bypass this process."
"the shared info page is used for sharing information among the vmm, domain u, and domain 0. through this page, the vmm notifies domain u of virtual cpu interrupts, and so on. domain u passes the p2m table to domain 0. the vmm can easily identify the page because the vmm itself allocates the page when it creates domain u. table the p2m table is a mapping table from pfns to mfns. the p2m table has a tree structure to allow machine page frames to be sparsely allocated to domain u. domain 0 needs to access this table to obtain all the mfns allocated to domain u when it migrates domain u. the vmm identifies the top page of the p2m table from the shared info page. it traverses the p2m table from the top node and recognizes all the pages used for the table. to prevent the attackers from making arbitrary pages be a part of the p2m table, the vmm checks the data structure and the validity of all the entries at this time. note that the vmm has to perform this traversal whenever domain 0 maps the shared info page of domain u. this is because the p2m table is constructed by the guest operating system in domain u."
"when domain 0 maps the pages used for the embedded bitmap, the vmm copies the encryption bitmap in the vmm to the pages and encrypts them. if the encryption bitmap changes during live migration, the embedded bitmap has to be re-transferred. the vmm makes the pages for the embedded bitmap dirty so that the migration software in domain 0 re-transfers them automatically. to detect the corruption of the bitmap during migration, the vmm embeds the hash value of the bitmap into the domain u's memory. the bitmap is critical because domain 0 can map random pages as unencrypted by corrupting the bitmap."
the connected dataflow object patientdata1 was interpreted and is shown within the use object properties view. the activity helicopterdelivery has already been executed as can be seen by the black colored dot. this represents that activity to be in the state done. activities get particular colors assigned depending on their execution states. activity states can be changed by the user by clicking on the activity buttons shown on the bottom of the workflow runtime plugin windows.
the bflow [cit] toolbox provides a workflow modeling environment with epcs. static aspects of workflow models are instantly checked at design time similar to the static analysis of process models within our approach (see [cit] ). bflow also uses a metamodel with emf and gmf in combination with eclipse [cit] . the models cannot be executed so that dynamic aspects cannot be validated.
"when all raw data is already processed, the \"input profiles\" that reflect particular time frames (necessary for the creation of the system components models) are created in sections 2.1.2 and 2.1.3."
"for each model we then add the character prefixes and suffixes up to length 3 for each word (+a), brown cluster [cit] for current word (+b), and word shapes (+s). for brown cluster features we used 100 clusters trained on the whole nus sms corpus. the cluster information is then used directly as a feature."
semi-crf has proven effective in chunking tasks. other variants of semi-crf models also exist. [cit] explored the use of higherorder dependencies to improve the performance of semi-crf models on synthetic data and on handwriting recognition. they exploited the sparsity of label sequence in order to make the training efficient.
"future research work will focus on investigating the actual amount of energy shifted (in kwh) on an annual basis, as well as on forecasting amount of flexibility that can be achieved. in order to do so, a price, as well as a specific statistical model for forecasting, should be introduced. this will follow with the aggregation of a group of such kind of buildings."
"the parameter p determines the polarity of edges and takes values either 1 or −1. the edge detection kernel is applied twice with alternating values of p. on the first pass, the boundaries between each pair of adjacent bright and dark"
"the remaining parts of the article are organized as follows. in the next section, we describe our research materials and methods, including retinal oct image data collection, screening edge detectors for comparison by reviewing and analyzing the edge detection techniques used in prior studies on the retinal oct image segmentation, and determining the most relevant performance evaluation metrics. in the third section, we present the comparison of the representative advanced approaches to retinal oct image edge detection against different performance evaluation metrics. finally, we discuss the findings and research opportunities."
"we evaluated the models in the original characterlevel spans and also in the converted word-level spans, to see the impact of the lossy conversion on the scores. in character-level evaluation, the system output is converted back into character boundaries and compared with the original gold standard, while in the word-level evaluation, the system output is compared directly with the gold word spans. for this reason, we anticipate that the scores in word-level evaluation will be higher than in the character-level evaluation. the results are shown in table 3 . the scores for \"gold\" in the character-level evaluation mark the upperbound of word-based models due to the presence of improper nps."
"for the hardware implementation, the present encryption and decryption algorithms were programmed in verilog using the xilinx vivado software. the programmable logic area in netfpga-10g is used for hardware implementation of the present encryption and decryption. the security function consists of encryption and decryption of packets. the former encrypts packets inside the switch, after all other actions have been performed on the packet header and before sending the packet to the output queues. the latter, decrypts incoming encrypted packet before performing other functionalities. both encryption and decryption functions are implemented inside of-data path module. to incorporate the security function in the of-switch architecture, we have modified the following components:"
"the performance of the integrated electrical and thermal systems, flexibility range, as well as the load-shifting potential and hence the ability to reduce peak loads will be evaluated by the following criteria in this paper:"
"where n b is the number of edge pixels in searching neighborhood of a ground truth edge pixel, n i the number of pixels in the ideal edge, and d i the euclidean distance of the current edge pixel in ground truth and edge pixels in searching neighborhood. for the retinal oct images, we limited the searching neighborhood to be within 3 pixels of the true edge along each a-scan."
"the results for fom, tpr, acc, and fpr after the adjustment (table 6 ) are all better than those before the adjustment (table 2 ). this finding is reasonable in that the false alarm reduces, and fom, tpr, and acc increase, when more detected pixels are considered as correct edge. note that in both tables, the higher values for fom, tpr, and acc mean the better performance, whereas for fpr, the lower the value, the better the performance."
"when all raw data is already processed, the \"input profiles\" that reflect particular time frames (necessary for the creation of the system components models) are created in sections 2.1.2 and 2.1.3. the first step \"input data\" is dedicated to obtaining all the necessary data for creating models. in this step sets of raw data are being processed, and all errors eliminated."
"where n i and n a represent the number of ideal and actual detected edge pixels, d i denotes the distance between the ith detected edge pixel and its correct position, and α is the scaling constant (normally set at 1/9) that is applied to provide a relative penalty between smeared edges and isolated, but offset, edges."
"when all raw data is already processed, the \"input profiles\" that reflect particular time frames (necessary for the creation of the system components models) are created in sections 2.1.2 and 2.1.3."
"we then use this dataset to evaluate some models on base np chunking task, where, given a text, the system should return a list of character spans denoting the noun phrases found in the text."
"the edgeflow technique is a novel boundary detection scheme proposed by ma and manjunath [cit] . the technique for boundary detection based on edgeflow utilizes a predictive coding model to characterize the direction of change in color (intensity of grey image) and texture at each image location at a given scale and constructs an edgeflow vector. by propagating the edgeflow vectors, the boundaries can be detected at image locations which encounter two opposite directions of flow in the stable state. differing from intensity-based detection methods that focus on finding the local gradient maximum, edgeflow technique computes the directions of edge energy according to intensity or texture in an image and associated probabilities. the edge energy and corresponding probabilities obtained from different image attributes are pooled together to form a single edge field for boundary detection:"
"in the third step, all necessary power output profiles and state of charge models are created using fundamental mathematical equations and specific methods (sections 2.1.4-2.1.6). this is followed by the control strategy (section 2.2) which is, based on pre-set conditions of the created algorithm and input data, evaluates the ability of the system to respond to the dr signals requested, charge/discharge storages to its maximal/minimal levels, as well as to manage energy import/export to and from the grid."
"this paper presented an fpga implementation of the lightweight present cipher for secure video streaming in a sdn testbed. in this work, the present cipher is implemented in both software and hardware for performance comparison. the fpga implementation proved to be more efficient and faster with fewer resources by taking advantage of verilog coding. this implementation executes one round per clock cycle to support both encryption and decryption at a minimal cost. a video application testbed outcome shows that only a negligible latency is observed for various video quality standards."
"all data sets were selected for the same year [cit] and the same location (nordjylland, denmark). all the following input load profiles were created, and the simulations were executed with a time step of 15 min, and an observation period of one calendar year."
"the energy can then be used by a hydrogen car or converted back into electricity by fuel cells. however, to evaluate the profitability of such a combination, a more comprehensive techno-economic analysis has to be executed."
"in this study, we intended to search for the edge detectors that best suit for the oct retinal image segmentation task. with the analysis of literature and our experiment, we have identified the most promising candidate algorithms, namely, canny edge detector, the two-pass method, and the edgeflow technique. using the performance evaluation metrics (fom, tpr, fpr, and acc) and their adjusted versions (fom adj, tpr adj, fpr adj, and acc adj ), we examined the three methods applied to the realistic oct retinal images. our results show that the two-pass method consistently outperforms the other two. in addition, the mld metrics shows that the two-pass method caused smaller edge shifting problem. although the computational cost for the two-pass method is slightly higher than the canny edge detector, it is over 100 times lower than that for the edgeflow technique. based on the above analysis and findings, we conclude that the two-pass method is among the three the best approach to edge detection for the oct retinal layer image segmentation task. furthermore, the outperformance of two-pass method measured by the original and adjusted metrics and the advantage of canny edge detector over edgeflow technique in terms of fom adj and tpr adj and mld lead to another conclusion that the intensity-based edge detectors outperform the texture-based edge detector for oct retinal image analysis."
"retinal layer thickness measurement relies on accurate oct image segmentation. for many automated segmentation algorithms, edge detection is an essential foundation [cit], notwithstanding some methods resort to other features of the images [cit] . literature shows that diverse types of edge detection algorithms can be employed as a key step in image segmentation. table 1 summarizes the commonly used algorithms for retinal oct image segmentation, including the canny edge detector [cit], two-pass edge detection method [cit], local mean gradient-based edge tracking [cit], peak detection method, gaussian smoothing in combination with the sobel kernel method [cit], and edgeflow technique [cit] . based on the nature of the information used in their algorithms, we can classify these different edge detection techniques into two categories, namely, the intensity-based and texture-based methods. the former category utilizes the intensity gradient in the images, whereas the latter tracks the texture changes rather than the intensity gradient."
"the number of hours for backup heat supply that can be provided by the hwst during the heating season comparing various volume scenarios (0.25, 0.5, and 1 cubic meter)."
"furthermore, the lack of available annotated data for such informal datasets prevents researchers from understanding what effective models can be used to resolve the above issues. in this work, we focus on tackling these issues while making the following two main contributions:"
"to implement this, we need to split the original nodes into begin and end nodes, representing the start and end of a segment. the end nodes connect only to the very next begin nodes of any label, while the begin nodes connect only to the end nodes of same label up to next l words. we denote the set of the earlier edges as e a (x, y) and the latter edges as e j (x, y). the normalization term z w (x) is then:"
"(3) edge location accuracy metrics. it is known that some image processing procedures cause the shift of detected edges ( [cit], chapter 3, p. 56). in order to characterize the extent to which the results from edge detection algorithms deviate from the ground truth, we introduce the location accuracy metrics, the mean localization deviation (mld) in the context of oct image analysis:"
"in weak semi-crf we use the same feature set as semi-crf, adjusting the features accordingly where segment-specific features (1) are defined only in the begin-end edges, and transition features (3) are defined only in the end-begin edges."
"our text corpus comes from the nus sms corpus [cit], containing 55,835 sms messages from university students, mostly in english. [cit] version of the corpus, containing 45,718 messages, as it is more relevant to modern phone models using full keyboard layout. we note that there are a small portion of the messages written in non-english language, such as tamil and chinese. as we are focusing on english, we excluded messages written by non-native english speakers based on the metadata (21.3% of all messages). we also excluded messages which contain only one word (6.1%) and we remove duplicate messages (8.1%). 1 we assigned the remaining 27,700 messages to 64 university students who conduct annotations, each annotating 500 with 100 messages co-annotated by two other annotators. after manual verification we excluded annotations with low quality from 3 students. we used the resulting 26,500 messages as our dataset. the students were asked to annotate the toplevel noun phrases found in each message using the brat rapid annotation tool 2, where they were instructed to highlight character spans to be marked as noun phrases. the number of noun phrases per message can be found in table 1 ."
"even though a large number of papers are already published, these factors in active buildings have not been analyzed as a complex set case study nowadays. moreover, when considering papers related to residential buildings, most of the existing solutions are initially intended to manage a combination of few separate components of the integrated thermal or electrical system within the building, such as pv arrays/wind turbine with battery or pv arrays/wind turbine with heat pump and different types of thermal storage. there is a lack of papers demonstrating solutions to manage all components including ress, heat pump, thermal and electrical energy storage as well as demand response applications as a complex integrated solution."
"the authors kindly acknowledge the editor and two anonymous reviewers for helpful observations and comments, which have helped to improve an earlier version of this manuscript."
"(3) edge detection. we randomly chose 8 images from our database of raw oct retinal images and apply the three edge detectors. as discussed earlier, the edge detection outcomes may be influenced not only by the algorithm itself but also by the input parameters [cit] . we varied the parameters systematically to obtain the optimal possible edge outcomes for each of the edge detection algorithms."
"in this paper we present a new np-annotated sms corpus, together with a novel variant of the semi-crf model, which runs in significantly lower time while maintaining similar accuracy on the np chunking task on the new dataset. future work includes the application of the weak semi-crf model to other structured prediction problems, as well as performing investigations on handling other types of informal or noisy texts such as speech transcripts. we make the code and data available for download at http://statnlp.org/research/ie/."
"from the other side, the percentage of flexibility obtained varies from 10-22%, which is quite satisfying results, considering the fact that dr response is applied only to very specific conditions focused on the user's comfort preferences. even though the share of dr responded signals are bigger in a scenario without battery, the fact of the presence of battery, in this case, plays opposite role. the larger battery size, the less energy imported from the grid, and the fewer request signals have to be responded."
"(4) adjusted tpr, fpr, and acc. some procedures of image processing can introduce edge shift ( [cit], chapter 3, p. 56-74). as a result, the detected edge may not match the position of actual edge. as the goal of retinal oct image segmentation is to extract the contours of retinal layer boundaries and measure the thicknesses of different retinal layers, small and constant shifts do not have effective impact when the layer thicknesses are of the only interest. therefore, edge pixels in the neighborhood detected by the algorithms may be accepted into true positive edge pixels when calculating the edge presence metrics. in this case, fom, the true positive rate, and false positive rate and accuracy measures need also to be adjusted. we define these adjusted metrics as fom adj, tpr adj, fpr adj, and acc adj : where n i and n a represent the number of ideal and actual detected edge pixels, d adj i is the distance between the ith detected edge pixel and its correct position, tp adj is the number of edge pixels detected by an algorithm that are considered as edges within the neighborhood of the ground truth, fp adj is the number of false positive pixels after the neighborhood searching and edge pixel adjustment, and tn adj equals tn as it is not affected by the neighborhood adjustment. these adjusted metrics allowing the edge shift can better reflect the amount of detected edge points."
"since the models that we consider are all wordbased 4, we tokenize the corpus using a regex-based tokenizer similar to the wordpunct_tokenize function in python nltk package. we also included some rules to consider special anonymization tokens in the sms dataset [cit] . the gold character spans are converted into word labels in bio format, reducing or extending the character spans as necessary to the closest word boundaries. the converted annotations are regarded as gold word spans. note that this conversion is lossy due to the presence of improper nps, which makes it impossible for the converted format to represent the original gold standard."
"the article will first define the elements of the created model (section 2.1). this is followed by a control strategy and an interaction algorithm between the smart building including on-site ress, flexible demand unit (viz. heat pump), in-house back up thermal and electric energy storage (battery and hwst) units, and the electrical grid (demand response application), section 2.2. the load-shifting potential and hence the ability to reduce peak loads applying different scenarios within a single household, amount of annually imported/exported energy from/to the grid, amount of demand requested and responded signals, as well as other results achieved will be tabulated and graphically presented in section 3 and discussed in section 4 respectively. the paper closes with conclusions, and proposal for further investigations in section 5."
"the control strategy is organized as follows. figures 7 and 8 below should be considered as two inseparable components of hybrid energy flow and control algorithm for energy management of active buildings. based on the initialized weather forecast data and the created power output models of renewable energy sources, the energy derived from the pv arrays and wt during a first 15-minute time step period is calculated. the energy produced by ress will be consumed by the household and the heat pump as a first priority step. these two blocks are shown separately due to fact that the"
"a basic vapor compression heat pump cycle is realized as follows. the working fluid, turning itself into a gas within the \"evaporator\", absorbs the heat from the external low-temperature heat source (the evaporator, in this case, works as a first heat exchanger). the compressor then raises up the pressure of the gas, increasing thereby its temperature. the hot gas, flowing through \"condenser\" and being hotter than the secondary source (water or in-building air), dissipates its heat to this source and condenses thus back into a liquid (the \"condenser coils\", in this cycle, work as a second heat exchanger). the liquid finally flows back through the expansion valve, reducing thus its pressure and cooling down, then, enters back the evaporator, turning itself again into gas, and the cycle repeats. figure 5 demonstrates a general overview of the heating system in the household, including the heat pump's cycle. source (the evaporator, in this case, works as a first heat exchanger). the compressor then raises up the pressure of the gas, increasing thereby its temperature. the hot gas, flowing through \"condenser\" and being hotter than the secondary source (water or in-building air), dissipates its heat to this source and condenses thus back into a liquid (the \"condenser coils\", in this cycle, work as a second heat exchanger). the liquid finally flows back through the expansion valve, reducing thus its pressure and cooling down, then, enters back the evaporator, turning itself again into gas, and the cycle repeats. figure 5 demonstrates a general overview of the heating system in the household, including the heat pump's cycle. this paper investigates the capabilities of an electrically driven air-to-water type vapor compression heat pump for heating application consisting of two units:"
"where: p hp el.rat. -rated electric power, (kw), cop hp (t) -coefficient of performance or, in other words, transformation coefficient from electrical to thermal energy, t-time step (which is 0.25 h in the current model)."
"in order to evaluate qoe for the security mechanism in sdn architecture, dedicated security functions are desired. in this section, we first describe the experimental configurations and afterwards, we investigate and compare the performance of hardware and software implementations of the present algorithm."
"in this paper, the energy flow model of a domestic dwelling consisting of pv array, individual on-site wind turbine, battery storage, a heat pump in combination with hwst was created. three different scenarios of the battery, three scenarios of the hwst, as well as two scenarios of the dr application (i.e., \"with\" and \"without\") were simulated. the paper assesses the flexibility for each scenario, and the results were summarized and compared in detail."
"(1) to measure the edge presence accuracy by calculating the rates of true positive, false positive, true negative, and false negative (missing) (2) to measure edge location accuracy by calculating the signed and unsigned edge shift distance (3) to allow edge shift when calculating edge presence accuracy (4) to examine the computational costs."
"despite the results obtained in this study, the battery may still be more attractive in other locations with different res generation and power demand patterns, as well as based on benefits provided from the network side."
"we conducted this study in accordance with the tenets of the world medical association's declaration of helsinki [cit] . ethical approvals were obtained from the ethical review board of southern medical university, the ethical review board of sun yat-sen university, and the research ethical committee of zhongshan ophthalmic center. after an introduction about the purpose of the study and explanation of the process and risks, the voluntary participants signed the informed consent for this data collection."
"in this paper, we will build our models based on a class of discriminative graphical models, namely conditional random fields (crfs) [cit], for extracting nps. the edges in the graph represents the dependencies between states and the features are defined over each edge in the graph. though crfs are undirected graphical models, we can use directed acyclic graphs with a root, a leaf, and some inner nodes to represent label sequences 3 . a path in the graph from the root to the leaf represents one possible label assignment to the input. in the labeled instance, there will be only one single path from the root to the leaf, while for the unlabeled instance, the graph will compactly encode all possible label assignments. the learning procedure is essentially the process that tries to tune the feature weights such that the true structures get assigned higher weights as compared to all other alternative structures in the graph."
"(4) performance evaluation. in edge detection performance evaluation step, we compared the edge detection outcomes from the three computer algorithms against the human manually traced retinal layer boundaries. we applied the metrics that were broadly used in the literature and relevant to our specific research context and purpose. we also applied the figure 2 : procedures of performance evaluation of edge detectors. the ground truth of oct retinal layer boundaries is labeled by an expert observer. raw oct images are firstly denoised and then applied with three automatic edge detection algorithms (i.e., canny, two-pass, and edgeflow) to obtain the algorithm-detected boundaries, which are compared with the ground truth."
"the smart active residential building model and the analysis of the integrated systems are based on the following approach, shown in figure 1 . the electricity, generated by the rooftop pv panels and the on-site small wind turbine is directly used to supply the household's demand. the excess of the electrical energy is firstly feeding a li-ion battery storage, and if the state of charge (soc) of the battery reaches the maximal level, electricity is exported to the grid. when the pv and wt production is not sufficient to feed the household's demand, the energy will firstly be discharged from the battery storage, and if any deficit still appears-it will be met by grid supply (import). the heat pump's electrical consumption (that fully depends on thermal demand of the household, external air temperature, state of energy (soe) of the hwst, defrost cycles, and dr signals) is added to the household's electrical load profile. this modification will finally give the residual profile after the effect of integration of ress and battery, hp with hwst and dr for household and the general overview of the flexibility achieved. for the model, 15 min time step, based on thermal and electrical energy distribution profiles of the household, historically measured weather data (irradiance, wind speed, and air temperature), as well as pv's, wt's, battery's, hp's and hwst's characteristics are required. figure 2 represents the framework of the created system. the first step \"input data\" is dedicated to obtaining all the necessary data for creating models. in this step sets of raw data are being processed, and all errors eliminated."
"the graphical and numerical results obtained in section 3 show that the proposed model can be successfully applied to estimate energy activity and flexibility range in smart active residential buildings, combining different system components within one hybrid energy flow and management algorithm."
"the results shown in table 8, figure 9, and table 9 represent the performance of the integrated thermal energy system. table 8 shows energy data and the performance of hwst, namely a number of backup heat supply hours that hwst can provide within the controlled temperature boundaries for three different volume scenarios (0.25, 0.5 and 1 cubic meter), as well as the amount of energy which is lost within the hwst during a year. a ratio of the thermal energy produced, and electrical energy used to produce this thermal energy (by the hp and an ihwh) on an annual basis, as well as shares of energies spent for defrosting cycle (which is highly important to operate the hp technologically properly, but can be said wastefully for household's needs), as a performance indicator, are illustrated in figure 9a,b. it is observed, for the production of 13,217 kwh of heat, 6063 kwh of electricity have to be used, thus, in accordance with equation (1) the annual cop of the hp is equal to 2.18, while the share of 7,6% of the total thermal energy produced (namely 1002 kwh). as well as electrical energy used (namely 460 kwh), was spent on defrosting, which is a rather wasteful amount. at the same time, the very insignificant amount of 177 kwh of electrical energy used by the ihwh, indicates that the hp covers most of the heat demand (i.e., 97%) in mono mode and that the size of the hp is chosen properly. [cit], 2 for peer review 16 of backup heat supply hours that hwst can provide within the controlled temperature boundaries for three different volume scenarios (0.25, 0.5 and 1 cubic meter), as well as the amount of energy which is lost within the hwst during a year. a ratio of the thermal energy produced, and electrical energy used to produce this thermal energy (by the hp and an ihwh) on an annual basis, as well as shares of energies spent for defrosting cycle (which is highly important to operate the hp technologically properly, but can be said wastefully for household's needs), as a performance indicator, are illustrated in figure 9a,b. it is observed, for the production of 13,217 kwh of heat, 6063 kwh of electricity have to be used, thus, in accordance with equation (1) the annual cop of the hp is equal to 2.18, while the share of 7,6% of the total thermal energy produced (namely 1002 kwh). as well as electrical energy used (namely 460 kwh), was spent on defrosting, which is a rather wasteful amount. at the same time, the very insignificant amount of 177 kwh of electrical energy used by the ihwh, indicates that the hp covers most of the heat demand (i.e., 97%) in mono mode and that the size of the hp is chosen properly. a comparison of two different scenarios (namely without and with dr application) for three different hwst's volumes using the same data indicators are summarized in table 9 to analyze the deviation and to investigate the dr impact on final performance. the results show that the dr application has no impact on overall performance (the hp's cop is equal to 2.18 in all cases). however, the smaller size of the hwst, the more energy is used by ihwh, which generally means that the small volume of the hwst is insufficient to support hp to cover the heat demand in mono mode for a long time, thus ihwh turns on more frequently and the total cop decreases. a comparison of two different scenarios (namely without and with dr application) for three different hwst's volumes using the same data indicators are summarized in table 9 to analyze the deviation and to investigate the dr impact on final performance. the results show that the dr application has no impact on overall performance (the hp's cop is equal to 2.18 in all cases). however, the smaller size of the hwst, the more energy is used by ihwh, which generally means that the small volume of the hwst is insufficient to support hp to cover the heat demand in mono mode for a long time, thus ihwh turns on more frequently and the total cop decreases. the curves that demonstrate the maximum and average values of the power produced by the pv array and the wt, as well as total energy produced during particular months are given in figure 10a -c. having 3.15 kw rated power of the pv array and 6 kw rated power of the wt, it is observed (figure 10a ) that pv production has not reached its maximum power output throughout the year (showing its maximum of 2.6 kw in the late spring and summertime), while the wt's power output of 6 kw observed very often due to windy weather condition in denmark (see figure 10b ). figure 10c illustrates the pre-dominant pv production in the summer, and early autumn seasons, while the share of wt prevails in the rest of the year. nevertheless, the low total res production (which is due to the weather conditions) is observed in the period of september-november, january-february, and a high production is observed in a period of march-august and december. it is expected that this discrepancy will be compensated by the battery and the energy import from the grid. the utilization of the renewable energy produced on-site, renewable energy exported, and the grid imported energy (which are, to a greater extent, affiliated with the size and efficiency of the battery, as well as the presence of load at a specific time) is demonstrated for two different battery and hwst scenarios in figure 11a,b below. a large energy import from the grid is observed between september and april (which is mainly due to a shortage of res energy production and high demand, as shown in figures 4a and 10c above), while export of energy is observed in a period of march -august and december. this phenomenon can be explained as an excess of res production and low demand in the summertime, as well as the insufficient battery storage capacity in both scenarios. one of the interesting findings by making an analysis of results obtained from 24 simulations is the difference between values of total annual energy imported from the grid under various battery scenarios. regardless of all expectations and more than a double difference in the battery size, the difference between the above values is very small (figure 12 and table 10 below). figure 12 demonstrates the shares of energy (namely imported from the grid and res generated), which are utilized to meet the household's demand in four scenarios. the discrepancy between scenarios with battery 1 and battery 3 in each case is in the range of 2-4%. that basically means, that regardless of the size of the battery (of course, on a reasonable scale), it cannot provide long-term energy compensation, mainly due to the fact that there are still many periods during a year where total res generation (from both pv and wt) is very low or absent at all (mostly in january and february), and that the largest battery, in this case-study, helps to utilize only 16% more res generated energy comparing to the scenario without battery at all. [cit], 2 for peer review 19 another interesting fact is that the use of the dr has no influence on the final annual consumption as well as the amount of energy imported from the grid. as an example, the total energy consumption in the scenario with 0.25 m 3 hwst without dr application is 11,246 kwh, while in the same hwst scenario with dr application; it is 11,258 kwh (see table 10 below for more results). the discrepancy in result values between \"without\" and \"with\" dr application is less than 0.2% which is completely negligible. that one more time proves the fact that the dr application does not harm the user preferences and the user will not pay more. the values for on-site res produced energy, including consumed by household, passed through the battery (i.e., charged/discharged), exported to the grid, and the values for energy imported from the grid are summarized for 24 different cases (comparison of the hwst, battery size, and dr application) in table 10 . figure 13a,b illustrate the application of the energy management without (a) and with (b) dr. the fourth day of the simulation (4th of january) with the largest hwst (i.e., 1 m 3 ) and a mediumsized battery (battery scenario 2, i.e., 8 kwh) is presented. even though the cycles were shifted during the first three days due to dr, these two figures can ideally represent most of the processes of the another interesting fact is that the use of the dr has no influence on the final annual consumption as well as the amount of energy imported from the grid. as an example, the total energy consumption in the scenario with 0.25 m 3 hwst without dr application is 11,246 kwh, while in the same hwst scenario with dr application; it is 11,258 kwh (see table 10 below for more results). the discrepancy in result values between \"without\" and \"with\" dr application is less than 0.2% which is completely negligible. that one more time proves the fact that the dr application does not harm the user preferences and the user will not pay more. the values for on-site res produced energy, including consumed by household, passed through the battery (i.e., charged/discharged), exported to the grid, and the values for energy imported from the grid are summarized for 24 different cases (comparison of the hwst, battery size, and dr application) in table 10 . figure 13a,b illustrate the application of the energy management without (a) and with (b) dr. the fourth day of the simulation (4th of january) with the largest hwst (i.e., 1 m 3 ) and a medium-sized battery (battery scenario 2, i.e., 8 kwh) is presented. even though the cycles were shifted during the first three days due to dr, these two figures can ideally represent most of the processes of the created model. the attention should mainly be focused on the period between 05:30-11:00 ( figure 13a )."
"making an analysis of the measured data obtained from 25 buildings located in the northern jylland region of denmark (i.e., electric power demand and thermal demand for space heating (sh) and domestic hot water (dhw)), an average statistical single-family detached household with the district heating (dh) heat supply is chosen as a basis. the area of the household is around 120 sq. m."
"as can be seen at 05:45, the level of thermal energy of hwst had reached a minimum level. to meet the household's heat demand, the heat pump was turned on, thus starting the charging cycle of the hwst. having very high thermal demand, as of 06:15, hp's production became insufficient in mono mode, and ihwh as an additional heat source was turned on, and, thereby the peak demand occurs. since the battery soc was at its maximum at 05:30 and the res production was very low, it is observed that the total load (including a peak value) was fully covered by the battery itself for two hours. review 20 created model. the attention should mainly be focused on the period between 05:30-11:00 ( figure 13a ). as can be seen at 05:45, the level of thermal energy of hwst had reached a minimum level. to meet the household's heat demand, the heat pump was turned on, thus starting the charging cycle of the hwst. having very high thermal demand, as of 06:15, hp's production became insufficient in mono mode, and ihwh as an additional heat source was turned on, and, thereby the peak demand occurs. since the battery soc was at its maximum at 05:30 and the res production was very low, it is observed that the total load (including a peak value) was fully covered by the battery itself for two hours. as one can see at 07:45, the battery soc reached a minimum level, and since res production was still very low, import appeared. based on the work performed in section 2.1.6 above, a series of dr request signals were received between 08:00-11:00. since, as of 08:30, the hwst had become halfcharged, having sufficient amount of thermal energy to cover the heat demand, as well as meeting all other conditions (described in the algorithm in figure 8, section 2.2), the system responded to the as one can see at 07:45, the battery soc reached a minimum level, and since res production was still very low, import appeared. based on the work performed in section 2.1.6 above, a series of dr request signals were received between 08:00-11:00. since, as of 08:30, the hwst had become half-charged, having sufficient amount of thermal energy to cover the heat demand, as well as meeting all other conditions (described in the algorithm in figure 8, section 2.2), the system responded to the received dr signal by turning off the heat pump (see 10:30 in figure 13b) . the same figure also shows that almost all of the energy imported between 07:45-10:30 was shifted to the evening time after 19:00. table 11 below shows the percentage of dr responded signals for different battery and hwst scenarios. the results show that the bigger the battery size the lower the percentage. as an example, battery 1 (under scenario with 0.25 m 3 hwst) shows 10.8%, while battery 3-10.1%. this can be explained by the fact that the battery covers a certain number of peak loads, and the share of power taken from the grid at that particular time (when the signal is requested) is very insignificant or does not exist at all. thus, scenarios without the battery, in general, demonstrate much larger responsiveness, namely: 14.8% of dr requested signals were responded under the scenario with 0.25 m 3 hwst, while the maximal flexibility of 22.3% was reached in the scenario with the largest hwst volume (i.e., 1 m 3 )."
"2.3.1. criteria for performance evaluation metrics. oct retinal layer segmentation aims to automate retinal layer thickness measurement in order to free ophthalmologists from laborious manual tracing of the layer boundaries. the ideal layer edge detector would give the same thickness measures to those from ground truth specified by human observers. however, even experts could not arrive at the same segmentation for a given retinal oct image [cit] . this is because manual segmentation is subject to human subjectiveness. the ground truth used for the evaluation is not really the ultimate truth. thus, it is important to note that the traditional edge presence accuracy metrics, the probabilities of true positive, false positive (spurious edges), and missing edge, cannot offer the complete evaluation of edge detector performance. we propose that the performance metrics need to meet the criteria as follows:"
"compact mono-block indoor unit with circulation pump, three-way diverter valve (heating/dhw heating), instantaneous heating water heater, control unit, and this paper investigates the capabilities of an electrically driven air-to-water type vapor compression heat pump for heating application consisting of two units:"
"the general idea of this study is to combine these components within one system, to be able to manage them within a single management algorithm, and to graphically and numerically demonstrate and evaluate how the flexibility of the hp in combination with a hot water storage tank (hwst) enables the flexibility in electrical energy use in a household without jeopardizing the occupants' thermal comfort as well as annual electrical energy needs by applying artificially simulated dr request signals. the paper analyses the battery and the hwst sizes impact on the amount of annually imported electricity, res utilized, and exported energy, when having own res generation on-site."
"the major steps for performance measurement include the preparation of ground truth, the preprocessing of oct images, the application of edge detectors with appropriate parameters to obtain the near optimal outcome for each detector, and using the performance metrics to evaluate the goodness of edge detectors against the ground truth. figure 2 summarizes the flow of performance evaluation."
"the set of edges and features defined in each model affects the feature expectation and the normalization term. computation of the normalization term, being the highest in time complexity, will determine the overall complexity of training the model. the set of edges and the normalization term in each model will be described in the following sections."
"in this paper, we implement the present lightweight block cipher as our encryption core. the algorithm encrypts a 64-bit block with an 80-bit key in 31 rounds. each of the 31 rounds consists of a linear bitwise permutation and a non-linear substitution layer as well as a xor operation to introduce a round key i k for1 31 i  . fig. 1 shows a top level functional flow and layout of present algorithm. a substitution box layer (s-box) is implemented in this algorithm. s-box takes an input of 4-bits and replaces it with another 4-bit value as the output. afterwards, a permutation layer (p-layer) is designed to jumble the bits. the key scheduler is a critical part of the present algorithm, which is used for updating the key with every iteration during the different states of the algorithm. the present algorithm is an iterative process, this means that the data will be passed through the s-box, p-layer and add round key process multiple times before the cipher-data is produced at the end. there are several research works on cryptanalysis of the present algorithm [cit] ]. multiple attack approaches are used to investigate how secure the present algorithm is. these attacks include, linear cryptanalysis of reduced round present [cit] ], differential cryptanalysis of reduced round present and key recovery attack for present using slender-set linear cryptanalysis [cit] ]. due to the high scale of time taken to crack the present encryption, the attacks presented in these works would be impractical to apply in reality in an attempt to crack the algorithm and get any useful information secured through the algorithm."
"in the third step, all necessary power output profiles and state of charge models are created using fundamental mathematical equations and specific methods (sections 2.1.4-2.1.6). this is followed by the control strategy (section 2.2) which is, based on pre-set conditions of the created algorithm and input data, evaluates the ability of the system to respond to the dr signals requested, charge/discharge storages to its maximal/minimal levels, as well as to manage energy import/export to and from the grid."
"the results shown in table 8, figure 9, and table 9 represent the performance of the integrated thermal energy system. table 8 shows energy data and the performance of hwst, namely a number"
"in the final fifths step the results are obtained (section 3). in the third step, all necessary power output profiles and state of charge models are created using fundamental mathematical equations and specific methods (sections 2.1.4-2.1.6). this is followed by the control strategy (section 2.2) which is, based on pre-set conditions of the created algorithm and input data, evaluates the ability of the system to respond to the dr signals requested, charge/discharge storages to its maximal/minimal levels, as well as to manage energy import/export to and from the grid."
"(2) image preprocessing. before applying the computer algorithm for each edge detector, we conducted necessary image preprocessing. due to constructive or destructive interference of the light waves from the object, spectral domain retinal oct images suffer from the inherit speckle noise [cit], which decreases the quality of image and causes unreliable retinal layer segmentation. in order to improve the quality of edge detection, preprocessing becomes a necessary step. we first converted the raw oct image bmp files into gray-scale images and cropped the images to the region of interest (roi, 200 by 400 pixels) in this study. the literatures have suggested the use of filters like mean, median, and gaussian [cit] for noise removal. we choose median filtering to remove the speckle noise. the original retinal oct image and the denoised image are shown in figure 3 ."
"most recent work focus on the texts generated through twitter, which, due to the design of twitter, contain a lot of announcement-like messages mostly intended for general public. in contrast, sms was designed as a way to communicate short personal messages to a known person, and hence sms messages tend to be more conversational and more informal compared to tweets."
"over the past decade, multimedia has been driving an increasing amount of the traffic over networks. [cit], it is expected that 78 percent mobile traffic will be video data [cit] ]. as mobile networks are migrating to 5g, the method for video transmission over the networks is evolving, especially in terms of encryption. this evolution includes load balancers, network address translators, qoe and value-add services (vas) such as content/url filtering and video compression. securing all these network devices in the cloud computing environment can be enhanced by incorporating the security functions. here, the context of security refers to the data encryption at data flow-level in the cloud. in other words, if the security feature is enabled by a software control node for a specific flow, the packets belonging to that flow will be encrypted before departing the network device and will safely pass through the public/private networks to reach end-users. the private encryption key is stored in a flow table created inside the network device. the encryption algorithm uses the associated per-flow key to encrypt packets, the same key is utilized to decrypt the packets and restore the original data. this paper implements the light-weight present algorithm on netfpga-10g platform in our sdn testbed. the prime motivation to design, implement and evaluate the proposed security enabled network function in the proposed environment using present are:"
"(2) edge presence accuracy. the criteria on which the fom of pratt is based include missed valid edges, localization errors, and false alarms. different configurations of detected edges may yield equal fom value [cit] . in order to decompose the sources of difference, [cit] developed three metrics (tpr, fpr, and acc) that are defined as follows. true positive rate (tpr):"
"quantitative analysis of retinal oct image has been critical for reliable and efficient diagnosis of diseases such as glaucoma, age-related macular degeneration, and macular edema caused by diabetic retinopathy and for the evaluation of development of diseases, medical treatment responses, drug effectiveness, visual functions, and so forth [cit] . among others [cit], automatic and semiautomated measurement of retinal layer thickness is considered as a class of key quantitative analysis. numerous research efforts have been devoted to this topic [cit], and these efforts have significantly promoted the clinical understanding of ocular diseases and improved the oct technologies and their applications."
"weather data (i.e., solar irradiance, wind speed, air temperature) were derived for the same location from two different sources, namely, the weather forecasting station based at aalborg university laboratory, and the energypro software. the data from each source were carefully analyzed, compared, and some very minor errors were interpolated and eventually eliminated."
"to quantify the performance of the three edge detectors, we use three sets of measurements discussed in materials and methods. the first set of metrics include fom, tpr, fpr, and acc, which have been broadly used in the literature [20, [cit] when evaluating edge detectors on images other than the oct retinal images. in order to calculate tpr, fpr, and acc, we use the ground truth as a template to screen the coincided edge pixels from the three edge detection algorithms. figure 6 shows the edge points overlapping with the manually traced edges (ground truth)."
"this result, coupled with the fact that the weak semi-crf requires 12.5% less time than the conventional semi-crf (1.811s vs 2.072s), shows the potentials of using this weak semi-crf as an alternative of the conventional semi-crf. with more label types (here only two), the difference will be larger, since the weak semi-crf is linear in number of label types, while conventional semi-crf is quadratic."
"optical coherence tomography (oct) is the optical equivalent of ultrasonography, with the capability of capturing the depth-resolved cross-sectional images of biological tissues in vivo at near-histologic resolution [cit] . due to its noninvasiveness and high resolution, in combination with the characteristics of the eye and retinal anatomy, oct has a rapid development of clinical applications in ophthalmology in recent years."
"our goal in this paper is to evaluate what type of edge detectors best suit for retinal oct image segmentation with given equivalent parameters, when measured using performance metrics that are meaningful for retinal layer thickness quantification. to this end, we first collect the commonly used edge detection methods in the literature on oct image analysis and choose the most representative ones for comparison. we then research the edge detection performance evaluation literature to select the most relevant performance metrics that are meaningful for oct image segmentation and adapt them when necessary. using these metrics, we examine which of the selected edge detectors gives the best edge detection outcome when they are applied to the oct images that we have collected from healthy subjects."
"the increase in mobile access to data services places a tremendous load on networks. traditionally, a majority of mobile data traffic is http over tcp. however, many services are in the process of transitioning to https [cit] ]. the reasons for moving services to https are ensuring security (i.e. media request cannot be intercepted), privacy (viewing habits cannot be inferred by inspecting traffic) and popularity (google page rank favoring sites delivered under https [cit] ]. the growing use of encrypted protocols is causing fundamental shifts, which results in added network burden for security functions in real-time applications."
"the present cipher is implemented in both software and hardware and is tested on two separate devices. a standard laptop for software implementation and a xilinx netfpga-10g for hardware implementation are setup for this comparative study. the results are shown in tab. 2. the software-based implementation was written using c and runs on a desktop server. the hardware implementation is deployed using verilog on netfpga-10g. fig. 5, a comparative study of average processing time for various data sizes is presented. fig. 5 shows that the hardware implementation was approximately a thousand times faster at encrypting and decrypting information then the software implementation."
"in this article, our goal is to evaluate the performance of edge detection algorithms for retinal oct image segmentation. because of their image dependence [cit], edge detectors perform well for other types of images but may not give as good results when applied to retinal oct image. therefore, all interested algorithms are tested on retinal oct image in this study."
"three criteria for our choice of edge detectors for evaluation were (1) to include the algorithms that have been most commonly used in the oct image segmentation literature, (2) to give preference to the ones representing the state of the art in edge detection, which were usually used to detect more than 3 retinal layer boundaries, [cit] (3) to include a diverse mix of algorithms utilizing different image feature information."
"considering still very high investment cost of lithium-ion batteries and a very poor price of energy exported to the grid in denmark (from res energy trading point of view), a more reasonable and forward-looking solution for the res energy utilization for household owners (instead of rising up the size of the battery) could be found in the following combination. a small battery can be used for the short-term (hourly) energy compensation, a large hwst for the provision of the demand energy flexibility services, and an electrolyzer coupled with fuel cells that stores energy in a form of pressurized hydrogen and oxygen for the storage of large quantities of energy for a long period."
"where f s is a complex number with its magnitude representing the resulting edge energy and phase representing the flow direction. after the edgeflow vector of an image is computed, boundary detection can be performed by propagating the edgeflow vector and identifying the locations where two opposite flow directions encounter each other. the scheme facilitates integration of intensity and texture into a single framework for boundary detection."
"in the next step, it was decided to increase the limit to 25% above the annual mean value and then it gives the percentages of appearance of the value that is greater or equal to 4876 mwh. the results are shown in figure 6b ."
"to evaluate the performance of the hardware enabled security, we consider a cloud multimedia secure delivery setup. for investigation, we configure the multimedia setup using video streaming services. video contents of various qualities are stored on a streaming server. we integrate the netfpga-10g as hardware accelerator near the video streaming server. we configure the video traffic to route through the netfpga-10g platform. the aim of this testbed is to evaluate the performance of streaming video of various video quality standards with/without security feature. in the video streaming server, a vlc program [cit] ] is configured to stream the video traffic via netfpga-10g to the client. the client is a typical user computer with an ubuntu 14 operating system. all measurements are conducted using the video streaming platform vlc and wireshark. fig. 4 shows the measured video streaming latency with/without encryption in our sdn testbed, for streaming videos of various video quality standards. the x-axis indicates various video quality standards, for the two types of each video sent directly (unencrypted) and securely (encrypted). the y-axis shows the latency, represented using bar levels for both types of transmission. the measured transmission latency of encrypted video streaming show approximately 8 to 10 percentiles increase compared with the latency of streaming the corresponding unencrypted videos. the slight increase in latency is not perceived in the decrypted video quality at the receiver end. this latency increase is so small that interacting services such as video conferencing would be unaffected."
"outdoor unit that contains evaporator, compressor, condenser, diverter and expansion valves, and variable speed fans. the hp is considered with output-dependent/inverter-controlled scroll type compressor (that uses an external variable-frequency drive to control the speed)."
"if to assume that the critical load limit, described in section 2.1.6, would increase to 35% above the annual mean value equal to 5266 mwh, then the number of dr request signals will be much less, since fewer periods will appear above this limit. this will definitely lead to the fact that the total amount of energy shifted per year will be also much less. however, the percentage of flexibility, in general, should not differ significantly from the results obtained in this study (initially, due to the given conditions of the created algorithm). it would also be interesting to investigate the model behavior and the amount of energy shifted under the local 10/0.4 kv grid conditions (that has fairly different peak load hour curve), and to compare the difference between two cases, however, this is the subject of another study."
"word shapes can be considered a generic representation of words that retains only the \"shape\" information, such as whether it starts with capital letter or whether it contains digits. the brown clusters and word shapes features are applied to each of the word features described in each model."
"the amount of res produced energy that has been consumed, stored in batteries by applying three different battery capacity scenarios, and the excess that has been exported to the grid."
"to protect the compressor from high pressure caused by the change in the flow of refrigerant at the reversing valve, a five-minute starting time delay is introduced. thus, having a model with a 15 min time step, the energy output for each first hp's \"on\" loop will be counted for the rest 10 min only. accordingly, thermal energy outputs, incl. delay time and defrost time are calculated by the following equations."
"due to the noisy nature of sms messages, there may not be proper capitalization or punctuation, and in some cases there might be missing spaces between words. figure 1 shows a sample sms message taken from the corpus. we can see that \"dr teh\" is not properly capitalized and \"she\" in \"butshe's\" is missing spaces around it. nps which do not have clear boundaries (improper nps) constitutes 4.0% of all nps."
"in semi-crf [cit], in addition to the edges defined in linear crf, there are additional edges from a node to all nodes up to l next words away, representing a segment within which the words will be labeled with a single label. the normalization term z w (x) is calculated as:"
"based on our analysis of existing performance metrics in the literature and the metric criteria discussed previously, we choose the figure of merit (fom, pratt) [cit], true positive rate (tpr), false positive rate (fpr), accuracy (acc), and mean localization deviation (mld) [cit] as the basis to develop our procedure for comprehensive evaluation of the chosen edge detectors."
"in the final fifths step the results are obtained (section 3). the first step \"input data\" is dedicated to obtaining all the necessary data for creating models. in this step sets of raw data are being processed, and all errors eliminated."
"given the large number (n−n i ) of nonboundary pixels in the images, fpr calculated in the form of (7) is close to zero, making the metrics insensitive to the change of edge detection algorithms. we redefined it as"
it is also known that feature selection is an important aspect when trying to use semi-crf models to improve on the linear crf. [cit] reported an error reduction of up to 25% when using features that are best exploited by semi-crf.
"for our purpose in this study, we were mainly interested in how the three edge detectors performed in detecting the 6 retinal layer boundaries, which were also the key information in the literatures for retinal layer thickness measurement [cit] . we defined the region of interest (roi) to be the area between the ilm and rpe that are the most outer boundaries of the retinal structure. figure 5(a) is the oct retinal image with overlaid ground truth edges marked by an expert observer. figures 5(b), 5(c), and 5(d) show the edges within roi detected by the three algorithms, which will be the basis of performance evaluation in the next section. visually, the result from the canny edge detector in figure 5 (b) shows a well-defined six boundaries, although with some breakages and noises. the result from the two-pass method shown in figure 5 (c) gives more than 6 layers in some locations, but in general, the six layer boundaries of interest are very clear with less breakages compared to those in figure 5 (b). the result from edgeflow algorithm depicted in figure 5 (d) shows more breakages and more noises, although all 6 layer boundaries are still recognizable."
"taking into account the hourly consumption profile and the annual mean value, the number of appearances above the mean value in each particular hour of a day during a year has been investigated and counted (e.g., 300 times from total 365 days has appeared at 17:00, and 298 times out of 365 days at 18:00). the percentages of the appearance of average consumption are presented in figure 6a ."
"first, we see that the two variants of semi-crf models perform better compared to the baseline linear crf model, showing the benefit of using segment features over only single word features. it is also interesting that, while being a weaker version of the semi-crf, the weak semi-crf can actually perform in the same level within 95% confidence interval as the conventional semi-crf. this shows that some of the dependencies in the conventional semi-crf do not really contribute to the strength of semi-crf over standard linear crf. as noted in section 3.3, weak semi-crf makes the decision on the segment type and length separately. this means there is enough information in the local features to decide the segment type and length separately, and so we can remove some combined features while retaining the same performance."
"as conversational texts, sms data often contains references to named entities such as people and locations relevant to certain events. recognizing those hmm dr teh says the research presentation should still prepare, butshe's not to sure whether they'd time to present references will be useful for further nlp tasks. one way to recognize those named entities is to first create a list of candidates, which can be further filtered to get the desired named entities. nadeau [cit] lists several methods that work upon candidates for ner. as all named entities are nouns, recognizing noun phrases (np) is therefore a task that can be potentially useful for further steps in the nlp pipeline to build upon. figure 1 shows an example sms message within which noun phrases are highlighted. as can be seen from this example, recognizing the np information on such a dataset presents some additional challenges over conventional np recognition tasks. specifically, the texts are highly informal and noisy, with misspelling errors and without grammatical structures. the correct casing and punctuation information is often missing. the lack of spaces between adjacent words makes the detection of np boundaries more challenging."
"(1) ground truth preparation. we asked an expert observer to manually delineate the edges for representative retinal oct images to form a base dataset of ground truth, as noted by i ref in figure 2 . because the ilm and rpe are the outer boundaries of the retinal structure and they are strong edges that can usually be reliably detected, we define the images between ilm and rpe (included) as the region of interest (roi). only those edges within the roi are extracted for comparison with the ground truth."
"with the development of oct technologies and their applications in the field of ophthalmology, more and more data is readily available. extracting meaningful information from the ever-increasing volume of clinical data reliably and efficiently forms the basis for modern medical decision making and research. reliable and efficient oct retinal image segmentation will contribute to the development of this trend. future research efforts would need to overcome several limitations in this study. first, the input parameters used in our experiments were selected over a relatively small sample space and the decisions on the \"optimal\" parameters were subject to human subjectiveness. although it is almost impossible to identify the absolutely optimal input parameters for each edge detector [cit], the choice of optimal input parameters may be improved by conducting a large number of experiments and averaging opinions from more expert viewers. the second limitation in our study is the use of a single expert observer to define the ground truth. individual subjectiveness may be reduced by averaging across multiple decisions for the ground truth. moreover, our data were all collected from voluntary healthy subjects. if the edge detectors perform differently for different types of images, it is necessary to examine how they perform on pathological retinal images in future studies."
"the findings in the study suggest that it is critical to use the most appropriate algorithms to detect the retinal layer boundaries in the oct images in order to automate the quantitative analysis of retinal oct images. combined with the findings in the literature that edgeflow method significantly outperformed canny algorithm in texture segregation tasks [cit], this study offers support to the idea that the performance of edge detectors is image property dependent [cit] as both canny and two-pass methods surpass edgeflow in the current application. in line with this thought and findings, it is necessary argue that the best performer for normal retinal oct images also work best for pathological retinal images. additionally, the intensity-gradient based methods (two-pass and canny algorithms) outperforming texturebased method (edgeflow) might suggest that the oct images contain more intensity gradient changes than texture changes along the longitudinal direction. the relative weight of intensity and texture information in oct retinal image warrants further study in the future."
"6 [cit] previously showed that off-theshelf np-chunker performs worse on informal text. then they trained a linear-crf model on additional in-domain data, reducing the error up to 22%. however no results on semi-crf was given."
"the case with the limit set to 25% above yearly mean value is used in the rest of the paper, as an example for peak shaping. this value might be a realistic threshold for the future application of demand response depending on actual local conditions for transformer loading, line loading etc. however, the method set up here can be applied for any chosen limit, so this is just to be seen as an example. based on the above, it is decided to simulate the dr request signals using random distribution function with percentage probability shown in the case (b). according to that, having one-year model with 35,040 (15-min) time steps, 4668 dr request signals were simulated."
"the created model and simulation results show that having such integrated systems and the demand response application may lead to techno-economic benefits for both household's owners, in terms of sustainability and reducing the cost of energy, as well as the dso's in terms of obtaining certain percent of the flexibility on demand-side and hence the ability to reduce peak loads thus making grid operation process smoother."
"results imply that the heat pump, without jeopardizing any user comfort preferences, even with the smallest thermal storage size (i.e., 0.25 m 3 ) and without battery provides quite satisfied load shifting flexibility for this condition, namely 14.8% (table 11 above). however, the battery seems not to be very efficient in northern climate conditions by showing the performance that does not exceed 16% in terms of res energy conservation (figure 12 above) ."
"in these equations, tp (true positive) and tn (true negative) represent the numbers of correctly detected edge pixels and nonedge pixels. fp (false positive) is the number of pixels not belonging to edge but recognized as one by the algorithm, and fn (false negative) is the number of pixels belonging to edge but failed to be recognized by the algorithm. n is the total number of pixels within the roi of the image, and n i is the number of ideal edge pixels."
"transductive learning [cit] ) is a learning paradigm that exploits a large amount of unlabeled data when a small amount of labeled data is available. it assumes that the testing data are exactly the unlabeled data. many transductive learning algorithms have been proposed in the literature. [cit] has formulated an optimization algorithm for learning transductive support vector mfachines (tsvms). this algorithm exploits the structure in both training and testing data for better positioning the maximum margin hyperplane. [cit] have formulated a fast, multi-switch implementation of the tsvms, called svmlin, which is significantly more efficient and scalable than the previous algorithm. they have exploited data sparsity and linearity of the problem, in order to provide superior scalability. they have investigated a multiple switching heuristic that further improves tsvm training by an order of magnitude. in particular, according to the multi-switch modality, more than one pair of labels may be switched in each iteration. these speed enhancements turn tsvm into a feasible tool for large-scale applications. in addition, they adopted deterministic annealing techniques, in order to alleviate the problem of local minima in the tsvms. another family of transductive algorithms is investigated in graph mining. a graph is defined with the nodes representing both labeled and unlabeled instances, while the edges reflect the similarity of instances. graph-based approaches usually assume label smoothness over the graph. one example is to exploit the structure of the entire data set in the search for min cuts [cit] or for min average cuts [cit] ) on the graph. finally, recent advances include transductive algorithms for multi-label classifications, to effectively assign a set of multiple labels to each instance [cit], as well as transductive relational probabilistic classifiers [cit], to apply transduction in probabilistic relational learning."
"the spatial dimensions used by birchfield & ragajaran and others are the spatial dimensions of the image and a primary use of spatiograms has been for color-based tracking in video images. note that there is nothing about the definition which constrains the spatial dimensions to be in the image. if, for example, the image information comes from a stereo camera, then the spatial information can be three-dimensional depth information."
"we perform a sensitivity analysis of the performance of s 2 tec along the number of iterations, the size of the initial labeled set and the size of the spatial neighborhoods. firstly, we consider the labeled sets sampled with the percentage 5 % and we monitor both the performance of s 2 tec along the dimension of the number of performed iterations. for this analysis, s 2 tec constructs spatial neighborhoods with sizes growing from 5 to 10, 15 and 20. secondly, we vary the percentage of pixels which are labeled in the image among 3, 5 and 10 %, while we run s 2 tec by constructing spatial neighborhoods with sizes growing from 5, 10, 15 to 20. finally, we construct spatial neighborhoods with sizes: 5-10-15, 5-10-15-20 and 5-10-15-20-25, while we run s 2 tec with the initial labeled set sampled with the labeling percentage equal to 5 %."
"the purpose of the landmark saliency architecture is to extract tsg landmark candidates from image and depth views. the saliency criteria need to include visual and spatial regions that can be represented well by a tsg. however, we also want our landmarks to be useful for humans, so we include some criteria that relate to human visual attention."
the computational complexity of s 2 tec depends on the cost of (1) classifying unlabeled pixels according to the supervised classifier learned from l on s; (2) constructing the neighborhood structure with the radius values collected in rset; (3) constructing the relational features according to both the frequency profile and the morphological profile; (4) constructing the ensemble of classifiers; (5) identifying pixels of u which are identically labeled by the majority of classifiers in the ensemble and moving these pixels from u to l with their consensus labels; (6) updating relational features according to the new consensus labels.
"the experiments were conducted on a pioneer at3 robot equipped with a videre stereocamera (6mm lenses) on a biclops pt base. the stereocamera was calibrated using the sri smallvision 3 system. the robot was instructed to follow a loop around an outdoor traverse area in which there were a variety of objects. the robot stopped at regular distances along its traverse and collected sets of image and depth information from the stereocamera, with pan angle set to 80 o,90 o,100 o (i.e., three side views). this resulted in a variety of views of the objects in the traverse area."
"we have introduced a landmark saliency architecture, lsa, based on raubal & winter's model of landmark saliency. in addition to the visual attraction component modeled by most saliency architectures, this includes a structural attractiveness component, capturing the spatial conciseness criteria for candidate tsg landmarks, and a semantic attractiveness component, a channel by which the task at hand can influence landmark saliency."
"the images r s (i c ) and r s (i d ) are the input to the structural attractiveness module, which focuses on salient region properties. figure 3 shows the structural attraction. the two images are linearly combined to form a fused conspicuity map [cit] as follows:"
"1. the ensemble of the multiple classifiers (algorithm 1, line 10) is learned from the currently labeled set l. this ensemble is composed of: (1) classi f er f (algorithm 1, line 7), learned from l as it is spanned on the vector of frequency-defined relational features f; (2) classi f er m (algorithm 1, line 8), learned from l as it is spanned on the vector of morphological-defined relational features f; classi f er s (algorithm 1, line 9), learned from l as it is spanned on the vector of spectral features s. the ensemble is used to predict labels of pixels of the currently unlabeled set u. 2. for each pixel in the currently unlabeled set u, each classifier in the ensemble is used to predict its label. we consider consensus patterns, pixels which are identically labeled by the majority of classifiers of the ensemble. a consensus pixel is finally labeled with the consensus label (algorithm 1, line 13) determined by the ensemble and definitely moved from u to l (algorithm 1, lines 14-15, fig. 4 ). on the other hand, pixels, which are left in u, stay still associated with the labels predicted by the spectral classifier learned during the initialization phase (see fig. 4 ). 3. the spatio-relational features of both the frequency profile and the morphological profile are updated according to the new consensus labels, which have been finally updated in c (algorithm 1, lines 18-19)."
"relational representations and relational learning algorithms have been investigated in the literature, in order to deal with spatial correlation in several real-world applications [cit] for a survey). relational learning algorithms can be directly applied to various representations of spatial data, i.e. collections of geo-located entities. they account for spatial correlation that biases learning in spatial domain. furthermore, discovered relational patterns reveal those spatial relationships, which correspond to spatial domains."
the module subsamples the filtered image at a scale s and computes the average av s and variance var s of the subsampled regions. figure 2
"different kinds of surfaces reflect radiating electromagnetic waves (e.m.) in different ways, due to the chemical composition, texture, color, roughness and moisture [cit] . this means that all the earth's surface features, including minerals, vegetation, dry soil, water and snow, have unique spectral reflectance signatures. hence, the spectral signature of different surfaces (e.g. soil, water, ice, vegetation) can contain informa- (x, y) tion to distinguish the different surface objects. imaging spectroscopy [cit], also known as hyperspectral imaging, is concerned with the analysis and interpretation of spectral signatures of hyperspectral data acquired from a given scene. this kind of analysis can be used to detect slight changes in vegetation, soil, water and mineral reflectance [cit] ). hyperspectral imaging is attracting growing interest in applications such as urban planning, agriculture, forestry and monitoring [cit] . in particular, hyperspectral image classification produces thematic maps from hyperspectral data. a thematic map represents the earth's surface objects. its construction implies that themes or categories, selected for the map, are distinguished in the remote sensed image. classification assigns a known class (theme) to each pixel (imagery data example). every pixel is expressed with a vector space model that represents the spectral signature as a vector of numeric features (namely spectral features) and is also associated with a specific position in a uniform grid, which describes the spatial arrangement of the scene. it is assigned a certain (possibly unknown) spectral response, i.e. class label."
1. is the defined transductive schema more accurate than the base inductive learner and the traditional transductive approaches that do not use collective inference (see sect. the experiments are run on a xeon 2.4 ghz 2 core processor.
"s-svm adopts a two-level learning schema. in the first level, a classifier is induced from the labeled set with the spectral features. it is used to predict labels of the unlabeled set. in it is used to finally classify the unlabeled part of the image. i + c is a fully inductive variant of s 2 tec. the learning phase is performed by using the labeled part of the image only. each classifier is induced from the labeled part of the image; both the frequency features and the morphological features of the labeled examples are constructed through collective inference by considering examples of the labeled set only. three classifiers are induced with the features of the three considered profiles (spectral, spatialfrequency and spatial-morphology). the ensemble of these classifiers is used to classify the unlabeled set with the majority rule. as unlabeled data are considered neither to learn the classifiers nor to construct the relational features, the iterative schema is left out of this case."
"the paper is organized as follows. the next section clarifies the motivation and the contribution of this paper. section 3 reports the basics of the presented algorithm. section 4 illustrates related work. section 5 presents the proposed algorithm, while sect. 6 reports the analysis of the learning complexity. section 7 describes the datasets, the experimental setup and reports the results. finally, in sect. 8 some conclusions are drawn."
"recent research in relational data mining has explored the use of the collective inference paradigm to exploit data correlation when learning predictive models. [cit], collective inference refers to the combined classification of a set of correlated instances. in contrast to traditional algorithms which label data instances individually regardless of correlations among the instances, collective inference predicts the labels of instances simultaneously and exploits correlations among the instances. in hyperspectral imagery classification, collective inference offers a unique opportunity to explicitly account for the spatial variation of the spectral signature, by reducing the labeling uncertainty that may exist when only spectral information is used and helping to overcome the salt and pepper appearance of the classification [cit] b) ."
"rauball & winter [cit] present a formal model of landmark saliency for human travellers consisting of visual attraction, structural attraction and semantic attraction components. their visual attraction component is what is usually seen in robot saliency architectures [cit] . however, their structural attraction component allows the definition of the spatial compactness criteria for tsgs. their semantic attraction component supports a well-defined communication channel for more general and task-related landmark selection, allowing different landmark selection criteria to apply when exploring, constructing a quick topological map of a new area, or constructed a metric map for a local region."
"1. the pixels of the unlabeled set u are initially labeled (algorithm 1, lines 1-2), by using the spectral classifier learned from the labeled set l, as it is originally described in the image in the space of spectral features s. 2. the spatial neighborhood structure of the imagery data d is constructed (algorithm 1, line 3). for each pixel, a set of square-shaped spatial neighborhoods is built and associated to the pixel. each neighborhood is constructed with a specified radius. the set of radius values (radius set) is a user-defined parameter. 3. the spatio-relational features are constructed to synthesize the information on the spatial variation of the class labels over spatial neighborhoods (algorithm 1, lines 4-5). to initialize these features, the real labels are associated with the pixels of the labeled set l, while the labels predicted by the initial spectral classifier (see step 1 of this initialization phase) are associated with the pixels of the unlabeled set u (see fig. 4 ). the constructed features are used to populate the frequency profile (f) and the morphology profile (m) of both l and u."
"collective inference, transductive learning and ensemble learning have been explored already in the literature. however, to the best of our knowledge, this is the first study that combines these three strategies in a single learning framework. this framework, which represents one of the main contributions of this work, proves effective for the challenging problem of hyperspectral classification. another contribution is the investigation of various application-specific relational operators to define a collective classification setting. we use both operators to describe the class frequency and operators to describe the class morphol-ogy of a hyperspectral image. although these operators have been already investigated in the hyperspectral classification literature [cit], they have been considered separately. in this study, inspired by the fact that they convey different kinds of information, we consider their combined use. indeed, the frequency information is computed to quantitatively describe the label structure making a spatial average (a sort of \"low-pass\" filtering), while the morphology information is computed to qualitatively follow the borders separating land cover types (a sort of \"high-pass\" filtering)."
"appearance-based approaches to landmark recognition include zhang and kosecka [cit] representing images of buildings using localized color histograms collected along the vanishing directions, and cummins & newman [cit] employing a surf-based, bag-of-words approach for mapping and localization. [cit] show that a combination of depth and appearance information can be a powerful tool for landmark recognition to implement loopclosure for outdoor slam."
"on the other hand, the morphological operators construct 4·k features, one feature for each morphological operator (erosion, dilation, opening and closing) and for each class label c i . they use spatial neighborhoods as structuring elements. the erosion and dilation of a class label destroy (fig. 3a-d) and enhance ( fig. 3e-h ), respectively the structure and the density of borders separating land cover types present in the structuring element [cit] . erosion e( p, r, c i ) is true if all pixels within n ( p, r) have label c i fig. 2c ), while dilation d( p, r, c i ) is true if at least one pixel within n ( p, r) has label c i (see fig. 2d ). the opening and closing operators are combinations of erosion and dilation. opening is erosion followed by dilation. it recovers most structures of the original image, i.e. structures that were not removed by the erosion and are bigger than the structuring element. closing is dilation followed by erosion. with opening or closing we can obtain objects of the image which are larger or smaller than the structuring element [cit] )."
"hence, this work is relevant for the relational learning community as it contributes to assessing that combining collective classification and relational features can gain improvements over a propopositional/non collective setting. these improvements are shown to be relevant in an applicative context (remote sensing) that has recently gained importance. this work is also significant for the hyperspectral image classification community, as it describes an algorithm that deals with spectral and spatial information by gaining accuracy with respect to state-of-the-art algorithms."
"the semantic attractiveness is the settings for the masks, thresholds and weight parameters for the visual and structural attractiveness modules. rather than having these be fixed values, or 'tuning' parameters hidden in the architecture, we have chosen to make these explicitly visible so that lsa's selection of landmarks can be modified by the needs of the task at hand. these are the preference inputs mentioned in the abstract."
"the application domain considered in this paper consists of team of robots deployed to cooperatively generate a map of a specific area: an area under reconnaissance or an urban disaster site, for example. the objective is to generate an accurate map showing hazards, obstacles, traversable routes, etc., very quickly and to communicate it back to a command center. this map will then be used by a combination of human and robot teams for effective operations in the mapped area."
"number of iterations we start by studying the performance of s 2 tec along the dimension of the number of performed iterations. this analysis is performed by considering the same samples of the comparative study presented in sect. 7.2. the accuracy metrics (oa, aa and κ), the computation time (in s) and the memory usage (in mb) are the plots in fig. 6a c. we observe that accuracy is gained as new iterations are performed. this confirms the effectiveness of the iterative learning approach. we can also observe that the highest accuracy gain is obtained in the initial iterations of the learning process, which are also those showing the highest increment in the usage of the time-memory resources consumed by the process."
"following rauball & winter [cit] 's formal model of landmark saliency for human travelers, we consider the saliency of a landmark to consist of three components:"
"in this study, spatio-relational features are constructed by resorting to a collective inference procedure, in order to express the label of a target pixel depending on the labels of all the related (task-relevant) neighbors of the target pixel. these features are formed through the application of the frequency-based operator [cit] and/or the morphology-based operators [cit] . given a target imagery pixel p and its spatial neighborhood n ( p, r) (with radius r), the frequency operator constructs k features, one feature for each class label c i . the value of the feature is proportional to the pixels within n ( p, r) that have label c i (see fig. 2b )."
"we run s 2 tec by setting the percentage of pixels labeled in the image equal to 5 %, 6 and the size of spatial neighborhoods equal to 5-10, 15 and 20, respectively. we compare s 2 tec to the inductive svm, to the fast linear tranductive svm (svmlin) [cit] and to the spectral graph transducer (sgt) [cit] ) (see a description in sect. 4.3). [cit] and with the number of neighbors k ranging between 25, 50 and 100. the inductive svm, as well as the transductive svmlin and sgt are all defined for binary classification problems. we use the \"one-against-all\" strategy, already adopted in s 2 tec, in order to adapt these binary transductive classifiers to the multi-class problem."
"finally, we illustrate some considerations concerning the spatial distribution of misclassified pixels. we compare the classification maps built by both s 2 tec (fig. 5d-f ) and its inductive svm counterpart (fig. 5g-i) . these maps show that s 2 tec takes advantage of the presented spectral-relational methodology. it gains accuracy when discriminating objects of interest on the map, by reducing visibly the salt-and-pepper distribution of pixels misclassified by the base svm learner. for example, we can note that s 2 tec diminishes visibly the number of pixels of the class \"vineyard untrained\" that the base svm learner wrongly detects as part of the object \"grapes untrained\" in the salinas valley (see fig. 5f-i ). this result is in agreement with the f-1 scores reported in table 2, which show that both the inductive svm learner and the transductive competitor learners (svmlin and sgt) perform poorly when labeling pixels of the classes \"grapes untrained\" and \"vineyard untrained\". on the other hand, the pixels misclassified by s 2 tec generally fall into the margin bound between homogeneously, well-classified zones, that is, where enhancing the classification accuracy with the spatial separability among classes is a more difficult task."
"this result supports our previous results [cit] for terrain spatiograms. however, this paper's results were based on unshared, single view, and non-occluded landmarks. future work will need to evaluate lsa used to build multiple view landmarks and to share landmarks. this latter is not trivial since the landmarks will need to appear salient on both robot platforms. the interaction of lsa with occluded landmarks may also be an issue, since both occluder and landmark may need to appear salient."
"we evaluate the performance of the compared algorithms in terms of overall accuracy, average accuracy and cohen's kappa coefficient. we also analyze the learning time (in seconds), the maximum number of iterations performed to complete the task and the peak of memory usage (in megabytes) during learning. as five partitioning trials between labeled and unlabeled sets are generated, the results are always averaged across these trials."
"the objects around the lot were mostly natural occupants of the area augmented with some additional candidate objects. a key issue for place detection in topological mapping and in loop-closure for slam is perceptual aliasing [cit] -for this reason a number of similar appearing landmarks were chosen: the garbage bins in figure 6 (a), 6(c) and 6(h). additional candidate landmark objects included a large compressor (fig. 6(e) ) and a yellow sign (fig. 6(f) ). in total, lsa extracted eight landmarks at a variety of poses and scales, some of which are shown in figure 6 . between four and ten poses for each landmark were generated."
"we investigate how the classification accuracy can be influenced by the several learning components (i.e. svm kernels, feature profiles, transductive learning, ensemble learning and iterative collective inference) that contribute to the definition of s 2 tec. by combining these components differently, we define several learning frameworks, whose characteristics are summarized in table 4 . s 2 tec-linear is equivalent to algorithm s 2 tec with the svms learned by considering the linear kernel in place of the gaussian kernel."
"several algorithms, which have been designed in the hyperspectral image classification literature, have been evaluated by considering indian pines, pavia university and/or salinas valley datasets. in this study, we consider the most recent (and competitive) results produced by investigating both transductive svms and spectro-spatial classifiers in these data scenarios."
"to illustrate the difficulty of the landmark recognition problem with this data set, two other approaches to landmark recognition from lsa results were used: a template-based approach and an image histogram based approach."
"finally, we observe that s 2 tec can produce the highest accuracy in table 5 only by learning svms with gaussian kernels, using both spectral and multiple spatial profiles, as well as performing transductive learning, ensemble learning and iterative collective inference in the defined learning framework."
"the iterative convergence approaches are investigated in many studies [cit] . they account for the correlation of labels and compute the label of an instance depending on the labels of all its related neighbors. in particular, they express an instance by combining the instance features and the relational features constructed by using the labels of all the related neighbors of the instance. the relational features are computed by using an aggregation function over the neighbors, such as count, mode and proportion. based on the descriptive features and the relational features, an algorithm trains a classifier and iteratively updates the predictions of all instances, by using the predictions for instances with known labels. this process continues until the algorithm converges. [cit] have recently described an iterative convergence algorithm to deal with multi-label classification problems. finally, collective classification has been recently investigated in combination semi-supervised and transductive learning [cit] ."
"fourthly, the comparison between s 2 tec, i + c and i + c + i allows us to perform the analysis of the presented algorithm as a function of the learning setting adopted (inductive learning vs transductive learning). we compare classification accuracies produced with classifiers learned in the inductive setting and collective inference performed in the inductive setting (i + c), to classification accuracies produced with classifiers learned in the inductive setting and collective inference performed with iterative learning in the transductive setting (i + c + i). we also compare them to classification accuracies produced with classifiers learned in the transductive setting and collective inference performed in the transductive setting (s 2 tec). the results (rows 2, 6-7, table 5) show that the highest accuracy is achieved when the learning process is completed in a purely transductive setting (s 2 tec), while the lowest accuracy is achieved when the learning process is completed in a purely inductive setting (i + c). [cit] that learning classifiers by accounting for both labeled and unlabeled data contribute to improving accuracy."
"the high-dimensionality of spectral data and the small number of ground truth labels can cause problems such as a reduction in classification accuracy. this behavior is known as hughes' phenomenon [cit] . in particular, the limited ground-truth samples are not always sufficient for a reliable estimate of the classifier's parameters. in fact, if the number of samples (training set) is too low compared to the number of variables, we risk overfitting the training data, i.e. we can learn a model that exactly fits the training data without accounting for a wider generalization [cit] ."
"global (mf and lbp) and local (ica and gs) [cit] . this study has revealed that global approaches achieve, in several cases, a better accuracy than local ones. however, global approaches are also the most difficult to work with in learning and inference. in particular, choosing the initial weights, so that they will converge during training, is nontrivial. the most trouble is observed with mf, which may be unable to converge or, when it does, it may not converge to the global optimum. this analysis is consistent with previous work [cit] [cit], it should be taken into consideration when choosing to apply these algorithms. [cit] have also concluded that ica and gs can produce very similar results, but ica is much faster than gs. these considerations motivate our preference towards implementing collective inference through iterative convergence learning."
"thirdly, the comparison between s 2 tec, s-svm and i + c allows us to evaluate the contribution of iterative learning in combination with collective inference. in the compared frameworks, collective inference is performed by learning the relational features constructed by using the labels of the related neighbors of the instance. both s-svm and i + c do not perform iterative learning, while s 2 tec resorts to iterative learning for collective inference. the results (rows 1, 5-6, table 5) show that the use of iterative learning really improves the classification accuracy. this confirms the results of previous studies [cit] in collective inference, which have assessed the effectiveness of iterative learning, in order to account for the correlation of labels."
"in this paper, we propose a new transductive hyperspectral image classification algorithm, that integrates spectral and spatio-relational features into an ensemble system developed with an iterative convergence algorithm. according to our knowledge, this represents an innovative contribution in the related field, showing that collective inference can be used with transductive learning and ensemble learning, in order to enhance the accuracy of traditional classifiers. in this context, collective inference is used to account for spatial autocorrelation of labels, transductive learning is considered to deal with sparsely labeled data, while ensemble learning is aopted to deal with spectral and spatial information. the empirical study proves that the presented methodology outperforms traditional classifiers, transductive classifiers and several spatio-spectral algorithms proposed in the hyperspectral image classification literature. however, it still fails to correctly classify some pixels along the margin bound between homogeneously, well classified zones. some directions for further work are still to be explored. new learning systems, such as co-training, can be investigated as an alternative to ensemble. at the same time, active learning solutions can be explored, in order to \"intelligently\" augment the labeled set along the iterative process. in addition, we can account for the conclusions suggested by the analysis of hyperspectral competitors and try to improve classification accuracy of minority classes by resorting to markov random fields. on the other hand, we can also explore the possibility of performing collective inference through gibbs sampling (as an alternative to iterative convergence learning) also in combination with markov random fields and/or active learning. additionally, it would be interesting to study new ways to adaptively determine both the size and the shape of spatial neighborhoods. finally, the algorithm combines principles of both collective inference and transduction, in order to address a classification task in the relational setting. although it is specifically designed for hyperspectral imagery classification, the algorithm can be considered general-purpose once the schema to determine relational profiles of data are established. a future piece of work will focus on the definition of a general schema to determine relational profiles of data with correlation. in this way, we will be able to investigate the effectiveness of the proposed algorithm in new application environments. in any case, this is out of the scope of the presented manuscript."
"ivc + i is a version of s 2 tec, which learns classifiers in the inductive setting, but performs collective inference with the iterative schema in the transductive setting. similarly to i + c, three classifiers are learned, in the inductive setting. features of the spatial profiles (frequency and morphology) are constructed iteratively by using the entire dataset. thus, differently from i + c, new classifiers can be iteratively learned from the spatial data profiles, even staying in table 5 analysis of learning components (indian pines): the accuracy metrics are collected on five trials, the average (sd) of the measures is computed on these trials"
"s 2 tec(s + f) is equivalent to algorithm s 2 tec with the spectral profile and only the frequency profile considered for populating the ensemble. s 2 tec(s + m) is equivalent to s 2 tec with the spectral profile and only the morphology profile considered for populating the ensemble. in both frameworks, a pixel is a consensus pixel for the ensemble, if both classifiers of the ensemble assign the same label to the pixel."
"using 21 stereo datasets collected with lsa, we show that the terrain spatiogram approach can effectively recognize landmarks in a range of poses and scales. an image template matching approach and a color histogram matching approach is applied to the same dataset with inferior results. this paper is laid out as follows. previous work is reviewed in section ii. in section iii, the landmark saliency architecture (lsa) is introduced. we recap the terrain spatiogram notation in section iv. experimental procedure and results are reported in section v followed by discussion and conclusions in section vi."
"single gaussian tsgs where generated for each lsa landmark candidate extracted (46 tsgs in total). these were filtered to the group of three best matches per landmark provided the match was above 0.6 (to eliminate poor landmarks). this resulted in one landmark candidate being discarded at all poses, leaving seven reliable landmarks, each with three poses. all the images in figure 6 are best poses."
"in previous work [cit], we have proposed a combined image and terrain spatial representation for landmarks, the terrain spatiogram. however, in that work, the input images were manually windowed. in this paper, we introduce a saliency-based architecture, lsa, for automatically generating candidate landmarks. lsa follows a model of landmark saliency initially proposed by rauball & winter [cit] for human way-finding."
"we evaluate the accuracy of the algorithms in terms of overall accuracy (oa), average accuracy (aa) and cohen's kappa coefficient (κ) [cit] . in addition, we analyze the f-1 score of predictions performed for each class. for each dataset, the labeled pixels are randomly selected from the available ground truth of the image by using the stratified random sampling without replacement; 7 the remaining pixels are used as the unlabeled part of the learning process. five partitioning trials between labeled and unlabeled sets are generated; metrics are averaged on these trials."
"closely related to the application context of this study are relational learning algorithms investigated to process document images, images from medical domains and images from one's daily life. [cit] have proposed multi-relational data mining algorithms to account for spatial dimension of page layout when recognizing semantically relevant components in the layout extracted from a document image. [cit] have applied inductive logic programming to construct concept descriptions from x-ray angiograms and classify types of blood vessels. [cit] have investigated how spatial relational representations can be generated from images and defined using a logical background theory (i.e. a set of prolog rules, as in relational learning). as an example, we can consider the relation \"close aligned horizontally to the right\" and the relation \"part of\". this relational knowledge is then used to recognize higher-level structures in images from daily life."
"methodologically, the automatic classification of hyperspectral data is non-trivial due to factors such as the spatial correlation of the spectral features, the high cost of labeling the data and the large number of spectral bands [cit] . in this paper, we propose a novel transductive collective classifier for dealing with all these factors in hyperspectral image classification. collective inference exploits the spatial correlation between spectral responses (class labels) of neighboring pixels by simultaneously labeling interacting pixels. the transductive inference paradigm allows us to reduce the inference error for the given set of unlabeled data, as sparsely labeled pixels are learned by accounting for both labeled and unlabeled information."
"the vector of spectral features is input as part of the hyperspectral imagery dataset and used to populate the spectral data profile s. the relational features are constructed by coupling imagery pixels with square-shaped spatial neighborhoods and synthesizing information on the spatial variation of labels over the imagery pixels of each neighborhood (see details in sect. 3). relational features are then used to populate the spatio-relational data profiles of the dataset. two application-specific spatio-relational profiles are constructed (see details in sect. 3), namely the frequency data profile f and the morphological data profile m. the frequency data profile is the vector of the spatio-relational features which are constructed, for every pixel p, by applying the frequency operator with every class label and every spatial neighborhood coupled with p. the spatial morphological data profile is the vector of the spatio-relational features which are constructed, for every pixel p, by applying the morphological operators (dilation, erosion, opening and closing), with every class label and every spatial neighborhood coupled with p."
"in general, the analysis of accuracies highlights that, although transductive inference has been specifically defined to deal with scarce labels [cit], it can be unsatisfactory for gaining accuracy in the data imagery scenario. on the contrary, coupling transductive inference with collective inference and ensemble learning can really improve accuracy in the identification of the pixels belonging to the different classes. the classification of pixels belonging to minority classes remains a problem, that requires further investigation."
"a top-level description of the iterative convergence learning is given in algorithm 1. the algorithm comprises an initialization phase and an iterative phase. both phases concern an ensemble system that comprises three classifiers. these classifiers are learned iteratively from the spectral profile s (classi f ier s), the spatial (-frequency) profile f (classi f ier f) and the spatial (-morphological) profile m (classi f ier m) of the imagery data, respectively. the unlabeled part of the image is initially classified according to the classifier (classi f ier s) learned from the original labeled part l of the image with the features in the spectral profile s. both real labels and predicted labels are used to initialize the features of the two spatial profiles f and m, respectively. at each iteration, pixels of the unlabeled part u, which are identically predicted by the majority of classifiers in the present ensemble, 1 are definitively classified with the majority class of the ensemble and transferred to the labeled part l of the dataset. spatio-relational features of the two spatial profiles are updated accordingly. a detailed description of the two phases is reported in the following."
"size of the initial labeled set and size of the spatial neighborhoods then we proceed by studying the performance of s 2 tec as a function of the size of the initial labeled set, as well as a function of the number and size of the spatial neighborhoods. the average and the sd of the accuracy metrics, the memory usage peak (mb) and the number of performed iterations are reported in table 3 . we observe that, as expected, the classifier gains accuracy by augmenting the number of pixels in the originally labeled set (rows 2-4, table 3 ). in addition, the sd of the accuracy metrics decreases as the number of the initially labeled pixels increases in the experiment. this result is in agreement with the literature [cit] b ). on the other hand, the classifier gains accuracy by enlarging the size of the spatial neighborhoods (rows (y axis, a), the computation time (y axis, b), and the memory usage peak (y axis, c) are plotted along the dimension of the number of performed iterations (x axis). s 2 tec is run by considering the labeled sets generated by sampling 5 % of pixels and by constructing the relational features over the spatial neighbourhoods with size growing from 5, 10, 15 to 20. results generated on five trials are averaged s 2 tec is run by setting the neighborhood sizes equal to 5-10-15-20 when varying the labeled set percentage between 3, 5 and 10 % and by setting the labeled set percentage equal to 5 % when varying the neighborhood sizes between 5-10-15, 5-10-15-20 and 5-10-15-20-25. \"*\" is used to mark the parameter setup of the specific configuration used in the comparative study 6-8, table 3 ), as in this way we increase the chances of building relational features that better match the spatial variation of classes, even when classes vary over space with different density and texture. additionally, the sd of these accuracy metrics, generally, assumes low values. the learning process is completed in five iterations on average regardless of both the number and the size of the spatial neighborhoods used to construct the spatio-relational features, as well as of the size of the initial labeled set. finally, the memory usage is mainly influenced by the size and the number of neighborhoods constructed. the higher the number of neighbors processed through collective inference, the greater the amount of memory consumed by the learning process. the memory usage is approximately stable with respect to the size of the initial training set."
"we show that landmarks selected by lsa can be recognized reliably when represented as tsg landmarks. however, when template matching or image histogram approaches are used, the recognition is less reliable."
"we compare the overall accuracy, average accuracy and cohen's kappa coefficient of these learning frameworks by constructing spatial neighborhoods with size 5, 10, 15, 20 and considering the initial labeled set sampled with the labeling percentage equal to 5 %. table 5 reports the accuracy metrics collected for the alternative learning frameworks described in table 4 . these results deserve several considerations."
"finally, selfsvm considers only spectral information. [cit] . initially, the svm is induced from the labeled part with the spectral features. this classifier is used to label all the pixels of the unlabeled set. subsequently, the training set is the \"entire\" dataset with real labels associated with examples of the labeled part and predicted labels associated with examples of the unlabeled part. iteratively, the svm is learned from this training set and used to re-predict labels of the unlabeled part. the algorithm stops when the number of classifications changed by the svm is less than a threshold (10 in this study)."
"the visual attraction module of lsa is shown in figure inverts the values on that plane (e.g., change from highsaliency red to high saliency green in i c ), 0 masks that plane (e.g., mask width and high and process only depth in i d ), and 1 passes that plane unchanged."
"steps (1), (2) and (3) are part of the initialization phase, while steps (4), (5) and (6) are part of the iterative convergence learning phase, thus they occur n i ter times. the time cost of learning the spectral classifier from the initial labeled set l is o (λ(n l, m) ). the time cost for constructing the neighborhood structure of the imagery data is n · (4r 2 max + 4r max + 1), that is, o(n r 2 max ). the time cost of constructing the relational features by using both the frequency operator and the morphological operators is 5 · k ·r · (4r 2 max + 4r max + 1) · n, that is, o(kr r 2 max n ). therefore, the time complexity of the initialization phase"
"in opposition to reachability judgment and egocentric distance estimation, allocentric length estimation was not influenced by the concurrent actions. this suggests that allocentric length estimation does not rely on motor simulation processes or body representation. an effect of stimulus magnitude was observed with participant's overestimating short compared to long lengths. this effect can be attributed to a contraction bias described in the literature, where participants have their magnitude estimations pulled toward the stimuli range center, leading to the underestimation of large and the overestimation of small stimuli [cit] . additionally, an effect of position was observed, where participants made faster and more accurate responses to targets positioned near compared to far from them. more specifically, participants reported far positioned lengths as being longer than near positioned lengths despite the fact that the stimuli were identical in length. this effect may be due to a compensation of size constancy in depth where objects presented further from participants appear smaller [cit] . this effect could also be due to a ponzo illusion [cit], where identical lines are perceived as different when placed within a triangle or a trapezoid shape [cit] . in our experimental set up, even if the table was rectangular, it appeared as a trapezium for participants (i.e., near edge is visually larger than the far edge of the table) perhaps causing ponzo illusion-like effect on the stimuli in this task."
"task order was counterbalanced across the participants. each task consisted of three blocks of trials, each with a different dualtask condition (order of dual-tasks was also counterbalanced). each block consisted of 16 different stimuli repeated four times, resulting in 64 randomized trials per block. each trial started with a beep sound lasting 700 ms, then a stimulus displayed until participants responded, and finally, a blank screen for 1000 ms. in all tasks, the participant had to respond as fast as possible while keeping errors to a minimum. each task started with a small practice session to make sure that participants fully understood the instructions and experimental set up, and that they performed the different dual-task actions at a specific frequency paced by a metronome. at the end of the experiment, the experimenter measured the height of participant's eyes, the length of their arms (from neck base to the edge of the middle finger) and their actual reaching limit (the participants furthest reach while being seated on the chair)."
"for the reachability judgment and egocentric distance estimation tasks, the 16 distances were averaged into four different distance categories (i.e., very close with distances 35; 55; 65; 75; close with distances 80; 85; 87.5; 90; far with distances 92.5; 95; 100; 105; and very far with distances 115; 125; 145; 165). repeated measures analyses of variance (anovas) were conducted with distance categories and dual-task conditions (i.e., no actions; arm actions or hand actions) as within-subject factors. for the allocentric length estimation task, two length categories were formed from the eight different stimuli (short: 8; 16; 24; 32; long: 40; 48; 56; 64) . repeated measures anovas were conducted with length categories, the position from the participant (70 cm vs. 130 cm) and the dual-task conditions as within subject factors. the dependent variables were response latency and accuracy. for the reachability judgment, accuracy was computed on the basis of the participant's real reach capability measured at the end of experiment. for each stimuli distance, a trial was considered as an error when participants under-estimated (i.e., responded that they could not reach a target when they could) or overestimated (i.e., responded that they could reach a target when they could not) their reachability. to assess accuracy for egocentric distance and allocentric length estimations independently of the magnitude of the target to be estimated, an error rate was computed by subtracting the actual distance from the participant's response and dividing this difference by the actual distance (i.e., [(participant's response − actual distance)/actual distance]; where a positive value indicates overestimation, a negative value indicates underestimation, and a value of 0 [cit] ) ."
"visually determining whether an object is at a reachable distance is thought to rely on pre-reflective representations of body capacities for action [cit] . in reachability judgments tasks, it has been shown that perceived reaching limit can be influenced by the manipulation of action capability, with postural or environmental constrains [cit], height or position of the table where stimuli are presented [cit], or participants wearing weights on the wrist [cit] . for example, hiding participants' hands and providing them a biased visual feedback about the end-point location of their pointing movement has been used to modify a person's perceived action capacity, and the manipulation has shown a moderation to perceived reachable space [cit] . in a second example, a study used a motor constraint paradigm (i.e., by blocking the arms of participants) and showed response speed and accuracy interference for spatial localization decisions of stimuli within peripersonal space [cit] . further, physical (non-manipulated) differences in action capability such as handedness and visual laterality of target placement can also moderate reachability judgments [cit],b) . finally, motor disruption, for example through the use of transcranial magnetic stimulation (tms) applied over the hand motor cortex of the left hemisphere, was shown to moderate response latencies in reachability judgments, particularly for stimuli positioned near the boundary of peripersonal space [cit] . therefore, together, these effects demonstrate that moderations that normally influence action, also influence judgments of reachability, suggesting that reachability may be based on action representations that are constrained by the context in which the action could be performed [cit] ."
"where l f h(x) and l g h(x) denote the lie derivative of the output function h(x), with respect to f (x) and g(x) respectively. then"
"bonferroni correction ( bc ) was applied where multiple post hoc comparisons were used. data from unreliable trials (no response or microphone failures), and outlier responses (on which response latency was above or below 2.5 standard deviations from the overall mean) were excluded from the analyses. this led to the removal of 2.6, 1.0, and 4.2% of unreliable trials, and 2.8, 2.3, and 1.3% of outlier response latencies from the total trials of the reachability judgment, egocentric distance estimation and allocentric length estimation tasks respectively."
"where r m denotes the armature resistance, k τ represents the torque constant of the motor and the control input of the system is denoted by v (t). with this last relation, the dynamics of the flexible-joint robot, are given by"
"the apparatus consisted of a projector placed above a white table that was 215 cm long, 122 cm wide and 70 cm high. black curtains surrounded the table in order to isolate the experimental environment from the rest of the room and reduce distractions. participants sat on a chair situated in the middle of the small edge of the table and a microphone was placed above their head in order to record response latencies. the stimuli were composed of white rectangles (5 cm width and 2.5 cm length) displayed on a black background at various locations on the table (see figure 1) . a customized e-prime program [cit] was used to display the stimuli on the table and to control the experimental procedures."
"the first step in this strategy, is to determine the relative degree of the system, and in order to determine it, we must first compute the derivative of the output function, defined aṡ"
"altogether, these findings suggest that space perception benefits and is scaled to the representations of the body and its capacities. however, whether internal simulated actions do contribute to the perception of spatial distance is still an open question [cit] . the goal of the present study was to examine the contribution of action representations in both reachability and distance perception behaviors by investigating whether a concurrent motor task that may disrupt internal action representations would influence the perception of space. to assess this, participants completed three different spatial perceptual tasks (i.e., reachability, egocentric distance and allocentric length estimation) while performing concurrent hand (i.e., foam ball squeezing; [cit] ) or arm (i.e., weight lifting) actions in a within-participant design. with this manipulation, we tested whether similar interference of the concurrent actions would be observed in the reachability judgment task and the egocentric distance estimation task. we propose that these two tasks should show similar patterns of response time interference as, in previous studies, the manipulation of reach capacities influenced distance estimation [cit] . in contrast, no dual-task moderation is expected in the allocentric length estimation task. indeed, we argue that allocentric length estimation does not involve spatial localization relative to the body (or body referencing) and that action simulation processes are thus not recruited in this task. for the reachability judgment task, we predict a typical increase of response latency and error rate for targets placed near the boundary of peripersonal space [cit] . moreover, an interaction between target location and dual-task for response latencies is expected in the reachability judgment task, with a stronger action dual-task effect for stimuli placed near the boundary of peripersonal space [cit] ."
"where k b represents the kinetic energy of the rotating base, and k a represents the kinetic energy of the arm. the potential energy of the system satisfies the relation"
"in this work, the tracking control problem of flexible joint robot was addressed. two control laws are proposed for the task of trajectory tracking. two nonlinear control schemes, one model based and another one of discontinuous class were selected to increase the performance of the classic schemes, with the aim of avoiding oscillations on the robot arm tip. state space dynamics description of flexible joint is used in order to design the nonlinear control schemes. by dynamic feedback linearization methodology, the robot dynamics was modified into a linear and controllable chain of integrators, to be then controlled by a full state feedback control. in order to reject nonlinearities in the state space description, while avoiding oscillation effect in the flexible joint robot arm, a sliding mode control was proposed. the experimental results show that the proposed nonlinear control laws, feedback linearization and slide mode control, are efficient in achieving precise moving of the robot if the dynamic model is exactly know, but the performance of both controllers decrease when unmodeled dynamics and disturbance are applied to the system. to increase the robustness we implemented an integral compensator for feedback linearization controller, in the same way we introduce a integral compensator in the sliding manifold. the experimental results show more efficient result with bounded error, where slide mode control has the best performance. an extension of the problem may consist in the use of schemes based in compensator networks. another suggestion for further study consists in the use of generalized proportional integral control."
"the tracking error e yfl and e yifl of the system are shown in figure 9, where can observe, that the position error is restricted to a small vicinity and it is seen to be uniformly bounded, in the steady state, and it is smaller in comparison to e ypid (see, figure 6 ). the control voltage v fl (t) and v ifl (t) is depicted in figure 10 . in contrast of the pid control voltage, here the peak voltage is smaller and cancel out the undesirable oscillations."
"all participants were required to perform three different tasks: reachability judgment, egocentric distance estimation, and allocentric length estimation. the reachability judgment and egocentric distance estimation tasks were made to the same stimuli. rectangular shapes were projected on the tabletop at 16 different distances along the participant's sagittal body-midline axis (35; 55; 65; 75; 80; 85; 87.5; 90; 92.5; 95; 100; 105; 115; 125; 145; and 165 cm), such that approximately half of the stimuli was placed within reach and the other half out of reach, with more closely spaced stimuli placed at the boundary of reach space. in the reachability judgment task, participants were asked to judge whether they could touch the stimuli displayed on the table without actually performing any reaching actions. they responded aloud \"yes\" if they thought that they could touch the rectangle, or \"no\" if they thought that the stimulus was out of reach. it was explicitly mentioned that they could imagine themselves leaning forward, but that their bottom could not leave the chair in their action simulation. moreover, they were asked to keep their back against the chair backrest during the entire experiment. in the egocentric distance estimation task, participants were asked to estimate the distance in centimeters separating them from the rectangular stimulus. in the allocentric length estimation task, they estimated the distance separating two rectangles presented on the tabletop. the two rectangles were presented either at 70 or 130 cm from the participant (i.e., within and out of reach space), and there were eight possible lengths between the rectangles (8; 16; 24; 32; 40; 48; 56; and 64 cm) . the two rectangles were always equidistant from the center of the table compared to the participant's sagittal body mid-line. within each task, the participants performed three different conditions of dual-task (run in separate trial blocks and counterbalanced within the tasks). participants were either instructed to simply place their hands on the edge of the table (baseline condition), to perform arm actions (i.e., from fully laterally outstretch arm span to the flexion of elbows with the hands above the shoulders) with one-kilogram weights (arm action condition), or to perform foam ball squeezing hand actions placing their arms alongside their body (hand action condition). they were also trained with a metronome in order to perform the different actions at a specific rate (i.e., 40 per min)."
"in the present study, the effect of the dual-task was limited to response latency effects. our analyses showed that the dual-task conditions had no influence on actual reachability judgments or distance estimates. this finding is consistent with previous dualtask experiments [cit], where it was reported that squeezing soft balls at the same time as estimating the distance of targets showed no moderation of the reported distances when no tool was present. this suggests that represented body metrics are not modified by the concomitant execution of actions, as no extension or reduction of perceived peripersonal space was observed. another interesting finding was that the response latency of the ball squeezing dual-task differed for reachability judgment and egocentric distance estimation. for reachability judgment, responses were slowed specifically for targets in the peripersonal space whereas the dual-task slowed responses to all target positions for egocentric distance estimation. these two findings suggest that while common motor resources are recruited for the two tasks, the manner in which those motor resources are used might be different. for reachability judgments, responses are particularly slowed for peripersonal targets close to the boundary between peripersonal and extrapersonal space. this could be explained through inefficiency or difficulty in the use of action simulation when at the limit of the participant's capability. for egocentric distance estimation however, the perceptual response latencies were similar for all targets, irrespective of whether they were within or outside of reach."
"in this section the instrumentation and set up of the flexible-joint robotic arm prototype are presented. the designed control laws were, both, implemented in matlab-simulink through a data acquisition device. some rest to rest tracking results by using a classic controller, as well as the proposed schemes are also presented to illustrate the effectiveness of the proposals. figure 2 shows the diagram of the experimental platform used to control the flexible-joint robotic arm system. the experimental device is shown in figure 3 . it consists of a dc motor nisca: model nc5475, which drives a rotating base through a synchronous belt and pulley system with a 16:1 ratio. the main arm is attached to the rotating base by two identical springs, resulting in a flexible joint. both, the angular positions of the rotating base and the arm were measured with incremental optical encoders of 1000 pulses/revolution. the data acquisition is carried out through a data card sensoray model 626. this board, is responsible to read the signals from the optical incremental encoders and supplies control voltages to the power amplifiers. the control strategies were implemented in the, and the device stays in this position until the experiment ends. this trajectory takes 18 seconds to be completed and it will be used throughout the experimental tests."
"the control of flexible manipulators has been studied with great interest by, both, practical engineers as well as academic researchers. this is mainly because the application of control algorithms for rigid joint robotic manipulators is limited, and their direct application may lead to poor results. the nature of flexible joint systems is generated by pulleys, timing belts or harmonic drives causing a flexible joint phenomenon. on the other hand, the flexible link model effect can be arisen by increasing payload-to-weight ratio, motion speed or control bandwidth, see [cit] . in both cases, this class of systems increases its order, as well as, the difficulties to control its end-position."
"the classical first order sliding mode control methodology is a discontinuous robust feedback technique, whose main disadvantage, is the appearance of chattering [cit], which may rise due to neglected fast dynamics or to digital implementation issues [cit] . this phenomenon is characterized by high frequency oscillations at the output system that can be harmful when the controller is applied in motion control systems. in order to reduce this phenomenon and reject perturbations, a higher order sliding mode control is proposed [cit] . first of all, the following sliding manifold is proposed"
"is used to compare the performances obtained, with fl and sm controllers tested, figure 14 shows that ism control leads to a minimal performance index isi ism (t) in contrast to the results obtained with the isi sm (t), isi fl (t) and isi ifl (t) based controllers."
"the manuscript is organized as follows: section ii the dynamic model of the flexible joint robot arm under study and the problem formulation are presented. the design of the proposed non-linear control laws is developed in section iii. section iv shows the experimental results of the proposed control laws. a classical pid, is taken as a starting point for the controllers analysis behavior and the same schemes without the integral compensation are included to show the improvement of the presented proposal. concluding remarks end this contribution."
"in conclusion, our study supports the idea that internal representations of action contribute to the perception of external space [cit] . visually determining what is reachable engages the simulation of a motor act that can be interfered with a hand motor dual-task. moreover, we find that similar internal simulations of reach might also serve as a metric for egocentric distance perception. therefore, reachability and egocentric distance perception appear to be linked [cit], requiring overlapping reaching simulation cognitive processes. however, despite the finding that similar motor resources appeared to be recruited for both behaviors, it could be that these resources are used differently for each behavior. this motor contribution to egocentric space perception may require body referencing, and we argue that in allocentric space, no such motor contribution is recruited. these findings suggest that in order to perceive the environmental layout surrounding a person, the viewer not only represents perceived space from sensorial inputs, but they also simulate the potential body and action interactions within space, supporting an embodied view of space perception [cit] ."
"to improve the robustness property of the system in the presence of parametric variations, small parametric uncertainties in the model dynamics or external shocks, a compensator in the classic structural sense will be implemented, where one of the most popular and used is an integral compensation."
"in this contribution a single flexible joint robotic system, as shown in figure 1 the angular position of the arm tip is defined by the relation"
"large-scale wireless sensor networks consist of a large number of sensors being deployed in a target area to report predetermined events or transmit sensed data to the base station for further analysis [cit] . in the large-scale wireless sensor networks (lwsn), routing problem becomes more important and tricky because a number of sensors are involved and the sensors are limited in power and calculation capacity. therefore, routing algorithm for the large-scale wireless sensor networks should be capable of finding energy efficient paths and, at the same time, achieving an energy balance over the network to prolong the lifetime of sensor networks through a decentralized decision making scheme [cit] . moreover, the following two issues should be considered to design routing algorithm for large-scale wireless sensor networks. first, the routing algorithm should give a good performance regardless of event generation pattern. in fact, events could be generated at the sensor, randomly [cit], uniformly [cit] over the target area, or repeatedly [cit] at a specific part of the target area. even, event patterns can change from one type to another over time. therefore, the routing algorithm should be sufficiently robust for diverse event generation functions. second, a network topology can be changed by energy depletion of sensors, introduction of new sensors, or change position of sensor. therefore, the routing algorithm should be able to handle these uncertainties caused by diverse event generation functions and change of network topology. to achieve this purpose, this work considers ant system (as). the basic idea of the ant system is a positive feedback mechanism in which following ants find better solutions by considering the results of preceding ants' decisions [cit] . the as is suitable for controlling large-scale dynamic systems like communication networks because of the following desirable features:"
the remainder of this paper is organized as follows: section ii describes the word spotting system. in section iii the proposed methodology is detailed. section iv presents evaluation results on representative historical documents while in section v the conclusions are drawn.
"in classes i have taken i have rarely gotten to know many of the students. when i start a homework problem, i am more likely to start working on the solution immediately."
"to apply the mdl-based causal inference rule in practice, we need a class of models suited for causal inference. as such, the model class must allow to causally explain y given x and vice versa. one such model class is that of decision trees. a decision tree allows us to model dependencies on other attributes by splitting, i.e. conditionally describe the data of an attribute x i given an attribute x j . in other words, decision trees can model local dependencies between variables that can identify parts of the data that causally depend on each other. note that this comes close to the spirit of average treatment effect in randomised experiments [cit] ."
it should be noticed that the actual relevant words are given by the ground truth during the evaluation process and are not known before hand. our intention is to determine a cut-off threshold index i cut for which fm(i cut ) is as close as possible to the expected maximum f-measure score fm opt .
"we use the ns-3 network simulator, and rsa as a basis for our public key system. for these preliminary experiments, we use the key size of 32. an advantage rsa does have over merkle trees is that rsa can authenticate with as many nodes as it wants. merkle trees however can only authenticate with the number equal to the number of its leaves, i.e., secrete tokens. so we evaluated how the network performed when the size of the merkle trees grew. there are two parts to the authentication, the time to build the merkle tree or rsa key pair (figure 2), then the time to authenticate (figure 3) . it is evident that rsa is very slow to build and to authenticate. if the height of merkel tree is 16, it would allow each node to have 65,356 leaves to authenticate with, which is more than enough for a typical smart grid application."
", that infers the direction with the lowest relative complexity. to apply this method in practice for univariate and multivariate continuous real-valued data, vreeken instantiates it using cumulative entropy."
"e-learning is taking a great attention worldwide. it is supposed to contribute to enhance the traditional education if properly implemented. it can be beneficial to most forms of elearning, e.g., training, girls\" education, continuing education, open education. it can even be used as a supporter and enhancer for traditional in-class education."
"the remainder of this paper is organised as follows. we introduce notation and preliminaries in sect. 2. section 3 explains how to do causal inference based on algorithmic information theory. in sect. 4 we show how to derive practical, computable, causal indicators using the minimum description length principle. we instantiate this framework for binary data using a decision-tree-based compressor in sect. 5. related work is covered in sect. 6, and we evaluate empirically in sect. 7. we round up with discussion and conclusions in sects. 8 and 9, respectively."
"although these results show the strength of our framework, and of origo in particular, we see many possibilities to further improve. for instance, pack does not work directly on categorical data. by binarizing the categorical data, it can introduce undue dependencies. this presents an inherent need for a lossless compressor that works directly on categorical data which is likely to improve the results."
"this section presents several experimental results to validate the effectiveness of as-rwsn, saas-rwsn, and distributed as-rwsn, which has exactly the same frame as as-rwsn except for the global updating rule. in the distributed as-rwsn, nodes calculate transition probability only based on local information obtained from neighboring nodes. distributed as-rwsn has no pheromone update after completing tours of data ants. this section also compares the distributed as-rwsn with as-rwsn and saas-rwsn to show the effect of the global update."
"the experiments show that origo works well in practice. origo reliably identifies true causal structure regardless of cardinality and skew, with high statistical power, even at low level of causal dependencies. on benchmark data it performs very well, despite information loss through discretization. moreover, the qualitative case studies show that the results are sensible."
"5) integrating the concept of bloom\"s taxonomy to enhance the comprehensiveness of the domain ontology. this adoption and enhancement to domain ontology affects all the learning components of authoring, delivery and assessment."
"for evaluation purpose, we define lifetime of sensor network and event generation patterns. first, the definition of sensor network lifetime is the time until the first node or a portion of nodes becomes incapable, due to energy depletion, of sending data to its neighbors. the portion can vary depending on the context of the sensor networks. in this paper, the lifetime of a sensor network is the number of rounds until the first ( 1 ), 10% ( 10 ), and 20% ( 20 ) node(s) become inactive. in the case of event generation, three patterns are considered as follows:"
"in addition, the system helps the student during the course delivery process for the goal of making the learning process more pleasant, efficient, and effective. it will help him/her through:"
"the key idea behind significance testing with swap randomization is to create several random datasets with the same row and column margins as the original data, run the data mining algorithm on those data, and see if the results differ significantly between the original data and random datasets."
"to this end, we generate data on a per-attribute basis. first, we assume the ordering of attributes-the ordering of attributes in x followed by the ordering of attributes in y . then, for each attribute, we generate a binary decision tree. in doing so, we only consider the attributes preceding it in the ordering as candidate nodes for its decision tree. then, each row is generated by following the ordering of attributes, and using their corresponding decision trees. further, we use the split probability to control the depth/size of the tree. we randomly choose weighted probabilities for the presence/absence of leaf attributes."
"jilles vreeken leads the exploratory data analysis group at the dfg cluster of excellence on multimodal computing and interaction (mmci) at saarland university, saarbrücken, germany. in addition, he is a senior researcher at the max planck institute for informatics. his research interests include virtually all topics in data mining and machine learning. he authored over 70 conference and journal papers, 3 [cit] acm sigkdd doctoral dissertation runner-up award, and won two best (student) paper awards. he likes to travel, to think, and to think while travelling."
"the major objective of this paper is to give recommendations based on theoretical and practical experience to build adaptive e-learning environment community. instead of building an environment from scratch to support all the educational services required by the educational institutions, we used moodle because of its popularity as it is used in several universities. moodle also is known as simple and easy to adapt and customize to the needs of the educational system. therefore, moodle is integrated to many of the components that were developed to compose the adaptive e-learning environment, as shown in fig. 1 . three main subsystems in the proposed adaptive elearning system are integrated to the open source moodle, namely, authoring, delivery, and assessment engines. each of those main engines works smartly with the aid of the knowledge base. this knowledge base, in turn, is composed of three main knowledge bases, namely, the ontology model (om), the learning object repository (lor), and the question bank (qb), each of which is maintained with the aid of a specially designed editor. finally, the normal database of moodle is updated to accommodate more data as required by the adaptive environment, such as:"
"(1) initialization of the routing table: during the initial setup period, each sensor finds its direct distance to the base station and calculates its energy cost ( ) defined previously. then, each sensor broadcasts a setup message to its neighbors with a preset transmission power for the neighboring distance. this setup message includes identification number and ec of the transmitting sensor itself. every sensor receiving this setup message registers the transmitting sensor as one of its neighbors. the corresponding pheromone is set to 1.0 and transition probability is calculated by (7) . since all sensors have an identical neighbor distance, every sensor will receive the setup messages from all of its neighbors. after the setup period, all sensors initialize their routing tables. (2) local update: the routing table reflects changes of neighbors' energy levels. when a sensor transmits a data packet, all of its neighbors receive the packet and the current energy cost of the transmitting sensor. as a result, whenever a sensor's battery level changes, all routing tables of its neighboring nodes are updated. the fields of ec and transition probability for the corresponding sensor are updated by (6) and (7)."
"we would also attempt to study the public key encryption systems that are not vulnerable to the quantum computer attack, such as the ntru cryptosystem [cit] . we would still expect them to be slower than merkle trees; they therefore offer lightweight, secure and quantum attack resistant security that should be considered for use in the smart grid."
"(iii) in routing problems, the transition probabilities of the as can be interpreted as availability of nodes. this availability reveals the probability that the framework of the as can be used for solving dynamic routing problems. with this feature data traffic in wsns can be dispersed to maximize the lifetime of networks."
"to evaluate whether origo infers relevant causal direction, we employ swap randomization [cit] . swap randomization is an approach to producing random datasets by altering the internal structure of the data while preserving its row and column margins. the internal then, we swap the values of these four cells either in clockwise or in anticlockwise direction. the swap operation is performed repeatedly until the data mix sufficiently enough to break the internal structure of the data, also called mixing time of a markov chain. although there is no optimal theoretical bound for the mixing time of a markov chain, [cit] empirically suggest the number of swap operations to be in the order of number of 1s in the data."
"the segmented words of the historical document as well as the query keyword are described by feature vectors which are used during the matching phase in order to measure similarity between word images. several features and methods have been proposed in the literature for word image matching based on strokes, contour analysis, etc, [cit] ."
"the adaptive and artificial intelligent tutoring systems (its) are developed using web 2 [cit] . the systems are developed to adjust the contents as per the effective learning styles that are identified using self-organizing maps (soms). artificial intelligent systems behave like human beings. supervised, unsupervised and reinforced are three types of artificial intelligent systems. supervised system needs examples and a teacher to train. unsupervised system is trained without a teacher and it rectifies itself after a mistake is reported. reinforced system needs a mentor to guide the system that the answers are true or not. unsupervised learning is selected to train the tutoring systems because it does not require a teacher and felder-silverman learning style model (fslsm) are used. the intelligent and adaptive tutoring systems are equally portable to run on web and mobile platforms."
"we implemented origo in python and provide the source code for research purposes, along with the used datasets, and synthetic dataset generator. 3 all experiments were executed singlethreaded on macbook pro with 2.5 ghz intel core i7 processor and 16 gb memory running mac os x. we consider synthetic, benchmark, and real-world data. we compare origo against the ergo score [cit] instantiated with pack, and dc [cit] ."
"in a course of a complexity level \"c\", all concepts of a complexity higher than \"c\" wouldn\"t be included in this course. for instance, if the course is a medium-level course, all advanced concepts (marked with d) would be ignored; only f & m concepts are included."
"we have designed sample lectures the web programming course (cpis358) at the department of information systems with faculty of computing and information technology at king abdulaziz university. for web programming course, some topics, such as javascript, php, html are discussed and presented based on the domain ontology prepared for the course."
"the instructor is responsible to identify elements of criteria for the test which are: the domain knowledge (the current course), concept to be measured and under any level of rbt (cognitive domain) wanted to measure this concept to determine the behavioral objectives, some adaptive rules related to the adaptation and evaluation process."
"the minimum description length (mdl) [cit] principle is a practical version of the kolmogorov complexity. both embrace the slogan induction by compression. instead of all possible programs, mdl considers only those programs for which we know they generate x and halt. that is, lossless compressors. the more powerful the compressor, the closer we are to kolmogorov complexity. ideal mdl, which considers all programs that generate x and halt, coincides with kolmogorov complexity."
"when the student registers in a course, the course clos are automatically adapted to suit this specific student according to his/her student model. his/her course syllabus and course table of contents are adapted accordingly. therefore, the moodle page for the student is adapted to display the student adapted clo, the adapted course syllabus, and the detailed adapted course toc."
"next we investigate the accuracy of origo against the fraction of decisions origo is forced to make. to this end, we sort the pairs by their absolute score difference δ in two directions in descending order. then we compute the accuracy over top-k% pairs. the decision rate is the fraction of top cause-effect pairs that we consider. alternatively, it is also the fraction of cause-effect pairs whose δ is greater than some threshold δ t . for undecided pairs, we flip a coin. for other methods, we follow the similar procedure with their respective absolute score difference."
"we run opus miner on the resulting data and get top 100 self-sufficient itemsets. then we apply origo on those 100 self-sufficient itemsets. in table 3, we report 5 interesting and non-redundant causal directions identified by origo drawn from the top 7 strongest causal we give 5 characteristic and non-redundant exemplars drawn from top 7 causal directions directions. inspecting the results, we see that origo infers sensible causal directions from the adult dataset. for instance, a professional with a doctorate degree working in a public office causes them to earn more than 50k per annum. however, working in a public office in an administrative position with a high school degree causes them to earn less than 50k per annum. these case studies show that origo discovers sensible causal directions from real-world data."
"there are many learning style models exist in literature, e.g. the learning style model by felder and silverman [cit], kolb [cit], mumford and honey [cit] . they agree that learners have different ways in which they prefer to learn. after a comprehensive study of the e-learning environment, we selected felder and soloman\"s index of learning styles (ils) [cit] ."
"word spotting produces a list of word images that are ranked according to their distance when compared to the query keyword. an important issue that arises is the proper separation of the ranked results into relevant and irrelevant word instances. this will help to provide the user with only the relevant results in a way that is convenient for searching purposes (e.g. extracting a list of document pages that include one or more instances of the query keyword). clearly, a fixed similarity threshold cannot be applied since the distribution of distance values obtained may vary when applied to different datasets. in this paper, we propose a method for the determination of a cut-off threshold which is based on an estimator that combines the distance of each ranked word with its cumulative moving average. the efficiency of the proposed method is demonstrated showing that the local maximum of the estimator is highly correlated to the overall maximum of the expected f-measure."
"merkle trees are resistant to the quantum computer attack. their strength is not based on the factorization of discrete log problem, rather it is based on a one-way hash function. hash functions are quick and safe. if the quantum computer is realized to defend against a known attack, a merkle tree only needs to increase the length of its hash code. merkle trees do have some disadvantages over public key, namely they use more memory and the number of devices it can authenticate itself with has to be determined before it is built. however, in addition to being resistant to a quantum computer attack, reasonably sized merkle trees are much faster that public key schemes, as can be seen from the section iv."
"to assess whether origo identifies causal relationship when causal relationship really exists, we test its statistical power. the null hypothesis is that there is no causal relationship between cause-effect pairs. to determine the cut-off for testing the null hypothesis, we first generate 250 cause-effect pairs with no causal relationship. then we compute their δ values and set the cut-off δ value at a significance level of 0.05. next we generate new 250 causeeffect pairs with causal relationship. the statistical power is the proportion of the 250 new cause-effect pairs whose δ value exceeds the cut-off delta value."
"in this paper, we present recommendations to add adaptation and personalization to one of the most common www.ijacsa.thesai.org open source learning management system (lms), moodle. the adaptation features are based on using learning styles, ontology, and cognitive bloom taxonomy in building and presentation of the e-learning material (learning objects). this is helpful to establish a nation-wide adaptable and cognition-based learning object repository and course development centers. the rest of the paper is organized as follows: section 2 presents the adaptive e-learning system (kau-aes) developed at king abdulaziz university. section 3 is directed to the knowledge base building recommendations and section 4 presents the recommendations of the authoring system. section 5 discusses the adaptive course delivery system recommendation. section 6 gives the recommendations for assessment system, and finally section 7 presents the discussion and conclusion."
"therefore, the total number of bits needed to describe the decision tree t i and describe x i over the complete data d using t i is given by"
"when i am learning something new, it helps me to think about it. in a study group working on difficult material, i am more likely to jump in and contribute ideas."
"9) identify the concept\"s \"isa\" and \"prereq\" linked concepts, if those concepts are not known by the student\"s bdkm, they should be added to the adapted syllabus before the concept as \"recall\" concepts so that they are briefed to the student before start teaching the concept itself."
where r is the number of rows for which the leaf l is used [cit] . it can be computed in linear time for the family of multinomial distributions [cit] .
"(iii) repeated event generation from a local area : only the sensors in a local area have data packets to be reported in any round. the shape of the area can be a point, a circle, a square, or any other shape [cit] ."
"i am more likely to be considered reserved the idea of doing homework in groups, with one grade for the entire group, appeals to me."
"many works have contributed to devise routing algorithms to prolong the lifetime of sensor networks. hierarchical routing, which includes leach, pegasis, heed, teen, and qda (qos-based data aggregation), forms clusters and elects a cluster-leader for each cluster responsible for sending all the data from its followers to the base station [cit] . through clustering and clusterleader selection rules, these hierarchical approaches distribute energy usages over the network."
"wireless mesh networks (wmn) have received a lot of attention in the last decade as a result of their cost effectiveness and robustness. in a wmn each node is a peer and messages can be forwarded through each peer to get to their final destination. each node is usually connected to several nodes, thus improving reliability since there can be multiple routes from source to sink. new nodes being added to the network must authenticate themselves. there are a number of ways for devices to authenticate themselves; one way is by using public key encryption. public key encryption is a very elegant and strong way to keep information safe. however, when dealing with maters of security one has to be concerned with something not only being safe today, but also being safe in the future. electrical power equipment is expensive, time consuming and disruptive to replace. therefore, as a result of a potential future threat to public key we argue for the use of merkle trees for authentication on wmn."
"the engine generates the question items tailored to the student ability and based on the test objectives and instructor rules. to measure the specific level of rbt, we must measure the test objectives which are matched with this level. the grade of proficiency is set to 1 if the student has knowledge and set to 0 if the student does not have knowledge. we start to measure the objectives from a simple level to the more complex levels of rbt or vice versa depending to the concept to be measured. there are two cases, if the concept to be measured is for the current course then start from the lowest (simple) level of rbt to the required level of rbt. the other case, if the concept to be measured is for the prerequisite course, then we start from the required level of rbt (more complex) in the objectives to the lowest level of rbt."
the main stages of a word spotting system are (a) word segmentation (b) feature extraction and (c) matching and ranking. in this section we describe how we implemented the main word spotting stages.
"an alternative to the public key encryption system for authentication of devices in a smart grid networks uses merkle (hash) trees. a merkle tree is a complete binary tree constructed from a set of secret leaf tokens, where each internal node of the tree is a hash of its left and right child. in a mesh application the client generates the tree, and the root of the tree is made public. thereafter, the client can prove its identity to any mesh router by comparing the published root against the root that is generated when the hash function and authentication path is provided. it is computationally infeasible to determine the secret token from the published root of the tree [cit] ."
"an intelligent information access system (iias) is engineered to introduce new learning theories for the undergraduate students [cit] . concept, case and internet based learning theories are taken into account while developing the proposed system. iias identifies and marks important notes about an experimental medical case and it also assists in conducting objective assessments. the complexity of test assessment can be tailored according to the semester number of a student."
"saas-rwsn has the same routing framework as as-rwsn, except for considering the degree of nodes when a node calculates the transition probability for neighboring nodes. similar to as-rwsn, sensors in saas-rwsn keep a routing table. this table contains node degree as well as all information which routing table in as-rwsn has (node identification number, energy cost to the base station, the amount of pheromone, transition probability, and node degree for each neighboring node). similar to as-rwsn, saas-rwsn also guarantees elimination of loops in any routing path with a tabu list. algorithm 2 provides a highlevel description of the routing decisions of saas-rwsn."
"further, we rely on discretization strategies to discretise continuous real-valued data. we observe different results on continuous real-valued data depending on the discretization strategy we pick. it would make an engaging future work to devise a discretization strategy for continuous real-valued data that preserves causal dependencies. alternatively, it will be interesting to instantiate the framework using regression trees to directly consider real-valued data. this is not trivial, as it requires both a encoding scheme for this model class and efficient algorithms to infer good sets of trees."
"we use opus miner on the icdm abstracts dataset to discover top 100 self-sufficient itemsets [cit] . then, we apply origo on those 100 self-sufficient itemsets. we sort the discovered causal directions by their δ value in descending order. in table 2, we give 8 highly characteristic and non-redundant results along with their δ values taken from top 17 causal directions. we expect the causal directions having higher δ values to show clear causal connection, and indeed, we see that this is the case."
"the electrical power grid forms the function foundation of our modern societies, but in the near future our grids will reach a limit due to increased demand and aging infrastructures. as a result nations worldwide have started to convert their power grids into modern, dynamic grids with improved communications and control systems. the smart grid will thus be better able to incorporate new forms of energy generation, as well as be self-healing and more reliable. this work looks at the security threat to wireless mesh networks (wmn) from the quantum computer attack, more specifically, we argue for the use of merkle trees [cit] as opposed to public key encryption for authentication of devices in wmn when used on the smart grid. this is an ongoing work on our research on wireless network security [cit] ."
"for example, consider a hypothetical binary dataset with three attributes x 1, x 2, and x 3 . pack aims at discovering the set of trees such that we can encode the whole data in as few as possible bits. in fig. 2a-c we give an example of the trees pack could discover. as the figure shows, x 1 depends on x 2 and x 3 depends on both x 1 and x 2 . these trees identify both local causal dependencies, as well as the global causal dag shown in fig. 2d ."
"where denotes the degree of node . in wireless sensor networks, the number of neighboring nodes can vary due to energy depletion or change of position. therefore, ( ) and ( ) can also vary with time, while, in the case of static networks, ( ) and ( ) are replaced by and ."
"instead of asking the student to fill the questionnaire in sequence (the 44 questions), we grouped the questions related to each two dimensions in a single group of questions as shown in table i . from the practical experience with students while they are filling the questionnaire, this enables them to choose the most related to their preferred learning style as they sometimes find some confusion in understanding each question separately."
"(3) global update: once the base station receives an ant or a data packet, the base station broadcasts the information gathered by the ant to the visited nodes to update the pheromone concentration. based on (8), the visited node updates the pheromone deposit of the corresponding link."
"the cognition-augmented knowledge base has two main components, namely, the lor and the ontology model (om). those two components are main drivers of the adaptation. om derives the authoring process, while lor derives the delivery process. both components play an important role during the pre and post assessment processes."
"embedding rbt in om: the second improvement in om is the accommodation of the rbt [cit] . each concept node is made of six levels corresponding to bloom\"s levels. this will make om as a multilayered diagram; one layer for each of the bloom\"s levels. this extension is intended to guide the course design phase in which the course objectives specify the target bloom\"s level for each concept covered in the course. accordingly, this concept\"s om\"s layer is employed and the relationship links are followed. most importantly of those links is the prerequisite link which might reference a specific layer of another concept, as shown in fig. 3, where the \"depth-limiting search strategy\", for instance, is having complexity level \"m\" and whose rbt's level of \"understanding\" requires, as a prerequisite, \"depth-first search\" at rbt's level of \"applying\"."
"the metadata model: the adaptation process applies different theories such as learning style, instructional design, and cognition theories, a knowledge that are usually applied by an expert instructor who happened to know them through study or by experience. inexpert instructors, on the other hand, though are subject matter experts, usually lack such knowledge. the e-learning model attaches a set of metadata attributes to each lo in order to aid the adaptation process. those attributes are so simple and naive in such a way that they don\"t require an expert to define them, yet are used by the expert system to deliver courses with a similar quality like that of an expert instructor. each lo is described in terms of several metadata attributes."
"system for routing problem in lwsn. different from the as for combinatorial optimization problems such as tsp, the one for routing problem in communication networks should be able to provide optimal routing policies for each node rather than find an optimal solution. in other words, the routing algorithm for communication networks should find routing policies for all nodes for routing a given data traffic."
"the student\"s bdkm is used to adapt the student\"s clo and course syllabus, while his/her lsm is used for adapting the course toc. the course clo represents the goal outcomes of this course as specified by the course designer. it takes the form of a list of items, each of which is described as follows:"
a similar study is conducted while developing an educational system [cit] . the proposed system depends on abilities of students and degree of interaction between students and instructor [cit] . the proposed educational system uses multi agent domain ontology to measure the progress of learning and judge the abilities of a student. a student interacts with the system to describe his/her opinion about a topic and it is matched with the data of text book. the system displays the mistakes of a student and it also suggests improvement in the course material.
"quick and efficient content exploitation is an important feature for any information system that provides access to historical document collections. such collections usually contain a large number of documents and a robust indexing methodology is an essential performance and efficiency indicator. due to document degradations, ocr systems often fail to support a correct segmentation of the printed historical documents into individual characters. word spotting is a content-based retrieval procedure that spots words directly on document images with the help of efficient word matching while avoiding conventional ocr procedures [cit] . in the case of historical documents, rath and manmatha [cit] presented a word matching scheme where noisy handwritten document images are preprocessed into onedimensional feature sets and compared using the dtw algorithm. [cit] present a method for retrieving large collections of handwritten historical documents using statistical models. [cit] present a holistic word recognition approach for handwritten historical documents. the query comprises either an actual example from the collection of interest or it is artificially generated from an ascii keyword. a crucial aspect in the retrieval procedure is the word image representation which relies upon robust features. the retrieval procedure is based on a similarity criterion to be maximized or a distance measure to be minimized [cit] . a common approach is to reduce the word representation into a fixed-length vector of features and use geometric distance measures like euclidean, cosine, etc [cit] ."
"by which we have shown that pack is indeed an information measure, and hence can pick up causal structure from observations where the causal mechanism is modelled by binary decision trees. next we discuss how to compute our mdl-based causal score using pack."
"next we instantiate the mdl-based causal inference framework for binary data. as such, we require a compressor for binary data that uses a set of decision trees as its model class. importantly, the compressor should consider both the complexity of the model and that of the data under the model into account. one such compressor that fits our requirements is pack [cit] . in particular, we build upon pack to instantiate the mdl-based causal score. next we briefly explain how pack works. (c) . 2 in a-c, we give the example decision trees generated by pack for a toy binary dataset containing three attributes, namely x 1, x 2, and x 3 . in d, we show the dependency graph for these trees. a tree for x 1 . b tree for x 2 . c tree for x 3 . d dependency dag"
"question selection field (manual or auto) that specify if the assessment question will be selected by the teacher (manual) or by the system (automatic selection), if auto is selected then the teacher should specify number of question and their difficulty level in the fields (number of low level questions, number of mid-level questions, number of high level questions) type field (pre. or post.) that specify when to view the quiz before learning object playing or after viewing it."
"notably, the value of the probability ( ) can be different for two ants on the same node because these two ants probably have different tabu lists."
"the student information is updated to accommodate the student model (sm) by adding both his/her background knowledge, learning style model, and some other data, such as, preferred language, etc. the course information is also updated to include the course learning outcomes (clo). in addition, the moodle itself is adapted to accommodate and seamlessly integrate to the different components of the adaptive e-learning model. for instance, the following was implemented to augment moodle with adaptation:"
"this section provides an example to clarify the mechanism of the saas-rwsn algorithm. in figure 4, sensor 1 has a data packet to be sent to the base station. it has two neighboring nodes, sensors 2 and 3, which have 4 and 2 node degrees, respectively. if and ∀ are 100 and 1.0, then, as shown in the second and third columns of table 3, and ( ) are determined by (6) and (9) where max and min are 2.0 and 1.5, respectively. with (10) the transition probabilities in the routing table (table 3) can be calculated and ant chooses one of the three paths with probabilities 0.19, 0.5, and 0.31. even though nodes 2 and 3 have the same energy cost, sensor 2 has a higher probability since it has a higher node degree. after routing ant, sensor conducts the global and local updating processes. the global and local updating rules are the same as the ones in as-rwsn. in addition, if node degree of any neighboring node changes, sensor updates the node degree of the corresponding node in its routing table. sensors under the saas-rwsn follow this procedure to route their data."
"the comparison of the proposed algorithms is with three other algorithms: direct communication (direct), minimum transmission energy (mte), and self-organized routing (sor). in direct, every sensor simply transmits data directly to the base station without any consideration of energy-efficiency. in contrast, mte considers energyefficiency by choosing a path with minimum total energy consumption but it does not take into account energybalancing. sor pursues energy-efficiency as well as energybalancing together and has several desirable properties as discussed earlier."
"the smart grid may create profound changes to the way we live our lives and it offers much in the way of efficiencies and robustness to the electrical power grid. however, its heavy reliance on communications networks will leave it more vulnerable to attack than present day power grids. that is to say it will have similar weaknesses as a present day communications networks, but the stakes will be much higher. therefore this new smart grid must be built on a solid foundation of information security."
"4) if student\"s bdkm misses any of the concepts predecessors (\"isa\" and \"prereq\" relationships), then add this concept to the student\"s slo at the same cognitive and complexity level as described in the clo for the specified concept."
"kolmogorov complexity is not computable, however, and to be able to put it to practice we derive a practical, computable version based on the minimum description length (mdl) principle [cit] . as a proof of concept, we propose origo, 1 which is an efficient and parameter-free method for causal inference on binary data. origo builds on the mdl-based pack algorithm [cit] and compresses data using decision trees. simply put, it encodes the data one attribute at a time using a decision tree. such a tree may only split on previously encoded attributes. we use this mechanism to measure how much better we can compress the data of y given the data of x, simply by (dis)allowing the trees for y to split on attributes of x, and vice versa. we identify the most likely causal direction as the one with the most succinct description."
"as explained earlier, as-rwsn guarantees elimination of loops in any routing path. since each data ant has a tabu list of visited nodes, the ant never visits any node more than once. therefore, as-rwsn always assures finding a routing path to the base station without loops. algorithm 1 provides a highlevel description of the routing decisions of as-rwsn."
"inferring causal direction from observational data is a challenging task if no controlled randomised experiments are available. due to its importance in practice, however, causal inference has recently seen increased attention [cit] . most proposed causal inference frameworks are limited in practice, however, as they rely on strong assumptions, or have been defined only for either continuous real-valued, or discrete numeric data."
"node, we use an extra log m bits to identify the split attribute [cit] . let intr(t i ) be the set of all intermediate nodes of a decision tree t i . then the number of bits needed to describe a decision tree t i is given by"
"objects repositories (lor). learning objects are drawn from an lor based on a certain criterion, which is described in terms of metadata attributes that are used to specify the selection criteria of the appropriate required material. in this research we suggested adapting the lo metadata of a standard lo model such as scorm by adding extra attributes necessary for supporting the concepts of the student model, especially the dimension of the learning styles."
"where f i is considered in the vicinity of the top r words. index r is determined by the ranking position that corresponds to the word whose distance is closest to the mean word distance. as mean word distance d m we denote the cosine distance of the query vector q from a vector m whose j-th element equals the mean j-th feature of all the word feature vectors. that is,"
"with the aid of the om, the generic course syllabus is generated. the syllabus is composed of numbered sections which in turn are composed of subsections, while the toc adds sub-subsections which go into pedagogical details. for instance, a section on stack may contain a subsection that explains the concept of \"lifo\", while the toc may further break down the \"lifo\" subsection into many sub-subsections, like an definition, an application of lifo from real life, etc. www.ijacsa.thesai.org using the background domain knowledge model (bdkm) of the student model of a certain student, the clo will be adapted to match this specific student (hence is named student learning outcomes (slo)) by adding unknown prerequisite concepts, and removing well known concepts. again, the authoring system will use the adapted slo, with the aid of the om, to automatically generate the adapted course syllabus, which will then be the input for generating the adapted student\"s course toc."
"first we evaluate origo on benchmark cause-effect pairs with known ground truth [cit] . in particular, we here consider the 95 univariate pairs. so far, there does not exist a discretisation strategy that provably preserves the causal relationship between variables. to complicate matters further we do not know the underlying domain of the data, and each cause-effect pair is from a different domain. hence, for exposition we enforce one discretisation strategy over all the pairs."
"sensor networks are dynamic and large-scale systems. a distributed control mechanism is necessary for managing such systems. an ant system algorithm provides such a distributed and robust control framework. this paper, based on ant colony system, proposes two ant-based routing algorithms: as-rwsns (ant system for routing in sensor networks) and saas-rwsns (structure-aware ant system for routing in sensor networks). as-rwsn modified the original ant system to be applied to sensor network applications by introducing a new link cost, energy cost. saas-rwsn advanced as-rwsn by considering heterogeneity of networks in routing. these two proposed routing algorithms outperform existing routing algorithms in wireless sensor networks. the convergence trait of ant system improves the system performance and, at the same time, its randomness features lead to a balance of the whole network in terms of traffic and energy. numerical experiments show that ant-based routing algorithms perform well in wireless sensor networks. this work only proposed node degree as a metric to identify heterogeneity of networks. however, many other metrics such as the clustering coefficient and local betweenness centrality [cit] are available in complex network theory. future work will integrate this study's ant-based routing algorithm with these other metrics for characterizing the heterogeneity of sensor networks."
"in the e-learning model, that in-depth knowledge regarding a specific knowledge domain is accumulated in the om, which is assumed to be incrementally and/or cooperatively designed by the domain experts. in fact, om is a key player in the e-learning model. it is a comprehensive model of interrelationships among concepts/topics. this comprehension gives more flexibility to the authoring process in composing a course. moreover, it gives an automation power to the authoring process. so, it is recommended to design om with the objective of supporting not only course authoring but also course delivery and assessment as well. to achieve this goal, the traditional ontology net scheme is extended to accommodate two extra updates to the classical scheme:"
"this feature is implemented with only the first tree levels in the rbt and the compensation of the missing prerequisite knowledge is done through \"recall\" branch as in fig. 3 . www.ijacsa.thesai.org 1) adding extra attributes necessary for supporting the theories it implements, such as learning style model, revised bloom\"s taxonomy, etc. of course, these attributes are not contradicting with any lo standard, but rather they are complementing them,"
"for instance, frequent itemset mining is one of the core topics in data mining. clearly, when frequent itemset appears in a text, it gives more information about the word mining than vice versa because mining could be about data mining, process mining, etc. likewise, neural gives more information about the word network than the other way around. overall, the causal directions discovered by origo in the icdm dataset are sensible."
"a subject matter expert course author, who is very much familiar and knowledgeable about the subject domain knowledge, knows much invaluable information about those concepts and the best ways of teaching them to a certain group of students with a specific average profile. for instance, the expert author should know what the best break down is for a certain specific topic; what the best sequence for certain topics would be; what topics would achieve the goals of a certain course; what the best depth is for each topic/subtopic; when to introduce exercises, quizzes, and tests, etc. to stimulate students\" enthusiasm and learning effectiveness. one goal of this research is to support course authors in doing the authoring job professionally, even if they lack the sufficient expertise."
"once the course is added to the system and its clos are defined, the algorithm of the \"generic course syllabus generator\" runs to generate the generic course syllabus, while the algorithm of the \"adapted student course syllabus generator\" runs once the student registers in a specific course. the files are placed in an agreed upon folder and named with an agreed upon naming convention. the idea of the generic course syllabus generator is summarized as follows:"
"2 international journal of distributed sensor networks (ii) the as is robust for unexpected changes of environments. in telecommunication networks, as adapts to traffic conditions with communications among ants and route data to less congested areas of the network. the as has an adaptive mechanism to the changes of environments."
"we propose a general framework for causal inference on observational data, and give a practical instantiation for binary data. we base our inference framework on the solid foundations of kolmogorov complexity [cit] and develop a score for pairs of data objects that identifies not only the direction [cit], but also quantifies the strength of causation, without making any assumptions on the distribution nor the type of causal relation between the data objects, and without requiring any parameters to be set."
"the main contributions of our work are as follows: -a theoretical framework for causal inference from observational data based on kolmogorov complexity, -a practical framework for causal inference based on mdl, -a causal inference method for binary data, origo."
"according to the transition probability, each sensor routes the data packet to the base station. once the data packet arrives at the base station, every edge involved in routing the data packet updates its pheromone concentration by"
"as models we consider sets of decision trees such that we have one decision tree per attribute in the data. the dependencies between variables modelled by these trees induce a directed graph. to ensure lossless decoding, there needs to be an order on the variables in a graph. it is easy to see that there exists an order of the variables if an only if the graph is acyclic. hence, we enforce that there are no cyclic dependencies between variables across these trees."
"our framework is based on causal sufficiency assumption. extending origo to include confounders is another avenue of future work. moreover, our inference principle is defined over data in general, yet we restricted our analysis to binary, categorical, and continuous real-valued data. it would be interesting to apply our inference principle on time series data. to instantiate our mdl framework the only thing we need is a lossless compressor that can capture directed relations on multivariate time series data."
"several methods have been proposed for evaluating the performance of the retrieval system [cit] . a well known performance measure is the f-measure fm which provides a certain tradeoff between specificity and sensitivity. taking into consideration the top i word instances, fm i is expressed as the harmonic mean of precision p i and recall r i metrics as follows"
"noteworthy, not only the course authoring is intelligently impacted by the extended om but also many other components in the knowledge base. for instance, the student\"s bdkm is updated to accommodate the six levels of rbt. accordingly, om plays an important role in the adaptation of the course delivery in two ways:"
"where and are tuning parameters and is the list of cities which ant at city can transit. ant calculates with its tabu list, tabu, which is a list of cities already visited. the tabu list prevents ant from visiting a city more than once."
"to compute l(x, m x ), we can simply compress x using pack., however, we do not want the attributes of x to depend on the attributes of y . therefore, we modify line 8 of greedypack such that an attribute of x is only allowed to split on other attributes of x, and an attribute of y is allowed to split on both the attributes of x and the other attributes of y . from here onwards, we refer to the pack-based instantiation of the causal score as origo, which means origin in latin. although our focus is primarily on binary data, we can infer causal direction from categorical data as well. to this end, we can binarise the categorical data creating a binary feature per value. as the implementation of pack already provides this feature, we do not have to binarise categorical data ourselves. moreover, as we will see in the experiments, with a proper discretization, we can even reliably infer causal directions from discretised continuous real-valued data."
"we considered causal inference from observational data. we proposed a framework for causal inference based on kolmogorov complexity, and gave a generally applicable and computable framework based on the minimum description length (mdl) principle."
"in fig. 8, we show the accuracy versus the decision rate for the benchmark univariate cause-effect pairs. if we look over all the pairs, we find that origo infers correct direction in roughly 58% of all pairs. when we consider only those pairs where δ is relatively high, i.e. those pairs where origo is most decisive, we see that over the top 8% most decisive pairs it is 75% accurate, yet still 70% accurate for the top 21% pairs, which is comparable with the top-performing causal inference frameworks for continuous real-valued data [cit] ."
"however, usually a higher-level course would also introduce those concepts of a lower complexity. therefore, for the navigation through the om net during the course design processes, it is recommended to use the following simple rule."
"to the best of our knowledge, we did not find similar integrated work in our region. by carefully inspecting of some related work, we can deduce the following comparative of our system kau-aes with other systems in the literature as shown in table ii. for our future research directions, we may have the following points:"
"to apply the framework in practice, we proposed origo, an efficient method for inferring the causal direction from binary data. origo uses decision trees to encode data, works directly on the data, and does not require assumptions about either distributions or the type of causal relations. extensive evaluation on synthetic, benchmark, and real-world data showed that origo discovers meaningful causal relations, and outperforms the state of the art."
"the algorithmic causal inference rule is based on the premise that we have access to the true distribution. in practice, we of course do not know this distribution and we only have observed data. mdl eliminates the need for assuming a distribution, as it instead identifies the model from the class that best describes the data. the total encoded size, which takes into account both how well the model fits the data as well as the complexity of the model, therefore functions as a practical instantiation of k (p(·))."
"in the proposed approach, two different types of features are combined providing a hybrid features vector for each dataset word as well as for the query keyword [cit] . the first one divides the word image into a set of zones and calculates the density of the character pixels in each zone. the second type of features is based on word (upper/lower) profile projections. the word image is divided into two sections with respect to the horizontal line that passes through the center of mass of the word image. upper/lower word profiles are computed by recording, for each image column, the distance from the upper/lower boundary of the word image to the closest character pixel."
"with that, we can perform causal inference by simply identifying that direction between x and y where factorization of the joint distribution yields the lowest total kolmogorov complexity. although this inference rule has sound theoretical foundations, kolmogorov complexity is not computable-due to the halting problem. we can approximate kolmogorov complexity from above, however, through lossless compression [cit] . more generally, the minimum description length (mdl) principle [cit] provides a statistically sound and computable means for approximating kolmogorov complexity [cit] . next, we discuss how mdl can be used for causal inference."
"suppose we are given data over the joint distribution of two random variables x and y of which we know they are dependent. we are interested in inferring the most likely causal relationship between x and y . in other words, we want to infer whether x causes y, whether y causes x, or whether the two are merely correlated. to do so, we assume causal sufficiency. that is, we assume that there is no confounding variable z that is the common cause of both x and y . we base our causal inference method on the following postulate."
"pack is an mdl-based algorithm for discover interesting itemsets from binary data [cit] . to do so, it discovers a set of decision trees that together encode the data most succinctly. the authors of pack show there is a connection between interesting itemsets and paths in these trees [cit] . while we do not care about these itemsets, it is the decision tree model pack infers that is of interest to us."
"last but not least, we observe that for all the above experiments inferring the causal direction for one pair typically takes only up to a few seconds. next we evaluate origo on real-world data."
"the course model is composed of three components, two of which, namely, the course syllabus and the toc are generated automatically by manipulating the course learning outcomes that are defined by the course designer. moreover, the course model has two levels of data: the highest level is more generic and concerns the course from a generic perspective, i.e., one course fits all, while the other is the adapted course for each individual student according to his/her student model. this generic course model is simply a course syllabus that is automatically generated from the course\"s clos with the aid of the domain knowledge ontology model. it is generated for all students with no guarantee it matches the student model of any of the students. in addition, the course\"s generic toc is automatically generated to match the teacher\"s teaching style. on the other hand, the lower level of data of the course model are the adapted student\"s clo, the adapted student course syllabus, and the adapted course toc, which are adapted for each individual student according to his/her student model. i understand something better after i think it through. when i am learning something new, it helps me to talk about it."
"this paper builds upon and extends [cit] . in particular, we give a much more thorough introduction to causal inference by algorithmic information theory. we present our instantiation for binary data using decision trees in detail and self-contained, including the rationale of why decision tree models make sense, the exact encoding that we use, as well as show that it is an information score that can indeed be used for causal inference, and the algorithm for how to infer good models directly from data. last, but not least, we provide a much extended set of empirical evaluations."
"as each learner has different learner\"s characteristics; so, utilizing diverse educational settings may be more appropriate for one group of learner than for another. so, adaptive elearning is an e-learning system that is more effective by adapting or personalizing the presentation of information to individual learners based on their preferences, knowledge and needs. this sort of e-learning systems tries to acquire knowledge about a particular learner and offer personalized services and enable one-to-one delivery [cit] ."
many approaches have been conducted to formulate maximum lifetime routing problems as linear or nonlinear programming (lp or nlp) problems and propose distributed algorithms designed to solve the lp or nlp problems [cit] .
"we tested our methodology on a french historical book which was published in 1838 and is owned by bibliothèque nationale de france. in fig. 6 a sample image is shown. we selected 153 pages from this book that contain an overall of 47715 words. we manually marked the ground truth for 20 keywords that have a variety of instances ranging from 33 up to 362, as shown in table ii . for each query keywords the precision p, recall r, and fmeasure curves are calculated according to the ground truth. based on these measures, the maximum value fm opt of fmeasure and its index i opt are determined. the index i cut of f i is given by (7) and the corresponding f-measure value fm(i cut ) is also calculated. the results for all the query keywords are shown in table iii v. conclusions this paper proposes an efficient method for the estimation of a cut-off threshold that can be applied to the ranked results list of a word spotting system in order to filter the most relevant words. as a performance measure, the fmeasure is used which provides a certain tradeoff between specificity and sensitivity. the method is based on an estimator that for each ranked word combines its distance with its cumulative moving average. the estimator has a local maximum which is highly correlated to the overall maximum of the expected f-measure. experiments on a database with representative historical printed documents evidenced promising results that demonstrate the feasibility of the proposed method."
"3) adapted course delivery according to the student model. 4) adapted student assessment: placement of quizzes during the course, assessment of prerequisite knowledge, post course assessment according to the student model, especially the student learning style."
"learning styles mean that individuals differ in regard to what mode of instruction or study is the most effective for them [cit] . so, they are distinct individual patterns of learning that vary from person to person. it is necessary to determine what is most likely to trigger each learner\"s concentration, how to maintain it, and how to respond to his or her natural processing style to produce long term memory and retention [cit] ."
"\"selection\" can be identified at large by the answers to few questions, which mainly direct the adaptation process through the selection of the appropriate lo based on the \"technical format\" attribute:"
"the course delivery system (cds) adapts the delivery of the course to the student according to his/her student model. the delivery system takes it from the adapted student course syllabus, to generate the detailed adapted course toc, and then to the presentation phase where the appropriate los are presented to the student, as shown in fig. 4 . each student would have his/her personalized toc. the toc is structured into: chapters, sections, and sub-sections. chapters and sections come from the adapted student course syllabus. sub-sections are identified in this phase according to the student\"s lsm."
"two database components that are essential to the adaptive processes, namely, the student model and the course model, which maintains data along those two models for each student and each course, respectively. the student model has two major components in addition to few other attributes. the www.ijacsa.thesai.org course model has three components; each is having two levels, generic and adapted to suit each student. the student\"s learning style model (lsm): each student has his/her own learning style model which is defined in terms of the fslsm\"s four dimensions (visual/verbal, global/sequential, active/reflective, sensing/intuitive). the lsm is identified for each student once, at the time he/she joined the e-learning system. the lms is identified through the index of fslsm questionnaire (http://www.engr.ncsu.edu/learningstyles/ilsweb.html) which is considered an easy way to identify the learner\"s learning style in more details. this questionnaire contains 44 questions and describes the learning style dimensions by using scales from -11 to +11; while zero indicates the origin of the axis, each direction on the axis refers to one of the two properties of the dimension."
"the as-rwsn proposed in the earlier section can be improved by considering node heterogeneity. the as-rwsn assumes that all nodes in networks are homogenous. however, many large-scale networks found in communications, biology, or sociology have heterogeneity among various properties. for example, complex networks were found to exhibit a scale-free degree distribution. complex network theory provides various methods for characterizing the heterogeneity. when the goodness of nodes is identified by such methods, the routing algorithms can be designed more effectively based on the utilization of a network's structural properties. proposing a structure-aware ant colony routing, called structure-aware ant system for routing in wireless sensor networks (saas-rwsns), this algorithm captures the advantage. in fact, the saas-rwsn advances as-rwsn by considering a property of network structure such as degree of node. the basic idea of saas-rwsn is that routing data traffic to node having more links leads to energy balance. if a node has many neighboring nodes, then it has many alternative paths and a high chance to disperse data traffic."
"although kolmogorov complexity is defined over binary strings, we can interchangeably use it over mathematical objects, or data objects in general, as any finite object can be encoded into a string. a data object can be a random variable, sequence of events, a temporal graph, etc."
"when i have to work on a group project, i first want to brainstorm individually and then come together as a group to compare i am more likely to be considered outgoing."
"then we encode the number of nodes in the decision tree t i . in doing so, we use one bit to indicate whether the node is a leaf or an intermediate node. if the node is an intermediate"
"when i start a homework problem, i am more likely to try to fully understand the problem first. i prefer to study in a study group."
"control for everything else [cit] . in practice, setting up such an experiment is often very expensive, or simply impossible. the study of the effect of combinations of drugs is good example."
"for our purpose, we also need the kolmogorov complexity of a distribution. the kolmogorov complexity of a probability distribution p, k (p), is the length of the shortest program that outputs p(x) to precision q on input x, q [cit] . more formally, we have"
"in practice, however, we also encounter cause-effect pairs with asymmetric cardinalities. to evaluate performance in this setting, we set, respectively, k and l to 5 and vary the other between 1 and 10, and generate 100 data pairs per setting. we see that origo outperforms ergo by a huge margin, the stronger the unbalance between the cardinalities of x and y . this is due to the inherent bias of ergo favouring the causal direction from the side with higher complexity towards the simple one. in addition, we see that origo outperforms dc in every setting."
in order to determine the cut-off threshold which approximately maximizes the expected f-measure we introduce an estimator f i that combines the distance d i of the i-th ranked word with its cumulative moving average. let
"i more easily remember something i have thought a lot about. when i have to work on a group project, i first want to have \"group brainstorming\" where everyone contributes ideas."
"extensive experiments on synthetic, benchmark, and real-world data show that origo performs well in practice. it is robust to noise, dimensionality, and skew between cardinality of x and y . it has high statistical power, and outperforms a recent proposal for discrete data by a wide margin. after discretization, origo performs well on both univariate and multivariate benchmark data. three case studies confirm that origo provides intuitive results."
"at the level of the generic syllabus, the cognitive level is considered only when following the relationships. the relationships (e.g., \"isa\", \"prereq\", \"composedof\", \"follow\") should be traced in om starting at the appropriate cognition level as specified by the clo."
"all components of the adaptive e-learning environment are centered on the knowledge base. as shown in fig. 2, the knowledge base is composed mainly of three major components: the system knowledge base; student database; and course database. the knowledge base is composed of the learning objet repository (lor) and the ontology model (om). while, database is composed of the student model (sm) and the course model (cm), which themselves are further decomposed. the sm is composed of two components: the student\"s learning style model (lsm) that is defined in terms of the four dimensions of fslsm [cit] and the sbdk representing the knowledge that the student captures with an acceptable cognitive depth for the domain of study. in addition, the cm is composed of three components: the clo, the course syllabus, and the toc."
"1) for each concept in the clo, consult om to identify its \"isa\" and \"prereq\" linked concepts. those concepts should be added to the syllabus before the concept as \"recall\" concepts so that they are briefed to the student before start teaching the concept itself."
"the algorithmic independence of markov kernels (postulate 2) links observations to causality: we can reject a causal hypothesis if the algorithmic independence of markov kernels is violated [cit] . the notion of algorithmic independence, however, uses kolmogorov complexity as an information measure, and is hence incomputable. while we know that mdl provides a well-founded way to approximate kolmogorov complexity in general, the question remains whether this also holds for causal inference, and in particular, whether this holds for our pack score. the answer is yes. [cit] show that independence of markov kernels is justified when we use a compressor as an information measure, if we restrict ourselves to the class of causal mechanisms that is adapted to the information measure. in general, let x be a set of discrete-valued random variables and ω be the powerset of x, i.e. the set of all subsets of x . we then have the following definition of an information measure."
"the computational power of present day computer comes from the transistor, but a new type of computer may replace it that will instead use atoms held in magnetic fields, called qubits. the quantum computer will not only be faster, but it will be able to solve problems in different, more efficient ways. this is the threat to public key encryption systems that are based on the \"factorization\" or \"discrete logarithm\" problem. [cit] a research at bell labs named peter shor, developed an algorithm that would break these problems in a reasonable amount of time using a quantum computer [cit] . if these types of public key systems are installed in the smart grid, then those systems will be vulnerable to a quantum computer attack. quantum computers are currently beyond our technical abilities, but who is to say they will be in 20, 30 or 50 years."
"for example, \"by the end of this course the student should be able to apply the concept of stack at a complexity level of 2\"."
"intelligent educational system (ines) is one of the components of an electronic learning platform [cit] . semantic management of users and contents, bdi-based (believes, desires, intentions) agent, an inference engine, ontologies and learning contents are the main components of ines. ines is used to identify the credentials of each student and check the status of his/her learning progress. the core objective of this exercise is to recommend a student whose progress of learning is not satisfactory."
"we hence consider causal inference from observational data. that is, our goal is to infer the most likely direction of causation from data that has not been obtained in a completely controlled manner but is simply available. in recent years large strides have been made in the theory and practice of discovering causal structure from such data [cit] . most methods, and especially those that defined for pairs of variables, however, can only consider continuous-valued or discrete numeric data [cit] and are hence not applicable on binary data such as one would have in the above example."
"the lsm adopted by the e-learning model is fslsm [cit] as it has applicability to e-learning and compatibility to the principles of interactive learning systems design [cit] . a student\"s learning style will affect the adaptation process in two directions, namely, the selection and sequencing of the los during the course delivery."
"learners are the main actor in the e-learning environment and they are usually having varied and diverse cognitive and psychological traits. one of the important facets of the adaptive model of e-learning is to adapt the presentations of the learning material to meet the needs of each individual learner during the course delivery process. to achieve such goal, we need to detect the learner profile to adapt the content and presentation of the learning material. this profile is called student model (sm). also, the learning materials are composed of small granular multimedia objects referred to as learning objects (los), to achieve a high level of adaptation."
(i) uniform event generation: every sensor has a data packet to be reported in any round [cit] . (ii) random event generation with random rate : every sensor has a data packet to be reported with probability in any round. the probability is called random rate [cit] .
"where is the transition probability for node at node, is the set of node 's neighboring nodes, and is a function of cost or distance from node to the destination. apparently, less cost leads to greater probability. even though this feature enables finding energy efficient paths, the routing algorithm does not consider energy balance over the network. some sensors in this algorithm are forced to deplete quickly."
"student model should be used for tailoring the teaching strategy and learning material for dynamically adapting it according to the student\"s abilities and his/her previous knowledge. student model is often based on various different dimensions. in this project, we focus on the student model in one dimension, namely, the cognitive model, especially the learning style. a learning style is defined, among many definitions, as \"the unique collection of individual skills and preferences that affect how a student perceives, gathers, and processes learning materials\" [cit] . therefore, the concept of student model, especially learning styles, is considered as a central component in this research\"s implementation. course authors should design their courses with their students\" styles in mind, course delivery should match the student style, and student assessment should also be adapted to match each specific student\"s learning style, while student portfolio helps identifying the student model."
"the qir is the central storage for that module, which is to be shared among instructors that maintain a collection of reusability test items to measure different levels of knowledge and skills in different difficulty levels. the cas is adapted based on fslsm to select, present and sequence the question objects to the preferred student learning style. we are employing a simple overlay student model. it reflects the student's estimation of current knowledge levels for a student in concepts in the current domain and prerequisite concept in every level of rbt."
"we considered various discretisation strategies-including equi-frequency and equi-width binning, mdl-based histogram density estimation [cit], and parameter-free unsupervised interaction-preserving discretisation (ipd) [cit] . overall, we obtained the best results using ipd using its default parameters and will report these below."
"putting it together, the total number of bits needed to describe all the trees, one for each attribute, and the complete data d is given by"
"to evaluate origo on the data with known ground truth, we consider synthetic data. in particular, we generate binary data x and y such that attributes in y probabilistically depend on the attributes of x, termed here onwards dependency. throughout the experiments on synthetic data, we generate x of size 5000-by-k, and y of size 5000-by-l."
"participants were tested individually in a quiet room. they were instructed to read at a normal, comfortable pace in a manner that would enable them to answer comprehension questions. comprehension questions were presented after 16 of the sentences, distributed among target and filler trials. participants did not receive feedback on their answers. all the participants in the analyses below scored at least 80% correct on the comprehension questions."
"this ambiguity arises because the noun phrase (np) the sock is a plausible object of the preceding verb knitting but actually serves as the subject of the following main verb fell. for sentences like this, readers have a bias towards the late closure interpretation, whereby the sock is treated as the direct object of the preceding verb [cit] . this creates a processing problem when readers reach the main verb fell. if the sock has been assigned as object of the preceding verb, then fell is left without a syntactic subject, which is disallowed in english. to repair this problem, readers must undo their prior syntactic commitment (or re-rank the structural possibilities). on the early closure interpretation, the initial clause is closed after the subordinate verb knitting. this leaves the np the sock free to be incorporated into the main clause as the subject (as in the sock fell)."
"5 on this account, parsing an early closure prime sentence involves sorting through structural representations that are activated in response to accessing a lexical representation. when readers encounter the verb knitting, they may initially favour the direct-object version (due to language-level probabilities or verb-linked probabilities or because the object analysis is structurally simpler than the alternatives). however, encountering the main verb (without another np that would allow the sock to serve as object of the subordinate verb) forces the comprehender to suppress or inhibit the link between the lexical representation of knit and the direct-object structural representation, and enhance the link between the lexical representation of knit and the (objectless) intransitive structure. when a subsequent sentence is encountered, that uses the same verb, the structural information that has just been accessed via the lexical entry for that verb affects the choice of syntactic analysis for the target."
"syntactic priming studies allow us to assess how one type of context affects sentence processing. syntactic priming occurs when structural information from a prime sentence affects the way a subsequent target sentence is processed. syntactic priming has been studied most extensively in production [cit] . in production, comprehending or producing a target structure increases the likelihood that a participant will produce a matching structure in the near future [cit] . in production, syntactic priming effects do not depend on prosodic, functional or lexical repetition between prime and target expressions. however, the tendency to reproduce a target structure increases when a lexical item appears in both the prime and the target expression [cit] . this lexical boost has been interpreted as reflecting short-lived enhanced activity in pathways that connect lexical representations (lemmas) to representations that support combinatory operations (combinatory nodes). currently, production priming is thought to respond to both shortterm, lexically driven influences [cit] and longer-term changes in the representational substrate driven by implicit learning processes (e.g., [cit] ."
the data collection and analysis procedures were identical to experiment 1. less than 1% of the data was eliminated based on the same criteria. all the participants included in the analysis scored greater than 80% correct on the comprehension questions.
"as yet, there is no evidence whether priming of early closure sentences is subject to the lexical boost. experiment 2 was designed to test whether repetition of the initial verb would influence the degree of priming observed at the syntactically disambiguating main verb and the next word. to do so, the stimuli from experiment 1 were rearranged to eliminate repetition of the subordinate clause verb. if early closure priming follows the pattern observed in other production and comprehension priming studies, then priming of early closure target sentences by early closure prime sentences should be reduced or eliminated in experiment 2. if lexical repetition is key to the priming effects, then there should be no difference in reading times for different types of early closure targets (primed vs. unprimed). there should also be no difference in reading times between the early closure prime sentences and early closure targets following early closure primes."
"the results of experiment 1 show, for the first time, that the processing of disambiguating material in early closure sentences can be primed by a single preceding sentence whose syntactic form matches the target. in fact, when early closure target sentences followed early closure primes, the disambiguating verb and the following word were read no more slowly than the same words in late closure sentences, for which syntactic processing is thought to be relatively straightforward. the lack of priming effects in the late closure conditions and the lack of a priming effect in the early closure 'unprimed' condition suggest that semantic repetition does not, by itself, speed up target sentence processing. in the case of the late closure sentences, this may be because reading times are already close to the floor. in the case of the early closure unprimed condition, the absence of a reduction in reading time compared to the early closure prime sentence suggests that semantic repetition has little observable effect on processing of syntactically challenging material in this kind of sentence."
"one final theoretical point is worth noting. there are now two (possibly three) sentence types for which evidence of priming in the absence of verb repetition is scant or non-existent (the current type, reduced relatives and (possibly) double-object/prepositional-object datives). this means that priming can fail to occur even when a prime and a target are perfectly aligned with respect to syntactic structure. in the priming studies to date, comprehension of early/late closure sentences and reduced relatives both entail performing cognitive processes that allow the comprehender to recover from syntactic misanalysis (this is because the available cues are stacked in favour of the 'garden path' analysis). the fact that priming has not been shown in the absence of verb repetition suggests that undertaking syntactic reanalysis during prime processing does not, by itself, help comprehenders process disambiguating information in the target sentence. if so, two possibilities spring to mind. first, syntactic reanalysis may be achieved via a lexically driven process similar to initial syntactic analysis, as opposed to general purpose heuristics. disambiguating a syntactic relationship involving one lexical item would then provide little benefit for a subsequent relationship involving different lexical items. alternatively, deploying general purpose reanalysis heuristics may take place only after a lexically driven stage of analysis has failed. this latter hypothesis would seem to require the corollary assumption that initial syntactic analysis interferes with any abiding trace of prior reanalysis procedures to such a degree that that undertaking the same reanalysis process twice in succession does not lead to a measureable processing benefit."
"because the nature of syntactic priming in comprehension, and in particular the existence of the lexical boost, has potentially significant implications for syntactic representation and processing theory, it matters whether the lexical boost is a general phenomenon that occurs widely when syntactic priming occurs, or whether it is restricted to a small set of sentence types. thus, the current study examined a sentence type which has not been previously language, cognition and neuroscience 479"
"the stimuli were identical to experiment 1. the only difference was that, by pseudo-randomly pairing prime and target sentences, lexical repetition between the primes and targets was eliminated."
"in experiment 1, early and late closure target sentences followed early or late closure prime sentences. within a given prime-target pair, the subordinate-clause verb in the prime was the same as the subordinate-clause verb in the target. early closure targets were processed more quickly when they followed an early closure prime than when the followed a late closure prime. the main verb and the next word in primed early closure targets were read faster than the same words in unprimed early closure targets. no priming effects were observed in the late closure conditions. these results indicate that processing an early closure prime sentence that has the same initial verb speeds processing of syntactically disambiguating information in an early closure target. this is the first time that priming has been demonstrated for this sentence type."
"accounts that assign a role to contextual likelihood/ conditional probability can be implemented in a number of ways, but however they are implemented, they must describe the mechanisms by which alternate syntactic analyses can rise and fall in prominence (otherwise a given syntactic analysis should always be preferred, given a specific collection of 'bottom-up' syntactic cues). some such accounts propose that combinatory syntactic information is associated with lexical items, that the strength of these associations can be modified by experience, and that contextual and lexical information are both consulted to determine the conditional probability of a syntactic analysis in a given instance [cit] . comprehenders may rely exclusively on lexically derived likelihood when a sentence is presented in isolation (within certain limits; [cit] . however, natural communication contexts provide a richer set of cues which may affect the way sentence-internal cues make contact with the syntactic structure information."
"while representational accounts such as unification grammar or hpsg provide a reasonable way to account for the lexical boost, it is important to note that the lexical boost is not universal. limits on the lexical boost have been documented in both production (e.g., [cit] ) and comprehension (e.g., [cit] ) . these limitations may be task-driven (e.g., [cit], 2008b ), but they may be driven by the organisation of lexically linked syntactic representations. boland and colleagues [cit] have proposed that lexical representation of syntactic structure information occurs only for arguments. thus, structural relations that reflect arguments (such as verb + object; preposition + object; subject -verb) are lexically stored, but other types that signal (optional) relationships, such as manner, time and place, are computed de novo each time they are encountered (construal theory makes a compatible claim relating to how parsing heuristics are applied; [cit] ) . according to the argument structure hypothesis, key structural relationships tested in the current experiments are lexically specified because the key structural ambiguity involves treating an np as the direct-object argument of a preceding verb or not. thus, the argument structure hypothesis is compatible with larger priming effects for conditions with repeated initial verbs (experiment 1) and smaller effects for conditions without repeated initial verbs (experiment 2). this account successfully predicts argument-driven effects in reduced relatives [cit], and the absence of such effects in adjunct processing [cit] ) ."
this study adds to the growing catalogue of sentence types that have been shown to produce syntactic priming effects. it may therefore be time to conclude that syntactic priming is a general phenomenon in sentence comprehension and to think about the implications this has for theories of representation and processing.
"classic studies of comprehension also purported to demonstrate the existence of syntactic priming [cit] . however, these early studies relied on mass repetition of particular syntactic structures and did not necessarily demonstrate the effects of syntactic repetition. they were vulnerable to alternative explanations, such as participant strategies and prosodic repetition [cit] . more recent studies, however, also demonstrated effects of syntactic repetition during online comprehension and do not rely on mass repetition [cit] b; [cit] ."
"standard theories of language comprehension posit a key role for representations and processes at the level of the sentence [cit] . according to these accounts, syntactic processes mediate between word-level and sentence-level semantic representations. contemporary studies have focused particularly on the nature of representations that underlie combinatory processing [cit], the degree to which semantic interpretation depends on syntactic processing (e.g., [cit] and the nature and timing of interactions between prior knowledge and experience and processing based on 'bottom-up' cues [cit] . studies often emphasise the effects of contextual manipulations because the degree to which a given stimulus evokes a particular syntactic configuration may depend on the likelihood of a given interpretation within a specific context [cit] ."
forty-eight undergraduates from uc davis participated in return for credit towards a course requirement. all were native speakers of english with normal or corrected vision and normal hearing.
"prior studies have indicated that sentence interpretation performance can be affected by the presentation of a prime sentence that matches the syntactic properties of the target sentence (e.g., [cit] . to date, none of these experiments have addressed priming effects in sentences such as (1). the current study was designed to fill this gap in the literature and to test hypotheses that have emerged from previous priming studies. one of these relates to the nature of syntactic representation and access to structural information during online sentence comprehension. syntactic theories such as unification grammar [cit], 2009 and head-driven phrase-structure grammar (hpsg; [cit] ) propose a close connection between lexical entries and syntactic structure representations [cit] . related approaches call attention to the fact that, while there are a potentially infinite number of syntactic structures, people have finite attentional and memory resources that make it unlikely that every syntactic configuration that a person encounters is stored in long-term memory [cit] . the argument structure hypothesis suggests that only syntactic relations that reflect arguments are stored in long-term memory. other aspects of syntactic structure are computed as needed using general purpose heuristics."
"1. recent studies have also demonstrated that exposure to a prime sentence that has a given structure can influence the final interpretation assigned to a subsequent globally ambiguous sentence [cit] 3. semantic repetition, in this case, refers to the simple fact that when two adjacent sentences contain the same verb, they are referring to similar actions. it is possible that when two sentences refer to the same action (e.g., 'examining'), the second could be easier to process than the first because the conceptual knowledge associated with the referenced event is already activated. 4. the terms 'early' and 'late' are not used here to imply or endorse any particular representation and processing theory. they are used because they are common terms for the kinds of sentences that were tested. 5. the argument structure hypothesis predicts the lexical boost in double-object dative vs. prepositional-object dative priming. the evidence regarding comprehension priming for this type is mixed, as noted previously."
"for sentences like (1), the main syntactic decision involves whether the second np (the sock) is to be treated as an argument (direct object) of the subordinate-clause verb. according to syntactic representation theories like unification grammar and hpsg, the lexical representation of the verb knitting specifies that it can take an np (direct object) argument or can appear (in a different form) with only a syntactic subject, as in the intransitive sentence mary was knitting. if parsing entails activation of structural information via individual lexical entries (as opposed to application of processing principles that are blind to the specifics of a given lexical item), then syntactic priming for sentences like (1) should reflect this principle. more specifically, priming effects for sentences like (1) should be larger if the prime sentence shares a critical verb with the target. the two experiments reported here test whether this is the case by manipulating verb repetition between primes and targets."
"prior studies have identified a range of local (sentenceinternal) and global (sentence-external) cues that influence processing of a range of syntactic structures. some such studies have investigated sublexical frequency information (e.g., a verb may be more likely to appear with a sentence complement than a direct object; [cit] ), lexical semantics (e.g., that a particular noun may be a better or worse syntactic subject; [cit] and contextual semantics (e.g., the degree to which a given syntactic analysis disambiguates reference; [cit] . others have focused on the way syntactic knowledge develops (e.g., [cit] and changes as the result of experience [cit] ."
"the texts were presented with a self-paced moving window procedure running on a desktop pc computer. each trial began with a series of dashes on the computer screen in place of the letters in the words. any punctuation marks appeared in their exact position throughout the trial. the first press of the space bar replaced the first set of dashes with the first word in the sentence. with subsequent space bar presses, the next set of dashes was language, cognition and neuroscience 481 replaced by the next word, and the preceding word was replaced by dashes. the computer recorded the time from when a word was first displayed until the next press of the space bar. data from two words were analysed. the disambiguating verb is the main verb in the sentence. this is the point at which the late closure analysis becomes untenable in the early closure sentences. prior studies have consistently indicated processing difficulty at this point. the next word is the word that follows the main verb. processing effects that arise from one word sometimes linger and affect reading times on the next word [cit] . thus, analysing the word after the main verb may produce evidence that a syntactic prime can reduce the lingering effects of syntactic misanalysis. prior to analysis, any reading time less than 120 ms or greater than 3000 ms was deleted and treated as missing. these criteria affected less than 1% of the data."
"the results of experiment 2 contrast with those of experiment 1. in experiment 1, priming effects were observed at both the main verb and the next word in the early closure sentences. in experiment 2, no such effects appeared. the cross-experiment analysis verified that priming effects for the early closure conditions were larger in experiment 1 than experiment 2. in experiment 2, only the difference between the late closure and the early closure conditions was statistically significant. the cross-experiment contrasts suggest that repetition of the subordinate-clause verb may be critical to the appearance of the priming effects (i.e., that no priming occurs without such repetition). more conservatively, greater priming effects in experiment 1 than experiment 2 suggest that priming of early closure is subject to the lexical boost. a stronger interpretation of this pattern of results is that structural repetition, in and of itself, does not substantially affect processing of target sentences. there are two aspects of the experiments that point towards this conclusion. both experiments included substantial numbers of early closure sentences -sentences that were disambiguated towards an intransitive subordinate clause. despite the relatively high proportion of such sentences in the experiment overall, there was no indication that processing of early closure target sentences was facilitated, except when the prime and target sentences shared an initial verb. a similar observation can be made about experiment 2. here, there were also a large proportion of early closure sentences overall, and many instances where two early closure sentences appeared immediately adjacent to one another. despite the repetition of structure, no priming effects were observed in experiment 2. these results may indicate that purely abstract structural priming does not occur for this structure (which would resemble similar outcomes for sentences containing reduced relatives). alternatively, it may indicate that priming effects are smaller for early closure sentences in the absence of verb repetition. that is, that the lexical boost increases the size of the priming effect to the point where it can be observed, but that a smaller, less detectable effect occurs in the absence of verb repetition."
"this project was supported by grants from the national science foundation (nsf#1024003, 1355776, sbe-0541953) and the national institutes of health (mh099327, 1r01hd073948). thanks to liv hoversten for assistance in data collection. thanks also to don donaldson for writing the self-paced reading software."
"2 responses have been gauged using eye-tracking (e.g., [cit] ), the visual world paradigm (e.g., [cit] ) and event-related potentials (erps; e.g., [cit] ) . these studies indicate that semantic repetition, by itself, is not sufficient for facilitated processing of syntactically challenging portions of sentences [cit] )."
"shown to produce syntactic priming. this study tested readers' responses to sentences containing 'early' vs. 'late' closure syntactic ambiguities, see (1). 4 the experiments serve a number of purposes, with the following two being the main ones. first, showing priming during comprehension for this sentence type would extend the range of sentence types for which priming has been demonstrated. second, showing larger priming effects with lexical repetition for this sentence type would extend the range of sentence types for which we have evidence of the lexical boost."
"the remainder of this paper is organized as follows. a background about ecc is presented in section 2. the design and the implementation of our bitparallel coprocessor are described in section 3. the behavior of our coprocessor is commented in subsection 3.1. our results are presented in section 4. finally, discussion is presented in section 5."
"in order to make job submission transparent to the user, we developed a tool called analisa, which takes into account the various features of all supercomputers currently supported. the tool consists of two parts, a submission layer and a worker layer. the submission layer runs on interactive (submit) hosts to manage job submission, preparation of the user software and the sandbox to contain the sub-job contents. the worker layer is executed on the compute nodes (or intermediate \"mom\" nodes depending on the cluster) and provides the mpi interface. it also handles the transfer of input files to the sandbox and output files to their final storage location. fig. 2 shows the general workflow of our tool."
"we presented the design and implementation of a coprocessor that reaches a speed comparable to projective coordinate designs [cit] . our coprocessor significantly accelerates the scalar multiplication performed over elliptic curve points represented by affine coordinates in polynomial basis. since the majority of previous works describing ecc-hardware designs are based on elliptic curve points represented by projective coordinates in a variety of bases, researchers use to suppose that these designs are much faster than ecc-hardware designs based on elliptic curve points represented by affine coordinates in polynomial basis. however, our results show that projective coordinate designs are no longer faster than affine coordinate designs. therefore, our coprocessor is suitable for comparing with any other ecc-hardware design. the obvious drawback of our design is the large area of our circuits. anyway, this large area no longer offers significant limitations for our design, since mobile computing was apart from the goal of this project. for mobile computing, we certainly will prefer a projective coordinate design."
"where m is the number of sensitive attributes. ideally, every tuple in original table should be assigned to one group in anonymous table. however, for the restriction of l-diversity, some tuples should be suppressed."
"this section experimentally evaluates the effectiveness of our approach using the adult database from the uci machine learning repository, which can be downloaded at http://kdd.ics.uci.edu/databases/census-income/."
"steering of the job submission is done using a configuration file. the content can be expressed either in a key-value format, as an xml-tree or as a json string. the minimal content must provide figure 1 . workflow of analisa: the submitter prepares the job sandbox on the scratch file system, splits the job into n-master jobs and submits the master jobs to the batch queue. the worker provides the actual mpi interface. it copies the payload from the file system to the sandbox, runs the payload assigned to the different cores within the master job and copies the output back the output registered via the job definiton."
the behavior of the pc-board adapter and the flow of data through its components is presented by fig. 4. fig. 4 shows that the cryptographic algorithm
"alice simulation jobs consist of a variety of components: event generators (pythia6/8 [cit], hijing [cit], dpmjet [cit], others), collision systems (pp, p-pb, pb-pb) center-of-mass energies, transport engines (geant3/4 [cit] ), and software versions of root and aliroot. therefore, the simulation tool must support a large number of configurations to be executed on the nersc computing clusters. for our tests, a cocktail of different configurations was created as a test suite to cover a range of typical use cases. all collision systems are included in this test suite, while for pp collisions two different center-of-mass energies were used. the tests were limited to minimum bias collisions, acknowledging that biased simulations might skew to a higher multiplicity and consequently longer job execution times, not relevant to our current investigation. the settings of the test suite are listed in tab. 1."
"our priority was to investigate ecc over binary finite fields (binary galois field -gf (2 m ) [cit] ). anyway, we found a wide range of works describing ecc-hardware designs, for which elliptic curve points are represented by projective coordinates in a variety of bases, such as normal basis [cit], optimal normal basis [cit], gaussian normal basis [cit], reordered normal basis [cit], redundant representation [cit], bit-parallel coprocessor... 243 [cit], type ii optimal normal basis [cit] . we also have verified that, while papers continuously and widely describe projective coordinate designs, affine coordinate designs still require more descriptions to allow performing more accurate comparisons among ecc-hardware designs. although we found several works describing ecc-hardware designs, for which elliptic curve points are represented by affine coordinates in polynomial basis, there is still plenty of space for research in this area. for example, we did not find any work describing a coprocessor for ecc − gf (2 m ) based on affine coordinates in polynomial basis that presents a speed comparable to other ecc-hardware designs based on projective coordinates in other bases. in other words, our research allowed finding advantages in using affine coordinate designs and recognizing absences."
"nersc has worked with cray to develop a tool called shifter [cit] based on linux container technology [cit] in which the experiments can customize some aspects of the operating environment. while use of shifter does not allow cvmfs to run natively, it does give the experiments some additional options, two of which were investigated. in one option, the full alice repository is unpacked directly into a shifter image and accessed during runtime as a normal file system. the second option used a preload procedure to deploy the alice repository onto a filesystem, mounted as a repository with a tool, parrot [cit], installed into the shifter image."
"angelms is a superset of generalization and anatomy. when k equals 1 and m equals 1, angelms actually gracefully degrades into msb [cit] . we propose a msb-kaca algorithm which combines msb [cit]"
"in this paper, we propose a framework, called angelms (anatomy and generalization on multiple sensitive), for microdata with multiple sensitive attributes based on angel [cit] . the main idea of angelms is to vertically partition attributes into several sensitive attribute tables and one quasi-identifier"
"addition is performed by an ordinary xor logic operation and is represented by the operator \" + \". we implemented addition as follows: square is represented by a 2 and uses a straightforward algorithm. we perform this operation by inserting a bit ′ 0 ′ between each bit of a. we implemented square as follows: if"
"is the number of sensitive attributes. for simplicity, we use qi denotes quasi-identifier, and s denotes sensitive attributes. we assume that t has n tuples, and t[x] is the value of tuple t on attribute x."
"one critical task for distributing work, both onto a cluster and across many resources, is the consistent distribution of experiment software to the actual compute nodes. for the test already described, locally pre-compiled packages on each cluster's scratch system were used for software distribution. this option is optimal for development work in which strict consistency with officially released software is not required. in production, however, use of software that is centrally managed by the experiment is required."
"2. the analisa tool running nuclear physics applications on hpc systems hpc compute clusters often have features that limit their usability by distributed workflows developed for serial applications. such features include whole-node scheduling, limited external network connection, time-based scheduling and historically small amounts of memory per cpu core. several of these limitations require adaptions in the workflow, particularly during the integration of hpc systems into the alice computing grid."
we randomly choose 10000 records. the description of census-income data is shown in table vii. the experiments are conducted on a pc with cpu 3.0 ghz and ram 2gb. all the algorithms are implemented in java on windows xp with jdk version 1.6.0_23.
"elliptic curve cryptography (ecc) [cit], see [cit] . the earliest researches in this branch presented several ecc-software designs [cit] . ecc-software designs are easier to develop than ecc-hardware designs. however, ecc-hardware designs are often faster than ecc-software designs. thus, ecc-hardware designs came later to supply speed requirements, [cit] . nowadays, although the literature provides descriptions of a significant variety of ecc-hardware designs, the researches often consider the following issues: elliptic curve points are either represented by affine coordinates in polynomial basis or converted to projective coordinates in other bases, [cit] . on one hand, affine coordinates in polynomial basis are suitable for hardware implementation and storage. nevertheless, they require a modular inversion (or modular division), which is the most complex and, consequently, slower operation among all important operations used to perform ecc algorithms, [cit] . on the other hand, projective coordinates in other bases allow replacing the slow modular inversion (or division) by a number of fast multiplications. nevertheless, they need more temporary storage space. therefore, accurate comparisons among ecc-hardware designs depend on finding descriptions of cryptosystems for these two elliptic curve point representations."
"since in the literature the majority of previous works describes ecc-hardware designs based on elliptic curve points represented by projective coordinates in a variety of bases, many researchers use to suppose that these designs are much faster than ecc-hardware designs based on elliptic curve points represented by affine coordinates in polynomial basis. then, these researchers discourage developing ecc-hardware designs, such as a bit-parallel coprocessor for standard ecc-gf (2 m ) on fpga, for which elliptic curve points are represented by affine coordinates in polynomial basis. however, this paper proposes the design of a high-speed coprocessor for ecc-gf (2 m ) based on affine coordinates in polynomial basis to show that this type of ecc-hardware design presents a speed comparable to projective coordinate designs. we chose elliptic curve parameters defined in standards, such as nist, ieee p1363, ipsec, wap, echeck, ansi x9.62 and ansi x9.63 [cit], since these standards are in accordance with international security requirements. our coprocessor speeds up modular inversion by using an efficient algorithm based on the stein's algorithm [cit] . it is a bit-parallel coprocessor, for which the speed is comparable to projective coordinate designs."
"a binary finite field, also called binary galois field (gf (2 m )), is a set of 2 m elements, each one represented by m + 1 bits, [cit] . therefore, the finite field arithmetic operates over these elements. the method used to perform finite field operations depends on the manner that these elements will be interpreted, i.e., the method depends on the basis representation [cit] ."
"the set of information necessary to schedule the job, such as the amount of cpu cores, the time requested for each job, the name of the job and pointer to the executable. other parameters like a set of input files and an output location are optional."
"to summarize, we have successfully commissioned a tool for running serial alice jobs on hpc systems designed for parallel processing. we have tested the tool with a suite of alice simulation jobs in which the payload is the same as would be run centrally from the alice grid and found that the single job efficiency was similar to that obtained on the more conventional pdsf cluster. we have tested and verified two methods to deliver the centrally managed alice software stack onto nersc hpc system, which represents significant initial steps towards the integration of nersc supercomputers into the alice computing grid."
"collision system center-of-mass energy the left panel in fig. 2 shows the mean time spent per event from running our test cocktail on the different platforms. in all test cases, cori performed the fastest of the three clusters. as mentioned above, pdsf is an evergreen cluster composed of several generations of cpus. when we restrict the tests to a subset of machines on pdsf with the same cpu type as used in cori, the same performance can be reached."
"a future goal of this work is the integration of cori into the alice grid facility [cit] ]. accessing the distributed software through the cvmfs system is just a first step. for example, the test jobs presented here take their payload from the local file system while, for grid jobs, it is assigned dynamically from the remote alien task queue. thus, additional integration will need to cope with scheduling differences between normal grid and hpc systems. to simplify that work, cori is initially intended only for simulations."
"the tests were run on cori using the pp-scenario at 7 tev listed in tab. 1. in the case where cvmfs is included in the image, the test is split into two parts: one with just the software stack taken from the image and the other with both the alice software and ocdb included in image. the results of the tests are shown in the right panel of fig. 2 and compared with the locally built test (labeled packman) as a measure of the optimal performance which can be reached on cori. all options for cvmfs distribution were successfully commissioned; however, the performance found with the locally-built software stack has not yet been achieved, suggesting further optimizations of the software distribution via cvmfs onto nersc supercomputers are needed."
"along side the hpc systems, nersc operates a large mid-range system of more conventional configuration, in which the henp compute cluster, pdsf, exists. pdsf is an evergreen system that has been operated for over 15 years for the high energy and nuclear physics communities at lbnl and consists of about 3000 compute cores with more that 3.0 pb of disk storage. pdsf is shared by several experiments and is part of the alice grid as a wlcg alice tier-2 center. the proximity of alice storage and grid resources on pdsf to the nersc hpc systems makes nersc an ideal site for evaluating use of hpc by alice computing operations."
"supercomputers or high-performance computing (hpc) systems are large-scale clusters that have been optimized for massively parallel processing tasks that require concurrent use of large numbers of compute cores to operate efficiently. as such, hpc systems are specialized hardware for targeted use cases that rely on low-latency communication between their processors. however, the scheduling of many large-scale jobs on a single hpc system can leave resource gaps that can be available for use by small-scale tasks, such as those in henp applications. this condition provides an opportunity for henp experiments to make use of these resources, while, at the same time, increasing the overall utilization of those systems. while the potential for opportunistic use of hpc resources can be significant, these systems are generally configured with restrictions on access and utilization not present on more conventional, commodity based clusters. these restrictions can include vendor-specific environments and workflow limitations that make these systems difficult to integrate into the computing infrastructures developed by the henp experiments to leverage their distributed resources. we report here the results of some initial work done by the alice collaboration to make use of hpc systems operated at the us department of energy's national energy research scientific computing center, nersc. the goal of this work is to evaluate the efficacy of these hpc systems for our henp applications, carried out within a framework developed to simplify an eventual integration into the larger distributed workflow of the experiment."
"the lhc experiments have adopted cvmfs [cit] as their software distribution tool, on which software is built centrally at cern and is efficiently migrated out to intermediate caches that the compute systems dynamically access. normal use of cvmfs requires a kernel modification and a disk cache local to each node, neither of which generally exists on hpc systems. our additional tests focus on making software distributed by cvmfs directly available on cori."
"the processing and analysis of event-based data from high energy nuclear physics (henp) experiments is well suited for commodity computing hardware, configured to match the memory and i/o bandwidth requirements per cpu of the workload. the event structure of the data and processing tasks are embarrassingly parallel, whereby event sets can be processed in independent jobs run on separate cpus, on different nodes, or even at different facilities. as a result, the experiments have come to rely on conventional compute clusters that range in capacity from a few hundred to many thousand cpus, which can be distributed over a number of facilities without a general loss of capability."
"tests were performed on the supercomputers cori and edison, as well as the local batch farm pdsf for direct comparison with a more conventional cluster. in all cases, the same versions of root, aliroot and geant3 were used. in the initial test, the software was obtained from a local build system component of the simulation tool on cori and edison but directly from cvmfs on pdsf. the alice conditions database, ocdb, was mirrored onto edison and cori scratch file systems, while direct access via cvmfs on pdsf was possible."
"privacy preservation on microdata with multiple sensitive attributes is a challenging work. there are some other aspects about this problem need to be considered in our future work, e.g., personalized privacy preservation [cit], efficient algorithms based on generalization and anatomy, etc.."
"nersc is the primary scientific computing center for the doe office of science and the principal provider of high performance computing services to their science programs. nersc currently operates two hpc systems, edison and cori, at its facility at lawrence berkeley national laboratory (lbnl). edison [cit], consists of over 130k cpu cores with peak performance of over 2.5 pf. cori, named after the us biochemist gerty cori, is the newest nersc hpc system and is being deployed in two phases. cori phase 1 [cit] and referred to as the \"data phase\", consists of over 1600 intel xeon haswell tm nodes and has been designed with data intensive features such as a large memory per core configuration, a 28 pb lustre scratch file system, and an 800 tb ssd-based burst buffer file system for workflows that demand high i/o capacities. the phase ii [cit] and will include about 9000 nodes, each with a large number ( 60) of intel phi knights landing tm cores. thus, cori phase ii will be optimized for massive parallel computing and is therefore an excellent testbed for aliceo 2 [cit], the next generation alice software framework."
"where [⋅] represents the nearest integer function, is the considered data, is its th neighbor, is the cost function, and is the euclidean distance between and ."
"comparing figures 3 and 5, it can be seen that although tmed tries to converge to better areas, it cannot do it in 51 steps (36 + 15), while mtmed does it very well."
"the mocus algorithm evaluates events starting from the top events down to the bottom events. every intermediate events found in the evaluation process will be substituted by the lower events. in phase 3: quantitative analysis phase. in this phase, basic events are classified into two evaluation groups, which are probabilistic method and fuzzy numbers. for illustrative purposes only, we just simply evaluate basic events x1, x2, x3, x4, x5, x6, x7, x8, x11, and x12 using probabilistic method and all these other basic events are evaluated using fuzzy numbers. basic events x9, x10, x12, x13, and x14 are evaluated using triangular fuzzy numbers of (0.1,0.15,0.25,0.3); meanwhile basic events x15, x16, x17, x18, and x19 are evaluated using trapezoidal fuzzy numbers of (0.1,0.25,0.3)."
"computational modeling plays an important role in the advancement of science by helping us to predict, optimize, simulate, and study the behavior of complex systems. there are many different ways to generate computational models but they can be divided into categories like white box and black box models. white box models are those in which every connection between input(s) and output(s) of a system can be tracked and analyzed correctly [cit] . unfortunately, understanding the interactions among components of a complex system in the real world bears some difficulties. therefore, the choice is limited by two options: finding a more suitable way for modeling and not using any model. since models have many advantages, an alternative kind of models called black box has been presented [cit] . in fact, these models mimic the behavior of the original systems so they need to be trained, although their structures and parameters may have no relation to the actual structures of the systems. thus, it is better to select the most appropriate model for the intended purpose. for example, two widely used structures which have shown good performance in real world applications are neural networks [cit] and neurofuzzy models [cit] . while the former is more efficient in extrapolation and more robust against high dimensional problems, the latter is very good for interpolation and providing better interpretation [cit] . however, proper performance of the selected structure is highly dependent on the real experimental data used to train it [cit] . the richness of the training set increases the accuracy of a model. if the dataset is not sufficiently rich, the model may be inaccurate, at least in the area where there was a paucity of training data [cit] . a solution for obtaining a richer dataset is increasing the number of data points. however, in real problems, generation of additional data may require considerable effort, time, and money [cit] . thus it is important to find a way to generate datasets that are as rich as possible using experimental design (ed) methods [cit] . although experimental design methods are not directly involved in the modeling procedure, they are equally important."
where r 0 is the probability of the top event for overall basic events and r − i is the probability of the top event by setting the probability of basic event i to 0. decision makers use this criticality index to improve the safety features of the analyzed npp. figure 4 shows the flowchart of this phase.
"if the value of the function in is less than the value of at the neighbors of that point, sign( ( ) − ( )) is equal to −1 for all . the nearest integer function changes numbers between −1 and 0 into −1. also, values between 0 and 1 (which represent data points for which the value of is greater than that at the point under consideration) are mapped into 0. therefore, (3) describes the number of neighbors of a point in which the value of is less than that point. finally, the ff will be"
"(b) for each point of the model return map, we find its nearest neighbor in the real return map and calculate its euclidean distance separation."
"this ff gives higher scores to points which (a) are local minimum, (b) have sharper slope with their neighbors, and (c) are located in areas with lower densities of data."
"the first step to achieve our purpose is constructing a return map based on one real observed time series and the data from the model. then, we perform the following steps:"
"fault tree analysis (fta) is widely used for safety analysis of complex engineering systems such as nuclear power plants (npps). conventional fta utilizes failure rates of every individual basic event constructing the tree to approximately calculate the occurrence probability of the top undesired event. however, it is often very difficult to exactly estimate the basic event failure rates due to insufficient data, environment changing or new components."
"the fps of fuzzy number a is the euclidean distance of fuzzy number a, which is calculated from the origin to the centroid of the fuzzy numbers as follows."
"modeling of systems and processes plays an essential role in their optimization and prediction. since many real systems are complex and we do not know the exact relations among their components, black box models have increasingly been of interest in modeling. a proper black box model requires a rich dataset for training. having a rich dataset may be difficult due to cost and time. in this paper, we have investigated the efficiency of a newly proposed experimental design method on gathering proper data for parameter estimation of a chaotic one-dimensional map. this method is a biologically inspired intelligent method which can be tuned to select data according to the purpose of the modeling. we are not aware of any such intelligent ed method. considering the possible costs of gathering biological data which deal with human health, we believe that any improvement in ed techniques is of great value. furthermore, there are some basic differences in parameter estimation of chaotic systems due to their sensitivity to initial condition or butterfly effect. accordingly, a new cost function which is proper for chaotic systems is used in this work. we have tested the proposed methods on a chaotic one-dimensional map which is a simple model of nonlinear feedback to account for period doubling in the erg response to periodic flashes, and the results clearly show its efficiency."
"the fnfta approach consists of four analysis phases, which are system analysis phase, qualitative analysis phase, quantitative analysis phase, and criticality analysis phase (fig. 1 )."
"phase. this phase evaluates how far a basic event contributes to the occurrence of the top undesired event by calculating the criticality index of every basic event. based on the calculated criticality index, the order of critical components can be justified. fussell-vesely (fv) importance (8) can be used to calculate the criticality index of a component [cit] ."
"the final population is shown in figure 3 . figure 4 shows the process of finding the best parameters using mtedm algorithm. parts (a), (b), (c), and (d) represent the first, 5th, 10th, and 15th iteration, respectively. it can be seen that the individuals (points) converge to the optimum area. red, green, and black points show the first, newly generated, and queen data in every iteration."
"value. an alternative method is provided in this section. in this approach we choose the best answer in each iteration as a queen. then a new term (a noise) is added in order to construct new data. this new term is like a perturbation to seek the neighbor's point for better answer. since the experimental design method is very sensitive to local minimum, this approach solves this problem by creating turbulence in the search space. so the only difference is that when the queen is selected (here queen is the point which has optimum value) and the best neighbor of queen is calculated, a little perturbation (white gaussian noise) is added to the queen in the direction of the best neighbor to achieve new data."
"heuristically, the higher the number of neighbor points that have a value greater than a point under consideration is, the greater the prominence of that point should be. thus the fitness function is multiplied by a correction term as follows:"
"this paper proposes a fuzzy numbers based fault tree analysis (fnfta) approach. the approach combines probabilistic method with fuzzy numbers to estimate the failure probability of the top event. fuzzy numbers can be in triangular and/or in trapezoidal form. the paper is organized as follows. the fuzzy numbers, fuzzy possibility scores and failure rates are explained in section 2. section 3 discusses the structure of the fnfta approach and its procedures. in section 4, a case study for npps shows the applicability of the approach. finally, section 5 summarizes future research tasks."
"although chaotic systems have random-like behavior in the time domain, they are ordered in state space and have a specific topology. here, we propose using the geometrical similarity between these attractors as the objective function for parameter estimation [cit] ."
"where p a and p b are the failure rates of basic event a and basic event b, respectively. the output of this phase is the occurrence probability of the top undesired event and is an input to the fourth phase. figure 3 shows the flowchart of this phase."
"fuzzy probabilities have been introduced to calculate the failure probability of a typical emergency cooling system using fta [cit] . the occurrence probability of the top undesired event is calculated using fuzzy multiplication rules for the boolean and gate and fuzzy complementation rules for the boolean or gate. however, this approach is limited only for trapezoidal fuzzy numbers. it also does not consider qualitative analysis and criticality analysis. the α−cut method has been introduced to calculate the failure probability of the reactor protective system (wash-1400) [cit] . all basic events are assumed to have probability distributions prior to designing triangular fuzzy numbers. the point median value and the error factor are used to represent the lower bound and the upper bound of the triangular fuzzy numbers. the middle value of the triangular fuzzy numbers is represented by the point median value. however, this approach does not consider qualitative analysis prior to estimating the occurence probability of the top event. a computational system analysis for fta called fuzzyfta has been developed and implemented to calculate the failure probability of auxiliary feedwater system (afws) of angra-i westinghouse npp [cit] and containment cooling system (ccs) of a typical four-loops pressurized water reactor [cit] . however, this methodology is applicable only for triangular fuzzy numbers."
phase 1: system analysis phase. system performance is analysed to build a fault tree describing failure modes that may occur to the system during its life time. this output becomes input to the second phase.
"phase 1: system analysis phase. in this case study, only the failure scenario of the automatic operation is analyzed. the fault tree describing the failure scenario of this operation mode is shown in fig. 5 . the top undesired event is the failure of automatic eccs (faeccs). fig. 5 still has one repeating event, which is a basic event x6. this repeating event can be removed using the combination of mocus algorithm and the rules of boolean algebra to obtain cut sets and minimal cut sets [cit] ."
"three advantages are gained from this fnfta approach. (1) probabilistic failure rate can be combined with fuzzy numbers to solve the limitation of the conventional fta. (2) the calculation of the failure probability of the top undesired event is more accurate than the previous fuzzy approach because cut sets and minimal cut sets are evaluated first in the qualitative analysis phase prior to the quantitative analysis phase. (3) the approach can be used to estimate the critical components and therefore decision makers can redesign or change critical components to improve the safety features of the system being analyzed. for future works, the conversion functions from fuzzy numbers into ffr need to be investigated in real-case studies."
"control and synchronization of chaotic systems have attracted lots of interest in a variety of areas of science in recent years [cit] . they usually require tuning the parameters of the model. however, direct measurement of parameters in a real system is often difficult. therefore, estimation of parameters from an observed chaotic scalar time series has become an active area of research [cit] . a basic method for achieving this goal involves optimization in which the model parameters are chosen to minimize some cost functions. in this work, we investigate the efficiency of the recently proposed ed method called the twilight method experimental design (tmed) [cit] for extracting rich data from a one-dimensional map. the main characteristic of the tmed is the use of prior knowledge obtained from the gathered data in extracting new data in a biologically inspired way. we show that this method not only has that advantage but also can be tuned for special purposes such as optimization, which usually is the main goal in modeling [cit] . the remainder of the paper is organized as follows: in the next section we introduce the problem which involves the details about the chaotic map we deal with and the proper cost function which we want to optimize. in the third section, we describe the tmed in detail. section 4 gives the numerical results, and section 5 is the conclusion."
"fuzzy possibility score (fps) is a crisp score that represents experts belief of the most likely score that an event may occur [cit] . centroid-based distance method is to convert fuzzy numbers into fuzzy possibility scores [cit] . the centroid (x 0,y 0 ) of the fuzzy number a is calculated as follows."
"(a) for each point of the real return map, we find its nearest neighbor in the model return map and calculate its euclidean distance separation."
"phase 2: qualitative analysis phase. repeating events are removed from the fault tree by identifying cut sets and minimal cut sets. the output of this phase is a simplified fault tree which is equivalent to the fault tree obtained in the first phase but it is free from repeating events. this output becomes input to the third phase. figure 2 shows the flowchart of this phase. phase 3: quantitative analysis phase. all basic events constructing the simplified fault tree are divided into two different evaluation techniques; probabilistic method and fuzzy numbers. the failure rates of basic events, which are evaluated using probabilistic method, can be obtained from reliable documents. the failure rates of other basic events are estimated using fuzzy numbers, which are designed based on expert justifications. these fuzzy numbers then are converted into fps and finally into ffr. after all basic events have failure rates, boolean algebra is used to estimate the occurrence probability of the top undesired event."
"effectiveness. cheetah effectively detects false sharing problems that occur in the current execution, and have high impact on performance. for effective detection, cheetah requires programs to run sufficiently long, perhaps more than few seconds. this should not be a problem for long-running applications, which are the primary targets of optimizations."
"the performance overhead of cheetah mainly comes from the handling of each sampled memory access and each thread creation. for each sampled access, we collect informationsuch as the type of access (read or write) and the number of cycles -then update the history table of its corresponding cache line. cheetah also intercepts every thread creation in order to setup the pmu unit, get the timestamp, and update the phase information. for applications with a large number of threads, including kmeans (with 224 threads in 14 seconds) and x264 (with 1024 threads in 40 seconds), setting pmu registers introduces non-negligible overhead, since it invokes six pfmon apis and six additional system calls. for other applications, cheetah introduces less than 12% performance overhead, with 4% overhead on average if these two applications are excluded."
"we show the average runtime overhead of cheetah in figure 4 . we run each application five times and show the average results here. these results are normalized to the execution time of pthreads, which means that an application is running slower if the value is higher. according to this figure, cheetah only introduces around 7% performance overhead, which makes it practical to be utilized for real deployment. during the evaluation, we configure cheetah with a sampling frequency of one out of 64k instructions. thus, for every 64k instructions, the trap handler is notified once so that cheetah can collect the information relating to memory accesses on heap and global variables. currently, cheetah filters out those memory accesses in kernel, libraries or others."
"although the presented model is essentially a theoretic one, it can be used to process continuous time input signals in real time. additionally, the internal dynamics of the network allows to tune the delay between the input signals and the responses on the output layer. finally, we want to note the possibility of hardware implementation of this model in real physical devices. in effect, devices of electronic oscillators with synchronization properties have been recently proposed for fast sensing and labeling of image objects [cit] . thus, this model is also very interesting from the practical point of view."
"as discussed before, cheetah only detects actual false sharing problems that may have significant performance impact on final performance. if the number of accesses on a falsely-shared object is not large enough, cheetah may not be able to detect it due to its sampling feature. additionally, occurrences of false sharing can be affected by the starting address of objects, the size of the cache line, or by the cache hierarchy, as observed by predator [cit] . thus, we further check the seriousness of these problems based on predator's detection results."
"list analysis pages contain analysis 'widgets' ( figure 5 ) -tools showing summary graphs or statistics, such as the statistically enriched go terms and publications for a given list of genes. information about things such as known interactions and pathways are also displayed. the list analysis pages are a very popular feature of intermine because they provide a useful summary of information from a range of integrated data. because all data objects have report pages and all query results can be converted to lists, this enables a natural exploration of different aspects of, for example, gene lists of interest, by browsing through related pathways and processes. this is helpful both for an exploratory analysis, where the aim is to expand the connections from a particular topic of interest, and for a funneling workflow, where the aim is to reduce a large starting list to a small number of promising candidates."
"the aim of this work is to combine oscillatory models of nodes with the results of classical feedforward neural networks. this combination is a step forward in order to extent the utility of powerful computational algorithms to more realistic models of neural systems. our approach is to use processing units of phase oscillators in multilayer feedforward networks. in order to use in this way these oscillators, we encode their activities as phase differences between them. as result we show that a phase oscillator perceptron behaves in analogy to the classical perceptron model, important fact that allows us to construct feedforward networks, and use the backpropagation algorithm for learning."
"cheetah collects the following runtime information of every thread: the execution time -rt _t, the total number of accesses -accesses_t, and the total cycles of all memory accesses -cycles_t. in order to avoid any lookup overhead, cheetah lets every thread handle the sample events of the current thread, and records the corresponding number of accesses and cycles. to collect rt _t, cheetah intercepts the creation of threads by passing a custom function as the start routine. cheetah acquires the timestamp before and after the execution of a thread using the rdtsc (read-time stamp counter) [cit], and regards the difference to be the execution time of a particular thread. in current implementation, we do not take into account the waiting time of different threads caused by synchronizations; we leave this for future work."
"intermine instances are built from various data sources; for example, uniprotkb, gene interactions, and go (gene ontology) annotations each have their own data format. to facilitate the integration of different data sources, data parsers written in java are provided. the use of data represented by a particular standard facilitates the incorporation of future data into the database. for example, interaction data can be represented by the psi-mi standard and by supporting this standard in intermine we can easily accommodate future data published in this format. the details of the data integration modules that intermine comes equipped with are shown in table 2 . parsers have been developed in response to demand from the intermine user community, and we will continue to respond to such demands in the future."
"intermine comes with a web application that includes flexible query capabilities and a number of analysis and visualisation features. the workflow of the web application is shown in figure 2 . searches can be run using template queries (saved searches for performing common tasks) or custom written ones made using the querybuilder. query results can be exported, analysed as lists or the report pages of individual objects from the results table query results can be explored. lists created from query results can in turn be used for running further queries. the search results are linked to report pages (figure 4 ), which display a rich collection of information."
"different classes of user have different capabilities: super-users can configure the positioning of content on report pages and the front page using a web-based tagging system. in addition they can use the web application to precompute and publish template searches, as well as creating and useful public lists, e.g. lists of specific manually curated genes."
"while different data integration solutions have their strengths and weaknesses, an ideal data management solution has not yet been found for biological sciences. instead, a number of established systems exist, and their use is appropriate in different scenarios. while a formal benchmarking review at present does not exist, a basic comparison in a neutral review has been published: [cit] looked at effective strategies for data integration, and in the process compared in-termine to four other systems -biomart, bioxrt, open genome resource (oger) and profess. [cit] in table 3, in order to give an idea of the way intermine compares to other available general data warehousing frameworks."
"as a matter of example, phase oscillator networks based on the kuramoto model have shown to be a good first approximation for oscillatory artificial neural networks and they have been used to perform some kind of computational operations [cit] . hopfield-like models of associative memory [cit] have been developed using phase oscillator networks (aoyagi [cit], aonishi [cit] and nishikawa [cit] ). additionally, a phase oscillator model has been used in the construction of a central pattern generator by using the symmetry breaking of a pacemaker, allowing to the system evolves between its fixed points [cit] ."
"as described, there are a number of quality checks looking at consistency and technically accurate data integration -e.g. highlighting and resolving missing, duplicate or inconsistent fields. however, intermine does not automatically check for contradictory facts -differences in findings between the different datasets. intermine is not a curation tool, and it is up to the database developers working with individual intermine instances to decide what datasets to include, and how to prioritise them in terms of reliability. this means that, while data integration will be performed correctly, it is up to the developers to choose data in an informed manner, and up to the users to interpret the data depending on its source. all integrated data is displayed, and in the case of contradictory facts, the database users can decide for themselves which source to believe. this means intermine can be used as a tool for highlighting inconsistencies between different datasets."
"we evaluate the precision of assessment on two applications that are reported to have false sharing problems: linear_regression and streamcluster. we list the precision results in table 1 . in this table, linear_regression is abbreviated as \"linear_reg\". we evaluate these applications when the number of threads is equal to 16, 8, 4, and 2, correspondingly. we list the predicted performance impact in the \"predict\" column, and the actual improvement in the \"real\" column of the table. the last column (\"diff\") of this table lists the difference between the predicted improvement and the real improvement. if the number is larger than 0, the predicted performance improvement is less than the real improvement. otherwise, it is the opposite. table 1 shows that cheetah can perfectly assess the performance impact of false sharing in every case, with less than 10% difference for every evaluated execution."
"since the couplings f i ði i, ϕ ! þ take constant values 7α, the fixed points of this dynamics are located on the corners of a hypercube of n dimensions in the space of phase difference fϕ i g. it has one corner in the origin and side of length π. in particular, as we will see later, this dynamics has only one stable fixed point, all the other fixed points are at least saddle ones. this is satisfied if we take the weights and thresholds from a classical feedforward perceptron network which has learned to solve a specific problem. fixing the input values i i to the nodes of the input layer, the system evolves to the corresponding learned stable fixed point independent of the initial conditions."
"intermine is data-model agnostic and can operate on any data model but we provide a core data model specifically for handling biological data based on the sequence ontology [cit] . we use an so-based core model for a number of reasons. firstly, so is already used by a number of model organism databases (mods) based on the generic model organism database (gmod) framework, including flybase, wormbase, sgd, rgd and mgi, among others. using an so-based core model facilitates easy integration of data from these mods, as well as facilitating interoperability. furthermore, it is an established sequence ontology, with its core set generally undergoing relatively minor changes (mainly additions of new terms). the intermine data model is extensible and customisable by editing an xml file, so when changes to so do happen [cit], the data model can easily be modified to take them into account. most recent so changes have not affected the data model used by intermine, but in case of major changes, we provide users with update scripts for switching to the updated ontology, while keeping the data intact."
"this paper presents cheetah, a lightweight profiler that identifies false sharing in multithreaded programs. cheetah employs the first approach that quantifies the optimization potentials of fixing false sharing instances, without actual fixes. for detection, cheetah distinguishes true and false sharing, and only reports problems that significantly impact overall program performance. cheetah provides insightful guidance for fixing problems while only introducing 7% runtime overhead, making it ready for real deployment."
"template queries can be created for commonly run searches and there is a sophisticated querybuilder ( figure 6 ) for constructing advanced custom queries. because both the templates and custom queries can be run within the web application using a graphical interface, querying the data does not require programming knowledge. queries can also be exported and imported as xml and thus can be shared between users as well as between intermine instances. query results can be saved and exported from the intermine web application in a range of common formats, for example as tab-delimited, commadelimited, gff3 or bed files and the users can customise the data fields to export. data can also be exported directly into galaxy."
"as prior work [cit], we perform experiments on two well-known benchmark suites: phoenix [cit] and parsec [cit] . we intentionally use 16 threads in order to run applications for sufficiently long time, as cheetah needs enough samples to detect false sharing problems. basically, we want to make every application run for at least 5 seconds in order to collect enough samples. for parsec benchmarks, we are utilizing the native input. for some applications of phoenix, such as linear_regression, we explicitly change the source code by adding more loop iterations."
"intermine is a data warehouse system suitable for any size of database (including very large ones), designed specifically to enable the integration of datasets of varying size, quality and complexity. it comes with features such as optimised query facilities, a web application with analysis and visualisation tools, and web services with libraries available in five languages. intermine is an open source software project and is freely available under the lgpl license. more information about the project and links to the source code can be found on the project website at http://www.intermine.org, with a guide to setting up an intermine database provided at http://www.intermine.org/wiki/gettingstarted. table 1 : load statistics of different intermine powered databases. the build time includes creating and storing objects, post-processing and making backups."
"intermine is a robust, established system suitable for large data warehouses containing complex data, intended to be maintained as a resource, and used for multiple analyses. users have access to an established data warehouse framework with advanced features, and the web application included means that the resource can easily be released for public use. there is a time investment that goes into setup and maintenance, so setting up an intermine instance is less appropriate for users looking for a quick solution or simple analysis of small scale data. however, intermine implementation through virtual instances will go some way to mitigating set-up costs, and this work is in progress."
"os-related approaches. sheriff proposes turning threads into processes, and relies on page-based protection and isolation to capture memory writes; it reports write-write false sharing problems with reasonable overhead (around 20%) [cit] . plastic provides a vmm-based prototype system that combines performance counter monitoring (pmu) with page granularity analysis (based on page protection) [cit] . both sheriff and plastic can automatically tolerate serious false sharing problems. however, these tools have their own shortcomings: sheriff can only work on programs using pthreads libraries, and without ad-hoc synchronizations or sharing-the-stack accesses; plastic requires that programs run on the virtual machine environment, which is not applicable for most applications."
"cheetah is the first tool that can assess the performance impact of false sharing problems. based on this information, programmers may save huge amounts of manual effort spent unnecessarily on applications with trivial false sharing problems."
"despite this work does not intent to claim any biological interpretation of the model, it is important to note that the effect of this proposed mechanism is to change the character of the synapse f pr between excitation and inhibition. although, we propose this mechanism just as an effective process, evidence of this possibility is known in real biological systems as reference [cit] presents. finally, a similar effect can be reached by cotransmission [cit] ."
"the data presented on the report page is hyperlinked both to the corresponding database objects and to external websites, and can be explored and queried further. for example, someone starting from a gene search can click on a pathway that the gene is involved in, save a list of all the genes associated with that pathway, and use that for further searches."
"this paper introduces the first approach to predict the potential performance impact of fixing false sharing instances, without actual fixes. based on our evaluation, cheetah can precisely assess the performance improvement, with less than 10% difference. by ruling out trivial instances, we can avoid unnecessary manual effort leading to little or no performance improvement."
"intermine doesn't provide facilities for federation, but otherwise matches the features of the other available data warehouses (an api with libraries in 5 [cit] publication). in addition, intermine has several unique features, including a range of data parsers, flexible query facilities allowing access to data at a range of levels, and tracking of data provenance. in general, federation as an approach has the starting advantage that the user can set up a simpler database and be connected to the data from federation partners without having to load the data from each one. this is not the case for intermine -while some interoperability links exist between different intermine instances such as the ability to export lists of gene identifiers via orthologues, in general, data has to be loaded separately into each intermine instance."
"precise detection. cheetah reports precise information for global variables and heap objects that are involved in false sharing. for global variables, cheetah reports names and addresses by searching through the symbol table in the binary executable. for heap objects, cheetah reports the lines of code corresponding to their allocation sites. thus, cheetah intercepts all memory allocations and de-allocations to obtain the entire call stack. cheetah does not monitor stack variables because they are normally accessed only by their hosting threads. it is noted that the default backtrace function in glibc is extremely slow due to expensive instruction analysis. cheetah utilizes the frame pointers to fetch the call stack efficiently. moreover, we only collect five function entries on the call stack for performance reasons."
"experimental setup. we evaluate cheetah on an amd opteron machine, which has 48 1.6 ghz cores, 64 kb private l1 data cache, 512 kb private l2 cache, 10 mb shared l3 cache, and 128 gb memory. we use gcc-4.6 with -o2 option to compile all applications. because the machine is a numa machine, and the performance may vary with different scheduling policies, we bind the threads to cores in order to acquire consistent performance."
"multicore processors are ubiquitous in the computing spectrum: from smart phones, personal desktops, to high-end servers. multithreading is the de-facto programming model to exploit the massive parallelism of modern multicore architectures. however, multithreaded programs may suffer from various performance issues caused by complex memory hierarchies [cit] . among them, false sharing is a common flaw that can significantly hurt the performance and scalability of multithreaded programs [cit] . false sharing occurs when different threads, which are running on different cores with private caches, concurrently access logically independent words in the same cache line. when a thread modifies the data of a cache line, the cache coherence protocol (managed by hardware) automatically invalidates the duplicates of this cache line residing in the private caches of other cores [cit] . thus, even if other threads access completely different words inside this cache line, they have to reload the entire cache line from the shared cache or main memory."
"with a variety of data solutions available, it can be difficult to choose the most appropriate one for your specific data management requirements. here we present an overview of the existing solutions, the specific strengths of intermine, and the situations in which it is an appropriate choice of data platform. there is a lot of overlap between the features available from a number of different data management solutions -however, it is the specific combination of features available together that makes them appropriate for particular uses. while a range of database management systems offer, variously, query flexibility, speed, and the ability to host large quantities of data, an ideal combination of these is rare, with for example, speed often coming at the price of reduced flexibility."
"cheetah reports false sharing problems with a significant performance impact, where they will incur a large number of cache invalidations on corresponding cache lines. however, tracking cache invalidations turns out to be difficult because invalidations depend on the memory access pattern, cache hierarchy, and thread-to-core mappings. to address this challenge, cheetah proposes a simple rule to compute cache invalidations: when a thread writes a cache line that has been accessed by other threads recently, this write access incurs a cache invalidation. this rule is based on the following two assumptions, which are introduced in prior work [cit] ."
"cheetah is an efficient false sharing detector, with only ∼7% performance overhead. cheetah utilizes the performance monitoring units (pmus) that are available in modern cpu architectures to sample memory accesses. cheetah provides sufficient information on false sharing problems, by pointing out the lines of code, names of variables, and detailed memory accesses involved in the false sharing. cheetah is a runtime library that is very convenient to be deployed; there is no need for a custom os, nor recompilation and changing of programs. overview of cheetah's components (shadow blocks), where \"fs\" is the abbreviation for false sharing. figure 2 shows the overview of cheetah. the \"data collection\" module gleans memory accesses via the address sampling supported by the hardware performance monitoring units (pmus) and, with the assistance of the \"driver\" module, filters out memory accesses associated with heap or globals, feeding them into the \"fs detection\" module. at the end of an application, or when cheetah is requested to report, the \"fs detection\" module examines the number of cache invalidations of each cache line, differentiates false sharing from true sharing, and passes false sharing instances to the \"fs assessment\" module. in the end, the \"fs report\" module only reports false sharing instances with a significant performance impact, along with its predicted performance improvement after fixes provided by the \"fs assessment\" module."
"in the end, cheetah assesses how fixes will change the performance of the application. actually, improving the performance of a thread may not increase the final performance of an application if this thread is not in the critical path. to simplify the prediction, as well as verify our idea, cheetah currently focuses on applications with the normal fork-join model shown as figure 3 . this model is the most important and widely-used model in actuality. all applications that we have evaluated in this paper utilize this fork-join model. the performance assessment will be more complicated if nested threads are utilized inside an application."
"according to the basic rule described above, it is important to track memory accesses in order to compute the number of cache invalidations that occur on each cache line."
"each source can add classes and fields to extend the data model if required, and each source defines how its own data should be integrated. construction of an intermine data warehouse (for example, flymine) means configuring which sources should be included and specifying the particular organisms or data files to include. during build time, all the model-based components of the system including the database, the java classes and the web application are automatically derived from the data model. this allows simple and error-free upgrading of the user interface and api as the data model is adjusted and new types of data are added. this system reduces the development time required to update instances of intermine, and the fact that the data model is extensible makes it possible to incorporate new data types. it also makes it possible to construct comparable data warehouses for different organisms and datasets, creating the potential for developing, for example, cross-species interoperability. data can also be loaded from an intermine xml format, allowing the parsing code to be written in a language other than java."
"the complexity of bioinformatics data introduces a number of issues for data integration [cit], to which, at present, there is no one solution. one of the contributions that will system advantages disadvantages biomart -tools for federating a variety of biological databases -queries limited to only two datasets at once -unified web-based user-friendly interface for data mining -not possible to edit or create new filters -supports programmatic access (perl api, restful web services) -queries defined as a set of successive filters bioxrt -flexible and extensible database structure -all pieces of data are defined as strings of characters -tools for importing spreadsheets -queries are constrained to string matching -no advanced data mining tools intermine -parsers for integrating data from numerous formats -no tools for classifying or clustering data -web access to integrated data at a number of levels, from simple browsing to complex queries -queries don't include similarity functions to address annotation errors -facilities for adding one's own data -user friendly web interface that can be easily customised -data provenance is tracked oger -genome sequences and annotations can be automatically downloaded -no tools for clustering and statistical analysis -features cross-references to external databases -no advanced data mining tools beyond sequence allignment profess -unified text field for mining data from any integrated database -query system is based on a noncustomisable set of filters -provides clustering and aggregation tools for statistical analysis of large datasets -does not support similarity functions between standard blast searches -features a user friendly modular web interface table 3 : [cit], describing the advantages and disadvantages of different data warehouse systems."
"after cheetah computes the possible execution time of an application after fixing a false sharing problem, cheetah will compute and report the potential performance improvement of every falsely-shared object, based on eq.(4). then, programmers can focus on those with the most serious performance impact. we further verify the precision of cheetah's assessment in section 4.3."
"cheetah specifically addresses these existing problems, and provides much richer information for optimization, including word-level accesses and potential performance impact after fixes. also, cheetah provides better effectiveness than existing pmu-based approaches since it samples much richer events such as memory accesses."
"as we have seen, the fixed points are located on the corners of the hypercube in the space of phase differences where by construction, only one corner of the hypercube can have all these pairs compatible and there is only one stable fixed point in the system. all the other fixed points have at least one incompatible pair, and in consequence, at least one unstable direction."
"cheetah successfully detects two known false sharing problems with significant performance impact, includinglinear_regression in phoenix and streamcluster in parsec. on average, cheetah only introduces around 7% performance overhead on all evaluated applications, which makes its use practical for real deployment."
"in the remainder of this section, we elaborate on how we track memory accesses in section 2.1, how we locate a sampled access's cache line in section 2.2, how we compute cache invalidation based on sampled memory accesses in section 2.3, and how we report false sharing in section 2.4."
"based on the runtime information of every parallel and serial phase, cheetah assesses the final performance impact by recomputing the length of each phase, and the total time after fixing. the length of each phase is decided by the thread with the longest execution time, while the total time of an application is equal to the sum of different parallel and serial phases."
"this work is organized as following. we present in the second section a model of interaction between phase oscillators, the phase oscillator perceptron model and the feedforward networks. in the section numerical study, we show a network able to solve the xor logic problem. in the fourth section, we show a linear stability analysis for this model. finally, we present a discussion and the conclusions in the last section."
"assumption 2 further defines the behavior related to cache eviction. in reality, a cache entry will be evicted when the cache is not sufficient to hold all active data of an application. without this assumption, we should track every memory access and simulate the cache eviction in order to determine the accurate number of cache invalidations, which is notoriously slow and suffers from limited precision [cit] . as with the first assumption, assumption 2 may also overestimate the number of cache invalidations. based on this assumption, if there is a memory access within a cache line, the hardware cache for a running thread always holds the data until an access issued by other threads (running on other cores, by assumption 1) invalidates it. by combining these two assumptions, cheetah identifies cache invalidations simply based on memory access patterns, independent of the underlying architecture and specific execution conditions."
"at first, cheetah predicts the possible cycles of accesses after fixing false sharing of the object o. cheetah tracks the number of cycles and accesses on each word, thus it is convenient to compute the total cycles of accesses -cycles_o, and the total number of accesses -accesses_o, on a specific object o."
"the remainder of this paper is organized as follows. section 2 describes false sharing detection components of cheetah. section 3 discuesses how cheetah assesses the performance impact of a false sharing instance. section 4 presents experimental results, including effectiveness, performance overhead, and assessment precision. section 5 addresses main concerns about hardware dependence, performance, and effectiveness. lastly, section 6 discusses related work, and section 7 concludes this paper."
"some of the cons are unavoidable trade-offs, such as the build time requirement for a read-only database. other ones are issues that we are working to improve, such as setting up a virtual machine on amazon with an intermine instance, giving users the ability to use intermine without having to install and configure third party software such as postgresql and tomcat. in general, however, we think that as a large, established data warehouse system, intermine has distinct advantages to offer to potential users."
"before final execution, sql queries are passed to the queryoptimiser (a java program developed to enhance intermine performance), which is able to re-write any sql query to make use of precomputed tables (analogous to materialised views). the whole query or parts of it may be replaced by precomputed tables can be created while the database system is running in production so performance can be adapted to match actual usage. within the web application, users with the appropriate permissions, super-users (typically developers), can immediately precompute any new template queries to ensure they run quickly. this approach separates the definition of the data model from performance optimisation, making it possible to tailor the performance of an intermine warehouse for types of queries not known at design time. a particular benefit of using this system is that the queries can be optimised without the need to de-normalise the data schema. additionally a cache is used to speed up the performance by using information from previously run queries."
"since pmus only sample one memory access out of a predefined number of accesses, this approach greatly reduces the runtime overhead incurred by performance data collection and analysis. moreover, using pmu-based sampling does not need to instrument source or binary code explicitly, thus providing a non-intrusive way to monitor memory references. cheetah shows that pmu-based sampling, even with sparse samples (e.g., one out of 64k instructions), can identify false sharing with a significant performance impact."
"the build system allows for the integration of data sets of varying size and complexity. it can be configured to make regular data backups, saving time and effort by having restore points for the database. post-processing tasks can manipulate the data and add extra information after all data has been imported -for example, gene or chromosome lengths can be calculated. further to this, a set of both manual and automated data quality checks also exist. data download scripts check for new updates and data parsers include a number of data validation and integration checks. quality checks are also run post-build. consistency checks such as looking at the number of database objects created and checking for duplicate identifiers and empty fields help highlight data issues. the standard postbuild quality check protocol runs scripts that execute a collection of both simple and complex queries, which confirm that the data have been integrated correctly and flag any inconsistencies. the scripts also compare the new database build to the previous build (if available), and indicate the percent change between the two. this improves database quality by highlighting any problems that have been introduced during the new database build."
"implementation. in order to sample memory accesses, cheetah programs the pmu registers to turn on sampling before the main routine. it also installs a signal handler to collect detailed memory accesses. cheetah configures the signal handler to respond to the current thread, by calling the fcntl function with the f_setown_ex flag. this method avoids any lookup overhead and simplifies signal handling. inside the signal handler, cheetah collects detailed information for every sampled memory access, including its memory address, thread id, read or write operation, and access latency, which can be fed into the \"fs detection\" module to compute the number of cache invalidations, as well as the \"fs assessment\" module to predict the performance impact."
"an intermine web application presents visitors with both a graphical user interface and access to machine readable web services. the web services, to be described in more detail in a forthcoming paper, expose the majority of the functionality underlying an intermine application through standard http 1.0 method calls (often termed a restful web service). this design allows for the greatest variety of clients to be supported directly, in particular it allows for a rich javascript client to access inter-mine functionality from third party web sites. to support bioinformaticians, intermine has published libraries to facilitate access to the intermine api in java, perl, python, ruby and javascript. figure 6 : the querybuilder provides a graphical interface for browsing through the data model and constructing and editing queries. in this example, the constructed query is searching for all genes containing the string \"alpha\" in drosophila, and then also displaying their homologues from the species m. musculus."
consider now an external signal s p arriving to the processing unit. we can compute such signal by making the synapse f pr a function of it as follows:
"the intermine system is based around the objectstore -a custom object/relational mapping system implemented in java which has been optimised to support read-only performance. the read-only performance gives speed benefits as extensive indexing can be performed during the database build. the objectstore can be accessed from the web application and through web services, and executes queries in a postgresql relational database. here we give the details of the objectstore and explain the process of query optimisation."
"as described, intermine has been designed to enable the efficient construction and flexible querying of large databases with complex data integration requirements. any parts of the data model can be queried, and intermine also includes functionality for querying features which overlap specific genome ranges. specific distances upstream and downstream of genes can also be represented to enable querying for genes that are near other features."
"the system is also fast enough to deal with large quantities of data -as shown in table 1, the modmine database contains 130 million objects, and its size with precomputed tables is 750 gb, with metabolicmine being even larger, containing 260 [cit] gb including precomputed tables."
"in this equation, the parameter h i is the threshold of the node ϕ i . for this system we design a kind of energy or potential for each layer of the network as follows:"
"one of the difficulties of data integration comes from the need for complicated cross-referencing of identifiers from different data sources. the information from different databases needs to be correctly interlinked, and the system also needs to be capable of dealing with changing biological knowledgefor example, incorporating information on new gene models into the database, while still keeping the older datasets usable. intermine tackles this using an identifier resolution system. in brief, when a data file is loaded, the identifier is checked against an \"id resolver\" -a map connecting the identifier to a collection of synonyms and cross-references. the id resolver is created using information from the relevant model organism databases."
"logging in is not required for using intermine -for users who are not logged in, details of lists are saved for the duration of the session using cookies. however, logging into an intermine instance does have advantages as it saves the lists and queries permanently under a given username in a user's private 'mymine' workspace. one way of logging in is by registering as a user of a given intermine instance. it is also possible to log in by authenticating with openid providers such as myid or yahoo. a user's lists are only saved in the single intermine instance that they were created in -e.g. logging into yeastmine with openid will not automatically transfer their gene lists from ratmine. lists can, however, be exported from one intermine instance to another. user input descriptions (where applied) are also saved alongside the lists."
"unlike true sharing, false sharing is generally avoidable. when multiple threads unnecessarily share the same cache line, we can add byte paddings or utilize thread-private variables so that different threads can be forced to access different cache lines. although the solution of fixing false sharing problems is somewhat straightforward, finding them is difficult and even impossible with manual checking, especially for a program with thousands or millions of lines of code."
"however, it is impossible to know the average cycles of every access after fixing -avercycles_no f s, without actual fixes (cheetah utilizes the average cycles in serial phases (avercycles_serial) to approximate this value). there is no false sharing in serial phases, and avercycles_serial represents the least number of cycles for memory accesses after fixes. if cheetah does not track any accesses in serial phases, a default value learned from experience will be utilized as avercycles_serial. in reality, avercycles_no f s can be larger than avercycles_serial, since fixing false sharing may lead to excessive memory consumption or the loss of locality [cit] . cheetah actually predicts the best performance of fixing a false sharing instance."
"a phase oscillator perceptron (processing unit) consists on two phase oscillators ϕ r and ϕ p as fig. 1a shows. they are coupled according with the previous model (eq. (2)) by a symmetric synapse f pr . in order to process information with this unit we have to define firstly the meaning of activation. in the classical perceptron model, a unit is activated when its output is one, and when it is zero, the perceptron is inactivated. in analogy, we say that a processing unit is activated if the phase difference between its oscillators is π, and inactivated, if this phase difference is zero."
"although written in java, the objectstore does not share java's restricted type semantics and can model any directed acyclic graph of types, such as those represented by multiple inheritance. techniques that are designed to manage the presence of such objects include the de-normalisation of tables and a separation between object storage and field value storage to minimise table reads when querying."
"cheetah tracks the creations and joins of threads in order to verify whether an application belongs to the fork-join model or not. cheetah also collects the execution time of different serial and parallel phases, using rdtsc (read-time stamp counter) available on x86 machines [cit] . in the forkjoin model, shown in figure 3, an application leaves a serial phase after the creation of a thread; it leaves a parallel phase after all child threads (created in the current phase) have been successfully joined."
"hardware dependence. cheetah is an approach that relies on hardware pmus to sample memory accesses. to use cheetah, users should setup the driver to enable the pmubased sampling beforehand. afterward, they can connect to the cheetah library by calling only two apis: one api is to setup pmu-based registers, while the other handles ev- performance overhead. on average, cheetah only introduces 7% performance overhead for all evaluated applications. however, cheetah does introduce more than 20% overhead for two applications that having a large number of threads, because cheetah should monitor thread creations and setup hardware registers for every thread. however, this should not be of large concern when using cheetah. first, the creation of a large number of threads in an application is atypical. secondly, we expect this overhead could be further reduced with improved hardware support."
"permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. thus, it is important to employ tools to pinpoint false sharing and provide insightful optimization guidance. however, existing general-purpose tools do not provide enough details about false sharing [cit] . existing false sharing detectors fall short in several ways. first, most tools cannot distinguish true and false sharing, or require substantial manual effort to identify optimization opportunities [cit] . second, tools [cit] based on memory access instrumentation introduce high runtime overhead, hindering their applicability to real, longrun applications. third, some tools either require os extensions [cit], or only work on special applications [cit] . fourth, no prior tools assess the performance gain from eliminating an identified false sharing bottleneck. without this information, many optimization efforts may yield trivial or no performance improvement [cit] ."
"assumption 1 is reasonable because the over-subscription of threads is generally rare for computation-intensive programs. but we may overreport the number of cache invalidations in the following situations: (1) if multiple threads are actually scheduled to the same physical core, or (2) different cores may share part of the cache hierarchy (instead of having private caches), or (3) hyper-threading technology is enabled such that multiple threads may share the same cpu. however, we argue that the problem of overreporting can actually cancel out the weakness of using the sampling technique. this assumption avoids the tracking of thread-to-core mapping, as well as knowledge of the actual cache hierarchy."
"the remainder of this section discusses the detailed assessment step-by-step. for reasons of simplicity, we abbreviate the falsely-shared object as \"o\", the related thread as \"t\", the prediction as \"pred\", the runtime as \"rt \", and the application as \"app\"."
"a data model is defined at the object level by an xml file. java objects, the relational database schema and all model-specific parts of the web application are generated automatically, reducing the maintenance overhead when data model changes are required. data are loaded as java objects or as xml conforming to the specified model. integration of data from multiple sources is configured to define how equivalent objects from different sources should be merged. as different data sources may provide different fields, multiple 'keys' can be defined for a particular type. for example, 'gene' objects may be merged according to an 'identifier' field or a 'symbol' field. as described above (section 3.4), a priority configuration system is used to resolve conflicts between data sources."
"correct detection. cheetah tracks word-based (four byte) memory accesses on susceptible cache lines using the shadow memory technique: that is, the amount of reads or writes issued by a particular thread on each word. when more than one thread access a word, cheetah marks this word to be shared by multiple threads. by identifying accesses on each word of a susceptible cache line, we can easily differentiate false sharing from true sharing, since multiple threads will access the same words in true sharing. word-based information also helps programmers to decide how to pad a problematic data structure during fixing phases. it is very common that the main thread may allocate and initialize objects before they are accessed by multiple child threads. prior work, including predator [cit], may wrongly report them as true sharing instances. cheetah avoids this problem by only recording detailed accesses inside parallel phases."
"we run these applications on our experimental hardware, with and without false sharing problems. figure 7 shows the performance impact. actually, these benchmarks do not show a significant speedup after fixing, with less than 0.2% performance improvement. this behavior actually exemplifies the advantage of cheetah: because cheetah only reports false sharing problems with significant performance impact, it can potentially save programmers manual effort unnecessarily spent on applications capable of only negligible performance improvement."
it is a heap object with the following callsite: linear_regression-pthread.c: 139 figure 6 . the data structure and source code related to a serious false sharing instance in linear_regression.
"implementation to use its custom heap, cheetah intercepts all memory allocations and deallocations. cheetah initializes the heap before an application enters the main routine. cheetah maintains two different heaps: one for the application itself, as well as one for internal use. for both heaps, cheetah manages objects based on the unit of power of two. for each memory allocation from the application, cheetah saves the information of callsite and size, which helps cheetah to precisely report the line information of falsely-shared objects. cheetah allocates two large arrays (using mmap) to track the number of writes and detailed access information on each cache line. for each memory access, cheetah uses bit-shifting to compute the index of its cache line."
"the core biological model has been expanded for individual intermine instances in order to include, for example, species-specific features or particular types of data that are of interest to the users and are not covered by the so (e.g. interaction data, or publications details). the ease of doing this, with the rest of the system being automatically updated, is one of the advantages of using intermine. in these cases, it is the responsibility of the individual databases to ensure correct model migration between versions, if the added elements of their core model change."
"to give an idea of the performance of the intermine system, an overview of statistics about a sample of currently active mines maintained by the intermine group is shown in table 1 . depending on the machine and data types, around 100,000/objects per minute is currently a typical average load rate."
"cheetah significantly reduces the performance overhead by leveraging pmu-based sampling mechanisms that are pervasively available in modern cpu architectures, such as amd instruction-based sampling (ibs) [cit] and intel precise event-based sampling (pebs) [cit] . for each sample, the pmu distinguishes whether it is a memory read or write, captures the memory address, and records the thread id that triggered the sample. these raw data will be analyzed to determine whether the sampled access incurs a cache invalidation or not, based on the rules described in section 2."
a large number of data types have been integrated into the various intermine instances. these data types range from basic genome annotation and protein information to experiment data such as gene expression and chip-seq results. an overview of the types of data included in different intermine instances is shown in figure 1 .
"external display tools such as gbrowse (http://gmod.org/wiki/gbrowse), jbrowse (http://jbrowse.org/) and cytoscape web (http://cytoscapeweb.cytoscape.org/) have been integrated within the web application and it is straightforward to add others. in the case of gbrowse, data are not served directly from the features stored in intermine, but from a gbrowse-specific database. this database is loaded as part of the intermine build process, so ensuring that the same features are stored both in gbrowse and in the intermine database."
"cheetah makes the first attempt to quantitatively assess the potential performance gain of fixing a false sharing instance based on the results of an execution. we agree that different executions may vary on the specific details, but this should not change the overall prediction result: whether a false sharing instance is significant or not. actually, the evaluation described in section 4.3 confirms that our predicted performance has less than a 10% difference from that of the actual fixes. based on this prediction, programmers can focus on severe problems only, avoiding unnecessary manual effort spent on insignificant cases."
"based on these two observations, we propose to use the sampled cycles to represent the whole execution, and further predict the performance impact of falsely-shared objects by replacing these cycles with the average cycles of memory accesses without false sharing. the assessment is performed in three steps, listed as follows:"
"the architecture of the proposed oscillatory perceptron corresponds to an high order synapses mechanism. in effect, the synapse f pr between the two oscillators is a function of the input signal s p, that in general as we shall see later, it is the sum of the synapses coming from other nodes of the system. this way of computation is quite different from previous models of phase oscillator networks (kuramoto-like models) where the synapses or couplings between nodes are constants. high order synapses are well known in biological systems and they have been also considered in artificial neural networks as for example references [cit] show."
"to address this problem, cheetah maintains a two-entry table (t ) for each cache line (l), in which each thread will, at most, occupy one of these two entries. in this table, each entry has two fields: a thread id and an access type (read or write). it computes the number of invalidations according to the rule described in section 2. in case of a cache invalidation, the current access (write) is added into the corresponding"
"this document provides further details of how intermine is implemented internally, as well as information on data types contained in different intermine instances, statistics such as size and build time, and highlights of major features. it is intended as a more detailed guide to the system to anyone wishing to set up an intermine database, as well as providing some background information to interested users. we also provide a discussion of the advantages and disadvantages of the intermine system, in order to help potential users decide whether intermine is a suitable application for their intended use."
"in this section, we review existing tools for the detection of false sharing issues, as well as other techniques of utilizing pmus for dynamic analysis. cheetah is the first work to predict the potential performance improvement after fixing false sharing problems, relying on access latency information provided by the pmu hardware. existing work utilizes the latency information to identify variables and instructions suffering from high access latency [cit] ."
"phase oscillator models can be seen as a middle level of description between simple and realistic models. the importance of this intermediate level consists on the generality of the results. in effect, a phase oscillator is a generic description for a wide variety of oscillators on the onset of their limit cycle behavior. on the other hand, its low level allows us to apply, directly, the results of simple models and powerful algorithms."
"for each sampled memory access, cheetah decides whether this access causes a cache invalidation or not, and records this access. for this purpose, cheetah should quickly locate its corresponding cache line. cheetah utilizes the shadow memory mechanism to speed up this locating procedure [cit] . to utilize the shadow memory mechanism, we should determine the range of heap memory, which is difficult to know beforehand when using the default heap. thus, cheetah builds its custom heap based on heap layers [cit] . cheetah preallocates a fixed-size memory block (using mmap), and satisfies all memory allocations from this block. cheetah adapts the per-thread heap organization used by hoard, so that two objects in the same cache line will never be allocated to two different threads [cit] . this design prevents inter-object false sharing, but also makes cheetah unable to report problems that are possibly caused by the default heap allocator."
"in this expression the sum is performed on the n k preceding nodes of the layer k. the nodes of a layer k have couplings f i ði i, ϕ ! þ which are function only of the states of the nodes in the preceding layer, or the corresponding input signals. thus, the couplings act as an external field that controls the dynamics of the layer k. fixing the external input signals i's, the oscillators in the first layer can only evolve to phase differences zero or π. the oscillators in the second layer follow the external field produced by the first layer, and in consequence, they also can evolve only to phase differences zero and π. and obviously, this process is repeated downstream through the network."
"based on predcycles_t, cheetah assesses the predicted runtime of a thread -predrt t -as the eq.(3). we assume that the execution time is proportional to the cycles of accesses, such that fewer cycles indicates less execution time, and corresponds to a performance speedup. it is expected that fixing the false sharing problem inside the object o will improve the performance for its related threads, with less predcycles_t and predrt t ."
"the perceptron is one of the first models of artificial neural networks, inspired in biological neural systems, widely accepted [cit] . its use in multilayer feedforward networks with the backpropagation algorithm for learning is a paradigmatic model in this field [cit] . although this model has been quite well understood for theoretical studies and applications, it has strong simplifications as artificial dynamics (updating schemes) and simple node operation. on the other hand, realistic models considering spiking neural networks also have been developed, allowing the use of a modified backpropagation algorithm for learning [cit] ."
