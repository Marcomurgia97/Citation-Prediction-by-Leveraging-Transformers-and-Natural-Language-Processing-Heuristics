text
"the need for random numbers in cryptographic processes is ubiquitous. initialization vectors block padding, challenges, nonces, and, of course, keys are some of the cryptographic objects where a string of unpredictable bits is required. often the same random number generator (rng) supplies bits for all of the above uses in a cryptographic system. many of the bits generated by the rng are transmitted in the clear and thus a passive attacker has sample opportunity to analyze the output of the rng and can leverage any weaknesses found there [cit] . rngs can be separated into two general categories:"
"where w denotes the parameters of the model, f (w, x i ) denotes the prediction of the air pollutant concentration, and ϕ(·) denotes a regularization function of the model parameters w. next, we introduce two levels of model regularization. the first level is to explicitly control the number of model parameters. the second level is to explicitly impose a certain regularization on the model parameter. for the first level, we consider three models that are described below:"
"therefore, we decided to use meteorological and pollutant data to perform predictions of hourly concentrations on the basis of linear models. in this work, we focused on three different prediction model formulations and used the mtl framework with different regularizations. to the best of our knowledge, this is the first work that has utilized mtl for the air pollutant prediction task. we exploited analytical approaches and optimization techniques to obtain the optimal solutions. the model's evaluation metric was the root-mean-squared error (rmse)."
"there is a wide consensus that intrinsic manufacturing variability of modern and pending deep submicron silicon is an excellent puf implementation platform [cit] . silicon technologies form the basis for almost all computing platforms today, while it is not technologically possible to reproduce the inherent silicon variability. security techniques that employ silicon pufs have numerous important advantages over traditional cryptography-based security techniques including much better resiliency against physical attacks (e.g., radiation, reverse engineering) [cit], the absence of covert channels (e.g., power, delay, electromagnetic measurements), and much lower time, speed, and power overheads. pufs have been used for a variety of security applications ranging from id creation and authentication, to hardware metering and remote enabling and disabling of integrated circuits [cit] ."
"adverse health impacts from exposure to outdoor air pollutants are complicated functions of pollutant compositions and concentrations [cit] . major outdoor air pollutants in cities include ozone (o 3 ), particle matter (pm), sulfur dioxide (so 2 ), carbon monoxide (co), nitrogen oxides (no x ), volatile organic compounds (vocs), pesticides, and metals, among others [cit] . increased mortality and morbidity rates have been found in association with increased air pollutants (such as o 3, pm and so 2 ) concentrations [cit] . according to the report from the american lung association [cit], a 10 parts per billion (ppb) increase in the o 3 mixing ratio might cause over 3700 premature deaths annually in the united states (u.s.). chicago, as for many other megacities in u.s., has struggled with air pollution as a result of industrialization and urbanization. although o 3 precursor (such as vocs, no x, and co) [cit] s, o 3 levels in chicago have not been in compliance with standards set by the environmental protection agency (epa) to protect public health [cit] . particle size is critical in determining the particle deposition location in the human respiratory system [cit] . pm 2.5, referring to particles with a diameter less than or equal to 2.5 µm, has been an increasing concern, as these particles can be deposited into the lung gas-exchange region, the alveoli [cit] . the u.s. epa revised the annual standard of pm 2.5 by lowering the concentration to 12 µg/m 3 to provide improved protection against health effects associated with long-and short-term exposure [cit] . so 2, as an important precursor of new particle formation and particle growth, has also been found to be associated with respiratory diseases in many countries [cit] . therefore, we selected o 3, pm 2.5 and so 2 for testing in this study."
"one of the major techniques used for designing a rng is pufs. a puf is a function that generates a set of responses while stimulated by a set of challenges. it is a physical function because the challenge-response relation is defined by complex properties of a physical material, such as the manufacturing variability of cmos devices. its unclonability is attributed to the fact that these properties cannot be controllably reproduced, making each device effectively unique."
"we utilized the accelerated stochastic subgradient (assg) method [cit] with proximal mapping to optimize this model. the algorithm runs in mutliple stages, and each stage calls the standard stochastic gradient method with a constant step size. to handle the non-smooth 2,1 -norm, we used proximal mapping [cit] . the stochastic gradient descent part is"
the paper addresses the security and stability issues in the design of puf-based random number generators. the main security flaw in the design of ring oscillator pufs is the modelling attacks and this will make them less likely to be used as random number generators. in this paper a novel architecture for ring oscillator pufs is proposed. this architecture has solved both of security and stability problems of the classic ring oscillators. this idea has a higher data complexity and also nonlinearity. the final architecture has also lower hardware complexity which make it suitable for lightweight random number generators.
"the majority of the puf designs are based on delay variation of logic and interconnect. the fundamental principle followed in these delay-based puf is to compare a pair of structurally identical/symmetric circuit elements (composed of logic and interconnect), and measure any delay mismatch that is introduced by the manufacturing process variation, and not by the design. arbiter puf and butterfly puf are inherently difficult to implement on fpga due to the delay skew present between a pair of circuit elements that are required to be symmetric in these pufs. this static skew is an order of magnitude higher than the delay variation due to random process variation. the equation for delay d of a net n in a circuit is shown in equation 1, where d s is the static delay as determined by the static timing analysis tools, and d r is the random delay component due to process variation."
"by embedding pufs into devices, the devices become unclonable. this makes them very useful for anti-counterfeiting applications. the challenge-response behaviour of a puf changes drastically when it is damaged-for instance, by an attacker. together with their unclonability property, this makes pufs very interesting as a means for secure key storage. instead of storing keys in digital form in the memory of a device, a key can be extracted from a puf embedded in the device. only when the key is required (e.g., for an authentication protocol), it is extracted from the puf and deleted immediately when it is no longer needed. in this way, the key is only present in the device for a minimal amount of time and hence less vulnerable to physical attacks. unfortunately, recent analysis has demonstrated that many of the current state-of-the-art puf structures are susceptible to a variety of security attacks."
"rngs used for cryptographic processes must, therefore, be considered a critical part of the cryptographic system. a weakness or failure in the rng can lead to a complete failure of the system."
"mtl focuses on learning multiple tasks that have commonalities [cit] that can improve the efficiency and accuracy of the models. it has achieved tremendous successes in many fields, such as natural language processing [cit], image recognition [cit], bioinformatics [cit], marketing prediction [cit], and so on. a variety of regularizations can be utilized to enhance the commonalities of the related tasks, including the 2,1 -norm [cit], nuclear norm [cit], spectral norm [cit], frobenius norm [cit], and so on. however, most of the former machine learning works on air pollutant prediction did not consider the similarities between the models and only focused on improving the model performance for a single task, that is, improving prediction performance for each hour either separately or identically."
"the first improved ring oscillator architecture discussed here is ic-ek generator [cit] . block scheme of a whole ic-ek generator is shown in figure 6 . an ecc encoder generates the configuration code word which is translated by a code conversion circuit into the configuration circuit control vector. the oscillations counted consecutively by counter c 1 and c 2 will be compared to generate a response bit ij β . although this architecture has a higher number of challenge response compared to the conventional ring-oscillator puf, there are a couple of major disadvantages. the first weakness of this circuit is the hardware overhead of the encoder circuit which is used for the routing of the ring oscillators. this circuit also will remove some of the possible input challenges. but the worst problem happens when only one or two bits of the challenge changes. in this case there is a big gap in the frequencies, with having the old outputs and the equivalent frequency the new output can be easily predicted with probability of 0.5. not that the prediction is easier if the number of inverters of each stage of ring oscillator is low. but if the frequency difference is not large enough then the output is not stable. so this circuit have either security problem or stability problem."
"in this paper, we have developed efficient machine learning methods for air pollutant prediction. we have formulated the problem as regularized mtl and employed advanced optimization algorithms for solving different formulations. we have focused on alleviating model complexity by reducing the number of model parameters and on improving the performance by using a structured regularizer. our results show that the proposed light formulation achieves much better performance than the other two model formulations and that the regularization by enforcing prediction models for two consecutive hours to be close can also boost the performance of predictions. we have also shown that advanced optimization techniques are important for improving the convergence of optimization and that they speed up the training process for big data. for future work, we will further consider the commonalities between nearby meteorology stations and combine them in a mtl framework, which may provide a further boosting for the prediction."
"the delay difference between two nets, n 1 and n 2, in a circuit maybe be expressed as a sum of static delay difference ∆d s and random delay difference ∆d r [cit] as shown in equation 2 ."
"it is noteworthy that the main contribution of this work is the incorporation of model parameter reduction and mtl with regularization into air pollutant prediction. as the previous content has illustrated, for the parameter reduction part, our light formulation reduces model parameters by removing heavy meteorological parameters of the other hours for one hour's submodel. for the mtl part, we considered that there could be some similarities for consecutive hours' models; therefore, we could add appropriate regularizers for this purpose."
"as depicted in figure 1 (b), each switch is configured to be either a cross or a straight connector, based on its selector bit. the arbiter compares the signal arrival times at the end of parallel paths (i.e., at its inputs) to produce the corresponding response. the path segments are designed to have the same nominal delays, but their actual delays differ slightly due to manufacturing variability [cit] ."
the final architecture of the novel ring oscillator is shown in the figure 9. this architecture is composed of 16 configurable classic ring oscillators each of which have 12 inverter gates and 3 multiplexer 4 ൈ 1 . therefore in total there are 96 input bit challenges which can be applied to the circuit. these input challenges are produced by output bits of two lfsrs. the last output will act as a scrambler and will choose which lfsr will be used for the input challenges.
"there is another architecture used for ro-puf which is called configurable ro-puf [cit] . the simple architecture of a configurable ro is shown in figure 7 . in this circuit, instead of the conventional ring oscillators, configurable ring oscillators are used. so with a slight change of the input bits the frequency of ring oscillator will change and with a comparison the output is evaluated. the main disadvantage of this architecture is again the easily prediction of the output. in other words the new output can be easily predicted with probability of 0.5 if the old output is known. as an example suppose the old output is one and the frequency difference of the ring oscillators is large, if one bit of the multiplexers select changes the probability to have another one in the output is higher than zero output. in other words, if the last two bits of the output is ‫‬ ଵ o ଶ, the above conditions happens then:"
the remainder of the article is organized as follows. a brief background on pufs and variation modelling is given in section 2. section 3 presents a survey of related literature. the novel architecture of puf-based rng is presented in section 4 and finally paper concludes in section 5.
"the pseudocode of the algorithm is as follows: the challenge in solving the nuclear norm reguralized problem of most optimization algorithms lies with computing the full singular value decomposition (svd) of the involved matrix w, which is an expensive operation. to avoid full svd, the svd-free convex-concave algorithm extension to a stochastic setting (secone-s) [cit] was employed to solve the problem. the algorithm solves the following minimum-maximum problem:"
"the total number of features is d. although the standard stochastic (sub)gradient method [cit] could be utilized to solve all the formulations considered in this work, it does not necessary yield the fastest convergence. to address this issue, we considered advanced stochastic optimization techniques tailored for solving each formulation."
"we used the names of the paired air quality monitoring sites and two weather stations to denote the two datasets, that is, lu-lv and lma-av. lu-lv contained the data to predict the concentration of the two air pollutants o 3 and so 2 . lma-av contained the data to predict the concentration of the two air pollutants o 3 and pm 2.5 ."
"a puf must be easy to evaluate which means the physical device must be capable of evaluating the function in a short amount of time. it should also be hard to characterize. hence from a limited number of plausible physical measurements or queries of chosen challenge-response pairs (crp), an attacker who no longer has the device, and who can only use a limited amount of resources (time, money, raw material, etc...) can only extract a negligible amount of information about the response to a randomly chosen challenge. pufs should be also prohibitively hard to copy (clone), emulate, simulate, or predict. the main goal of this paper is to investigate the puf-based architectures for rngs and compare their advantage and disadvantages. at the end of the paper a novel puf-based architecture for rngs will be presented. this architecture has a higher number of challenge responses compared to the conventional puf-based architectures."
"with the exception that the frobenius norm regularized model (with 2 -norm cc regularization or not) has a closed-form solution, we solved the other models via advanced stochastic optimization techniques. we denote the following:"
"the difference between the top and bottom path delays on the segment n is denoted by n δ on figure 2 . to ensure larger variations, one could insert additional delay elements on the path segments. the puf challenges (inputs) are the selector bits of the switches. the output bit of the arbiter depends on the challenge bits and is permanent for each ic (for a range of operational conditions). parallel puf's liability to reverse engineering was previously addressed by introducing nonlinearities, such as feed forward (ff) arbiters, in the puf structure [cit] . figure 2 also includes a ff arbiter (dashed line) that controls a switch selector. unfortunately, our preliminary study shows even this structure can be reverse engineered using a combination of combinatorial and linear programming technique [cit] ."
"in this section, we describe different regularizations for the model parameter matrices w in the heavy and light models. we consider the problem using mtl, in which predicting the concentration of air pollutants over one hour is one task. in the literature, a number of regularizations have been proposed by considering the relationship between different tasks. we first describe three baseline regularizations in the literature and then present the proposed regularization that takes the dimension of time into consideration for modeling the relationship between models at different times."
"a similar idea can be applied to other kinds of work. first, if the submodels are not built for each hour but for each day (or even for each location from a spatial perspective), we can still apply the parameter reduction idea that only keeps more important information and removes the information with low priority. second, for the mtl part, we can still add regularizations for the similarities of the submodels. furthermore, in this work, the submodel w i was a linear regression model; it is also practical to replace it with support vector regression (svr), nonlinear regression, neural networks, and so on. finally, the techniques used in this work can be further combined with many other transfer learning techniques, such as feature-representation transfer for deep neural networks."
"we paired the collected meteorological data and air pollutant data on the basis of time to obtain the required data format for applying the machine learning methods. in particular, for each variable, we formed one value for each hour. however, the original data may have contained multiple records or missing values at some hours. to preprocess the data, we calculated the hourly mean value of each numeric variable if there were multiple observed records within an hour and chose the category with the highest frequency per hour for each categorical variable if there were multiple values. missing values existed for some variables, which was not tolerable for applying the machine learning methods used in this study. therefore, we imputed the missing values by using the closest-neighbor values for four continuous variables and one categorical variable: wind gust, pressure, altimeter reading, precipitation, and weather conditions. we deleted the days that still had missing values after imputing. we applied dummy coding for two categorical variables, the cardinal wind direction (16 values, e.g., n, s, e, w, etc.) and weather conditions (31 values, e.g., sunny, rainy, windy, etc.). then, we added the weekday and weekend as two boolean features. finally, we obtained 60 features in total (9 numerical meteorological features, 16 dummy codings for wind direction, 31 dummy codings for weather conditions, 2 boolean features for weekday/weekend, 1 numerical feature for pollutants, and 1 bias term). we applied normalization for all the features and pollutant targets to make their values fall in the range [cit] ."
the above problem has analytical solutions. we denote w i as a column vector for w and w i as a column vector for w t . then the solution to equation (4) can be computed by the following [cit] :
"as we discussed in the section 4, there are several stability and security problems with the current ring oscillator architectures. in order to combat these issues, using the conventional architectures, a novel ring oscillator circuit is proposed in this section. so in each stage of the ring oscillator m inverter gates are employed. the circuit works in a way that the output of the ring oscillators is applied to a up/down counter. the counter first up count according to the output of the ring oscillator (for an input challenge). then the counter down counts and the results is compared to the original state of the counter. if the challenge of the ring oscillator is not changed or in other words the hamming distance of the input challenges are the same then the output of the ring oscillator will produce a zero in the up/down counter. in the implementation of each block, each inverter is implemented in only on lut so we have a difference of the delays in the inverters and ring oscillators. for an n stage ring oscillator which has 2 m inverter gates, the delay of the circuit after one challenge input can be computed according to equation (4):"
"the rest of the paper is organized as follows. in section 2, we discuss related work. in section 3, we describe the data collection and preprocessing. in section 4, we describe the proposed solutions, including formulations, regularizations and optimizations. in section 5, we present the experimental studies and the results. in section 6, we give conclusions and indicate future work."
"we compared 11 different models that were learned with different combinations of model formulations and regularizations. the 11 models were the following: it is noteworthy that we also added the standard frobenius norm regularizer for the heavy/light-nuclear, -ccl2, and -ccl1 models, because their regularizers were mainly considered for controlling the similarities of submodels and may not have been enough for preventing overfitting. we divided each dataset into two parts: training data and testing data. each model was trained on the training data with proper regularization parameters and the learning rate selected on the basis of 5-fold cross-validation. each trained model was evaluated on the testing data. the splitting of the data was done by dividing all days into a number of chunks of 11 consecutive days, for which the first 8 days were used for training and the next 3 days were used for testing. we have used the rmse as the evaluation metric."
"in this study, we focus on refined modeling for predicting hourly air pollutant concentrations on the basis of historical metrological data and air pollution data. a striking difference between this work and the previous works is that we emphasize how to regularize the model in order to improve its generalization performance and how to learn a complex regularized model from big data with advanced optimization algorithms. we collected 10 years worth of meteorological and air pollution data from the chicago area. the air pollutant data was from the epa [cit], and the meteorological data was from mesowest [cit] . from their databases, we fetched consecutive hourly measurements of various meteorological variables and pollutants reported by two air quality monitoring stations and two air pollutant monitoring sites in the chicago area. each record of hourly measurements included meteorological variables such as solar radiation, wind direction and speed, temperature, and atmospheric pressure; as well as air pollutants, including pm 2.5, o 3, and so 2 . we used two methods for model regularization: (i) explicitly controlling the number of parameters in the model; (ii) explicitly enforcing a certain structure in the model parameters. for controlling the number of parameters in the model, we compared three different model formulations, which can be considered in a unified multi-task learning (mtl) framework with a diagonal-or full-matrix model. for enforcing the model matrix into a certain structure, we have considered the relationship between prediction models of different hours and compared three different regularizations with standard frobenius norm regularization. the experimental results show that the model with the intermediate size and the proposed regularization, which enforces the prediction models of two consecutive hours to be close, achieved the best results and was far better than standard regression models. we have also developed efficient optimization algorithms for solving different formulations and demonstrated their effectiveness through experiments."
"in order to increase the stability of the circuit, the comparison is done by a threshold call th. so the effect of the oscillators which have a marginal frequency difference will be omitted and the stability of the circuit will be increased. the outputs of the ring oscillators are xored to produce the final output. in this architecture the current frequency of a ring oscillator is compared to the last state frequency. this will decrease the speed of the architecture but on the other hand the required hardware for implementation is also decreased. because of the usage of a threshold in this novel architecture the probability of prediction of the output by knowing the last output is still greater than 0.5. this weakness can easily lead to a modelling attack. if this threshold is picked with a small number the stability of the circuit will face problems and if the threshold is pick a large number then security is in danger. in order to solve this problem and increase the complexity and nonlinearity of the architecture all the ring oscillators are xored first and the output like a scrambler will decide the next challenge. for this architecture two lfsrs with the length of 96 flip-flops are used the primitive update function of the lfsrs are chosen as equation (6,7)."
"silicon pufs exploit manufacturing variability to generate a unique input/ output mapping for each ic. delay-based silicon pufs uses the delay variations of cmos logic components to produce unique responses. the responses are generated by comparing the analog timing difference between two delay paths that must be equivalent by logic-level construction, but are different because of manufacturing variability. the delay-based structures use a digital component, arbiter that translates the analog timing difference into a digital value. an arbiter is a sequential component with two inputs and one output. the arbiter output is one if a rising edge signal arrives at its first input earlier by at least a threshold value compared to the signal arriving at the second input. the arbiter's output is zero otherwise. figure 1 (a) shows an arbiter implemented using an edge-triggered latch. if the time difference between the arriving signals are smaller than the setup and hold times of the latch, the arbiter may become metastable and not be able to produce an accurate and deterministic output."
"the high-level idea of mtl lies in transfer learning, which generally aims to transfer knowledge from a related source task to a target task and consequently improve the performance for the target task. there are multiple variants for transfer learning, such as inductive transfer learning, transductive transfer learning and unsupervised transfer learning, and the approaches for transfer learning mainly include instance transfer, feature-representation transfer, parameter transfer and relational-knowledge transfer [cit] . one of the most common examples is feature-representation transfer for deep neural networks. after either supervised or unsupervised learning from other related datasets, the pretrained model can be appropriately reused for learning the target task with a better performance. the mtl technique in this work is an example of parameter transfer in an inductive-transfer-learning setting."
"in order to cause a change in classification, the value of one output neuron magnitude must overcome the output neuron magnitude of the preirradiation classification. fig. 10 shows the output neuron magnitude changes due to radiation that resulted in the 17 classification errors while fig. 11 shows the 17 classification corrections. in both figures, the x-axis is the output neuron magnitude. the changes in output neuron magnitudes are represented by arrows. the start of the arrow corresponds to the value of the preirradiation output neuron magnitude, and the termination of the arrow corresponds to the postirradiation value of the output neuron magnitude. the pairing of a red and black arrow represents the output neuron magnitudes for a single image. the red, dashed arrow represents the output neuron magnitudes for the incorrect classification. conversely, the black, solid arrow corresponds to the output neuron magnitudes of the correct classification. black digits are the values of the correct classification while red digits are the values of the incorrect classification."
"since the value of the output neuron magnitude indicates the correlation between the output neuron magnitude's digit and the input image, output neuron magnitudes on the right side of the graph indicate high confidence in the resulting classification, while those on the left suggest low confidence. if high confidence is considered to be output neuron magnitudes with values greater than 0.95, then 93% of preirradiation classifications on the 10 000 images set were found to be high confidence. in addition, of these high confidence classifications, 73% of them had an output neuron magnitude value of exactly 1. figs. 10 and 11 show that the classification errors due to irradiation resulted from output neuron magnitudes which are not classified as high confidence. none of the classification changes resulted from high confidence output neuron magnitudes, and only seven of the classification changes resulted from preirradiation output neuron magnitudes above 0.8. fig. 10 . output neuron magnitudes for 17 classification errors resulting from radiation. black arrows and digits correspond to the correct classification while red arrows and digits correspond to the incorrect classification. the beginning of the arrow is the output neuron magnitude preirradiation and the termination of the arrow is the postirradiation output neuron magnitude."
"mapreduce is a simple processing model based around three functions that execute at two distinct stages: the mapper function, map(), transforms raw input data into a set of intermediate key-value pairs; then the combiner function, combine(), sorts and packs the key-value pairs in the map stage; finally the reducer function, reduce(), takes the related data (usually sorted by the key in the key-value pairs emitted by the mapper) and merges the data together. the mapreduce task division is based around how the input data is distributed and processed by the mapper and reducer at each stage. initially the input data is split and distributed to mappers that run on different compute nodes. after emitting some key-value pairs, the mapreduce system sorts the pairs into related groups by key. these groups are then each provided as input to reducers that also run on different compute nodes. finally, the output of the reducers in collected and combined into a single output dataset."
"in order to verify the universality of our new method we tested all types of correlation functions. currently, there are three main types of correlation coefficient calculation algorithms, product-moment coefficient, rank correlation coefficients and other dependence measurements. we took pearson correlation for product-moment type, spearman and kendall for rank type and euclidean distance for other dependence measures, which are implemented in r packages, r-base and fields. we compared a vanilla r instance and parallel r (snowfall) against mapreduce via rhipe."
"taking all the false positive and false negative data from figs. 14 and 15 and combining them into a histogram results in fig. 16. fig. 16 has the false positives at the top half of the y-axis and the false negatives going down the lower half of the axis. the preirradiation data sets are the yellow bars, while the blue bars are the postirradiation data. the positive x-direction corresponds to increasing fluence within each digit's data set."
"finally we have designed our workflow to take into account uneven numbers of available reducers to mappers. the hash function in mapreduce cannot always map mappers' output to the expected number of reducers even if the mapper output keys are well designed. for example, if six mapper output keys are sent to six respective reducers and only three reducers are available to receive the data, this results in one or more reducer receiving a greater workload to process sequentially. to ensure all reducers are fully utilized, calculations in reducers are shifted to combiners, which read pairwise data blocks directly from the file system, that calculate the result at the mapper stage before the hash mapping, as shown in figure 5 ."
"we carried out a performance evaluation between vanilla r, parallel r with snowfall, and mapreduce implementation. we calculated a subject correlation on the all subjects, calculating the coefficient matrices of the two benchmarks using euclidean distance, pearson and spearman correlation functions."
"in this paper our optimisation method applies on four kinds of correlation methods used to generate correlation matrices that are used by the hierarchical clustering algorithm in transmartthe pearson product-moment correlation [cit], spearman's rank-order correlation [cit], kendall's rank correlation [cit], and euclidean distance correlation. we describe a series of processing optimisations, based around the mapreduce programming model [cit], on the calculation of correlation matrices used for hierarchical clustering. we go on to present how our correlation matrix calculations implemented on r mapreduce package rhipe [cit], which works in combination with hadoop [cit], a robust and well-supported distributed data storage and computation framework that supports mapreduce, significantly outperforms their comparable implementations configured for distributed execution in r using snowfall [cit] package with rmpi [cit], a parallel computing package for r scripts."
the molding compound above the truenorth chip ( fig. 3 ) had to be removed to reduce the range necessary for the ions to reach the sensitive area. a picture of the exposed chip is shown in fig. 1 .
"this figure allows comparison between digits and also analysis of trends within a specific digit. the digit with the highest occurrence of false positives is \"9,\" meaning that there are more incorrect outputs that are \"9\"s than any other digit. the highest number of false negatives belongs to the digit \"4,\" meaning that the \"4\"s input into the system are the most often incorrectly classified digit. the number of false positives for \"0\" and \"5\" decreases with fluence, while it increases for \"2,\" \"4,\" and \"6.\" on the other hand, the number of false negatives increases with fluence for the digits \"2\" and \"8.\""
"the radiation tests were conducted at vanderbilt university using the pelletron. the proton energy selected was 4 mev, and at that energy, the possible proton fluxes range from 10 5 to 10 10 protons/cm 2 -s [cit] . the truenorth board ( fig. 1 ) allowed for trained corelets to be uploaded via scripts from a computer. during irradiation, the board was mounted in the pelletron's vacuum chamber, and an ethernet feed-through from the chamber was connected to the board."
"the chip was designed to be power efficient. the power efficiency is due mainly to local, distributed memory that limits the distance on the chip that spikes must travel. truenorth also uses an event-driven architecture, the snn, therefore, components only have to be \"on\" and consuming power when prompted by an event. the lack of a global clock network also contributes to the power efficiency of the chip [cit] ."
"truenorth is scalable and fault tolerant. up to 16 truenorth chips can be physically connected, and the software is already designed to allow communication among the various chips. in addition, the high connectivity between neurons, cores, and chips creates redundancy. a result of the interconnectedness is increased fault tolerance via avoidance of defective cores and the implementation of memory redundancies [cit] ."
"in the example shown in figure 1, each data block can contain several subjects, where in this example we show two subjects per data block. each block is loaded by a mapper, and then copied to each reducer. based on the reducer id, a corresponding data block is loaded and coefficients calculated against all other data blocks. for example, reducer1 loads data block 1 and performs a pairwise comparison with outputs from each mapper (data block 1, data block 2, and data block 3), producing coefficient values d 11, d 12 and d 13 ."
"the operation of the truenorth board was accomplished through the use of trained corelets. the particular corelet used for these experiments came pretrained and the training was conducted based on an ibm model file. the corelet classified handwritten digits 0 through 9 where the digits came from the modified national institute of standards and technology (mnist) database. the mnist database is a collection of images of handwritten digits between 0 and 9. of the 70 000 digits in the database, there are 60 000 digit images for the training data set and 10 000 for classification. of the 10 000 digits used for classification, there are approximately 1000 samples of each digit [cit] ."
"in prior work, an static random access memory (sram)based artificial nn tasked with image recognition was hit with a pulsed laser to generate seus [cit] . the beam was focused on cells in the sram that contained weights and thresholds for the artificial nn. in most cases, the image recognition rate did not change, indicating that the errors did not affect the output. in the cases that the recognition rate did change, it was improved in some cases and diminished in others. the experiments with the pulsed laser affected artificial nn performance in a manner that error injection through software simulation could not [cit] ."
"r is a widely used analysis tool for clustering and correlation calculation. many emerging parallel r packages, such as rhipe, sparkr [cit], rabid [cit], snowfall, rmpi and pbdmpi [cit], can be used to parallelize r processes. rhipe is a hadoop mapreduce based r package that transforms r functions into mapreduce jobs. sparkr and rabid are mapreduce packages, which works in combination with apache spark [cit] . though hadoop performs slower than spark in several cases, such as iterative computations, hadoop is much more mature to provide a more stable performance. snowfall with rmpi is a combination of parallel packages that works in a master-slave interactive mode, where all code and data are distributed to each process within a cluster, then the code works respectively in each process, and finally the master collects the final result. while pbdmpi uses single program multiple data parallel programming model, which is not as popular as snowfall."
"to do this, we have used a matrix transformation algorithm (see pseudo code below) to balance all reducers by moving the bottom left triangular section of the correlation matrix to the top right, as shown in figure 3 ."
"the classification of a digit is accomplished through the use of output neuron magnitudes. for every input image, the system assigns 10 output neuron magnitude values. these 10 output neuron magnitudes ranging from 0 to 1 inclusive correspond to the digits 0 through 9. the output neuron magnitude can be thought of as the likelihood of a particular digit being the correct classification of the image. (note, however, that the sum of all output neurons is not required to add up to one like independent probabilities.) the output classification is chosen by selecting the digit with the highest output neuron magnitude."
"the effect of radiation on the number of classification errors for specific digits is analyzed next. figs. 14 and 15 show how the number of false positives changes with fluence for fig. 15 . as fluence increases, the number of false positives for the digit \"6\" increases. \"0\" and \"6,\" respectively. as the fluence increases for the digit \"0,\" the number of false positives decreases. the trend is negative which indicates improvements in the network for \"0\"s. on the other hand, the number of false positives for \"6\" increases with fluence. this is similarly supported by the calculation of the mean and standard deviation ( fig. 15 inset), where the mean is positive and greater than the standard deviation."
"information from genomic, proteomic and metabolic measurements has already benefited identification of disease subgroups and the prediction of treatment responses of individual subjects, which is known as molecular profiling based patient stratification [cit] . biomedical research is moving towards using high-throughput molecular profiling data to improve clinical decision making. one approach for building classifiers is to stratify subjects based on their molecular profiles. unsupervised clustering algorithms can be used for stratification purposes. this paper introduces significant optimisations to unsupervised clustering using four kinds of correlation methods with high-dimensional molecular profiling data (gene expression data), by taking full advantage of a programming model specifically designed for parallel processing of big datasets."
"the classification changes resulting from radiation occur due to single events, not total ionizing dose. for one, reloading the model file returns truenorth to its original, preirradiation state and classification. additionally, the dose accumulated over all the exposures to the truenorth chip was found to be less than 1 krad (sio 2 ). this provides further evidence that sees, not total ionizing dose, are causing the changes in classification."
"as previously mentioned, the output neuron magnitudes of the trained corelet determine the resulting classification of an image, and these values are stored in the sram. there are 10 000 mnist images being classified per run and each image has 10 output neuron magnitudes, so there are a total of 100 000 output neuron magnitudes per run. because the output neuron magnitudes are not the final output, changes to these output neuron magnitudes are considered to be tolerable errors."
"in the micro-benchmark, as shown in figures 6, vanilla r performs fastest and default snowfall performs the slowest. vanilla r has a larger data preparation overhead than rhipe, but the calculation itself greatly outperforms all the other methods. all parallel r methods do not perform any better due to the data communication overhead. there is an extreme example in default snowfall spearman where the parallel calculation is 9 times slower than vanilla r. the optimised rhipe demonstrates a 3.26-5.83 fold increase compared to the default snowfall. the optimised rhipe conducts 1.56-1.64 times faster than the basic rhipe, which almost achieves the expected two times acceleration, considering all the overheads, such as data preparation and hadoop job initialization."
"as previously mentioned above, the active region of the truenorth chip is approximately 10 µm of back end of line materials. as a result, the incident ion must have sufficient range to reach the sensitive volume to produce a single event effect (see) within the chip. stopping and range of ions in matter (srim) was used to calculate the minimum energy required to pass through the back end of line materials for various ions [cit] . protons were found to have sufficient range for see testing, while heavier ions were found to not have enough range at the maximum pelletron energy."
"in addition to analyzing the overall errors and classification changes, the effect of soft errors in the sram on specific digits is analyzed. there are two ways of looking at every classification error that occurs. fig. 12 shows a sample string of inputs to the corelet and the corresponding output classification. as seen in the figure, there is a single error, the \"8\" being incorrectly classified as a \"5.\" the \"8\" is an example of a false negative, where a false negative is not classifying the input correctly. on the other hand, the \"5\" is a false positive, which is an indication of an incorrect result. in other words, false negative corresponds to the input that is incorrectly classified, while false positive corresponds to the erroneous output. while a false negative is the incorrectly identified digit (the \"8\" in fig. 12 ), a false positive is the classification of a digit that is not actually there (the \"5\" in fig. 12 )."
"our approach to applying mapreduce to calculating correlation matrices on gene expression data is inspired by work presented in li q, et. al. [cit] who used a mapreduce-like model for distributing large datasets on gpus to calculate euclidean distance. in the case of our mapreduce implementation, our input corresponds to a set of vectors, each containing the probesets intensity values for a single subject. this input set of vectors is divided into data blocks that are dispatched to different mappers, dependent on the total number of cpu cores available. in the mapper stage, each data block is copied to each reducer by emitting as a key the index of the corresponding reducer and as the value the data block. each reducer then calculates correlation values by loading the data block corresponding to the key and calculating the correlation coefficients against each data block output from the mappers."
"this optimisation the coefficient calculations shown in figure 2 on results, however, now produces an imbalanced workload on the reducers. if we look at figure 2 more closely, we can see that reducer 1 receives a workload corresponding to one pair-wise calculation (d 11 ), while reducer 2 pairs calculations (d 21, d 22 ), and so forth. with this pattern of workload, the longest running reducer determines the overall performance of the mapreduce workflow. if we can balance the load on the reducers, the workload will execute fully in parallel, thus reducing the overall execution time."
"in the tests using the macro-benchmark, as shown in figure 7, the optimised rhipe outperforms all other methods. though the optimised rhipe calculation time is still a little longer than optimised snowfall, the optimised rhipe outperforms the optimised snowfall due to faster data transfer via hdfs and thus shorter data preparation times. in figure 7a (2158 subjects), benefiting from the 3.30 times faster data preparation, the optimised rhipe performs 1.22 -1.30 times faster than the optimised snowfall. in figure 7b (4254 subjects), benefiting from the 3.69 times faster data preparation, the optimised rhipe performs 1.31 -1.49 times faster than the optimised snowfall. in figure 7c (8508 subjects), benefiting from the 5.13 times faster data preparation, the optimised rhipe performs 1.50 -1.71 times faster than the optimised snowfall and 7.24-16.56 times faster than the vanilla r. we propose that rhipe holds great promise for large data analysis with the data size increasing."
"the physical arrangement of one truenorth chip is a 64 by 64 array of cores (4096 total cores) covering an area of 4.3 cm 2 including a total of 428 mb of sram [cit] . a schematic of one core is shown in fig. 2 . the chip is fabricated in a 28-nm cmos technology, and the memory (which composes approximately 30% of the area of the chip) is sram. the total amount of sram per core is approximately 107 kb [cit] . the data stored in the sram include storing neuron data, weights, thresholds, and neuron magnitudes (see [cit] for additional information)."
"the packaging of the truenorth chip includes 1.2-mm molding compound above the chip, as shown in fig. 3 . below the molding compound, approximately 10 µm of metallization, dielectric, and via layers exist above the active region. fig. 4 shows a sketch of the back end of line (not to scale). these metallization layers are composed of various metals including aluminum, tungsten, and copper."
"as demonstrated before, there is approximately the same number of classification corrections (like \"8\" in fig. 7) as incorrect classifications due to soft errors (like \"7\" in fig. 7 ). in addition, there are some instances where an incorrect classification is not corrected, but the resulting output changes. an example of this is shown in fig. 9 . low confidence classifications were most susceptible to classification changes after irradiation."
"the accuracy of preirradiation runs on the 10 000 digit mnist test set using a pretrained, fixed network (corelet) is 98.82%, which corresponds to 118 errors of the 10 000 digits classified. this is the baseline classification accuracy. since truenorth operation is repeatable, all runs without radiation yield this exact accuracy. (note these errors are not due to radiation, but rather the inherent nature of a finite, imperfect training set and occasionally ambiguous, sloppily handwritten digits.) a sample of some digits classified by the trained corelet is shown in fig. 5 . the green boxes in the figure represent correct classifications. the red box is an example of an incorrect classification, which can also be called a critical error since this is a change in the output. the red box shows an example of a \"9\"; however, the trained corelet incorrectly classifies this digit as a \"4\" preirradiation."
"our optimisations were tested against large datasets to validate being able to handle large studies that we would expect to see in transmart. we used three publicly available datasets: oncology (gse2109) [cit] taken from ncbi geo consisting on 2158 subjects and 54,675 probesets (1.9 gb comma-delimited value file), leukemia (gse13159) [cit] subjects and 54,675 probesets (1.8 gb comma-delimited value file) multmyel consisting on 559 subjects and 54,675 probesets (493 mb comma-delimited value file), and a breast invasive carcinoma dataset taken from tcga [cit] consisting of 547 subjects and 17,814 probesets (157 mb comma-delimited value file). we used ic cloud [cit], a computational testbed based at imperial college london to set up comparable virtual machine configurations for the r implementation and our mapreduce implementation."
"artificial nns are claimed to be intrinsically fault tolerant; however, most artificial nns cannot be considered fault tolerant without a proper design. passive fault tolerance 0018-9499 © 2019 ieee. personal use is permitted, but republication/redistribution requires ieee permission."
"k is the number of all the data blocks. i is the id of a data block. by using this matrix transformation to balance the reducers, each reducer will process either c pairs or f pairs of data blocks, where in theory all reducers load are fully balanced and each reducer only calculate about half of the pairs in an original reducer. in the example shown in figure 2, six pairs of data blocks are calculated with an imbalanced workload. after balancing using the above matrix transformation, we can see in figure 4 each reducer now only calculates two pairs of data block."
"though the optimised rhipe is outperformed by optimised snowfall, it has a lower data preparation overhead. this is advantageous as rhipe is likely to be able to perform better overall with much larger datasets. thus, we utilized the macro-benchmark to further test the optimised snowfall and the optimised rhipe with vanilla r as a baseline."
"our motivation for optimisation of unsupervised clustering is based on our experiences in using transmart, a biomedical data integration platform that includes support for clinical and molecular data [cit] . transmart was originally developed by johnson & johnson for in-house clinical trial and knowledge management needs in translational studies. it has been open-sourced recently. our aim is to optimise transmart so that clinicians can make use of it for faster and more confident clinical decision making. however, we have found that the performance of the r workflow currently used in transmart for preparing correlation matrices when analysing high-dimensional molecular data is sub-standard."
"the particular hardware studied in this work was ibm's truenorth neurosynaptic system fabricated in samsung's 28-nm low power (lp) process ( fig. 1 ). it is programmable and executes trained \"corelets,\" which are truenorth programs uploaded to the board via a host computer. the chip implements a spontaneously spiking neural network (snn), meaning that the output spikes are not governed by a global clock. the neurons in the snn integrate the input spikes and add to the membrane potential. this potential takes into account the weights of the neurons, and the potential decreases as a function of time, causing newer events to have more effect than older ones. when the membrane potential surpasses the predetermined threshold, the output neuron fires a spike into the network. in addition, the snn in the truenorth chip also includes leakage from the potential to emulate \"forgetting\" [cit] . therefore, older events have less of an effect on the neural potential than more recent events [cit] ."
"in addition, the figure shows an example of an error and how the false positive and negative relates to the table. looking at the yellow box, that there are 9 instances that an \"8\" is incorrectly classified. for reference, there are approximately 1000 examples of the digit \"8.\" on the other hand, there are 10 cases where the number input to the corelet is classified as a \"0\" when some other digit is actually input into the system. since false negatives and false positives each represent a different way of looking at one error, the sum of the false-negative column is equal to the sum of the false positive column. these two sums are both 118, which is to be expected since that is the total number of errors in the system preirradiation. it can also be seen from the table in fig. 13 that there is a bias toward certain numbers. of the ten digits, \"0\" has the lowest false-negative classification rate. of the approximately 1000 examples of \"0\" input into the system, only 2 were classified as something else. on the other hand, \"4\" has the highest instance of classification errors as shown by 29 false negatives. as far as false positives (the incorrect output classifications) go, \"4\" has only 5 instances, meaning that there are only five times that some input is incorrectly classified as a \"4.\" in this case, there is a bias against classifying \"4\"s, since \"4\" has a high number of missed classifications (false negatives) and a low number of incorrect classifications (false positives). there is a bias toward classifying numbers as \"0,\" indicated by the low number of false negatives and moderate number of false positives. these observations may depend on the training data set."
"as part of our baselines for comparison, we performed a full kendall correlation calculation in our vanilla r configuration, where we found that the total execution time was indeterminately long. we used the tcga and multmyel datasets to estimate the full time because figure 7 performance on the macro-benchmark. this is the performance evaluation using vanilla r, optimised snowfall and optimised rhipe package. the upper part of each figure indicates the total execution time. in this part, the bottom three bars in each method shows the data preparation time; while the upper three bars respectively indicate the euclidean (e), pearson (p) and spearman (s) calculation time. the lower part of each figure details the data preparation of each method. in this part, data split shows the time used for splitting the large data matrix into smaller pieces, data transfer for the snowfall shows data copy time for the pieces to corresponding mpi workers, data transfer for the rhipe shows the data uploading time for the same pieces to hdfs, system boot respectively shows the boot time of the mpi cluster and the hadoop cluster, and the direct load shows the data loading time for vanilla r. a: performance on oncology dataset. (2158 subjects). b: performance on the cross-study consisting of oncology and leukemia (4254 subjects). c: performance on the large artificial dataset (8508 subjects)."
"the results of this experiment can be applied more generally to neuromorphic architectures. in the data presented here, the overall classification accuracy remained constant after introducing errors in the sram by proton irradiation; however, changes occurred within the specific digits. in neuromorphic computing architectures, it may be necessary to examine the radiation-induced changes of the weak classifications and their effect on the classification accuracy of specific classes. in this work, the accuracy did not measurably change. however, for some classes, the number of errors more than tripled after radiation. these radiation effects were only revealed through more in-depth analysis of the occurrence of individual errors."
"two benchmarks are used for the performance evaluation. the micro-benchmark used datasets multmyel (559 subjects). the vanilla r configuration used 32 cpu cores and 32gb of memory in a single vm. snowfall used 4 vms each with 8 cpu cores, 8gb of memory and 6 workers. rhipe used 4 vms each with 8 cpu cores, 8gb of memory and 6 mappers. the macro-benchmark used datasets oncology, leukemia and tcga. three main tests are performed using three datasets, including on-cology (2158 subjects), a cross-study consisting of on-cology and leukemia (4254 subjects), and an artificial dataset consisting of dual oncology and dual leukemia (8508 subjects). due to the extremely long execution time of kendall correlation, only the smallest tcga data was used to calculate kendall correlation. the vanilla r configuration used 64 cpu cores and 64gb of memory in a single vm. the master node of snowfall used 8 cpu cores, 8gb of memory and 6 workers, with 14 slave vms each with 4 cpu cores, 4 gb of memory and 3 workers. the master node of rhipe used 8 cpu cores and 8gb of memory using 6 mappers, with 14 slave vms each with 4 cpu cores and 4 gb of memory using 42 mappers. each experiment consists of two stages: data preparation and calculation. there are five methods for comparison: vanilla r, default snowfall using socket connections, optimised snowfall using the rmpi [cit] package, and rhipe using both the basic mapreduce algorithm and the optimised one. the vanilla r data preparation is loading commaseparated value (csv) data matrix from an ext4 (fourth extended filesystem) local file system to an r matrix object. with default snowfall, the data preparation overhead consists of loading the csv data matrix from an ext4 local file system, initializing the worker nodes, and exporting the data matrix and code to the workers. the data matrix is split by row (subject) rather than data block, where the corresponding computation calculates the correlation between rows. with snowfall using rmpi, the data preparation overhead includes splitting the data matrix using the linux split command and copying each of the data block files to every vm. the number of data blocks depends on the number of workers. during the calculations, rmpi workers perform the same tasks as in the mappers in mapreduce. each worker loads each corresponding data block sequentially from the local ext4 filesystem. after each data block is loaded, the worker performs the corresponding calculations. using rhipe, the data preparation overhead consists of splitting the data matrix and uploading each file to hdfs [cit] . the calculation then follows the algorithms described in the method section."
"vanderbilt's pelletron is an electrostatic particle accelerator primarily used to conduct radiation testing on electronic devices. the primary particles that can be accelerated are protons, alpha particles, oxygen ions, and chlorine ions. there are electrical feed-throughs to allow for testing under bias and for real-time data acquisition [cit] ."
"in data mining, hierarchical clustering is a method of cluster analysis that seeks to build a hierarchy of clusters using correlation matrices. currently there are three main types of correlation coefficient calculation algorithms: productmoment correlation, rank correlation and other dependence measurements. pearson's product-moment correlation is a measure of the linear correlation between two variables x and y, giving a value between +1 and −1 inclusive, where 1 is total positive correlation, 0 is no correlation, and −1 is total negative correlation. spearman and kendall correlation methods are two examples of using a rank correlation coefficient. spearman's rank correlation coefficient is a non-parametric measure of statistical dependence between two variables by using a monotonic function. if there are no repeated data values, a perfect spearman correlation of +1 or −1 occurs when each of the variables is a perfect monotone function of the other. kendall tau rank correlation coefficient is a statistic used to measure the association between two measured quantities using a tau test, a non-parametric hypothesis test for statistical dependence based on the tau coefficient. euclidean distance is a popular dependence measurement that differs from correlations by calculating the distance between two points according to pythagorean theorem within a metric space."
"this work uses image classification accuracy to gauge the effects of radiation in a neuromorphic computing architecture. with proton irradiation of a neuromorphic chip, we observed nearly unaffected overall classification accuracy, but offsetting trends in false positives and negatives in the recognition of individual image classes. this metric of image classification accuracy provides further insight into the radiation response."
"the errors resulting from irradiation are not permanent; they are soft errors. reloading the model file returns truenorth to the original, preirradiation state. in addition, classification of a set of postirradiation data resulted in the identical classification each time. truenorth's classification of the data was repeatable. as long as new errors were not introduced via irradiation, the resulting classification of all 10 000 digits remained the same each time. this is consistent with fault injection simulations on this architecture [cit] and the expectation that the observed effects are the result of single-event upsets (seus) in the sram."
"for example, we performed unsupervised hierarchical clustering on the publicly available multiple myeloma (multmyel) [cit] dataset taken from ncbi's gene expression omnibus (gse24080) [cit] . the dataset contains 559 subjects' gene expression data produced by an affymetrix genechip human genome u133 plus 2.0 array. in order to build the classifiers, the subjects are clustered using a hierarchical clustering algorithm hclust() implemented in vanilla r [cit] . in our preliminary tests, we found that running this algorithm on a virtual machine configured with 32 cores and 32 gb of memory took over 6 minutes calculating a euclidean distance matrix using the function rdist() in package fields, and more than a week if performing a kendall rank correlation using the function cor(). based on these observations we concluded that the bottleneck in the hierarchical clustering algorithm was in generating the correlation matrix. clearly optimising the performance of these analyses would expedite clinical decision making. with high-throughput sequencing technologies [cit] promising to produce even larger datasets per subject, we expect the performance of the state-of-the-art statistical algorithms to be further impacted unless efforts towards optimisation are carried out."
"here are false positives and false negatives in a more general context. a false positive is falsely alerting to something, or \"crying wolf.\" on the other hand, a false negative is not alerting to something that it should. this means that something slips by and is \"invisible\" to the classification system. both of these situations could be undesirable. for example, say that classifying something as x causes alarm. on one hand, every false positive for x produces unnecessary alarm since x did not really occur. on the other hand, every false negative for x means that there is no alarm when there rightfully should fig. 11 . output neuron magnitudes for 17 classification corrections resulting from radiation. black arrows and digits correspond to the correct classification while red arrows and digits correspond to the incorrect classification. the beginning of the arrow is the output neuron magnitude preirradiation and the termination of the arrow is the postirradiation output neuron magnitude. be, because x did occur even though the classification system did not classify it as such."
"in a polyhedral model, optimized code is generated from the rewritten iteration spaces by scanning the polyhedra representing the iteration spaces of an optimized loop nest from the first dimension to the last. the quality of the generated code directly impacts performance. therefore, at the heart of chill is a code generator called codegen+ that has advanced the state of the art in polyhedral scanning in the presence of conditionals [cit], as arise in the gsrb code. codegen+ seamlessly combines iteration spaces and guards through a specialized polyhedral ast, as detailed elsewhere [cit] ."
"as the goal of this work is to develop domain-specific optimization techniques for gmg, the compiler algorithm can be specialized for mg implementations that involve composing a set of operators together. the code for the smooth operators that included either gsrb or jacobi was generated by instantiating a template transformation recipe that is then applied to the input code to generate the optimized code. the recipe that is generated for the smooth that includes gsrb is the following: the commands in this recipe refer to applying a transformation to a statement at a particular loop level. once fused, the same transformations are applied to the set of statements s0, s1 and s2 when applied to s0. the only differences for the recipe for the smooth including jacobi is that there are two statements to which the transformations are applied corresponding to odd/even iterations, and the dependence distance for skewing is different."
"is a half-length that is designed to generate a specific in-control arl. under this monitoring framework, the arlout performance of the t 2 control chart, together with the ewma chart, is evaluated for phase ii analysis in the next section. in terms of the aforementioned methods, the proposed monitoring framework can be formalized as pseudo-code 1 in the appendix."
"median filter the median filter constitutes a nonlinear method that performs a smoothing/denoising process for objects in images [cit] . during this process, the neighborhoods of pixels are considered to perform smoothing [cit] . herein, the new intensity of a pixel is defined as the median of its neighborhoods."
"to facilitate consistent entry and analysis of the data, data models need to be fully normalized, simple and universally applicable. the data model outlined in fig. 1 based on only five tables has so far met the demands in all our current projects. despite being simple, it still respects all the pertinent object-entity relationships in our research data. limiting the number of tables containing observations greatly streamlines the data analysis as fewer tables need to be joined during data analysis."
"for data analysis, a modular architecture is chosen. first, data are re-aggregated by means of sql views implemented on the database server. statistical analysis software is then connected to the database server via open database connectivity (odbc) for further analysis and for quality control against the original observations. in line with requirement #6, we use the open source statistics program r [cit] . the library \"rodbc\" is used for data import [cit] . compared to library \"rmysql\" [cit], \"rodbc\" has the advantage that all character data are automatically converted to factors by default, greatly facilitating statistical analyses in subsets of the data with a minimum of coding (see supplementary materials 1 for an example illustrating the complete workflow from scaffolding a cakephp application, data transformation with mysql, and data analysis with r)."
"only five of 154 [cit] claimed the use of dedicated solutions for data entry (see table 3 ): four mammography screening studies and one registry study. none used a cdms for data management. the most popular statistics program was spss, followed by sas and r, while 21% of the articles in radiology and 39% in jnmmi did not specify which statistics program was used."
"operator fusion: the input code in figure 3 consists of three statements s0, s1, and s2, that correspond to the three smooth operators laplacian, helmholtz and gsrb, respectively. once parsed by chill, the iteration spaces corresponding to these operators are as follows:"
"image dilation dilation signifies a morphological process that concretizes the edges and other elements of objects in an image. it enlarges (thickens) objects according to the filter (structuring element) [cit] . these elements can be chosen as disk, line (with a different angle), diamond, etc. on grayscale images, dilation is defined as in eq. (1) [cit] . in eq. (1), a and b are respectively the original image and structuring element, d b symbolizes the domain of structuring element b, and a(x,y) is presumed to be − ∞ outside the image's domain."
"spatial fcm (sfcm) is inspired using the spatial information beyond the neighbor pixels, since these pixels involve similar features. for this, the spatial function should be defined as in eq. (7) [cit] ."
"the unlabeled examples with high confidence are put into training set. the main classifier is adjusted until the classification errors of the three classifiers are not reduced. finally, the algorithm is terminated. figure 2 shows the procedure of cegsl algorithm. the labeled example used by cegsl is defined as"
"the remainder of this paper is organized, as follows. section 2 presents the three different fitting models, together with the autocorrelation effect. the basic engineering details of the reflow process will be elaborated in section 3. simulation results of profile monitoring are presented to demonstrate the performance of different fitting models. lastly, the conclusions of the paper and the summary of our findings are remarked in section 4."
", which is logical in deeming the profile as a time series data set. since the autocorrelation effect often occurs in profile monitoring, the first-order autoregressive model (i.e., ar (1)) is considered as the disturbance term, , and is incorporated into the profile model. therefore, the profile model will include the firstorder autoregressive parameter to compensate. the parameter  is the first-order autoregressive (ar (1) (1) and (2) . also,"
"where r a is the amplitude, r b is the frequency, and r c is the horizontal phase constant at each sine wave term. for example, when the profile model is considered as the modified sum of two-sine functions, then the model can be defined by c influences the horizontal shift. the parameter estimation is performed by using the nonlinear least squares estimation method (see [cit] ). to strengthen the fitting of nonlinear models by means of the modified sum of sine functions, we also use the nonlinear mixed effects model (nlme) to test the fitted performance, which then is extended into a nonlinear model with random effects. the generic form of nlme is given by the following equation:"
"in eq. (4) [cit], u ij symbolizes the membership of x j belonging to the i th cluster, c is the number of clusters, v i is the center of the i th cluster, and m is a constant parameter. herein, the minimization of the cost function is obtained when the pixels close to the center of cluster are handled."
"the comparative results are also summarized in table ii to table v, which present the classification error rate of hypothesis, the final hypothesis generated by cegsl and the improvement of the latter over the former under 80%, 60%, 40%, 20% unlabel rate. the biggest improvements achieved by each algorithm have been boldfaced in tables. tables ii to vi show that cegsl algorithm can effectively improve the hypotheses with bp neural network and id3 decision tree under all the unlabel rates. in fact, if the improvements are averaged across all the data sets, classifiers and unlabel rates, it can be found that the average improvement of cegsl is 5.33% with bp neural network and 4.65% with id3 decision tree. it is impressive that with all the classifiers and under all the unlabel rates, cegsl achieved the biggest average improvement. moreover, tables ii to v also show that if the algorithms are compared through counting the number of winning data sets, cegsl is almost always the winner."
"; / * laplacian * / s1(t,k,j,i); / * helhmoltz * / s2(t,k,j,i); / * gsrb * / } } (b) after adding ghost zones."
"statistical process control (spc) has globally been applied for dealing with process monitoring in a variety of manufacturing processes [cit] . the control charting technique is typically designed to monitor a univariate statistic, e.g., the sample average, standard deviation, range of a sequence of sample data, among others. however, several productive processes (e.g., reflow oven, heat treatment, etc.) have proven difficult to manage with a traditional spc operation. the difficulty in these cases arises because a quality characteristic cannot be suitably characterized. if the quality characteristic of a product or process can be represented by a functional form between the quality characteristic and the input variable, then effective monitoring can be established. this scenario is the so-called \"profile monitoring\"."
"to understand the salient characteristics of performance, let us examine tuning along one axis at a time. first, figure 6 presents smooth performance as a function of fusion, wavefront, and ghost-zone depth. we restrict our presentation to the two finest grids which nominally dominate the run time. on hopper with its relatively small caches, increasing ghost zone depth (increasing the grid size) does not improve performance when using only the fused version. however, the wavefront technique does reduce the cache working set and allows for a significant speedup. conversely, on edison and its large caches, increasing ghost zone depth allows for more on-chip locality and thus improved performance. our compiler framework does not unroll the inner-loop, nor does it generate explicitly simdized code as yet. thus the benefit of increasing the ghost zone depth to 4, which increases computation, is currently limited by the code generation capabilities of the back-end compiler."
"in detail, figure 3 (a)~(d) show the average of classification error rate during the iterative process when bp neural networks is used under all data sets. from the results, we may see that the tri-training can effectively reduce the classification error only in the first two or three rounds. with the further iterations, the classification error rate has a greater increasing. since there is no effective way to prevent the introduction of noise data, the noise data will continue to accumulate during the iteration of the algorithm. therefore, it will give a negative impact on tri-training, especially in the case of less labeled example [cit] . moreover, when the cotraining is used, the introduction of noise data can be prevented in a certain extent by using the 10 crossvalidation. figure 3 reveals that on all the subfigures, the final hypotheses generated by cegsl are better than the initial hypotheses. comparing with the other two algorithms, the final hypotheses of cegsl are almost always better after first two or three iterations."
"the rest of the paper is organized as follows: section 2 includes comprehensive explanations of the methods and proposed pipeline. parameter settings are presented with their reasons in section 3, and the experiments are realized using well-known statistical metrics for evaluating the performance of a pipeline. the necessities of pipeline operators are examined regarding whether the operators are essential. a visual representation is settled to prove the pipeline's efficiency. besides, the pipeline performance is examined on training, test, and validation datasets. finally, section 4 contains concluding remarks."
"in medical image analysis, abdominal ct scans can contain several disadvantages during abdomen-based processes; these disadvantages can be defined as insufficient edge information, effect of the bed section, writing in the scan, low contrast, narrow histogram, and proximity between the edges of the abdomen and the ct. herein, the most important one at the forefront is the low-quality edges that can preclude accurate segmentation. the second most important handicap is the bed section, which can affect the results of segmentation, feature extraction, and classification. during these operations, other disadvantages (low contrast, narrow histogram, and writing in the ct scan) can also change the ability of a computer-aided diagnosis (cad) system in terms of integrability."
"most observations in clinical studies are based on multiway entity relationships (fig. 1) . a patient may have many follow-up visits or imaging studies (hence referred to as \"studies\") (1:n relationship), and each of these studies may generate zero, one, or many findings (hence referred to as \"lesions\"). all entities may be associated with categorical variables such as disease status or uptake of a contrast agent, or continuous variables such as a physical measurement or the blood level of a biochemical marker. the most appropriate way of handling multidimensional data is a relational database. we thus decided to base our development on a transactional database management system (dmbs) using structured query language (sql)."
"as seen in fig. 6, the first image includes an abdomen scan in which the edges of the abdomen are very close to the edges of the scan. the second image involves the appearance of a bed section in the ct surrounding the bottom side of the abdomen and the third image includes an abdomen in which the edges of the abdomen are discontinuous."
"as seen in table 5, every section benefits from segmentation, and these operators remain essential for an accurate segmentation process. in addition, the percentage differences between with/without parts prove the efficiency of operators in abseg. at this point, the most important sections are presented in table 5, meaning that other parts do not work for all images. this means that other parts are operated for the elimination of different problems. in table 5, arterial_12 stands for the image containing a low-quality (edged) abdomen."
"in table iv, under 40% unlabeled rate, the classifiers get enough labeled data for learning. both of classifiers become more stronger. therefore, the initial error rates for three algorithms are decreased. this causes the improvement of classification precision becomes smaller. cegsl, tri-training and co-training only get improvement of 3.35%, 1.96%, 2.4% respectively. under these circumstances, cegsl still expresses better performance."
"(1) the system should be network-based, allowing for concurrent data entry by several authenticated users. (2) the system should meet all current regulatory requirements in respect to data protection and security. ( 3) the system should allow for hierarchical data models supporting complex entity relationships and provide built-in mechanisms to enforce relational integrity. (4) modifications to the data models must be easy to implement even when data acquisition is under way. (5) the system should be cheap so that it can be shared between groups and projects without being limited by software licensing. (6) the software should be vendor-independent and multi-platform (e.g. linux, microsoft windows ® ) so that it can be expected to be viable for the entire duration of projects spanning several years [cit] ."
"as seen in table 8, abseg presents a remarkable performance by achieving a 99.5% average rate for six different metrics and it obtains better results for the test dataset than the training set. the average rates for training/test were 98.95/99.36% (jaccard), 99.47/ 99.67% (dice), 100/99.91% (sensitivity), 98.47/99.23% (specificity), 99.38/99.63% (classification accuracy), and 98.98/99.45% (precision). in other words, abseg proved its efficiency by increasing the rates of jaccard, dice, specificity, ca, and precision on a different dataset than that utilized in the parameter settings. figure 6 shows the output of pipeline on different sections to demonstrate the visual results of different characterized images."
"the main contribution of this work is the first exploration of compiler-directed communication-avoiding optimization for gmg. as compared to prior research on domain-specific compilers for the stencil computations that are included within gmg [cit], our work more closely addresses the needs of real-world applications because it optimizes in the context of an existing scalable parallel benchmark (minigmg), and it examines the complex and more representative operator gauss-seidel red black rather than the simpler jacobi. additionally, we enable autotuning to derive the ghost zone depth and threading strategies, thus allowing the automation of differing optimization schemes across individual levels of the v-cycle. this infrastructure is therefore adaptable to nextgeneration platforms with increasing memory-hierarchy and threading complexity. overall, we demonstrate portability with a 4x improvement for the most time consuming smooth of the v-cycle, while attaining up to 94% of previously published, highly hand-tuned performance [cit] ."
"when id3 decision tree is used, figure 4 (a)~(d) also show the average of classification error rate during the iterative process. it could be observed from the figures that the line of cegsl is always below those of the other compared algorithms after first two or three rounds. but, the error rate of cegsl keeps on decreasing when utilizing more unlabeled example, and converges quickly within just a few learning iterations. from subfigure (a) to (d), cegsl keeps comparable with all the classifiers under all the unlabel rates from figure 3 to figure 4, we may found that on all the subfigures, the final hypotheses generated by cegsl are better than the initial hypotheses. it confirms that cegsl can effectively exploit unlabeled examples to enhance the learning performance."
"as seen in abseg, necessary operators are processed to guarantee the segmentation of the abdomen on different characterized abdominal ct images. parameter settings and experiments are presented in the next section."
"to facilitate comparison of the system with competing solutions for data management, spss (v. 22.0.0.1, ibm inc.) was installed on a hospital system under microsoft windows 7-64 as an example of a popular statistics program, while an openclinica server (v. 3.1.4; https://community.openclinica.com) was set up as an example for a state-of-the-art open-source cdms. systems were evaluated by the author (m.b.) by entering test data originating from multimodal imaging of thyroid cancer patients. criteria included: gcp-compliance, provision of relational integrity, ease of upgrading the application in a networked environment, ease of making changes to the data model (such as adding a table column), and representation of categorical data by means of dropdown lists for rapid and reliable data entry."
"we focus on optimizations required for the smooth operator, the most compute-intensive portion of gmg. vertical communication-avoiding optimizations necessitate the support for data-flow analysis, which must be incorporated into the transformations to enable (1) fusing several operators so that intermediate data remains in cache from definition to use; (2) avoiding writes back to memory of temporary data; as well as (3) creating a wavefront so that multiple planes can share data in cache with a minimal cache working set. a horizontal communication-avoiding optimization adds ghost zones to reduce the frequency of inter-processor communication at the expense of redundant computation. while many of these optimizations rely on composing standard loop transformations, most would not be implemented in a standard compiler as they are either domain-specific (e.g., introduction of ghost zones), or specifically effective for gmg classes of computations."
"the particularly high performance based on the four measures arises from the suitability of the modified sum of two-sines with ar(1) for the reflow process data. thus, the model previously mentioned is considered to be the best model to undertake research. the polynomial model of order 4 with ar(1) outperforms the other polynomial models, thus being considered as the benchmark model. these two process models with ar(1) show great flexibility in dealing with complex modelbuilding situations, and therefore they are also expected to be extensively applied in a wide variety of nonlinear processes. they will be selected for evaluation in phase i and ii monitoring."
"to provide a base case, we first ran the 256 3 problem using a gsrb relaxation (from figure 3) with one process per numa node on edison and hopper. by fusing the operators in smooth, the compiler yields dramatic speedups in smooth time on the finer grids. however, the benefit degrades as one descends through the v-cycle. this effect is caused by the fact that in the baseline implementation, the working set of each triply-nested loop within smooth exceeds cache capacity for the fine grids. as a result, to construct the helmholtz, the laplacian must be refetched from dram. eventually this working set becomes small enough that it fits in the last-level cache at which point the disparity between on chip-bandwidth and compute capability limits the performance benefit."
"for use in our own non-gcp clinical research projects and based on earlier experience with a custom-designed data management system for a clinical trial [cit], we were looking for a system that met the following specifications:"
"in other parts of the pipeline, we try to eliminate this handicap for the obtainment of the abdomen. in addition, lof is added to the pipeline to guarantee the segmentation of the abdomen with the other efficient parts that are also prepared to minimize the effect of the aforementioned handicaps. some of the disadvantages can be eliminated at this point, or the"
"adaptive histogram equalization (ahe) improves the irregular contrast distribution by transforming each pixel with a function derived from a neighborhood region. in its basic form, the transformation function is chosen as the histogram of a square surrounding the pixel [cit] . contrast-limited adaptive histogram equalization constitutes an improved contrast enhancement technique using histogram features, and it differs from the general histogram equalization with contrast limiting process [cit] . at this point, contrast limiting is utilized to prevent the amplification of the noise level that arises from the contrast level increase [cit] ."
"5. to achieve desirable monitoring performances for other potential applications, the parameter setting of the control chart bears further scrutiny. a real-data examination of phase ii analysis should be further conducted to complement the research outcomes that are delivered in this paper."
the rest of the paper is structured as follows: section 2 describes graph-based semi-supervised learning. section 3 introduces the proposed algorithms. section 4 shows experimental and comparative results in different uci data sets. section 5 makes concludes.
"in the first part of the experiments, the parameter settings and training performance were obtained using the training dataset (16 ct scans) using six statistical metrics (dice, jaccard, etc.). in table 4, the segmentation results of the rectangles enclosing the abdomen are realized, and the results are obtained by comparing the rectangle template with the output of abseg."
"graph-based semi-supervised learning algorithm makes use of example sets and similarity to create a diagram. the nodes in the graph correspond to example. the weight of edge represents similarity that connects two examples. graph-based semi-supervised learning problem is a regular optimization problem. definition of the problem includes the objective function needed to optimize and regular items defined by decision function. it solves the problem by optimizing the parameters of optimal model. decision function for the model has two properties: (1) the output label from unlabeled example tries to match that from labeled example. (2) the whole graph satisfies smoothness. graph-based semi-supervised learning algorithm uses the popular assumption directly or indirectly. the assumption requires similar labels in a small local region and it also reflects local smoothness of decision function. under this assumption, a large number of unlabeled examples make the space of example more compact, thus it can indicate characteristic of local region more accurately and makes the decision function fit the data better."
". even so, it is very difficult to compare the modified sum of two-sine functions and the polynomial regression model in the composite approach, although the former performs slightly better than the latter. in a word, it is reasonable to allege that the modified sum of two-sine functions can be a viable modeling option for nonlinear profiling monitoring circumstances where only small samples are available for the reflow process."
"following the reflow zone, the product is cooled and the solder joints are solidified so that it can rejoin the assembly process. note that figure 2 only provides an overall, schematic diagram of the temperature profile of the reflow process. in fact, the temperature control in the reflow process belongs to a nonlinear profile pattern. hence, any linear approximation approach will not be suited to this research. the high nonlinearity and curvature somewhat warrants the need for a new profile monitoring approach. in this paper, the practical data of the same product type is gathered so as to form individual profiles for process monitoring. the production line in the smt practice is essentially constructed with high flexibility to deal with the different types of products. hence, to perform profile monitoring of a wide variety of low-volume products in small-to-moderate batches is our research target. note that the data set that was used in this research is available upon request."
"note that s2 has an additional term in its iteration space related to checking the color for the current element. operator fusion falls out implicitly from the iteration space alignment algorithm, which attempts to carve out a unified iteration space for the imperfect loop nest of the original code [cit] . with smooth operators such as jacobi, iteration space alignment is performed by default in chill. in gsrb, the difficult challenge is to rule out any fusion-preventing dependences when the iteration spaces are not a perfect match."
"in this section, the proposed monitoring framework is illustrated and evaluated while using the simulated reflow process in smt. the application domains of the monitoring system and some implementation issues are discussed. in terms of the simulation results, the analysis of profile monitoring can be done in three parts: (i) making a comparison of the fitting performance between the polynomial regression with ar(1), the modified sum of sine functions with ar (1), and the nlme model; (ii) screening the outlying profiles by means of the"
"the flow diagram of the algorithm proposed in this paper is shown in figure 1 . cegsl algorithm consists of following steps. first, reading examples to built a graph with labeled and unlabeled examples as vertex and the similarity between examples as edge. then, re-sampling labeled example set l with bootstrap to built initialized training set for three classifiers. for each classifier, the other two classifiers are auxiliary classifiers in each iteration. they classify the examples which are in unlabeled example set u and put the identified examples and their labels into a buffer. the confidence is calculated explicitly using the graph."
"the operation of the reflow process is the heating sequence for assembling printed circuit boards (pcb) using solder paste at successively higher temperatures. as an assembly moves through a soldering system, it will perform a controlled temperature curve in order to achieve the required quality. such a temperature curve is also called a \"temperature profile\". the temperature profile is often measured along a variety of technical dimensions, such as slope, soak, time above liquidus, and peak. in general, reflow soldering processes contain four stages. each operation presents a unique temperature profile: preheat, thermal soak (dwell), reflow (liquidus), and cooling. figure 2 shows a typical example of a schematic temperature profile. in the preheat zone step, the changes of the temperature curve can be described as an ascending tendency from normal temperature to approximately 150 °c. in this step, the ascending temperature facilitates the removal of solvent and water vapor in the solder paste. rapid heating helps the flux softening temperature to be reached quickly, so the flux can spread quickly and cover the maximum area of solder joints. it also integrates some activator into the actual alloy liquid. furthermore, because some parts of the motherboard cannot deal with the sharp temperature changes, the rate of temperature change in the preheating zone is set to between 1.5 °c/s and 3 °c/s."
"in fig. 1, the area between the red arrows (a3) is cleaned, but other parts (bed sections a2 and a1) cannot be removed from the ct scan because the horizontal vector obtained at the bottom of the image and the down sight of the vertical vector include a continuous gray change. concerning this, the lof algorithm can only clean a3 because of the (close) surrounding bed section."
"this study contains an efficient and statistical structure for the optimum segmentation of the abdominal region in ct images. according to the results, abseg performs the task of abdomen segmentation under different handicaps that occur in abdominal ct images. besides, it obtained very high segmentation results for the above six statistical metrics, with respective rates of 98.95, 99.2, and 99.57% for the training, test, and validation sets. moreover, it was not affected by the appearance of the bed section, patient information, narrow histogram, poor contrast, or low-quality edges by processing sequential operators that eliminate the handicaps. in addition, abseg is designed using optimal operators and parameters for the segmentation of the abdominal region. table 12 shows the literature comparison of studies based on abdomen segmentation. herein, it should be remembered that the results of abseg can be obtained on the segmentation of the rectangle surrounding the abdomen part. in other words, the results are based on a rectangle roi that surrounds the abdomen, meaning that a higher performance is achieved in the event that the rectangle overlaps the edges of the abdomen."
"cakephp was chosen as an application framework. cakephp is one of several open source application frameworks such as ruby, zend or symphony [cit] that allows the rapid generation of a web-based graphical user interface for an sql database. cakephp is written in php and distributed under the mit license. like many competing frameworks, cakephp incorporates a number of key concepts and technologies that reduce the need of hand coding web application pages [cit] . the model view controller (mcv) paradigm separates the application logic (controller) from the underlying data models (model) and the physical webpages (view). \"convention over configuration\" imposes a set of strict rules on the structure of the underlying sql tables including full normalization of the underlying database. when these rules are followed, cakephp will \"automagically\" [cit] choose the correct interface elements to represent a given entity, e.g. a dropdown list for representing categorical data or a checkbox for logical data (see supplementary materials 1). in combination with the \"natural language\" paradigm, this leads to easily maintainable databases with user-friendly human readable uniform resource locators (url) for the web interface. rapid development is promoted by scaffolding the application: a complete create read update delete (crud) interface for a table can be generated by 10 lines of cakephp code. the table is then dynamically read from the database server, and one can make repeated changes to the underlying sql table without having to re-code the application. when the database meets all requirements, the scaffolded application can be cast into php code by running the \"bake\" script. the static php code can then be manually edited to produce the final web-based application. special functionality not available within the cakephp framework such as semi-automatic import of patient and study data from one of the department's image databases is implemented outside the framework by means of hand-coded php pages. to facilitate re-use of existing code, cakephp projects can be cloned from existing related projects via a custom developed python script running on the server, while the underlying mysql database can be cloned by means of a custom php script."
and r denotes the order of the model in (3) . note that all of the parameters in equation (3) are estimated by using the ordinary least squares estimation method (see [cit]
"hopper: is a cray xe6 mpp at nersc. each node is in effect four 6-core opteron chips each with two ddr3-1333 memory controllers.there are thus four (non-uniform memory access) numa nodes per compute nodes. each superscalar out-of-order core implements the 2-way sse3 simd instruction set and includes both a 64kb l1 and a 512kb l2 cache, while each socket includes a 6mb l3 cache with 1mb reserved for the probe filter. the relatively small last-level cache makes capturing locality difficult on fine grid operations."
"the parameter estimates are obtained by using the maximum likelihood estimation method (see [cit] ). herein, it should be noted that the nlme model does not include the ar(1) term. in previous literature (see [cit] ), the nlme model has been used to solve the problem of autocorrelation. therefore, the two fitting models together with ar(1) and the nlme model are compared in phase i analysis."
"for problems on relatively few nodes, the performance of smooth on the finer grids dominates the run time. in this paper, we therefore focus on the smooth bottleneck, optimizing both the simpler jacobi that is common to compiler papers [cit], and the more complex gauss-seidel, red-black (gsrb), which predominates real-word applications and requires data-flow analysis and other support for control flow. overall we demonstrate that our compiler infrastructure can successively optimize both of these relaxation techniques and deliver high performance across our evaluated platforms."
includes both a 32kb l1 and a 256b l2 cache. the cores on a chip are connected by a ring to a 20mb l3 cache. the relatively large last-level cache makes capturing locality easier for 64 3 boxes.
"computed tomography (ct) has a wide range of uses in medicine. as a type of ct, abdominal ct scans are utilized to find abnormalities around or inside of abdominal organs such as the liver, spleen, colon, pancreas, kidney, adrenal, and stomach. in this area, valuable solutions and hybrid structures are designed using effective algorithms for segmentation, feature extraction, and classification tasks."
"this section first presents the parameter settings and problems, and then the experiments are investigated and interpreted in detail. [cit] b on a personal computer with 2.60 ghz cpu and 6 gb ram running windows 8."
"the right-hand side of the next coarser grid is defined as the restriction of the residual (f h − lu h ). eventually, the grid (or collection of grids) cannot be coarsened any further using geometric multigrid. at that point, most algorithms switch to a bottom solver that can be as simple as multiple relaxations or as complicated as algebraic multigrid, a krylov iterative solver, or a direct sparse solver. once the coarsest grid is solved, the multigrid algorithm applies the solution (a correction) to progressively finer grids. this requires an interpolation of u 2h onto u h . at each level, a smooth operator is applied to the new correction."
"validation data was taken from the cancer imaging archive (tcia) [cit] . in the validation dataset, chest and pelvic ct images were added to the trials beside the abdominal ct images. sensitivity, specificity, precision, classification accuracy (ca), dice, and jaccard were used as performance metrics for a detailed examination [cit] . in eq. (9-14), tp, tn, fp, and fn, respectively, symbolize the number of true-positive, true-negative, false-positive, and false-negative results."
"applications such as web search, pattern recognition, text classification, genetic research are examples where cheap unlabeled data can be added to a pool of labeled samples. in these applications, a large amount of labeled data should be available for building a model with good performance. during past decade, many supervised learning algorithms (e.g. j4.8, bays and svm) have been developed and extensively learned use labeled data. unfortunately, it is often the case that there is a limited number of labeled data along with a large pool of unlabeled data in many practices [cit] . it is noteworthy that a number of methods called semi-supervised learning have been developed for using unlabeled data to improve the accuracy of prediction [cit] . it has received considerable attention in the machine learning literature due to its potential in reducing the need for expensive labeled data. early methods in semi-supervised learning were using mixture models and extensions of the em algorithm [cit] . more recent approaches belong to one of the following categories: self-training, transductive svms, co-training, split learning, and graph-based methods [cit] ."
"in experiments, 58 ct images were utilized to evaluate the performance of abseg. these images include some parts of the following features: low-quality abdomen edges, appearance of bed section and writings, low contrast, narrow histogram, and close abdomen edges to the edges of the scans. in performance evaluation, the results are obtained for the segmentation of the rectangle (roi) surrounding the abdomen."
"four uci data sets are used in this experiment. detailed information on these data sets are tabulated in table i .the data set used in the comparative experiments includes two sets of credit card data sets-australian and german; two sets of medical diagnostic data set-breastcancer and diabetes. the experiment includes two groups. it takes bp neural networks and id3 decision tree as a classifier respectively, the performance of cegsl algorithm is compared with two semi-supervised learning algorithms, i.e. tri-training and co-training. figure 3 and figure 4 give the plots of the average classification error rates versus the learning iterations before the algorithm stops. the error rates of the compared algorithms are also depicted in figure 3 and figure 4 . the semi-supervised learning algorithms, three single classifiers with bp neural network and id3 decision tree are trained from only the labeled training examples, i.e. l. the average error rate of the single classifiers is shown as a vertical line in each figure, the iteration number for each algorithms is shown as a horizontal line."
"in eq. (7), h(x j ) is a square window on pixel x j, h ij stands for the probability of whether x j belongs to the i th cluster. herein, the value of h ij is higher if most of its neighbor pixels pertain to the same cluster with x j . spatial information is added to general fcm as in eq. (8) [cit] . in eq. (8), p and q are control parameters and the whole process prevents the noise affecting the clustering results by utilizing the neighbor intensities and weights [cit] ."
"after trials based on the training dataset, the proposed pipeline was implemented on test data that was completely diverse from the training set. table 8 shows the performance evaluation of abseg for test data."
"when the operation approaches the thermostatic zone, the temperature is usually maintained in a region of 150 ± 10 °c. this operational zone is a flat temperature profile to enhance the effect of the soldering, and it especially prevents tombstoning. the reflow zone is also called time above liquidus (tal). the tal is the period of time above the maximum temperature at which crystals can coexist with the melt in thermodynamic equilibrium. the peak of reflow temperature usually depends on the melting temperature of the solder, while also taking into account the temperature that assembled components can endure. for instance, a typical lead-free manufacturing process must not exceed the limit of 260 °c, which is the highest temperature that tantalum capacitors can endure."
"in the past, operations on large structured grids could easily be bound by capacity misses in cache, leading to a variety of studies on blocking and tiling optimizations [cit] . however, a number of factors have made such approaches progressively obsolete on modern platforms. on-chip caches have grown by orders of magnitude and are increasingly able to capture sufficient locality for the fixed box sizes associated with typical mg methods. the rapid increase in on-chip parallelism has also quickly out-stripped available dram bandwidth resulting in bandwidth-bound performance."
"the local computation (smooth) for the wavefront performs poorly on the coarsest grids as the size of those grids has ballooned from approximately 12kb per box to nearly 100kb. the increase in data movement impedes performance. figures 5(c) and (d) show the overall time required for the multigrid solve. although smooth may be slower on the coarse grids, the reduction in inter-box communication can actually accelerate the multigrid operations. we see this benefit applies to all levels of the v-cycle. however, where as on the coarsest grids, the benefit of reduced inter-box communication compensates for reduced smooth performance, the additional cost of communication actually impedes smooth performance on the fine grids. this presents an interesting tuning space for our compiler infrastructure, which was addressed via autotuning."
"largest object finder (lof) reveals the largest volume by utilizing the summation of rows and columns. after summation, the largest grayscale changes are found in the vertical and horizontal vectors, and the rectangular volume including the abdomen is obtained by utilizing the coordinates of these largest gray values. two handicaps are eliminated through the lof approach: (1) the unnecessary parts belonging to an irrelevant region and (2) the rows and columns including zero values. herein, the unnecessary parts stand for the areas including the appearance of the bed section and the writings in the scan. the pseudocode of lof is presented in table 1 . the first process of lof is contrast enhancement using logarithmic nonlinear stretching, as shown in eq. (3) [cit] ."
"generalizing to other gmg applications: this general approach would apply to other gmg implementations that use a similar composition of operators. when other operators are optimized by the compiler, the approach must be generalized for their associated stencils, deriving the appropriate ghost zone depth based on the dependence distances arising from the stencil shape."
"in the ordinary smt operation, if the process recipe is suitably tuned before volume production, then the profile specification should be pre-determined and only subjected to a minuscule adjustment as a result of product changes. therefore, seven different types of shifts are considered in our simulation study. the shifts of these process parameters are applied to the amplitude, the frequency, and the horizontal phase constant in the model, as shown in equation (5) . the simulation results of arlout can be used to evaluate the on-line monitoring capability. these arlout values are calculated by setting equally spaced parameter shifts every 100 simulated profiles (maximum"
"starting with the baseline minigmg code of figure 3 (a), we now describe how our compiler transforms the code to realize both vertical and horizontal communication-avoiding optimizations. the compiler first fuses the multiple smooth operators together. in the case of gsrb relaxation, the control flow guarding the update to phi may prevent fusion in some compilers. incorporating data-flow analysis allows us to fuse the loops safely by contracting the iteration space of the first two statements (see next section). fusion is itself a vertical communication-avoiding optimization, since the results computed by one operator will remain in cache when used as input by the next operator; an additional communication-avoiding optimization is to replace the array temp with a scalar and not write it back to memory on completion. the compiler generates the code in figure 4 (a), with placeholders for the statements corresponding to figure 3 (a)."
"in tables 6 and 7, performance comparison is presented using the outputs of binarization, otsu [cit], and sfcm approaches. as seen in table 6, sfcm always outperforms otsu and binarization algorithms in terms of achieving better dice, jaccard, and ca rates on images including low-quality edged abdomen. as seen in table 7, the performance of sfcm and otsu is the same for qualified edged abdomens, and their performance draws a graph that remains better than the binarization process. consequently, it is inferred that the sfcm algorithm seems the best choice, since it achieves optimum results for both low-quality and qualified edged abdomen images."
"t control chart is employed to evaluate the different fitting models while identifying the outlying profiles. while phase i is executed, the process should be able to achieve a stable situation. subsequently, the data of the in-control profiles is employed to estimate the unknown parameters. in phase ii, the exponentially weighted moving average (ewma) chart is additionally used for detecting the autoregressive (ar) effect in residuals in order to determine whether the ar parameter in the process model should be re-estimated. in sum, the 2 d t control chart is used to monitor the parameters of the model (i.e., profile shape). in the meantime, the ewma control chart is used to monitor the residuals (i.e., profile variability). the ewma statistic is computed by"
"in this paper, the cegsl algorithm is proposed. this algorithm combines graph-based semi-supervised learning and collaboration-training algorithms. it makes use of structure information of sample data to calculate the classification probability of unlabeled example explicitly. this algorithm is facilitated with good efficiency and generalization ability because it can effectively select sample data to label and use multiple classifiers to help to perform the final hypothesis. experiments on uci datasets prove the efficiency of this algorithm. cegsl is worth studying in determination of classification error rate in future work. its applicability is wide because it does not requires sufficient and redundant views. moreover, using statistical techniques to further identify and deal with noise data can be researched in the future."
"in this section, the polynomial regression model, the modified sum of sine functions, and the nlme model are first used to fit the reflow process data. the polynomial models of orders 3-5 and the modified sum of 1-3 sine functions are selected for model fitting. in every profile, n measurements in the ith random profile are collected over time, as indicated by ), ( (4)- (7). in here, seven models, including the polynomial model of different orders with ar(1), the modified sum of sine functions with ar (1), and the nlme model, are tested. fifteen profiles of 48 data points that were collected each in the reflow process are individually modelled by using the seven different models, and the parameter estimates are utilized for phase i monitoring. to compare the fitting results, four performance measures ( table 1 and figures 3-5 . from the fitting results in table 1, it can be seen that the modified sum of two-sine functions exhibits a better fitting performance (with less sic and aic) than the other fitted models. typically, using a large sample of profiles for parameter estimation in the phase i analysis is necessary, especially for nonlinear profiles. in this study, only fifteen profiles can be obtained due to a technical limitation. the excellent fitting performance that is shown in table 1 and figures 3-5 must be attributed to the appropriateness of model selection and the flexibility of the chosen models under investigation. if the fitting performance is not satisfactory, then more profile data need to be collected for the estimation purpose before proceeding to the phase ii analysis."
"chill is polyhedral transformation and code generation framework designed specifically for autotuning. a polyhedral model represents each statement's execution in the loop nest as a lattice point in the space constrained by loop bounds, known as the iteration space. then a loop transformation can be simply viewed as mapping from one iteration space to another, and various transformations can be composed. chill manipulates iteration spaces derived from the original program, using a data dependence graph as an abstraction to reason about the safety of the transformations under consideration [cit] ."
"1. if the profile pattern exhibits a significant autocorrelation effect, then the proposed framework can use a different profile model with ar(1) and the proposed model selection procedure to strengthen the fitting performance. furthermore, we feel safe to conclude that the sum of twosine functions with ar(1) can be a viable modelling option for nonlinear profiling monitoring instances where only small samples are available for the reflow process. 2. in phase i of the reflow process that is investigated in this paper, two types of composite models all have good monitoring ability for identifying outlying profiles. however, the nonlinear mixed effects model cannot resolve the problem of autocorrelation in the residuals. this situation will cause difficulties in monitoring when autocorrelation is present. 3. according to the phase i results of the reflow process that was investigated in this study, the hotelling t 2 control chart can produce satisfactory performance for monitoring of the process profile. 4. on the whole, the proposed monitoring framework displays better detecting performances than the traditional polynomial regression model in phase ii analysis for the reflow process that is discussed in this paper. in addition, the proposed ewma control chart is also effective in detecting changes of the autocorrelation effect in residuals. this study pinpoints a major finding, a fact that the modified sum of two-sine functions is able to statistically fit the nonlinear profile of the reflow process data extremely well. in the proposed framework, the hotelling t 2 control chart and the ewma control chart work in harmony to simultaneously monitor the parameter estimates (i.e., profile shape) and residuals (i.e., profile variability), respectively. the simulation results in phases i and ii illustrate the proposed monitoring framework. therefore, the practitioner can follow the guidelines of model building and process monitoring that are demonstrated in this paper, as the nonlinear profile monitoring task of the reflow process is necessary."
"thus, in recent years, numerous efforts have focused on increasing temporal locality by fusing multiple stencil sweeps through techniques like cache oblivious, time skewing, wavefront or overlapped tiling [cit] . many of these efforts examined 2d or constant-coefficient problems -features rarely seen in realworld applications."
"and then averaging across 20,000 simulation cycles. the parameter shift in the scale of  β ranges from 0.5 to 3 for the six process parameters. any parameter shift will cause the change of curve shape that is closest to the type of sustained shift in spc practice. figure 10a shows the comparative profiles under various shifts of different process parameters. in order to detect the autocorrelation effect in the residuals of each profile, the ewma control chart is used to implement the related monitoring tasks using the different scales of the autocorrelation coefficient, from 0.1 to 0.9 (see figure 10b) . table 2 gives the arlout estimates for shifts of the six parameters. the experimental results indicate that, for monitoring the shape of the model, both of the composite models perform reasonably well regardless of whether the data is modeled by the modified sum of two-sine functions or the polynomial regression model. we also use the ewma control chart to monitor residuals if any autocorrelation effect in addition to the ar(1) already included in the model is exhibited. the results show that the modified sum of two-sine functions, combined with ar(1), performs much better than the pure polynomial regression model as 1.5"
"the most closely related work are domain-specific compilers for parallel code generation from a stylized stencil specification [cit] or from a code excerpt [cit] . pochoir uses a cache oblivious strategy, which limits the control over the code generation [cit] . the other compilers introduce parallelism and ghost regions through tiling and expanding both the data set and the tile size, rather than starting with already parallel code [cit] . these tiling approaches do not produce the hyper-trapezoidal loop nests of this paper, but rather compute and then ignore some incorrect results. none of these approaches appear capable of supporting the optimization of a collection of operators, particularly if gsrb is included."
"finding no suitable software solution that met all our current requirements, we set out to develop a new simpler and more scalable solution for data management in our own clinical research projects."
"the target function of graph-based semi-supervised learning algorithm includes two parts, loss function and regular items. different algorithm selects different loss function and regular item. zhu x j [cit] . this method is a continuous relaxation method for discrete markov. the loss function in objective function is a quadratic function with infinite weight. regular item is a combinational laplacian based on graph. although a variety of graphbased semi-supervised learning algorithm set the objective function differently, they can be concluded to formula (1) cegsl algorithm combines the advantages of semisupervised learning and collaboration-training algorithms. it uses three classifiers to perform collaborative training and compare the confidence of unlabeled examples implicitly. in order to select more reliable unlabeled example to join to training set, it makes use of structure information of examples to calculate the classification probability of unlabeled examples explicitly."
"in fig.7, the abdomen, chest, and pelvis areas change as they are diverse from one another, and the shape of the rois seems non-proportional. as it differs from the training and test datasets, the validation set includes high-resolution ct images. herein, pet/ct images are not included in the validation data because the segmentation operation of these images constitutes another qualified paper because pet/ct images include the arm parts of patients, which prevent the accurate segmentation of ellipsoid rois."
"by default, chill reports a fusion-preventing dependence between s2 and s0 related to the reads and writes of phi. however, we make the observation that the iteration spaces for the laplacian and helmholtz operators (statements s0 and s1) may compute values of temp that are never used by the gsrb of s2. array data-flow analysis can be used to analyze the iteration spaces and access expressions and derive a conservative approximation of the elements of temp defined in s0 and s1 and used in s2 [cit] . our compiler determines that the array region read by s2 is a proper subset of the regions defined by s0 and s1. since temp is a local variable redefined on every sweep and it is not live after the smooth operator is completed, it is safe to contract the iteration spaces of s0 and s1 to match that of s2. after the compiler contracts the iteration space, the fusion-preventing dependences are eliminated and chill is able to safely fuse the loops. the iteration space contraction used here is an example of a domain-specific optimization that was proven safe by the compiler, but is more likely to be profitable for gmg operators where red-black conditional execution is common."
"these limitations do however not affect the main conclusion of the paper that the application of a generic web application framework based on the mcv paradigm is a feasible, flexible, low-cost, and user-friendly way of managing multidimensional research data in researcher-initiated studies."
"the constants in lof (a and b) are adjusted to obtain contrast enhanced-distinct edges and to form a clear input to the lof algorithm. for the size of median filter, optimum kernel is revealed as [5 5 ], since smaller values cannot remove noisy and blurry vision. in addition, the larger size can cause the loss of edge information. the sfcm threshold number is chosen as b1^and the proposed pipeline is designed according to this rule."
"this paper presents a new monitoring framework for dealing with the autocorrelation effect that exists in the errors around the functional relationship when only small samples are available. the research framework includes model building and phase i and ii analyses. the central idea of the proposal is how to construct an appropriate profile model that is capable of dealing with the time series effect. using different profile models (the polynomial regression model, the modified sum of two-sine functions, and the nonlinear mixed effects model), the phase i and ii analyses of reflow process data can be conducted. in phase i, the hotelling 2 d t control chart is utilized to screen the outlying profiles. when the outlying profiles are investigated and removed, then the same control charts with the ewma control chart for monitoring autocorrelation are used for phase ii monitoring, where the detectability of parameter shifts in terms of arlout is evaluated. according to the comparison results, some concluding remarks and suggestions can be provided:"
"abseg includes coherent operators that can regulate one or more problem. at this point, abseg's parameter settings play an important role in the elimination of handicaps for accurate abdomen segmentation. in other words, parameter settings are figure 4 a shows the closing process attaching the abdomen to the edges of the ct scan in which the structuring element is chosen as disk. figure 4 b presents the closing that includes two line-type structuring elements that combine the results with angles 15 and 165°to prevent adhesion around the right and left edges of the abdomen. as seen in fig. 4 a and b, it is explicit that the structuring element should be properly chosen for abdomen images in which the outer edges of the abdomen stay close to the edges of the ct scan. as seen in fig. 4, the type of structuring element plays a key role in eliminating the negative effects of close ct edges and close bed appearance to the abdomen. besides, the other types of structuring elements (diamond, square, etc.) ruin the as seen in fig. 5, the outer edges of the abdomen involve unconnected parts and should be linked together. in fig. 5 a and b, the dilation process is realized to reveal the outer edges of the abdomen. figure 5 a presents dilation using the disktype structuring element, but this process cannot reveal the abdomen's unconnected edges. figure 5 b shows dilation using the line-type structuring element. the output image is obtained by summing the outputs of line-type dilated images using 36 different angles (0°, 5°,…, 175°) that strengthen the outer edges of the abdomen and complete the unconnected parts. herein, suitable types of structuring element constitute an indispensable need for the implementation of edge detection."
"in table 5, the importance of operators is clarified by extracting the parts, and by running the system on different images. tables 6 and 7 compare the performance comparisons of three thresholding techniques in terms of obtaining the abdomen section for low-quality and qualified edged abdomens. higher rates are italicized in the tables."
"to facilitate reliable and consistent data entry, a customized graphical interface with on-screen forms is mandatory. if several users are to take part in data collection, a network-based client-server architecture is necessary. in accordance with modern internet practice, we opted for a three-tier architecture consisting of database server, application server and \"thin\" clients ( fig. 2) . to avoid the need for excessive hand coding of web pages and increase reusability of code, we looked for a web framework that would allow the easy generation of a graphical front end for a given sql database."
"as seen in the visual results, morphological processes fulfill their tasks without any disruptive effect on the abdomen, the best results are presented as italicized numbers the best results are presented as italicized numbers the best results are presented as italicized numbers and this situation is confirmed by the output images. besides, the effectiveness of abseg can be seen with regard to its operation as immune to different handicaps. in other words, the optimum and statistical structure can be obtained for the best results are presented as italicized numbers the best results are presented as italicized numbers changeable situations that can prevent the abdomen's segmentation."
"the basic aim of this study is to obtain the abdomen for both low-quality and qualified edged abdomen images. this requires forming morphological and spatial transforms and the pipeline should be registered as unaffected by the disadvantages of the nature of ct scans. concerning this, several efficient approaches (both formed by us and taken from the literature) are used to design a robust pipeline. herein, the pipeline's name (abseg) is defined using the abbreviation abdomen segmentation process. in this section, the algorithms used are defined in four subparts: (1) well-known morphological algorithms, (2) proposed simple morphological approaches, (3) sfcm, (4) proposed pipeline. herein, the techniques are presented in an obvious and brief manner to prevent monotony and provide a better understanding."
"for performance evaluation, the used data was divided into three groups: training (16 images), test (16 images), and validation (26 images). training and test images were provided by siemens somatom definition flash ct. in this device, the reconstruction package is advanced modeled iterative obtain nonzero output using these last coordinates reconstruction (admire) and used the industry's first rawdata-based iterative reconstruction method (safire)."
"in detail, under 80% unlabel rate, cegsl has 4 wining data sets when bp neural network is used; when id3 decision tree is used cegsl has 3 wining data sets while co-training has 1 wining data set. under 60% unlabel rate, when bp neural network and id3 decision tree are used, cegsl has 3 wining data sets respectively while tri-training has 1 wining data set respectively; under 40% unlabel rate, cegsl has 4 wining data sets when bp neural network is used; when id 3 decision tree is used, cegsl has 3 wining data sets while co-training has 1 wining data sets. under 20% unlabel rate, when bp neural network is used, cegsl only has 2 wining data sets while co-training has 2 wining data sets; when id3 decision tree is used cegsl has 3 winging data sets and co-training has 1 wining data set. in table ii, under 80% unlabeled rate, the average error rate of corresponding improvements for cegsl algorithm is 5.33% when bp neural network is used. it is better than tri-training (2.0%) and co-training (3.45%). similarly, when id3 decision tree is used as classifier, cegsl algorithm not only has higher error rate of corresponding improvement for german, breast-cancer, and diabetes than tri-training and co-training, but also the final error rate (12.48%) is better than tri-training (14.34%) and co-training (13.62%)."
"to restrict access to the database, the server is run inside the protected hospital network. communication between thin client and application server is encrypted by transport layer security (tls, https). each project has its own user administration with usernames and passwords. user roles were implemented through an extension of the cakephp 1.x framework described in supplementary materials 2. current norwegian legislation [cit] demands that data stored in research databases should not contain patient identification, and that the key list between the patient code and the unique national person identity number (npid) be stored in a location separate from the other observations. this condition is met by using the dbms to partition the data set into different databases so that the key list resides in a database that is only accessible to administrators. to avoid duplicate entries in the patient table, a hash of the npid is retained with the data. npid and hashed npid are inserted into the key list in the protected database by means of an sql trigger (see supplement 3 for details)."
"introducing ghost zones: once fused, the iteration spaces from the previous section end up with a combined iteration space that matches that of statement s2. we observe that introducing ghost zones as in the previous section is really just introducing a new loop t and changing the bounds for each of the loops in the fused loop nest to compute ghost regions and generate a hyper-trapezoidal iteration space."
"the life cycle of most researcher-initiated projects, especially when they are of a more exploratory nature, is different. software applications for research projects are special in that they are often specific to a particular project with very few users and that the life cycle of the application is strictly determined by the duration of the research project. reusability and robustness of the code are therefore paramount to minimize development costs. as figs. 4 and 5 document, there were regular code changes in all of our three pilot projects over the entire duration of each project, 37% of them because of adjustments to data model and/or data validation rules. based on our previous experience with our own custom-developed cdms for a clinical trial [cit], we early on decided that cdms were unsuited for managing data in our current projects and looked toward a generic public domain web application framework for our software development."
"even more impressive, our compiler may now insert additional ghosts zones and generate a wavefront-based update for the smooth operation. once again, on the finer (large) grids, the performance benefit arises from trading dram accesses for cache accesses. we generally see a roughly 66% increase in smooth performance on the finer grids except for edison where the benefit is roughly 40% for the finest level. the benefit is likely smaller on edison as its massive 20 mb cache is nearly capable of capturing the locality required on the finest grids."
"outside the sphere of gcp-compliant clinical trials run for approval by regulatory bodies, data management appears to be an often-underappreciated topic in clinical research. [cit] among over 70 european academic centers running clinical trials found that 90% had cdms in routine use [cit], the vast majority researcher-initiated non-gcp studies are restricted to spreadsheet software [cit] or statistics programs for data collection. these suffer from a simple tabular representation of the data and from being single-user systems. there is no good reason why standards for data consistency and data security in non-gcp researcher-initiated studies should be systematically lower than in clinical trials."
"in (1) and (2), the variance estimate is denoted by  is calculated after the autocorrelation effect has been discounted by using the first-order autoregressive model, which will be addressed shortly."
"after training, tests, and visual results, a validation process is performed to confirm the performance of abseg on a new data taken from the cancer imaging archive (tcia) [cit] . for this, 26 ct images were randomly chosen from a dataset that included different types of scans (pet/ct, chest ct, abdominal ct, and pelvis ct images). the data used consists of 12 abdominal cts, 11 chest cts, and three pelvis ct images. herein, the performance of abseg is evaluated on chest ct and pelvis ct images besides abdominal-based trials, for whether it can be utilized on the segmentation of similar (ellipsoid) shaped rois (chest ct and pelvis ct images)."
"gradient magnitude gradient means the generalized version of the derivative and stands for the direction of the largest increase of function [cit] . the gradient magnitude (gm) refers to the slope of the graph in the direction that owns the largest increase [cit] . gm is generally utilized to determine the direction of objects in an image, and this feature brings gm to the forefront for applications based on edge detection."
a fuzzy c-means (fcm) algorithm constitutes a clustering method that divides the data into one or more classes [cit] . fcm constitutes an iterative structure that obtains partitions by minimizing the objective in eq. (4) [cit] .
"geometric multigrid (gmg) is an important family of algorithms used by computational scientists to accelerate the convergence of iterative solvers for linear systems. in gmg, floating-point computation is dwarfed by the overhead of data movement, making managing the memory hierarchy and crossprocessor communication critical to achieving high performance. prior work on optimizing the stencil computations that are contained within gmg have led to techniques like cache oblivious algorithms, time skewing, wavefront optimizations and overlapped tiling [cit] . for many of these efforts, the problems were simplified as compared to real-world applications, using 2-dimensional or constant-coefficient stencils without control flow and starting from a sequential specification rather than parallel specification."
"according to table 10, the segmentation performance of abseg remains remarkable for the abdomen and it can segment the abdomen better than the chest and pelvis ct images for all metrics. in future evaluation, its usage may be extended to chest and pelvis ct images in terms of including an ellipsoid roi as in abdomen ct images, but the amount of data should be increased to generalize the performance of abseg to chest and pelvis ct images. currently, it can be seen that abseg can segment abdominal ct images with a high success rate, and its performance seems remarkable for similar shaped rois and the chest and pelvis areas. table 11 reveals that abseg performs the task of abdomen segmentation on different datasets with higher performance for approximately all metrics (except sensitivity) compared to trials realized on training and test datasets. it can be concluded here that the resolution quality directly affects the system's output, and a higher resolution is required to attain better segmentation results."
edison: is a cray xc30 mpp at nersc. each node contains two 8-core xeon sandy bridge chips. each chip has four ddr3-1600 memory controllers.each superscalar out-of-order core implements the 4-way avx simd instruction set and overview of evaluated platforms.
"based on this experience, we set out to develop a new, simpler solution for data management in our own clinical research projects which met the specifications as detailed in section 1."
"in table v, under 20% unlabeled rate, the original labeled data can train a strong classifier and the performance of unlabeled data is decreased. the improvements of cegsl, tri-training, and co-training only reach 2.48% ， 1.46% ， 1.35% respectively. the cegsl shows greater achievement."
"to assess user experience in an unbiased manner, a user survey comprising 22 questions was conducted using surveymonkey (http://www.surveymonkey.net) [cit] (supplementary materials 4) . survey results were plotted using r library \"ggplot2\" [cit] ."
"the compiler system of this research effort employs autotuning to make these optimizations portable across different architectures. autotuning systems employ empirical techniques to evaluate the suitability of a search space of possible implementations of a computation. communication-avoiding smooth operators aggregate communication and perform some redundant work in the ghost zones in order to dramatically improve locality in the last-level cache. the wavefront op- timization exploits reuse deeper in the memory hierarchy (l1/l2), but risks exceeding capacity limitations if applied too aggressively. the granularity of profitable thread-level parallelism depends on both architecture and the level in the gmg v-cycle. through autotuning, these tradeoffs can be explored to select the context-specific optimal solutions, across a variety of architectures."
"as seen in table 4, abseg achieves reliable segmentation results with high scores for all performance metrics. this means that abseg performs the task in different circumstances by obtaining high success rates (especially for dice and jaccard) of approximately 99%."
"the input to chill is a source code written in c or fortran, and optionally, a transformation recipe describing the set of transformations to be composed to optimize the provided source [cit] . this recipe can be written by an expert programmer, or derived automatically by a compiler decision algorithm [cit] ."
"in the fused code, the compiler recognizes that array temp is a local variable, and does not need to be rewritten back to memory. because there are no dependences on temp crossing iteration boundaries, scalar replacement is then employed to make this a scalar that is overwritten on each iteration of the loop."
"a user satisfaction survey was conducted among the 14 active users of the software (excluding the developer m.b.), all of whom responded. 57% of users were over 40 years old with an even male to female ratio. 29% had college-level education (technician, mercantile), 71% university training, 22% at phd-level. 72% of users characterized themselves as \"normal\" computer users, 1 as computer novice, 2 as power users, and none as it professional or developer while 1 user did not disclose her level of expertise. 57% of the users had been using the software for more than 1 year. 78% of users had been using the software for one research project, 22% for two. 93% of users had been involved in data entry while 21% used the platform for publications and abstracts with a total of 2 manuscript submissions so far. 56% of users had edited datasets belonging to more than 100 study subjects. system downtime reported by the users was nil. four users reported experiencing bugs, and 1 user missing features in the software which interfered with their work up to 3 times a year. all bugs were repaired within 24 h while missing features were typically implemented within one week. when asked what they liked best with the software, 8 out of 10 users emphasized the software's user-friendliness. average score was 4.7 on a 5-point scale from 1 (worst) to 5 (best) (fig. 6) . 79% of the users would like to use the software for future projects."
"in this study, the two simple morphological approaches we formed were processed to ease the segmentation of the abdomen and improve the abdomen segmentation performance."
"we now describe the compiler implementation that generates the desired code from the previous section. this compiler is domain specific, in that it employs optimizations that are designed for multigrid applications. the input to the compiler is the code excerpt shown in figure 3 . we performed the implementation using chill, which is a polyhedral transformation and code generation framework [cit] . the remainder of this section describes the abstractions used in chill, how these abstractions are used to represent the transformations described in the previous section, the algorithm that performs these transformations, and how autotuning is employed to optimize the code for different architectures."
"lof is used to prevent the negative effects of bed section and the writings that can influence edge detection for the abdomen. in addition, the lof algorithm can effectively remove the bed section and the writings that do not intersect with the abdomen in the vertical and horizontal directions. however, the handicap of lof is revealed as the adjacent segmentation of the abdomen and bed section as one part if the appearance of the bed section surrounds the bottom of the abdomen; this situation is shown in fig. 1 ."
where k denotes the degrees of freedom and  denotes the significance level. sullivan and woodall [cit] argue that the simulation results can be used to discover that the 2 c t control chart (see equation (9)) performs worse in detecting the step change and the ramp shift in the mean vector during phase i than the
"limitations of the system: (1) the system is not intended for conducting clinical trials according to gcp standard. (2) the system is not meant to compete with clinical data warehousing solutions integrated into electronic medical records [cit] . the system is meant to provide a means of consistent data entry where such systems are not available or where the needs for data analysis goes beyond the level of detail provided by such systems. (3) the emphasis of the present system is on simplicity and flexibility with ready adaptability of existing code to new research problems. there is less focus on consistency of data models and data dictionaries across research applications. this flexibility is an advantage when conducting exploratory research projects. for example, there is yet no loinc term for human thyroglobulin analyzed in the washout of an us-guided fine-needle biopsy [cit], a method which we routinely employ in our \"mmtc\" project. however, the openness of the system entails that existing classifications such as icd-10 can be readily integrated into the system as for example in our \"petdb\" project. (4) while the proposed system has been used in single workstation configurations (all 3 tiers on a windows 8-64 laptop computer) and with up to 15 active users inside the protected hospital network, a major development effort would be needed before the system can be exposed to a larger circle of users and/or less secure networks. the entire application would need to be hard-coded, not just scaffolded, user roles and privileges would need to be more granular, and an audit log would need to be implemented for recording all changes made to the data. while all these changes are possible to implement, they would detract from the main virtue of the system, its simplicity. (5) cakephp may not have the necessary performance for supporting a very large number of concurrently logged on users."
"in the pseudo-code that invokes smooth in figure 3 (b), for each subdomain, the smooth operation is called on all the boxes in parallel. after the smooth operator, the boxes go through a communication phase, where they exchange boundary data with their neighbors. an important communicationavoiding optimization is to create ghost zones, which replicate some of the input data across processes and threads. through the use of ghost zones, the computation can perform several sweeps of the grid per communication step, trading off increased computation for lower communication costs. for the seven-point second-order stencil we consider for our study, an n -deep ghost zone allows n sweeps of the grid between communication. for higher order stencils, the ghost zone depth required increases with order."
"the variable color gets updated with every sweep of the grid, so its value will also be affected by the additional loop. for this purpose, we apply another mapping to color:"
"the main novelty in this manuscript is that we present a simple and flexible approach for data management in researcher-initiated projects based on common public domain components. so far, our project-specific applications have been easy to clone and adapt to use in new research projects focusing on different organ systems and/or research questions, and we now intend to migrate the application for the management of our pre-clinical imaging projects and our departmental pediatric hip imaging registry [cit] . the user survey documents the validity of our solution in the context of our three pilot projects. satisfaction scores among the 14 active users of the software were high (fig. 6 ) independent of the level of education or computing experience. interestingly, the category \"flexibility\" received the lowest satisfaction scores in the survey. this is presumably because the flexibility of the software lies in the implementation of data models and data analysis, not so much in the user interface, which is designed to enforce a standard workflow for data entry."
"the three different parametric models will be considered as the modeling candidates. afterwards, the different fitted models are evaluated by means of 2 adj r, akaike information criterion (aic), and schwarz information criterion (sic). next, in terms of the best-fitted model, phase i and ii process monitoring is performed."
". a large number of unlabeled example is defined as in the algorithm, step3 and stet4 keep the quality of selected unlabeled examples; step5 performs the selection of unlabeled example. when selecting the number of unlabeled examples, if more unlabeled examples are selected, it will increase the introduction of possibility of noise. if the selected example set is small, the convergence rate will be affected. after repeated experiments, the algorithm takes top 10% unlabeled examples to help the training of classifiers with better achievement. also, the number of iterations k is set to 20 in this experiment. since the calculation for classification error of unlabeled example is more different, this paper assumes that there is a same distribution for labeled and unlabeled examples. the classification error rate i e is defined as number of error classification for labeled examples/number of same labeled examples. similarity s ij is defined as:"
"the autotuning phase tunes the ghost zone depth at each level (ghostzonedepth [level] in figure 3(b) ) and the number of threads x and y controlling intra-and inter-box parallelism, respectively. during the tuning process, constant values for these are bound during code generation, resulting in very efficient context-specific specialized code such as in figure 4 (d)."
"in future work, we want to use abseg to detect and segment different types of tumors that can occur in the abdomen. moreover, the usage of abseg will be extended to segment different regions in other scan types."
"similarly, figure 7 shows how performance varies as a function of threading. the compiler generates a nested openmp region to deal with inter-and intra-box parallelism. thus on our evaluated platforms using today's technology, it generally seems to be best to maximize inter-box parallelism within the wavefront."
"the zerocross eliminator (ze) fulfills the cleaning of rows and columns in which all intensities equal zero; for this, it uses the summation of the rows and columns. after the summation process, ze moves forward to the gray values from the middle of the outer edges of the image in four directions. thus, it eliminates the zero values that came from the rest of the pipeline, meaning that ze removes unnecessary zeros, sensitively; this situation is presented in fig. 2 . figure 2 shows that the ze algorithm eliminates the zeros in the direction shown with a red arrow, since the other parts include nonzero intensity levels. the pseudocode of the ze algorithm is as shown in table 2 ."
"in the proposed framework, three different models are investigated to seek an appropriate profile model. additionally, the hotelling t 2 and the ewma control charts are employed in order to monitor the profile shape and the profile residual, respectively. the flowchart is shown in figure 1 . first, the different process models are compared on the basis of 2 adj r, aicc, and sicc. according to the parameter estimates of the profile model, the nonlinear profile can be monitored and analyzed. in phase i, the hotelling t 2 control chart is used to evaluate the process stability and remove any outlying profiles. the hotelling t 2 control chart is also considered for phase ii analysis via the outof-control average run length (arlout). an ewma control chart is utilized to check the residuals of the fitted model if the autocorrelation effect is changed or not."
"code changes over time in the three above projects are plotted in fig. 4 based on the commit logs of the subversion server. 35% were due to usability enhancements, 9% to new features, and 20% due to bug fixes (fig. 5) . the median changed lines of code were 11 for model changes, 30 for changes in validation rules, 16 for usability enhancements, 59 for new features, and 9 for bug fixes with a median of 3 source code files affected. there were no statistically significant differences in the types of changes or the number of lines per change between the three projects."
"as seen in the explanations stated above, the optimum parameter values are investigated and ensued according to the abdomen segmentation requirements. similarly, all operators and the used parameters of operators are examined to achieve the optimal parameter values and operation conditions. thus, the parameter settings of abseg are as shown in table 3 . herein, these settings are performed by using the training dataset (16 abdominal ct images)."
"we had previously developed our own client-server application based on an oracle database (oracle corp. inc., redwood city/ca) with oracle forms graphical clients [cit] for data management in a prospective randomized multicenter trial. the msds trial on external beam radiotherapy (rtx) for locally advanced differentiated thyroid cancer (dtc) was run in close collaboration with the department of biometrics/competence centre for clinical studies (kks) at the university of münster. challenges in managing the trial were its interdisciplinary design involving endocrine surgery, pathology, radiotherapy, and nuclear medicine with separate reference centers for each specialty, and the trial's size (429 patients), duration (10 years), and geographical distribution (50 participating centers in 3 countries). as none of the then available cdms were found to be suited to the task within the funding constraints of the trial, we decided to proceed with our own development based on earlier prototypes on the same platform. [cit] . the client-server architecture had the advantage that data could be entered simultaneously by several concurrent users and that all entered data could be validated by the client application before being committed to the database. a number of fundamental drawbacks became however apparent over time: (1) client application updates were difficult to enforce at distant sites. (4) the data models were too complicated, as they had to be derived from the paper-based case report forms approved before the start of the trial. (5) data analysis based on structured query language (sql) was inflexible. changes in a single column would have to be propagated through a series of cascading sql views, making even minor changes costly to implement."
"our third major application called \"pro\" (prostate) is dedicated to mri of the prostate. the data model underlying the lesions table had to be modified to cover several sets of observations (three radiologists who read three mri series each; histopathological gleason score by one pathologist) in each of 27 anatomical segments in the prostate gland. the lesions table was expanded to cover all 27 segments. these segments are shown in anatomical arrangement in order to eliminate coding errors by the observers. each radiologist codes four sets of observations per study (three mr series, one overall impression) while the application blinds him/her as to the pathology and the observations entered by the other radiologists. this application, which currently contains more than 60 000 [cit] . a manuscript has recently been submitted [cit] ."
"in table iii, under 60% unlabeled rate, the average error rate of corresponding improvements for cegsl algorithm is 4.32% when bp neural network is used. the improvement of average error is higher than tri-training (2.15%) and co-training (2.13%)."
"abseg is an efficient pipeline that can be used on low-quality and qualified edged abdomen images. the proposed pipeline is not affected by the appearance of the bed section close to the bottom of the abdomen. the small writing at the corner of the images is also removed by wise additions (operators) in pipeline. besides, abseg can be used on images that include low contrast and on scans owing to a narrow histogram. the flowchart of the pipeline is as shown in fig. 3 ."
"the convergence curves for all optimization methods on all problems were evaluated (see fig. 2a for a representative example and supplementary figs s15-s28 for the complete set). convergence curves for ms were plotted by mimicking the scenario in which ess and pso were used: we emulate 10 virtual processors, each of which performs local searches sequentially for the same time allowed to the ess/pso runs. the local searches in these emulated sequential optimizations are sampled without replacement from the pool of 100 searches launched in each ms. thus, the success ratio (sr) reported for ms methods is the fraction of the 10 virtual processors that find a solution whose objective function value falls below the vtr. note that this sr is not the same as the fraction of successful searches of the 100 launched in the ms. numerical values of the horizontal and vertical views of said curves are provided in the supplementary tables s3-s20, and graphically in supplementary figures s65-s82 ."
"in this study, we present the results of this collaboration: the development of a performance metric suited for the comparison of different methods, and the evaluation of the state-of-the-art in parameter estimation methodologies. based on these results we provide guidelines for their application to large kinetic models in systems biology. to this end, we use seven previously published estimation problems to benchmark a number of optimization methods. the selected problems are representative of the medium and large scale kinetic models used in systems biology, with sizes ranging from dozens to hundreds of state variables and parameters (see table 2 for details). to the best of our knowledge, this is the first time that a systematic evaluation of parameter estimation methods is conducted on a set of problems of this size and characteristics. we compare several variants of state-of-the-art optimization methods which have been recently reported as competitive options for large problems, including multi-start [cit] and hybrid metaheuristics [cit] . we perform systematic comparisons between these different approaches using metrics capturing the performance/robustness trade-off. finally, we discuss the implications of our results and provide guidelines for the successful application of optimization methods in computational systems biology."
"in which x(t) is the vector of state variables at time t, x 0 is the vector of initial conditions, f is the vector field of the ode, g is the observation function and p is the vector of unknown constant parameters with lower and upper bounds p l p p u . parameter optimization for dynamical systems is a nonlinear dynamic optimization problem that aims to find the vector of parameter values p that minimizes the distance between model simulation and measured data subject to the dynamics of the system and (potentially) other possible constraints. the distance is measured by a scalar objective function (or cost function), which can be of several forms. one common choice is the weighted least squares objective function given by:"
"similar to 0-1 degree and 0-1 closeness centrality, the 0-1 katz centrality measures the importance of a gene among disease genes and non-disease genes, respectively, which is more appropriate for disease gene prediction. the new feature vector of each gene is then defined as"
"the first step is to evaluate convergence curves, which show the objective function value as a function of computation time (fig. 1a) . for ess and pso, the convergence curves are constructed from single searches as they reach the predefined maximum cpu time. for ms optimization, each convergence curve corresponds to a sequence of local searches and continues until the predefined maximum cpu time was reached."
"in previous studies, katz centrality calculated from heterogeneous networks had been used to prioritize disease genes [cit] . however, results of directly using katz centrality were not better than existing methods, such as rwr [cit] . to make katz centrality suitable for disease gene prediction, we define the 0-1 katz centrality as follows:"
"our comprehensive evaluation clearly shows that high-quality sensitivity calculation methods provide a competitive advantage to local methods that exploit them. optimization using adjoint and forward sensitivity analysis (fmincon-adj and nl2sol-fwd) usually outperform the gradient-free alternative (dhc). this is reflected in the dispersion plots (see, e.g. fig. 2b ) and in a higher cumulative oe ( fig. 2c ) and holds for ms and ess settings. the combinations of ess with gradient-based methods, ess-fmincon-adj and ess-nl2sol-fwd, clearly outperform the gradient-free alternatives, ess-dhc and ess-noloc, as well as the also gradient-free pso. notably, successful optimization of bm3 for the given computational budget required adjoint sensitivity analysis in combination with optimization in the log-scale (fig. 2d )."
"to assess the performance of the different optimization methods, we solved the 7 benchmark problems using the 16 optimization methods listed in table 1 . for each problem, multi-start local optimization (ms) used 100 starting points, while enhanced scatter search (ess) and particle swarm optimization (pso) were run 10 times, each time until the predefined, maximum problem-specific cpu time ( supplementary table s1 ) was reached. the overall computational effort was $450 cpu days (intel xeon e5-2600 2.50ghz processor)."
1 a single sample-based network is constructed for each case sample; 2 case samples are classified into different clusters; 3 networks of the samples in the same cluster are fused together.
"given m disease genes and m non-disease genes, features of these genes extracted from the l fused networks are used to train l logistic models, respectively. equation (4) is then used to predict the probability of each gene being disease-associated in each fused network."
where θ is a parameter vector and φ i is the feature vector of gene i extracted from the biomolecular network labeled by a prior configuration x.
"in this study, we assessed the average performance of optimization methods for the benchmark problems. complementary, it would be interesting to see how the performance of each method depends on problem characteristics, e.g. problem size. the assessment of this would however require a large number of problems with different characteristics. since this is currently not available, we refrain from attempting a systematic evaluation of this feature."
"for the first step, we assume that a ppi exists in a single sample-based network n s only if the two interacted proteins are both activated in sample s. concretely, a gene i in a case sample s is considered being activated if"
"parameter estimation for dynamical systems is an inverse problem [cit] ) that exhibits many possible challenges and pitfalls, mostly associated with ill-conditioning and nonconvexity [cit] . these properties, which are in general only known a posteriori, influence the performance of optimization methods. even if we restrict our attention to a specific class of problems within the same field (e.g. parameter estimation in systems biology), there are often large differences in performance between different applications [cit] . hence, methods need to be benchmarked for a representative collection of problems of interest in order to reach meaningful conclusions. in this study, we consider the class of medium to large scale kinetic models. these models pose several challenges, such as computational complexity, and an assessment of the performance of optimization methods is particularly important [cit] ."
"the benchmark problems have been implemented in matlab (mathworks, natick, ma, usa) using the amici toolbox (frö [cit] b), a free matlab interface for sundials solvers [cit] . the optimization methods have been implemented as matlab scripts calling solvers from the matlab optimization toolbox, the matlab global optimization toolbox and the meigo toolbox [cit], and making use of the efficient gradient computation provided by the amici toolbox. the code necessary for reproducing the results reported here is provided as supplementary material and is also available at zenodo https://doi.org/10.5281/zenodo.1304034."
"where mcase[ i, s] is the expression value of gene i in sample s, and mean(mcntl[ i] ) is the mean expression value of gene i over all control samples. to construct n s, every edge (i, j) in the static ppi network is validated and only the one with both i and j being activated is added to n s . then, s single sample-based networks are constructed for the s case samples. for the second step, hierarchical clustering is used to classify case samples into different clusters. given two samples s 1 and s 2, their pairwise distance is calculated by"
"assuming m disease genes are known to be associated with the disease under study, we randomly select m genes from the set of non-disease genes, and these 2m genes form a set of gold standard genes. this process is performed 50 times and finally we obtain 50 sets of gold standard genes and regarded them as benchmarks."
"the static ppi network is downloaded from the inweb_inb-iomap database [cit] _09_12) [cit] . the database consists of more than 600,000 protein interactions collected from eight source databases, which insures that valuable protein interactions are not missed during the construction of the sample-based ppi networks. in this study, the proteins in the ppi network are mapped to their corresponding genes to form a gene-gene interaction network. in the paper, the term \"ppi network\" is used to represent the gene-gene interaction network because of simplicity."
"the resulted roc curves for bc, tc, and ad are depicted in figs. 5, 6, 7, respectively. the auc values of edgcsn for bc, tc and ad are 0.970, 0.971 and 0.966, respectively, which are much better than those of the competing algorithms. for bc, our edgcsn is 7% more accurate than the competing algorithms, and for tc and ad, edgcsn is 20% more accurate than the other three algorithms."
"global methods aim to locate the global solution by means of either deterministic [cit] or stochastic [cit] strategies. deterministic methods include so-called complete and rigorous approaches, both of which can ensure convergence to the global solution under certain circumstances. in contrast, stochastic (also known as probabilistic) methods can only guarantee global optimality asymptotically in the best case [cit], but can solve many problems that cannot be"
"in addition, de novo validation is performed by ranking all the unknown genes in descending order by their average probabilities calculated by the models trained tc, respectively, and two fused networks are constructed for ad."
"overall, the best performing method in our tests was ess-fmincon-adj-log, that is, a hybrid approach combining the global metaheuristic ess with the local method fmincon, provided with gradients estimated with adjoint-based sensitivities. this was the only method that succeeded in calibrating all the benchmarks and it also achieved the highest overall efficiency for the set of thresholds adopted in this study. to facilitate the application of this and other methods, we provide their implementations in the supplementary material. in the case of the best performing method, our solver is-to the best of our knowledge-the first publicly available implementation. accordingly, our study provides access to a novel optimizer applicable to a broad range of application problems in systems biology."
"a central goal of our work was to re-examine past results regarding the performance of multi-start and metaheuristics (i.e. enhanced scatter search). firstly, we have confirmed that multi-start local optimization is a powerful approach [cit] as it solved most considered benchmark problems in a reasonable time. the only exception is b3, a problem for which numerical simulation fails for many parameter points. secondly, we verified that the enhanced scatter search metaheuristic often possesses higher success rates and efficiency compared to plain multistart optimization methods (gá [cit] ) . however, the difference of a factor of two was smaller than suggested by several previous studies and will likely depend on the set of benchmark problems. furthermore, the average improvement by a factor of two is smaller than the variability across benchmarks, implying that for many problems the use of multi-start methods is still beneficial (e.g. each method is represented by a stack of the oes observed for the individual benchmark problems. the maximum possible score is the same as the number of benchmarks, i.e. seven. (d) successful methods for each benchmark are shown in color; methods which never succeeded for a given problem are shown in white. a, b and d use the thresholds vtr c and maxt a as defined in the supplementary table s1 . panel c shows the average oe across all considered vtrs and maxts bm3). thirdly, our results confirm that purely global optimization strategies (i.e. not combined with a local method), such as pso and ess-noloc, are less efficient than hybrid ones. finally, we have assessed the influence of parameter transformations, concluding that optimizations in logarithmic scale clearly outperform those in linear scale. this parameterization can always be easily adopted, irrespective of the optimization method used."
"once an optimization has reached the desired vtr (horizontal view), it is considered successful. the success rate (sr) of an algorithm is the fraction of searches that reached the vtr within this maximum computation time, maxt (fig. 1b) . complementary, we evaluate the average computation time required by an algorithm, hti, which is the minimum of the time required to reach vtr and maxt. in the third step, we consider dispersion plots of the success rate versus average computation time to study the relation of the two quantities (fig. 1c ). note that this dispersion plot may reveal in some cases a pareto set structure, consisting of algorithms which provide an optimal trade-off between conflicting goals (in this case, high success rate and low computation time): it is not possible to improve one of its objectives without worsening the other. we are interested in methods that are located towards the bottom (i.e. high success rate) and left (i.e. low computation time) of this plot. therefore, in the fourth step, we quantify the trade-off between success rate and average computation time using a novel metric called overall efficiency (oe). the oe for method i on a given problem is defined as:"
"as expected, the optimization results indicate that the performance of the optimization methods varies substantially among the benchmark problems. this is in agreement with previous studies [cit] ."
"in summary, our workflow considers multiple metrics and summarizes the trade-off between computational complexity and success with the novel performance metric oe. as the oe is interpretable, comparable between models and methods and accounts for computation time and final objective function values, it fulfils all the aforedefined criteria."
"in the experiments conducted on bc, tc and ad, our edgcsn is much better than the competing algorithms in terms of auc scores. further analysis of the top 10 unknown genes also illustrate that edgcsn is capable of predicting novel disease genes. our study has provided insight into how clustering patient samples might improve the prediction of disease genes."
"where a is the adjacency matrix of the network, k is the length of the path between i and j, α is a damping factor penalizes the impact node j on i. the longer the path, the smaller the impact node j is on i."
"for the quantitative evaluation we performed the analyses for two maxts and nine vtrs per benchmark (see supplementary table s1 ). we found that the ranking of the methods with respect to the oe depends only weakly on the maxts and the vtrs ( supplementary figs s47-s64 ). for visualization in figure 2a, b and d, we use a high maxt (maxt a) and a moderate vtr (vtr c) which ensures a good agreement of model simulation and data."
"hybrid global-local methods attempt to exploit the benefits of global and local methods. by combining diversification phases (global search) and intensification phases (local search), hybrid methods facilitate reliable global exploration and fast local convergence. as a result, hybrid methods can potentially outperform the efficiency (convergence rate) of purely stochastic methods while keeping their success rate. one such hybrid method is the so called multi-start (ms) strategy [cit], which solves the problem repeatedly with local methods initialized from different (e.g. random) initial points. thus, ms can be regarded as one of the earliest hybrid strategies, and there are different extensions available (hendrix and tó [cit] . an alternative family of hybrid methods are metaheuristics (i.e. guided heuristics). an example is the enhanced scatter search (ess) method [cit] b), [cit] . the ess method combines a global stochastic search phase with local searches launched at selected times during the optimization, in order to accelerate convergence to local optima. further accelerations can be achieved by parallelization [cit] ."
"after downloading the gene expression data, four steps are performed to control the genes used in our study. (1) . tpm values less than 1 are replaced by 0 because of the unreliability. (2) . log 2 (tpm + 1) is used instead of the original tpm values. (3) . genes expressed in less than 10% of samples (case and control) are removed. (4) . genes not existing in the ppi network are removed. in total, 14436 genes, 13959 genes and 13370 genes are left for bc dataset, tc dataset and ad dataset, respectively."
"results for other choices of vtr including larger and smaller values are shown in the supplementary figures s29-s82 and tables s3-s21. in the following, we present the key findings of our analysis and address, amongst others, the question of which is the most efficient method for performing parameter optimization. the detailed evaluation results are presented in the supplementary material. in particular, quantitative values for the performance improvements mentioned in sections 3.2-3.4 can be found in supplementary table s21 ."
"in this study, datasets of breast cancer (bc), thyroid cancer (tc) and alzheimer's disease (ad) are used to evaluate the algorithm. the bc-associated genes and tcassociated genes are obtained from the cancer gene census category (http://cancer.sanger.ac.uk/census) [cit] . in total, 35 bc-associated genes and 33 tc-associated genes are used as the benchmarks. the ad-associated genes are obtained from malacards: the human disease database (http://www.malacards.org/). the database contains 182 potential ad associated genes ranked by their probability of being ad-associated in descending order. 39 of the first 50 genes exist in the static ppi network are used as benchmarks."
"another common choice for the objective function is the loglikelihood. assuming independent, normally distributed additive measurement noise with standard deviation r ;o s, the likelihood of observing the data d given the parameters p is:"
"to obtain the most informative ppis and remove the false positive ones, sample-based networks are used in this study instead of the universal static ppi networks. in addition, since the real caustic genes of different patients may not be the same, case samples are divided into different clusters so that patients with distinct conditions are analyzed separately. specifically, three steps are performed to obtain the sample-based networks."
"previous studies reported that the transformation of the optimization variable to log-scale improves the reliability and computational efficiency of local methods [cit] . our findings corroborate these results and show for the first time that also global optimization methods are more efficient when using logscale (log) than linear-scale (lin). overall, we observe an average improvement of the cumulative oe at least by a factor of 2 ( fig. 2c) . indeed, for some problems (bm3, tsp), reasonable fits could only be obtained using the log-transformed parameters (fig. 2d )."
"the calibration of large-scale kinetic models usually requires the optimization of a multi-modal objective function [cit], i.e. there will be several local optima. local optimization methods, such as levenberg-marquardt or gauss-newton [cit], which converge to local optima, will only find a global optimum for appropriate starting points. convergence to a suboptimal solution is an estimation artifact that can lead to wrong conclusions: we might think that the mechanism considered is not suitable to explain the data, while the real reason might be that the method failed to locate the global optimum [cit] . in order to avoid suboptimal solutions, many studies have recommended the use of global optimization techniques [cit] . one of the earliest and simplest global optimization methods is the multi-start, which consists of launching many local searches from different initial points in parameter space, assuming that one of them will be inside the basin of attraction of the global solution. it has been shown that multi-starts of local optimization methods can be sufficient for successful parameter estimation in kinetic models (frö [cit] b; [cit] ), although the use of other approaches, such as metaheuristics, has also been advocated (gá [cit] ."
"to validate the performance of edgcsn in predicting new disease genes, unknown genes are ranked in descending order by their average probabilities of being table 4 shows the top 10 predictions of the three diseases. functions of the genes that have not been studied in existing literature are left blank. most of the genes have been analyzed as disease-associated in existing studies, especially for bc, where all the 10 genes have been studied in the existing literature. for tc, although only 5 of the 10 genes have been studied, 3 of the 5 genes that have not been studied ('cep72', 'cep131' and 'gpr83') belong to the centrosomal protein family and g proteincoupled receptor respectively. many proteins belong to these families are closely related to cancers [cit], which means 'cep72', 'cep131' and 'gpr83' might be predicted as being tc-associated in the future."
"in this study, we consider seven benchmark problems based on previously published kinetic models [cit] which describe metabolic and signalling pathways of different organisms (from bacteria to human). these problems possess 36 to 383 parameters and 8 to 104 state variables. the data points are collected under up to 16 experimental conditions, corresponding to the number of required numerical simulations. the features of all problems are summarized in table 2 . the benchmarks b2-b5 had been previously included in the biopredyn-bench collection [cit], and bm1 & bm3 were used in (frö [cit] )."
"to solve these issues, in our previous study, gene expression data of clinical samples have been used to construct sample-specific ppi networks [cit] . each single samplebased network only contains the significant ppis associated with the disease under consideration, which reduces the false positive interactions. a network that fuses all the single sample-based networks was used to predict the disease-associated genes, so that disease genes that function in different patients could all be identified. in this study, to further extend our research, an ensemble algorithm that predicts disease genes from clinical samplebased networks (edgcsn) is proposed. meanwhile, katz centrality is used instead of edge clustering coefficient to better extract local structural information from the sample-based networks. figure 1 depicts the work flow of edgcsn which is explained as follows. (a)-(b). a single sample-based network is constructed for each case sample by combining clinical samples and the universal static ppi network. (c). the case samples are clustered into a few groups and single sample-based networks of the samples in the same group are fused to one network. (d). a logistic model is trained by the centrality features extracted from each fused network, and the probability of each gene being disease-associated is predicted. (e). the maximum probability of a gene calculated from all the logistic models is regarded as its probability of being disease-associated. in the following subsections, details of the five steps in fig. 1 are first discussed. then, the data sources and evaluation metrics are explained."
"mechanistic kinetic models provide a basis to answering biological questions via mathematical analysis. dynamical systems theory can be used to interrogate these kinetic models, enabling a more systematic analysis, explanation and understanding of complex biochemical pathways. ultimately, the goal is the model-based prediction of cellular functions under new experimental conditions [cit] . during the last decade, many efforts have been devoted to developing increasingly detailed and, therefore, larger systems biology models [cit] . such models are often formulated as nonlinear ordinary differential equations (odes) with unknown parameters. as it is impossible to measure all parameters directly, parameter estimation (i.e. model calibration) is crucial for the development of quantitative models. the unknown parameters are typically estimated by solving a mathematical optimization problem which minimizes the mismatch between model predictions and measured data [cit] ."
"interestingly, our study of the dispersion plots revealed that ess-fmincon-adj-log often maximizes success rate and minimizes mean computation time. accordingly, in these cases there is-in contrast to what we expected-no trade-off, but we have a clear winner."
"the comparison of all methods reveals that ess-fmincon-adj-log possesses the best overall efficiency on the considered benchmark problems and settings, followed by ess-nl2sol-fwd-log (fig. 2c) . the difference in performance between both methods is small; indeed, for certain vtrs, ess-nl2sol-fwd-log is the best performer (vtr c, see supplementary figs s51 and s52) . however, ess-fmincon-adj-log is the only method that successfully solves all problems (fig. 2d), while ess-nl2sol-fwd-log fails for bm3, possibly due to the very large number of states and parameters of this problem. in summary, our performance evaluation hence suggests the use of ess-fmincon-adj-log."
"in this paper we have presented a comparative evaluation of stateof-the-art optimization methods for parameter estimation in systems biology. we have applied these methods to benchmark problems of different sizes (medium to large) and complexities. to compare the different methodologies in detail, we have used a multi-criteria workflow, exploring several possible ways of assessing the performance of optimization methods for this task. we have reported results using a number of selected indicators and evaluation tools. furthermore, we have introduced the concept of overall efficiency (oe), which quantifies the trade-off between success rate and computation time, providing a numerical indication of the most efficient method. we have found that this metric is a convenient summary of the comparative performance of a method on a set of problems."
"our edgcsn use ensemble learning to predict disease genes from clustered sample-based networks. in the future, the strategies used for clustering can be further improved. for instance, eq. (2) uses the expression data of all the genes to calculate the pairwise distances, and the results might be dominated by non-disease genes. we could reduce the number of genes used for clustering and choose those differentially expressed genes or marker genes that are associated with a specific subtype. these subsets of genes should improve the clustering results as well as the final prediction."
"the ideal optimization method for the above class of problems would be able to find the global optimum with guarantees and in a short computation time. furthermore, it should scale well with problem size and be able to handle arbitrary non-linearities. currently, no such method exists. local gradient-based methods [cit] can be efficient but will converge to the local optimum in the basin of attraction where they are initialized. local gradient-free (also called zero-order) methods, such as pattern search [cit], are less efficient than gradient-based methods but more robust with respect to situations where the gradient is unavailable or unreliable [cit] ."
"our results show that ms is usually sufficient to find a good solution, given the same computation time as ess (fig. 2d ). for strict vtrs (i.e. vtr e and vtr f), ms and ess perform equally well. however, ess were generally more efficient than ms (fig. 2c ). on average a 2-fold improvement of the oe is observed, almost independent of the local method. the reasons for the efficiency improvement is probably that ess starts the local searches from promising points found through advanced exploration and recombination strategies. in this regard, it can be considered as an 'advanced multistart' [cit] ."
"in all hybrid methods the efficiency of local methods plays a major role. the most efficient local methods are gradient-based, so their performance depends crucially on the accuracy of the gradient calculations [cit] . the simplest way of approximating the gradient is by finite differences. however, more accurate gradients are provided by forward sensitivity analysis [cit] and adjoint sensitivity analysis (frö [cit] b) . while the former provides information on individual residuals which can be used in least squares algorithms, the latter is more scalable."
"is the corresponding simulated output, and w ;o s are constants that weight the observables in the objective function according to their magnitudes and/or the confidence in the measurements."
"upon consideration of a variety of different evaluation criteria, we decided to adopt a workflow consisting of several steps, which lead to a newly proposed metric that is a distillation of the information obtained in previous steps. the workflow considers the following criteria:"
"where dsp(i, j) is the length of the shortest path between node i and j, n 0 and n 1 are the number of nodes labeled as 0 and 1, respectively katz centrality measures the relative influence of a node in the network [cit] . it is defined by"
"1. single, interpretable quantity 2. comparable across models and methods (to enable an integrated analysis) 3. account for computation time and objective function value a number of evaluation criteria have been used in the literature to compare the performance of optimization methods, e.g. dispersion plots of objective function value versus computation time and waterfall plots showing the ordered objective function values found by the different searches. these and other plots are reported in the supplementary figures s1-s14. alternative criteria are performance profiles (dolan and moré, 2002) which report for a given set of optimization problems how often one algorithm was faster than all others. the required assumption that all algorithms converge is relaxed in data profiles (moré [cit] ) by considering the decrease in objective function value and reporting the fraction of solved problems as a function of the budget per variable. while all these plots are useful tools, they do not provide a single, interpretable quantity and fail in other ways."
"many algorithms have been proposed to predict disease genes, and most of them rely on ppi networks to achieve the prediction. however, ppi is dynamic and tissue- potential disease gene [cit] specific, static ppi networks downloaded from online databases contain many false positives, and directly using them would limit the accuracy of disease gene prediction. moreover, for patients with a specific disease, their disease states might be driven by different subset of disease genes, and analyzing their data together might affect the identification of rarely mutated disease genes . therefore, in this study, an ensemble algorithm is proposed to predict disease genes from clinical sample-based networks. the algorithm first constructs single samplebased networks by combining clinical samples and a universal static ppi network. a group of networks which contain disease-related ppis are generated. then, case samples are divided into different clusters and networks belong to the samples in the same cluster are merged together. this step allows patients with similar causing genes to be analyzed together. after that, 0-1 centrality features extracted from the fused networks are used to train the logistic models that calculate the probability of each genes being disease-associated in each fused network. finally, an ensemble strategy is performed by choosing the maximum probability obtained from different fused networks as the final probability of a gene being disease-associated."
"we considered two sophisticated gradient-based methods, fmincon with adjoint sensitivities and nl2sol with forward sensitivities, whose use was mostly beneficial. a gradient-free local method, dhc, was found to be less precise than the gradient-based counterparts, although its use may still be advantageous in problems with numerical issues that limit the efficacy of gradient-based techniques."
"since the number of genes used as benchmark is small, leave-one-out cross validation (loocv) is performed the average auc value is then used to evaluate the algorithm."
"we remark that a good agreement of model output and data does not imply that the parameter estimates are correct or reliable. practical and structural non-identifiabilities can prevent a parameter from being precisely determined [cit] . still, an accurate fit-and hence optimization-is the starting point for many uncertainty analysis methods. state-of-the-art identifiability analysis methods have been recently evaluated elsewhere (chis¸et al., 2011; [cit] ."
"to evaluate the algorithm, the receiver operating characteristic (roc) curve is created by plotting the tpr against fpr with various . the area under the roc curve (auc) is also used to evaluate the overall performance of the algorithm."
"in addition to measures mentioned above tague-sutcliffe also calculated the precision averaged over the 11 standard recall levels (0.0 to 1.0) and precision averaged over the 9 number of document levels. using these output measures an analysis of variance was carried out. the model was a repeated measures design, where the runs were performed over the same set of queries:"
"-∆ is a finite set, which contains (amongst others) all individual names (occurring in the considered abox) -c is a function that maps each element of ∆ to a set of concepts -e is a function that maps each role to a binary relation on ∆."
"the participants in trec-3 returned ranked lists of the first 1000 documents retrieved by their systems. the union of all retrieved documents for each query was used as the set of documents to be judged for relevance by an independent set of judges. the total number of relevant documents in this set was used as the basis for judging recall (ratio of relevant-retrieved to total relevant). so in fact the calculated recall is actually a maximum recall. precision (ratio of relevantretrieved to total retrieved) was calculated at each rank. various other measures can then be calculated such as the average precision at standard recall levels [cit] . another measure used throughout is the average precision over all relevant documents retrieved. the precision is calculated after each relevant document is retrieved. if a relevant document is not retrieved, the precision is zero. these precision values are then averaged together to get a single performance number for one system on one query. these values are averaged over all queries to get an average precision for a system. they can also be averaged over systems to get an average performance for a query. they also used r-precision, which is the precision calculated after r documents are retrieved where r is the total number of relevant documents. another measure used was the precision at standard numbers of documents retrieved, at 5, 10, 15, 20, 30, 100, 200 and 1000."
"proof. this lemma follows from the observation that, after a state w getting status incomplete, all edges coming to w will be deleted (see step 1 of procedure applyconvrule)."
"in this paper we study automated reasoning in the dl shi, which extends the dl alc with transitive roles, inverse roles and role hierarchies. the aim is to develop an efficient tableau decision procedure for the sat problem in shi. it should be complexity-optimal (exptime), cut-free, and extendable with useful optimizations. tableau methods have widely been used for automated reasoning in modal and description logics [cit] since they are natural and allow many optimizations. as shi is a sublogic of sroiq and reg c (regular grammar logic with converse), one can use the tableau decision procedures of sroiq [cit] and reg c [cit] for the sat problem in shi. however, the first procedure has suboptimal complexity (nexptime when restricted to shi), and the second one uses analytic cuts."
"for any state w, every predecessor v of w is always a non-state. such a node v was expanded and connected to w by the static rule (f orming-state). the nodes v and w correspond to the same element of the domain of the interpretation under construction. in other words, the rule (f orming-state) \"transforms\" a non-state to a state. it guarantees that, if beforeformingstate(v) holds then v has exactly one successor, which is a state."
"our calculus c shi for the description logic shi will be specified, amongst others, by a finite set of tableau rules, which are used to expand nodes of tableaux. a tableau rule is specified with the following information:"
"for trec-3 each participant was given a set of \"user needs\" statements (called topics) as constructed by users of retrieval systems. they are referred to as \"topics\" to differentiate the user statements from actual queries constructed by the participants and submitted to the information retrieval systems. for this research the topics from the trec-3 tests, fifty for routing, fifty for ad-hoc were analyzed by counting the number of content words in total and in the various sections of the topic statement. a stop list of 168 common words was used. the topics for the routing and ad-hoc parts of the test each had different sections (see appendix 1) recorded in an sgml-like format. for the ad-hoc queries word counts were calculated for the complete query and the title, description and narrative sections. for the routing queries word counts were calculated for the complete query and the title, description, summary, narrative, concepts and definition sections. the statistics provided by the trec-3 project also included the number of relevant documents for each query and the median (over runs) average precision for the topic. the statistical description of these topics is in table 1 ."
"rule (i.e. a rule with only one possible conclusion) or an \"and\"-rule then its conclusions are \"firm\" and we ignore the word \"possible\". the meaning of an \"or\"-rule is that if the premise is satisfiable w.r.t. r and t then some of the possible conclusions are also satisfiable w.r.t. r and t, while the meaning of an \"and\"-rule is that if the premise is satisfiable w.r.t. r and t then all of the conclusions are also satisfiable w.r.t. r and t ."
"when a node w gets status incomplete, unsat or sat, the status of every predecessor v of w will be updated as shown in procedure updatestatus(v) defined on page 10. in particular:"
"by the local graph of a state v we mean the subgraph of g consisting of all the path starting from v and not containing any other states. similarly, by the local graph of a non-state v we mean the subgraph of g consisting of all the path starting from v and not containing any states."
to extend retrieval evaluation to larger databases and to a larger number of systems a series of text retrieval conferences (trec) have been conducted by the united states national institute of standards and technology (nist) and sponsored by the advanced research projects agency (arpa). [cit] with 24 [cit] .
"ontologies provide a shared understanding of the domain for different applications that want to communicate to each other. they are useful for several important areas like knowledge representation, software integration and web applications. web ontology language (owl) is a layer of the semantic web architecture, built on the top of xml and rdf. together with rule languages it serves as a main knowledge representation formalism for the semantic web. the logical foundation of owl is based on description logics (dls). some of the most well-known dls, in the increasing order of expressiveness, are alc, sh, shi, shiq and sroiq [cit] ."
"retrieval algorithms using various measures of retrieval effectiveness. when tague-sutcliffe (in press) performed an analysis of variance on the average precision there is a large group of systems at the top of the ranking which are not significantly different. in addition the queries contribute more to the mean square than the systems. to gather further insight into the results, this research investigates the variation in query properties as a partial explanation for the variation in retrieval scores. for each topic statement for the queries, the length (number of content words), length of various parts and total number of relevant documents are correlated with the average precision."
"-the rule has label (v) as the premise (thus, the rules (⊓), (⊔), (h), (∃) are applicable only to simple nodes, and the rules ("
"the induction hypothesis follows from the inductive assumption with (x, z) replacing (x, y) and from the inductive assumption with (z, y) replacing (x, y)."
"for any rule of c shi except (f orming-state) and (conv), the distinguished formulas of the premise are called the principal formulas of the rule. the rules (f orming-state) and (conv) have no principal formulas. as usually, we assume that, for each rule of c shi described in table 1, the principal formulas are not members of the set x which appears in the premise of the rule."
"this work characterizes the topics to some extent, but there needs to some analysis of the relationship between the topics (or queries) and the many characteristics of the retrieval systems used in the various runs. the next step for the topics will be to cluster the queries according to the similarity of performance on the runs. the basic correlation matrix has been calculated and the range of similarities (pearson's correlation of r-precision) is -0.41 to +0.85 which shows that there are some very different patterns of performance of the topics over the runs. the resulting clusters will have to be explained by referring back to the characteristics of the topics. at the same time the clustering of the runs by the different systems will be carried out."
"a formula is defined to be either a concept or an abox assertion. we use letters like ϕ, ψ and ξ to denote formulas, and letters like x, y and γ to denote sets of formulas."
"an \"and-or\" graph for (r, t, a ′ ) is presented in figure 1 . in figures 2 and 3 we give an \"and-or\" graph for the knowledge base (r, t, a)."
"expanding a simple (resp. complex) state v of a tableau by the transitional rule (∃) (resp. (∃ ′ )), each successor w i of v is created due to a corresponding principal formula ∃r i .c i (resp. a i : ∃r i .c i ) of the rule, and rfmls(w) is set to the empty set."
"description logics represent the domain of interest in terms of concepts, individuals, and roles. a concept is interpreted as a set of individuals, while a role is interpreted as a binary relation among individuals. a knowledge base in a dl consists of axioms about roles (grouped into an rbox), terminology axioms (grouped into a tbox), and assertions about individuals (grouped into an abox). one of the basic inference problems in dls, which we denote by sat, is to check satisfiability of a knowledge base. other inference problems in dls are usually reducible to this problem. for example, the problem of checking consistency of a concept w.r.t. an rbox and a tbox (further denoted by cons) is linearly reducible to sat."
as a first step correlation of the number of relevant documents was correlated with word counts and average precision. the number of relevant documents for a topic is significantly correlated with the average precision in the routing exercise.
"-the kind of the rule: an \"and\"-rule or an \"or\"-rule -the conditions for applicability of the rule (if any) -the priority of the rule -the number of successors of a node resulting from applying the rule to it, and the way to compute their contents."
"it may be that the question for large retrieval systems tests should not be \"which retrieval system is the best?\" but \"which retrieval methods should be used for this query and this user on this database?\". currently users don't have a choice of retrieval method but can compensate by changing query formulation strategies in different situations."
"input: a knowledge base (r, t, a) in nnf in the logic shi. global data: a rooted graph (v, e, ν). apply(ρ, v); // defined on page 8"
"the rules (∃) and (∃ ′ ) are the only \"and\"-rules and the only transitional rules. the other rules of c shi are \"or\"-rules, which are also called static rules. the transitional rules are used to expand states of tableaux, while the static rules are used to expand non-states of tableaux."
"global data: a rooted graph (v, e, ν). purpose: applying the transitional rule ρ, which is (∃) or (∃ ′ ), to the state u."
"the analysis of variance table for average precision on the ad-hoc data shows that the mean square for the runs (systems) was 0.38 but for the queries was 0.94 (tague-sutcliffe, in press, table 1). both these values are statistically significant but the variation amongst the queries was much greater than amongst the runs. in addition if a post-hoc scheffé test is done to see which runs are significantly different, the top 20 runs form a group which are not significantly different from each other, with a range of average precision from 0.42262 to 0.2689. thus the retrieval tests do not differentiate between systems very well. it seems that some systems do very well on some queries but not on others. the difficult question to answer is \"what are the characteristics of the queries and retrieval methods which work well together?\". as a preliminary step this research investigates the query characteristics and how they are related to performance measures."
"one possible explanation is that for a topic with a very small number of relevant documents, any method which ranks retrieved documents will eventually have a low precision at high ranks. the number of relevant documents per topic was also slightly negatively correlated with the total number of words in the ad-hoc topic statements. the negative correlation means the longer the topic statement, the smaller the number of relevant documents, but the magnitude of the correlation is fairly small. this may be related to the phenomenon that saracevic (1988, p.211) reported when he says that the number of relevant documents is greater when the topic is of low clarity and specificity."
"the primary purpose of these tests was to show either which indexing methods or which retrieval methods perform the best. the results have usually not been conclusive because of the great number of variables involved and the difficulty of constructing a useful measurement of retrieval effectiveness. until recently, almost all these retrieval tests were done with relatively small document collections ranging from several hundred to tens of thousands. this still falls short of operational systems with millions of documents. [cit] ."
"the rule (conv) used for dealing with converses will be discussed shortly. the priorities of the rules of c shi are as follows (the bigger, the stronger):"
"where yij is the score for the ith participant on the jth query, µ is the overall mean score, ai is the effect of the ith run, bj is the effect of the jth query, eij is the random variation about the mean."
"our language uses a finite set c of concept names, a finite set r of role names, and a finite set i of individual names. we use letters like a and b for concept names, r and s for role names, and a and b for individual names. we refer to a and b also as atomic concepts, and to a and b as individuals."
"the nodes are numbered when created and are expanded using dfs (depth-first search). at the end the root receives status unsat. therefore, by theorem 3.4, (r, t, a) is unsatisfiable. as a consequence, (r, t, ∅) is also unsatisfiable."
"the size of closure(r, t, a) is polynomial in the size of (r, t, a), where the size of a set of formulas (resp. a knowledge base) is the sum of the lengths of its formulas (resp. formulas and axioms). note that, each tableau node is re-expanded at most once, by using the rule (conv). it is easy to see that g has size 2 o(n) and can be constructed in 2"
"we call trans r (x, r) the transfer of x through r w.r.t. r, call trans r (x, r, a) the transfer of x through r to a w.r.t. r, call trans r (y, a, r) the transfer of y starting from a through r w.r.t. r, and call"
"tague-sutcliffe analyzed the data from trec-3 with this question in mind. the fifty queries were considered as a random sample from the population of all queries, so that the results can be generalized to any similar sample of queries."
"secondly, the correlations of various word counts with the median (over runs) average precision was calculated (tables 2 and 3) . correlations with average rprecision were also calculated but significant correlations were almost all the same as for precision. this is to be expected because of the high correlation between all the output measures used (tague-sutcliffe in press). looking at the correlations with average precision, we see a significant positive correlation with the \"description\" part of the ad-hoc topics and a negative correlation with the \"title\" portion. the titles are generally only a few words long, so it is difficult to explain the correlation, but perhaps the simple topics with short titles are easier to search than long more complicated titles. the description part for the ad-hoc topics is generally only one or two sentences long and describes the type of document being sought. longer descriptions probably give a better description of the documents which results in better precision. the narrative often adds conditions for relevance to the basic description which often makes the task more complicated, so a longer narrative does not help precision. there were no signigicant correlations of word counts of the routing topics with average precision."
the rest of this paper is structured as follows: in section 2 we recall the notation and semantics of shi. in section 3 we present our tableau decision procedure for the sat problem in shi. in section 4 we give proofs for the correctness of our procedure and analyze its complexity. section 5 concludes this work.
"fourth, we use cubic spline interpolation to get a continuous function, which is an approximation of the discrete data. the pseudocode of this algorithm is described as in algorithm 1."
"the effect of suppression for adversary a controlling location . suppressing from t reduces the support of p a ðtþ by 1 (p a ðtþ is no longer supported by t). on the other hand, the support of p a ðt 0 þ is increased by 1. thus, we only recalculate problems for the pairs of projections p a ðtþ and p a ðt 0 þ. the effect for every other adversary b. as suppressing location from t only affects pair ð; p b ðtþþ, our computation simply recalculates the number of problems for this affected pair."
"if the final problems of trajectory t are resolved with suppression then mix suppresses location from t, otherwise it splits trajectory t at . the intuition behind this approach, is that splitting a trajectory t to t 0 and t 00, decreases the number of total problems, but also introduces two new trajectories t 0 and t 00 which may be problematic. on the other hand, suppressing from t results in a problems free trajectory."
"in addition to image ranking measures, we also evaluate concept ranking performance by mean reciprocal rank (mrr). for each image, we rank all its possible concepts by their predictions, then compute mean of the reciprocal ranks of the manually annotated concepts for this image and finally average them over all the images. this measures how well we can automatically identify or recommend relevant annotation concepts for images."
"the simulation results are shown in figure 9 . as shown in table 1, 2000 temperature measurements are obtained, and the interval is 0.5 ∘ c. figure 9 shows that there are two peaks: one peak is at 30 ∘ c and the other peak at 100 ∘ c indicates a local fire."
"in simple words, p t ð; t a þ is the fraction of trajectories in the support set (e.g., s t ðt a þ) that include . definition 9. a dataset t is called unsafe, if it has one or more problematic pairs; otherwise is called safe. the total number of problems of an unsafe dataset t is the sum of the number of problems of every problematic pair in t ."
"to predict the normal temperature in the subway transportation system, the association rules between the climate of the city and the normal temperature in the system are to be determined. a weather report provides the climate of city in a discrete way, for example, every 30 minutes or 60 minutes, and through the association rule, we can predict the normal temperature in the subway transportation system."
"estimating the utility of the anomymized dataset is challenging, since it highly depends on its intended use. thus to evaluate our methods, we use different utility metrics that capture trajectory changes but also measure the impact on count queries and frequent patterns. the latter two, are the basis for utility metrics in several related works [cit] ."
"with the rapid process of urbanization, more and more people live and work in big cities. as a result, public transportation systems are under great pressure, and subway transportation systems are good choices to help relieve this pressure. however, fire is a common and disastrous phenomenon in subway transportation systems, and great attention must be paid to guarantee the safety of the public in such systems. thus, the research on fire detection in subway transportation systems is very important."
"two reverse-inference meta-analyses were performed using the neurosynth website [cit] . neu- figure 3 . connectivity between network parcels and the hippocampus. a, for each parcel in the anterior and posterior scene networks, we computed its resting-state connectivity with the hippocampus, showing a striking increase in hippocampal activity for anterior network parcels overlapping with cipl, rsc, and appa (magenta circles) compared with posterior network parcels (blue circles). b, along the dorsal network boundary, hippocampal activity first dips slightly and then increases substantially, becoming strongest in the most anterior parcel intersecting cipl (and is also high in rsc). c, ventrally along parcels overlapping with ppa, we observe a similar increasing posterior-to-anterior gradient in connectivity. d, computing the connectivity between each coronal slice of the hippocampus and the two scene networks shows that this increased coupling to the anterior network is present throughout the hippocampus, but is especially pronounced in anterior hippocampus (mni coordinate y ͼ ϫ21 mm). error bars are 95% confidence intervals across subjects. ‫ء‬p ͻ 0.05, ‫‪p‬ءء‬ ͻ 0.01."
"the fire is very severe alarm and message the security department about the fire there is no fire do nothing the best consistency comparison and the large fire incident is used mainly to check the best squared comparison. for different fire, our experimental parameters are set as in table 1 . through our mechanism, the fire incidents will be predicted. if there are local fires in various places, we will locate them quickly and inform the security officers; if there is a large fire that is almost everywhere in the system, we will locate the places where the fire is very severe, and then we will inform the security officers."
"c onsider a company smartcard (e.g., a contactless card company, a bank) that issues electronic money cards (like the octopus or the ez-link card) or credit/debit cards (e.g., visa, mastercard) to its users, to allow cashless payments. assume that the company wants to publish sequences of transactions for analysis and querying purposes. we consider as a motivating example, the octopus (http://www.octopuscards.com/) smart rfid card, commonly used by hong kong residents to pay for their transportation and for their transactions at pos services (e.g., shops, vending machines). transactions created by the usage of cards in physical locations have a very important spatiotemporal aspect; they reveal that a person was at a certain place in a specific time. smartcard and similar companies accumulate vast amounts of transactional data that reveal user trajectories daily, and they are under pressure to publish or share them with third parties. such data can be used to extract movement and behavioral patterns as well as the causality relationships between geographic locations. a similar problem can arise in the context of location based social networks, where the user trajectory is the sequence of her check-ins in predefined locations."
"it has been seen that large-scale data can directly benefit visual concept detection [cit] . rather than designing more intelligent classification algorithms and robust image features, we can simply use more data. the acquisition of reliable annotations, nevertheless, is a labor intensive process. for each concept to be learnt, training examples have to be annotated manually by expert annotators making these annotations expensive and limited. [cit] dataset, for instance, requires collaborative annotation efforts from up to 47 research teams or organizations for 119,685 shots or keyframes with totally 130 concepts [cit] . such a tedious and costly manual labeling process will become extremely hard for the ultimate aim of annotating billions of images for thousands of visual concepts."
"in sections 4 and 5 we have described two anonymization methods that are based on suppression of locations. although these transformations produce safe datasets, they need to suppress an increasingly large percentage of the dataset's locations, as the trajectories grow in size. with this problem in mind, we developed a third method that keeps the distributions and the number of locations intact, by employing trajectory splitting."
"overall, the current state-of-the-art approaches in visual concept learning and annotation tasks are based on the \"bagof-words\" model obtained by clustering of sift-like features. within the \"bag-of-words\" representation, different point sampling strategies (e.g. keypoint detector or dense sampling), choices of descriptors (e.g. sift or surf) and visual word assignment (e.g. hard or soft assignment) have also been studied. specifically, salient point detectors, such as laplace-of-gaussian [cit] and harris-laplace [cit] based detectors, introduce robustness against viewpoint and illumination changes. [cit] showed that sampling on a regular dense grid in a uniform fashion consistently outperforms complex salient point methods in scene classification, since using more image patches means that more of the appearance of an image can be captured. however, salient points have the advantages of ignoring the homogenous areas in the image which is superior for object detection. sift [cit] and surf [cit] are two commonly used local feature descriptors. [cit] presented several improvements upon speeding up the calculation of densely sampled sift and surf descriptors for real-time classification."
"the specific functions of the individual components of this network have also been studied in a number of contexts. rsc appears to be most directly involved in orienting the viewer to the structure of the environment (both within and beyond the borders of the presented image) for the purpose of navigational planning; it encodes both absolute location and facing direction [cit], integrates across views presented in a panoramic sequence [cit], and shows strong familiarity effects [cit],b) . this is consistent with rodent neurophysiological studies, which have identified head direction cells in this region [cit] . rsc is not sensitive to low-level rectilinear features in nonscene images, such as objects or textures [cit], though it does show some preference for rectilinear features in images of 3d scenes [cit] ."
"there are numerous works on data anonymization and many of them focused on the anonymization of location data [cit] . in the following, we survey the recent development in the field and position our work against them."
insert into p every trajectory t 2 t that has problematic pairs 5 for every t 2 p do 6 for every 2 t do 7
"since the anterior scene network overlaps with default mode regions, while the posterior scene network does not, we predict that the anterior network should be more connected to the hippocampus [cit] . to test this hypothesis, we measured the functional correlation at rest between mean hippocampal activity and the mean activity in each parcel within the posterior and anterior scene networks. as shown in figure 3, there is a dramatic difference in hippocampal connectivity for parcels in the posterior network (overlapping with opa and posterior ppa) compared with the anterior network (overlapping with rsc, cipl, and anterior ppa). moving posterior to anterior along the dorsal path, hippocampal connectivity first decreases slightly (first parcel to second parcel: left: t (19) ϭ ϫ3.04, p ϭ 0.007; right: t (19) ϭ 2.15 p ͻ 0.04; two-tailed paired t test), then increases significantly when moving to the third parcel (left: t (19) ϭ 5.62, p ͻ 0.001; right: t (19) ϭ 3.79, p ϭ 0.001) and to the fourth parcel (left: t (19) ϭ 4.17, p ͻ 0.001; right: t (19) ϭ 5.74, p ͻ 0.001). along the ventral path, hippocampal connectivity jumps from the first to the second parcel overlapping with ppa (left: t (19) ϭ 5.27, p ͻ 0.001; right: t (19) ϭ 5.76, p ͻ 0.001) and from the second to the third parcel (right:"
"subprocess2: get g(t)-the ordinary distribution of the data on the condition that there is no fire in the urban railway transportation systems. the best squared comparison is a good measure for estimating the situation because this method accumulates all the differences between the two functions. on the contrary with the best consistency comparison, the shortcoming of the best squared comparison is that it cannot locate a fire, so these two comparisons will become complementary."
"pairs lost. our algorithms use ploss to heuristically measure the effect of a trajectory change. we calculate ploss on the dataset level, but taking into account the difference in the total number of pairs in the original and the anonymized dataset. unfortunately, ploss on the dataset level is not that informative, since it only reflects the sum of all pairs lost in the anonymization process, independently of their value for data analysis. note, for instance, that the removal of a very frequent pair would not affect the statistical distribution of the dataset as the removal of a less frequent pair. the removal of a very rare pair is frequently not important either, since it does not reveal a trend. to this end, we do not use ploss as a main evaluation metric, but for completeness, we report it in fig. 9a."
"in this section we investigate svms, boosting variants, as well as their importance weighted extensions for visual concept learning by use of social tags or manual annotations. first, we evaluate different variants of the boosting algorithm, and feature combination approaches integrated at each round of boosting procedures. second, an overall comparison of performance between svms, boosting variants and tag relevance learning methods is presented. third, we analyze the results of our importance weighted svms and boosting algorithms when learning visual concepts from weakly labeled social images."
"split has the advantage of publishing the same number of locations as the original dataset, but in some cases lsup solves problems with less information loss. to combine the advantages of both strategies, we developed algorithm mix, where its main difference to split algorithm is in step 13. in more details, at each iteration mix finds the best splitting location of a trajectory t as in step 12 of split. next, it calculates the problems of trajectory t after suppressing location ."
"summary of the evaluation. the experimental evaluation confirms that the performance of methods that rely solely on generalization and suppression greatly decreases as size of the trajectory grows (which is consistent with most anonymization methods). on the other hand, the relevant experiments in (figs. 6d, 7, and 8) show that split, manages to address this problem, and moreover that mix efficiently combines the best behavior of split and lsup."
"learning visual concepts from social images is a difficult and challenging problem. this is in large part due to the fact that user supplied tags are typically ambiguous, subjective and incomplete. we have two conclusions from this study. first, overall, the \"cost-sensitive\" and \"importance weighting\" approaches are promising and typically have top tier performance in our experiments. second, the performance measure does have a major impact on the comparative results. any single algorithm is unlikely to perform best for all performance measures. one grand challenge in the future will be designing algorithms which address the issues of tag ambiguity, subjectivity and incompleteness. another grand challenge is to design new social tagging learning methods to optimize different performance measures which arise due to the needs of different real life situations."
"our local anonymization method, illustrated in algorithm lsup, takes as input an unsafe dataset t and iteratively removes a location until dataset t becomes safe. lsup algorithm uses set p storing trajectories with problematic pairs. the algorithm also uses array locmaxgain that, for every trajectory t, stores at locmaxgainðtþ a pair ðloc; gainþ where gain is the maximum suppression effect gain for trajectory t, and loc the location corresponding to that gain."
"the connectivity results described thus far suggest a functional division for scene-related regions, with some belonging to a posterior network and others belonging to an anterior network. to assess the functional significance of these two networks, we ran two reverse-inference meta-analyses using the neurosynth tool [cit] . this system automatically extracts activation coordinates from many fmri studies (ͼ10,000 at the time of writing); given a particular set of studies, it can identify voxels that are more likely to be activated in this set of studies relative to the full set of studies. these voxels are therefore preferentially active in the query set compared with general fmri experiments. based on the areas involved, we hypothesized that the posterior network processes the current visual properties of the scene, whereas the anterior network incorporates episodic memories and contextual aspects of the scene. thus, in figure 4a, we compare meta-analyses for the query \"scene\" (47 studies) with the query \"episodic memory, navigation, past future\" (125 studies). along the parahippocampal gyrus, we find that the visual scene activations tend to be posterior to the memory activations, and that the transition point corresponds almost exactly to the division between our two networks. dorsally, we also observe a separation between the reverse inference maps, with scene and memory activations falling into our two separate networks. overall, voxels significant only in the scene meta-analysis were concentrated in the posterior network (66% in posterior network, 18% in anterior, 16% in other), while voxels significant only in the memory/navigation meta-analysis were spread more widely across the cortex, but were concentrated more in the anterior than the posterior network (16% posterior, 42% anterior, 42% other). voxels significant in both the scene and memory/navigation meta-analyses tended to fall near the border between the two networks and divided approximately equally among them (44% posterior, 53% anterior, 4% other). table 1 . anterior and posterior hippocampus connectivity to scene parcels."
"algorithm gsup (section 4) uses global suppression to unify two projections t a r and t a r . every location that needs to be suppressed for this unification is globally suppressed, i.e., all its appearances are removed. to further reduce information loss, we propose a local suppression approach, which suppresses a location from a single trajectory t at a time."
"after collecting the temperature data from the sensors deployed in the subway transportation system, we process the data to get their approximate distribution function, as described below."
"finally, in fig. 9a we present the algorithms performance with respect to pair loss (ploss), when we vary p br . we calculated dataset ploss based on the difference in total number of pairs in the original and the anonymized dataset. lsup appears as the algorithm which preserves ploss better, since ploss penalizes splitting more than suppression. at the same time, as the previous experiments show, lsup provides less accurate answers to count queries and preserves less frequent itemsets."
"the specific properties of anterior ppa have been less well studied, since it was not recognized as a separate region within the ppa until recently. it has been shown to be driven more by high-level category information than by spatial frequency content [cit], to represent real-world locations (even from perceptually distinct views; [cit] ), to encode object cooccurrences [cit], and to represent real-world physical scene size [cit] . its representation of scene spaciousness draws on prior knowledge about the typical size of different scene categories, since it is affected by the presence of diagnostic objects [cit] ."
"in the case of visual concept learning using approaches from the research literature on the unsupervised social imaging test set, three different algorithms performed best for three different performance measures. specifically, the uniform, rank and svm methods performed best for the performance measures ap, bep and mrr, respectively. no single research literature approach had the best performance for all accuracy measures."
"to our knowledge, the association rule between the climate and temperature in the subway transportation system can be mined by the technique of data mining, and then we can get the normal temperature, but it is not the core algorithm in this paper. therefore, normal temperature in beijing subway transportation system is given and is a set of discrete data at different times on a day as shown in figure 5 ."
"note added in proof -minor revisions were made to the version that was published on-line october 10, 2016, as an early release, including adjustments to the labeling of figures 2 and 3, and small wording changes in the abstract and materials and methods."
"compare ( ) and ( ) to find whether there is fire. we have got ( ), ( ) and we will compare them through the two comparisons to predict whether there is a fire. if there is no fire in the subway transportation system, ( ) and ( ) are almost all the same; if there is a local fire, ( ) will be a little different from ( ), for example, ( ) has two peaks and ( ) has one peak and in this situation, the best consistency comparision is a good method; if the fire is occurring almost everywhere of the system, ( ) will be very different from ( ) and in this situation, we cannot only focus our attention on the places where the fire is the most severe but also on the overall conditions. the flow to judge whether there is a fire is shown in figure 8 ."
"support vector machine algorithm constructs a hyperplane or set of hyperplanes in a high-dimensional space, which can be used for classification and regression analysis. it is a representation of the examples as points in space, mapped so that a good separation is achieved by the hyperplane that has the largest margin between the training data points of different classes, since in general the larger the margin the lower the generalization error of the classifier. let"
"where x is the input feature vector and t is the number of boosting rounds. h t (x) denotes a weak learner at each round t, and h (x) is the final strong classifier learner. thereby, they derive a \"gentler\" version called gentleboost, which differs from realboost in that it takes adaptive newton stepping rather than exact optimization at each stage and tends to put less weight on the outlier data points. in order to merge multiple visual features into our concept learning system, a feature combinationprocedure is introduced to be integrated at each round of these three boosting variants. the traditional boosting produces only one component weak classifier at each iteration. by contrast, at each round of our extension of the boosting procedures, several weak classifiers are trained on samples of each feature, and then combined into a single one (a middle final classifier) [cit] :"
"a straightforward idea to feed example importance weights to boosting procedures is to modify the initial boosting weights so as to break the importance symmetry. however, boosting re-updates all the weights at each iteration which may quickly destroy the initial asymmetry, and the predictor obtained after convergence usually makes little difference from that produced with symmetric initial conditions. another natural heuristic is to modify the way of updating weights in the boosting procedures. most of the previously proposed approaches [cit] attempt to address this problem in adaboost, achieving cost-sensitivity by manipulation of its re-weighting mechanism and confidence parameters. adacost [cit], for instance, introduces a cost adjustment function into weight updating rule of adaboost, aiming to increase the weight of a training example with higher importance \"more\" if it is misclassified, but decrease its weights \"less\" if otherwise. however, the selection of the cost adjustment factor in adacost is ad-hoc and may easily induce poor performance [cit] suggested a justified inference of weight updating parameter to maintain the boosting efficiency in reducing the weighted training error, while integrating the misclassification cost into the weight updating formula. our importance weighted extensions of adaboost are implemented using adac2 and adac3 algorithms [cit], which respectively feed the importance weights to the weight updating rule of eqs. (28) and (28) at each round:"
"it is evident that there are two peaks of the data collected by the sensors in the subway transportation system. the first peak is at 30 ∘ c and the second one is at about 200 ∘ c. obviously, it is unusual for the second peak which is much higher than the first peak, and this indicates that the fire is large."
"the mechanism proposed in this paper is an alternative of the mechanisms based on data fusion. compared with other mechanisms, our mechanism can detect fire incidents all around and the result is more accurate. fire incidents can be sensed, located, and monitored and the situation of the fire incidents can be estimated. therefore, the mechanism proposed in this paper is more valuable."
"any location may act at the same time as a sensitive location for an adversary a and as a quasi identifier for some other adversary (section 2.2). thus, suppressing a location from a single trajectory t (unlike global suppression of a location), does not guarantee a reduction in the privacy threats. to evaluate the effect of suppressing location in trajectory t of a dataset t, we introduce the suppression effect gain, denoted by p gain ð; tþ that is defined as"
"for instance, for the original dataset of fig. 4a the naive method would have resulted in the dataset of fig. 4b . although this dataset is safe, it does not withstand the whereabouts of users. instead, we propose a method that measures the effects of each trajectory split and selects splits that solve anonymity problems and, at the same time, minimize trajectory changes."
"to determine how these local parcels are organized into distributed networks, we performed hierarchical clustering to group together parcels with high functional connectivity (regardless of their spatial position). these networks are remarkably similar between hemispheres (despite not being constrained to be symmetric), as shown in the 10-network clustering in figure 1 ."
"natural scene perception has been shown to rely upon a distributed set of cortical regions, including the parahippocampal place area (ppa; [cit] ), retrosplenial complex (rsc; [cit] ), and the occipital place area [opa; also called the transverse occipital sulcus (tos); [cit] ]. more recent work has suggested that the picture is even more complicated, with multiple subdivisions within ppa and the possible involvement of the parietal lobe . although there has been substantial progress in understanding the functional properties of each of these regions and the differences between them, the field has lacked a coherent framework for summarizing the overall architecture of the human scene-processing system."
"average locations appearance ratio. the first metric measures the level of suppression performed by each method. we calculate the change in the number of appearances of locations in the original and the anonymized dataset. for every location l, we define the locations appearance ratio rðlþ, as the ratio of the appearance of l in the original dataset t to the appearance of l in the anonymized counterpart t 0 . the values of rðlþ are in ½0; 1; a value of 0 means that l was completely suppressed in the anonomymized dataset while a value of 1 means that l was unaffected."
"based on these results, as well as a review of previous work, we propose that scene processing is fundamentally divided into two collaborating but distinct networks, with one focused on the visual features of a scene image and the other related to contextual retrieval and navigation. under this framework, scene perception is less the function of a unified set of distributed neural machinery and more of \"an ongoing dialogue between the material and symbolic aspects of the past and the continuously unfolding present\" [cit] ."
"ventral for each scene-network parcel (as labeled in fig. 3), the group-level connectivity (fisher-transformed pearson correlation) was calculated separately for posterior and anterior hippocampus. parcels in the posterior scene network have lower hippocampal connectivity than those in the anterior scene network, especially for the anterior hippocampus. ahipp, anterior hippocampus; asn, anterior scene network; l, left; phipp, posterior hippocampus; psn, posterior scene network; r, right."
"since one-against-all strategy is performed to reduce our multi-class classification problem into multiple binary problems, two tag relevance-based importance weighting schemes are proposed, namely per-concept weighting and per-image weighting, concentrating on the binary distinction of positive versus negative. in general, for a given concept, higher relevance value leads to a higher importance weight in the training process. and we assume that all tag relevance values are normalized into (0, 1). in per-concept weighting scheme, for each annotation concept, we first learn the visual relevance of this concept with respect to all the training images even if it is not present in the user-contributed tags of an image. then, to solve the binary classification problem of a target concept, all the images labeled with this concept are trained as positive examples and take importance weights that equal to their tag relevance value, while images not labeled with this concept, as negative examples, take importance weights according to (1 − t ag relevance). on the other hand, for each training image, we only learn the relevance of all its userprovided tags in per-image weighting scheme. and then their tag relevance and importance weights are equivalently used regardless of an image being trained as positive or negative example in a binary classification problem of a given concept."
"the best consistency comparison is very sensitive to the big differences in a point and it is quite good for locating the fire. but the shortcoming is that this method cannot provide any information about the fire situation. to offset the shorting of the best consistency comparison, we propose another rule for comparison in section 3.3."
"arel. the average relative error (arel) measure [cit] estimates the average number of trajectories that are retrieved incorrectly due to anonymization, as part of the answers to a workload of count queries. lower arel scores indicate that the anonymized data estimate more accurately the number of co-occurrences of locations. arel is used in many anonymity methods [cit] . to measure the arel, we issue a query workload on the original and the anonymous datasets. our query workload selects the 200 most frequent ordered subsequences. such queries are of the following form:"
"in addition to boosting algorithms, we also use svms to learn separate classifiers for each concept by one-against-all strategy. in order to rank the concepts for a given image we need to compare the output scores of different svm classifiers."
"(1) difference of properties in sensor measurement between the sensors is not considered. we assume that all the sensors have the same properties, such as the same measuring range and the same measuring error. in this paper, we assume that each sensor used can measure temperature up to 1000 ∘ c and the errors in the measurements are presented in the distribution of the data measured by the sensors. (2) the temperature around a fire is all the same. in fact, the temperature decreases with the increase of the distance between the fire and the sensors. in this case, it is very difficult to simulate. therefore, in this paper, we ignore the influence of this phenomenon. (3) the temperature of the subway transportation system is all the same. this hypothesis is reasonable because for a subway transportation system, the environment of a subway is almost all the same. if the subway transportation system is too big that the temperature is different between different parts, we can divide the subway transportation system into several parts and it is feasible that the environment of each part is almost all the same. we can use our mechanism in each part. (4) to guarantee a good result, we assume that the number of sensors is large enough and the sensors are deployed uniformly in the subway transportation system. in our experiment, we assume that for a local fire, there are 200 sensors that can defect the fire, and for a large fire, there are about 1500 sensors that can defect the fire."
"the 172 ϫ 172 parcel functional connectivity matrix was converted into a distance matrix by subtracting every entry from the maximum entry. hierarchical ward clustering (unconstrained by parcel position) was applied to the distance matrix to compute a hard clustering into 10 networks. after identifying the 16 parcels (8 per hemisphere) overlapping with scene-related regions, we computed a similar distance matrix for these parcels (subtracting every entry from the maximum entry) and applied classical multidimensional scaling to yield a twodimensional visualization of its structure."
"we found that some semantic concepts remain difficult to learn in our experiments. regarding unsupervised visual concept learning, it was found in the experiments that classes such as tiger and airplane had low average precision across all the machine learning algorithms. from studying the manual visual concept learning results, it appears that a significant reason is the noise in the social training tags."
"input: ( ) is the distribution, is the normal temperature, 1 and 2 are probability thresholds, 1 and 2 are the temperature thresholds, limite is the limit value of output: the new period (1) take the derivation of ( ) and get ( )."
"where n w is the number of training images tagged with concept w, and n denotes the size of the entire training set. in general, the more neighboring images annotated with the target concept, the larger the tag relevance value would be. in the meanwhile, tags with high frequency are penalized for their high prior probabilities. as a result, we obtain the unsupervised tag relevance learning model using multiple features as follows [cit] :"
"for the following experiments, we also use these three tag relevance learning methods for comparisons with other visual concept learning algorithms, and the parameter of k neighbors is always set optimally."
"in this paper, we have proposed a novel mechanism based on wsns for fire detection in subway transportation systems. for different types of fire incidents, it is obvious that we focus our attention on different aspects of them. for a local fire, it is important to locate the places where the fire happens quickly and efficiently because it is the best result to put out the fire when it is small, and for a large fire incident, it is important to estimate the situation as accurate as possible because it is impossible to extinguish a big fire immediately. for different order of severity we should take different measures. through the two comparisons we proposed in our paper, we can get a satisfied result. the simulation shows that the performance of the mechanism described in this paper is satisfactory, and we can detect almost all the situations associated with a fire efficiently and quickly."
"an important reason we opted for a syntactical anonymity approach is the sparse multidimensional nature of our data, which forces methods based on differential privacy to add substantial noise to the result. at the same time differential privacy is not immune to attacks [cit] . the comparison with ngrams backs up our decision, since it shows the superiority of our approach in terms of utility. almost all algorithms in all settings outperform the ngrams algorithm for all utility metrics. ngrams adds significant noise; as shown in fig. 11a, it doubles the points in the dataset. ngrams's results, in terms of arel and preserved frequent itemsets, are also inferior to all proposed algorithms. this behavior is expected, since differential privacy cannot easily preserve accurate counts when subtrajectories are considered. the sparse multidimensional nature of the data results to low counts for subtrajectories and ngrams adds substantial noise to preserve the privacy guaranty."
"compared with the exponential influence of the term y h(x) which is associated with the generalization error, the importance weight c has much less effect on the cost function (34) as a linear factor. therefore, a second heuristic idea is to formulate it inside the exponent of the cost function: (38) now the second order taylor approximation we want to optimize is defined as follows:"
"our approach is a novel mechanism for monitoring fires, and it is based mainly on statistical data and the process is different from the approaches used in previous methods. in order to illustrate the mechanism easily, we assume that there are only temperature sensors present. the occurrence of fire is judged by combining the distribution of the data collected and the normal distribution of the ordinary data when there is no fire. if the data show significant differences, it is very likely that there is a fire."
"we can visualize the connectivity differences among the parcels overlapping with scene-related regions using classic multidimensional scaling (fig. 2a), which shows that the network clustering captures the primary dimension of variance in connectivity properties, separating the most posterior parcels overlapping opa and pppa from the most anterior parcels overlapping cipl, rsc, and appa. to evaluate the reliability of this shift in connectivity properties within individual subjects, we measured the functional connectivity between these parcels and a reference parcel in the anterior network. we selected the reference parcel to be on the opposite side of the cortical surface (in order to avoid influences from local noise correlations) and to be as far anterior as possible; for dorsal parcels on the lateral surface (overlapping opa and cipl), the reference parcel overlapped rsc on the medial surface; and for ventral parcels on the medial surface (overlapping ppa), the reference parcel overlapped cipl on the lateral surface. in both cases, we observed rapid increases in connectivity as we moved posterior to anterior across the network boundaries (fig. 2b,c) . along the dorsal boundary, we see significant increases in connectivity to the rsc parcel when moving from the first to the second parcel (left: t (19) ϭ 6.98, p ͻ 0.001; right: t (19) ϭ 6.35, p ͻ 0.001; two-tailed paired t test), from the second to the third parcel (left: t (19) ϭ 7.72, p ͻ 0.001; right: t (19) ϭ 6.16, p ͻ 0.001), and from the third to the fourth parcel (right: t (19) ϭ 2.44, p ϭ 0.025). we observe a similar significant (though less dramatic) increase in connectivity to the cipl parcel when moving from the first to the second ppa parcel (left: t (19) ϭ 4.21, p ͻ 0.001; right: t (19) ϭ 2.68, p ϭ 0.015) and from the second to the third ppa parcel (right: t (19) ϭ 3.03, p ϭ 0.007)."
"2010; [cit] (component c10 of ) ]. it comprises a subset of the broader default mode regions, but functional and anatomical evidence suggests that it is a distinct, coherent subnetwork [cit] ) . the broad set of tasks that recruit this network have been summarized in various ways, such as \"scene construction\" [cit], \"mnemonic scene construction\" [cit], \"long-timescale integration\", or \"relational processing\" [cit] . a review of memory studies referred to this network as the posterior medial memory system, and proposed that it is involved in any task requiring \"situation models\" relating entities, actions, and outcomes [cit] ."
"when there are two continuous functions, for example, ( ) and ( ), with the domain of definition of [, ], we define the best squared comparison as follows:"
"the visual network shows a close correspondence with the full set of retinotopic maps identified in previous studies [cit] . previous measurements in individual subjects have also shown strong overlap between opa and retinotopic maps, especially v3b and lo2 [cit], and between pppa and vo2, phc1, and phc2 [cit] ). the only portion of cortex with known retinotopic maps that is not clustered in this network is the shared foveal representation of early visual areas, which segregates into its own cluster, which is consistent with other work showing a peripheral eccentricity bias in the scene network [cit] ."
"this cost difference controls the importance of correct classification and just vary on an example-by-example basis. then given a set of examples with the form (x, y, c), we aim to find a classifier h achieving the minimal importance weighted misclassification error:"
"as shown in figure 2, the peaks of δ( ( ), ( )) are recorded as (−0.2, 0.128) and (6.1, 0.116). we assume that the normal temperature is inferred around 0 ∘ c. therefore, the peak (−0.2, 0.128) is normal and the peak (6.1, 0.116) attracts our attention. the sensors that detect a temperature around 6.1 ∘ c will be selected in the database and the sensor positions can be located by location system in wsns."
"on the other hand, with the popularity of social media, there are increasingly large amounts of images and videos available on the web. for example, flickr now hosts over 5 billion images with roughly 10 million new uploaded photos daily [cit] and youtube serves close to 3 billion video views per day with 48 h of video uploaded every minute [cit] . apart from these rich multimedia databases, images and videos on the social networks are often accompanied by various forms of metadata like tags, ratings, comments and exif information."
"as shown in figure 4, subprocess4 is the core of our mechanism by which we can judge whether there is a fire. subprocess1 and subprocess2 are the basement of subpro-cess4 through which we can get ( ) and ( ). the period we collect data in is important for real-time of our mechanism; the smaller the period is, the more real-time the data is. in the normal situation, we need not to collect the data very frequent, but if there are some indications of fire incident, we should collect the data more real-timely which means we should shorten the period. in subprocess3, we described the method to control the period."
"ad-hoc adversaries and colluding adversaries. we assume that the background knowledge of the adversaries is known to the publisher. however, in some cases an adversary can hold an arbitrary subset of each trajectory. for example, a person physically followed by an adversary a can reveal to a a part of her trajectory, regardless the ownership of the pos services the person uses. the proposed method does not protect against such ad-hoc adversaries. similarly, the proposed method does not provide a privacy guarantee against colluding adversaries since it does not take into account their combined background knowledge. in order to apply our approach in the case of such collusions, the publisher should treat each disjoint group of colluding adversaries as a single adversary by merging all their data. moreover, in the motivating setting, where card companies are data publishers and adversaries are commercial companies, collusion requires sharing of companies' customer data, which is not a common practice."
"where n and n 0 are the total number of problems before and after the suppression of location, respectively, and plossðt; t 0 þ is the pairs lost metric (equation (2)). negative values of p gain denote that suppressions increase anonymity threats. in general, greater values of p gain indicate better suppression choices. more specifically, for a given trajectory t (where n is fixed), p gain ð; tþ is maximized for the location, which when suppressed from t, solves the larger number of problems, i.e., when n 0 is minimized. when we compare different trajectories, p gain ð; tþ takes into account the number of initial problems n and the pairs lost metric ploss, promoting trajectories t having high n and low ploss. promoting low ploss values is a natural choice; we also promote trajectories with many problems n since they are more likely to change in order to fulfill the anonymity guaranty than trajectories with fewer problems."
"initially, to evaluate the effect of splitting a trajectory t at location of a dataset t with respect to the anonymity gain, we introduce the splitting effect gain (s gain ð; tþ) defined as"
"by combining a variety of data sources, we have shown converging evidence for a functional division of scene-processing regions into two separate networks (summarized in fig. 5 ). the posterior visual network covers retintopically organized regions, including opa and pppa, while an anterior memory-related network connects cipl, rsc, and appa. this division emerges from a purely data-driven network clustering, suggesting that this is a core organizing principle of the visual system."
"the remaining sections are organized as follows. section 2 reviews some related works on visual concept learning and social image analysis. in sect. 3, we introduce and discuss the traditional svm and boosting algorithms for visual concept learning using multiple image features. a visual neighbor voting model to exploit the tag relevance of social images is presented in sect. 4. section 5 describes the cost-sensitive learning problem and introduces the importance weighted extensions of svm and boosting classifiers instead of directly learning visual concepts from weakly labeled social images. the experimental setup is described in sect. 6 and the comparative results are presented in sect. 7. finally, we give conclusions in sect. 8."
"in this paper, we review and empirically compare methods of learning visual concepts from social images. first, we investigate two dominant algorithms: support vector machine (svm) and boosting, using multiple image features for visual concept learning. in particular, a common feature combination procedure is proposed to be integrated into different variants of the boosting algorithm. second, to analyze social tagging, we discuss a visual neighbor voting model to learn the visual relevance of tags with respect to the image content."
we discuss a heuristic function to estimate the information loss in section 4 and we present a series of information loss metrics in section 9.
"incorporating the importance weights into svm classifiers gives the best performance. and the largest improvement made in terms of ap, bep and mrr score is 2.5, 1.7 and 0.9 %, respectively. a limitation of the importance weighted classification is that for visual concepts that have large intraclass variations, it may fail to learn the example images with relatively rare visual appearance, since these examples probably have less visual neighbors in the training dataset, thus have smaller importance weights. as a result, the semantic concepts that are hard to learn due to intra-class variations will become harder to learn. table 5 lists the performance in terms of ap for all 20 annotation concepts in our evaluation dataset. it reveals that only around 52 % of the user-supplied annotation concepts are truly related to the visual content of the training images. in general, concepts with higher user tagging accuracy achieve higher ap scores. for example, the most precisely userlabeled concept \"flower\" yields a higher score than the others when training with social tags, and the concept \"lion\" obtains a significant improvement when using manual annotations. however, some concepts can still perform well even with bad tagging accuracy, such as \"kitchen\" and \"classroom\". on the other hand, there is no obvious rise in terms of ap score for semantic concepts, such as \"boat\", even though when learning from manually annotated images. and similar observations can be made on the performance in terms of bep and mrr which are not given here."
"applying the technologies introduced in section 3, this paper proposes a new fire detection mechanism as shown in figure 4 briefly to predict the fire perfectly. to be specific, the implementation details of each subprocess are expounded in sections 4.1-4.5."
"a unification between projections of the same adversary may also solve problems that other adversaries have. from the point of view of another adversary b, the appearances of points that do not belong to the unified projection are reduced. subsequently, the confidence of b that a trajectory includes any of these points is also reduced. for example, when unifying a 3 ! a 1 with a 3 in fig. 1, a 3 is removed from t 5 to t 7, and at the same time, a 3 appears fewer times in the trajectories where projection b 1 of adversary b is mapped to. to this end, we re-assess (line 11) the existing problems after performing the unification and, while there are more breaches, we repeat the unification process."
"svm is commonly regarded as a competitive choice for classification. and many state-of-the-art visual concept detection systems achieved their best results using svm classifiers with χ 2 [cit] . recently, multiple kernel learning has been a topic of interest which associates image features with kernel functions and jointly learn the optimal combination of the kernels [cit] . in this paper, we combine several kernels of multiple features into a single model by averaging their values. the rbf-based kernel function is used to measure the similarity between two images:"
"the probability threshold p br (default value 50 percent). the number of adversaries jaj (default value 4). we have examined two models for the adveraries. in the oldenburg adversaries are equal; all locations of the original dataset have been uniformly distributed to the adversaries. in the case of gowalla, we partitioned all locations of the original dataset to adversaries, according to skewed, zipfian distribution."
"we found that these scene rois fell almost entirely onto two of the connectivity networks. a posterior network (dark blue), overlapping opa and posterior ppa (pppa), covered all of visual cortex outside of an early foveal cluster. an anterior network (magenta), overlapping cipl, rsc, and anterior ppa, covered a parietal/medialtemporal network that includes anterior temporal and orbitofrontal parcels. this corresponds to a portion of known default mode regions, with other default mode regions being grouped into a separate network (green); a similar fractionation of the default mode has been proposed previously [cit] . within the ppa, this anterior/posterior split occurred at approximately mni coordinate y ϭ ϫ42 mm, with both segments of the ppa falling largely in the collateral sulcus and extending onto the parahippocampal gyrus."
"the functional distinction between pppa and opa is currently unclear. previous work has speculated about the purpose of the apparent ventral and dorsal \"duplication\" of regions sensitive to large landmarks, proposing that it may be related to different output goals (e.g., action planning in opa, object recognition in pppa; [cit] ), or to different input connections (e.g., lower visual field processing in opa, upper visual field processing in pppa; [cit] . opa and pppa may also use information from different visual eccentricities, with opa processing less peripheral, relatively high-resolution environmental features and pppa processing more peripheral, large-scale geometry, and context [cit] ) ."
"where r m is the tag relevance learner trained using feature m. note that function (15) does not necessarily obtain positive results, so in practice we set the minimum value ϕ, a very small constant (e.g. 10 −5 ), to avoid negative results in our experiments."
"the importance weights given here, thereby, are more like benefits than costs, since larger costs will be mapped to smaller weights. however, as we adopt one-against-all strategy to solve multi-class classification problem using binary classifiers, in the following study we will focus on two-class cost-sensitive learning in which there is only one importance weight per example. how to further formulate this problem when the output space is out of binary is our future work and beyond the scope of this paper. below, we will make an attempt to incorporate these importance weights into svm and boosting classifier learning process, rather than employing resampling techniques, though it is more general and can be applied to arbitrary classifier learners."
"definition 10. given a dataset t of trajectories, a user defined probability threshold p br and a set of adversaries a, construct a safe dataset corresponding to t, with minimum information loss."
"the majority of the data used in this study were obtained from the human connectome project (hcp), which provides detailed documentation on the experimental and acquisition parameters for these datasets [cit] . we provide an overview of these datasets below."
"the proposed approach tackles the threats posed by adversaries who own a large portion of the published dataset, but at the same time they are known to the publisher. this is a practical scenario, but it also has limitations."
"frequent sequential patterns published. we also evaluate the performance of our methods in sequential pattern mining. we use prefixspan [cit], to obtain the frequent sequential patterns (i.e., the sequential patterns having support larger than a threshold) in the original and the anonymous versions of the dataset. then, we calculate the percentage of the frequent sequential patterns of the original dataset that are preserved in the anonymous counterpart."
"we have explored two dominant classification paradigms, namely, svm and boosting, for visual concept learning. in our experiments, we considered both the use of social tags and manual annotations of training images to evaluate the proposed methods. the results show that the visual neighbor voting model works well for image ranking when learning from user-tagged images, while svm classifiers perform best using manual annotations. visual neighbor voting using rankbased weights and gentleboost classification also achieve top tier performance relative to other variants of the tag relevance learning model or the boosting algorithm. note that a limiting aspect of our work is that there are many diverse parameters in each approach. it would not be surprising that any single approach can be optimized further and this would logically have an effect on the quantitative performance. indeed, for a given concept, relevant images have to be emphasized more in the training process than irrelevant images. therefore we introduced an importance weighted extension to incorporate the example-dependent importance weights into svm and boosting classifiers. experimental results demonstrate that the importance weighted approaches are competitive with the state of the art approaches."
"to identify ppa, rsc, and opa, we deconvolved the localizer data from the 24 localizer subjects using the standard block hemodynamic model in afni [cit], with faces, scenes, objects, and scrambled objects as regressors. the scenes ͼ objects t statistic was used to define ppa (top 300 voxels near the parahippocampal gyrus), rsc (top 200 voxels near retrosplenial cortex), and opa (top 200 voxels near the transverse occipital sulcus), with mask sizes chosen conservatively based on typical roi volumes [cit] ). the roi masks were then transformed to mni space, summed across all subjects, and mapped to the closest vertices on the group cortical surface. the group-level roi was then manually annotated as the cluster of highest overlap between the subject roi masks. these rois are consistent with typical definitions in the literature [cit] ."
"the 468-subject eigenmaps distributed by the hcp are approximately equal to performing a singular value decomposition on the concatenated time courses of all 468 subjects, and then retaining the right singular values scaled by their eigenvalues [cit] . this allows us to treat these eigenmaps as pseudo-time courses, since dot products (and thus pearson correlations) between eigenmaps approximate the dot products between the original voxel time courses. we generated a voxel-level functional connectivity matrix by correlating figure 1 . connectivity clustering of cortical parcels. the cortex was first grouped into 172 local parcels (black lines), such that the surface vertices in each parcel had similar connectivity properties. performing a second-level hierarchical clustering on these parcels identified distributed networks of strongly connected parcels (parcel colors denote their network membership). scene-related regions of interest (identified using standard scene localizers in a separate group of subjects) are split across two networks, which are largely symmetric across left (top row) and right (bottom row) hemispheres. opa and posterior ppa overlap with a posterior network (dark blue) that covers all of visual cortex outside the foveal confluence, while cipl, rsc, and appa overlap with an anterior network (magenta) that covers much of the default mode network."
"tirely different large-scale dataset, and shows that there is a strong connection between connectivity changes in ppa and the boundaries of retinotopic field maps. there is now a growing literature on anterior versus posterior ppa, including not only connectivity differences [cit] but also the response to lowlevel [cit],b; [cit] ) and high-level [cit] scene features, as well as stimulation studies [cit] . our results place this division into a larger context, and demonstrate that the connectivity differences within ppa are not just an isolated property of this region but a general organizing principle for scene-processing regions."
"splitting of long trajectories. splitting is a novel data transformation operator. record partitioning strategies have only rarely been employed in data anonymization literature, so there is not a large related literature to assess their impact on large records and large frequent patterns. in our setting, splitting helps the anonymization algorithm to better preserve the original data, as we experimentally demonstrate in section 9. still, it can cause the omission of long trajectories in the anonymized data. this effect cannot be denied in theory, but in practice, splitting helps preserve more frequent patterns than algorithms relying only on suppression. this happens because long trajectories are often rarer and they allow the identification of a person. in such cases, the the anonymization algorithm has to eliminate the trajectory, independently of the data transforation operator that will be employed. if only suppression is available, then a large part of long trajectories is completely suppressed, whereas splitting allows to preserve parts of them unaffected. moreover, large patterns are often a lot smaller than long trajectories in a given dataset, e.g., if we have trajectories with an average of 10 points a large pattern could have 5-6 points in practice. in these cases, splitting does not necessarily destroy the large patterns of a trajectory, since they are preserved in one of the sub-trajectories that are created. this is experimentally supported by the superior preservation of frequent sub-trajectories, as shown in fig. 8 ."
"to examine the mechanism for fire detection in subway transportation systems, we set two different situations of fire incidents and they can check the mechanism in different aspects. the local fire incident is used mainly to examine compare f(t) with g(t) through the best square to estimate the situation f(t) and g(t)"
"as shown in tables 3a and 4, the cost-sensitive extensions of adaboost, i.e. adac2 and adac3, have very poor performance in terms of mrr, while they make some improvements in ap or bep in comparison to classic adaboost without using importance weights. the importance weighted gentleboost works much better than them. in particular, compared with original gentleboost, its mrr score increases by up to 1.1 %, which is hard to achieve for gentleboost even using manual annotations. note that there was not a comparison on gentleboost using cost-sensitive versus importance weighting because we were not aware of a cost-sensitive gentleboost in the research literature."
"let a be the set of adversaries, t a be the subset of the trajectories in t owned by an adversary a, t be a trajectory in t a, and n be the number of anonymity problems. algorithm threatid iterates over all locations of all trajectories of all adversaries. this results in an overall complexity oðjaj á jt a j á jtjþ time. the most expensive loop of algorithm gsup is done in lines 3-6. in the worst case, gsup iterates over all anonymity problems n, over all adversaries in a and over all pairs of trajectories in t a to compute p gain (that requires scanning t a ). in total, gsup requires oðná jaj á jt a j 3 þ time."
"given a parcellation, we computed the group-level connectivity between a pair of regions by taking the mean over all eigenmaps in each region, then correlating these mean eigenmaps (which, as described above, can be treated as pseudo-time courses) and applying the fisher z-transform (hyperbolic arctangent). we computed subject-level connectivity in the same way, using the resting-state time course for each voxel rather than the eigenmap."
"moreover, [cit] also pointed out that largescale training data can directly benefit visual concept learning. rather than designing more intelligent classification algorithms and robust image features, we can simply use more data. however, manually annotated image collections are usually size-limited due to the labor intensive process of manual labeling."
"the cipl (also referred to as posterior ipl, pgp, or the angular gyrus) has been proposed as a \"cross-modal hub\" [cit] that connects visual information with other sensory modalities as well as knowledge of the past. it is more intimately associated with visual cortex than most lateral parietal regions, since it has strong anatomical connections to higher-level visual regions in humans and macaques [cit], and has a neurotransmitter receptor distribution similar to v3v and is distinct from the rest of the ipl [cit] . it has been mostly ignored in the scene perception literature, primarily because it is not strongly responsive to standard scene localizers that show sequences of unfamiliar and unrelated scene images. for example, a study showing familiarity effects in cipl described this location only as \"near tos\" [cit] b) . the cipl appears commonly, however, in studies involving personally familiar places, which are associated with a wealth of memory, context, and navigational information. it is involved in memory for visual scene images [cit], learning navigational routes [cit], and even imagining past events or future events in familiar places [cit] . it can integrate information across space [cit] and time [cit], and has been shown in lesion studies to be critical for orientation and navigation [cit] . our connectivity results and metaanalysis suggest that cipl may play a prominent role in connecting visual scenes to the real-world location they depict."
1. uniform weighting: π i j is equally weighted for all the visual neighbors. 2. distance-based weighting: π i j is weighted according to the measure of distance in the feature space between image x i and neighboring image x j .
"in this paper, wsns are used in subway transportation systems to monitor fire. the sensors deployed can collect data about temperature, and then we store the data in the database. fire incidents can be predicted by comparing the distribution of the data collected and the normal distribution of the data on the condition when there is no fire. there are two rules that will be followed when comparing the distributionsthe rule of the best consistency comparison and the rule of the best squared comparison. in this paper, these two rules are the core of the mechanism for fire detection. the best consistency comparison is a good method for fire locating, but it cannot provide any indication of the situation of the fire. conversely, the best squared comparison cannot locate a fire, but it is a perfect method for estimating the severity of the situation, because this method has accumulated all the differences of these two functions rather than just the large differences. therefore, the two methods have their own advantages and shortcomings and fire can be detected more accurately and efficiently by making use of the strengths of these two methods."
"third, for each segment, the number of the data that fall into this segment is added up, and a bar graph of data is obtained that comprises of discrete points."
"computing p gain . we calculate p gain without revisiting the data, as we did for u gain in section 4.1. this computation considers the suppression of location from trajectory t (resulting in trajectory t 0 ) and calculates:"
"range of topics such as those related to objects (e.g. car, lion), indoor and outdoor scenes (e.g. classroom, beach), events (e.g. parade, skiing), people, etc. automatically detecting these concepts helps in improving text-based image or video retrieval, as well as complementing their manual annotations. however, how to effectively bridge the semantic gap between low-level visual features and high-level semantic concepts is still a key hindrance [cit] . the performance of existing approaches can also be easily affected by the presence of intra-class variations, occlusion, background clutter, viewpoint and illumination changes in images and video clips [cit] . in addition, another critical step along this task is the acquisition of sufficiently large amount of quality training data."
"the above modeling of timestamps, together with the model of predefined locations (which is natural for transaction points) does not lead to trajectories, which are continuous routes, but to semantic trajectories, which are sequences of discrete points. this model offers protection against minimality attacks by adversaries who could observe gaps due to suppression of points. while for continuous trajectories (e.g., trajectories created by gps tracing) suppressing certain points, would still allow for accurate assumptions by adversaries due to the continuity of measurements, this is impractical for trajectories that are created based on arbitrary user actions, e.g., card transactions. in the latter case, any gap or discontinuity can be attributed to user behavior."
"wireless sensor networks (wsns) have extensively been used due to their excellent capability of monitoring real physical environments and collecting data [cit] . for example, wsns have been used in military, medical, and environmental monitoring applications, among others. surely, the fields in which they are applied will continue to expand."
"in order to compare the distribution of the discrete data with the normal distribution through our two comparison rules, the discrete data are interpolated by cubic spline interpolation, and the results are shown in figure 12 . figure 13 shows that there are two big differences between them: one is at 30 ∘ c and the other is at 200 ∘ c. figure 14 shows the consistency difference and the square difference between the normal distribution and the continuous bar graph."
and the problems of both a 3 ! a 1 and a 1 are resolved; a 3 ! a 1 is no longer supported in t 0 and a 1 does not map to any b-location with probability higher than 50 percent (see also fig. 3b) .
"subprocess4: compare f(t) with g(t) through the best square to estimate the situation, and if there is a fire, compare f(t) with g(t) through the best consistency to locate the fire."
"scene-localizer data were collected from 24 subjects (6 females; age range, 22-32, including one of the authors). subjects were in good health with no history of psychiatric or neurological diseases, and with normal or corrected-tonormal vision. the experimental protocol was approved by the institutional review board of stanford university. subjects were recruited only at stanford university and gave their written informed consent."
"where d θ is a distance or rank metric with parameter θ that we want to optimize. therefore, the weights π i j decay exponentially with the distance or rank metric. here, we use linear"
"to determine whether moment-by-moment visual processing versus dependence on past experience is a major organizing principle of the brain, we take a data-driven approach to identifying scene-sensitive regions and clustering cortical connectivity. we first aggregate local highresolution resting-state connectivity information into spatially coherent parcels, in order to increase signal to noise and obtain more interpretable units than individual voxels. we then apply hierarchical clustering to show that there exists a natural division in posterior human cortex that splits scene-related regions into two separate, bilaterally symmetric networks. the posterior network includes opa and the posterior portion of ppa (retinotopic maps phc1 and phc2), while the anterior network is composed of the rsc, anterior ppa (appa), and the caudal inferior parietal lobule (cipl). we then show that these two networks differ in their connectivity to the hippocampus, with the anterior network exhibiting much higher resting-state hippocampal coupling (especially to anterior hippocampus), suggesting that memory-and navigation-related functions are primarily restricted to the anterior network. we provide supporting evidence for this functional division from a reverse-inference meta-analysis of previous results from visual, memory, and navigation studies, and an atlas of retinotopic maps."
"which of these networks are directly related to scene perception? we used data from a standard localizer in a separate group of subjects to define group-level regions of interest for scene-selective regions opa, ppa, and rsc. we also anatomically identified cipl as was done in a previous study, since this region has been shown to have functional connections to scene regions."
"timestamps and continuity. in this paper, we assume that timestamp information is removed from the trajectories and only sequences of locations are published. the proposed approach uses suppression and splitting as its main data transformation operators. both operators are not suitable for trajectories of timestamped locations, since it is high likely that each timestamp will be unique and the algorithm will have to suppress it. if we use some simple generalization based preprocessing, i.e., we transform timestamps to time-slots, e.g., 22:23:45 will be reported as 22:00-0:00 timeslot, then each transaction can be treated efficiently by our algorithms as a spatiotemporal point in the trajectory. this is exactly the way we have treated the data from gowalla network in section 9. for example point a 1 might indicate a transaction at location a in timeslot 1 and point a 2 a transaction again in location a, but at timeslot 2."
"where n and n 0 are the total number of problems before and after splitting trajectory t at location . naturally, greater values of s gain indicate better splitting choices and negative values denote that the splittings increase anonymity threats."
"since it is likely that more than one projections are problematic, at each loop we choose the one, which is speculated to be the most beneficiary. algorithm gsup uses an array u gain, where each element u gain ðt n . the maximum value of the ratio is 1 and, naturally, a greater ratio denotes a larger anonymity gain."
"spiros skiadopoulos received the diploma and phd degrees from the national technical university of athens and the mphil degree from the manchester institute of science and technology (umist). he is currently a professor in the department of informatics and telecommunications, university of peloponnese. he has worked in a variety of areas including data management, knowledge representation, and reasoning. his current research interests include anonymity and big data management."
"boosting is an ensemble learning framework to construct a strong classifier by combining a set of inaccurate classification rules (weak learners). we propose to use three variants of the boosting algorithm, including adaboost [cit], realboost [cit] and gentleboost [cit], for visual concept learning. adaboost is the most commonly used version in which the weak learner directly outputs discrete class labels and the final classifier is defined to be a linear combination of the weak learners from each stage. while in realboost procedures, the weak learner produces a class probability estimate and its contribution to the final classifier is half the logit-transform of this probability estimate. [cit] also showed that boosting provides a generalized way to sequentially fit additive regression models of the form:"
"the group-level functional connectivity data were derived from the 468-subject group-principal component analysis (pca) [cit] \"500 subjects\" hcp data release. resting-state fmri data were acquired over four sessions (14 min, 33 s each), while subjects fixated on a bright cross-hair on a dark background, using a multiband sequence to achieve a tr of 720 ms at 2.0 mm isotropic resolution (59,412 surface vertices). these time courses were cleaned using the oxford centre for functional mri of the brain independent component analysis-based xnoiseifier (fix; [cit] ), and then the top 4500 eigenvectors for each vertex were estimated across all subjects using group-pca [cit] . these data were used to perform the parcellation and network clustering, and to generate whole-brain maps (figs. 1, 2a, 3a ) because using the full dataset in its entirety would be computationally challenging to assess statistically, we performed more detailed analyses on a subset of 20 subjects (figs. 2b, c, . for 20 subjects within the \"500 subjects\" release with complete data (subject identifications 101006, 101107, 101309, 102008, 102311, 103111, 104820, 105014, 106521, 107321, 107422, 108121, 108323, 108525, 108828, 109123, 109325, 111413, 113922, and 120515), we created individual subject resting-state datasets by concatenating their four resting-state sessions (after removing the per-run means)."
"to measure image ranking performance we use average precision (ap) and break event point precision (bep). for a given semantic concept, we rank all the images by their predicted probabilities and evaluate precisions at each position according to the manual annotations. ap averages the precision over all positions of relevant images, whereas bep computes the precision just at one position, which is the number of relevant images that are manually labeled with that concept. both measures are evaluated per concept, and finally averaged over all the concepts to obtain a single measure. these measures indicate how well we can retrieve relevant images from the database in response to the keyword-based user queries."
"when we suppress a location or split at a location term n@s t ð; t a þ is affected directly while term s t ðt a þ indirectly. in our setting, each location has two conflicting roles. it can either act as a quasi identifier or as a sensitive item for each adversary. thus, suppressing a location or splitting a trajectory at a location may eliminate some but may also create some other privacy threats. the latter may happen if the suppression or the splitting operations reissues a previously eliminated threat. for example, assume that a 1 ! b 1 is the unique trajectory that violated the p br and the algorithm has already suppressed a 1 . if, at a latter stage, it examines a 1 ! a 2 ! b 1 the algorithm can re-create it by suppressing a 2, if it is not checked."
"just as we described in the introduction, the best consistency comparison and the best squared comparison have their own advantages and disadvantage. so, we will use both of them to get an all-sided result and for different results we should take different methods to process the fire appropriately."
"as our future work, to improve the reliability and safety of the mechanism, we will introduce some other important measures for a fire, such as the humidity of the environment and the concentration of smoke in the subway transportation system. in this way, there will be several results for the prediction of fire. however, it is not competent for our mechanism to fuse these several results. therefore, we will use the technique of fuzzy set theory or d-s evidence theory to fuse the results on decision level."
"connectivity between cortical parcels and the hippocampus was computed similarly, using eigenmaps (for group data) or time courses (for subject data) extracted from the hippocampal volume data distributed by the hcp. in order to focus on hippocampal connectivity differences among parcels, we used the mean gray time course regression version of the group data and regressed out the global time course from the subject data."
"following in fig. 8, we evaluate the percent of the published frequent sequential patterns. in our experiments, we set the frequent patterns threshold to 2 percent that considers as frequent the sequential patterns that appear at least in the 2 percent of trajectories of the original dataset. in fig. 8a, we report the percentage of preserved frequent patterns for various values of jaj. as seen in the previous experiment, more adversaries decrease the probability of finding problematic pairs, thus, the number of frequent published subsequences increases. algorithms split and mix have the best preservation percentage, lsup follows and gsup comes last. fig. 8b presents the percent of the published frequent sequential patterns for variable p br . larger values of p br, result in fewer problematic pairs. thus, all methods manage to preserve more frequent sequential patterns for larger p br . split preserved on average 4.45 percent more frequent sequential patterns than mix algorithm and up to 2.3 and 4.34 times more frequent sequential patterns that lsup and gsup respectively. in a similar manner, split and mix outperform the other algorithms for varying dataset sizes and average trajectories length (figs. 8c, 8d, 9a, 9b, 9c, and 9d) ."
"we identified group-level scene localizers (used only as functional landmarks) from a separate set of 24 subjects (see below). subjects viewed blocks of stimuli from up to six categories: child faces, adult faces, indoor scenes, outdoor scenes, objects (abstract sculptures with no semantic meaning), and scrambled objects. functional data were acquired on one of two ge mr 750 3 t scanners, with an in-place resolution of 1.56 mm, a slice thickness of 3 mm (with 1 mm gap), and a tr of 2 s; a high-resolution (1 mm isotropic) spoiled gradient-recalled acquisition in a steady state structural scan was also acquired to allow for transformation to mni space."
"in table 3, we present the overall results of all the visual concept learning algorithms described in this paper. as illustrated in table 3a when learned from social-tagged images, gentle-iw and svm-iw denote the importance weighted gentleboost and svm. (i) and (c) denote per-image and per-concept weighing scheme for tag relevance-based importance weighting. the better performance between methods using each weighing scheme is italicized, while the best performance among all methods is bolded the visual neighbor voting model using uniform and rankbased weights obtains the best results in terms of ap and bep respectively, while the svm approach outperforms other classification algorithms in terms of concept ranking evaluation. in table 3b, by contrast, we observe an obvious improvement in performance when trained using manual annotations. svms now achieve the best performance in terms of all of our evaluation criteria. furthermore, in both cases, visual neighbor voting model using rank-based weights and gentleboost classifier gives more competitive performance than other variants of the tag relevance learning model or the boosting algorithm. we have to emphasize that svm classifier exhibits more powerful discriminative capabilities between semantic concepts than all the other classifiers in our experiments, as it yields much higher mrr scores in both cases. in order to feed the importance weights to our importance weighted classifiers, we first perform tag relevance learning on the training dataset. specifically, we learn the tag relevance of each training example by visual neighbor voting in a leave-one-out manner. here, the unsupervised tag relevance learning model using uniform weights is preferred, since the supervised learning models require manually labeled training data. we also study two relevance-based importance weighting schemes, i.e. per-image and per-concept weighting, to convert the tag relevance into importance weights for each training example. apart from this, we use the same configurations, such as the choice of kernel function in svm or weak learner in boosting, as above for our importance weighted svms and boosting algorithms in the following experiments."
"because of the inherent features of sensors, accurate measurements of the temperature in the subway transportation system cannot be obtained. the observed value often follows a gaussian distribution with a variance 2 of the real value, and has a relationship with the property of the sensors deployed. obviously, 2 is an important parameter of the sensors, since smaller 2 means better quality and accuracy of the sensors. in section 4.1, we obtain the normal temperature and the distribution of the temperature obtained from the sensors should be a gaussian distribution with a mean of and a variance of 2, namely, (, 2 )."
"is decreased to observe the situation more carefully. in fact, there are some occasional cases in which data collected implies that there is fire and that would lead to erroneous . however, it may have less impact as data will be collected more often, and the influence of occasional cases will be smaller."
"our meta-analyses can be viewed on-line at http:// neurosynth.org/analyses/custom/dda0e003-efd0-4cfa/ and http://neurosynth.org/analyses/custom/9e6df59d-02df-4357/. the first used the query \"scene,\" and consisted of 47 studies. manual inspection of all studies confirmed that they all studied the perception of environments, and 45 of 47 studies involved the presentation of visual scenes. the second meta-analysis used the query \"episodic memory or navigation or past future,\" which returned 125 studies that were nonoverlapping with the first query."
"various cost-sensitive extensions of adaboost algorithm are available in the literature, including adacost [cit], csb0, csb1, csb2 [cit], and adac1, adac2, adac3 [cit] ."
"if an adversary knows up to l locations of a user's trajectory, a user is indistinguishable from at least k à 1 users, while the probability of linking a user to its sensitive values is at most c."
"to get the temperature at any time, continuous function to represent the discrete data based on cubic spline interpolation is developed, and the result is presented in figure 5 . we can find that the continuous function crossed all the discrete points and it is very smooth and it satisfied our feeling better than data fitting. this is the continuous function of temperature for a day obtained by cubic spline interpolation, and the actual temperature at any time of the day can be obtained using this function. getting the accurate temperature is the basement of fire detection."
compare f(t) with g(t) through the best consistency to locate the fire compare f(t) with g(t) through the best consistency to find the places where the fire is most severe
"another prediction of our framework is that voxels whose activity is tied to specific locations in the visual field (i.e., retinotopic) should, as clearly visual voxels, be present only in the posterior scene network. in figure 4b, we compared our networks to a group-level probabilistic atlas of retinotopic visual field maps [cit] . the vast majority of the probability mass in this atlas is concentrated in the posterior network. in early visual cortex (v1, v2, v3, hv4), all nonfoveal portions of the visual field maps fall in the posterior network (80% posterior, 0% anterior, 20% other). ventrally, the posterior network covers vo1/2 (100% posterior, 0% anterior, 0% other), phc1 (98% posterior, 2% anterior, 0% other), and the peak of the probability distribution for phc2, which also extends slightly across the anterior network border (78% posterior, 22% anterior, 0% other). laterally and dorsally, the posterior network includes most of the lo1/2 and to1/2 maps (82% posterior, 0% anterior, 17% other), v3a and v3b (96% posterior, 0% anterior, 3% other), and ips0 -ips5 (68% posterior, 4% anterior, 28% other), with spl1 being the only map falling substantially outside the networks that we consider (18% posterior, 2% anterior, 80% other)."
"this way is much like a feature selection process. our adaboost, realboost and gentleboost algorithms are respectively described in figs. 2, 3 and 4. \"butterfly\", the seed image will obtain four votes for its tag relevance estimation. moreover, if we consider to recommend new tags for the seed image, the concept \"garden\" would be preferred, because the accumulated neighbor votes for it is three 4 exploiting tag relevance"
"next, we test the arel scores of our methods (fig. 7), varying the number jaj of adversaries (fig. 7a) . arel is low for very few adversaries, because there are more projections per adversary, and thus, our methods have more flexibility in selecting the most beneficiary problematic pair. as the number of adversaries increases, arel scores gradually increase and then decrease again, since if there are many adversaries, the chances of finding problematic projections decrease. between our methods, mix has the lowest arel, as splitting preserves the number of published locations, while suppression reduces the number of iterations in order to produce the anonymized dataset. the arel scores of mix are on average 52.34, 43.64 and 6.18 percent better than gsup, lsup and split respectively. we then evaluate arel against the identification probability threshold p br (fig. 7b) . as expected, increasing p br leads to less problematic pairs and thus, in better arel scores. similarly to the previous experiment, mix is slightly better than split, but significantly better than gsup and lsup. in fig. 7c we observe that arel scores improve for larger datasets. the reason is that as jdj increases, the initial supports of the projections increase in size (i.e., the probability that two trajectories have the same projection increases) and, thus, the number of problematic pairs is reduced. finally, arel scores decrease as the average trajectories length increases (fig. 7d), as a dataset comprised of small trajectories has fewer problematic pairs (i.e., each trajectory holds only a small combination of locations)."
"another interesting question is how spatial reference frames differ between and within the two networks. given its retinotopic fieldmaps, the visual network presumably represents scene information relative to the current eye position; previous work has argued that this reference frame is truly retina centered and not egocentric [cit] . the context network, however, likely transforms information between multiple reference frames. models of spatial memory suggest that medial temporal lobe (possibly including appa) uses an allocentric representation, while the posterior parietal lobe (possibly including cipl) is based on an egocentric reference frame, and that the two are connected via a transformation circuit in rsc that combines allocentric location and head direction [cit] . there is some recent evidence for this model in human neuroimaging: posterior parietal cortex codes the direction of attention in an egocentric reference frame (even for positions outside the field of view; [cit] ), and rsc contains both position and head direction information (anchored to the local environment; [cit] . this raises the possibility that another critical role of cipl could be to transform retinotopic visual information into a stable egocentric scene over the course of multiple eye movements. the properties of appa, however, are much less clear; it seems unlikely that it would use an entirely different coordinate system than neighboring phc1/2, and some aspects of the scene encoded in appa, such as object co-occurrence [cit], do not seem tied to any particular coordinate system. finally, we note that a hard division into two networks is only a first-order description of the structure and function of scene regions. a number of these regions (e.g., phc2) fall on a continuum from visual to contextual, and recent theories of information processing argue that almost all cortical regions accumulate information at varying timescales . task demands will also shift the functions of these regions (e.g., during top-down imagery; [cit] ) and can lead to the dynamic reconfiguration of networks [cit] . our proposed framework is intended to capture the primary functional dimension that distinguishes between scenesensitive regions during natural perception, and to offer a starting point for future work on the organization of the human scene-processing system."
"the network has strong functional connections to the hippocampus, which has been implicated in a broad set of cognitive tasks involving \"cognitive maps\" for organizing declarative memories, spatial routes, and even social dimensions [cit] . during perception, the hippocampus binds together visual elements of an image [cit], which is especially important for scene stimuli [cit],b; [cit] and then stores this representation into long-term memory [cit] ). as we become familiar with an environment, the hippocampus builds a map of the spatial relationships between visual landmarks, which is critical for navigation [cit] . recalling or even imagining scenes also engages the hippocampus, especially anterior hippocampus, which may serve to integrate memory and spatial information [cit] . our results suggest that only the anterior scene regions interface directly with the hippocampus, potentially enabling the construction of hippocampal environmental representations, and retrieval of relevant memories and navigational information for a presented or imagined scene."
"the computation of u gain is crucial in algorithm gsup since, in every while iteration (lines 3-11), it is calculated for every adversary and every problematic pair (line 6). according to equation ( this computation drastically improves efficiency, as it considers only the affected trajectories, and users the initial calculation of algorithm threatid for unaffected trajectories."
"of data collected and process by analyzing ( ). at the beginning of fire, the distribution of ( ) tends to have two peaks because the temperature increases in the local area, and the discrete data of the temperature tends to be greater there. therefore, if the function has two peaks that are not obvious, the period of data processing should be decreased to get more information about the fire."
"where c i denotes the importance weight for each example x i . the weight updating function of adac2 or adac3, i.e. eq. (28) or (28), will be equivalent to the weight updating function of original adaboost algorithm in fig. 2, when the importance weight items are all set to 1."
"in two of the above three cases, a is able to associate the user of projection a 3 ! a 1 with location b 1 that is not controlled by a. this type of attack is captured by the following definition."
"the main idea behind our anonymizing algorithms is to transform long and detailed projections to smaller and simpler ones. in doing so, we are able to (i) increase the supports of projections and (ii) diversify the locations that are not monitored by adversaries, making thus impossible for them to infer with high certainty if a trajectory includes such a point. more specifically, to make dataset t safe, we will eliminate problematic pairs ð; t a þ by employing suppression or splitting. suppression deletes a location 2 t from a trajectory t. for instance, trajectory"
"in this section, we will present algorithm threatid that identifies potential privacy threats. this algorithm takes as input a dataset t, the identification probability threshold p br and the background knowledge of adversaries, and returns (a) a list q holding all problematic pairs ð; t a þ (where is a location and t a is a projection of adversary a), (b) the total number of problems n in dataset t and (c) the number of problems n@s t ð; t a þ by every problematic pair ð; t a þ."
"our problem of learning visual concepts from weakly labeled social images can be viewed as a cost-sensitive learning problem, since for a given concept misclassifying a more relevant image should result in a higher cost than misclassifying an irrelevant image."
"the proposed novel mechanism for fire detection mainly has three preliminary techniques (1) interpolation is an alternative method of data fitting, and, in some conditions, it is an easier and more accurate method especially for the complicated functions; for some extreme cases, the performance of data fitting cannot be accepted. because of the reasons we proposed before, in this paper, cubic spline interpolation is used rather than gaussian data fitting or some other data fitting techniques to get the continuous function of the discrete data collected by the sensors. (2) the second preliminary technique is the best consistency comparison. it is a method that can identify the points at which the two distributions have obvious differences. the differences will be used to locate fire. (3) the third preliminary technique is the best squared comparison, which can be used to obtain the whole difference between two distributions that can be compared. it can accumulate all of the differences rather than just the main differences. therefore, it is a good way to estimate the situation of the fire."
"a . in the example of fig. 1, after the unification of a 3 ! a 1 with a 1, trajectories t 5, t 6 and t 7 become t"
"however, tags occurring frequently in the training image collection may dominate the results. to restrain such effects, we take into account the tag's prior frequency to estimate its prior probability [cit] . concretely, the prior probability for a given concept w is approximated as:"
"algorithms lsup, split and mix in the worst case execute gsup, thus, they also have an oðn á jaj á jt a j 3 þ time complexity. note, that this is worst case complexity, in practice the algorithms behave a lot better. as we see in the results for our real-life dataset fig. 11, the behavior of all algorithms is almost linear to the size of d. moreover, we can regulate the actual performance of the algorithms by adjusting m (the number of trajectories that are transformed in each iteration)."
to this end we perform cross-validation on the training data to fit a sigmoid function to map the svm scores to probabilities. the regularization parameter c of the svms is also optimally selected by fivefold cross-validation.
"definition 7. let t be a set of trajectories, a be an adversary controlling locations l a and t a be a projection of a. the number of trajectories in the support set"
"this model was inspired by recent successful tag relevance learning methods [cit], that propagate the annotation tags of training images to a target image. we summarize their work by literature-based weighting schemes, i.e. using uniform, distance-based and rank-based weights for each visually similar image, associated with a weighted nearest neighbor model. we also discuss a variation of cost sensitive learning called \"importance weighted\" classification that incorporates the example-dependent importance weights into the learning frameworks of svm and boosting classifiers. these importance weights are based on the tag relevance learned by visual neighbor voting, since more relevant example images have to be emphasized more in the training process for a given concept. therefore, we aim to discriminate between different training examples by their importance weights in the classifier learning procedure using cost-sensitive learning techniques. apart from this, all the proposed algorithms are evaluated by both the socially tagged and manually tagged images so as to explore the impact of the user-contributed tags, in terms of tagging accuracy, towards visual concept learning, and in comparison with manual annotations."
"where c i is the importance weight of example x i and now regularization constant c controls model complexity versus importance weighted training errors. as shown in eq. (26), the biased penalties method has direct effect on the support vectors of svm classifier. however, it suffers from a flaw that it has limited ability to enforce cost-sensitivity when the training data points are separable, which is the opposite case of boundary movement method. since, in practice, the training data are more likely to be non-separable, our implementation is based on the loss function (26) employing the biased penalties."
"second, the range [, ] is divided into segments. usually, is a number in 30-50. however, can be set somewhat larger to get a more accurate result at the expense of increasing the amount of calculations required."
"in this paper, we studied the problem of protecting datasets, holding user movements, from adversaries who can use their partial knowledge to infer locations unknown to them. we proposed four anonymization algorithms, employing locations suppression, trajectories splitting, or both suppression and splitting, to protect such datasets. finally, we experimentally showcased the effectiveness of our algorithms, in terms of data utility preservation and efficiency. giorgos poulis received the bachelor's degree in informatics from the university of piraeus, greece, the msc degree from the national technological university of athens, greece, and the phd degree from the university of peloponnese, greece. he is currently a software engineer with nokia networks, athens, greece. his main research interest lie in the areas of data privacy."
"based on data-driven connectivity analyses and analysis of previous literature, we have proposed a unifying framework for understanding the neural systems involved in processing both visual and nonvisual properties of natural scenes. this new two-network classification system makes explicit the relationships between known scene-sensitive regions, re-emphasizes the importance of the functional subdivision within the ppa, and incorporates posterior parietal cortex as a primary component of the scene-understanding system. our proposal that much of the scene-processing network relates more to contextual and navigational information than to specific visual features suggests that experiments with unfamiliar natural scene images will give only a partial picture of the neural processes evoked in real-world places. experiencing our visual environment requires a dynamic cooperation between distinct cortical systems to extract information from the current view of a scene, and then to integrate it with our understanding of the world and determine our place in it."
"our primary dataset is a 1.8 billion element resting-state connectivity matrix distributed by the human connectome project [cit], which estimates the time course correlation between every pair of locations in the brain at 2 mm resolution based on a group of 468 subjects. since we wish to understand the large-scale structure of visual cortex, it is helpful to abstract away from individual voxels and study the functional and connectivity properties of larger parcels. rather than imposing a parcellation based on specific regions of interest, we used a data-driven approach to produce spatially coherent parcels tiling the cortical surface in a way that retains as much information as possible from the full connectivity matrix. this parcellation consists of 172 regions across both hemispheres, each of which contains surface vertices that all have very similar connectivity patterns with the rest of the brain. the connectivity matrix among these 172 parcels captures ͼ76% of the variance in the original connectivity matrix, despite being dramatically smaller (by five orders of magnitude)."
"we also investigated whether this effect was being driven by a subregion of the hippocampus, by correlating the mean time course in both scene networks with the time courses of each posterior-to-anterior coronal slice of the hippocampus. our results show that the entire hippocampus is more strongly connected to the anterior scene-network than the posterior scene-network, but this difference is especially large in the anterior hippocampus. to confirm this pattern of results, we divided the hippocampus into posterior and anterior subregions at mni coordinate y ϭ ϫ21 [cit] and correlated their mean time courses with the two scene-network time courses. this analysis confirmed that the anterior network is more strongly connected to both posterior (t (19) ϭ 7.66, p ͻ 0.001; two-tailed paired t test) and anterior (t (19) ϭ 6.58, p ͻ 0.001) hippocampus than is the posterior scene network, and that this anterior-network connectivity is larger in anterior hippocampus (t (19) ϭ 3.29, p ϭ 0.004); a repeated-measures anova shows significant main effects of both hippocampal subregion (f (1,19) ϭ 11.32, p ϭ 0.003) and scene network (f (1,19) ϭ 59.2, p ͻ 0.001), and an interaction (f (1,19) ϭ 7.03, p ϭ 0.016). group-level connectivity values are reported in table 1 . note that both the anterior and posterior scene networks are closer to posterior hippocampus, ruling out a distance-based explanation for this pattern of results."
"the division of the ppa into multiple anterior-posterior subregions with differing connectivity properties replicates previous work on an en- figure 4 . overlap of posterior and anterior scene networks with previous work. a, two meta-analyses conducted using neurosynth identified overlapping but distinct reverse-inference maps corresponding to studies of visual scenes and to studies of higher-level memory and navigation tasks. these maps separate into our two scene networks, with visual scenes activating voxels in the posterior network and memory/navigation tasks activating voxels in the anterior network, as shown on example axial (z ϭ ϫ8) and sagittal (x ϭ ϫ30) slices. false discovery rate ͻ 0.01; cluster size, 80 voxels (640 mm 3 ). b, voxels having a ͼ50% chance of belonging to a retinotopic map (orange) overlap with much of the posterior scene network, but end near the border of the anterior scene network. breaking up the contributions of individual regions, we find that the probability mass of the topographic maps falls primarily within the posterior network, with only phc2 showing a small overlap with the anterior network (probabilistically at the group level)."
"although our work is the first to propose the visual versus context networks as a general framework for scene perception, several previous studies have shown differential effects within these two networks. contrasting the functional connectivity patterns of rsc versus opa or lateral occipital cortex (loc; [cit] ) or anterior versus posterior ppa show a division between the two networks, consistent with our results. contrasting scene-specific activity with general (image or word) memory retrieval showed an anterior versus posterior distinction in ppa and cipl/opa, with only more anterior regions (appa and cipl, along with rsc) responding to content-independent retrieval tasks [cit] . our twonetwork division is also consistent with the \"dual intertwined rings\" model, which argues for a high-level division of cortex into a sensory ring and an association ring, the second of which is distributed but connected into a continuous ring through fiber tracts [cit] ."
"despite the high popularity and advantages of social tagging, it is well known that tags provided by the grassroot internet users are actually far from satisfactory as qualified descriptive indexing keywords for the visual content of the web images. therefore, in this section, several cost/importance weighted concept learning algorithms are considered to solve the problem of directly using noisy tags of social images for visual concept learning. these approaches are inspired by current cost-sensitive learning techniques. first, we exploit the visual relevance of the tags that are present in the social images as shown in the previous section. second, the tag relevance with respect to each training examples is integrated into the supervised learning process of svm and boosting classifiers, in the form of importance weights."
"where n is the number of training examples. first, we propose to incorporate importance weight into the cost function formula as a linear factor: (34) where c denote the importance weight for each example x. hence, we also choose to minimize the second order taylor approximation of this new cost function: (36) empirically, this also reduces to minimizing the weighted square error in (33), but with a new weight function (36) . the weighs thus get updated by:"
"the rest of the paper is organized as follows. section 2 introduces the related work of this paper, and section 3 introduces the preliminary techniques. the fire detection system is discussed in detail in section 4, and its performance is evaluated in section 5. the conclusions are presented in section 6."
"the group-level eigenmaps for every pair of voxels and applying the fisher z-transform (hyperbolic arctangent). we parcellated this 59,412 ϫ 59,412 matrix into contiguous regions, using a generative probabilistic model [cit] . this method finds a parcellation of the cortex such that the connectivity properties within each parcel are as uniform as possible, making multiple passes over the dataset to fine-tune the parcel borders. we set the scaling hyperparameter 0 2 ϭ 3000 to produce a manageable number of parcels, but our clustering results are similar for a wide range of settings for 0 2 (producing between 140 and 216 parcels). connectivity shifts across the network border. a, using classic multidimensional scaling (mds), we can visualize the connectivity structure among the eight parcels overlapping with scene-related regions (darker/lighter shading denotes left/right hemisphere). the first mds dimension shows a parallel transition along both dorsal and ventral paths from parcels overlapping opa and pppa to those overlapping cipl, rsc, and appa. b, connectivity between dorsal parcels and the medial rsc parcel increases markedly near the opa/cipl border. b, ventral parcels also show a shift in network connectivity properties, with increasing connectivity to the most anterior cipl parcel as we move from pppa to appa. error bars are 95% confidence intervals across subjects, ‫ء‬p ͻ 0.05, ‫‪p‬ءء‬ ͻ 0.01."
"flight simulators are devices in which air crews and pilots can train without the use of an actual aircraft [cit] . in flight training, they are used mainly to reduce costs and increase safety. in their most sophisticated form, they simulate an aircraft's vehicular motion, instrumentation and sounds, gravitational forces, radar and electro-optical sensor displays, and out-the-window views. according to the usa's federal aviation administration (faa) regulations, any device called a flight simulator must have at least one motion platform, otherwise it can only be termed a flight training device [cit] . therefore, the vehicular motion platform in flight simulator is one of the most important parts."
"computed tomography (ct) reconstruction is the process of recovering the structure and density of objects from a series of x-ray projections, called sinograms. while traditional full-view ct is relatively easier to solve, the problem becomes under-determined in two crucial scenarios often encountered in practice -(a) few-view: when the number of available x-ray projections is very small, and (b) limited-angle: when the total angular range is less than 180 degrees, as a result of which most of the object of interest is invisible to the scanner. these scenarios arise in applications which require the control of x-ray dosage to human subjects, limiting the cost by using fewer sensors, or handling structural limitations that restrict how an object can be scanned. when such constraints are not extreme, suitable regularization schemes can help produce artifact-free reconstructions. while the design of such regularization schemes are typically driven by priors from the application domain, they are found to be insufficient in practice under both few-view and limited-angle settings."
"the next step is to establish a relationship between ξ 1 and $ p while eliminating ξ 2, ξ 3, ξ 4, ξ 5 . r $ 1 denotes a reciprocal screw and is derived by means of the following rules:"
"instead, our approach utilizes a recently proposed technique referred to as corruption mimicking, used in the design of mimicgan [cit], that achieves robustness to the noisy seed reconstruction through the use of a randomly initialized shallow convolutional neural network (cnn), in addition to pgd. by modeling the initial guess of this network as a random corruption for the unknown clean image, the process of corruption mimicking alternates between estimating the unknown corruption and finding the clean solution, and this alternating optimization is repeated until convergence, in terms of effectively matching the observed noisy data. the resulting algorithm is test time only, and can operate in an artifact-agnostic manner, i.e. it can clean images that arise from a large class of distortions like those obtained from various limited-angle reconstructions. furthermore, it reduces to the well-known pgd style of projection, when the cnn is replaced by an identity function."
": many. for simplicity, in this paper, screw theory is used to build a jacobian matrix [cit] for which the moving platform angle velocity $ p is derived as follows: where and where ξ 1, ξ 2, ξ 3, ξ 4, ξ 5 are angular velocities associated with unit screws $ 11,$ 12,$ 13,$ 14,$ 15 ."
"looking at figure 3, to achieve 360° rotation of the cabin, two flange-bearing-gear reducers are used. the outer flange is fixed to the moving platform by bolts, and the inner flange is fixed to the cabin by bolts as well. together, the outer flange and inner flange serve as a bearing. three axis-fixed gears are fixed to the moving platform via shafts, and the center gear is linked to a motor."
"is electrode also showed bci feasibility under ssvep and assr [cit] . e current capacitive electrode designs involve bulkier structures than the wet eeg electrodes widely used both clinically and academically. to translate laboratory nonclinical work into real-world clinical applications, studies should consider methodologies that maximize coupling capacitance while using small capacitive electrodes because the size of an eeg electrode is directly related to the spatial resolution of the eeg. table 1 summarizes the features, strengths, and drawbacks of the dry eeg electrodes reviewed above."
"as shown in figure 1, the flight simulator is a hybrid manipulator developed on the basis of two identical 3-rrs spherical parallel mechanisms and one cabin with an independent dof. therefore, it has kinematic redundancy and a redundantly actuated manipulator as well. the basic makeup of the whole platform consists of two fixed platforms, one cabin, six rrs legs, two flange-bearing-gear decelerators, and eight motors. in the fixed coordinate frame, the x-axis is normal to plane c 1 on, and extends outwards. the z-axis superposes on line mn, and extends upwards. the y-axis is defined by the right hand rule. in the moving coordinate frame, the u-axis is normal to plane a 1 on, the w-axis superposes on line qv (the points q and v are the geometrical center of triangles ∆a 1 ′a 2 ′a 3 ′ and ∆a 1 a 2 a 3 ), and extends outwards. the v-axis is defined by right hand rule."
"as follows: therefore, the mechanism has three degrees of freedom, and the cabin's spin becomes a redundant degree of freedom with regard to the w-axis."
"figure 2: relative pandemic risks of us cities, according to the city where the initial case appears. city risks are defined as the number of active pandemic cases arriving via airline travel during the course of the pandemic. city risks are remarkably insensitive to the geographic location of the index case, with important and beneficial implications for benefits of a priori mitigation efforts for high-risk cities."
"here, we evolve and analyze the most effective geographic deployment of intervention resources to critical cities, to compare against inaction (no intervention) or pro rata allocation. specifically, strings specify interventions and geographic deployments for pandemic control. the fitness function maximizes indirect protection; intuitively, the number of people protected indirectly by each direct vaccination. the ga employed in this study includes a number of features specific to this task. the ga first profhvvhv wkh vlpxodwlrq prgho ¶v lqsxw sdudphwhuv dqg h[hfxwhv wkh qr-intervention control case using those specific parameters for each of thirty stochastic seeds roughly 25% of the seeds used in the control case stochastically fail to spark a pandemic, and are discarded [cit] ). the ga then generates a random population of fifty candidate interventions. for each candidate intervention string, the ga executes fifteen sets of simulations with the disease originating in each of the (fifteen) [cit] . for each intervention candidate and origin city, the ga executes the simulation across the remaining stochastic seeds (~23). since the intervention process changes the stream of random numbers used in the simulation, roughly 25% of the seeds used for intervention simulations fail to spark a pandemic because of stochastic variation and are discarded. the fitness value of the string is the average indirect protection across the twice-filtered seeds and origin cities. the ga then generates each new generation of interventions using a tournament selection process; the ga completes when the fitness of the best intervention has been constant for fifty generations."
"when considering the independently redundant dof of the cabin, because there are three outputs and eight inputs, it is innumerable about the inverse solutions. if we ignore the independently redundant dof of the cabin, and fix the independent dof of cabin, the analysis of the inverse kinematics is as follows."
"our first set of interventions deploys vaccines to a targeted subset of metropolitan areas selected by the genetic algorithm. we test subsets of 6 cities. for each level of deployment, we consider two forms of distribution: uniform distribution across the selected metropolitan areas and distribution proportional to the population of the selected metropolitan areas."
(1) r $ 1 is through the center of the spherical joint of the a 1 -axis and b 1 -axis; (2) r $ 1 does not intersect with the c 1 -axis.
"the ga evaluates optimal strategies in terms of the indirect protection generated by the intervention, i.e., in terms of its fitness function. this process raises three issues that are important in the ex-post evaluation of optimal strategies. first, since the ga is optimizing over a static set of stochastic seeds, it is feasible that it optimizes interventions that are effective for those particular seeds and not others. with a sufficiently large set of stochastic seeds, and an underlying process which converges at the limit, this possibility is vanishingly small. we investigated various numbers of stochastic seeds, and determined wkdw vhhgv dv xvhg lq wkh dqdo\\vlv khuh zrxog eh vxiilflhqw wr dyrlg vxfk duwlidfwv +rzhyhu ³vxiii-flhqwo\\ odujh´ lv qrw zhoo defined for this simulation model, and thus we check ex-post for the validity of wkh *$ ¶v rswlpdo vwudwhjlhv xqghu qhz vwrfkdvwlf vhhgv $oo uhvxowv suhvhqwhg ehorz in figures 6 and 7 duh edvhg rq wkhvh ³qhz´ qrq-optimized) seeds."
"we developed a compact, efficient agent-based model of the geographic spread of influenza within the united states and placed it in a computational laboratory to evaluate the impact of the uncertainty resulting from epidemiological characteristics, stochastic travel and transmission behavior. there exists a rich history of work on the spread of disease, including considerable recent work with agent-based simulation models. the widely used seir model provides differential equations which approximate disease spread [cit] . the seir model predicts the mean quantity of four categories of individuals--susceptible, exposed, immune, and recovered--who interact over time within a single perfectly mixed population. [cit] examined the role of discretization in seir models, finding that discretization more accurately approximates disease spread. [cit] examined the impact of spatial heterogeneity and stochasticity on the spread of the disease, similarly verifying the importance of these extensions."
"although the number of subjects in the current demonstrations is too small to be representative of the general population, our results are in general agreement with published values for healthy adults, and we suggest that our demonstration offers sufficient power to assess the feasibility of abci applications. however, our results represent just a small sample of the broad future potential of abci technologies. as bcis become more popular with different user groups, including healthy people, their increasing commercial possibilities will likely encourage new applied research efforts that will make bcis even more practical. consumer demand for reduced cost, increased performance, computational intelligence and neuroscienceand greater flexibility and robustness could contribute substantially to making bcis mainstream tools. e development of abcis requires clear validation of their real-life value in terms of efficacy, practicality, and impact on quality of life. future bci systems should (1) be comfortable, convenient, and offer aesthetically acceptable mountings, (2) be easy to set up, (3) function for many hours without maintenance, (4) perform well in all environments, (5) operate by telemetry instead of requiring wiring, and (6) interface easily with a wide range of applications. before the results of ongoing and planned research efforts for abci become available, bcis using various methods remain a fascinating research toy. if the intensive research into various aspects of abci continues to increase exponentially, as it has done recently, bci systems could become routine clinical, assistive, and commercial tools in the not-too-distant future."
"(10) where t is the euler angle rotation matrix with regard to the moving coordinate frame o ′ − uvw, and is depicted as follows:"
"another approach is to use electrode-finger-based sensors for eeg acquisition over hair (figure 1(a) ) [cit] . is kind of electrode offers high geometric conformity between the electrode fingers and the irregular scalp surface, thus maintaining low electrode impedance. additionally, the flexible substrate in which the spring probes are inserted permits the attachment of the sensor to the scalp without pain when force is applied. [cit] ( figure 1(b) ) and showed bci feasibility using the motor imagery and oddball paradigms [cit] . e main drawback of those electrodes is that they still require skin preparation to ensure contact between the finger-electrode and the scalp. also, some participants reported prickling and other uncomfortable sensations. figure 1(c) shows a reverse-curvearch-shaped dry eeg electrode 3d-printed from sterling silver to increase the skin-electrode contact area over hair [cit] . e curvature of the arches was designed to match the curvature of the scalp to maximize the contact area and disperse the pressure, thereby lessening the pain induced by conventional finger-type eeg electrodes."
(1) the flight simulator for a fighter-aircraft with a hybrid configuration of symmetrically double parallel manipulators named twins is presented. (2) twins is a multi-functional flight simulator. it can be used as a normal flight simulator like stewart mechanism. and when the cabin spins that will be a flight simulator of fighter-aircraft. (3) screw theory is used to establish the jacobian matrix that simplifies the process of establishing the jacobian by the means of closed equations. (4) a numerically forward kinematics is adopted by the inverse kinematics and jacobian matrix and the method is more simple.
"new eeg electrodes will improve the state of the art and increase practicality, efficacy, and ease of use. aesthetic perspectives also should be considered. for applications outside of hospital and laboratory environments, eeg measuring devices should not make users look strange."
"the remainder of this paper is organized as follows. in section 2, the structure of the twins flight simulator is described. in section 3, the kinematic properties of the twins are discussed, including the development of the direction-cosine matrixes in section 3.1 with analytical spherical theory, the analysis of dof in section 3.2, and the development of the jacobian matrix via screw theory in section 3.3. inverse kinematics and forward kinematics are conducted in sections 4 and 5. numerical examples are provided in section 6. lastly, the conclusions are discussed."
"is paper has focused on the challenges faced when moving from bci systems designed for experimental use in laboratory settings to those intended for use in real-world environments. we have discussed the problems with eeg sensing technologies and new bci paradigms and explored representative methods for handling laboratory, more realistic, or real-world settings."
"the fifty metropolitan areas included in the model represent roughly half of the us population, or 150 million agents. by testing the sensitiviw\\ ri wkh prgho ¶v uhvxowv wr wkh qxpehu ri djhqwv zh irxqg wkdw a population as small as 100,000 agents provided unbiased results."
"the epidemiological component consists of a discretized, spatialized, agent-based seir simulation model in which agents track carefully calibrated travel behavior, each having a set of internal clocks which govern the progression of disease phases from susceptible through exposed, infectious, and recovered or removed. these agents are proportionally allocated to the fifty most populous metropolitan areas of the united states. agent travel among these areas is calibrated according to a representative sample of us air travel behavior. within each metropolitan area, the agents interact randomly with one another. the model thus builds on previous work on stochastic, discrete, spatial seir models, but adds the capacity for agent heterogeneity and customized travel behavior."
"in the recent years, there is a surge in research interest to utilize deep learning approaches for challenging inverse problems, including ct reconstruction [cit] . these networks implicitly learn to model the manifold of ct images, hence resulting in higher fidelity reconstruction, when compared to traditional methods such as filtered backprojection (fbp), or regularized least squares (rls), for the same number of measurements. while these continue to open new opportunities in ct reconstruction, they rely of directly inferring mappings between sinograms and the corresponding ct images, in lieu of regularized optimization strategies. however, the statistics of sinogram data can vary significantly across different scanner types, thus rendering reconstruction networks trained on one scanner ineffective for others. furthermore, in practice, the access to the sinogram data for a scanner could be restricted in the first place. this naturally calls for entirely image-domain methods that do not require access to the underlying measurements. in this work, we focus on the limited-angle scenario, which is known to be very challenging due to missing information. instead of requiring sinograms or scanner-specific representations, we pursue an alternate solution that is able to directly work in the image domain, with no pairwise (sinogram-image) training necessary."
"the power and complexity of our technical infrastructure is exploding, and our control systems kdyh qrw nhsw sdfh :khq \\rx grxeoh dq hqjlqh ¶v kruvhsrzhu \\rx ¶g ehwwhu lpsuryh \\rxu eudnhv tires, suspension and steering as well. interpreting this data instead of ignoring it is our main chance of beefing up the brakes and the steering. without adequate control systems, we face real gdqjhu´ *hohuqwhu 2, page 112)"
"although the novel mechanisms are proposed one by one, they don't adapt for the flight simulator. regarding existing parallel manipulators, the 3-dof spherical parallel manipulator is a compact configuration with large rotational posture [cit] . therefore, given the above conclusions, a flight simulator of a fighter-aircraft based on a double and hybrid 3-rrs [cit] spherical parallel manipulator was chosen for this study."
"ey also demonstrated an ssvep-based phone dialing application that used the developed capacitive eeg electrode with two subjects [cit] . e result was feasible, but the average itr was lower than that with conventional wet electrodes. [cit] . e rigid surface of previous conventional capacitive electrodes cannot adapt to head curvature and the hair-made irregular surface that produces hundreds of micrometer-wide air gaps between the scalp and the electrode face. e use of foam minimized the loss of electrode contact area and generated increased contact impedance. e soft foam used in baek's study enabled intimate electrode contact on the hairy scalp topography, thereby increasing the effective contact area. in addition, the foamsurfaced electrode maintained stable contact during motion, minimizing how much the electrode slid over the hair through its cushioning effect and textures."
"we analyzed pandemic risks for simulations, resulting in pandemic influenza affecting more than 1,000 agents and spreading to at least two other cities. we defined pandemic risk for each city according to the average number of infected individuals arriving at that city during each pandemic, averaged across all simulated pandemics, producing 51 rankings of the 50 cities. one ranking is for overall pandemic risk, averaged across all the epidemics, regardless of where the initial case was introduced. this establishes overall pandemic risk when the location of initial case(s) is yet unknown. each of the other 50 rankings shows the relative risks for each city depending on the location of the initial case, ranking the relative risks for each of the us cities where the first outbreak occurs. significantly, regardless where the pandemic begins, the relative risks across cities remain stable. this is clear in figure 2, where minor changes exist in the rank order, but where the membership of the top 15 high-risk cities is consistent, with only one city in the bottom of the origin-specific rankings. the spearman rank correlation coefficient (srcc) across city rankings was ~98% (see figures 3 and 4) ."
"we also performed sensitivity analyses on the density of links in the network (for minimum 10k, 1k, and 10 passengers per year for each airline route included in the network), the numbers of random number (monte carlo) repetitions for each scenario, travel probabilities, incubation period, and the standard reproductive number r 0 (i.e. the expected number of secondary cases from the first case in a fullymixed susceptible population)."
"genetic algorithms use evolutionary methods to solve otherwise intractable optimization problems. an optimization problem is represented as a population of strings of parameters; initializing the first generation of strings with valid but otherwise random values; evaluating the fitness of each string according to an objective function; creating child strings by selecting parent strings according to their relative fitness, applying crossover and (rare) mutations to parent strings to create child string(s); and repeating the process for subsequent generations (see figure 5 ). fitness is determined by running sufficient numbers of dibble simulations to distinguish statistically significant differences."
"on the basis of analytical theory of spherical space [cit], the direction cosines matrices of b i, b i ′ are built first. the fixed cartesian coordinate frame of the two 3-rrs spherical parallel manipulators is shown in figure 4, where the x-axis and x′-axis are inverses of each other, as are the z-axis and z′-axis and the y-axis and y′-axis. the direction cosines matrices of o b i and o b i ′ in the fixed cartesian coordinate frame o-xyz are as follows:"
"tightly coupled global systems of twentieth-century transportation and communication technologies ensure that cascading effects of economic, public health, and environmental disasters propagate rapidly with potentially dire consequences for citizens of many countries. winter simulation conference and related venues provide opportunities to explore and evaluate state-of-the-art computational systems for sensing, simulating, and guiding such system-of-systems cascading dynamics toward beneficial or at least less harmful directions."
"this work complements the community-level pandemic models of midas and other nature, science, and pnas papers referenced in this paper by providing a lighter, faster model through which regional and global patterns of pandemic diffusion and risks can be evaluated. [cit] . this is also a proof-of-concept for the usefulness of inter-city analyses based on rapid diffusion of pandemic influenza via airline travel. this approach does not yet include analysis of diffusion via slower, localized land travel such as interstate highways and rail systems, and presumes travel behavior would continue as normal during a pandemic. however, because behavior during an actual pandemic would likely be limited voluntarily (even if not mandated by the cdc or other restrictions), thereby reducing the risk to most cities, it would be important to explore the possibility of increased local diffusion due to panic travel or other local/ground travel to nearby cities."
"technological advancements have greatly simplified the measurement and assessment of biopotential signals, particularly electrocardiograms. however, the sites for eeg electrodes are mostly covered with hair, and eeg signals are weaker than those used in other bio-potential measurement tools, which makes the use of dry electrodes in eeg difficult. most dry eeg electrodes make signal measurements by penetrating the outermost layer of the skin, the stratum corneum, using microelectromechanical or carbon nanotube (cnt) techniques [cit] . however, those types of dry electrodes are somewhat invasive, and electrodes that penetrate tissue always carry the risk of infection. in addition, those techniques do not allow eeg signals to be recorded through hair, and therefore, hair and scalp preparation is still required."
"since stewart's initial use of a 6 [cit], this approach has become standard. over the past five decades, the stewart parallel manipulator has been used to make significant contributions to aeronautical research [cit] . even so, there is still the disadvantage of the stewart-type flight simulator in that its posture rotation range is less than 30°. the limitation of the posture range of a stewart parallel manipulator hinders its ability to serve as a flight simulator of a fighter-aircraft. the motion of a fighteraircraft involves continuous 360° rolling frequently. in order to achieve continuous 360° rolling, [cit] presented an innovative motion base as a flight simulator, based on a 6-dof parallel mechanism, called eclipse-ii. the eclipse-ii allows continuous 360° rotation in a, b, and c-axes as well as translational motions in x, y, and z-axes. however, the rotations of the eclipse-ii parallel"
"as shown in tables 2 and 3, the results are obtained numerically by the forward algorithm and analytically by the inverse equations. the time costs of the numerical forward algorithm are less than 3 ms using matlab software by intel core i7-3.2g cpu, thus proving that the algorithm of the iteration meets the typical real-time control requirement of less than 6 ms."
"to this end, we advocate the use of generative adversarial networks (gans) [cit] as image manifold priors. gans have emerged as a powerful, unsupervised technique to parameterize high dimensional image distributions, allowing us to sample from these spaces to produce very realistic looking images. we train the gan to capture the space of all possible reconstructions using a training set of clean ct images. next, we use an initial seed reconstruction using an existing technique such as filtered back projection (fbp) or regularized least squares (rls) and 'clean' it by projecting it onto the image manifold, which we refer to as the gan prior following [cit] . since the final reconstruction is always forced to be from the manifold, it is expected to be artifact-free. more specifically, this process involves sampling from the latent space of the gan, in order to find an image that resembles the seed image. though this has been conventionally carried out using projected gradient descent (pgd) [cit], as we demonstrate in our results, this approach performs poorly when the initial estimate is too noisy or has too many artifacts, which is common under extremely limited angle scenarios."
"a jacobian matrix is a mapping relationship between the angular velocity of active joints and the angular velocity of the moving platform. the establishment of a jacobian matrix is fundamental to the analysis of forward kinematics in this paper. there are many methods for establishing a jacobian matrix. in general, a jacobian matrix is obtained by virtue of derivations of constraining equations. it is troublesome to obtain a jacobian matrix by virtue of derivations if the constraining equations are too (23)"
"for the assr-based abci, 37 and 42 hz were selected as the message frequencies because the optimal modulation frequency for assr has been reported to be around 40 hz. a water stream and insect sound were chosen as alternatives to the conventional pure tone burst to provide a natural and pleasant sensation. also, subjects could easily distinguish the different sound streams."
the direction cosines of a vector are the cosines of the angles between the vector and the three coordinate axes. deriving direction cosines are required in this paper to perform inverse kinematics and obtain the jacobian matrix. a 3-rrs spherical parallel manipulator and its mirror image are shown in figure 4 .
"interventions occur at the start of the simulation, and provide permanent immunity to the disease as though through allocation of effective vaccinations. as in the risk model presented in section 1, the agents are perfectly mixed within each metropolitan area; vaccines are allocated randomly among agents in a given metropolitan area. if the number of vaccines in a given metropolitan area exceeds the population there, the vaccines are considered wasted and do not contribute to halting the pandemic."
"for each simulation we introduce 1 flu case per city, then model flu diffusion within and between cities via airline travel of infected individuals not yet too sick to travel. diffusion within cities is agentbased and calibrated to other midas community level papers and mathematical seir models."
") are obtained via inverse kinematics. in general, the forward kinematics of a parallel manipulator is fairly complicated when using the analytical method, and many solutions are derived. moreover, in practical application, the configuration of a manipulator is just one of the forward solutions. the motion of the flight simulator is continuous and the control system is based on a dynamical model of mimo (multiple input multiple out) that needs to obtain the posture of the moving platform in real-time using the forward equations. thus, an effective numerical method is employed to solve the forward equations. the numerical method was first used in stewart's parallel manipulator [cit] ."
"we gauge the effectiveness of the treatments relative to the no-intervention control case in terms of the ³lqgluhfw surwhfwlrq´ ri wkh lqwhuyhntion, i.e., the decrease in morbidity among members of the population who were not directly targeted by the intervention. the subsequent sections provide more detail on ga optimization, the intervention strategies, and the post-optimization evaluation of interventions"
"this document was prepared as an account of work sponsored by an agency of the united states government. neither the united states government nor lawrence livermore national security, llc, nor any of their employees makes any warranty, expressed or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the united states government or lawrence livermore national security, llc. the views and opinions of authors expressed herein do not necessarily state or reflect those of the united states government or lawrence livermore national security, llc, and shall not be used for advertising or product endorsement purposes."
"although it is not necessary to be able to continuously rotate in all orientations, a flight simulator of a fighteraircraft needs to be able to roll in an additional 360° to perform most motions of a fighter-aircraft. therefore, the purpose of this study is to achieve continuous 360° rotation of a fighter-aircraft flight simulator."
"depth sequential history. depth sequential history feature method is used to observe pixel intensity information in overall sequence of each activity (see figure 4) . it contains temporal values, position, and movement velocities. therefore depth sequential history is defined as where and are the initial and final images of an activity and is the duration of activity period."
"to perform experimentation over msraction3d, we evaluated all 20 actions and examined their recognition accuracy performance based on loso (leave-one-subjectout) cross-subject training/testing mechanism. table 2 shows the recognition accuracy of this dataset."
"the proposed activity recognition system consists of sequence of depth images captured by rgb-d video sensor, background removal, and human tracking from the time-sequential activity video images. then, feature representation based on spatiotemporal features, clustering via k-mean, and training/recognition using recognizer engine are processed. figure 1 explains the overall steps of proposed human activity recognition system."
"the rest of the sections of this paper are structured as follows. section 2 describes the system architecture overview of the proposed system where depth maps preprocessing, feature extraction techniques, and training/testing human activities using hmm are explained. in section 3, we explain experimental results by considering proposed and state-ofthe-art methods. finally, section 4 presents the conclusion."
"standard deviation is computed as the sum of all the differences of the image pairs with respect to the time series (see figure 5 ). it provides quite disperse output and hidden values (i.e., especially coordinates) having large range of intensities values"
"we compare our spatiotemporal features method with the state-of-theart methods including body joints, eigenjoints, depth motion maps, and super normal vector features using depth images. it is cleared in table 1 that the spatiotemporal features achieved highest recognition rate as 63.7% over the state-of-the-art methods."
"for spatiotemporal features extraction, we composed features as depth history silhouettes, standard deviation, motion variation among images, and optical flow for depth shape features, while joints angle and joints location features are derived from joints points features. combination of these features explores more spatial and temporal depth-based properties which are useful for activity classification and recognition. all features are explained below."
"second is public depth database as msraction3d dataset and third is msrdailyactivity3d dataset. in the following sections, we explain and compare our method with other state-of-the-art methods using all three depth datasets."
"each feature vector of individual activity is symbolized based on k-mean clustering algorithm. however, a hmm consists of finite states where each state is involved in transition probability and symbol observation probability [cit] . during hmm, the underlying hidden process is observable by another set of stochastic processes that provides observation symbols. in case of training each activity, initially, hmm is trained having a size of codebook of 512. during har, trained hmms of each activity are used to choose maximum likelihood of desired activity [cit] . however, sequence of trained data is generated and maintained by buffer strategy [cit] . figure 10 describes the transition and emission probabilities of cleaning hmm after training."
"recognition accuracy only joints position features [cit] 68.0 moving pose [cit] 73.8 motion features [cit] 79.1 hybrid features [cit] 85.3 actionlet ensemble [cit] 85.7 super normal vector [cit] 86.2 volumetric spatial features [cit] 89.7 spatiotemporal features learning [cit] 90.4 proposed method 93. 2 experimental results, these features are applied over proposed im-dailydepthactivity, msraction3d, and msrdailyactivity3d datasets, respectively. our proposed activity recognition system shows superior recognition accuracy performance as 63.7% over the state-of-the-art methods using our depth annotated dataset. in case of public datasets,"
"joints angle features. due to similar or complex postures of different activities, it is not sufficient to just deal with silhouettes features; therefore, we developed skeleton model having 15 joints' points information (see figure 8) . however, joints angle features measure the directional movements of the th joints points between consecutive frames [cit] and − 1 as"
"finally, we reported the comparison of recognition accuracy over the msrdailyactivity3d dataset where the proposed method shows superior recognition rate over stateof-the-art methods in table 5 ."
"motion-based optical flow features. in order to make use of the additional motion information from depth sequence, we applied optical flow technique based on the lucas kanade method. basically, it calculates the motion intensity and directional angular values between two images. figure 7 shows some samples of optical flows calculated from two depth silhouettes images."
"the proposed method is evaluated on three challenging depth videos datasets. first is our own annotated depth dataset known as im-dailydepthactivity [cit] . it includes fifteen types of activities as sitting down, both hands waving, bending, standing up, eating, phone conversation, boxing, clapping, right hand waving, exercise, cleaning, kicking, throwing, taking an object, and reading an article. during experimental evaluation, we used 375 videos sequences for training and 30 unsegmented videos for testing. all videos are collected in indoor environments (i.e., labs, classroom, and halls) performed by 15 different subjects. figure 11 shows some depth activities images used in imdailydepthactivity dataset."
recognition accuracy bag of 3d points [cit] 74.7 shape and motion features [cit] 82.1 eigenjoints [cit] 82.3 spatiotemporal motion variation [cit] 84.6 stop features [cit] 84.8 joints plus body features [cit] 85.6 actionlet ensemble [cit] 88.2 cuboid similar features [cit] 89.3 spatial and temporal part-sets [cit] 90.2 pose-based features [cit] 91. figure 13 . table 4 shows the accuracy performance of 16 different human activities that is obtained from the proposed spatiotemporal features method over the specific dataset.
depth motion identification. motion identification feature mechanism is used to handle intra-/intermotion variation and temporal displacement (see figure 6 ) among consecutive frames of each activity.
"in this paper we focus on an integration of tggs and ocl as a foundation for model transformation. incorporating ocl conditions in triple rules allows us to express better transformations. our approach targets both declarative and operational features of transformations. within the approach a new method to extract invariants for tgg transformations is introduced. we propose a specification method of transformations and an ocl-based framework for model transformation. we realize the approach in use [cit], a tool with full ocl support. this offers an on-the-fly verification of transformations and means for quality assurance of transformations."
"network initialization: the macro base station bs is arranged at the center of the rectangular area of length x 0 and width y 0, and the coverage radius is r b, and the user distribution of 9 different scenarios is created: three kinds of low load, three medium loads and three high loads, the three cases are equally likely to occur. the number of low-load, medium-load, and high-load active users is m 1, m 2, and m 3, respectively. all users are distributed uniformly, and all users and base stations adopt sinr correlation."
"where x and y represent the vertex of the top left corner of the roi, w is the width of the roi, h is the length of the roi, and w1 and w2 are the reserved values of the rectangular roi in the x direction."
"let us explain the central steps of the transformation development method that we propose: (1) models as the input and output of transformations are defined by metamodels together with ocl restriction. (2) our transformation models are established based on the incorporation of triple rules and ocl at two levels, declarative and operational specification of transformations. (3) for a transformation quality assurance framework, the reasonable assumptions of transformations in our focus include the following ocl properties: model properties, invariants of transformations, well-formedness of models, and pre-and postconditions of transformation operations."
"accordingly, we can obtain the distributions of disx and disy for various weld joint images. figures 12-15 show the disx and disy of the lap weld joint and splice weld joint."
"for this analysis, 600 weld joint images, including 120 images for each type of joint, were selected as the training images. a total of 250 weld joint images were selected for testing: images 1-50 were images of lap weld joints, labelled with a value of 1; images 51-100 were the images of butt weld joints, labelled as 2; images 101-150 were images of splice weld joints, labelled as 3; images 151-200 were images of fillet weld joints, labelled as 4; and, images 201-250 were images of v-type weld joints, labelled as 5. all of the experiments that were reported in this paper were performed on a computer with an intel i5-6500 cpu, with a main frequency of 3.2 ghz and 8 gb of ram."
"currently, the most commonly applied classification algorithms include logistic regression, the k-nearest-neighbour algorithm, the decision tree algorithm, bayesian classification, the svm algorithm, and neural networks. the svm results were compared with the results of other classification methods to verify the effectiveness and applicability of the svm method selected in this paper. logistic regression is mainly used for binary classification, since the number of weld types considered here is greater than two, logistic regression is not a suitable classification algorithm for the problem of interest. bayesian classification is based on the premise that the posterior probability can be obtained with the prior probability. bayesian classification is not suitable for the problem of interest because it is difficult to find a suitable prior probability for the problem considered in this figure 19 . results of weld joint identification using the reference [cit] . table 3 . recognition results obtained with different feature extraction algorithms."
"3) check model properties: the declarative language ocl allows us to navigate and to evaluate queries on models. therefore, we can employ ocl to express properties of models at any specific moment in time. for"
"maximizing network energy efficiency and minimizing electromagnetic radiation intensity while ensuring the quality of individual user services is the purpose of researching the green deployment of ultra-dense network micro base stations. in order to establish its corresponding mathematical model, referring to the wireless sensor model [cit], the relevant research model, the definition of user service quality, network energy efficiency and electromag-netic radiation intensity are given."
"l2: the visual sensor arrive the weld starting point-the welding torch arrive the weld starting point determine whether the visual sensor has captured the weld joint image. the visual sensor will identify the weld joint types when the weld joint image is captured. at this stage, the welding torch does not arrive the starting point of the weld joint and does not perform welding. therefore, the obtained image is without splash and other interference."
the rest of this paper is structured as follows. section ii explains our basic idea. section iii focuses on the formal foundation for specification and realization of transformations. section iv explains an ocl-based framework for transformation quality assurance. section v shows how the approach is realized in the use tool. section vi comments on related work. the paper is closed with a conclusion and a discussion of future work.
"we have introduced an approach for quality assurance of model transformations: (1) the foundation of the approach is based on the integration of tggs and ocl. we have further formulated operation contracts for derived triple rules in order to realize them as ocl operations with the two views: declarative ocl pre-and postconditions are employed as operation contracts, and imperative command sequences are taken as an operational realization. (2) both declarative and operational views are obtained by an automatic translation from the specification of transformations into the rtl language. (3) this work also embodies a new method to extract invariants for transformations. the central idea is to view transformations as models. (4) an ocl-based framework for model transformation is established. as being realized on a full ocl support environment like use, the framework offers a support for validation and verification of transformations."
"each method has its advantages. a single parameter set might not be appropriate for all ĸ(ĸ − 1)/2 models. however, the overall accuracy is the ultimate consideration, and individual sets of model parameters may lead to over-fitting of the overall model. therefore, the second strategy is adopted in this paper, the same set of parameters is used for all of the models, and the parameters are set based on the overall performance."
"according to the above analysis, the abrupt differences of laser stripe at weld joints in image can be used as features to describe and, thus, recognize different weld joints."
"the infeasible solution set update method in the dualpopulation mechanism only considers the constraint condition and retains the individuals with less constraint violation. however, individuals with less constraint violations are likely to have poor objective function values, that is, not at the edge of the feasible domain, and it is difficult to provide excellent information for individual evolution, thus failing to promote the convergence of the population to the feasible domain edge. therefore, when selecting the infeasible solution, individuals who are close to the feasible domain edge and have better target values should be selected. hence, this section comprehensively considers the constraints and target values to improve the ways of comparing the inferior individual's advantages and disadvantages. the specific judgment methods are as follows: suppose n (x i ) represents the number of violations of the individual x i, and g(x i ) represents the constraint violation of the individual x i . if the vector"
"in order to fully prove the effectiveness of the cmdsa-based heterogeneous cellular network micro-base station green deployment algorithm, the algorithm is compared with two other excellent algorithms. due to the green deployment of micro base stations, most scholars only study a single object. therefore, this paper selects the constrained single-objective heeda algorithm [cit] and b algorithm which have better performance."
"we added a new weld joint type to the model to verify the robustness of the proposed weld recognition algorithm. the new weld joint type is \"filler layer weld joint\". figure 20 shows a laser curve image of such a filler layer weld joint. during the actual welding process, the welding voltage and current are lower than those in the bottom layer, because one layer has already been welded, and the swing range of the welding torch should be smaller. thus, the weld characteristics meet the needs of weld joint recognition, as presented in this paper. with this addition of this weld joint type, the final recognition accuracy of the new weld joint recognition system is 98.1%. when compared with the previous performance, the accuracy of the proposed system is reduced by only 0.3%, thus showing that the proposed joint recognition system exhibits good robustness."
"the k-nearest-neighbour algorithm recognizes the weld joint type by calculating the euclidean distances between the feature vector of the weld joint to be recognized and the feature vectors of all known joint images. in the decision tree algorithm, a decision tree is recursively generated by selecting features as criteria for node splitting. the traditional id3 algorithm was used in this experiment. table 4 shows the final experimental results. the experimental results show that the joint recognition system that is based on the proposed svm model is superior to the k-nearest neighbour algorithm in terms of both recognition accuracy and computational cost. by contrast, the time cost of the decision tree algorithm is lower than that of the svm algorithm by 9.88 ms; however, its accuracy rate is only 90.3%. therefore, based on comprehensive consideration of the computational cost and accuracy, we conclude that the svm model that is presented in this paper is also superior to the decision tree algorithm."
"we focus on the sc2eha transformation between a statechart and an extended hierarchical automaton in order to illustrate our approach. models in this example [cit] represent a traffic supervisor system for a crossing of a main road and a country road (fig. 1) . the lamp controller provides higher precedence to the main road as follow: if more than two cars are waiting at the main road (this information is provided by a sensor), the lamp will be switched from red to redyellow immediately, instead of a waiting period as usual. a camera allows the system to record cars running illegally in the crossing during the red signal."
"l3: the welding torch arrive the weld starting point-the visual sensor arrive the weld ending point. when the welding torch arrive the weld starting point, the welding torch starts welding. the image processing method of weld joint location and relevant welding parameters used in this stage are based on the identification result in l2. in this stage, there is a large amount of splash interference in the image that was captured by the camera."
"during image preprocessing, the laser curve in the image might be interrupted, as shown in figure 8d . therefore, to ensure the accuracy of the subsequent feature vector extraction and fitting of the laser curve in the image, in this paper, the laser curve in the image is fitted by using equation (5) to calculate the slope:"
"the laser curve slope distribution can be used as an important feature for recognizing the weld joint type; however, there are some weld joints that are very different in appearance, but have similar slope distributions, such as fillet weld joints and v-type weld joints. therefore, it is necessary to select a feature as a condition to identify the type of weld."
"many approaches have been proposed for model transformation. most of them are in line with the standard qvt [cit] such as atl [cit] and kermeta [cit] . like our work, they allow the developer to precisely present models using metamodels and ocl. the advantage of our approach is that it is based on the integration of tggs and ocl, which allows the developer to automatically analyze and verify transformations, and supports for bidirectional model transformation."
"figure 10a,b show the final laser curves slope distributions of the lap weld joint and splice weld joint images. on this basis, the laser curve slope feature can be defined as equation (8)."
"in order to further determine the more representative excellent infeasible solutions, the update method of the infeasible solution set is improved as follows: first, the newly generated infeasible solution is combined with the original infeasible solution, and the above judgment method is not feasible. the solution is sorted and retained in the new infeasible solution set from small to large according to the level. if the number of new infeasible solution sets is greater than n 2, equation (16) is changed to (17), and the cycle deletion strategy based on harmonic distance is used. delete the number of new infeasible solution sets to n 2 ."
"l4: the visual sensor arrive the weld ending point-the welding torch arrive the weld ending point when the visual sensor captures the welding ending point, the welding scanning is completed. once the welding torch reaches the welding ending point, welding is completed."
"where feature point 1 x, feature point 1 y, feature point 2 x, and feature point 2 y represent the x and y coordinates of feature point 1 and feature point 2, respectively, in the pixel coordinate system. as shown in figure 11, the slope of the laser curve changes only at the edge of the weld joint. therefore, in this paper, feature point 1 is defined as the first pixel point at which the slope is greater than 1 and feature point 2 is defined as the last pixel point at which the slope is greater than 1, as follows:"
"in this paper, we proposed an algorithm to address the low adaptability and automation of traditional weld joint feature extraction algorithms that are based on visual tracking sensors for determining welding parameter configurations in multi weld joint type environments. based on images that were captured by a visual tracking sensor, a weld joint type recognition algorithm that considers the slope distribution at points along the laser curve and the distance between feature points of a weld joint to construct the feature vector for svm classification is proposed. the following conclusions can be drawn from the experimental results:"
"a. considering the k-th scene distribution, calculating the distance between all users and the micro base station and the macro base station, and the user rate c k when only the macro base station is deployed."
"according to the final experimental results, the recognition accuracy achieved 98.4%, and the computational cost of single-image recognition is 148.23 ms. figure 18 shows the recognition results for various weld joint images. figure 18 . results of weld joint identification using the proposed method. figure 18 . results of weld joint identification using the proposed method."
"b. according to the distance and the coverage radius r i of each base station, it is determined which base stations are covered by the user and the number of users covered by each micro base station, the number of users covered by the micro base station i is m i, and the number of users covered by the macro base station is m 0 . volume 8, 2020 c. calculate the average electromagnetic radiation intensity under scene k according to (6) ."
"in this paper, we proposed an algorithm to address the low adaptability and automation of traditional weld joint feature extraction algorithms that are based on visual tracking sensors for determining welding parameter configurations in multi weld joint type environments. based on images that were captured by a visual tracking sensor, a weld joint type recognition algorithm that considers the slope distribution at points along the laser curve and the distance between feature points of a weld joint to construct the feature vector for svm classification is proposed. the following conclusions can be drawn from the experimental results:"
"where ki represents the slope at point pi on the laser curve in the image and pi−1 to pi+5 represent the adjacent points of pi. the slopes at the laser curve for each joint type are mainly distributed in the range from 0-5. figure 9a,b show the lap weld joint image and splice weld joint. for statistical convenience, we define the slope distribution, as follows:"
"a visual sensor is installed on the end effector of the robot. for communication, there are cable connections between the auxiliary welding equipment and the robot controller, between the computer and the robot controller, and between the robot controller and the welding robot, and the visual sensor transmits image data to the computer through a usb connection."
"we compared the proposed feature extraction algorithm for weld images with the weld image feature extraction method presented in reference [cit] to verify the effectiveness and superiority of the feature extraction method proposed in this paper. the cited paper presents a comparison with previously proposed weld joint feature extraction algorithms and it shows that the selected method achieves better recognition accuracy with a lower computational cost; thus, it can be used as an object of comparison in this paper. we used both feature extraction algorithms to extract feature vectors with which to build svm models. table 3 shows the final recognition accuracies and computational costs of the two methods. figure 19 shows the results of weld image recognition."
"according to the above analysis, the abrupt differences of laser stripe at weld joints in image can be used as features to describe and, thus, recognize different weld joints."
"finally, we can establish the mapping matrix between a point in the image coordinate system and the corresponding point in the robot base coordinate system through the coordinate transformation model, that is:"
"according to the final experimental results, the recognition accuracy achieved 98.4%, and the computational cost of single-image recognition is 148.23 ms. figure 18 shows the recognition results for various weld joint images."
where p 0 and rc 0 are the nominal transmit power and corresponding coverage radius of the base station. the calculation formula of the average electromagnetic radiation intensity in the whole area is shown as:
"according to the above analysis, the abrupt differences of laser stripe at weld joints in image can be used as features to describe and, thus, recognize different weld joints."
"where n is the number of small cell base station sc, p b0 and p bt represent the static energy consumption and dynamic energy consumption of the macro base station, respectively, p b0 represents the static energy consumption of the micro base station, p it is the transmission power of the i-th micro base station, and u represents the user set in the network."
"in addition, considering the influence of the deleted crowd with small crowding distance on the remaining individuals, referring to the cyclic crowding distance strategy, propose a cyclic harmonic distance deletion strategy instead of the final step in the feasible solution set update, as follows: step 1, according to (16), calculate the harmonic distance of the individual to be screened; step 2, delete the individual with the smallest harmonic distance; step 3, repeat steps 1 and 2 until the number of remaining individuals meets the requirements."
"analysis (21) can be seen: first, in view of the fact that the dolphin swarm algorithm sets three search strategies to match different optimization problems, (21) retains this idea, provides two search strategies, and automatically selects one to explore the new location according to the change of p; second, using the characteristics of the constrained multi-objective problem, that is, there are multiple non-dominated sorting solutions in the early stage of evolution, that is, most solutions are not close to the real pareto frontier, and most solutions are in the optimal sorting level in the late stage of evolution. in (21), in the early stage of evolution, most individuals with poor grades learn from the better grades, and can quickly approach the real pareto front, while at the same time let the excellent infeasible solutions participate in evolution to increase population diversity; by applying variability perturbations to individuals with superior levels, more excellent solutions can be explored, making them evenly distributed at the front of pareto."
"all of the weld joint images that were considered in this paper were obtained while using our independently developed visual sensor, as shown in figure 1 . (2) a weld joint type identification model that is based on an optimal svm is constructed to identify various kinds of weld joints. (3) a comprehensive study on the feature extraction, recognition, experimental comparison, and discussion of weld joint images is performed to handle the weld joint recognition task of the arc welding systems."
"for this analysis, 600 weld joint images, including 120 images for each type of joint, were selected as the training images. a total of 250 weld joint images were selected for testing: images 1-50 were images of lap weld joints, labelled with a value of 1; images 51-100 were the images of butt weld joints, labelled as 2; images 101-150 were images of splice weld joints, labelled as 3; images 151-200 were images of fillet weld joints, labelled as 4; and, images 201-250 were images of v-type weld joints, labelled as 5. all of the experiments that were reported in this paper were performed on a computer with an intel i5-6500 cpu, with a main frequency of 3.2 ghz and 8 gb of ram."
"where feature point 1x, feature point 1y, feature point 2x, and feature point 2y represent the x and y coordinates of feature point 1 and feature point 2, respectively, in the pixel coordinate system. as shown in figure 11, the slope of the laser curve changes only at the edge of the weld joint."
"step 5: determine if the termination condition is reached. if it is reached, the pareto optimal solution in the feasible solution set is output, and the algorithm terminates; otherwise, return to step 2 and continue searching."
"population initialization: the initial population pop is randomly generated. the 1∼ n dimension of the individual represents the abscissa of the location of the micro base station, and the n +1∼2 n dimension represents the ordinate of the location of the micro base station, respectively, and the 2 n +1∼3 n represent the coverage radius of the micro base station, respectively."
"according to (21), the deployment scheme of the micro base station is updated, and the fitness value and the constraint violation degree of each body are calculated, and the feasible solution set f and the infeasible solution set if are updated according to section iii."
"the laser curve in the image must be extracted to extract the feature vector. therefore, weld joint image preprocess include noise processing, laser curve extraction, region of interest (roi) extraction, and laser curve fitting."
"in order to ensure the user's data transmission rate, we use the user rate as the evaluation standard of user service quality. assuming that the total bandwidth of the system is w, the bs and the sc share a section of spectrum, so there is inter-layer interference between the macro base station and the micro base station, and there is intra-layer interference between the micro base station and the micro base station due to the use of resources of the same frequency. all user terminals are allocated to the same bandwidth resources on different carrier frequencies, so interference between user terminals can be ignored. therefore, the user rate c(u,n) (bit/s) of the user u communicating with the base station n can be written as:"
"it is injective, if the morphisms s, c and t are injective. triple graphs and triple graph morphisms form the category triplegraph. example. triple graph: the graph in fig. 3 shows a triple graph containing a statechart together with correspondence nodes pointing to the extended hierarchical automata (eha). references between source and target models denote translation correspondences."
"a comprehensive analysis of fig. 7 and table 4 can be drawn: first, under the same user distribution, the network energy efficiency of the algorithm is slightly lower than that of the heeda algorithm. this is because the algorithm is limited by the intensity of electromagnetic radiation and needs to consider two indicators comprehensively. heeda only considers network energy efficiency factors when optimizing the deployment of micro base stations. second, under the same user distribution, the electromagnetic radiation intensity of the proposed algorithm is significantly lower than that of the heeda algorithm, and the average user rate of the proposed algorithm is higher than the heeda algorithm. therefore, the algorithm of this paper can comprehensively consider two evaluation indexes of network energy efficiency and electromagnetic radiation intensity under the condition of guaranteeing user speed, which has practical application value. third, with the in-crease of the number of users, the network energy efficiency of the two algorithms increases to a stable level, which also indicates that the algorithm optimization scheme can better adapt to the traffic demand under high load."
"as shown in figure 7, regarding the relative position of the visual sensor, the welding torch and the workpiece, the welding process is divided into four stages: l1: the robot start moving-the visual sensor arrive the weld starting point set the scanning path before the welding operation and use the visual sensor to find the starting point of the weld based on the imaging difference between the weld joint image and background image. at this stage, the visual sensor has not captured the weld joint image and the welding torch does not start welding."
"2) validation: the validation of a transformation is the process of applying the transformation in various scenarios and comparing the de facto result with the expected outcome. the process cannot be fully automated: the modeler often has to define relevant scenarios together with the expected outcome, so-called test cases, and then to compare the obtained and expected result. the process often depends on a semi-automated solution, where test cases may be generated automatically, the execution may be animated and debugged, and the difference between the result and the expected outcome may be highlighted."
"in the hunting stage of the dolphin swarm algorithm, according to the relationship between rs 2, dk i and dkl i, it is divided into three kinds of position update strategies. it is necessary to compare the fitness value, but the constrained multi-objective problem has multiple targets, and it is impossible to judge the individual superiority directly through the volume 8, 2020"
where sx i and sy i are the horizontal and vertical coordinates of the i-th base station. the coverage radius rc i of each micro base station may take values within rc min and rc max and may be different from each other.
"all of the weld joint images that were considered in this paper were obtained while using our independently developed visual sensor, as shown in figure 1 . the visual sensor is equipped with a complementary metal-oxide-semiconductor (cmos) camera, the parameters of which are shown in table 1 . the main function of the laser is to project a laser curve on the workpiece for features extraction and three-dimensional (3d) reconstruction. the partition is designed to isolate splash to reduce the noise in the image. the laser wavelength that was used in this paper as 650 nm and the laser power was 30 mw. the adjustable width of the fringes ranges from 0.5 mm to 2.5 mm. the penetration of filters is 80%; the filters filter splash and light. the focus length is 12 mm. the distance between camera and welding torch is 200 mm. in a real welding environment, the posture of the visual sensor will be adjusted in real time to keep it perpendicular with the weld joint, so that the laser curve is vertical in the weld joint image, that is, the visual sensor is oriented the same way relative the path, even for curved paths."
"the parameter setting is consistent with the previous one. the proposed algorithm and b algorithm optimize the deployment of the micro base station respectively, and obtain two sets of pareto optimal solutions. the multi-objective decision is used to select the better three sets of schemes. the specific results are shown in table 5 ."
"image features determines the recognize accuracy and applicability of a weld joint recognition system. in this paper, we take full advantage of the laser stripe deformation information in the image as the features for identifying the weld joint type. figure 6 outlines the extraction process."
"at a weld joint, the laser curve will exhibit deformation, which will result in changes in the slope of laser curve. additionally, the slope distributions of the laser curve that are observed in images of different types of weld joints are different. therefore, the slope distribution of the laser curve in the image can be used as an effective basis for weld joint type recognition. we define the slope of a point at the laser curve as:"
"where p n is the transmission power of the base station n, the channel gain mainly considers the path loss, d un is the distance from the user u to the base station n, α is the path loss factor, generally takes 2, n 0 is the noise power spectral density, and w u represents the bandwidth of the user u."
"we compared the proposed feature extraction algorithm for weld images with the weld image feature extraction method presented in reference [cit] to verify the effectiveness and superiority of the feature extraction method proposed in this paper. the cited paper presents a comparison with previously proposed weld joint feature extraction algorithms and it shows that the selected method achieves better recognition accuracy with a lower computational cost; thus, it can be used as an object of comparison in this paper. we used both feature extraction algorithms to extract feature vectors with which to build svm models. table 3 shows the final recognition accuracies and computational costs of the two methods. figure 19 shows the results of weld image recognition. figure 19 . results of weld joint identification using the reference [cit] ."
"in addition, since the call and reception phases are only used to update the neighborhood optimal solution k i in a single-objective dsa, the cmdsa algorithm will no longer calculate the call and reception phases."
"in summary, cmdsa has certain advantages in distribution and convergence compared with other three algorithms. we have reason to believe that the cmdsa algorithm can better solve the complex constrained multi-objective problem of the micro-base station green deployment problem as described in (7) ."
"finally, we can establish the mapping matrix between a point in the image coordinate system and the corresponding point in the robot base coordinate system through the coordinate transformation model, that is:"
where s i represents the slope of the ith fit line and m i and n i denote the endpoints of the interrupted laser curve in the image. the disconnected curves can be determined by row scanning. figure 8e shows the line fitting results.
"in the next step, we can conduct further research in the following aspects: first, how to determine the optimal number of micro base stations and reduce the complexity of deployment problems. second, because we do not have data on actual user distribution, we use a uniform distribution to simulate user distribution in this paper. if there is data on actual user distribution, it will better reflect the network status and facilitate the practical application of the algorithm. third, when establishing a constrained multi-objective model for micro base station deployment, economic factors such as deployment costs and site rental costs can be taken into account."
". this is what we need to prove. for the backward, and integration transformation between g s and g t, we can obtain a similar result. the (ii) condition in this case is shown respectively as follow. fig. 5 . a forward transformation step by the forward rule derived from the rule shown in fig. 4 example. figure 5 shows a transformation step for the forward transformation from a statechart to an eha model. the forward rule is derived from the rule shown in fig. 4"
"the remaining chapters of this paper are organized as follows. section ii establishes a mathematical model for the green deployment method of micro base stations in ultra-dense heterogeneous cellular networks. section iii introduces the principle of the proposed multi-objective dolphin swarm algorithm. section iv introduces the principle of green deployment method of micro base station based on constrained multi-objective dolphin swarm algorithm. section v gives experimental simulation and analysis, and summarizes the full text at the end."
"as shown in figure 7, regarding the relative position of the visual sensor, the welding torch and the workpiece, the welding process is divided into four stages:"
"2) animation of transformation: after each transformation step, we can see the combination of the source, correspondence, and target part as a whole model. we could employ ocl expressions in order to explore such a model. mappings within the current rule application can be highlighted by ocl queries. this makes it easier for the modeler to check if the rule application is correct."
"our approach for verification and validation of transformation is realized with the support of use [cit], which is a tool for analysis, reasoning, verification and validation of uml/ocl specifications. specifically, use allow us to check class invariants, pre-and postconditions of operations, and properties of models, which are expressed in ocl. in use system states are represented as object diagrams. system evolution can be carried out using operations based on basic state manipulations, such as (1) creating and destroying objects or links and (2) modifying attributes. in this way a framework for model transformation based on the integration of tggs and ocl are completely covered by use. figure 7 shows metamodels for the sc2eha transformation in use. due to space limitations, the full realization for the transformationis only shown in the long version of this paper [cit] ."
"where d i,1, d i,2, · · ·, d i,n 3 is the euclidean distance of the n 3 individual closest to the infeasible solution x i in the target space."
"f. considering all the distribution scenarios, the network energy efficiency and the average electromagnetic radiation intensity are calculated in the form of probability weighting according to (7) ."
"the laser curve in the image must be extracted to extract the feature vector. therefore, weld joint image preprocess include noise processing, laser curve extraction, region of interest (roi) extraction, and laser curve fitting."
"the dolphin swarm algorithm itself does not have the ability to deal with multi-objective optimization problems. to this end, we improve it and combine it with the dual-population search mechanism to propose a constrained multi-objective dolphin swarm algorithm."
"figure 10a,b show the final laser curves slope distributions of the lap weld joint and splice weld joint images. on this basis, the laser curve slope feature can be defined as equation (8)."
"where pareto(f i ) represents the non-dominated ranking of the feasible solution f i, and k ' i represents the individuals randomly selected in the optimal ranking. r is a random number on [cit], rand is a random number on [cit], and p is taken as (20) ."
"during image preprocessing, the laser curve in the image might be interrupted, as shown in figure 8d . therefore, to ensure the accuracy of the subsequent feature vector extraction and fitting of the laser curve in the image, in this paper, the laser curve in the image is fitted by using equation (5) to calculate the slope:"
"in this study, we recognize the welding joint type before welding begins. however, there might be multiple welding robots working together in an actual welding workshop. in this case, the other welding robots affects the welding joint image, and the arc light and splash generated by other welding robots appear in the image. we input the image with splash and arc light into our welding system to test the effectiveness of the proposed algorithm in the case of external noise interference. the results show that the proposed algorithm can effectively process an image with splash and arc light, as shown in figure 21 ."
"this paper studies a two-layer heterogeneous cellular network consisting of a macro base station (bs) and a small cell base station (sc), as shown in fig. 1, and based on the following assumptions and definitions."
"(2) an image feature extraction method is proposed to extract two kinds of feature information, which can increase the recognition information and improve the recognition accuracy."
"consider the distribution of users under low load, medium load, and high load, respectively. the number of micro base stations is 30, the number of population is 100, and the maximum number of iterations is 200. other parameters are consistent with the above. since the heeda algorithm belongs to the constrained single-objective algorithm for optimizing network energy efficiency, this experiment first optimizes the algorithm and heeda algorithm on the network energy efficiency model, and then calculates the average electromagnetic radiation intensity by using the optimal solution optimized by heeda algorithm. fig. 7 shows the relationship between the network energy efficiency of this algorithm and heeda algorithm with the number of iterations under three load scenarios. the objective function values corresponding to the optimal solution obtained by the heeda algorithm are shown in table 4 ."
"the slope of the laser curve will jump twice in the weld joint image. as shown in figure 10, the positions of the two jumps are marked as feature point 1 and feature point 2. there is a position difference between feature point 1 and feature point 2 in the x and y directions due to the differences in relative position between the two welding work-pieces. we define dis x as the pixel difference in the x direction and dis y as the pixel difference in the y direction, as shown in figure 11 . these distances can be calculated as"
"the slope of the laser curve will jump twice in the weld joint image. as shown in figure 10, the positions of the two jumps are marked as feature point 1 and feature point 2. there is a position difference between feature point 1 and feature point 2 in the x and y directions due to the differences in relative position between the two welding work-pieces. we define disx as the pixel difference in the x direction and disy as the pixel difference in the y direction, as shown in figure 11 . these distances can be calculated as"
"section ii describes the line fitting results. we already know that the weld joint type can be determined based on the abrupt differences of the laser curve at the weld joint in the image. when considering the image characteristics, we propose a method using the laser curve distribution in the image and the weld joint feature point intervals as the basis of a characteristic description to recognize different weld joints types."
"this paper 98.4% 148.23 ms reference [cit] 89.2% 165.62 ms a comparison reveals that the recognition accuracy and computational cost of the weld image feature extraction algorithm that was proposed in this paper are better than those of the feature extraction method presented in reference [cit] . the vertical distance from the groove to the surface of the weld joint is used to construct the feature vector. this feature vector is relatively simple. consequently, this method is more suitable for weld joint types or weld grooves that exhibit large differences. weld joints that do not show significant groove differences will be difficult to distinguish. the method of reference [cit] misidentifies some lap weld joints, splice weld joints, and fillet weld joints, because the weld grooves of these three types of joints are relatively small, which makes it difficult to differentiate among them, as shown in figure 19 . by contrast, the method that was proposed in this paper not only utilized the characteristics of the weld joint, but also accounted for laser deformation, which made it suitable for a wider scope of applications and enabled it to achieve a higher accuracy rate."
"the existing dual-population search mechanism usually adopts the following methods to perform feasible solution set updating: first, the feasible solution set is merged with the newly generated feasible solution, and the population is sorted by fast non-dominated sorting; then, the scale is from small to large. the order sequentially puts each level of individuals into a new feasible solution set until the total number of individuals of the previous several levels exceeds the capacity n 1 of the feasible solution set; finally, the individuals with smaller crowding distances are removed until the number of feasible solutions reaches n 1 . as shown in fig. 5, the above final step has certain defects: first, for two individuals a and b of the same dominant level, sorting according to the crowded distance will delete the individual b while retaining the individual a, but from the overall distribution of the population. in view, the individual a is more dense around, should delete a; second, for individuals c and d, they have the same crowded distance, so they will be deleted or retained at the same time. if they are deleted, the overall distribution will not improve. instead, it will fall."
"where k i represents the slope at point p i on the laser curve in the image and p i−1 to p i+5 represent the adjacent points of p i . the slopes at the laser curve for each joint type are mainly distributed in the range from 0-5. figure 9a,b show the lap weld joint image and splice weld joint. for statistical convenience, we define the slope distribution, as follows:"
"determine whether the termination condition is satisfied. if yes, output the pareto optimal solution in the feasible solution set, and the algorithm terminates. otherwise, return to search phase and continue searching."
"(1) a weld joint type identification method that is based on the deformation information of a laser curve in the image is proposed to improve the welding system adaptability and automation degree. (2) a weld joint type identification model that is based on an optimal svm is constructed to identify various kinds of weld joints. (3) a comprehensive study on the feature extraction, recognition, experimental comparison, and discussion of weld joint images is performed to handle the weld joint recognition task of the arc welding systems."
"it is difficult to recognize a weld joint that is based on another part of the image, except for the part near the laser stripe. (2) the laser stripe in the weld joint image has the characteristics of discontinuity, grey value change, and deformation. for different weld joints types, the degrees of deformation and the change of grey value of the laser stripe in the image also differ."
"where ki represents the slope at point pi on the laser curve in the image and pi−1 to pi+5 represent the adjacent points of pi. the slopes at the laser curve for each joint type are mainly distributed in the range from 0-5. figure 9a,b show the lap weld joint image and splice weld joint. for statistical convenience, we define the slope distribution, as follows:"
"figure 10a,b show the final laser curves slope distributions of the lap weld joint and splice weld joint images. on this basis, the laser curve slope feature can be defined as equation (8)."
"according to the final experimental results, the recognition accuracy achieved 98.4%, and the computational cost of single-image recognition is 148.23 ms. figure 18 shows the recognition results for various weld joint images. the five weld joint types that were considered for recognition in this paper were lap weld joints, butt weld joints, splice weld joints, fillet weld joints, and v-type weld joints. table 2 shows the parameters of each weld type."
"in general, energy efficiency is defined as the ratio of the total rate of the system to the total power consumed, which is the number of bits that can be transmitted in energy per unit of joule, in bits/ j. the power consumption of the base station includes static energy consumption p 0 and dynamic energy consumption p t, wherein p 0 is the energy consumed by the circuit of the base station itself, regardless of the working state of the base station, and p t is the transmitting power of the base station. obviously, the network energy efficiency of the system involved at a certain moment in this paper is shown as:"
"svms are mainly used for binary classification; however, there are two main schemes that can be used to adapt them for multi-class classification. the first is the one-to-many scheme, in which multiple classifications are performed, each for identifying whether the samples belong to one particular category. therefore, for k classes of samples, it is necessary to construct k svms. classification is achieved by comparing the distances between each of the input samples and the hyperplane in each svm. the second scheme is the one-to-one scheme, which is based on svms for distinguishing between two particular classes of samples. therefore, k(k − 1)/2 svms are needed for the k sample classes. the category to which each sample belongs is determined by voting to make the classification decision. the samples are input into all svms, and the results of the individual decisions are then counted. the category with the most votes for a given sample is determined to be the category of that sample. in this paper, a total of 10 svms are designed for classifying five types of weld joints while using the second scheme."
", where tri i are integration rules and m i are triple matches. in this way the transformation can be better animated for the modeler."
"image features determines the recognize accuracy and applicability of a weld joint recognition system. in this paper, we take full advantage of the laser stripe deformation information in the image as the features for identifying the weld joint type. figure 6 outlines the extraction process."
"ultra-dense heterogeneous cellular networks (udhcn) are one of the key technologies of 5g [cit] . it is implemented by densely deploying micro base stations within the coverage area of the macro base station or at the edge of the cell, in order to increase the network throughput and signal reception strength, thereby meeting the increasing speed and capacity requirements. however, udhcn brings about serious electromagnetic pollution at the same time. in order to achieve sustainable development, the green deployment problem of micro base stations in udhcn, that is, satisfying certain user service quality, maximizing network energy efficiency and minimizing electromagnetic radiation intensity, has become an extremely complicated yet challenging problem."
"in order to make the dolphin swarm algorithm better solve the constrained multi-objective problem, the search mechanism of the dolphin swarm algorithm search and hunting is improved as follows."
"in summary, the deployment of micro base stations has evolved from a single performance requirement to a comprehensive consideration. at present, its related research is still in the exploration stage, and there are still two major defects in the following two aspects: first, the existing results focus on the pursuit of only the largest network energy efficiency or the minimum electromagnetic radiation intensity. the comprehensive consideration of the two is only a onedimensional linear scene facing the highway, which does not meet most practical application scenarios [cit] . second, from the perspective of mathematics, the green deployment of micro base stations belongs to the constrained multiobjective optimization problem. the current deployment strategy is to convert the constrained multi-objectives into linearly weighted and then into the constrained single-objective problem, because the weight selection is related to the importance of the target. therefore, it is impossible to achieve comprehensive consideration of network energy efficiency and electromagnetic pollution [cit] . similar to the two performance indicators in the fields of intelligent tracking, doa estimation, resource allocation, etc., only the constrained multi-objective algorithm can solve the real balance between the two [cit] . however, the existing constrained multi-objective algorithm has obvious shortcomings such as low convergence precision and easy to fall into local optimum because of its own evolutionary strategy, which leads to its obvious convergence and distribution. a large number of literatures have confirmed that compared with evolutionary algorithms such as genetic [cit], particle swarm [cit], and bee colony [cit], the dolphin swarm algorithm (dsa) [cit] performs better on single-objective problems [cit] . then the constrained multi-objective algorithm based on the dolphin swarm algorithm is expected to get better performance than the existing constrained multi-objective algorithms."
"section ii describes the line fitting results. we already know that the weld joint type can be determined based on the abrupt differences of the laser curve at the weld joint in the image. when considering the image characteristics, we propose a method using the laser curve distribution in the image and the weld joint feature point intervals as the basis of a characteristic description to recognize different weld joints types."
"in this paper, we set w1 to 15 and w2 to 15 based on a large number of experiments. figure 8d shows the extraction results."
"it can be seen from fig. 9 that as the number of micro base stations increases, the intensity of electromagnetic radiation also increases. this is because the micro base stations are deployed in the coverage area of the macro base station, and the electromagnetic radiation of the micro base stations is increased on the basis of the original electromagnetic radiation. the intensity of electromagnetic radiation also increases. for the number of any micro base stations in fig. 9, the electromagnetic radiation intensity of the proposed algorithm is lower than that of the b algorithm. this proves that the proposed algorithm and the b algorithm have better micro base station position and coverage radius under the same number of micro base stations. although the intensity of electromagnetic radiation has been increasing, the growth rate is very slow and has been below 0.4 w/m2, so it is within the safe range."
"in this study, we recognize the welding joint type before welding begins. however, there might be multiple welding robots working together in an actual welding workshop. in this case, the other welding robots affects the welding joint image, and the arc light and splash generated by other welding robots appear in the image. we input the image with splash and arc light into our welding system to test the effectiveness of the proposed algorithm in the case of external noise interference. the results show that the proposed algorithm can effectively process an image with splash and arc light, as shown in figure 21 ."
step 3: search phase. perform the search phase according to (18)- (20) and update the feasible solution set and the infeasible solution set according to the section iii.
"where d is the distance between the base station and the observation point, g is the antenna gain (multiplier), and p t is the transmission power of the base station. generally, the transmission power of the macro base station is fixed, and the volume 8, 2020"
"we need to check if there are any defects in a transformation. here we consider a transformation as a program taking the source and target model as the input and output respectively. based on such a transformation model, we could expect several reasonable assumptions to be satisfied. for example, with the sc2eha transformation, the input statechart and the output eha model need to fulfill restrictions at the metamodel level for well-formedness criteria. failing to satisfy such criteria will be a symptom of an incorrect transformation."
"step 1: parameter initialization and population initialization, setting initial parameters including population size n, dimension of optimization problem, feasible solution set size n 1, infeasible solution set size n 2, maximum search time t 1, maximum transmission time t 2, upper and lower bounds h and f of search problems, function maximum call times callmax, time transfer matrix ts, etc."
"it is difficult to recognize a weld joint that is based on another part of the image, except for the part near the laser stripe. (2) the laser stripe in the weld joint image has the characteristics of discontinuity, grey value change, and deformation. for different weld joints types, the degrees of deformation and the change of grey value of the laser stripe in the image also differ."
"the selection of the parameter set plays an important role in determining the quality of an svm model. additionally, multi-type weld joint recognition involves the selection of parameters for multiple svm models. in general, there are two methods that can be applied for choosing parameters: (1) the parameter set for each svm model (i.e., the model for distinguishing between each pair of weld types) is independently selected, and each model will have its own parameter settings. for example, with this approach, there will eventually be 10 sets of parameters since there are 10 svm models for the problem considered in this paper. (2) all of the models share one set of parameters, and the parameters that yield the highest overall performance are selected."
"the dual-population search mechanism means that in addition to the iterative population, additional populations are added to retain some feasible solutions and infeasible solutions, and are updated with iterations. in order to improve the optimization effect, this paper improves the update of the feasible solution set and the infeasible solution set as follows:"
"the remaining sections of this paper are organized, as follows. section 2 introduces the visual tracking sensor used in this paper and the feature analysis of various welding joint images. in section 3, the weld joint image feature extraction method used in this paper is proposed. in section 4, an svm-based weld joint recognition model is built, and the weld joint type recognition method proposed in this paper is introduced. section 5 discusses the experimental. finally, the conclusions of this paper are provided."
"the laser curve in the image must be extracted to extract the feature vector. therefore, weld joint image preprocess include noise processing, laser curve extraction, region of interest (roi) extraction, and laser curve fitting."
"we can see triple rules as templates establishing mappings between source and target models. therefore, the transformation is correct only if such mappings conform to triple rules. for example, with the triple rule shown in fig. 4 a mapping that conforms to the rule must include 11 objects and 14 links. for the check we aim to maintain \"traces\" for such mappings. we propose to add a new node into the correspondence part of each rule. the new node represents an instance of a class whose name coincides with the rule name. the node is linked to all nodes in the correspondence part so that from this node we can navigate to them within an ocl expression. we can define an ocl condition in order to represent the pattern of this rule. for example, the following ocl condition represents for the triple rule compstaten est shown in fig. 4 : an invariant of the compstaten est class is defined. the transformation is correct only if such an invariant are valid. -"
"as shown in figure 17, the welding system that was considered in this paper consisted of five parts: a welding robot, a visual sensor, a robot controller, a computer, and the auxiliary welding equipment."
"where num is the population number, d is the dimension of the optimization problem, and x ij represents the value of dol i in the j-th dimension, which is randomly generated as:"
"studies have shown that harmonic distance can better reflect the degree of crowding between individuals, so as to solve the problem that the crowded distance ignores the distribution of other ranks, so the harmonic distance is used instead of the crowded distance [cit] . however, the calculation of harmonic distance is too complicated, and it does not highlight the difference between the different levels of individuals. for this reason, the harmonic distance is improved, and shown as:"
"the five weld joint types that were considered for recognition in this paper were lap weld joints, butt weld joints, splice weld joints, fillet weld joints, and v-type weld joints. table 2 shows the parameters of each weld type. the five weld joint types that were considered for recognition in this paper were lap weld joints, butt weld joints, splice weld joints, fillet weld joints, and v-type weld joints. table 2 shows the parameters of each weld type."
"therefore, in this paper, feature point 1 is defined as the first pixel point at which the slope is greater than 1 and feature point 2 is defined as the last pixel point at which the slope is greater than 1, as follows:"
"we define the rtl 2 language in order to specify triple rules incorporating ocl. the declarative specification in textual form can generate the different operations for transformation scenarios as explained in subsect. iii-a. we realize the operations by taking two views on them: declarative ocl preand postconditions are employed as operation contracts, and imperative command sequences are taken as an operational realization. figure 6 illustrates the rtl specification of triple rules and the generated corresponding ocl operations."
"at a weld joint, the laser curve will exhibit deformation, which will result in changes in the slope of laser curve. additionally, the slope distributions of the laser curve that are observed in images of different types of weld joints are different. therefore, the slope distribution of the laser curve in the image can be used as an effective basis for weld joint type recognition. we define the slope of a point at the laser curve as:"
"currently, the most commonly applied classification algorithms include logistic regression, the k-nearest-neighbour algorithm, the decision tree algorithm, bayesian classification, the svm algorithm, and neural networks. the svm results were compared with the results of other classification methods to verify the effectiveness and applicability of the svm method selected in this paper. logistic regression is mainly used for binary classification, since the number of weld types considered here is greater than two, logistic regression is not a suitable classification algorithm for the problem of interest. bayesian classification is based on the premise that the posterior probability can be obtained with the prior probability. bayesian classification is not suitable for the problem of interest because it is difficult to find a suitable prior probability for the problem considered in this paper. neural networks have high requirements in terms of the number of samples needed and they are not suitable for classification with small sample sizes, and they require a high performance gpu for training and testing; they are not suitable for welding industrial sites. therefore, the svm model that was established in this paper was compared with the k-nearest-neighbour algorithm and the decision tree algorithm. the classification accuracy of each algorithm and the computational cost of single image recognition were analysed based on the feature vectors that were extracted in this paper."
"the remaining sections of this paper are organized, as follows. section 2 introduces the visual tracking sensor used in this paper and the feature analysis of various welding joint images. in section 3, the weld joint image feature extraction method used in this paper is proposed. in section 4, an svm-based weld joint recognition model is built, and the weld joint type recognition method proposed in this paper is introduced. section 5 discusses the experimental. finally, the conclusions of this paper are provided."
"robotic welding technology is an important indicator on which to measure the technical development of the welding industry in today's highly developed environment. currently, the two most common operating modes for welding robots, namely, the teaching mode and the off-line programming mode, do not depend on sensor measurements during welding; the welding trajectories are set in advance by workers, and the robot moves in accordance with the desired trajectory. these two modes are suitable for use in a standardized, modular, strictly coordinated welding system [cit] . however, in actual welding operations, the welding environment might not be static. therefore, these two modes do not offer sufficient flexibility and robustness to handle such a complex and dynamic welding environment. the teaching mode and off-line programming mode require a lot of time to teach and preprogram each time the workpiece is replaced; therefore, with the rapid improvement in sensor technology and integration technology, intelligent welding robots that can overcome the difficulties that are encountered when operating welding robots in the teaching and off-line programming modes are emerging. [cit], 20, 471 3 of 20 the main contribution of this paper is three-fold, as follows:"
"this paper 98.4% 148.23 ms reference [cit] 89.2% 165.62 ms a comparison reveals that the recognition accuracy and computational cost of the weld image feature extraction algorithm that was proposed in this paper are better than those of the feature extraction method presented in reference [cit] . the vertical distance from the groove to the surface of the weld joint is used to construct the feature vector. this feature vector is relatively simple. consequently, this method is more suitable for weld joint types or weld grooves that exhibit large differences. weld joints that do not show significant groove differences will be difficult to distinguish. the method of reference [cit] misidentifies some lap weld joints, splice weld joints, and fillet weld joints, because the weld grooves of these three types of joints are relatively small, which makes it difficult to differentiate among them, as shown in figure 19 . by contrast, the method that was proposed in this paper not only utilized the characteristics of the weld joint, but also accounted for laser deformation, which made it suitable for a wider scope of applications and enabled it to achieve a higher accuracy rate."
"step 4: hunting phase. update the position of the dolphin individual dol i according to (21), and update the feasible solution set and the infeasible solution set according to the section iii."
"(1) the parameter set for each svm model (i.e., the model for distinguishing between each pair of weld types) is independently selected, and each model will have its own parameter settings. for example, with this approach, there will eventually be 10 sets of parameters since there are 10 svm models for the problem considered in this paper. (2) all of the models share one set of parameters, and the parameters that yield the highest overall performance are selected."
"it can be seen from the above coding method that the number of micro base stations affects the size of the population and directly affects the complexity of the algorithm. combining (3) and (6), the impact of the number of micro base stations on the deployment result described in (7) is further analyzed: after the micro base station is deployed, some users of the macro base station are offloaded to the micro base station, and the distance between the user and the micro base station is closer, and the corresponding path loss and interference are reduced. in addition, since the micro base station and the macro base station share the bandwidth, the user who communicates with the micro base station obtains more bandwidth resources, the user rate thereof also increases, and the user rate of the same macro base station is also improved, and therefore, the average user of the entire network. the rate will increase, and the power consumption of the micro base station is small, so the network energy efficiency will be improved. as the number of deployments of micro base stations continues to increase, the interference between them increases, and the gain brought by the micro base stations will be offset by the inter-layer interference, resulting in an average user rate will be stable, while network power consumption continues to increase, so the network energy efficiency will decline. in short, when the number of micro base stations is a certain number, the network energy efficiency of the system will be maximized. if the number is exceeded, the energy efficiency of the network will decrease and the electromagnetic radiation will continue to rise."
"in summary, when the number of micro base stations is 30, the maximum network energy efficiency and the micro electromagnetic radiation can be obtained. for the green deployment problem of the above micro base stations, the number of the micro base stations is 30, and the result is optimal. in the next study, we can determine the optimal number of micro base stations by some method, without manually setting the number of multiple micro base stations, and then determining the optimal number of micro base stations according to the target."
"2) check contract fulfillment of transformation steps: according to the algorithm for translating triple rules into ocl operations [cit], it follows that the sequence of operation applications for a transformation corresponds to a triple derivation for forward transformation:"
our future work includes the following issues. we aim to enhance the technique to extract invariants for transformation models. a control structure like sequence diagram for the rtl specification is also in the focus of our future work. the goal is to increase the efficiency of transformations. the technique to generate test cases from the rtl specification will also be explored. we will focus on other properties of transformations such as the determinateness of transformation. these are efforts towards a full framework for quality assurance of model transformations. larger case studies must give detailed feedback on the proposal.
"for this analysis, 600 weld joint images, including 120 images for each type of joint, were selected as the training images. a total of 250 weld joint images were selected for testing: images 1-50 were images of lap weld joints, labelled with a value of 1; images 51-100 were the images of butt weld joints, labelled as 2; images 101-150 were images of splice weld joints, labelled as 3; images 151-200 were images of fillet weld joints, labelled as 4; and, images 201-250 were images of v-type weld joints, labelled as 5. all of the experiments that were reported in this paper were performed on a computer with an intel i5-6500 cpu, with a main frequency of 3.2 ghz and 8 gb of ram."
"where i is the ith row in the image, p i (x) and p i (y) are the coordinates of the points on the laser curve in the image, g is the grey value of the current point on the laser stripe, whose value is 0 or 255, h is the x coordinate of the current point on the laser stripe, and k and j are the x coordinates of the left and right edge points, respectively, of the laser stripe in each row of pixels. figure 8c shows the laser curve extraction results."
"(2) an image feature extraction method is proposed to extract two kinds of feature information, which can increase the recognition information and improve the recognition accuracy. (3) the weld joint image feature extraction algorithm that was proposed in this paper offers better recognition accuracy and a lower computational cost than algorithms from other papers."
"given a test case including the source model m s and the expected target model m t . to check the transformation with the test case means we check if m t coincides with the resulting model m t . instead of this, we could employ integration rules in order to obtain an integration of m s and m t : a mapping between these models is established. the derivation is such that"
"to this end, we propose a green deployment method for micro base stations based on the constrained multi-objective dolphin swarm algorithm. the specific work is as follows: firstly, with reference to the established single network energy efficiency model and electromagnetic pollution model that meet the actual communication scenario, a mathematical model for the green deployment of micro base stations is established. secondly, the update mechanism of the infeasible solution and the feasible solution in the two-population search mechanism is improved. according to the characteristics of the constrained multi-objective problem, the location update strategy of the dolphin swarm algorithm is improved, and the constrained multi-objective algorithm based on the dolphin swarm algorithm is proposed. thirdly, a two-stage green deployment method for micro base stations based on constrained multi-objective dolphin swarm algorithm is proposed. the simulation of the communication scenario including low load, medium load and high load shows that the proposed deployment method has certain advantages in network energy efficiency, electromagnetic radiation and user speed."
"we define a rectangular roi based on the location of laser curve in the image, and the parameters of the roi can be determined, as follows:"
"in the case of linearly inseparable data, it is difficult to find a suitable hyperplane for classifying the two types of data. the common solution to this problem that is adopted in the svm algorithm is to find a mapping function for mapping the data into higher dimensions; thus, data that are inseparable in lower dimensions can become separable in a higher dimensional space, such that an appropriate hyperplane can be found to classify them. a kernel function k(x i, x j ) is introduced to map two vectors from the original space into the higher-dimensional space to solve the problem of high-dimensional data and to calculate the inner product during optimization. the input to this kernel function consists of two vectors, and its output is the inner product of the two vectors mapped into the higher-dimensional space. common kernel functions include linear kernels, polynomial kernels, gaussian kernels, laplace kernels, and sigmoid kernels. in this paper, a gaussian kernel (radial basis function, rbf) is adopted: (14) where g is the bandwidth of the gaussian kernel function. as g goes toward infinity, almost all of the samples become support vectors, which can lead to overfitting, as all training samples will be accurately classified. as g goes toward zero, the discriminant function of the svm becomes a constant function, and its classification ability for new samples becomes 0; in other words, it will classify all the samples into the same class, which leads to under-fitting. therefore, choosing an appropriate bandwidth has great influence on the performance of the model, and it is necessary to find a balance between correctly dividing the current data and ensuring suitability for a wider range of data to ensure that the model has good practical value."
"it is difficult to recognize a weld joint that is based on another part of the image, except for the part near the laser stripe. (2) the laser stripe in the weld joint image has the characteristics of discontinuity, grey value change, and deformation. for different weld joints types, the degrees of deformation and the change of grey value of the laser stripe in the image also differ."
"d. if the user is not covered by any micro base station, the user rate and the constraint violation degree of the user are directly calculated. otherwise, calculate the signal to interference and noise ratio sinr of the user and all the base stations, select the base station communication with the largest signal-to-noise ratio, and calculate the user rate and constraint violation degree of the user. e. calculate the network energy efficiency under scene k according to (1) and (3), and accumulate the constraint violation degree of all users."
"it can be seen from table 5 that the algorithm can obtain higher network energy efficiency when the number of micro base stations is the same. the electromagnetic radiation intensity is similar to or slightly higher than the b algorithm, but both are within the safe range. the rate has a certain advantage over the b algorithm, and the network energy efficiency can be improved by up to 9.9% compared with the b algorithm."
"it can be seen from fig. 8 that as the number of deployed micro base stations increases, the network energy efficiency of both algorithms increases first and then decreases, which is consistent with the theoretical analysis in section 4. for any number of micro base stations, the cmdsa optimized network energy efficiency is better than the b algorithm, and when the number of micro base stations is around 30, the energy efficiency of the algorithm network is the largest, the energy efficiency of the b algorithm is improved by 8.7%, and the network energy efficiency can be increased by 18.19% . therefore, this also shows that the proposed algorithm performs better than the b algorithm in optimizing the deployment problem of the micro base station. fig. 9 shows the relationship between the number of micro base stations and the intensity of electromagnetic radiation."
"image features determines the recognize accuracy and applicability of a weld joint recognition system. in this paper, we take full advantage of the laser stripe deformation information in the image as the features for identifying the weld joint type. figure 6 outlines the extraction process."
"(4) the weld joint recognition system that was proposed in this paper exhibits good robustness. after the addition of a new weld joint type, this method can still effectively recognize different types of joints with higher recognition accuracy. (5) in future work, we will attempt to achieve online recognition to allow for weld joint types to be recognized during the actual welding process, which will require a stronger ability to deal with noise."
"we observed that the gd values obtained by cmdsa are significantly better than other algorithms on all test functions, indicating that cmdsa has the best convergence. the sp standard value of the cmdsa algorithm on ctp6 is slightly inferior to that of nsga-ii. on the functions ctp1 and ctp7, the sp of the cmdsa algorithm is equal to bb-mopso and nsga-ii, respectively, and the sp values obtained by cmdsa on other issues are lower than other algorithms show that the distribution of cmdsa is optimal compared to several other algorithms. b algorithm, bb-mopso, cmo-dsa obtained all the discrete points of the real frontier of ctp3, and the standard deviation is 0, reflecting the superior ability of the algorithm to maintain diversity, and the average value of discrete points found by nsga-ii. it is 13.58, which also reflects the lack of capacity of the crowd to maintain the diversity of the population. on ctp4 and ctp5, cmdsa obtains all discrete points of the real frontier, which indicates that the distribution of cmdsa is better than the other three algorithms."
"we added a new weld joint type to the model to verify the robustness of the proposed weld recognition algorithm. the new weld joint type is \"filler layer weld joint\". figure 20 shows a laser curve image of such a filler layer weld joint. during the actual welding process, the welding voltage and current are lower than those in the bottom layer, because one layer has already been welded, and the swing range of the welding torch should be smaller. thus, the weld characteristics meet the needs of weld joint recognition, as presented in this paper. with this addition of this weld joint type, the final recognition accuracy of the new weld joint recognition system is 98.1%. when compared with the previous performance, the accuracy of the proposed system is reduced by only 0.3%, thus showing that the proposed joint recognition system exhibits good robustness. it is necessary to test the recognition results for different weld sizes to verify the generalizability of the proposed recognition algorithm. therefore, along with the additional weld joint mentioned above, our system was applied to weld joint image of different sizes, as shown in table 5 . ultimately, it is necessary to test the recognition results for different weld sizes to verify the generalizability of the proposed recognition algorithm. therefore, along with the additional weld joint mentioned above, our system was applied to weld joint image of different sizes, as shown in table 5 . ultimately, the recognition accuracy of the new model is 98.4% and the computational cost is 148.23 ms. in fact, the changes in weld size have no effect on the recognition accuracy."
"paper. neural networks have high requirements in terms of the number of samples needed and they are not suitable for classification with small sample sizes, and they require a high performance gpu for training and testing; they are not suitable for welding industrial sites. therefore, the svm model that was established in this paper was compared with the k-nearest-neighbour algorithm and the decision tree algorithm. the classification accuracy of each algorithm and the computational cost of single image recognition were analysed based on the feature vectors that were extracted in this paper."
"where p bt is the transmission power of the macro base station, which is a constant, m 0 is the number of users falling in the coverage area of the macro base station (which is the number of observation points), and m i is the number of users falling in the coverage area of the i-th micro base station."
model transformation can be seen as the heart of modeldriven approaches [cit] . transformations are useful for different goals such as (1) to relate views of the system to each other; (2) to reflect about a model from other domains for an enhancement of model analysis; and (3) to obtain a mapping between models in different languages. in such cases it is necessary to ensure the correctness of transformations. this is also a challenge because of the diversity of models and transformations.
"as shown in figure 17, the welding system that was considered in this paper consisted of five parts: a welding robot, a visual sensor, a robot controller, a computer, and the auxiliary welding equipment."
"where x and y represent the vertex of the top left corner of the roi, w is the width of the roi, h is the length of the roi, and w 1 and w 2 are the reserved values of the rectangular roi in the x direction. in this paper, we set w 1 to 15 and w 2 to 15 based on a large number of experiments. figure 8d shows the extraction results."
"the selection of the parameter set plays an important role in determining the quality of an svm model. additionally, multi-type weld joint recognition involves the selection of parameters for multiple svm models. in general, there are two methods that can be applied for choosing parameters:"
"quality assurance for transformations means systematic monitoring and evaluation of various aspects so that the transformation meets modeler expectations. here we focus on the questions \"is the transformation right?\" and \"is this the right transformation?\" that characterize verification and validation."
"in order to obtain a mapping between a pair of models, we consider such a combination as a triple graph. triple graph transformations allow us to build states of the integration."
"as shown in (7), the mathematical nature of the green deployment model of heterogeneous cellular network micro-base station is a complex constrained multi-objective problem."
"a visual sensor is installed on the end effector of the robot. for communication, there are cable connections between the auxiliary welding equipment and the robot controller, between the computer and the robot controller, and between the robot controller and the welding robot, and the visual sensor transmits image data to the computer through a usb connection."
"for the characteristics of constrained multi-objective problems, the following improvement is made to (9): first, some excellent infeasible solutions in constrained multi-objective problems play an important role in promoting population convergence and increasing population diversity. the excellent infeasible solution has the opportunity to participate in the composition of the basis vector. second, the length of the sound wave determines the search step size of the base vector itself, but the algorithm has different requirements in different evolutionary periods: in the early stage of evolution, each dolphin individual is far apart, the global search ability of the enhanced algorithm can use a larger sonic length, so that the dolphins can search in a larger area; in the later stage of the search, the dolphins are in the vicinity of the optimal non-dominated front, and if the search range is too large. not conducive to the even distribution of dolphin population, resulting in invalid search. therefore, the length of the sound wave should gradually decrease as the number of iterations increases. specifically, it is shown as:"
where si represents the slope of the ith fit line and mi and ni denote the endpoints of the interrupted laser curve in the image. the disconnected curves can be determined by row scanning. figure 8e shows the line fitting results.
"in the system model of this paper, each user terminal is an observation point. in order to facilitate comparison with the electromagnetic communication standard of mobile communication, the average electromagnetic radiation intensity is used in this paper. the electromagnetic radiation intensity can be written as:"
"to further verify the above analysis, we experimented with the scenario described in the experimental part: changing the number of micro base stations, using the method and algorithm b to deploy, and obtaining the relationship between the number of micro base stations and the network energy efficiency and electromagnetic radiation. the parameters are set as follows: the number of populations is 100, the maximum number of iterations t is 200, and other parameter settings are consistent with section 3. the algorithm runs independently 20 times, and the average value of the objective function is calculated. fig. 8 shows the relationship between the number of micro base stations deployed in the network and the network energy efficiency of the proposed algorithm and b algorithm."
"the visual sensor is used to capture the two-dimensional coordinates of weld feature point in the pixel coordinate system, to map it to robot base coordinate system through a series of transformations matrix. figure 2 shows the coordinate transformation model. the visual sensor is equipped with a complementary metal-oxide-semiconductor (cmos) camera, the parameters of which are shown in table 1 . the main function of the laser is to project a laser curve on the workpiece for features extraction and three-dimensional (3d) reconstruction. the partition is designed to isolate splash to reduce the noise in the image. the laser wavelength that was used in this paper as 650 nm and the laser power was 30 mw. the adjustable width of the fringes ranges from 0.5 mm to 2.5 mm. the penetration of filters is 80%; the filters filter splash and light. the focus length is 12 mm. the distance between camera and welding torch is 200 mm. in a real welding environment, the posture of the visual sensor will be adjusted in real time to keep it perpendicular with the weld joint, so that the laser curve is vertical in the weld joint image, that is, the visual sensor is oriented the same way relative the path, even for curved paths."
"for the ultra-dense heterogeneous cellular network oriented to two-dimensional communication scenarios, this paper establishes a constrained multi-objective mathematical model for the green deployment of micro base stations, and improves the search mechanism and dual-population strategy of the dolphin swarm algorithm, and proposes a constrained multi-objective dolphin swarm algorithm. finally, the constrained multi-objective dolphin swarm algorithm is used to solve the problem of green deployment of micro base stations. simulation of 9 communication scenarios shows that the proposed method can balance network energy efficiency and electromagnetic radiation. next we will study how to determine the optimal number of tiny base stations and reduce the complexity of deployment problems."
"where h i and f i is the upper and lower limits of the search range of the j-th dimension, and rand is a random number between [cit] ."
"each method has its advantages. a single parameter set might not be appropriate for all k(k − 1)/2 models. however, the overall accuracy is the ultimate consideration, and individual sets of model parameters may lead to over-fitting of the overall model. therefore, the second strategy is adopted in this paper, the same set of parameters is used for all of the models, and the parameters are set based on the overall performance."
"perform the search phase according to (18)- (20), and calculate the fitness value and constraint violation degree of each body, and update the feasible solution set f and the infeasible solution set if according to section iii."
"the rtl specification of a transformation is translated into transformation operations in ocl. the operation is realized by taking two views on it: declarative ocl pre-and postconditions are employed as operation contracts, and use command sequence are taken as an operational realization. with the full ocl support, use allows us to realize the verification and validation of transformations as discussed in sect. iv."
", where trf i are forward rules and m i are triple matches. in order to check the correctness of the transformation we check if each operation application realizes correctly a rule application. by checking the contract of the operation, i.e., a pair of preand postconditions it allows us to ensure the correctness of the transformation step. it offers an on-the-fly verification for different transformation properties."
"we define a rectangular roi based on the location of laser curve in the image, and the parameters of the roi can be determined, as follows:"
"as shown in figure 7, regarding the relative position of the visual sensor, the welding torch and the workpiece, the welding process is divided into four stages:"
"ocl application conditions (bacs 1 ) of a triple rule consist of ocl conditions in source, target, and correspondence parts of the triple rule. bacs within the lhs and rhs of the triple rule are application pre-and postconditions, respectively:"
"the laser curve slope distribution can be used as an important feature for recognizing the weld joint type; however, there are some weld joints that are very different in appearance, but have similar slope distributions, such as fillet weld joints and v-type weld joints. therefore, it is necessary to select a feature as a condition to identify the type of weld."
"accordingly, we can obtain the distributions of disx and disy for various weld joint images. figures 12-15 show the disx and disy of the lap weld joint and splice weld joint."
"parameter initialization: number of micro base stations n, maximum number of iterations g max, feasible solution set size n 1, infeasible solution set size n 2, maximum search time t 1, maximum transmission time t 2, time transfer matrix ts, etc."
"the k-nearest-neighbour algorithm recognizes the weld joint type by calculating the euclidean distances between the feature vector of the weld joint to be recognized and the feature vectors of all known joint images. in the decision tree algorithm, a decision tree is recursively generated by selecting features as criteria for node splitting. the traditional id3 algorithm was used in this experiment. table 4 shows the final experimental results. the experimental results show that the joint recognition system that is based on the proposed svm model is superior to the k-nearest neighbour algorithm in terms of both recognition accuracy and computational cost. by contrast, the time cost of the decision tree algorithm is lower than that of the svm algorithm by 9.88 ms; however, its accuracy rate is only 90.3%. therefore, based on comprehensive consideration of the computational cost and accuracy, we conclude that the svm model that is presented in this paper is also superior to the decision tree algorithm."
"the human hand is capable of producing a wide variety of functional tasks involving grasping, object manipulation, and playing musical instruments, which are unmatched by the function of any current robotic hand. the versatility and fierce precision in the control of our hand is largely attributed to the intricate biomechanical and neural structures that allow individualized as well as coordinated control of different digits."
"all experimental data were analyzed offline in matlab (mathworks inc.). the data analysis mainly included two sections. in the first section, the features of activation were extracted via the 128-channel hd emg recordings. in the second section, a classification was performed across different finger flexions. a block diagram of the entire process of the methods is shown in fig. 3 ."
"the method predicts characters to be input next in the same unit used for recording the log. it does not predict the combination of several input units at one time. the occurrence patterns become increasingly diverse and the prediction accuracy might become low if we predict the occurrence of the combination of input units. prediction proposals are limited to characters that cannot be input using a keyboard. for math input, it is easier to input characters directly using a keyboard than to input them using predictive math entry if they can be input using a keyboard."
"generally, an n-gram model has a problem called the zero frequency problem. because the n-gram probability is calculated using occurrence frequency, the probability for a word pair that does not occur in the learning dataset becomes zero. this fact suggests cases in which no prediction proposal is presented. to solve this problem, we conduct a smoothing of the probability value. we adopt a smoothing method called linear interpolation [cit], which calculates the n-gram probability"
the basic role of the probabilistic language model is to calculate the string generation probability ) ( 1 () n pw can be transformed to the following formula using the multiplication rule in probability theory [cit] .
"at ucd, the minho robot detection script runs nightly. new robots are labelled and aggregated in the download database weekly. anything flagged by the robot detection script is assumed to be a robot. there are currently 49,556 ip addresses and 1,086 user agent strings flagged as robots in the database."
"to label each download, the flagged field was concealed from view and four passes were taken through the data. self-declared robots were marked in the first pass. reverse dns lookup was performed on the remaining ip addresses in the second pass, where unambiguously robotic behaviour originating from hosting companies, cloud servers and search engine companies were labelled robots. in the third pass, the sessions created by ip addresses exhibiting ambiguous behaviour were examined in the log files and the ip addresses checked against the project honeypot database (unspam technologies inc., 2015) . at this point, all downloads had been labelled and were compared against the flagged field to determine true and false positives and true and false negatives. all false positives and true negatives were then checked against the project honeypot database and examined in the log files, completing the fourth and final pass."
"in recent years, the predictive entry of natural language has been put to practical use in cell phones and smart phones. pobox is a major predictive entry system of japanese [cit] . it outputs a word that matches the starting few characters. it also outputs characters followed by the recent input words. t9 is a popular predictive entry for cell phones [cit] . characters are divided into nine groups in t9. each group is assigned to one key in the cell phone. matched words in the dictionary are shown to the user as prediction proposals if a user pushes a key. predictive entry is useful for cell phones because they are equipped with limited number of buttons. our assessment is that it is also useful for inputting mathematical formulas because current keyboards are not equipped with keys for numerous special mathematical characters."
"despite the high complexity and variations across individuals of the forearm flexors, our hd emg approach was able to capture unique patterns of activation of individual finger flexion, which are largely in agreement with earlier studies using intramuscular recordings. 5 we can identify individual finger motions with varying degrees of confidence from the 2d activation map based on emg amplitude and the iz estimates based on single motor unit activities. we found that the activation of the ring finger is located in the most ulnar and proximal region and is distinct from the rest of the fingers, and that the classification accuracy is not surprisingly high. the activation of the middle finger was located in the most radial region with some degree of overlap with the index and little fingers. unlike the extensor digitorum muscle activation, 3,13 the activation patterns of flexors in the proximal-distal direction is less distinguishable, despite the elongated muscle orientation. overall, the iz distribution is more distinct across different fingers in comparison with the macro-level muscle activation (rms map) patterns. namely, the centroids of the rms map typically span within a 10 cm region, but the mean coordinates of the iz cover over three electrode columns (the physical distance is 20 cm) in the radial-ulnar direction and over six electrode rows (or 50 cm) in the proximal-distal direction. these large differences can facilitate accurate identification of the involved compartments during individual finger motions."
"there are two important points to note about the minho robot detection script. once an ip address is labelled as originating from a robot, it is added to a near permanent black-list. if the ip address is reused later by a human user, it will still be discounted from the download totals. there is only limited functionality to remove an ip address from the blacklist. this is problematic at least in theory due to the constant recycling of ip addresses by the dynamic host control protocol (dhcp). this issue will be addressed further below."
"finally, the mean positions of the iz (iz x and iz y ) in the x-and y -axes were calculated to quantify the location of iz by averaging the x-and y-axes coordinates of all izs in each trial."
"in our experience, institutional repository content is most often discovered via the major search engines, a direct and positive result of robotic indexing. this supports the claim that \"discovery happens elsewhere\" [cit] and has implications for web robot detection in oa repositories. on the one hand, repositories by nature wish to attract search engines in order to increase the visibility and discoverability of their content; however, they must present accurate usage statistics that have nonhuman usage filtered out. since repositories cannot simply exclude robots altogether (and this would probably be impossible), they must develop practical web robot detection techniques that can stay apace of robot development."
"the status of 11 downloads was ambiguous after the first two passes. seven came from hosting companies. one download was removed from the dataset due to lack of data and replaced with a new random download from the period (without replacement of the previous 341 downloads). of the remaining ten ambiguous downloads, eight were found to be robots through detailed log analysis, evidence from project honeypot, and web searching on the ip addresses, agents, and dns name."
"the average of the six users' prediction accuracies are presented in table 3 . the results show that the prediction accuracy increases a little when the ratio of the personal logs increases. the improvement is not so large. however, we can also say that our prediction method achieves accurate prediction even if it does not use the target user's personal logs. it is expected that our method improves the prediction accuracy when many log data are used as learning data even if they are other users' logs."
"the nat problem, where many users on one network appear to be using a single ip address, has been queried by at least one user of the eprints system [cit], but only an empirical test can determine the impact of the repeat download filter's algorithm on the accuracy of download statistics in eprints."
"checking false positives and true negatives (downloads where manual labelling could have missed a robot) against project honeypot resulted in five new robot downloads being identified. however, six ip addresses flagged by project honeypot as potential comment spammers were ruled out and were not labelled as robots."
"while there is some evidence to suggest that the finding of 85% robot downloads is generalizable for oa repositories, this study alone cannot make broad conclusions as to the accuracy of web robot detection in irs. the study is performed on only one repository that uses a single, somewhat idiosyncratic web robot detection technique. search engine optimization and crawl behaviour influences (e.g. differing use of robots.txt, use or non-use of sitemaps.xml files) will likely change the effects of robots on repositories' usage statistics. future studies adding to the breadth of empirical data, or larger studies across multiple sites and platforms can improve on these limitations."
"1,2 at the biomechanical level, the individual phalanges are driven by muscles with distinct anatomical compartments and separate tendons, [cit] allowing dexterous finger movement. meanwhile, tendon re-branching and tissue connections between tendons to the individual fingers can lead to coordinated motions. 6, 7 at the neural level, there is evidence that individual compartments are controlled through subpopulations of the motoneuron pool, and the different groups of motoneurons receive both unique and shared synaptic input from spinal and supraspinal pathways. [cit] these complex biomechanical and neural control properties have † corresponding author."
"ip addresses originating from cloud, rack space, it infrastructure and/or hosting companies were generally assumed to be from robotic agents. this raises a number of problems such as proxy servers, vpn users, cloud-hosted ethernet users, and outsourced it infrastructures. in general the download behaviour of these agents was sufficiently indicative, but in a many cases detailed log analysis, web search on agent/ip address pairs, and checking against project honeypot was required to make a final decision."
"finally, this benchmarking study shows that a low cost and practical robot detection technique can produce remarkably high robot recall and precision. the technique consists of an extendable list of known ip addresses and user agent strings garnered from robots.txt accesses via an automated process. this is coupled with a simple way to visually locate unusual behavior (outliers), allowing for manual robot detection."
"we assume that our input method works on an ordinary formula editor such as microsoft office formula editor or mathtype. these formula editors usually provide an input interface based on what you see is what you get (wysiwyg) 1 . users can check their input formulas on the screen immediately after they input them. they also provide buttons for inputting characters which cannot be input using a keyboard on the top of the screen. in this study, we implemented our original formula editor to evaluate our proposed input method. our formula editor follows the characteristics of ordinary formula editors described above."
"the aim of this study was to quantify the distinct activation patterns on both macro-level muscle activation and localized motor unit activities of the forearm flexor muscles during individual finger flexions. we have established the findings that the spatial activation patterns cannot be readily distinguishable based on certain features (e.g. the centroid of muscle activation and mean coordinates of iz) of the emg activities. specifically, the activation patterns of the middle finger were the least distinguishable, and the ring finger compartment was mostly distinct from the rest of the fingers. on the other hand, the different finger activation patterns can still be classified in high accuracy including the effort levels of muscle contraction, using pattern recognition approaches based on detailed features from hd emg recordings. our overall findings provide baseline information regarding altered neural activation of forearm flexor muscles in individuals with neurological impairment. additionally, our findings can also provide guidance in selecting control input variables for assistive or rehabilitative devices involving hand grasp function."
"next it checks the ip address against lists of six large search engines' ip addresses, one list of 2,528 other known search engines' ip addresses, and one list of 48 robots not associated with search engines. most of these ip address lists can be automatically updated via web queries. finally, the detector does a reverse dns name lookup for the fully qualified domain name (fqdn) and matches against a list of domains of known robots [cit] ."
"the users input 60 formulas (20 formulas using each interface). the number of characters in each formula is set within some range. these formulas are selected from a textbook of mathematical analysis [cit] . we carefully selected formulas different from the formulas in the learning dataset. all the 3,000 formulas used in evaluation of prediction accuracy are used as the learning data for our interface. the quantitative indices are measured in each input formula. after inputting 60 formulas, users answered the questionnaires for qualitative evaluation. the question items are shown in the 1-st column of table 4 . q1 and q3 are provided to ascertain which interface the users can use to input formulas with a good level of comfort. we set q2 to find the time the user felt for inputting formulas. the actual time might differ from their sensory time. q4 is provided to elicit how fast the user can learn the input method in each interface. q5 is provided to find out which interface the user prefers with all evaluation viewpoints considered."
"this study offers some findings towards the question of the dhcp problem. at 0.88%, even the total number of false positives is very low, suggesting that permanently blacklisting an ip address (even if it could be reassigned to a human user at a later date) is not a major problem. the trade-off in precision is likely insignificant in comparison to the increase in recall."
"first, we began by comparing each of the five reduced features (mean rms, c x, c y, iz x, and iz y ) across different finger flexions at two different force efforts."
"however, some users supported the tex interface in answers to the questionnaires. they gave the opinion that they are comfortable with the interface, and that they are used to it because they usually use tex for inputting formulas. actually, they were able to input formulas smoothly when using the tex interface in the experiment. in the tex interface, users can input formulas only by a keyboard when they learn the tex command. therefore, users who learn the tex command tend to assign positive opinions to the input interface using tex."
"the n-gram probability is calculated in each hierarchical level. the n-gram probability corresponding to the k-th level log data is designated as the \"k-th level n-gram probability\". the model of math input considering hierarchical level is designated as the \"hierarchical n-gram model\"."
"the results of questionnaires for qualitative evaluation are presented in table 7 . most users selected the high-accuracy method as the best method for all question items. this result corresponds to the results on the quantitative evaluation. for q1 and q3, one user selected the low-accuracy method as the best method. the reason is related to the user's input activities explained in the previous subsection. the user preferred the prediction that is easily forecasted before shifting the input cursor to the prediction proposal display."
"we implemented an interface equipped with our proposed prediction method. the interface was implemented in javascript. it runs on major web browsers. figure 1 portrays our developed interface, which conforms to general formula editors. therefore, the interface realizes wysiwyg. it has buttons for inputting characters that cannot be input using a keyboard. the usage of our interface is the following. when a user moves a cursor to the place where the user inputs characters, predictive proposals are shown below the cursor in the predictive proposal display (see figure 1 ). the user uses cursor buttons of the keyboard to move the cursor. the user moves a cursor to the predictive proposal display by hitting the space key if the user wants to select a character from the predictive proposals. to select a character from the predictive proposals, the user moves the focus to the character that the user wants to input by hitting the space key consecutively or using the cursor buttons. the selected character is input in the target place in the formula if the user pushes enter key. the user can obtain the input formula as in the form of mathml code by clicking the \"get mathml\" button."
"number of studies syntactic log analysis user agent string 6 1, 4, 5, 9, 11 robots.txt access 5 1, 4, 5, 8, 9 list of known robot ip addresses 3 volume of requests 5 3, 5, 6, 7, 11 duration of session 3 5, 8, 11 interval between requests 3 3,4,9"
"this method has been validated as highly accurate, and has been applied for multiple hd emg-based studies. 18, 19 the decomposition processing was performed offline in ot biolab (version 2.0.6092.0, torino, italy). the 10-s steady contraction period for each trial was used for decomposition. the parameters of decomposition algorithm were selected based on the previous studies 17 and were the same for each subject or trial (see table 1 ). a post-decomposition processing based on previous studies was performed to remove motor units with potential decomposition errors:"
"the results of questionnaires for qualitative evaluation are presented in table 4 . the value for each item is the number of users who selected the item. values shown in the parentheses are the numbers of users who can input mathematical formulas without seeing any references (hereinafter \"tex users\"). our interface achieves the best evaluation for all questionnaire items (q1 -q5). especially for q3, all users answered that they can input formulas most intuitively using our interface. in our interface, users can see the input formulas right after they input each character. our interface also provides the prediction proposals that the users want to input. these characteristics engender the users' high evaluations to our interface. when we checked users' free descriptions, many users gave the opinion that switching a keyboard and a mouse took a burden in inputting mathematical formulas. this result supports our proposed interface that decreases the mouse input using the predictive entry."
"given the importance of accurate usage statistics, the sizable and widely variable impact of web robots, and the complexity of detecting them, we endeavor to answer the following questions: what techniques are commonly used for web robot detection? how do the main institutional repository software packages implement web robot detection out-of-the-box? we then describe and test a web robot detection technique used in practice by an oa institutional repository at a large irish university and discuss an effective and practical approach to web robot detection for repositories that takes advantage of the theoretical models."
"generally, when the probability of an event that might occur at some point in time is influenced only by the events which happened at the last n time point, we call this phenomenon an n-th order markov process [cit] . an n-gram model is a model that approximates word occurrences as an n-1-th order markov process. in other words, it is considered that the occurrence of a word at some time point depends only on the last n-1 words. the general prediction model in the n-gram model becomes the following."
"the results showed on input time, it is apparent that our interface achieves the shortest input time. input time is an important index to show that the user can input formulas smoothly. when comparing our interface and the interface without predictive entry, it is apparent that our interface decreases the #switching that is regarded as a reason that inputting mathematical formulas is bothersome. the reduction rate is about 89.1%. in the tex interface, #incorrect becomes the three times that of the proposed interface and the interface without predictive entry. from these results, it is apparent that the incorrect inputs decrease when users can see the input formulas immediately after inputting characters. no significant difference was found between our proposed interface and the input interface without predictive math entry. however, the value of #incorrect is lower in the proposed method than the interface without predictive entry. a clear difference might be found if we increase the number of users in the experiment. #keyinputs is higher in the proposed method than the interface without predictive entry because users input characters by selecting the prediction proposal with a keyboard and move the cursor with cursor key."
"we manually selected 1,000 mathematical formulas from a textbook of mathematical analysis [cit] for evaluation. we invited six test subjects to input those mathematical formulas to build a dataset. the six test subjects were divided into two groups comprising three persons each. one group inputs half of the mathematical formulas. the other group inputs the remainder. each test subject in the same group inputs the same 500 mathematical formulas. consequently, three pieces of log data per mathematical formula are obtained. our dataset contains 3,000 pieces of log data of math input."
"to some degree the lack of discovery occuring within the actual repository site itself sets them apart from the detection studies listed above. session data beyond the initial download request is limited or non-existent; a session in a repository often consists of a single direct bitstream (file) download with neither leading click-stream nor trailing lateral browsing. the only available trace in the logs may be limited to the date and time, the http method and response code, and the ip address, user agent string, and referring website. it is not unusual for many of these fields to be left blank."
"secondly, a user agent string that has been associated with a robot ip address will cause any new ip addresses using that user agent to be flagged as a robot. this includes legitimate human user agent strings forged by robots or an agent string belonging to a human user accessing the robots.txt file. the problem is mitigated somewhat by the fact that the match must be exact to the letter, so in the example ua string, \"mozilla/5.0 (windows; u; windows nt 6.1; en-us) applewebkit/534.16 (khtml, like gecko) chrome/10.0.648.204 safari/534.16,\" any change in the version numbering for the operating system, browser, rendering engine, etc. will prevent a match and the ip address will not be flagged as a robot. since these numbers change constantly, the exact ua string for human users will most likely be phased out over time."
"we need dictionaries specialized for mathematical formulas if we apply this method to mathematical formulas. however, unlike natural language, the subsequent characters are not narrowed down in mathematical formulas when a character is input. for example, \"no see\" usually comes after \"long time\" in english. however, the connections among characters in mathematical formulas are not so definitive. it is difficult to create a dictionary that is effective for forecasting mathematical formulas. therefore, we propose to make a prediction based on a probability model. we apply a probabilistic language model [cit] that is popular in natural language processing, to mathematical formulas and to make a prediction using the probability output from this model. this method outputs prediction proposals in the order of the probability of the next input for the user."
"for solving the problem described above, we propose a hierarchical n-gram model. in this model, the user's log of math input (hereinafter, \"log data of math input\") is divided in hierarchical levels. a model is constructed in each hierarchical level."
"we apply ten-fold cross validation to these datasets and evaluate our proposed method in terms of its prediction accuracy. in detail, the mathematical formulas are divided into 10 subsets. then nine subsets are used as a learning dataset, another subset is used as an evaluation dataset. to calculate the prediction accuracy, we presume that a user inputs each formula of the evaluation dataset from its head to the tail. the prediction is made in each input position (see figure 2) . we examine which prediction proposal matches the actual character at the input position in the log data of the evaluation dataset. that is to say, at each input position in a mathematical formula in the evaluation dataset, we acquire prediction proposals using n-1 recent inputs extracted from the log data. we regard the character at each input position in the log data as a correct character and examine which prediction proposal corresponds to the correct character. we calculate prediction accuracy as a ratio of the input position where the top k candidates of the prediction proposals include the correct character in the log data to the entire input positions. we use the top 10 candidates for evaluation. this calculation is done for all input positions for all formulas in the test dataset."
"given the high proportion of robot downloads being made in oa institutional repositories, small improvements in robot detection increase the precision and veracity of reported usage statistics exponentially. it is well worth the effort in order to demonstrate the value of these services in a more transparent and trustworthy manner. ratio of robots to total downloads 9. factor out fn from 8.:"
"mathematical formulas are helpful tools for representing knowledge in all research fields such as science, engineering, social science, and economics. the world wide web consortium (w3c) has standardized the markup language for representing mathematical formulas, called mathml. users have become able to present mathematical formulas on web pages. most web browsers and formula manipulation software comply with mathml standards. community sites related to mathematics are just beginning to become widespread. those users share how to solve mathematical problems and teach each other their knowledge of mathematics. in the internet research field, search technologies are beginning to be applied to searching for mathematical formulas on web pages [cit] . it is believed that potential needs for presenting mathematical formulas are becoming greater on the web."
"the data elements in table 4 were captured for each download using a combination of sql queries, bash scripts and regular expressions (see appendix 1)."
"in predictive text entry, a user inputs characters sequentially. the system predicts a word to be input next, as inferred from the characters that the user has already input. natural language is a simple time series of data when we specifically examine the apparent sequences of characters. therefore, it can be modeled appropriately by the probabilistic language model explained in the preceding section. mathematical formulas have structures in their presentation. they are not simple symbolic sequences. for example, mathematical formulas including fractions or integrals have hierarchical structures. this indicates that mathematical formulas cannot be modeled using simple probabilistic language models."
"only motor units which satisfied these conditions were used for further iz analysis. the iz represents the origin of the motor unit action potential (muap) which propagates along the muscle fibers towards two opposite directions. since each column of the grid was placed along the muscle fibers, an obvious phase reversal of bipolar muaps can be observed."
"a close review of the existing literature on web robot detection yielded ten individual studies [cit] and one overview/review article [cit] ) that describe and test the main techniques and data used in web robot detection. table 1 lists 23 [cit] . while the majority come from the field of computer science, three studies were found that focus on scholarly information systems [cit] ."
"if this is generally true, then many of the robot detection techniques listed in table 1, such as requests for web page components, image:html ratios, and resource type requests are of limited or no use in repositories. the lack of interaction with the site rules out the use of real-time techniques such as mouse movements and key clicks. captcha could limit the discoverability of open resources since they are usually intended to deny access to robots. a list of robot detection techniques that remain relevant to repositories is put forward and the techniques used by the main oa repository systems are compared in table 3 . bearing in mind their mostly non-commercial nature, open source community based development, and unique usage patterns, the web robot detection techniques used in these systems are described in detail below. data available as a configurable report for manual decision making table 3 . robot detection techniques used in institutional repository systems"
"lastly, it was assumed that downloads originating from end-user oriented isps are typically from human users. this was occasionally overridden by abnormal download behaviour, for example a download from an end-user ip address provided by a major irish isp, with a user agent string mozilla despite the assumptions, we endeavored to examine the data for each download holistically and in many cases our assumptions were overridden."
"two previous studies were quite similar to this study in that they were not based on session data. [cit] tested a technique for an open access journal, a system and environment that is very similar to open access irs. however, the purpose of the study was to measure the impact of robot usage on the journal rather than to propose or benchmark new detection techniques. [cit] investigated click-fraud on advertisements appearing on multiple websites to whose logs the authors had no access. thus the data they analysed was limited almost exclusively to ip address, user agent, and click behaviour and did not include session data as would be created when browsing a website. behavioural indicators included the total number of clicked ads (malicious users often have dense clicks on one advertiser), average clicks per advertiser (trusted users show high diversity), and total clicks per ip address among others. we adapted these indicators to our study in addition to a number of standard indicators as found in the literature. table 4 describes these data elements. at the time the data was taken for the current study, the main download table in the ucd ir's database contained close to four million downloads. the minho statistics package calculates a \"relative value\" for downloads of multi-file items by dividing each download by the total number of bitstreams attached to the item. for example, a journal article could be uploaded as two separate pdfs in one item: full text in one pdf and figures in a second pdf. a download of either pdf is counted as 1 download / 2 bitstreams, for a relative value of 0.5 downloads. there are currently 39 items (less than 1%) with more than one bitstream in the ucd repository. for the purposes of this study, we ignore the relative value and consider a download of a single bitstream to be one full download."
"to this end, two reduced features of the 2d rms map, the mean amplitude, and the centroid coordinates, were calculated. these features were used to quantify the difference of 2d rms map across different finger activations. the mean value of the 2d map was acquired by averaging the rms values across the 128 channels. in order to reduce cross-subject variability, the rms of the four-finger flexion at 60% effort (with the maximum rms) was used for the normalization of the rms values of all other trials within a subject. the centroid coordinates were calculated through eqs. (1) and (2) (the pseudo-code was also provided):"
"the fds and fdp are multi-tendon and multicompartment muscles, and the specific anatomical structure varies across different persons. [cit] for example, the superficial layer of the fds form two compartments inserting to the phalanges of the middle and ring fingers. the deep layer of the fds forms an intermediate tendon and further divides into two compartments that insert on the phalanges of the index and little fingers. the two layers are organized obliquely relative to the skin surface, and there are also passive connective tissues and shared muscle fascicles across compartments. 32 these anatomical complexities can contribute to the neural activation patterns of the flexor muscles observed in our study. our results show that the middle finger activation was the least distinguishable one based on the centroid features and the classification outcome. specifically, the middle finger activation was occasionally confused as little finger activation, and the activation of these two fingers also varied across different subjects (high error bars in fig. 6 ). another possible reason is that the centroids of the rms map and mean coordinates of iz for middle finger are located approximately at the center of the flexor muscles, which make the activation patterns of the middle finger more likely to be overlapped with other fingers. additionally, there was also a tendency that the activation of the index and ring fingers was confused as the little finger, as shown in the centroid location of the rms emg map and mean coordinates of iz, as well as in the classification results. the x centroid (radial-ulnar direction) of the little finger activation was located between the centroids of the index and ring fingers. overall, the shared fascicles across these compartments, the spatial overlap of these compartments from the skin surface, and activation interference across different flexor muscles (e.g. fds, fdp, and wrist flexors) can all impose challenge in securely classifying these individual finger activation patterns."
"neither recall, precision, nor by extension the f-score, take into account the number of true negatives captured by a system [cit] . since the studies in our review focused exclusively on robot detection and did not report true negatives, there is no way to compare the effectiveness of the system in our study with previous studies in terms of the accuracy of human usage. with this caveat in mind, the inverse recall, precision and f-score in the present study are 0.9388, 0.7302, and 0.8214 respectively."
"digital commons is a hosted institutional repository platform, with 400 participating institutions (digital commons, n.d.-c). all open access articles hosted on a digital commons institutional repository are discoverable through a single system known as the digital commons network, which consists of more than 1.5 million open access works (digital commons, n.d.-a)."
"limited our ability to systematically quantify the activation patterns of the muscles in the forearm. previous studies have investigated the degree of independence among individual fingers, focusing on the biomechanical coupling and neural control aspects. specifically, the extent of selective activation of the flexor digitorum superficialis (fds) has been examined using focal intramuscular electromyogram (emg) recordings. 5 a high degree of independence between the index and middle finger compartments of the fds has been observed, and to a less degree in the ring and little compartments. these findings are largely in agreement with behavioral findings in finger kinematics with a low level of enslaving between the index and middle fingers, and a relatively high level of enslaving between the ring and little fingers. 11, 12 using recently developed high-density (hd) emg recording techniques with closely distributed electrode grid placed on the skin surface, several studies have quantified the overall patterns of muscle activation on different extrinsic finger extensor muscles. 13, 14 however, the features of distinct activation patterns of forearm flexor muscles, which include both global muscle and localized motor unit activities during individual finger flexions, have not been fully investigated. accordingly, the objective of this study was to quantify the spatial activation patterns of the forearm flexor muscles during individualized finger isometric flexions at different force effort levels. specifically, hd emg signals were obtained from the forearm flexor muscles, and were used to explore the macro-level spatial patterns of emg activity and micro-level motor unit distribution. the extent of unique patterns of muscle activation during individuated finger flexion was quantified using pattern recognition approaches. our main findings revealed that the partial overlapping of neural activation of the forearm flexor muscles can limit us in uniquely identifying specific finger movement based on limited recording channels and limited emg features, and that hd emg recordings capturing detailed spatial activation patterns at both macro-and microlevels may be necessary to address the issue. one major advantage of our approach is that it allows us to accurately capture individual finger movement using noninvasive recordings, which can facilitate further development of rehabilitation or assistive technologies. the rest of the paper is organized in the following sections. section 2 introduces the experimental protocol, the features of quantifying muscle activation and the method of pattern recognition. section 3 reports the summary of muscle activation features and the performance of classification. section 4 provides the implications of our findings, guidance for future work, and a summary of our current work."
"in addition, the results of the confusion matrix for the different finger pairs under individual classifiers were illustrated in fig. 10 . for example, if \"1.9\" was shown in the index row and middle column, it means the percentage of the classifier falsely detecting middle finger as index finger was 1.9%. the results generally revealed that the full feature set tended to yield a better classification accuracy. specifically, the ring finger tended to have the best performance and the middle and index fingers tended to have worse performance, compared with the other two fingers."
"overall, using hd emg recording and processing techniques, we observed distinct activation patterns of the forearm flexor muscles during individual finger flexions. our overall results revealed that the spatial activation of the ring finger compartment was mostly distinct from the rest of the finger compartments, whereas the activation patterns of the middle finger were the least distinguishable. in contrast, the different finger activation patterns can still be classified in high accuracy (∼97%) including the levels of muscle contraction, using pattern recognition approaches. these results can provide guidance, at both macro-and micro-levels, in locating the different compartments of the finger flexors. our findings may provide baseline information for the evaluation of altered activation of the forearm flexor muscles in clinical populations, and provide feasible control input variables that can be used in rehabilitation/assistance of individual finger control."
"to label downloads, a robot detection script reads the server log file and checks each request to see if the ip address and/or the agents are in the database. if the exact agent string is found in the database, any new ip addresses using that user agent string are flagged as potential robots. if neither the ip address nor the exact user agent string is found, the script checks whether the agent matches the prepopulated agent list or if the request was for the decoy web page or robots.txt. if any of these conditions are true, the new ip/agent pair is recorded and all downloads from the flagged ip addresses from a given date forward are discounted from the download totals, pending a manual decision [cit] . figure 1 describes this decision tree. in addition to the robot detection script, the minho add-on provides an administrative interface that identifies ip addresses with an access frequency of less than one minute, with more than 10 hits, and with a high number of sessions [cit] . lists of the most frequent ip addresses can also be viewed for any period of time. these tools can be used in combination to make manual decisions on an individual ip address basis, which can then be added to the database using a bash script."
"logically this method will result in a number of false negatives (unfiltered robot downloads). false positives (human downloads discounted as robots) are also a potential side-effect due to factors such as network address translation (nat). this results in a forced trade-off between recall and precision: the shorter the timeout period, the more false negatives (reducing recall); the longer the timeout, the higher the false positives (reducing precision)."
dspace has functionality to detect robots using three methods. it first checks the user agent string for each download and page view against a list of 235 known user agent patterns (regular expressions).
"regardless of which stance one takes, any data used as a metric or simply publicised for promotional purposes must be accurate in order to be useful and credible. a great challenge to this in any web environment is the use of web robots, operated by search engines and comment spammers alike, and accounting for between 8.51% and 32.6% of web traffic [cit] . robot traffic can vary widely depending on the type of web site, with a study on the internet archive finding as much as 93% of requests attributable to robots [cit] ."
"this subsection shows the effectiveness of our interface of predictive math entry. our interface was compared to a general formula editor and an input interface using tex. our interface of predictive math entry used in the experiment is the interface depicted in figure 1 . we eliminated the function of our predictive entry from the above interface. that is used as a general formula editor. we originally implemented an input interface using tex on javascript. the screen shot is depicted in figure 3 . a user inputs mathematical formulas using tex command in its text area. when the user clicks the compile button, the system outputs the compiled formulas with rendering. this interface does not comply with wysiwyg."
"although multiple finger movement covers a wide range of daily tasks, here we focused on the individual finger movement because many daily tasks require individualized finger movement (e.g. typing or dexterous object manipulation) and it can be used as the foundation for the multiple finger study."
"the importance of the usage statistics, both as indicator of the effectiveness and value of the service, and as a service itself, raises the following questions: how successful is the robot detection technique used at this repository? how accurate are the alleged human download statistics given to end-users? the study may be able to shed light on the effect an ip address-based permanent blacklist (the dhcp problem) has in terms of human downloads classified as robots (false positives)."
"unfortunately, robot detection in dspace seems to have suffered a certain amount of neglect: at the time of this writing, the ip address lists have not been updated in nearly six years [cit] ."
"as described in this paper, we proposed an input method that predicts the next input characters for mathematical formulas. n-gram model is applied to our method, which is a popular probabilistic language model. we incorporated hierarchical information in mathematical formulas into an n-gram model. the calculated probability was used for predictive math entry. the proposed method is evaluated using prediction accuracy. results showed that the prediction accuracy of our method is higher than that of other baseline methods. the proposed input interface was evaluated using a user experiment. results showed that our interface outperforms the input method without predictive entry and the input method using tex in the usability. we expect that our interface for predictive math entry shall contribute to the spread of math contents. the ease with which users can forecast the prediction proposals is important for usability. we will examine the relation between user predictability and usability for the predictive math entry."
"furthermore, two sets of features were also considered separately for the classification. first, we used \"full features\" including all the rms values in the entire 2d rms map and all the identified iz locations. since the number of motor units varied after decomposition and selection, each trial may have a different number of izs. a bootstrap method was used to resample the iz locations to make the iz feature an equal length for the classification. the matlab function \"bootstrap\" was used to randomly resample the iz location of each trial, while maintaining the mean of the resampled data the same as the mean of the original data. after the bootstrap procedure, all the three trials would have the same number of data samples, which can then be used for classification. second, only five \"reduced\" features of the 2d rms map and izs were employed, including the mean rms value, c x, c y, iz x, and iz y . these reduced features have been widely used to understand the muscle activation patterns. however, from a computational efficiency perspective, the calculation of these reduced features can add extra computation load, and make the classification inefficient."
"the classification results show that the full features including the 2d activation map and the motor unit iz features provide the most accurate performance. however, the high feature space can be computationally inefficient, which poses challenge for real-time classification and control. additionally, the motor unit decomposition in our study was performed offline, although online emg decomposition has been studied, 42 much work still need to be performed before motor unit features can be used for real-time classification. to reduce the computation load, we also showed the classification performance using the 2d rms map with reduced density of electrodes. our findings revealed that the accurate performance (∼93.52%) can still be acquired using 2d rms map only with eight electrodes, though the robustness of the classification in daily activities or during long-term recordings need to be further evaluated. other studies 43, 44 have also investigated the robustness of pattern recognition for myoelectric control when some channels of hd electrodes are corrupted or the location of electrodes is shifted."
"in this paper, we propose an input method incorporating a function to forecast mathematical characters that the user will input next. for predicting the subsequent characters that the user will input, the forecasting method uses the characters in the mathematical formulas that the user has already input. the target of the forecast is limited to mathematical structures or characters that cannot be input using a keyboard. then, using keyboard, a user selects a prediction proposal given by our input method."
"the robot detection technique used by digital commons consists of a number of filters. downloads from known robots declared in the user agent string are all discounted, as are download requests that result in a http response code other than 200 or 302. the referrer field is checked for automatically generated urls, for example a referring url that is identical to the url of the requested resource."
"however, mathematical formulas have still not spread to the internet as a representation media to the same degree as other media such as text, graphics, sounds and movies. the currently available bothersome methods of inputting mathematical formulas are a deterrent against formula usage on the web. although we usually use only a keyboard when inputting natural languages, one must use both a keyboard and a mouse when inputting mathematical formulas. mathematical formulas are not simply presented as the sequence of numerical numbers, alphabet and other symbols. they are usually presented as a part of some structures such as fraction and exponent, which cannot be input using a keyboard. users input them by clicking special buttons for the functions on the formula editor with a mouse. we believe that insisting on users using both a keyboard and a mouse engenders irritation when inputting mathematical formulas."
an n-gram model is a popular probabilistic language model in the natural language processing. we propose a method for predicting user's inputs of mathematical formulas using an n-gram model.
"the results related to quantitative indices are presented in table 5 . the number of times necessary for key inputs and those of mouse clicks (hereinafter, #keyinputs and #clicks) are also presented in table 5 . furthermore, we examined whether a statistically significant difference exists among the interface on these indices using a student t-test. the results are presented in table 6 . in tables 5 and 6, \"ours\" means our interface with predictive math entry, \"w/o prediction\" means the input interface without predictive math entry and \"tex\" means the input interface using tex. we confirmed the statistically-significant difference among interfaces in all indices except #incorrect between our interface and that without predictive entry."
"experimental data from nine right-dominant healthy human volunteers (7 male, 2 female; aged 19 to 35) were recruited. all subjects provided written informed consent, and none of them reported a previous history of arm or hand pathology or surgery. the experimental protocols were approved by the institutional review board at the university of north carolina at chapel hill."
"manual outlier checking, performed monthly at ucd, increases robot recall by 0.05; this in turn improves the reported human download precision by 0.14. currently, neither dspace nor eprints support the ability to manually check for outliers. adding this capability and/or the robots.txt/trap file feature, common to many studies and the minho system, could significantly improve the accuracy of these systems' usage statistics."
"for modeling the math input, the log data are divided by the hierarchical level. the log data corresponding to the k-th level are designated as \"k-th level log data\". not only the characters in the k-th level but also those in the higher level are used for representing the k-th level log data because, when predicting the next input in the lower level, the last characters in the upper level can be a trigger here. for example, in the sequence (1), 0(2), (2)   appeared in the above log data, 0 and  is often used for integral range. therefore, these characters occurred by  as a trigger. the log data of each hierarchical level obtained from the above log data are shown below."
"subjects were seated upright in the experimental apparatus, with the right arm comfortably placed on the horizontal table, the proximal forearm and elbow stabilized on a padded platform, and the palm oriented at a neutral position with respect to flexion/extension. their four fingers (index, middle, ring, and little) spread comfortably, and each finger was secured via a velcro strap to one load cell (interface, sm-100n), which measured the force generated from individual fingers. to avoid utilizing other muscles to generate the force, their wrist was in 0"
"the decoy web page (trap file) feature is not currently used, so the detection procedure is essentially based on accesses to robots.txt, self-declared robots (in the user agent string) and all ip addresses and user agent strings previously determined to be robots. the date limit also is not used, so downloads from all flagged ip addresses are removed from the totals from the earliest date."
"the first manual pass through the data produced 242 robots (82.88% of the total robots found, 70.97% of all downloads) that self-identified in their user agent string. 40 robots (13.70% of all robots, 11.73% of all downloads) were determined through a combination of origin, user agent string and behaviour in the second pass."
"the minho stats add-on stores every bitstream download (pdf or other file format) in the dspace database. the system takes a multi-faceted approach to robot detection including matching against a pre-populated list of 793 known agents, detecting accesses to a decoy web page, and accesses to the site's robots.txt file. the database contains related tables of ip addresses and user agent strings (in addition to the pre-populated agents list) that have been previously identified as robots through log analysis. downloads found to be robots are labelled and discounted from the download figures presented to end users."
"the user agent filter in eprints is static and will not detect badly behaved robots. since it is not intended to detect robots (though it probably does, and is a legitimate detection technique), the repeat download filter is a missed opportunity for basic machine learning, since it never records the ip addresses or user agent strings of the agents that it detects. this means that a robot that eprints correctly identifies today (albeit accidentally) could be completely ignored tomorrow."
"the objective of this evaluation is to ascertain (1) whether our proposed prediction method outputs more highly accurate prediction results than those provided by baseline methods, and (2) whether the hierarchical n-gram model is effective for math input. as baseline methods, we use a method using only the user's recent inputs, a method using only the user's past inputs, which means that the method uses user's recent inputs, and a method using both methods. these baseline methods can be regarded as the simplest prediction methods. to evaluate the effectiveness of hierarchical modeling, we compare our hierarchical n-gram model to a general n-gram model that uses no hierarchical information."
hierarchical information works well for a case in which the user inputs characters at a shallower level after inputting characters at a deep level. we examine the prediction accuracy for this case. those cases cover about 15% of input positions in our dataset. table 2 shows results for those cases. they reveal that we can increase the prediction accuracy using the hierarchical information. it might decrease the overall usability if the prediction accuracy decreases for some input conditions. we infer that incorporating hierarchical information into the model is effective for predictive entry.
"the fallibility and inconsistency of expert opinion, the paradoxical nature of technique testing, and the final results of various detection studies (as seen in table 2 ) suggest that absolute certainty in robot detection is very likely an unrealistic goal. cast more positively, many studies clearly note that detection techniques should be multi-modal, making use of a variety of data and techniques to arrive at a bestpossible result [cit] ."
"web robot detection is most successful when a variety of data and techniques are combined to achieve a best-possible result; no technique or combination of techniques will produce usage statistics that are completely free of robot downloads. this study has shown that very accurate robot detection at low cost in terms of computing resources and staff time is possible in community developed, free open source oa institutional repository systems. this is the first web robot detection benchmarking study performed on a scholarly oa institutional repository to be reported. [cit] ."
"as with other information services, oa repositories often collect usage statistics for the items they host, typically as full text download counts. opinions on download statistics are somewhat divided, with some arguing that they are problematic and unhelpful (cornell university library, n.d.), while others make free use of download statistics, ranking papers and even authors, distributing them monthly to participants, and advertising them broadly to the public (gordon and jensen, n.d., zimmerman and baum, n.d.) . download statistics have even been shown under certain conditions to be predictors of future citations [cit], arguably the most important metric for scholarly and scientific research publications."
"since digital commons is a centrally managed network of repositories, robot detection is carried out across a much larger dataset than any single repository. the size of the dataset affords a view of user behavior that would not be possible at a local institutional repository, and any rule applied to one repository's data is applied to all. this results in comparable counter compliant download statistics across all sites (digital commons, n.d.-b)."
"a number of assumptions were made while labelling the downloads. first, that ip addresses originating from search engine companies are always robots. this could potentially rule out genuine downloads made by employees of the company."
"usage metrics are commonly used in library and information service environments to assist with decision making such as journal purchasing, collection building, and item deselection, and to demonstrate the overall value of the services themselves. scholarly open access (oa) repositories, freely accessible full text repositories of scientific and scholarly publications, are one such service within the higher education and research information sector. [cit] with arxiv.org, the electronic pre-print archive of papers in physics and similar subjects (cornell university library, n.d.), the number of oa repositories has grown to more than 4,000 [cit] (university of southampton and eprints.org, n.d.) . many of these repositories are hosted locally by universities for selfarchiving by the academic and research staff of those institutions and are known within the community as institutional repositories (irs)."
"irstats filters downloads based on two principles. the first is a list of 960 user agent strings (regular expressions) of known robots or crawler software. if the user agent string recorded in the eprints access database matches any of these, the download is not counted. the second filter checks how often a single ip address downloads distinct items; by default if it downloads an item more than once in a 24 hour period, only one download is counted towards that item for that period. the stated goal of this filter is \"to detect so-called double-click downloads\" (françois, 2015) ."
"the content and hierarchical level of the user's input are recorded in the log data of math input. here is an example of mathematical formula for showing the actual log data. for characters that can be input using a keyboard, the unit for recording the log is a variable, numerical value, operator, function like sin and log. the name of function is detected by preparing a dictionary in which popular function names are recorded in advance. for characters that cannot be input using a keyboard, the unit for recording the log is a character that is obtainable by clicking an input button in the formula editor. characters that cannot be input using a keyboard are greek alphabet characters, mathematical symbols such as differential  and quantifier , fraction, operators such as  and , large operators such as a summation symbol and integral symbol, accents such as tildes and circumflexes, script such as subscript and superscript. one of the log data of the above formula becomes the following. the number written in a parenthesis is the character's hierarchical level in the formula."
"a learning dataset is required for constructing a hierarchical n-gram model. it is ideal to create the model from the user's own log data. however, it is difficult to prepare a massive amount of log data of a target user (a user who will use the predictive math entry) in advance. therefore, we prepare general log data of math input obtained from several users. the n-gram probability in each hierarchical level is calculated based on the general log data. to reflect the target user's input pattern in the model, the input log obtained while the user uses the predictive math entry is added to the log data. the hierarchical n-gram model is updated using the additional log. the prediction is made based on the n-gram probability according to the hierarchical level where the user sets focus by a keyboard or mouse. characters with high probability to the last n-1 inputs are output as prediction proposals."
"the ratio of robots to humans visiting a site determines the relationship between robot detection recall and precision and the precision of reported human downloads (inverse precision; see appendix 2). since the robot to human ratio is very high in our data, an increase of 0.01 in robot recall at the current precision would improve the precision of reported human downloads (inverse precision) by 0.03. increasing robot recall by 0.05 would improve human download precision by 0.22. if the robot:human ratio of 85% is generalizable, as the findings here and by irus-uk would suggest (information power ltd., 2013), small improvements in any oa ir's robot detection could have significant effects on the precision and veracity of their usage statistics. conversely, robot detection techniques that do not evolve with the advancement of web robots will result in usage statistics whose accuracy diminishes exponentially over time."
"outside the robot detection script a number of indicators are browsed monthly including the most downloaded items, the top twenty most frequent ip addresses (not already flagged as robots), and the daily download rate for the previous month. any notable spikes (outliers) are investigated by checking the logs and performing a reverse dns name lookup on the ip address. figure 2 shows an example of an outlier that would warrant investigation. quite often, a number of \"badly behaved\" robots are found this way each month. [cit] . through a combination of the automatic robot detection script and manual intervention including reverse dns name lookup, rate, volume, and interval between downloads, the robot detection technique used at ucd takes into account up to nine of the 17 elements listed in table 3."
the institutional repository at university college dublin (ucd) collects usage statistics using the university of minho statistics add-on for dspace (version 4 for dspace 1.8.2). download statistics are visible in the item record of each paper and at each level of the collection hierarchy from a dedicated subsection of the website. individual statistical reports are sent automatically each month to every author that has uploaded a paper to the repository. reports are occasionally provided to schools and research centres in the university and are often used by them in formal quality reviews.
"predictive text entry is popular for inputting natural language, especially for handheld devices. in the research area of natural language processing, dictionaries are invariably used to realize predictive text entry [cit] . the dictionary usually covers text elements with high appearance frequency. the forecasting method using the dictionary outputs prediction proposals when the text part the user has input most recently begins with some text in the dictionary. it shows the user the remainder of the matched text parts in a word or a phrase as a prediction proposal. it usually orders prediction proposals according to the most recently used order or the most frequently used order [cit] ."
"the repeat download filter is an interesting application of the volume of requests technique. it is not robot detection in the strictest sense (and is not advertised as such) since it intentionally allows some downloads to be reported as legitimate despite having been (very likely correctly) detected as robot downloads. at least one robot download per item per 24 hours is allowed by the filter, and if the same robot downloads every item in a repository once in a single day, all of these downloads will be counted as legitimate. still, the filter no doubt greatly limits the effect of robots that could only be detected this way, and does so without manual intervention."
"the majority of studies reviewed were based on session data for the identification of robot sessions [cit] . as mentioned, in our experience irs typically have little or no real session data, instead usage is mostly limited to once-off file downloads directly from search engines. for this reason, rather than group downloads into sessions, we manually label a simple random sample of individual downloads to determine the recall and precision of the robot detection technique in use at ucd."
"for the rms calculation, we subtracted the rms value of the resting period from the steady contraction period to reduce the influence of noise and undesired residual muscle activities. since the signal components are not orthogonal and the calculation itself is not linear, our direct subtraction cannot completely remove the interference effects."
"the lag cross-correlation as a common iz identification method 23, 24 was utilized in this study. the procedures of the lag cross-correlation method in our study were conducted as follows:"
"this subsection validates the effectiveness for adding the target user's log data to the learning data. this might deal with the inconsistency problem of the input order among users. 400 log data are always used as learning data in this evaluation. one user's 500 input logs are divided in to five sets, each of which has 100 input logs. each set is used as test data in turn. at first, 400 input logs consist of those of the other two users. as we explained, one formula has three user's input logs. one user is randomly selected from the other two users. the selected user's log is used here. we increase the ratio of the target user's log data to the all log data in the learning data set from 0% to 25, 50, 75, 100% and calculate the prediction accuracy. note that we do not add the target user's log data to the original 400 learning data but replace the log data for the same formula in the original learning data set. the number of logs in the learning data stays constant. this eliminates the influence of the increase of the learning data to the prediction accuracy. six users' logs are used for this evaluation."
"the evaluation of usability is accomplished according to quantitative indices and qualitative indices. the number of times of switching between a keyboard and a mouse (hereinafter, \"#switching\"), input time and the number of incorrect inputs (hereinafter, \"#incorrect\") are used as quantitative indices. questionnaires related to the usability of the interface, comprising multiple-choice questions and free description, were used as qualitative indices. in this experiment, 12 graduate and undergraduate students participated. we asked them about their experiences of inputting mathematical formulas on pcs. all users answered that they had some experiences on inputting mathematical formulas on pcs. they answered that they had used general formula editors or tex. among the participants, three users were good at inputting formulas using tex and were able to input most formulas without seeing the reference."
"the previous section showed that our proposed method outperforms other prediction methods in prediction accuracy. however, that fact does not mean that our proposed method helps users to input mathematical formulas. the objective of this section is to show that our interface of predictive math entry (an input interface incorporating our proposed prediction method) helps users' actual inputs by conducting user evaluation according to the interface's usability."
"innervation zone (iz): the location of iz, which reflects the regions of motor nerve innervation with the muscle fibers, 15 was exacted as a classification feature in this study. the hd emg recordings were automatically decomposed into individual motor units using fastica combined with the convolution kernel compensation (ckc) algorithm."
"this function shows primarily the effect that increasing or decreasing robot recall (at a given precision and ratio of robot downloads) has on the precision of human download counts. it could also be used to determine the inverse precision of a benchmarking study that reports r, p and t."
"in order to test a robot detection technique, researchers must first know \"exactly which sessions in the log file were created by robots\" [cit] . this often involves checking and labelling the data manually [cit], though often, due to the size of the dataset, robots in the test data are labelled automatically or semi-automatically [cit] using reliable detection techniques different from the ones being tested. [cit] discuss the strengths and weaknesses of manually versus automatically generated test sets. on the one hand, \"there is a reasonable guarantee\" that a manually generated test set is correctly labelled, however, manual labelling risks including only a narrow and nonrepresentative selection of robots that would normally crawl the server [cit] (p. 9) . on the other hand, they argue that there is no reason to assume an automatically generated test set is correctly labelled. to overcome the problem, the authors devise a method to test the test data (pp. 9-10)."
-prediction using recent inputs and their frequencies (rct & frq): this method orders the prediction proposals output by the recent input method described above according to their frequencies. the frequency of each candidate is calculated by counting its occurrences in the formula that the user is currently inputting.
"finally, the relative location of electrodes was placed based on anatomical landmarks of the arm, and was consistent across different subjects. given that our electrode pad has a fixed area, different subjects with different arm sizes could give rise to variations in the spatial features of the centroid calculations. however, it did not affect our classification results, since all our pattern recognitions were performed within each subject."
"we designate this probability as the n-gram probability. the calculation of this probability presents the process by which the user inputs the next word in the context that the user has input the recent words. this helps the prediction of the next input. here,"
"neither quantity (frequency) nor quality of tool use influenced performance. only quantity of tool use (time spent on tool) showed a significant effect on performance. added to this was the effect of selfefficacy which also impacted performance. these findings not only add to literature but also raise questions in relation to research on tool use in ile's. first our results contradict other findings in which a positive relationship was also found between tool use (quantity and quality) and performance [cit] which suggests that if the tool is there, it should be functional. the tool provided in this study was probably not entirely beneficial nor effective for the learning process [cit] . third, these findings also raise a question on whether the time spent on the tool may be indirectly related to the quality of tool use. the time spent on the tooladjunct question-could also be an indicator of quality of tool use as learners could spend more time thinking on the question than actually writing down a detailed and comprehensive answer. fourth, even though we aimed to ensure the relevance of the adjunct questions, it is possible that the relationships between the adjunct question and the questions in the post-test were a cause of our results [cit] ."
"automated computational techniques are essential for the quantitative analysis of cellular dynamics using time-lapse light microscopy. for example, to quantitatively reconstruct the development of large multi-cellular organisms such as entire drosophila and zebrafish embryos, tens of thousands of cells need to be segmented and tracked at high spatial resolution [cit] (fig. 1) . such analyses are of fundamental importance to understanding the development of biological tissues, to reconstructing functional defects in mutants and disease models and to quantitatively dissecting the mechanisms underlying the cellular building plan of entire complex organisms [cit] . however, many computational challenges are encountered when performing key tasks, such as image registration, cell segmentation and cell tracking, in complex microscopy datasets [cit] ."
"the regression analyses showed that unexpectedly, neither quality of tool use nor frequency affected performance. only the time spent on tools and self-efficacy influenced performance significantly (table 4) . table 4 . regression analyses. tool use and selfefficacy on performance even more surprising was that self-efficacy accounted for 9.61% of the variation in performance more than the time spent on tool use which accounted for 3.24%. this means that the relationship between self-efficacy and performance is stronger than that of time spent of the tools (figures 5 & 6) . figure 6 . effects of time spent on tool on performance."
"this paper aimed at gaining more insight into learners' quantitative and qualitative use of tools by exploring the impact of tool presentation (non-/embeddedness), tool intervention (non-/explained of tool functionality) and a motivational learner characteristic (self-efficacy) on performance. some learners had to answer the questions (embedded conditions) whereas others could choose whether to use the questions or not (non-embedded conditions). within these conditions the explained tool functionality was added in two of the four conditions (one embedded and one non-embedded condition)."
1. what is the effect of tool presentation (non-/embedded) on quantity and quality of tool use? 2. does non-/explained tool functionality influence quantity and quality of tool use? 3. how does self-efficacy affect quantity and quality of tool use? 4. do tool use and self-efficacy affect performance?
"self-efficacy had a negative influence on the frequency of tool use in both non-embedded conditions. that means that the higher the selfefficacy the fewer the learners accessed the tools. possibly the non-embedded conditions interacted with self-efficacy levels in learners. the higher the self-efficacy the lower the tool use access. this effect contradicts previous findings where the frequency of tool use was positively correlated to self-efficacy [cit] . however, unlike waldman [cit], who studied tool use by means of a survey, our study explored the use of tools in an experimental setting. additionally, self-efficacy is related to the belief of one's ability to plan/execute a behavior [cit] . thus, if one believes they have a great ability to organize and execute the course of actions required to succeed and reach a certain goal then one is capable of adapting their use of tools. this adaptation of tool use could be reflected in the frequency of tool use by the learners in our study. they probably adapted their use of tools and instead of accessing the tool 'too' often, they rather spent more time on it which leads us to our next result."
"to see if learners differed in their prior knowledge among conditions, a pre-test was conducted. it consisted of nine multiple-choice questions (with four choices each) and each correct answer was worth one point. learners could obtain maximum nine points. the nine pre-test questions explored learners' factual knowledge related to the topic of the hypertext."
"one of the results from this study is that learners in the embedded conditions spent proportionally more time on the tools than the learners in the nonembedded conditions. additionally, the nonembedded conditions used tools more qualitatively than the embedded conditions. however, based on the tukey pos hoc analyses, it was also found the quality of tool use was only better than the embedded condition with non-explained functionality. this effect could not be retrieved in the embedded condition with explained functionality. this finding suggests that the condition influenced quantity and quality of tool use. this result is also similar to previous research [cit] where participants in the embedded conditions also used tools more, but the conditions that were non-embedded had better quality of tool use. however, based on our results it is still not clear whether or not to embed tools. on the one hand, embedding tools can encourage more time on the tool(s). on the other hand, nonembedding tools can guarantee more quality of tool use. however, if we observe the results on what aspects of tool use influenced performance, in this case, time spent on tool. then we could suggest that embedding the tool may be in the end the best solution."
"the study was divided in two sessions. in the first session, the questionnaire of self-efficacy was administered to all participants at the same time in one of their classes of learning and instruction. afterwards participants got enrolled for the second session by means of an electronic learning platform. in the second session, participants attended by groups of maximum 20. first they were given the instructions, next they answered the pre-test, after they were confronted to the hypertext. once they finished reading the hypertext, they were given the post-tests."
"for the effects of self-efficacy on tool use (third question), we conducted regression analyses with self-efficacy as independent variable and quantity and quality of tool use as dependent variables."
"the ile had two introductory pages. in the first page, the participants had to write down their name. in the second page there were the instructions and the conditions with the explanation on the functionality of the tool, had the explained tool functionality ( figure 1 ). the explanation read as follows: \"each question will explore a part of your knowledge. if you answer each of the questions, you will be able to find a clearer connection between the topic of the text and everyday life situations. by establishing this link, your knowledge will become more meaningful. if your knowledge is more meaningful, you will have more sources to answer the post-test in a more effective way.\" after the second page, there was a hypertext titled: waarom water broodnodig is (why water is essential) which comprised 1,544 words and was divided into five paragraphs. after each paragraph, in the embedded conditions, the tool was automatically attached (figure 2 ) making a total of five tools, namely adjunct questions. in the nonembedded conditions, after each paragraph, learners had the option to access the tool by clicking on a button located in the upper right of the screen ( figure 3 ). as aforementioned, the tools were five adjunct questions which are considered a cognitive tool. adjunct questions were chosen based on the attention they have received on studies on tool use in hypertexts [cit], also because they aim to influence what it is learned from a text [cit] . moreover, research has indicated that adjunct questions enhance text comprehension by helping learners attend and focus on specific relevant portions of the text [cit] ."
"both pre-and post-tests were designed by three different researchers, authors of this paper, two of which had experienced in the design of this type of materials. consequently, the design of the pre-and post-test followed similar criteria used in previous studies [cit] ."
"for the data analyses, we conducted descriptive statistics with each condition and the quantity and quality of tool. next, to possibly observe differences among conditions, a manova was run with condition as independent variable and self-efficacy and the pre-test (prior knowledge), as dependent variables. if groups were not equal, then these variables were to be considered covariates in the further analyses."
"given two n-dimensional images of the same size, i t (source volume) and i tþ1 (target volume), our final goal is to estimate a motion field v p for each voxel p to register the target volume to the source volume."
"setting the correct value for nðsþ is crucial to achieve good flow estimations. in our case, the size of nðsþ is controlled by the parameter d s max, which defines the maximum distance (in voxels) between two region centroids to consider them neighbors or not. intuitively, we have reduced the complexity of nðsþ to one parameter per node that controls how global or local we expect object dynamics to be. we can determine an appropriate value for the d max parameter by qualitatively experimenting on different volumes or testing against some ground truth [cit] . tables 1 and 2 show that it is possible to find a single value that works well across very different motion regimens. however, if the user has a priori information of cell division locations or group motion, it is straightforward to locally set the appropriate d max for each region to improve accuracy results."
"drosophila dataset using the software package imaris (bitflow). each region represents different dynamic regimens (fig. 4) . we then manually assigned correspondences between segmented nuclei to calculate the displacement (fig. 4) . given that the nuclei are textureless, we cannot assign unique voxel-to-voxel correspondences, and thus, our ground truth evaluates center of mass displacement for each nucleus. we use the flow ee metric"
"the regression analyses illustrated in table 3 show that self-efficacy did not significantly influence quality and quantity of tool use, more specifically time spent on tool. an effect of self-efficacy was found in the non-embedded conditions where high levels of self-efficacy had an impact on the frequency of tool use, that is the clicks made to access the tool. the impact that self-efficacy had on frequency of tool use was, however, negative. the higher the levels of self-efficacy, the less the learners accessed the tools. this effect can be observed in figure 4 ."
"relevant tool characteristics are tool presentation and tool interventions. tool presentation refers to whether the use of tools is optional or obligatory for the learners [cit] also known as nonembedded or embedded tools [cit] . tools are nonembedded when learners have the choice and decide themselves whether or not to use tools and they are embedded when learners have no choice but to use the tools [cit] .with this in mind, one would assume that by increasing learner control thus by not embedding tools, the probabilities of using tools would increase. however, empirical studies have often falsified this claim. for example, in the study of greene and land's [cit] learners were provided embedded scaffold tools, namely guiding questions. the results indicated that learners tended to omit questions and/or give superficial answers. in a more recent research, it was revealed that learners with embedded tools use tools more than those with nonembedded tools, but learners with non-embedded tools used tools more qualitatively [cit] . thus, evidence seems unclear on what path to take regarding tool presentation. as a consequence in ile's, tool interventions have been implemented. tool interventions aim at increasing tool use probabilities without 'forcing' the learner, making the tool functionality more discernible and encouraging the use of tools more adequately [cit] . these interventions have been addressed in literature as instructional cues [cit], advice [cit] or pedagogical agents [cit] among others. they either provide guidance during a learning task [cit], encourage the use of tools that provide information that the learner has not accessed [cit] or strictly specify the functionality of the tools as the benefits that can be obtained by using the tool(s) [cit] . while evidence points out their positive effects on tool use [cit] mixed effects have also been revealed [cit] ."
"to answer question 1 and 2, hence to see effect of tool presentation and tool intervention on tool use, a second manova was conducted with same independent variable (condition) and quantity of tool use (time) and quality of tool use as dependent variables. an anova was conducted, to answer the same questions 1 and 2 but in relation to frequency of tool use (quantity) which was only in the nonembedded condition. non-embedded conditions were the independent variable and quantity of tool use (frequency) the dependent variable."
"interactive learning environments (ile's) are intended to support learner knowledge construction [cit] through the use of different types of (learning) tools which can be informative, cognitive or scaffold [cit] . the different kind of tools provide distinct learning opportunities. informative tools, for instance, give information to be learned or used in face of an assignment or problem (e.g., dictionaries); cognitive tools extend enhance or augment thinking (e.g., adjunct questions, concept maps) and scaffold tools guide learning efforts (e.g., study guides, guiding questions) [cit] . unfortunately, evidence suggests that providing tools does not necessarily result in learning gains as learners tend to avoid the use of tools and when they use the tools, the usage is suboptimal [cit] . research has pointed out that the problematic of tool use can be attributed to two main factors [cit] . one of them relates to characteristics of the tools. the other factor is linked to characteristics of the learners."
"finally to see the effects of self-efficacy and tool use on performance, thus answer the last question, regressions were also conducted. one regression was done with self-efficacy and quantity (time) and quality of tool use as independent variables and performance as dependent variable, another regression was done only with frequency as independent variable."
"most biomedical optical flow applications tend to implement and report results using similar methodologies to the ones explained earlier in the text without tailoring them to the characteristics of the data. [cit] presented a total variation (tv)-l 1 optical flow model for clinical datasets. however, even with the use of image pyramids to solve the problem efficiently, this approach was still slow for large 3d datasets, and it did not always outperform the insight toolkit (itk) implementations. itk is a multi-threaded cþþ library for n-dimensional image registration and segmentation, and it is the most common baseline for comparing the performance and accuracy of new algorithms in the bioimaging domain. many recent articles use similar strategies to target specifically time-lapse light microscopy datasets [cit], which demonstrate the general interest in applying optical flow to the type of datasets presented in this article. in the following sections, we present an optical flow formulation specifically tailored to solving optical flow for 3d time-lapse microscopy volumes. we show that our method is 10 â faster and reduces the average flow end point error (ee) by 50% for complex dynamic processes, such as cell divisions, with respect to optical flow algorithms available in the itk library."
"characteristic that would lead to (optimal) tool use. given that the nature of the motivation is broad, different associated constructs exploring different motivational aspects have been identified [cit], such as self-efficacy. self-efficacy is considered a key element of the social cognitive theory [cit] . it is defined as the thoughts and beliefs about one's capabilities to organize or perform activities to produce a given achievement [cit] . evidence [e.g. 16] has revealed that there is a positive relationship between self-efficacy and frequency of tool use, also known as quantity of tool use. this positive relationship has been extended to time spent on the tool (another aspect of quantity of tool use) [cit] . however, research suggests that that high levels of self-efficacy also seem to be negatively related to quality of tool use [cit] . therefore it is important to clarify the effects self-efficacy can have on tool use. furthermore, literature has implied that self-efficacy is also crucial in the relation to learning outcomes (performance) [cit] as it predicts performance [cit] . therefore, the impact of self-efficacy should be investigated both in relation to tool use and performance."
"no concrete results with respect to the explanation of the tool functionality could be retrieved, however, a slight effect could be observed in relation to quality of tool use. the pos hoc analysis revealed that the non-embedded conditions (with explained and non-explained functionality) used tools more qualitatively than the embedded condition with non-explained functionality. this effect could not be observed in the embedded condition with explained functionality. hence, the explanation of the tool functionality probably played a significant positive role in relation to quality of tool use."
"with respect to the result on self-efficacy, it seems that self-efficacy not only influenced performance but the self-efficacy effects were stronger than quantity of tool use, as they explained more variance. this finding adds to the literature on self-efficacy and its power to predict performance [cit] . they also bring to light the question on whether presenting the tools in different ways (non-/embedded) or influencing tool use by means of interventions (explained functionality) is beneficial for learners that are self-efficacious. however, we are far from disentangling the complexity of tool use in ile's. therefore further studies could not only give a deeper insight on the role of self-efficacy in ile's but also on whether or not to embed tools, on the design of an 'effective' explanation of the tool functionality as when and how often the explanation should be provided. additionally, methodologically speaking, as in previous studies [cit] adding a control condition-a condition without tools nor explanationcould provide deeper insight on the effects of tool presentation and explanation of tool functionality on the use of tools as this could provide a baseline for the analyses."
"we evaluate our approach in scanned light-sheet microscopy datasets. light-sheet microscopy provides exceptionally high imaging speeds while minimizing the energy load on the biological specimen, and has thus emerged as an essential tool for life sciences. this combination of capabilities is invaluable for live imaging applications and enables quantitative imaging of cellular dynamics throughout the development of complex organisms such as entire drosophila and zebrafish embryos ( fig. 1 and videos in the supplementary material). light-sheet microscopes often produce terabytes of image data per specimen, which need to be analyzed with efficient computational approaches."
"we developed and tested a new model for optical flow tailored to microscopy volumes, in which a large fraction of the objects are textureless and similar in appearance. moreover, the information in the volume tends to be sparse because many voxels do not contain any information and cellular dynamics can be very variable. a key idea in our approach is to generate a volume partition graph over the foreground voxels, and to perform optical flow directly on that model instead of computing it at the voxel level. this model is tailored to the specific characteristics of time-lapse light microscopy datasets, as it provides the regularization needed to solve optical flow robustly for these types of volumes. at the same time, our method reduces the complexity of the problem by an order of magnitude, which is an invaluable advantage when working with large 3d datasets."
"additional evaluation of the proposed and baseline methods using synthetic data is provided in the supplementary material. we simulate fluorescent nuclei images with different types of motion (linear, cell division and brownian), different signal-to-noise ratios, different cell densities and different photobleaching settings to show that our method is applicable to different types of fluorescence microscopy techniques and cell dynamics."
"our results reconfirm that embedding tools may increase the time spent on tools and consequently have a positive influence performance, but nonembedding tools may lead to superficial tool use (low quality) which could possibly affect learners' performance. although in our study the effects of quality of tool use on performance could not be retrieved, previous studies [cit] have suggested that there is a relationship between quality of tool use and performance. this paper also provides evidence that tool interventions such as the explanation of the tool functionality may play a role in ile's. finally, this research also sheds light to the study of self-efficacy on tool use. the importance of self-efficacy in ile's is emphasized as effects of self-efficacy on tool use were not only retrieved but also direct effects of selfefficacy on performance. overall, these results reveal that tool presentation and self-efficacy may affect the quantity and quality of tool use and that quantity of tool use and self-efficacy impact performance."
"for the purpose of a quantitative performance analysis, we selected two regions from two consecutive time points in the drosophila dataset and performed a ground truth annotation for both of them. figure 5a shows comparative results for the first test region between time points 39 and 40. this region comprises 214 cells with an average diameter of 11 voxels moving all in the same direction, although at different speeds. in this example, the motion between cells is coherent, and thus, smoothness constraints are sufficient in most voxels to compensate for lack of texture. in this case of simple dynamics, our method has an average ee of 0.07, whereas the best itk method has an average ee (normalized by nuclei diameter) of 0.10. however, tested on the same hardware, our implementation is consistently 10 â faster. in particular, it takes 3 min to converge for each 3d volume, whereas both itk algorithms require $30 min for the same task. one of the main reasons for the speed improvement is the dimensionality reduction achieved with super-voxels. as an example, in this particular stack, there were 1 117 920 foreground voxels, which resulted in 19 274 super-voxels, reducing the size of the optimization problem $60-fold."
descriptive statistics illustrated in table 2 show that all students in all conditions used the tools as well as the differences among conditions. table 2 . descriptive statistics with every condition and quantity and quality of tool use. only frequency of tool use is reported in the non-embedded conditions due to the fact that in these conditions the tool access was optional.
"when most objects present in the volume are textureless and similar to each other, single voxels are not very informative. in other words, just trying to match single intensities leads to poor solutions. most optical flow approaches try to guide the registration in textureless areas by imposing a smoothness constraint between adjacent voxels. unfortunately, microscopy volumes tend to contain many background voxels, which also misguide the smoothness constraint. thus, we need better partitioning of the volume to improve optical flow."
"the super-voxels form a partition of the elements in the volume foreground. the final step needed to model the volume is to connect neighboring super-voxels to capture common dynamics between regions. we will define an edge between two super-voxels if their centers of mass are below a distance threshold d max . this definition forms an mrf (or equivalently a partition graph) over the foreground voxels (fig. 3d), where we can directly impose smoothness constraints to calculate optical flow. this setup is necessary because often two regions with coherent dynamics are completely disconnected by background voxels, so traditional voxel-based regularizations are not as effective."
"this possible effect, however, could be overpowered by the presentation of the tools (embedded vs. non-embedded). it is possible that the explanation of the tool functionality did not have a strong impact on tool use due to the nature of the explanation. that means, the explanation was only presented once in all the hypertext. perhaps, adding the explanation more than once or before the learner could access the tool could have stronger effects."
"effects of the different non-embedded conditions could not be retrieved in relation to frequency of tool use. the non-embedded condition without explained functionality accessed tools more frequently, in comparison with the non-embedded conditions with explained functionality which reported fewer clicks to access the tools. even though these effects were not significant, they may be to a certain extent related to the finding regarding self-efficacy."
"to quantitatively assess performance, we manually segmented nuclei in two different regions of adjacent time points in the the most significant improvement is obtained by moving from a voxel-based registration to a super-voxel-based registration. however, all elements described in this article improve optical flow accuracy. the default method refers to our method with the parameters defined in section 4.2. section 1.3 in the supplementary material contains a full description of implementation decisions involved in the deactivation of algorithmic modules for each row in this table. each entry in the table is equivalent to a data point in the plots from figure 5a . ee x% ile indicates the x th percentile of the list of ee errors for all nuclei in the ground truth annotation. each entry in the table is equivalent to a data point in the plots from figure 5b . ee x% ile indicates the x th percentile of the list of ee errors for all nuclei in the ground truth annotation."
"where d r, s is the distance between the center of masses of super-voxels r and s, and volðrþ is the number of voxels contained in region r. intuitively, the first term decreases interaction between super-voxels if regions are far apart, and the second term decreases interaction if they do not represent large sets of voxels. even with this region-based regularization, the data term is still not powerful enough to always return the right solution (table 1), as most of the objects in the volume look very similar (fig. 1) . in our case, the term nðsþ connects entire neighboring regions (not only adjacent voxels), which agrees with the assumption that we have multiple cells with common dynamics in some areas. by connecting non-adjacent super-voxels, the smoothness constraint is imposed much more efficiently over non-connected objects with similar dynamics."
"where p is the set of voxels in the volume, nðpþ are adjacent neighboring voxels in the volume (using 2n or 3 n à 1 connectivity) and d and c are robust cost functions such as huber penalty, l 1, tv or lorentzian [cit] . the first sum term in equation (2) with d can be considered a unary potential or data term, in which we want to match the intensity between two volumes. in this context, robust metrics are important to allow fluctuations in the volume intensity. however, this term by itself does not offer enough constraints for the motion field v p . thus, the second term in equation (2), referred to as the pairwise potentials or smoothness term, is incorporated to regularize the solution. here, robust metrics are important to allow for discontinuities in the flow field between different objects in the scene [cit] . finally, it is common to adapt the smoothness term at the pixel level by defining a weight w p, q based on edge intensity, effectively reducing the importance of the smoothness constraint in areas of possible motion discontinuities. fig. 2 . block diagram representing the pipeline described in this article to estimate optical flow. optical flow is performed over a set of super-voxels in the volume foreground, and the smoothness constraints are imposed between neighboring (and possibly non-adjacent) super-voxels instead of between connected voxels. this approach guides the registration process of neighboring nuclei with similar dynamics to a better solution than previous approaches robust metrics alone and voxel-wise smooth flow assumptions are not enough to handle the challenges present in microscopy volumes: given the sparsity, the lack of distinct features between objects and the multiple dynamics in a single volume, the energy terms defined in equation (2) are not strong enough to guide the optimization process to the right minimum, as shown in section 4. using equation (2) as a model and the mrf over super-voxels constructed in section 3.1, we can define a new optimization problem:"
"given the mixed results regarding tool and learner characteristics on tool use, this study aims at gaining more insight into the effects of tool presentation (embedded vs. non-embedded tool), tool interventions (explanation of tool functionality vs. no explanation of tool functionality), and self-efficacy on tool use. moreover based on the theoretical framework, tool use will be explored in two different ways: quantitatively (time spent on tool and frequency of tool use ) and qualitatively. finally the effects of tool use and the influence of self-efficacy on performance will be analyzed. the following questions are then addressed:"
"in section 4.1, we showed that the method might fail in some extreme cases for $1% of the nuclei, when neighboring nuclei move in opposite directions. in those scenarios, we are left only with the data term to determine the correct flow. thus, a possible future direction would be to use different features or point descriptors in the volume intensity to increase robustness of the data term [cit] . it is also possible to constrain the flow field to a diffeomorphism, as two objects cannot originate from the same source point. finally, if a faster implementation is required, it is straightforward to parallelize the computation of the data term in equation (3) for each super-voxel using gpu technology. at the moment, this operation takes $40% of the time for each function evaluation in the quasi-newton method, and it is thus a primary candidate for code optimization."
"first, we generate a foreground/background mask ( fig. 3b ) to ignore voxels containing no information in the volume. this mask can be as simple as an intensity threshold or any other existing background detection method. aside from removing non-informative voxels, the mask also helps speed up convergence, as it reduces the number of motion vectors v p we need to estimate. data sparsity is problematic and advantageous at the same time, as it precludes the imposition of standard smoothing constraints but it allows a reduction in the size of the problem in the flow calculation."
"tool use was analyzed through log and text files. for quantity of tool use, frequency of tool use (clicks made by learners into the tool in non-embedded conditions only) and time spent on the tool (in seconds, all conditions) were kept in an individual log file per learner. for quality of tool use, the correctness of the answers on the tool: adjunct questions were individually logged as text files and later analyzed."
"we tested our approach in two different biological model systems using previously published datasets of drosophila [cit] and zebrafish [cit] . two videos are included in the supplementary material to show the complete results of the optical flow estimation and how it allows analyzing different motion patterns for different groups of cells. each volume of the drosophila dataset consists of 602 â 1386 â 110 voxels (179 mb in uint16), and each pair of time points was processed in 3 min with our method (all central processing unit (cpu) running times reported in this article were determined on a workstation with intel õ xeon õ x5690 cpu with 3.47 ghz clock rate). in total, we processed 50 time points (9 gb of data) following a cell division wave in early development. each volume of the zebrafish dataset consists of 1064 â 1034 â 500 voxels (379 mb in uint16), and each pair of time points was processed in 9 min with our method. in total, we processed 220 time points (83 gb of image data) to follow epiboly and the formation of the body axis."
"context aware systems are capable to detect changes and are able to change their behavior to adapt to the changing context. in such systems, changes are not only performed by users by also other sources are involved. therefore, good understanding of the context is necessary. furthermore, there is a need for user and context modeling in the re design phase. this should be done through precise context engineering, identifying different context elements and their dependencies. distinguishing between stable and non-stable context is important and useful for the decision phase. with respect to adaptation, the needs and strategies for adaptation should be identified. model-based re such as scenario-based approaches could be applied in order to link context information to adaptation strategies. scenario-based approaches are useful for the development of systems when the context changes are predictable or at least have a low degree of uncertainty. additionally, user modeling techniques are useful to present the participant aspect of the usage context. the so far activities should be done at the requirement and engineering design phase through a requirement elicitation and modeling."
"9 transitions for a total dimension of 110gb of data (reached by the model representing a simple load balancing system composed of 10 clients, 2 servers, and between these, a load balancer process). it is worth noting that most of the tools belonging to the current state-of-the-art were not able to verify temporal logic formulas on this model [cit] . our experiments point out a clear trend: the more is the complexity of the model to be analyzed, the more is the scalability of our distributed algorithm. in fact, the speedup gained during the analysis of the most complex system (the simple load balancing model) greatly overcome the one gained in the analysis of the other ones (sized with a lower order of magnitude). we reach a super-linear speedup during the evaluation of a eg-type formula ( fig. 1 ) on this model."
"feedback loop: from control theory to software engineering: the notion of feedback loop has been widely used in the field of control engineering. actually the control loop is recognized as the central element of control theory. [cit] presents a generic model of a control loop that includes four key activities namely: collect, analyze, decide and act. [cit] upgraded the generic model by identifying properties of control for each activity which were ignored in the generic model. for example in the analyzing part, we need to know how much past state may be needed in the future. in the decision part, we need to know how the future state of the system is inferred. or what are the priorities for adaptation across multiple control loops. and finally in the last part, action part, we need to know when the adaptation should be performed."
"the authors would like to express their thanks to s. stanko and t. brehm of fraunhofer fhr as well as e. meier and c. magnard of rsl zurich for providing the memphis test data. this work is supported by the helmholtz association under the framework of the young investigators group \"sipeo (vh-ng-1018, www.sipeo.bgu.tum.de)."
"in this paper, we try to further reduce the gap between these different areas of expertise by providing tools and techniques able to cope with the complexity of real world examples models, in a \"big data\" fashion. our software tools permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org."
"(b) formula (2) can be used to predict the distribution of long-range white matter connectivity. the areas of the yellow rectangles in the left and middle panel represent the percentage of bundles for a frequency domain. note that the area of each rectangle is constant and that the two sides of the rectangles change according to the doubling-halving algorithm of formula (2). the empirical distribution is shown in the right panel [cit], provided by olaf sporns)."
"wherec is the sample coherence matrix of a resolution cell, is optimized in order to find the most likely combination of up to n −1 scatterers (where n is the number of receiving antennas of the multi-baseline system). instead of the one-dimensional search with following peak detection that usually is employed if music is used for the reconstruction of discrete scattering profiles, here a k-dimensional optimization for the unknown scatterer heights is solved by a multi-dimensional grid search. this is realized by not using a simple steering vector as a (h) in (1), but a so-called model signal vectorz (h), which describes the signal expected from a mixture of k scattering contributions at heights"
"(1) suppose that another cabinet member charlie is a good friend of the prime minister. to help the prime minister, charlie generates a ring signature on an announcement. it states that he is the leaker and the previous published story about the prime minister is not true but a political joke. of course, bob's ring signature and charlie's ring signature use the same \"ring\" -the whole cabinet. now, how can bob prevent this impersonation? (2) suppose that the journalist is very interested in these leaked secrets and wants to communicate with the real signer in order to discuss more details. so the journalist publishes his telephone number and wants the real signerto contact him through an anonymous phone call. how can bob convince the journalist that the anonymous call is from the real signer through a untransferable proof? (3) suppose that bob needs to publish further proofs for the escapades of the prime minister. how can bob make people believe that both the previous secrets and these further proofs are leaked by the same anonymous cabinet member? (4) after the disgraced prime minister is disposed, bob maybe wants to remove the anonymity of the ring signature. in other words, how can bob convert the ring signature into a standard digital signature? roughly speaking, (2) motivate the topic of secure anonymous identification; (3) can be captured by the notion of the linkability of anonymous signatures; (4) can be formalized as the notion of convertibility of a ring signature."
"a six-phase srm was recently designed and driven by a three-phase full bridge inverter. compared with a conventional three-phase srm, it produced lower torque ripple, and with the use of diodes, can still had unidirectional current in each phase [cit] . more recently a converter topology is proposed by the authors which only consists of six switches, and with which conventional control techniques for srm drives are applicable [cit] . however, this six-phase srm drive still suffers from obvious torque ripple. therefore, an effective torque ripple reduction method is desirable for this six-phase srm drive."
"since the matter of time is a key factor in adaptive systems, therefore linear temporal logic (ltl) has been applied in goal-oriented approaches [cit] . in addition, event modeling has a key part in adaptation modeling. for example, [cit] addresses an event modeling approach for adaptation modeling by proposing an xmlbase rule modeling for providing more flexibility to system design in dealing with possible changes. however, current goal-oriented approaches cannot completely cope with the challenges of re for self-adaptive systems [cit] ."
"the iteration process continues until there is no or only little shift in m (p i ) anymore, i.e. the length of the shift vector m (p i ) is close to 0. due to the gradient ascent nature, the mean shift algorithm returns clusters using the concept attraction of basin, i.e. those points whose trajectories lead to the same mode form the basin of attraction for that mode and are clustered into one group. the clustering procedure is repeated until all points are assigned to their respective modes."
"this work focuses on requirements engineering, as a basic discipline in developing each system, and aims at presenting the state of the art of this fundamental discipline for self-adaptive systems and in particular sbas."
"there is a traditional belief that fast frequencies are associated with smaller networks and slow frequencies with larger networks (cf. [cit] ) . here it will be shown that formula (2) can be used to predict the architecture of white matter connectivity which has meanwhile been well-described by dti and dsi tractographical methods (e.g., [cit] ) . we"
"in order to compare the torque-speed performance of the proposed low torque ripple drive systems, a 200 v, 48 a dc power supply is employed. the torque-speed curves are shown in fig. 24 . at low speed the torque is limited by the maximum current from the power supply, whilst at high speed it is limited by the available voltage. the proposed method can produce larger mean torque at low and medium speed because the currents are shaped to give increased torque per amp, whilst the conventional control method can produce larger average torque at high speed because it simply applies full voltage throughout the conduction period."
"environment context: it has information about the time in which users access the sba or the information about where the user is located. it also covers the surrounding environment such as the current date, temperature and weather. the modification of this context is performed by either users or external events."
"in this paper, a torque control method for torque ripple reduction has been proposed to control six-phase srm drive systems. although six-phase srms have lower torque ripple compared with other conventional srms, they still suffer from obvious torque ripple. therefore a torque control method for torque ripple reduction is proposed to further reduce the amount of torque ripple with both conventional and proposed converter."
"the main distinction between the six-phase ahb converter and the proposed converter is phase independence, therefore the voltage vectors and switching table of the proposed method for the ahb converter cannot be directly applied to the novel converter. for example, u1 in fig. 8 is (+1, +1, 0, −1, −1, 0), in which two adjacent phases are conducting and two phases are freewheeling. for the proposed converter, to achieve the same effect, the equivalent voltage vector v1 can be defined as (+1, +1, +1, −1, −1, −1), where '+1' represents the on-state and '−1' represents the off-state for each switch. u2 in fig. 8 is (+1, +1, +1, −1, −1, −1), in which three adjacent phases are conducting and other phases are non-conducting. for the proposed converter, every switch is shared by two phases. it is obvious that there are always two phases in freewheeling loops, therefore it is impossible to define an equivalent voltage vector with u2. consequently, there are six reasonable voltage vectors that can be employed with the proposed converter. table iii demonstrates the six voltage vectors for the proposed converter."
"darpa broad agency announcement (baa) [cit] presents the following definition for self-adaptive systems: \"self-adaptive software evaluates its own behavior and changes behavior when the evaluation indicates that it is not accomplishing what the software is intended to do, or when better functionality and performance is possible\". firstly, the definition expresses that software systems have several ways of realizing their functionalities. second, software systems should have adequate knowledge of their structure in order to make suitable changes at runtime. therefore, the software should be able to evaluate its behavior, and re-plan its operations. indeed, a typical self-adaptive system uses a closed loop like mape (monitoring, analysis, planning, execution) similar to the one used in autonomic computing [cit] represented these four activities in a similar approach with a closed control loop: collect, analyze, decide and act."
"the sensor used for the experimental considerations in this paper is the german memphis system created by the fraunhofer fhr [cit] . although it is able to be operated in different modes and configurations, in this work only the basic airborne side-looking configuration with a carrier frequency of 35 ghz (ka-band) and a bandwidth of 900 mhz is employed. this leads to a slant range resolution of 16.7 cm, whereas the azimuth resolution is 5.2 cm. since memphis is equipped with four receiving antennas, it is able to provide multi-baseline insar datasets from just a single pass over the scene of interest. [cit] . the test scene contains the \"alter nordfriedhof\", an abandoned cemetery, which is used as a public park today. as can be seen in fig. 2, it is mainly characterized by a light planting of trees, resembling a grove or little wood."
"after this, requirement engineering is to support which adaptation need to be done given a situation. at the operation and management phase, the context changes should be monitored and detected. understanding the degree and scope of change and uncertainty level are important and help to come up with the right decision for adaptation. this information provides triggers for the next phase to define adaptation requirements. it should be identified whether the adaptation is going to be automatic or semi-automatic. as in the case of semi-automatic, the user may involve in the process of adaptation decision. this can be done by providing appropriate feedback to users."
"an important and surprising implication of formula (2a) is that hr and brain oscillations on the one hand and hr and body oscillations on the other hand can be harmonically coupled. one obvious question that arises is, whether changes in hr may lead to a direct, concomitant change in the frequency of brain (and body) oscillations. brain oscillations may change slightly (e.g., α may exhibit a fatigue related decrease of about 1or 2 hz) but never to an extent as hr is capable. thus, in most cases a direct coupling with brain and body oscillations will not be possible. two aspects are important here. one refers to a state of decoupling between brain oscillations and hr if the change in hr is very pronounced, such as during heavy exercise. the other aspect refers to an adaptive change in hr that may indeed allow a direct but short lasting, transient, coupling with brain oscillations even in cases where hr is tonically increased or decreased. as an example, if hr is increased to 90 bpm (1.5 hz; period of 667 ms) a transient decrease to 75 bpm (i.e., an increase in the period of 800 ms for a few heart beats) or increase to 150 bpm (a decrease in the period to 400 ms for a few heart beats) would still allow for a transient task-related harmonic coupling with brain oscillations. such an adaptive mechanism could be responsible for the generation of hrv. it is interesting to note that hr may also operate to \"reset\" brain activity as the existence of heartbeat evoked potentials suggest [cit] ."
"for that purpose, an approach using event-based formal language (called flea) is developed to give users the ability to monitor functional requirements and assumptions on-the-fly [cit] . the approach is able to describe the conditions required for executing an adaptation."
"(2) despite the unconditional anonymity, the real signer has enough power to control his signature in the sense that he can anonymously prove his authorship, generate a linkable signature, and convert the controllable ring signature."
"the relationship between the stator flux and voltage vectors is shown in fig. 11 . if the angle between these two vectors is acute, the voltage vector u(k) has a component along the positive direction of ψ(k − 1) and the flux magnitude increases, whereas if the angle is obtuse, u(k) has component along the negative direction of ψ(k − 1) and the flux amplitude decreases. in addition, when the voltage vector u(k) is at an angle in advance of the stator flux vector, the stator flux is advanced. providing the advance is also positive with respect to rotor electrical position, the instantaneous torque t e increases according to (5), and vice versa."
"a \"key\" is not unique, in fact many pairs with a given key could be generated from the map function; the reduce function is applied, for each key, to its associated list of values. the result is a list of key-value pairs consisting of whatever is produced by the reduce function applied to the list of values."
"simulation under the ccc method with the proposed converter is subsequently conducted. in order to guarantee enough magnetization energy, the switching signals for the six switches in the proposed converter are the logical or operation results of every adjacent two single phase switching demand. for example, with reference to fig. 4, if either phase 2, phase 3 or both demand a positive voltage then t3 is switched on. similarly, if either phase 1 or phase 2 demand a positive voltage then t2 is switched on. due to the special electrical connection of the proposed converter, there is no longer complete independence of phase voltages: this leads to current distortion when controlled using the ccc method. for an average torque output of 20.0 n·m, the trr is 32.1% in fig. 5 . clearly torque ripple of the ahb and the proposed converter is high when using conventional control method and needs to be further reduced."
"in order to overcome such challenges we argue that two consideration need to be taken into account. first is a move from binary satisfaction of the requirement. degree of the requirement satisfaction need to be evaluated (e.g. using a fuzzy approach) and corresponding adaptation actions should be selected accordingly. second is defining critical and non-critical requirement. therefore we can distinguish between vital and trivial requirement. for example, it is possible to temporarily ignore some requirement with non-criticality in favor of other critical requirement (requirement trade-off approach)."
"in order to implement a real-time low torque ripple six-phase srm drive system, a six-phase srm drive system test rig was designed and constructed as shown in fig. 19 . this drive system is made up of a six-phase srm coupled to a permanent magnet load machine, igbt inverters, a f28335 digital signal processor (dsp) based controller, a shaft absolute encoder interfaced to the dsp controller, etc.. the winding connection for this machine is so as to ensure there is minimal coupling between phases, caused solely by low levels of cross-slot leakage flux. it was found that the mutual effects could therefore be ignored in this case. method, the proposed torque control method reduces the trr from 29.7% to 12.1%. fig. 21 shows experimental waveforms including phase current and torque output at 800 r/min and 1500 r/min. with a mean torque of 10 n·m and 15 n·m, the torque ripple with the ahb converter is significantly reduced under the proposed method."
"in addition to making geometric changes, the torque ripple can be further decreased by employing an advanced torque ripple reduction control technique. torque sharing functions (tsf) and direct instantaneous torque control (ditc) have been developed with some success to produce more uniform torque production for three-phase srms [cit] . with the tsf the overall torque demand is distributed to each single phase. by controlling the phase current profile according to the single phase torque demand torque ripple appearing between commutation was reduced [cit] . many different tsfs have been applied to reduce torque ripple for a three-phase srm [cit] : both offline and online tsfs have been investigated, and the torque ripple of the three-phase srm has been reduced to 40% [cit] . a ditc method has also been employed in three-phase srms to reduce the torque ripple to 40% and lower [cit] . unlike the tsf method, this latter method does not require a highprecision rotor position sensor. ditc comprises a digital torque hysteresis controller, which generates the switching signals for all activated machine phases. a hysteresis controller regulates the estimated torque of one phase. during phase commutation, the torque of two adjacent phases is controlled indirectly by controlling the total torque."
experimental tests have been carried out to validate the torque ripple reduction performance of the proposed control method. test results show that the proposed torque control method can reduce torque ripple significantly throughout the whole speed range with both conventional and proposed novel converter.
"user context: the user context includes the user's requirement and preferences. requirement priorities from user perspective are expressed in this category. for example, regarding qos requirement it shows which properties will be maximized among others. it also contains the information about the role of the user in the application e.g. guest or administrator."
"requirement reflection: requirements of adaptive systems need to be represented at run-time (run-time entities) to support adaptation. this involves modeling requirements at design time and reasoning them at runtime according to changing context to support adaptation. therefore, the selection of the best adaptation strategy will be postpone at run-time by reasoning the existing requirements model and run-time data acquired from the context changes. this way, it is possible to revise and re-evaluate design-time decisions at run-time."
"we classify context elements into six distinct categories: resource, user, provider, environment, web service quality and web service functionality. this provides us a comprehensive view of information that influences service-base applications. the context information model drives situation that triggers adaptation. context classification is illustrated in figure 2 . these elements are subject to change during the life-cycle of sbas. we explain each of them in the following briefly."
it is worth noting that the set of predecessor states' identifiers should be immediately available inside state definition because our mapreduce based approach exploits the evaluation of r − as a basic operation without communication among computational units.
"it is noticeable that, whilst the torque ripple predicted by simulation is typically 5% of the mean output torque, the measured torque ripple is generally higher. simulations revealed that this is caused by cross-saturation effects in the core backs, which are not accounted for in the single phase characteristics used in the controller. note also that, due to voltage limitations, the demanded rate of change of flux angle will sometimes exceed that which the converter can supply: consequently the torque ripple reduction at high speed is inferior to that at low speed."
"classical requirement engineering is based on the assumption that the environmental context is static and can be understood sufficiently. in the presence of these two factors of environment namely being static and wellunderstood, traditional requirement engineering could be performed well. this might be still possible due to the speed of change tendency. this means, where such context changes slowly enough, existing techniques are capable of capturing, managing and adapting these contextual changes. however, if we consider novel contexts such as pervasive systems and systems supporting dynamic b2b interaction, requirements change so fast that the research community is studying how to build systems that are able to self-adapt on the fly to some of these changes. the dynamicity and uncertainty of the environmental context are the main two obstacles in understanding requirement for adaptive systems. these make it difficult to understand, discover, formulate, validate, reason and manage the requirement both at design and specially at runtime."
"vi. e-prosecution scheme based on controllable signatures in this section, based on the above controllable signature scheme, we design the e-prosecution scheme as follows. this e-prosecution scheme involves two parties: the public authority such as the police office, and the group (ring) of all possible prosecutors. by this scheme, the prosecutor can prosecute sequential messages first and i-th offline prosection, and even anonymously initiates an online discussion with the authority (online anonymous prosection), and collect the reward by opening this identity to authority (award collection). as will be shown in the security analysis, this e-prosecution can well protect the identity privacy of the prosecutor."
"in this paper, we revisited the classic application of a ring signature in leaking secrets and point out a list of problems unsolved by a standard ring signature. motivated these problems, we formalized a new cryptographic concept called a controllable ring signature and propose a concrete scheme. this extension of a standard ring signature can fully ensure the interests of the real signer: (1) the real signer remains unconditional anonymous as long as he does not remove anonymity; (2) only the real signer can control the ring signature: only he can anonymously prove the authorship, generate a linkable ring signature or convert it. on the other hand, a ring member is rightly restricted since he can not generate a controllable ring signature and convince one that it is generated by others. at last, using this controllable ring signature scheme, we design a secure e-prosecution scheme."
"to control the instantaneous torque, the nonlinear torque characteristics have to be derived. fig. 6 shows the determination of electromagnetic torque considering saturation. the average output torque is determined by"
"as shown in fig. 8, u2, u4, u6, u8, u10, and u12 have the same voltage directions as phases 1 to 6, and there are three adjacent phases conducting simultaneously. for more flexible control, the six-phase srm employs six extra voltage vectors. to achieve a balanced voltage vector arrangement, the extra six voltage vectors u1, u3, u5, u7, u9, and u11 are designed."
"considering demands of the stator flux and torque simultaneously, table iv shows the switching rule for the proposed converter. if torque and flux are to rise the voltage vector can be selected in one zone ahead. to increase stator flux whilst instantaneous torque is to fall, the voltage vector can be chosen in one zone behind. to increase torque and decrease flux, the voltage should be chosen in two zones ahead. if torque and flux are to fall the voltage vector should be selected in two zones behind. for an average torque output of 20.0 nm, compared with the conventional current control method, the proposed torque control method reduces the trr from 32.1% to 6.8% as shown in fig. 18(a) . by plotting stator flux vectors of two torque control methods in the x-y stationary frame in fig. 18(b) it is further demonstrated that, as with the ahb converter, the stator flux linkage with the ccc method is close to a hexagon, whilst the proposed torque control method has a circular trajectory."
"therefore, the change of mechanical energy is the area surrounded by the dashed line in fig. 6 which equals to t e dθ, so that instantaneous output torque can be expressed as (4)"
"while tomosar basically aims at the reconstruction of continuous reflectivity profiles [cit], it has also been adapted to and frequently used for discrete scenarios as they occur, e.g. in urban remote sensing [cit] . depending on the goal of tomographic analysis and on the utilized wavelength, in principle both models can be of interest for forested areas. a schematic sketch of the tomosar configuration for forested areas, comparing both the continuous and the sparse reflectifity hypotheses is presented in fig. 1 . for display purposes, only logarithmic amplitudes are shown in the remainder of this paper."
"as we presented above, some significant approaches have been addressed to support re activities for adaptive systems. however, the research in this area has still much to do and is in its beginning steps. besides, these approaches are dealing with a specific activity at one time and therefore they are isolated approaches and there is a need to provide a comprehensive framework that incorporates complete re activities in one approach. in the following we argue about some initial work trying to provide a comprehensive approach including re activities and adaptation decisions with respect to the context changing."
resource context: it includes hardware and software properties that influence sbas. availability of the resources has an impact on satisfying the requirement. the information of resources and their availability could be updated during changes. the resource context also contains characteristics of network and operating systems for accessing the sba.
"although both of the above two methods give good reductions in torque ripple in three-phase drives, there are obstacles to their application in six-phase srms. for six-phase srms, there are always three/four phases conducting simultaneously, so it is much more complex distributing the torque contributions to three or four phases directly. despite the simple structure of ditc, it requires complex switching rules for smooth torque generation during commutation, which become much more complex again in six-phase srms."
"goal-oriented approaches (i.e. kaos [cit], i* [cit] and tropos [cit] ) allow analysts to obtain and define goals as well as requirements. they are also able to identify constraints that environment enforces to requirements. hierarchical goal models and refinement approaches can be used for adaptation techniques and developing of adaptive systems in which they allow analysts to describe various contextual requirements in order to achieve a goal."
"note that for an ordinary ring signature, although every ring member can anonymously generate a signature, he has to \"close the ring\" at his own position using his own secret key. if the real signer hides some proof for the \"closing position\" in the ring signature (in our construction, we perfectly hide the proof through pedersen's commitment scheme.), he will be able to control it as follows. on the one hand, before the hidden proof is public, this controllable ring signature is just like a standard ring signature. and the real signer can anonymously prove his authorship, or generate linkable ring signatures by using the hidden proof as the secret key. after the hidden proof is public, this controllable ring signature is converted into a standard signature generated by the real signer."
"for example, consider a situation that there are different possible future scenarios but it is possible to list them all. considering adaptation scenarios, this requires all possible scenarios to be taken into account at the design time. this could be done by considering all alternative contextual conditions and design all adaptation scenarios based on them. therefore, run-time decision will be mainly based on requirement engineering at design time. in general, as long as the changes in the context are known, run-time decisions could be handle using existing requirement engineering techniques such as defining adaptation trigger conditions. now, consider a situation that it is feasible to construct future scenarios but these are mere possibilities and are unlikely to be exclusive. this is a situation that context changes cannot be anticipated. re at design time would not support run-time decisions. defining adaptation trigger conditions at design-time is not adequate any longer as the new triggering conditions cannot be predicted."
"this design process should include a requirement engineering phase that somewhat differs from the traditional one. also, the identification of these requirements for adaptation requires a good knowledge of the context in which the system will be executed. at one side, we argue that what is missing now is a structured and robust design process for re. however, the design time decisions need to be done in the situation of incomplete and uncertain knowledge about environmental context. this way, we need to understand to what extent the requirements are being satisfied and this can support adaptation strategies at run-time. therefore, at the other side, we argue that requirements for adaptive systems should be supported not only at design-time but also at run-time."
"the concept of direct torque control (dtc) for ac machines [cit] has been developed in srm drives [cit] to simplify the control algorithm and improve the torque response. the nonlinear characteristics and non-sinusoidal excitation of srms has hitherto been an obstacle to its application in srm drives. a dtc methodology was produced for three-phase srms from analysis of the nonlinear torque characteristics [cit] . this method does not require stator winding modification and can work with unipolar drives. it uses a flux hysteresis controller to keep a constant magnitude of the stator flux, whose vector is accelerated or decelerated to control the instantaneous torque. simulation showed that under this very simple control algorithm torque ripple in a three-phase srm is reduced."
"model checking of dynamic, concurrent and real-time systems has been the focus of several decades of software engineering research. one of the most challenging task in this context is the development of techniques and tools able to cope with the complexity of models coming from real word examples. in fact, the main issue that affects model checking is the state explosion problem [cit] : the most significant breakthroughs to cope with this problem are symbolic model checking with ordered binary decision diagrams (especially in the area of hardware verification), partial order reduction techniques, and bounded model checking [cit] . these breakthrough techniques have enabled the analysis of systems with a fairly big number states. nevertheless, there are valid reasons to use explicit approaches against symbolic ones, in fact, it is often accepted that explicit state model checking is better for verifying software systems [cit], moreover symbolic approaches sometimes disable the computation of specific state properties (e.g. state probabilities for stochastic petri nets [cit] ) in the context of explicit state model checking, taking advantage of a distributed environment is important to cope with real world problems. the idea is to increase the computational power and a larger available memory, by using a cluster of computers. the use of networks of computers can provide the resources required to achieve verification of models representing very complex systems. in fact, our recent works were focused on the connection between formal methods in software engineering and big data approaches [cit] . the analysis of very complex systems certainly falls into this context, since formal verification often requires high performance data processing software able to make sense of unprecedented amount of data. although formal verification has so far poorly explored by big data scientists, we believe the challenges to be tackled in formal verification can benefit a lot from results and tools available for big data access and management. in fact formal verification requires several different skills: on the one hand, one needs an adequate background on formal methods in order to understand specific formalisms and proper abstraction techniques for modeling and interpreting the analysis results; on the other hand, one should also strive to deploy this techniques into software tools able to analyze large amount of data very reliably and efficiently similarly to \"big data\" projects. recent approaches have shown the convenience of employing distributed memory and computation to manage large amount of reachable states, but unfortunately exploiting these results requires further skills in developing complex applications with knotty communication and synchronization issues. in particular, adapting an application for exploiting the scalability provided by cloud computing facilities as the amazon cloud computing platform [cit] might be a daunting task without the proper knowledge of the subtleties of data-intensive and distributed analyses."
"since our distributed algorithms are quite different from message passing approaches, the number of cross-border transitions is not a crucial issue to cope with [cit] . anyway a better comprehension of how partitioning can impact performances of our mapreduce based approach calls for further research in a theoretical setting."
"some extensions of a standard ring signature can only partially solve the above mentioned problems. in fact, the above problems were not so comprehensively pointed out in existing literature. now we briefly review these related work."
"the reason most hitherto research on sar-based remote sensing of trees and forests used to center either on large-scale forest classification, biomass and forest volume estimation using l-band sar tomography, or canopy height model reconstruction using x-band sar interferometry is the rather coarse resolutions most conventional sar sensors were able to provide to this date. in order to focus on individual trees, resolutions well in the sub-meter domain are necessary. another reason certainly is the fact that most operational sar systems are working in x-, c-or l-band, i.e. the long wavelengths let the signals penetrate the canopy and thus lead to a more or less significant under-estimation of the tree heights. last, but not least, usually just one receiving antenna is mounted on carrier platforms, which means that only interferometric repeat-pass data can be collected. since vegetation movements caused by, e.g., wind quickly lead to a decorrelation of the interferometric signals, only single-pass phase measurements allow for a reliable height reconstruction. however, airborne single-pass millimeterwave sar interferometry provides new access to the topic: providing highcoherence multi-baseline interferometric sar data, it enables the derivation of tree heights providing a convenient indicator for tree detection. also, due to the short wavelength, it is expected to penetrate the tree canopy less than more common radar bands with longer wavelengths."
"mapreduce relies on the observation that many computations have the same basic design: a same operation is applied over a large number of records (e.g., database records, or vertices of a graph) to generate partial results, which are then aggregated to compute the final output. the mapreduce model consists of two functions: the map and the reduce functions that are both defined with respect to data structured in key-value pairs. the map function turns each input element into zero or more key-value pairs."
"design parameters of the machine, for instance the air gap, dimension of the core back, winding arrangement and pole shape have a significant impact upon torque ripple [cit] . many researchers focused on modifying the geometry to minimize torque ripple [cit] . increased phase numbers can be used to advantage [cit] . by introducing more pulses per cycle they reduce torque ripple, lower the dc link current harmonics and have higher fault-tolerance. however, the added complexity of the high number of power devices and connections to the machine have limited the application."
"with a mean torque of 14.9 n·m at 800 r/min, the proposed torque control method restrains the trr to 17.8%, nearly half of the trr under conventional method. with a mean torque of 10.0 n·m at 1500 r/min, the trr under the angle position control (apc) method is 47.7%, meanwhile the proposed torque control method restrains the trr to 28.6%. fig. 22 shows experimental waveforms including phase current and torque output with the proposed converter at 200 r/min, with a mean torque of 20 n·m. compared with the conventional method, the proposed torque control method reduces the trr from 42.1% to 14.3%. fig. 23 shows experimental waveforms including phase current and torque output at 800 r/min and 1500 r/min. with a mean torque of 10 n·m and 15 n·m, the torque ripple with the proposed converter are significantly reduced under the proposed method. with a mean torque of 15.0 n·m at 800 r/min, the proposed torque control method restrains the trr to 18.5%, lower than half of the trr under conventional method. with a mean torque of 10.0 n·m at 1500 r/min, the trr under the consequently, compared with conventional control methods, the proposed torque control method can reduce the torque ripple for six-phase srms with both the ahb converter and the proposed converter throughout the whole speed range."
"a common limitation of above approaches is a lack of proper context model that provides information for adaptation decisions. the requirement that identified at design-time, may not be satisfiable when the context changes. this can affect the performance of sbas. however, context is a very broad term and understanding it requires a special care. different elements of the context need to be accurately classified. moreover, dependencies between context elements need to be identified in order to prevent propagation of changes from one context element to the other one. in the following we first of all present our definition of the term context and then classify context elements into different categories."
"depending on the on-off states of switches, there are three voltage states for each phase in the ahb converter as shown in fig. 7 . afterward, voltage vectors are employed to represent a combination of all the six phase switching states at different instant. however, switching states in srms are time functions and they have no relevant relationship with the stator flux space vector."
"to implement this control concept, the torque demand and the stator flux demand have to be considered simultaneously. generally, if torque and flux are to rise the voltage vector can be selected in one zone ahead. to increase stator flux whilst instantaneous torque is to fall, the voltage vector can be chosen in two zones behind. to increase torque and decrease flux, the voltage should be chosen in four zones ahead. if torque and flux are to fall the voltage vector should be selected in five zones behind. consequently the switching rule for the six-phase srm with the ahb converter is summarized in table ii, where k is zone number where the stator flux locates. fig. 12 shows the control diagram of the proposed torque control method for six-phase srms, in which control parameters are instantaneous torque and stator flux linkage. six phase flux amplitudes are obtained from integration of the phase voltage according to (11), and therefore the amplitude and angular position of the stator flux vector is calculated according to (7)- (10) in the flux estimation block. estimated torque is calculated using measured nonlinear magnetization characteristics of the prototype when phase current and rotor position are available from a current sensor and position encoder. two hysteresis controllers are employed in this control system for torque and stator flux linkage amplitude regulation. the regulation commands of these two hysteresis controllers are the basis of selecting a proper voltage vector from the switching table during each control cycle. ahead of the rotor movement the torque increases, and when the flux lags behind the rotor movement the torque decreases."
"according to the above analysis, amplitudes of stator flux and instantaneous torque are regulated by switching proper voltage vectors. for example, if the stator flux is in zone n1 and is smaller than the command value, voltage vectors u2, u3, u11 and u12, which have an acute angle with the stator flux, are selected. if stator flux is larger than command value, voltage vectors u5, u6, u7, u8 and u9, which have obtuse angle with the stator flux, are selected. the adjustment of the instantaneous torque can also be determined according to whether the voltage vector leads or lags the stator flux vector."
"for the motivation of our new concept, we revisit the classic application of ring signatures in leaking secrets. suppose that bob (also known as \"deep throat\") is a member of the cabinet of lower kryptonia, and that bob wishes to leak a juicy fact to a journalist about the escapades of the prime minister, in such a way that bob remains anonymous, yet such that the journalist is convinced that the leak was indeed from a cabinet member. at a glance, it seems that a standard ring signature can help bob to perfectly complete this task: he signs the message using a ring signature scheme on behalf of the whole cabinet. however, the following cases will show that a standard ring signature is not enough for leaking secrets in the real world."
"(4) despite the unconditional anonymity, any other party (non-signer) cannot abuse the anonymity. for example, there is no way for him to present the proof that the ring signature is (or not) due to him."
"web service quality context: it covers information about non-functional properties of web services in sbas. typical non-functional properties include availability, throughput, response time, level of security and they are often collectively referred to as quality dimensions. changes of other contexts have mostly direct impact on the quality context. however, any changes in the quality context will trigger adaptation."
"our contributions are twofold, as listed below. on the one hand, we revisit the classic application of ring signatures in leaking secrets and point out a list of practical problems unsolved by a standard ring signature. motivated by these problems, we formalize the new notion of controllable ring signature. it is a useful cryptographic primitive which can fully ensure the interests of the real signer and rightly restrict him as follows."
"third, when f 1 queries the corrupting oracle o k on the public key in l, f 2 returns the secret key if this public key is generated by f 2 . otherwise, f 2 aborts."
"for an average torque output of 20.0 n·m, compared with the conventional current control method, the proposed torque control method reduces the trr from 40.1% to 5.1% as shown in fig. 15(a) . by plotting stator flux vectors of the ccc method and the proposed method in fig. 15(b), it is further demonstrated that the stator flux linkage in the ccc method is close to a hexagon, whilst the proposed method has a circular trajectory."
a monotonic predicate transformer on p(s) always has a least fixed-point and a greatest fixed-point (by tarski's fixedpoint theorem [cit] ). the temporal operators eg and eu can each be characterized respectively as the greatest and the least fixed-point of two different monotonic predicate transformers:
"the torque ripple can be 70% and higher in three-phase srms [cit], and 50% or more in four-phase srms [cit] . many investigations have sought to reduce the torque ripple: these can be classified into two fields, one based upon machine design optimization and one upon control algorithms."
"the propagation of dynamic adaptive systems in various fields has provided opportunity in conducting research in different development phases from preliminary analysis to implementation. such systems have been applied in autonomic computing, pervasive systems, ubiquitous computing and service-oriented computing. although there are some definitions for self-adaptive systems, the existing concepts in various domains such as pervasive systems and autonomic computing need to be clarified. in order to explain the boundary of self-adaptive systems, we present some well-known definitions and characteristics self-adaptive systems should support."
"there are issues that need to be taken into account as following. first of all, the monitoring data should be used to evaluate the context properties identified in the context model. therefore the context changes need to be detected and the degree of changes need to be evaluated. understanding uncertainty level as we explained earlier is also necessary. the aggregation of this consideration will result to identify the adaptation triggers. the triggers are the base to define adaptation needs. the existing rules and links between the context and adaptation strategies need to be updated according to the information obtained at monitoring. apart from these, the user preferences could mainly affect choosing the right adaptation. furthermore, adaptation purpose need to be identified in the early stage whether is it for optimization, recovery or prevention as each may have different requirements."
"(3) despite the full power to control his signature, the real signer is rightly restricted since he is not able to generate a controllable ring signature and then convince a third party that it is generated by others."
"an automatic analysis of forested areas by remote sensing means is an important research topic in the context of sustainable forest management. this holds particularly for sensors and methods which are capable of providing information on the level of individual trees: in many countries, single-tree related parameters are the basis for forest inventory. until these days, the majority of these parameters are collected manually by measurement of sample plots in cost-and time-intensive field surveys. however, remote sensing-based methods have gained increasing attention during recent decades."
it is well understood that increasing the number of phases reduces torque ripple. thus a six-phase 12/10 srm is proposed in fig. 1 . table i gives the design parameters for this machine.
"to locate the stator flux the vector space is segmented into twelve zones. as shown in fig. 10, each zone occupies 30°and the voltage vectors are respectively positioned in the central axis of the corresponding zones. on the basis of the proposed control method, the amplitude and rotational speed of the flux vector is regulated by means of choosing a proper voltage vector. owing to this, the stator flux expression is shown in differential form:"
"comparing the flux trajectory under the proposed torque control method with the ahb converter, the flux trajectory under direct torque control applied to the novel converter has a longer path before it reach the reference value, suggesting that the dynamic torque response may be slower. this is because the voltage vectors employed with the proposed converter only allow two phases conducting simultaneously, whilst the ahb converter allows three, which helps the ahb converter build stator flux linkage faster."
i wish to thank olaf sporns who has kindly provided us with data that allowed us to calculate the distribution of white matter bundles as shown in the right panel of figure 1b . i also wish to thank julia lechinger for performing this analysis. this research was supported by the austrian science foundation (fwf project p21503-b18).
"the remainder of the paper is organized as follows. we start by an introduction to self-adaptive systems and their major features in section 2. in section 3, we explain an extended life-cycle for adaptive sbas in general and the phases and corresponding activities that need to be supported for each phase. section 4 discusses about sbas as a dynamic system and the fact that adaptation need to be supported in such systems. a context information model to support adaptation is described as well. an overview on the main challenges in re for adaptive service-base systems is reported in section 5. we conclude the paper in section 6."
"our algorithm can be considered a coordinate system for the coupling of brain and body oscillations. brain body interactions may, thus, be described as complex system that couples and decouples [cit] on the basis of a specific harmonic frequency structure. as a scale free law it probably underlies all animal species. the fact that hr exhibits a tremendous between species variation (about 600 bpm for rats and 20 bpm for elephants) means-according to formula (2a)-that the physiological function of the domains change although the absolute frequency values may change little or remain even identical [if the animal fd (0) obeys the doubling-halving algorithm relative to human hr] as is illustrated in the right panel of figure 1 . in a mathematic sense formula (2) represents a binary system. since the emergence of information theory it became clear that any kind of information can be encoded on the basis of binary units (e.g., [cit] ) . one may speculate that this algorithm represents a basic physical law of information encoding that requires the least amount of energy. this view is based on the fact that the scaling factor s has far reaching consequences from hr to body and brain size to metabolic processes. another aspect is its fractal property because the doublinghalving relationship repeats over all different scales. for future research, the establishment of a large normative data base for brain and body oscillations would be helpful to clarify the questions raised here."
"web service functional context: it contains information about functional properties of web services in sbas regarding interface, structure of messages and different protocols. the cause of a functional change could be performed by changing requirement from user, provider and even environment contexts. consequently there is a need to add, remove or update functionality. for instance, changes in the physical environment (e.g., temperature) can influence the network characteristic and the quality context (e.g. response time) also change. this will triggers an adaptation situation and the functional context is required to be updated."
"torque in a srm is generated by excitation current pulses coordinated with the rotor position. the adjustment and timing of excitation currents are regulated by the drive circuit and the torque control techniques. the conventional control methods can vary the mean torque, but because of its highly nonlinear electromagnetic characteristics, even the high phase number srms still suffer from torque ripple. as shown in figs. 3 and 5, deviations from the mean torque are particularly large near the phase commutation points. to solve this problem an instantaneous torque control method has been investigated."
"the set of states identified by the temporal operator ex, can be defined trivially if we consider the counterimage with respect to the relation r. thus we can verify easily that the following holds:"
"here the current is assumed to be constant during a small displacement dθ, because a first order delay exists in phase current variation with respect to the phase voltage and the stator flux linkage variation [cit] . ignoring the resistive loss, the energy exchanged with the supply dw e is given in (2), and the change of mechanical energy dw m is what is left after the change of magnetic stored energy dw f as (3) ."
"between these two main phases the system sorts the keyvalue pairs by key, and groups together values with the same key. a mapreduce job is a two-step processing where several map tasks are executed in parallel on a large cluster of machines, and after the shuffle phase several reduce tasks are executed in parallel as well, to compute the final output."
"requirement engineering for adaptive systems and in particular for service-based applications can be categorized into three parts: requirements elicitation, requirements modeling and specification, and finally requirements monitoring. in the following we present an overview on related work discussing main contributions in each part."
provider context: it covers information from the provider side on the usage of the sba. provider may change the offered requirement during the execution. for example the provider may increase or decrease the computational charge and this will have a direct impact on the perceived requirement of sba from the used side.
"where the change of instantaneous torque t e is completely dependent on the value of (6) . to simplify the analysis with (5), assuming stator flux ψ can be controlled as constant, then the change of t e only depends on the velocity change of ψ with respect to rotor position. therefore an increasing value of (6) is defined as flux acceleration, whereas a decreasing value of (6) is defined as flux deceleration. hence, this control method for srms contains two essential factors: 1) choose a proper method to keep the magnitude of the stator flux linkage constant; 2) accelerate or decelerate the rotational stator flux to control the overall torque."
"the chain of harmonic frequency domains as described by formula (2) may be considered a coordinate system for global synchronization, which most likely is typical for conscious cognition. this is well in line with the idea that consciousness is associated with coherent global and long-range brain processes [cit] . three groups of empirical data are also in support of this view, the task dependent emergence of between frequency phase coupling (e.g., [cit] ), the observation that erp's can be described by a superposition of transiently phase coupled frequencies [cit] and-most importantly-that a change in the state of consciousness from active cognition to drowsiness and slow wave sleep (sws) is accompanied by a dramatic change in the frequency architecture. it is characterized by a decoupling between those frequency domains that are described by formula (2a) and the emergence of frequencies (slow waves and spindles) that do not play a role during conscious cognition. sws may be characterized by a loss of phase coupling and the emergence of phase to amplitude envelope coupling between slow waves and spindles (e.g., [cit] ) ."
"is calculated, where a (h) is the so-called steering vector corresponding to a scattering contribution expected at height h, and e n is the matrix containing the eigenvectors corresponding to the noise subspace."
"it can be shown that any ctl formula [cit] can be written in terms of ¬, ∨, ex, eg, and eu the possible combinations are eight: ax, ex, af, ef, ag, eg, au, eu."
"asymmetric half bridge (ahb) converters are the most popular choice for srm drives because they give independent con- trol of each phase. fig. 2 shows a six-phase ahb power inverter. note that this requires twelve controlled switches and twelve diodes -double that of a standard six pulse, three phase ac drive -adding complexity and cost. the conventional current chopping control (ccc) is first simulated with the ahb converter. the simulation parameters were set as follows: dc link voltage v dc of 200 v; rotation speed n of 200 r/min; current reference i * of 15.0 a; turn-on θ on and turn-off angle θ off are 0°and 160°respectively, where 0°corre-sponds to the unaligned position. simulated six phase current, flux linkage and torque and the overall instantaneous torque waveforms are shown in fig. 3 . producing 20.1 n·m average torque, the trr (torque ripple ratio) under the conventional method is 40.1%."
"requirements elicitation: includes activities to identify stakeholders, goals, and requirements in general [cit] . regarding self-adaptive systems, requirements are dependent on the contexts the software system under consideration belongs to. therefore, self-adaptive systems have to adapt their behavior according to context changes. for such purpose, applying context engineering during requirement elicitation can be beneficial."
"in this paper a torque control method is proposed for sixphase srm drives with two different converters. the paper is organized as follows. the six-phase srm prototype, its converters and the torque ripple existing in srm drive systems are illustrated in section ii. in section iii, the principle of the torque ripple reduction method is introduced and applied to the conventional converter and the proposed converter. firstly the stator flux definition and the selection rule of voltage vectors are demonstrated with the six-phase conventional converter. following this, considering the phase independence, the control method for the proposed converter is introduced. afterwards the performance of the control method is verified with detailed simulation. to further validate the proposed torque control method, a six-phase srm test rig is constructed for experimental verification in section iv. experimental results show a significant reduction in torque ripple throughout the whole speed range when the proposed method is applied. section v concludes this paper."
"in order to build a relationship between switching states and the stator flux space vector for further investigation, the voltage vector direction of each phase is selected along the direction of the phase flux vector in the time-space coordinate system. however, for each phase there are three voltage states, thus for six phase srms there are huge quantity of voltage vectors, which increases the complexity of this method. therefore, the first task is to find some reasonable voltage vectors. based on the original electrical design of the six-phase srms, there are three or two adjacent phases working simultaneously. thus reasonable voltage vectors can be selected as follows."
"context-aware sbas are required to be aware of the context, sense the environment, detect context changes and act accordingly. the main issue here is the relationship and linking between the environmental behavior (context) and the system behavior (requirement). the state of the environment and consequently its context will have a major effect on such relation. if the context is well-understood and stable then the appropriate adaptation actions could be define perfectly and clearly at the design time. however, where the context is not wellunderstood and not-stable then such relation is not clear and it is hard to make decision. this is mainly due to the uncertainty aspect of the context. therefore it is necessary to monitor, detect and analyze the context at runtime when the system is deviating from the early requirements. in this situation, adaptation decisions made at design time are not adequate and therefore new decisions need to be made at run-time according to the information of context changes. this is even more difficult when considering non-functional requirements i.e. qos issues in web services."
"the proposed torque control method for six-phase srms is firstly researched with the six-phase ahb converter. simulation results verify that the proposed control method with the ahb converter can observably reduce the trr from 40.1% to 5.1%. afterwards the proposed method with the proposed converter is investigated. there is reduced phase independence in the proposed converter, consequently a modified torque control method with six voltage vectors is proposed based on the control method designed for the six-phase ahb converter. although fewer voltage vectors are used with the proposed converter, the torque ripple reduction performance has not been affected. simulation results verified that the modified control method with the novel converter can reduce the trr from 32.1% to 6.8%."
"our contribution is a set of parallel algorithms designed for distributed memory architectures and cloud computing platform based on a new emerging distributed paradigm. it is worth noting that departing from the current literature on distributed ctl model checking, we considered an important aspect, sometimes understated: we wanted to completely remove the costs of deploying our application into an end-to-end solution, for this reason we developed our software on top of the consolidated hadoop mapreduce framework. as far as we now, the effectiveness of a mapreduce based approach, typically employed to solve big data problems, has been not explored so far by the formal verification community."
requirement modeling: goal-oriented requirement engineering approaches have been mainly considered as a key solution for requirements modeling and specification in adaptive systems [cit] . stakeholders' goals and system objectives are relatively stable [cit] whereas requirements define one of the possible ways that a goal can be realized which means goals are operationalized through requirements [cit] .
note that we refer the reader to the 3 facts in the next section for the basic idea underlying the above construction and the next protocols or algorithms.
"according to the stator flux relationship shown in fig. 9, the amplitude of the stator flux is a combination value of the six phase fluxes according to (7), (8) and (9) . the position of the stator flux is calculated by (10) ."
"we presented techniques and tools aimed at managing and verifying properties on very complex systems by exploiting approaches typically used by the big data community. therefore we remark a clear connection between formal verification problems and big data problems conveyed by the recent widespread accessibility of powerful computing resources. despite model checking software tools are so called \"push-button\", the setup phase required by a distributed application, is far from being considered such, especially whenever one wants to exploits general purpose \"cloud\" comput-ing facilities. our framework aims at re-enabling a \"pushbutton\" mode into the distributed verification context our experiments report that our approach can be used effectively to manage and analyze state spaces of different orders of magnitude. in particular, the major is the complexity of the model to be analyzed, the major is the scalability of our distributed algorithms that shown a potential for a super-linear speedup. we believe that this work could foster a synergy between two very different, but related communities: the \"formal methods\" community and the \"big data\" community. exposing this issue to scientists with different backgrounds could stimulate the development of new interesting and more effective solutions."
"the characteristic of context aware systems bring the need to elicit, model and monitor requirement for such systems. thus we discuss requirement engineering activities and corresponding techniques to support aforementioned issues. besides, we provide a context information model to support the adaptation of sbas."
"adaptation strategies and decision making: a range of available adaptations (strategies) could be identified at the design time. requirements obtained from various context information models (see figure 2) can identify triggering conditions for adaptation. then the changes of the context can be linked to the adaptation strategies by identifying rules. therefore, finding most suitable adaptation strategies (between the alternatives) will be done at run-time. however, the two aforementioned challenges namely dynamicity and uncertainty of the requirements make the adaptation decisions to be unpredictable. in this situation, decisions have to be evaluated. besides, each adaptation strategy has a different trade-off and consequences that has to be analyzed with the information at run-time. multi-objective decision making may be applicable when the uncertainty exists. it usually defines a utility function that calculates the weighted sum of different objectives. however, regarding adaptation strategies this can be a difficult task identifying the weight of each strategy."
"dynamicity of requirements: due to the dynamicity of the requirements, it is necessary to specify the evolu-tion of the requirement model [cit] . there are some challenges and research issues that need to be taken into account when the requirement model changes at run-time."
"requirement monitoring: in order to ensure that the requirements are properly fulfilled, self-adaptive systems need to be able to monitor the environment. [cit] argues that requirements as well as designs issues are typically formulated in a set of assumptions about the context. therefore, requirements specification and system design are based on a set of assumptions which their stability cannot be guaranteed."
we explain some key challenges in supporting requirement engineering techniques for dynamic systems in presence of volatile and uncertain environmental context. we argue that requirement engineering activities should be supported at run-time to handle requirements for dynamic systems.
"in the above scheme, given a controllable ring signature, there is no way for the receiver to check whether this ring signature can be correctly converted. in other words, for a controllable ring signature, the verifier can only check whether it is generated by a ring member but can not check whether it is controllable. however, in some applications, it may be necessary for the verifier to be convinced of the convertibility. in fact, the above scheme can be easily extended to support a non-interactive proof for the convertibility of the controllable ring signature. we will show that the proof for controllability can be implemented using 1-out-of-n witness indistinguishable proofs with a concrete discrete logarithm setting [cit] ."
"the dtc method is well-adapted to multi-phase srms because it considers the machine as a whole: by simply increasing the number of voltage vectors, it can be easily developed for higher phase number srms. the application of this dtc method to such machines has been simulated [cit], but all previous publications on this topic are purely simulation based or at very low speed."
goal-oriented approaches [cit] seem to be a promising method for supporting requirement reflection. for example kaos [cit] provides a modeling language with formal semantics that allows automated reasoning over requirements and goals.
"since three-dimensional rotational ellipsoids can be seen as a good approximation of deciduous tree crowns, the individual tree clusters are modeled using generalized tri-axial ellipsoids. for this purpose, parameters of an arbitrarily oriented minimum volume enclosing ellipse (mvee) are estimated by first projecting points belonging to individual tree clusters onto the xy-plane followed by extruding the 2d xyellipse in z-direction to form a 3d ellipsoid. the motivation for expanding the ellipsoid along the z-axis is based on geometrical considerations: it is assumed that correct tree models may have an arbitrary orientation in the xy-plane, but remain upright or vertical with respect to the ground. this is based on the light prior that tree trunks are modeled to be vertical to the ground surface."
"as the quantity of voltage vectors are reduced from twelve to six, the zone arrangement is changed accordingly. in order to locate the stator flux, the vector space is segmented into six zones as shown in fig. 16, every zone occupies 60°. the stator flux definition is similar to the ahb converter. the amplitude and position of the stator flux vector are calculated by (7) to (10) . stator flux vector and voltage vectors also fulfil the relationships in fig. 11 ."
"concretely speaking, to convince the receiver of the controllability, the real signer should present an noninteractive proof of knowledge of (c t, s t ) such that:"
"in realizing the adaptive behavior of sbas, the role of the context is very important. requirements modeled at design time can be vary over the time, therefore, they may not be satisfiable when the context changes. the term context may vary from different perspective as different literatures define various definitions and elements for context. we argue about our definition of context and classify different elements of it. it is necessary to define a context model as part of the requirement engineering for sbas. such context model provides information for triggering situations for the adaptation of sbas."
"is there a specific biological meaning of f (0) ? the frequency of 1.25 hz equals 75 beats per minute (bpm) which is very close to the average heart rate (hr) of young adults. this suggests that hr-which is known to vary with body size, age and sex-is the basic frequency and the scaling factor for all other frequency domains."
"this paper presented experimental results discussing the potential of millimeterwave sar for forest remote sensing on the individual tree level. as can be seen from the experimental results, although there is a certain amount of canopy penetration, a significant part of the signal response is received from the tree crowns. this provides both interesting perspectives for an analysis of forest volumes by continuous tomosar models as well as the reconstruction of individual tree models by utilization of discrete tomosar models."
"uncertainty of requirements: uncertainty is a fundamental issue and a major challenge in almost all intelligent systems. theories of uncertainty have been identified in management and economics. such theories could be application for self-adaptive software systems. so far there is a lack of such theories in dealing with uncertainty in requirement engineering models. in order to deal with uncertainty, we need to be able to represent/ model it and reason about it. various techniques and frameworks have been introduced for reasoning uncertainly [cit] . apart from this, understanding the degree of uncertainty of the context is necessary. classification of uncertainty degree is reported in the literatures [cit] ."
"we discussed the state of the art of requirements engineering for adaptive systems. we started by briefly describing significant characteristics of self-adaptive systems and continued by explaining the adaptation lifecycle in sbas. with this regard, we discussed about corresponding activities and issues need to be incorporated into the framework in each phase. we focused on requirements engineering activities namely requirements elicitation, requirements modeling and specification, and requirements monitoring, as a basic discipline in developing adaptive systems and in particular for sbas. we argued about our definition of context and classified different elements of it. moreover, we pointed out the importance of defining a context model as part of the requirement engineering for sbas. such context model provides information for triggering situations for the adaptation of sbas."
"tree fruit production is a very labor-intensive business. in the us, for example, labor account for over 50% of the variable costs to produce apples. additionally, the number of workers required varies significantly throughout the year-in the state of washington, for example, it fluctuates between 5,000 workers in the winter time to 35,000 at the peak of harvest. clearly, there is an opportunity to introduce automation solutions into tree fruit production to lower labor costs, smooth out labor requirements, and increase production efficiency. this opportunity is compounded by the introduction of high-density planting architectures in the past twenty years, where fruit grows along \"walls\" formed by the branches of trees just four to six feet apart. autonomous vehicles driving down along these fruit walls can mow and spray, as well as carry workers pruning, thinning, performing tree maintenance, and harvesting."
"it was critically important for the project to encapsulate and separate dataset-specific details so that we can maximize the reusability of the code in addition to enforcing proper geospatial information management of secured un mission critical data. for that purpose, we devised a simple yet programmable interface called modify(f) to modify the vector tile schema on the fly. the dataspecific codes are written in a javascript function named modify(f) that takes a geojson feature, and that returns a geojson feature. in this function, a vector tile designer can modify all three properties, i.e. geometric properties, thematic properties, and tippecanoe properties which feed such as the layer name, minimum zoom, and maximum zoom. when we need to drop a specific given feature, the modify(f) function shall return null. in the vector tile production code, modify(f) is called after a feature is imported from the data source and before the feature is piped to tippecanoe. by this on-the-fly modification method for vector tile schema, we could dramatically reduce disk access while keeping the memory usage small."
"the obstacle position is found by combining the clusters representing the obstacle's edges and body. the process is executed by a sweeping algorithm using the obstacle body clusters. the algorithm evaluates the distance from body clusters to all edge clusters. if the distance is smaller than a threshold d o, it is assumed that the body and edge clusters represent the same obstacle:"
"however, not all technologies in dominant web map platforms are open source. vector tile technology has been too expensive for other players including public organizations because they need to develop everything from scratch."
"existing enterprise geospatial application frameworks are largescale and therefore complex. as a result, the front-end web map libraries used in such frameworks are not state-of-the-art. sometimes the front-end libraries used are aged a few years older than the newest one. in such case, direct use of vector tiles in the browser applications makes the application rather slow. in the history of web mapping, similar cases were there when the browser-side vector graphic rendering is in general weak. the was a generation of web maps called \"2.75 generation\" [cit] where tiles are stored as vector tiles, but sent to the client after rendering to the image tiles. we implemented real-time server-side image tile rendering using mapbox gl native for this 2.75 generation solution where necessary."
"google maps, which symbolizes web 2.0, started the service with image tiles. [cit] that google maps 5.0 for android introduced vector tiles in dominant web map platforms [cit] . the choice of android as the first platform for vector tiles suggests that vector tiles require high-performance vector graphics."
"we started the un vector tile toolkit inside the un open gis initiative, aiming for supporting public organizations, such as the un and government organizations, to provide basemap vector tiles."
"finally, for the low-cost chr-6dm imu, the main source of error is the assumption made by the imu internal extended kalman filter (ekf) that the accelerometers only measure the gravity vector. because of that, the errors associated with pitch angle measurement increase whenever the vehicle accelerates. the maximum absolute error measured is about 5"
"many researchers have chosen to overcome these camerabased limitations by adopting active sensors such as sonars, radars [cit], and laser range finders. the laser is the most recommended sensor for the application, due to critical advantages in accuracy, compared to radar, and processing speed, compared to vision systems [cit] ."
"web maps of public organizations are used not because their performance or functionalities are better than dominant web map platforms. in many cases, they are used because of their information content that fit into respective administrative purposes. this gap allows public organization web maps to make clients' duties more efficient, by catching up with dominant web map platforms using open source vector tile technology. the adoption of open source vector tile technology is also an opportunity to make their mapping efforts more cost-effective."
"the project for tabular maps is external to the un vector tile toolkit, aiming for visualizing administrative bodies by an array of simple squares rather than the realistic administrative area polygons. the un vector tile toolkit is used to produce vector tiles to visualize tabular maps using mapbox gl js."
"from this viewpoint, we decided to use existing reliable open source software such as tippecanoe, maputnik, mapboxgl, vector tile optimizer, and osmium tool. we also decided to implement automated production and vector tile server in node.js, a single javascript runtime platform. also, we decided to publish these node.js scripts under open source licenses. this strategy led us to two major challenges as below."
"presentation of maps is transforming, due to the digitalization of information environment and the mobilization of information terminals. responsive and real-time presentation of the maps in the field environment is becoming indispensable functions in modern web maps."
"we have identified four major tasks for the toolkit in figure 1 and the list below. to produce vector tiles from source data, in such form as postgis database, shapefile, or openstreetmap pbf, using tippecanoe ."
"the integrated obstacle detector will present different characteristics from the ones presented here, caused by the new laser configuration and vehicle's pose provided by the apm localization system, among others. one important future requirement consists on obtaining the final sensor setup error covariance and consider uncertainty during 3d laser scanner measurements registration."
"from old days, maps often consist of points, lines, and polygons. in such cases, it is natural and reasonable to handle maps in vector form."
"once lateral and longitudinal discontinuities are identified, 3d points that belong to both categories are marked as obstacle edges. this is useful because many obstacle avoidance approaches operate based on finding edges of objects in the vehicle path [cit] ."
"the international archives of the photogrammetry, remote sensing and spatial information sciences, volume xlii-4/w14, 2019 foss4g 2019 -academic track, 26-30 [cit] openstreetmap, natural earth, srtm, glc30, and un internal geospatial data which includes data from un missions."
"in this article, web maps refer to maps presented in information terminals including mobile ones by using internet connections. in this section, we shortly review the need for vector tiles in web maps."
"for styling and optimization, there was no development required. we used maputnik for assisting configuration of the mapbox style description. we used vector tile optimizer for assisting the measurement of the size of vector tiles."
"where α is a reference angle and h is a reference height employed for discontinuity detection. figure 7 illustrates how the height and angle are calculated. the classification is executed in two directions: first we look for lateral discontinuities along the laser measurement plane; then for longitudinal discontinuities along the vehicle's direction of motion. the former depends only on the laser's current measurement vector; the latter depends on the analysis of multiple vectors along a period of time, stored in short-term memory."
"in addition to the application for the basemap vector tiles for the internal use in the un, the un vector tile toolkit is used for the following applications."
"we propose a practical obstacle detection system for autonomous orchard vehicles, i.e., one that does not add hardware cost to the vehicle and does not depend on expensive, high-accuracy gps-based localization. the system takes as inputs the 3d laser scanner, imu, and encoders measurements and outputs a binary assessment of obstacles on the vehicle's path. obstacle detection is performed in four steps: sensing and sensor data filtering, registering the 3d laser scanner measurements with the vehicle and inertial coordinate frames, classifying the registered 3d points as candidate obstacles, and clustering candidate points as obstacles. in its present form the system is implemented as an offline matlab process that runs on actual field data collected in apple orchards. in its final form it will run on-line to adjust the vehicle speed in reaction to detected obstacles, and alert the operator as to their presence."
"the base map is a most fundamental and a biggest data component with a longest lifespan. therefore, the stability and sustainability of the toolkit is the most fundamental value. another important value is the speed both in production and application use."
"the obstacle detection system described in this paper is part of the larger goal of demonstrating an apm's feasibility to operate year-round in a commercial production environment. the methodology was tested and validated using real-world data from our experimental orchard-like nursery and a commercial orchard. currently, the data processing is executed off-line in matlab. there are several steps we need to follow before the system can be integrated into the apm's navigation system."
"we have measured the performance of the production of vector tiles with the specification in table 1 . table 2 shows the measured production time for each area in fig. 3 and the whole world as a sum. according to github.com/openmaptiles/openmaptiles/issues/242, openmaptiles requires 37 days, which is as long as around 900 hours, to produce global vector tileset with its default production script."
"we propose a push-broom configuration where the laser is installed in a fixed position on the vehicle, tilted to the horizontal, such that the measurement plane intersects the ground in a line at some distance ahead of the vehicle. as the vehicle moves forward, the laser measures different lines on the ground, allowing us to create point clouds representing the terrain surface while dispensing with sweeping mechanisms. as we show in the results section, this simple and practical configuration is robust enough to meet the requirements of obstacle detection in orchard environments."
"the first step in obstacle detection consists of taking measurements from the laser scanner, imu, and encoders, and appropriately filtering them to account for various sources of errors."
"public organizations do not necessarily have enough resources for information technology infrastructure and expertise. also, this project does not have plenty of resources for information technology infrastructure and expertise. therefore, we reduce the use of middleware as possible to minimize the running and maintenance cost. also, for flexibility and cost reduction, we eliminate packaging efforts and decided to expose internal modules transparently."
we needed to continue to learn open source culture and needed to join the open source geospatial software community for the sustainment of the project.
"in this paper we address obstacle detection for autonomous orchard vehicles driving at working speeds of up to 1 m/s, using only the sensing suite already in place for row following-namely, a laser scanner, imu, and steering and wheel encoders. our methodology consists of four steps: first, 3d range measurements are taken by the onboard laser scanner; second, the measurements are registered to the vehicle and inertial coordinate frames; third, the 3d points are classified according to their belonging (or not) to an obstacle; and fourth, the 3d points are clustered and the obstacle position is estimated. obstacle avoidance is currently implemented as an off-line matlab process. tests conducted at our experimental orchard-like environment and an actual apple orchard in washington state indicate that the method is able to detect people and bins placed on vegetation along the vehicle path. we also conducted \"stretch\" tests to verify how far beyond the original requirements the system can respond. the results indicate that it is capable of dealing with objects as small as 15 cm tall as long as they aren't covered by grass, and to detect people crossing the aisles at walking speed."
"for benchmarking and reference purposes, we released vector tile production software from planet.osm.pbf at github.com/unvector-tile-toolkit/produce-320. with this software, we tried to focus on benchmarking the speed of vector tile production from a global dataset. in order to ensure a good speed, we have devised the following two techniques."
"on the top of the architecture of the 'onyx' above, we prototyped realtime server-side image tile rendering at github.com/un-vectortile-toolkit/carbon using node-mapbox-gl-native. the first implementation of the server that provides vector tiles and realtime rendered image tiles is at github.com/un-vector-tiletoolkit/moai. the server is implemented as a simple node.js script which is less than 140 lines of code."
"we have developed several variations of vector tile production software depending on different data source. our main focus remained in producing basemap vector tiles to be used internally inside the un. in this primal use case, we make use of data from"
"once the requirements to detect people and bins in orchards were met, we turned out attention to studying the obstacle detection system's operational limits. in particular, we were interested in finding out what's the smallest identifiable obstacle, and what's the system's response to moving obstacles."
the last step in obstacle detection is to cluster the candidate obstacle points to eliminate false positives caused by terrain irregularities and vegetation. the idea here is to declare as an obstacle only those clusters consisting of a minimum number of points.
"the main onboard processor is a rugged, waterproof, industrial computer with an intel core 2 duo 1.6 ghz cpu with 4gb ddr2 dram from small pc. the navigation software runs on ubuntu linux, with the message passing provided by willow garage's robot operating system (ros)."
"the laser is installed in a push-broom configuration, with the inclination with respect to the horizontal defined according to a safety buffer that we wish to maintain ahead of the vehicle. laurel travels at a maximum speed of 2 m/s, and has a stopping time of one to two seconds. therefore we choose to detect obstacles at least 4 m away to avoid abrupt maneuvers. since the maximum possible mounting height is 1.45 m above the ground, we installed the laser scanner at an angle of −20"
"the data collected was processed by the obstacle detection methodology in matlab. it takes as input the log files and outputs a three dimensional representation of the environment, a binary assessment of the presence of obstacles, and the obstacle's location with respect to the inertial frame o i ."
"by varying the cluster parameters c e and c b, it is possible to adjust the system's response. decreasing the parameters will increase the system sensibility, allowing it to detect smaller obstacles, but also introducing a larger number of false positives. this undesirable effect is aggravated when dealing with irregular vegetation."
"the international archives of the photogrammetry, remote sensing and spatial information sciences, volume xlii-4/w14, 2019 foss4g 2019 -academic track, 26-30 [cit], bucharest, romania 7.2.2 cartotiles: un geospatial service is planning to release vector tiles for geospatial visualization in very small map scale. un vector tile toolkit is used for the production and update of such vector tiles."
"in naro one of the authors developed vector tiles of agricultural plot polygons, which were provided as open data from the ministry of agriculture, forestry and fisheries (maff) of japan, by making use of the un vector tile toolkit."
"the international archives of the photogrammetry, remote sensing and spatial information sciences, volume xlii-4/w14, 2019 foss4g 2019 -academic track, 26-30 [cit] with assistance from these tools, we repeatedly updated style.json and modify.js to obtain optimal vector tiles and its styling settings."
"a similar procedure is executed for the other, non-edge points classified as candidate obstacles. these discontinuities correspond to obstacle body points (pb), that are clustered together if the distance between them is smaller than a threshold d b :"
"as described above, the transition to vector tiles is inevitable in web maps. it is worth noting that the transition involves the use of open source methodology especially among players other than dominant web map platforms to share the key vector tile technology. thanks to the open source methodology, the vector tile specification is loosely converging, which benefits both map providers and map users."
"stereo vision [cit], omnidirectional vision [cit] and color segmentation [cit] have been used for obstacle detection and to determine the shape of the world around the vehicle. the main advantage of these approaches is their relatively low cost, as they rely essentially on commercial cameras. passive vision, however, suffers from lighting, color constancy, and dynamic range effects that decrease the performance of the obstacle detection system and cause both false positive and false negative detections."
"the obstacle detection system, with its push-broom configuration, was designed for low-cost, stationary obstacle detection. in practice, however, we can always expect people moving in front of the vehicle. therefore, it is important to check the system's capacity to detect dynamic obstacles. to that end we had a person perpendicularly crossing the path of the vehicle at walking speed. the result is presented in figure 14 . the detection system identifies several obstacle edges, according to the person's motion. due to the cluster distance d o, the edges are grouped in one obstacle, marked by a black star in the graph."
"although [cit] s, public organizations, such as international organizations, governmental organizations, and nongovernmental organizations, has not fully adopted the technology. this delay of adoption results in the use of more conventional web map technology than dominant web map platforms."
the purpose of the un vector tile toolkit is to enable all players to catch up with the vector tile technology of dominant web map platforms.
we need a good networking mechanism of vector tile experts distributed in different organizations and different geographical areas. vector tile experts tend to isolate in their respective work areas because vector tile technology is highly specialized.
"the clustering procedure is divided in three stages. first we cluster the obstacle edges. second, we cluster the other candidates points that correspond to the obstacle body. finally, we combine the clusters in order to estimate the obstacle position."
"the second step is to integrate the obstacle detection system with the vehicle's driving system. the detection system will be implemented within the robotics operating system framework, to enable online obstacle detection and collision avoidance. whenever an obstacle is detected in front of the vehicle, a message indicating the position of the object with respect to the vehicle must be generated. the autonomous driving system then uses this information to take appropriate action, including if necessary stopping the vehicle and alerting the operator. in the case of multiple obstacles, the detection system informs the position of the closest one to the vehicle. once the operator removes the obstacle, the system resumes driving."
"gsi has been prototyping its vector tiles from several years ago. as a partner of the un vector tile toolkit, they use the toolkit to produce their prototype vector tiles. their feedback was useful to improve the codebase of the un vector tile toolkit."
"the smaller cubes with 20 and 15 cm sides are partially occluded by the tall grass, and the detection system is not able to distinguish them from the vegetation. if we decrease the cluster parameters c e and c b to increase the system's sensitivity, the algorithm starts presenting false positives before detecting the boxes. figure 13 presents a typical result obtained with the 20 cm box, where one can see the three false obstacles marked by black stars. detecting a 25-cm wooden box in tall grass. this experiment is at the limit of the obstacle detection system's capability to find the obstacle. fig. 13 . the obstacle detection is not capable of uniquely identifying a 20 cm box in tall (17.5 cm) grass. in this example, the system found the box and three other false positives."
"for the past three years we have been developing a family of such vehicles, which we call autonomous prime movers, or apms (figure 1 ). the current apms are capable of autonomously driving between a row of trees, turning at the end of the aisle and entering the next one. row following is conducted at the center of the aisle (e.g., for sensing or mowing) or at a predefined distance from the trunk line (e.g., for pruning, thinning, or spraying). to be affordable to growers, the apms do not carry a high-accuracy gpsassisted inertial navigation system (ins), as is usual in agricultural automation. rather, they navigate using only one laser rangefinder, a low-cost inertial measurement unit (imu), and steering and wheel encoders. [cit] the five vehicles in the apm family drove a combined 330 km in research and commercial orchards in several us states. preliminary results indicate that workers on an apm-mounted platform can conduct some tasks on the top of the trees in half the time taken by workers on ladders or on foot [cit] ."
consider as an example the wooden box in figure 8 . the edges are composed by two point sets forming red vertical lines. we begin the clustering process combining the points forming these vertical lines. all the points (pe) identified as edges are clustered together if the distance between them is smaller than a threshold d e :
"it remains a crucial challenge for the un vector tile toolkit to formulate, develop, and sustain a community of developers by appropriate documentation. also, we need to keep the following two principles to sustain the un vector tile toolkit."
"in this primal use case, the enterprise architecture is organized to centralize all the data into a postgis database. therefore, we implemented data import from postgis using node-pg. also, for benchmarking and testing purpose, we implemented data import from shapefile using node-shapefile and ogr2ogr. we also implemented data import from openstreetmap pbf using osmium-tool."
"obstacle detection is a key capability for autonomous vehicles, and a specially important one for vehicles navigating in agricultural environments. in essence, obstacle avoidance consists of determining whether the space ahead of the vehicle is clear from obstructions for safe travel [cit] . its goal is to detect all obstacles along the path in time for the vehicle to react to them, while minimizing misclassifications."
"the paper is organized as follows. in section ii we present a review of related work, with special focus on obstacle detection for ground vehicles operating in orchardlike agricultural environments. in section iii we present the autonomous orchard vehicle used as a platform in which the obstacle avoidance method was implemented and tested. in section iv we present the method per se, and in section v the results of extensive field experiments conducted in orchards. we conclude the paper in section vi indicating avenues for future work."
"several obstacle detection approaches proposed and implemented in real systems, e.g., [cit], showed good results for low speeds (under 3 m/s). still, many difficulties are associated with obstacle detection in natural terrain, especially due to the presence of vegetation. on one hand, tall grass can be erroneously classified as an obstacle; on the other hand, an obstacle occluded by vegetation may not be detected. additionally, we look here for technologies that are both practical (i.e., low-cost) and robust enough to augment the well-established navigation and driving systems."
"note that the obstacle's body clusters are used solely to identify the obstacle, reducing the number of false positives, and do not influence the estimation of its position. the procedure is illustrated in figure 8, with the detected obstacle marked as a black star."
we have also developed several variations of vector tile servers. a standard implementation is published at github.com/un-vectortile-toolkit/onyx. the implementation makes use of http/2 protocol to accelerate the access to vector tiles.
"the obstacle detection system performed satisfactorily when using actual field data. at robot city in pittsburgh, both the bin and the person were scanned in 16 experiments from different orientations with no false positives. at sunrise orchard the system, designed to detect large bins, was able to detect boxes as small 15 cm tall on 5 cm grass, and 25 cm tall on 17.5 cm grass. finally, back at robot city the system was able to detect a person moving perpendicular to the vehicle's path at walking speed. of course, we do not claim that this means the system can deal with any type of dynamic obstacle, but the result obtained is certainly a positive one."
"in implementing the toolkit, we make maximal use of existing fast and stable open source software while develop and share some mission functions and integrations in an open source manner."
"the base vehicle used in this work is the apm \"laurel\" (figure 2 ). it is based on the toro mde eworkman electric utility vehicle, retrofitted to function either in manual or drive-by-wire mode. the base retrofitting process consisted on installing a steering motor, brake motor, motor drivers and steering and wheel encoders. laurel is a research vehicle, where we implement and test orchard navigation technologies before they are ported to other vehicles in the apm family. it is important to note that, while laurel is equipped with a high-accuracy applanix pos 220 lv ins/gps system, we do not use it for the obstacle detection described here-otherwise, it would be impossible to port the software to the other apms, since they do not have such a system onboard. on the contrary, applanix data is used for the sole purpose of calculating sensor measurement errors during the obstacle detection system development process. likewise, while laurel is equipped with a variety of laser scanners and cameras, here we only use one laser scanner. the relevant sensors for this work are: steering and wheel encoders with angular resolution of 0.38"
"however, in reality, digital maps are first managed in image forms and then in vector forms, because of the graphics performance of computers or some constraints in information management. this compromise in using image forms has been seen in web maps, too."
"although not for public use, we also produced vector tiles from data other than openstreetmap. table 3 shows the size of the produced mbtiles packages."
"to assess the feasibility of the obstacle detection methodology presented in section iv, we collected field data from an experimental orchard-like nursery and an actual apple orchard. the former is a half-acre ornamental tree nursery planting at robot city, a robotics experimentation site in pittsburgh, pa. the latter is washington state university's sunrise orchard in rock island, wa. at both locations we manually drove the vehicle as it approached a person and a commercial apple bin, and recorded the registered 3d laser scanner, encoder, and imu data. in pittsburgh we collected additional data while a person walked in front of the vehicle; and at sunrise we collected 3d data as the vehicle drove toward a set of wooden boxes of sizes 30 cm, 25 cm, 20 cm, and 15 cm, both in short and tall grass. the objective with these additional datasets was to stress-test the system and assess how well it can perform beyond the stated requirements of detecting stationary people and bins."
"fast and real-time map presentation in web maps has been studied from the last century. there were many examples in streaming mosaiced maps observing spatial locality of map data extraction and aiming to minimize the latency between each information request and information presentation. such mosaiced maps are called 'tiles.' a tile is defined as a tessellated representation of geographic data, often part of a set of such elements, covering a spatially contiguous extent which can be uniquely defined by a pair of indices for the column and row along with an identifier for the tile matrix [cit] ."
"first and foremost, the obstacle detection method was developed and tested with data from a dedicated laser scanner mounted to meet the method's requirements. in an actual field deployment, that one laser scanner would have to provide data to both the navigation and the obstacle detection modules. the navigation module, however, currently uses a horizontally-mounted scanner about a meter off the ground. therefore, we need to refactor it so it can operate with the laser mounted at a higher location in the push-broom configuration required for obstacle detection."
"while performance when driving between rows is satisfactory, the current system does not include the capability to detect obstacles in the aisle and adjust vehicle speed accordingly. clearly, this is a safety requirement that must be addressed before apms can become part of the tree fruit grower toolbox. additionally, any obstacle detection system must not add to the hardware cost of the vehicle, lest cost issues increase the adoption barrier. finally, the system must robustly detect the two major obstacles found in orchards: people and bins."
we set four challenges to both to promote producing and application of basemap vector tiles inside the un and also sustain the project by meeting the requirements of various public organizations:
"vector tiles enable dynamic map styling and hyperlinking from features because vector data are sent to the information terminal directly. also, if well-designed, vector tiles tend to be smaller than image tiles. for these reasons, vector tiles are already a standard in dominant web map platforms."
"it was said that 70% of the earth surface is the ocean. therefore, there are modules without any data. we defined such no-feature-modules (nfm) that are calculated from the actual openstreetmap data, which sums to 1669 modules out of 4096 modules that cover the whole globe."
"keeping these knowledge layers conceptually separated, implementation will guarantee the flexibility and reusability of the callimachusdl application for every type of domain. figure 1 shows the framework of this approach, along with named examples for better understanding. the domain layer holds the different domain ontologies that can be used. the application layer will use one or more domain ontologies depending on the sort of topics the dl application is going to deal with. the dublin core ontology will always be used to represent the basic metadata concepts of every resource."
"following the arguments in the function call above, this model allows the conditional mean to depend on whether or not a site was mined and to vary randomly by site. it allows the number of structural (i.e., extra) zeros to depend on mining. additionally, it allows the dispersion parameter to depend on the day of the year. this model can be represented by the following set of equations"
"a corotational finite element method is used with a linear elastic model to compute the displacement of the organs under abdominal gas pressure. an implicit time discretization is employed for unconditional stability. as a solution method, dynamic modeling is selected to cope with inertia effects and organ collisions. the mechanical system with abdominal wall, liver, and surrounding tissues is deformed under the internal pressure generated in the abdominal cavity, which represents the insufflated gas, shown in fig. 3 . the applied force field together with the internal tissue properties deform the meshes while preserving the mesh topology. the force, acceleration, and displacement field for each node element are integrated and computed in an iterative approach."
"in (2), the partial differential equation for the mechanical model is given. when the model achieves stability, the gas pressure forces (f gas ) are balanced with internal mesh stiffness forces, and the final node positions (u) are determined. the parameters m, d, and k represent the mass, damping, and stiffness matrices. in the proposed system, the model parameters are the gas pressure, poisson's ratio, and young's modulus for each tissue. the initial parameter values are set according to the literature [cit], and the gas pressure value is collected during surgery. the framework has been implemented in sofa 2 library."
"recently, a new breed of user generated content aware technologies which have been encompassed by the \"web 2.0\" buzzword have turned up to provide a huge amount of metadata and information about the user as a particular entity."
"the sample was composed of students in the final year of the computer science degree of the university carlos iii. the students, as part of their tasks within the subject \"software"
"from the perspective of the results, the implementation of callimachusdl may be considered a success. the opinions of the user in relation to \"user experience\", \"faceted search experience\""
"the design goal of glmmtmb is to extend the flexibility of glmms in r while maintaining a familiar interface. to maximize flexibility and speed, glmmtmb's estimation is done using the tmb package [cit], but users need not be familiar with tmb. we based glmmtmb's interface (e.g., formula syntax) on the lme4 package -one of the most widely used r packages for fitting glmms [cit] . like lme4, glmmtmb uses mle and the laplace approximation to integrate over random effects; unlike lme4, glmmtmb does not have the alternative options of doing restricted maximum likelihood (reml) estimation nor using gauss-hermite quadrature to integrate over random effects [cit] . the laplace approximation may perform poorly when there is little information available on each random effect level [cit] . reml may be added to glmmtmb in the future. the underlying implementation using tmb is a fundamental difference compared to lme4 and provides glmmtmb with a speed advantage when estimating non-gaussian models (figures 1 and 2 ) and gives it greater flexibility in the classes of distributions it can fit (table 1) ."
"default number of iterations, burn-in samples, and thinning. in each package, we fit zero-inflated poisson models with six fixed effects, one random effect; we also accounted for overdispersion, although sometimes (of necessity) in slightly different ways with different packages (e.g., negative binomial vs. log-normal-poisson models). we allowed zero-inflation to vary with food treatment and vary randomly with nest. see appendix b for details of these methods, including code."
"these links relate one page to another basically according to user considerations. if the relation between dl resources were represented by means of semantics, the application would be able to provide mechanisms to semantically navigate between related resources with real meaning."
"in this paper, a method is proposed for accurate registration of pre-operative images to intra-operative images driven by biomechanical modeling of abdomen deformation under gas insufflation. the coupling between the registration and insufflation model is achieved by optimizing the intensity similarity measure between the modeled pre-operative image and the intra-operative image. the dynamic model parameter optimization with intensity information differs our approach from the existing methods [cit] that used constant model parameters."
"observed response variables are often in the form of discrete count data, e.g., the number of times that owl nestlings beg for food [cit], counts of salamanders in streams [cit], or counts of parasite eggs in fecal samples of sheep [cit] . these counts are often analyzed using generalized linear models (glms) and their extensions [cit] . glms quantify how expected counts change as a function of predictor variables, e.g., nestlings change their behavior depending on which parent they interact with [cit], salamander abundance decreases in streams affected by coal mining [cit], and helminth infection intensity in sheep decreases in response to treatment with anthelmintic drugs [cit] . repeated measurements on the same individual, at the same location, or observations during the same time period are often correlated; this correlation can be accounted for using random effects in generalized linear mixed models (glmms; [cit] ."
"the sample was composed of 17 women (32%) and 35 men (68%), with an average age of 25.6.during the administration of the questionnaire, the subjects were aided by a research assistant with the objective that the coding of the questionnaire was carried out accurately, eliminating any student doubts or errors."
"these are estimates of the linear predictors (i.e., predictions on the link scale: logit(prob) and log(cond)), not the predictions themselves. the easiest thing to do for the point estimates of the unconditional count (ucount) is to transform to the response scale and multiply:"
"the domain of dl has not been ignored during the growth of the semantic web. the use of semantics in dl can outperform the current endeavors that require finding data spread out across the dl structure and dynamically drawing inferences, something continually hampered by their reliance on ad-hoc, task specific frameworks in present dl technologies. in this paper, callimachusdl is presented, a semantics-based dl which uses semantic information gathering and browsing to enhance search and retrieval. one of the originalities of our work is to include social web multimedia content semantic annotation as a part of the digital library."
"in this article, we outline the r packages available for fitting models to count data while introducing glmmtmb. we assume that the reader already has a basic understanding of glms [cit], glmms [cit], and zero-altered models [cit] )."
the model based registered pre-operative image is warped to intra-operative image using diffeomorphic non-rigid registration [cit] . it is a dense matching method driven by the gradient of local cross correlation similarity measure. the gaussian smoothing regularization is applied on time-dependent velocity fields and their composition is used to estimate deformations.
"fundamentally, the callimachusdl approach is radically different to the ones detailed before since semantic navigation is proposed, along with faceted search and browsing, metadata representation format and usability as the main building principles of the entire approach."
"the motivation for this work proposes a solution to these requirements. callimachusdl is focused on the aforementioned requirements, solving them, and proposes an integrated solution that uses semantic information gathering and browsing to enhance search and retrieval. in the next section, the approach for callimachusdl will be discussed."
"and \"performance\" have displayed satisfactory results in average figures. in particular, the factor \"user experience\" displays a score of 3.04 points over 4, with a highly adjusted standard deviation of 0.59, which indicates that, apart from the scoring being high, the agreement between the subjects is acceptable. the attempts to develop the graphical user interface of callimachusdl so that it would be attractive to users has been judged positively by test users."
"in relation to the searches carried out by the subjects, the judgments of the faceted search present higher values than the keyword search. taking into account that the users have experience with keyword search, the higher ranking of faceted search in comparison with keyword search is an extremely significant result. apart from the statistical evidence provided by the statistical test which evaluated the improvement, it can be indicated that the performance of faceted search, the differentiating element of callimachusdl over other platforms, has been highly strengthened."
"callimachus ( [cit] ) . following the callimachus efforts, the man that improved subject search in alexandria, this paper presents a novel approach to improve browsing and searching in dl by adding semantics to the definition of resources, particularly multimedia resources. in a larger context, the problem of dl scaling may be multiplied by thousands of data structures located in hundreds of incompatible databases and message formats. the uses of semantic digital libraries and social networks have been identified as one of the most promising research line in the digital libraries field [cit] . in this scenario, callimachusdl can be seen as an original initiative to bridge the gap between digital libraries and social web."
"here we fit the model with zero-inflation depending on some of the predictor variables. we can no longer use glmmadmb or inla (inla allows the zero-inflation probabilities to depend on covariates in hurdle models -\"type 0\" in the inla documentation -but not for zero-inflated models)."
"in this paper, a method has been proposed to estimate the deformation caused after gas insufflation using pre-and intra-operative images. the validation of the proposed approach was performed on both synthetic human ct and in vivo pig ct scans. in the synthetic case, under noise and downsampling conditions, a mean registration error of less than 2.5 mm was obtained, which demonstrated the robustness of the method against noise disturbance and low resolution. in the second evaluation, insufflated and non-insufflated in vivo pig ct images were used. a mean registration error of 0.88 mm for liver and 1.75 mm for abdominal wall showed the applicability of the method in laparoscopic surgeries as a registration algorithm. furthermore, the error values were compared against the ones obtained with standard diffeomorphic non-rigid registration method, demonstrating the main contribution of the proposed method. additionally, the error values showed that the use of registration approaches on estimating the gas insufflation deformations can provide more precise tissue alignment compared to only biomechanical model based approaches [cit] . the proposed approach is generic; as such, it can be applied to other medical problems where large deformations appear due to external intervention (e.g. brain-shift, histology needle guidance). the future work will investigate: validity of the method on these problems and the effect of respiration and cardiac motion on the algorithm accuracy. lastly, the authors thank prof. j. marescaux, dr. a. hostettler, ircad france & taiwan, and show-chwan memorial hospital for providing the pig data."
this function returns a list of vectors. the list has one element for each simulation (nsim) and the vectors are the same shape as our response variable.
"zero-inflation can be used with any of the distributions in glmmtmb, so we compare the same conditional and zero-inflation models with poisson, conway-maxwell-poisson, and negative binomial distributions."
"where m and r denote the moving (pre-op) and reference 3d images (intra-op) respectively, and d lcc is the local cross correlation intensity similarity measure. the model guided registration and diffeomorphic transformation are represented with ψ m and ψ d respectively."
"evidently, the results are not identical, the second variable displaying a difference of 0.10 positive points with respect to the first. this circumstance can be explained by the differences of scales which both variables represent."
"given the aforementioned problems that traditional dl cope with, the callimachusdl approach is based on several design principles to avoid these drawbacks, and built as a kernel to develop a fully-fledged semantic working environment for the final users. these design principles are as follows:"
"experience\", obtained the opinion of the user in relation to the perception of the feature, and \"faceted search\" included the search results. similarly to the statistical analysis previously mentioned, the student's t-test was applied, setting the level of statistical significance at 0.05."
the flexibility of glmmtmb enables users to fit and compare many varieties of models with assurance that the log-likelihood values are calculated in a consistent way. comparing likelihoods of models fit by multiple packages must be done carefully because some packages drop constants from the log-likelihood calculations while others do not.
"the package is available from the comprehensive r archive network (cran) via the command install.packages(\"glmmtmb\"). the current version is 0.2.0. development versions are available from github and can be installed using devtools [cit] . current details for installing development versions should be accessed on the github page https://github.com/glmmtmb/glmmtmb."
"evaluation of the proposal was required in order to determine the level of acceptance of the proposal. with the objective of calculating the grade of adjustment of the proposal, a study was designed which was aimed towards students in the final year of the computer science degree of the university carlos iii, specifically, those undertaking the subject \"software engineering iii\". among its learning objectives, this subject has the aim that the students are capable of extracting the requirements of a software application of a client, with the final objective of being able to construct it according to a defined methodology. therefore, the students were requested to use callimachusdl to categorise distinct contents generated by the students themselves during the course of completion of the subject. in the first place, the students carried out two interviews with the final clients (a role represented by one of the lecturers). the multimedia formats generated were uploaded to youtube. in the second place, the students visited one of the work locations of the client, where they photographed the installations as part of the modeling of the current system. the photographs were uploaded to flickr for sharing among the members of the work team. thirdly, the students were instructed to share the links which were internet resources related to software engineering using the tool del.icio.us. the use of these three web 2.0 related technologies were the basis for the evaluation of callimachusdl."
"hence, future work will consist of evaluating the implementation and approach more carefully, validating callimachusdl with a number of quality-aware case studies and using large dl resources where pooling out of results can determine more accurately if the effectiveness of the breakthroughs of the approach detailed in section 3 take place. in a more general view, future work should further integrate social networks full potential into digital libraries. in particular, the increasing organizational capacities of distinct web 2.0 tools represent a challenge for callimachusdl, which in the future should be able to combine different multimedia contents generated from various platforms, to act as support for various tasks such as customer relationship management (crm), talent management (tm) or knowledge management (km)."
"lastly, it should be indicated that the results of the variables \"faceted search experience\" and \"faceted search\" are very similar, not presenting significant differences between both."
"estimates and confidence (or credible) intervals (ci) from brms, mgcv, mcmcglmm, and inla were nearly identical to those of glmmtmb, when running the bayesian models with flat priors (figures 8 and 9 )."
"for the standard errors/confidence intervals, we could use posterior predictive simulations (i.e., draw multivariate normal samples from the parameter for the fixed effects). this conditions on/ignores uncertainty in the random-effect parameters."
tissue characteristics used to parameterize the biomechanical model are not patient specific and cannot comprehensively estimate the subtle complex deformations in the abdominal cavity or cardiac/respiration phase specific deformation. this results in a small amount of residual deformation. in this framework a refinement step is proposed to account for this residual deformation.
"because we ran brms with flat priors, the estimates are very close to the ml estimates of glmmtmb. the most different results are from gam, because we used a quasi-likelihood model with a slightly different implicit variance scaling."
"the mean distance error on the in vivo pig images was recorded as 0.88 mm and 1.75 mm for the abdominal wall and liver. it can be concluded that the proposed method gives better registration accuracy compared to biomechanical model alone approaches [cit] that achieved an average registration error of 3.3 mm and 4.3 mm with the same validation method for skin and abdominal viscera. furthermore, comparison against the method with only non-rigid registration approaches shows the contribution of including the insufflation model in registration framework. the computation time for the complete registration process was measured as 12 minutes per 3d image pair (intel xeon w3530 2.80ghz), which can be reduced to a few minutes through parallel optimization."
"the biomechanical model deforms mesh elements and computes the displacement of mesh points. in the proposed framework the pre-operative image is segmented as liver, abdominal wall and surrounding tissue using a semi-automated method, and tetrahedral mesh elements are generated using cgal 1 . similarly, automatic and manual segmentation tools can be used as well since this task is performed before surgery. next, surface points of these volume meshes are tagged automatically, which are used to apply the insufflated gas pressure."
"one of the known advantages of hamiltonian monte carlo, as implemented in stan (on which brms is built), is that it achieves high effective sample size per mcmc step; [cit] and still achieve a minimum effective sample size of approximately 500 (the minimum effective sample size achieved was 460)."
"the method was also validated on the real scenario by using three different pairs of insufflated and non-insufflated pig abdominal 3d-ct scans. the first two pairs were collected from two different pigs under full inspiration and contrast enhancement conditions. in the last pair (pig pair-3), the first pig was scanned with insufflation and half expiration, and registered with the full inspiration noninsufflated scan of the same pig. the quantitative assessment was done again using surface mesh to mesh distance. to demonstrate the contribution of the method, the same datasets were also registered using only diffeomorphic nonrigid registration. the ground-truth surface meshes were collected from intraoperative images using manual segmentation. the quantitative results and the alignment of the first pair are provided in table 2 and fig. 5 ."
"to illustrate how to use glmmtmb and to compare it to other packages, we applied it to two data sets that are distributed with glmmtmb. additional code and graphs for these examples can be found in appendicies a and b."
"secondly, the user provided his opinion about the performance of the framework. the responses to these questions were coded using a likert scale ranging from 1 to 4 points, with the following values. 1: limited, 2: regular, 3: good, 4: very good. lastly, it was suggested that each user carried out two types of search. the first search required the user to perform a keyword search, and the second search proposed to the user was to perform a search using the interface which implements faceted search. upon examining the results of the search, the user could quantify the result using a likert scale with the following values: 1: unsolved, 2: low matching, 3: near matching, 4: correct matching."
"for example, a user called john smith has uploaded a number of videos in you-tube about his stay in norway. in particular, the videos are about norwegian fjords so he tags them with the \"fjord\" and \"norway\" tags. however, tags are freely chosen keywords describing a particular resource. they offer a simple way of retrieving content but they are subjective conceptualizations, being potentially aggregated to a flat bottom-up categorization or folksonomy. [cit], folksonomies have been claimed to be an interesting emergent attempt for information retrieval but serve different purposes to ontologies, the latter are attempts to more carefully define parts of the data world and to allow mappings and interactions between data held in different formats. in this scenario folskonomies had been used for creating semantic metadata [cit] or as a support to learning [cit] . hence, ontologies are defined through a careful, explicit process that attempts to remove ambiguity, whereas the definition of a tag is a loose and implicit process where ambiguity might well remain. finally, the inferential process 13 applied to ontologies is logic based and uses operations such as join. the inferential process used on tags is statistical in nature and employs techniques such as clustering."
"digital libraries represent a new breed of software applications whose aim encompasses categorizing, classifying, archiving and providing access to the vast constellation of web resources. currently, digital libraries (dl for short) are facing a new paradigm shift coping with various challenges which include overcoming traditional browsing or keyword-based strategies. fundamentally, dl infrastructure improvement attempts have been trying to increase the quality of information retrieval, from query expansion to collaborative filtering or multi-faceted browsing [cit] . however, current approaches are still not fulfilling expectations, leading the user in many cases to frustration."
"for these distributions, the expected number of zeros decreases as the mean increases. however, when multiple processes underlie the observed counts, the counts can contain many zeros even if the mean is much greater than zero. for example, an observation of a stream with zero salamanders could be a \"structural\" zero due to the stream being uninhabitable due to mining waste, or a \"sampling\" zero due to the combination of a low mean (due to poor ecological suitability and/or low detectability) and sampling variation [cit] . zero-inflated (more broadly zero-altered) glms allow us to model count data using a mixture of a poisson, negative binomial, or conway-maxwell-poisson distribution and a structural zero component. models that ignore zero-inflation, or attempt to handle it in the same way as simple overdispersion, can yield biased parameter estimates [cit] ."
"benchmarking showed that fitting the negative binomial model to simulated data with the same structure as the original data was, on average, equally fast in glmmtmb and inla, 14 times slower with glmmadmb, 29 times slower with lme4, and 190 times slower with brms. mgcv fit the model the fastest, taking 0.03 times as long as glmmtmb. gamlss took 0.24 times as long as glmmtmb. with increasing numbers of observations, the estimation times of all packages appeared to follow power law functions (figure 1 ). for simulated data sets with increasing numbers of random effect levels, estimation time increased as a power-law function for all packages except inla which had estimation times that accelerated (figure 2 ). the speed of glmmtmb for models with more random effect levels is due to the sparseness handling by tmb. benchmarking nuances such as memory usage and how timings scale with model complexity could be investigated in more detail in future studies."
"once the students had completed the subject, they were requested to respond to a questionnaire in which they were asked to provide their opinion about the tool in relation to different aspects. firstly, they were asked about their user experience of callimachus dl."
"the current section describes the structure and features of callimachusdl, outlining the resources used which comprise its components. these include metadata representation format, multi-ontologies, semantic navigation, usability, and faceted search."
the callimachusdl implementation is based on the swan architecture successfully deployed a motivating scenario will now be presented to illustrate how callimachusdl can be used.
"the callimachusdl architecture is heavily based on the swan architecture [cit] . taking into account these apparently different levels of knowledge (ontologies, resources and semantic information), this knowledge can be divided into three layers:"
"the remainder of the paper is organized as follows. in section 2 an overview of the state of the art is provided. in section 3, a number of requirements are discussed and the benefits of tackling them with a semantically-enhanced approach are presented. in section 4"
"on the other hand, the semantic web has emerged to be a new and highly promising context for knowledge and data engineering [cit] . the term \"semantic web\" [cit], to describe the evolution from a 3 document-based web towards a new paradigm that includes data and information for computers to manipulate. the semantic web enables automated information access based on machine-processable semantics of data. this means that this data will be available for providing precise and exhaustive information retrieval. thus, the semantic web provides a complementary vision as a knowledge management environment [cit] that, in many cases has expanded and replaced previous knowledge and information management archetypes [cit] . semantic web has been named as web 3.0 [cit] as a new version of web 2.0 [cit] has termed the \"giant global graph\"."
"(2009). semalinks are ordinary hyperlinks in appearance but built upon semantic information. this semantic information, consisting both of the ontology concept to which a particular part of the content is referring and its value, will lead the user to pages with content semantically similar to the semalink. that is, if a set of words have been used to form a semalink, with a property x and a value, when a mouse is placed over this link, the nodes appearing will make reference to other pages with 9 the same property x and value, and many more references as properties directly related to property x which exist in the repositories, with the same value."
"it's easiest to see the pattern by using the predict function. to avoid marginalizing over or conditioning on random effects, we can refit the best model without the random effect of site; however, this is not ideal because it ignores the correlation within sites. we present a more rigorous version next."
"the results of the surveys, which were performed using printed copies, were subsequently coded in the statistical analysis tool spss. in the tables displayed below, the average and standard deviation of the responses offered by the students are shown in relation to the questionnaire applied, and the two groups of questions formulated. figure 3 shows the relative importance of the scores by factor and value in the likert scale. search. in this case, this is represented as an improvement in the results of faceted search with respect to keyword search."
"since the first initiatives for their development, digital libraries on the web had to strive for classifying, locating and accessing resources. however, the advantage of the simplicity in dl leads to their great drawback, the increasing volume of information being stored without a clear structure. actually, most current dl cannot be used as fully-fledged environments to create and search knowledge in an efficient way, since the information collected in these systems lies unused by computers, mainly due to the human language in which the resources are written. as further processing is needed, new formal approaches are used to make computers \"understand\" the web content or, more precisely, the application of semantics."
"here we introduce a new r package, glmmtmb, that estimates glms, glmms and extensions of glmms including zero-inflated and hurdle glmms using ml. the ability to fit these types of models quickly and using a single package will make it easier to perform model selection. we focus on zero-inflated counts, but note that there are many other distributions available in glmmtmb, including continuous distributions. we demonstrate the package using two examples without going into details of the reasons why a user would want to fit these models. we use an example of salamander abundance to show how to fit and compare zero-inflated and hurdle glmms and then how to extract results from a model. we then use variations of the salamander data to compare how the timings of different packages scale with the number of observations and random effect levels. we use a classic example of owl nestling behavior to compare the timing and parameter estimates from glmmtmb with other r packages."
"as is generally the case for model formulas in r, the * indicates an interaction plus main effects. we used akaike information criteria (aic) to compare all models via the aictab function from the bbmle package [cit] . for convenience, glmmtmb reports the log-likelihood of unconverged models as na and version 1.0.19 of bbmle puts these models at the bottom of aic tables. the code for fitting these models and doing model selection is presented in appendix a."
we could also examine the distribution of simulated values from the best fitted model. for this we use the function simulate.glmmtmb. this function works for zero-inflated and hurdle models as well as less complex models.
"callimachusdl is described in detail, as well as its architecture and proof-of-concept implementation. section 5 provides an experimental setup for callimachusdl. finally, section 6 concludes the paper providing a number of conclusions and summarizes future work."
"the list of features documented here is not exhaustive. it should be appreciated that brms, gamlss and mcmcglmm have additional features that go beyond the scope of zero-inflated glmms (bürkner, 2017; [cit] . we focus on the process of fitting models, largely neglecting questions of statistical frameworks (frequentist vs. bayesian) or post-fitting procedures such as inference and prediction. for example, having mcmc samples from a fitted model allows a wide range of inferential and predictive procedures."
"the unlimited potential of the web 2.0 is an open field for technology investigators around the globe, and it is also a great opportunity for digital libraries researchers to put together social features and limitless content into a single package."
"we can use aictab to compare all the glmms, including zero-inflated and hurdle models. here, to save space, we only output the aictable for the top four and bottom four models. the most parsimonious model has a conway-maxwell-poisson distribution with effects of species, mining, and their interaction. (pm0, pm1, pm2, cmpm0, cmpm1, cmpm2, nbm0, nbm1, nbm2, nbdm0, nbdm1, nbdm2, zipm0, zipm1, zipm2, zipm3, zicmpm0, zicmpm1, zicmpm2, zicmpm3, zinbm0, zinbm1, zinbm2, zinbm3, hpm0, hpm1, hpm2, hnbm0, hnbm1, hnbm2) the log-likelihood of the unconverged models is reported as na so that these models appear at the end of the aic table. the negative log-likelihood could be extracted with zinbm1$fit$objective if it was needed."
"set of functions f (x, y) are integrated using the integration schemes above (numerical, semi-analytical and exact integration techniques), over a symmetrical hexagon with coordinates shown in fig -1(a) . set of functions selected include polynomial function, rational function, function with rational power, natural logarithmic function and exponential function. the symmetrical hexagon is separated into 2 regions; r 1 and r 2 according to the requirement of fubini's theorem, as shown in fig -1(b) . each region is enclosed by two constant lines and two linear functions. tables 1-6, it is seen that for numerical integration technique, the generalized gaussian quadrature tends to provide converging solution when integration order is increased. for all the functions which have been tested, generalized gaussian quadrature outperforms classical gauss legendre and 5 th order gaussian quadrature. fractional functions could not be solved using classical gauss legendre and 5 th order gaussian quadrature for certain integration orders. this is due to the fact that some of the integration points or weights of these rules contain value of zero (leads to singular point). apart from that, these quadrature rules were not generated based on fractional functions."
"1. place k points into the space represented by the objects that are being clustered. these points represent initial group centroids 2. assign each object to the group that has the closest centroid 3. when all objects have been assigned, recalculate the positions of the k centroids 4. repeat step 2 and 3 until the centroids no longer move"
"in the decision phase, the mobile entity cooperatively decides, based on its evaluation of the current situation, which action to perform at the action phase."
"where d coll is the minimal distance at which two mobiles are allowed to be. figure 1 illustrates the computation of c 1,stayput for m 1 regarding two mobile entities m 2 and m 3 in its perception zone."
"the studied problem is combinatorial: the huge number of heterogeneous airplanes, the size of the airspace and the fourth dimension ( space + time ) make the configuration space c tremendous. the size of the configuration space has led most research to discretize the configuration space, either for the maneuvers or the airspace, and explore it with graph search or evolutionary algorithms, or by using heuristics to guide the exploration. meta-heuristics using discretization of maneuvers, such as genetic algorithms [cit] or ant colony algorithms [cit], along with artificial intelligence algorithm as neuronal networks [cit], give interesting results but they scale poorly. indeed, [cit] are one of the few that handle more than 20 airplanes and still find a global optimum."
"in this system, each airplane is an agent that decides by itself of its trajectory giving its local point of view. as shown in fig. 16, one airplane will only perceive a restricted number of airplanes (those in its perception zone described by fig. 4 ) per step of life cycle, and only partial information about its neighbors (as explained in sect. 3.2.1). this kind of decentralization brings more resilience to the system, and should allow to take into account non-cooperative obstacles in the scenarios, such as airplanes unable to avoid others or weather."
"the evaluated metrics (mean delay, standard deviation, first and third quartile) are presented in table 2 . the mean relative delays are acceptable. the standard deviation underlines the fact that the relative delay is not always equally distributed among airplanes. this is due to the implementation made for this experimentation in which airplanes (agents) decide iteratively; thus, the first-executed airplanes are more penalized than the others. finally, in our approach, agents return to their trajectory once the collision is avoided before going to their destination, while in most algorithms airplanes go directly to the destination. this can increase the relative delay caused to airplanes."
"gaussian quadrature method such as gauss legendre uses set of polynomial functions to evaluate the integration points and weights, and thus the method performs well when the integrand is a polynomial function. the method performs poorly when the integrand is different from polynomial function."
"with ⃖⃖⃖⃖ ⃗ p i,k (t) the vector position of m i if it does the action ac i,k, and ⃖⃖ ⃗ p j (t) the perceived position vector of m j . since the speed vector is considered as constant, d min,i,j,k is computed using:"
"the k-means algorithm is sensitive to outliers since an object with an extremely large value may substantially distort the distribution of data. how might the algorithm be modified to diminish such sensitivity? instead of taking the mean value of the objects in a cluster as a reference point, a medoid can be used, which is the most centrally located object in a cluster. thus the partitioning method can still be performed based on the principle of minimizing the sum of the dissimilarities between each object and its corresponding reference point. this forms the basis of the k-medoids method. the basic strategy of kmediods clustering algorithms is to find k clusters in n objects by first arbitrarily finding a representative object (the medoids) for each cluster. each remaining object is clustered with the medoid to which it is the most similar. k-medoids method uses representative objects as reference points instead of taking the mean value of the objects in each cluster. the algorithm takes the input parameter k, the number of clusters to be partitioned among a set of n objects. a typical kmediods algorithm for partitioning based on medoid or central objects is as follows:"
"to sort the criticalities, the algorithm first considers the collision criticality. the trajectory criticality is considered if the collision criticality is equal or if no collision is detected."
"airplanes fly from geographical points to geographical points, called waypoints. their trajectory from one airport to another is then reduced to a list of waypoints, called flightplan. experience shows that they do not always follow their flightplan [cit] for different reasons such as controller orders or weather. in our study, we consider that their predetermined trajectory p is approximated by a broken line."
"the generalized equation ( (14) it can be seen that the method shown in equation (14) requires handling of symbols to identify the variables x, y and their respective powers, m and n. the exact integration method is limited to polynomials functions."
"accuracy of three different quadrature rules -classical gauss quadrature, gauss legendre and generalized gaussian quadrature in performing numerical integration within two dimensional domains have been successfully compared through simulations. three different integration techniques have been demonstrated in this work, which are the numerical, semi-analytical and exact integration techniques. the integration limits were converted accordingly through utilization of the generalized equations developed by the author in previous work [cit] . it is seen that generalized gaussian quadrature performs better compared to other quadrature rules. semi-analytical method is recommended for simple integrals and when high accuracy is required, since the method requires high computational time and solution might not exist for complex integrals. the exact integration technique is applicable for fem based on stiffness matrices which consist of simple polynomials, such as virtual node method and strain based elements."
"the u and l in the equation (11) above represents upper and lower limits of the integration, which can be set as 0, 1, or -1, depending on the quadrature rule of choice."
"single integration given by equation (12) is carried out numerically using classical gauss legendre, generalized gaussian quadrature and 5 th order gaussian quadrature. equation (12) is converted to numerical form through the following relation:"
k: the number of clusters d: a data set containing n objects output: a set of k clusters that minimizes the sum of the dissimilarities of all the objects to their nearest medoid. method: arbitrarily choose k objects in d as the initial representative objects; repeat:
"the time taken for one execution of the program for the uniform distribution is less than the time taken for normal distribution. usually the time complexity varies from one processor to another processor, which depends on the speed and the type of the system. the partition based algorithms work well for finding spherical-shaped clusters in small to medium-sized data points. the advantage of the k-means algorithm is its favorable execution time. its drawback is that the user has to know in advance how many clusters are searched for. from the experimental results (by many execution of the programs), it is observed that k-means algorithm is efficient for smaller data sets and k-medoids algorithm seems to perform better for large data sets."
"another interesting point in this experiment is the emergence of the usual pattern called roundabout expected to avoid collision in such scenario. figures 13, 14 and 15, respectively, show the obtained trajectories for 6, 8 and 10 airplanes."
"to more thoroughly evaluate the performances of caamas, we intend to implement a standard global optimization method-commonly used for solving similar problems-in order to compare the difference in terms of computation time and result optimality."
"these criticalities are computed for every possible action ac i,k as shown in algorithm 2. the algorithm starts by evaluating the collision criticalities for each ac i,k regarding each m j in the perception zone, crit i,j,k,coll . the calculated criticalities are stored in a collision criticality list collcl ac i,k ordered by crit j asso-ciated with a ac i,k . then, the trajectory criticality if m i performs ac i,k, crit i,k,traj, is evaluated."
"the obtained results show that in most cases more than 88.4% of the collisions have been solved (more than 99.12% for 40 airplanes). still, detailed studies must be conducted in order to count the number of new collisions added by solving predicted collisions."
"the execution time of each run is calculated in milliseconds. the time taken for execution of the algorithm varies from one run to another run and also it differs from one computer to another computer. the result shows the normal distribution of 1000 data points around ten cluster centers. the number of data points is the size of the cluster. the size of the cluster is high for some clusters due to increase in the number of data points around a particular cluster center. for example, size of cluster 10 is 139, because of the distance between the data points of that particular cluster and the cluster centers is very less. since the data points are normally distributed, clusters vary in size with maximum data points in cluster 10 and minimum data points in cluster 8. for the same normal distribution of the input data points and for the uniform distribution, the algorithm is executed 10 times and the results are tabulated in the table 1 ."
"multi-agent systems (mas) are composed of different entities called agents. an agent is a physical or software entity, that is autonomous, evolves in an environment with perceive, decide and act abilities. the agent has a partial perception of the environment. as a result, agents do not share the same information, and do not take decisions knowing the global state of their environment. this results in a system that might make more adjustments, but is more scalable and more robust. an agent is able to communicate with other agents, has its own resources and capacities, and can offer services. an agent follows a life cycle composed of three steps, repeated until the agent achieves its local goal: perception, decision, action. during the perception, it acquires new information about its environment. it then decides the next action to perform. then, it realizes this action. the agents' execution can be done synchronously-all agents perform in parallel a life cycle before starting a new one-, asynchronously-agents perform their life cycles without waiting others-, or iteratively (used in the experimentation conducted in this paper)-the agents execute their life cycle one after the other; the order of the agents execution is being randomly changed (or not) at each step of the system execution. from the agents local interactions, with their environment or between themselves, a self-organization is established making the solution emerge. in some cases, an agent may be non-cooperative, which means it may bother other agents in their task, or it cannot fulfill its goal and thus cannot help the group at all. in the amas approach, the cooperation among agents interactions is the engine of the self-organization. the amas approach aims to create mas in which agents act cooperatively between themselves in order to maintain a cooperative behavior. the amas theory identifies seven generic non-cooperative situations [cit] among the three steps of the life cycle. in such situations, the agent has to decide cooperative actions in order to solve difficult situations (conflict, concurrence, ambiguity, etc.), based on the criticality of the agents."
"from table 1 (against the rows indicated by 'n'), it is observed that the execution time for run 6 is 7640 m sec and for run 8 is 7297 msec. in a normal distribution of the data points, there is much difference between the sizes of the clusters. for example, in run 7, cluster 2 has 167 points and cluster 5 has only 56 points. next, the uniformly distributed one thousand data points are taken as input. one of the executions is shown in fig. 2 . the number of clusters chosen by user is 10. the result of the algorithm for 10 different execution of the program is given in table 1 (against the rows indicated by 'u'). n 62 116 106 108 117 96 109 53 94 139 7609 u 118 87 101 109 103 91 104 100 94 93 7281 run 2 n 149 108 60 123 90 132 99 74 81 84 7500 u 101 98 111 93 96 99 79 125 106 92 7500 run 3 n 79 108 134 85 89 95 78 72 125 135 7578 u 103 86 98 87 77 109 108 91 121 120 7469 run 4 n 54 102 88 83 86 131 113 129 101 113 7391 u 84 117 90 98 106 97 85 111 110 102 7375 run 5 n 78 55 143 81 79 101 157 91 123 92"
"nevertheless, despite the encouraging obtained results concerning the remaining collision and the relative delays, . 13 near-optimal solution for 6 airplanes caamas is unable to solve all conflicts for overloaded scenarios. experiments with additional possible actions for airplanes will be conducted. a further study of the impact of the density of the scenario must be done."
"where i represents the iteration number, n represents the polynomial degree, and r represents the r th root of the polynomial. once integration points x i are determined, the weights can be calculated using the relation:"
"contrary to a clear majority of motion planning problems, air traffic management does not rely on finding a complicated trajectory to satisfy one aircraft. airspace is rarely cluttered by obstacles, except for the weather; thus, finding a trajectory is quite straightforward. the difficulty of motion planning for aircraft resides in finding a feasible trajectory for each aircraft (i.e. respecting the capabilities of the aircraft), collision free with the others, globally optimal (i.e., optimal for all the airplanes), and resilient to changes and uncertainties, all this in a wide configuration space [cit] ."
"in the next example, one thousand normally distributed data points are taken as input. number of clusters chosen by the user is 10. the output of one of the trials is shown in fig. 3 . next, the uniformly distributed one thousand data points are taken as input. the number of clusters chosen by the user is 10. one of the executions is shown in fig. 4 . the result of the algorithm for ten different executions of the program is given in table 2 . for both the algorithms, k-means and k-medoids, the program is executed many times and the results are analyzed based on the number of data points and the number of clusters. the behavior of the algorithms is analyzed based on observations. the performance of the algorithms have also been analyzed for several executions by considering different data points (for which the results are not shown) as input (250 data points, 500 data points etc), the outcomes are found to be highly satisfactory."
"the proposed collision avoidance system is a fully decentralized distributed approach based on adaptive multi-agent technology. we have shown its relevance with several criteria: efficiency of management even for dense traffic, limited amount of communication between airplanes, and small computation time."
"(a) predicted and remaining collisions for each randomly generated scenario, we compute the number of predicted collisions ( fig. 9 ), i.e., the number of collisions that would occur if no change is made on the trajectories of the airplanes. we compare this number to the remaining collisions ( fig. 10 ) that may be new collisions created by the system while avoiding others, or the same old collisions."
"eventually, caamas could be used as a support decision system for air traffic controllers because it is able to work over different scales of time and space. this would require comprehensive experiments with data obtained from real air traffic."
"from the result, it can be inferred that the size of the clusters are near to the average data points in the uniform distribution. this can easily be observed that for 1000 points and 10 clusters, the average is 100. but this is not true in the case of normal distribution. it is easy to identify from the fig. 1 and 2, the size of the clusters are different. this shows that the behavior of the algorithm is different for both types of distributions."
"we introduce in this section our decentralized collision avoidance adaptive multi-agent system (caamas) approach, for collision avoidance management for multiple heterogeneous mobile entities with dynamic constraints. in our model, every m i is represented by an agent following the life cycle described in algorithm 1."
"the evaluation of the collision criticality is based on an extrapolation called nominal projection in the literature [cit] . it is a simple extrapolation of position and velocity vectors. the idea is to consider that the situation of a mobile entity m j will evolve in the same way it does currently (same speed and same direction) and thus to calculate the criticality of the possible occurrence of a collision if m i realizes ac i,k ."
"airplanes are increasingly capable of communicating data to each other such as their positions, heading or speed by means of messages. messages can be transmitted using automatic dependant surveillance-broadcast (ads-b)."
"problem specification: in this research, two unsupervised clustering methods, namely k-means and k-medoids are examined to analyze based on the distance between the various input data points. the clusters are formed according to the distance between data points and cluster centers are formed for each cluster. the implementation plan will be in two parts, one in normal distribution and another in uniform distribution of input data points. both the algorithms are implemented by using java language. the number of clusters is chosen by the user. the data points in each cluster are displayed by different colors and the execution time is calculated in milliseconds."
"moreover, since caamas is naturally distributed and decentralized, it could eventually be implemented on board, removing the need to rely on ground equipment. as it does not rely on a computed plan, we can assert that our method is fully adaptive when facing unexpected events. for example, an airplane could become highly critical, due to a loss of an engine, and others would take this new event and this new criticality into account when deciding their trajectories."
"potential fields are also expensively studied, starting from the standard method, to its extension with navigation functions [cit], the usage of more complex potential fields [cit], or the combination between potential fields and swarming [cit] . those methods have the particularity of providing a proof of convergence. however, those methods need to be tuned carefully to be effective. finding generic rules to do so seems quite difficult."
where n represents the polynomial degree. the integration points are the roots for the legendre polynomials and can be determined using numerical technique such as newtonraphson;
"for different input data points, the algorithm gives different types of outputs. the input data points are generated in red color and the output of the algorithm is displayed in different colors as shown in fig. 1 . the center point of each cluster is displayed in green color."
"as for semi-analytical integration technique, the classical gauss legendre and 5 th order gaussian quadrature are able to solve fractional functions. this is caused by the analytical integration of the first variable. accuracy of the results obtained from this technique has slightly improved compared to the numerical integration technique. however, this technique would require more computational time compared to the numerical integration technique, due to the symbolic manipulation."
"in today's air traffic management, airspace is divided into several zones each under the supervision of air controllers. in order to help air controllers to manage real-time traffic and avoid collisions, the traffic is regulated upstream. with the increase in traffic and the insertion of new aerial vehicles such as drones into the airspace, with different flight performances, air traffic control must evolve by increasing the level of automation and introducing partial delegation of the control to on-board systems."
"once the action ac i,k to perform is decided, the agent determines its criticality as action phase the action part for the agent is straightforward. m i does the action decided by algorithm 3, and sends messages containing its current situation and its criticality to the mobile entities inside its perception zone."
"k-means algorithm: k-means is one of the simplest unsupervised learning algorithms that solve the well known clustering problem. the procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters) fixed a priori. the main idea is to define k centroids, one for each cluster. these centroids should be placed in a cunning way because of different location causes different result. so, the better choice is to place them as much as possible far away from each other. the next step is to take each point belonging to a given data set and associate it to the nearest centroid. when no point is pending, the first step is completed and an early group is done. at this point, it is need to re-calculate k new centroids as centers of the clusters resulting from the previous step. after these k new centroids, a new binding has to be done between the same data points and the nearest new centroid. a loop has been generated. as a result of this loop it may notice that the k centroids change their location step by step until no more changes are done. in other words centroids do not move any more. finally, this algorithm aims at minimizing an objective function, in this case a squared error function. the objective function: x and the cluster centre c j, is an indicator of the distance of the n data points from their respective cluster centers. the algorithm is composed of the following steps:"
"in the case of the collision avoidance problem, representing each airplane by an autonomous cooperative agent with local interactions brings a natural decentralized solution to the complexity of the global problem. the system we propose can be used for several issues:"
"in this part, we start by introducing the adaptive multi-agent systems (amas) theory, and then, we present our general approach, called caamas for collision avoidance adaptive multi-agent system."
"where ω represents domain, b represents strain displacement matrix and d represents matrix property matrix. the stiffness matrix, k is later used to determine response of a material to external loads, such as the nodal displacements, strain and stress components. apart from solid mechanics, similar integration is also encountered in other engineering applications (which are analyzed using fem) such as in thermal problems, fluid flow problems and etc. thus, there have been many integration techniques developed and proposed to perform the integration. most common technique used in fem to perform the integration is by using numerical integration technique. numerical integration is preferred, since it yields good results at lower computational time compared to analytical method. analytical solutions give accurate results, but require high computational time. furthermore, analytical solutions might not exist for certain cases."
"from the experimental results, for k-means algorithm, the average time for clustering the normal distribution of data points is found to be 7476.5 m sec. the average time for clustering the uniform distribution of data points is found to be 7404.8 m sec. for the k-medoids algorithm, the average time for clustering the normal distribution of data points is found to be 7442.3 m sec and for the average time for clustering the uniform distribution of data points are 7307.8 m sec. it is understood that the average time for normal distribution is greater than the average time for the uniform distribution. this is true for both the algorithms k-means and k-medoids. if the number of data points is less, then the k-means algorithm takes lesser execution time. but when the data points are increased to maximum the k-means algorithm takes maximum time and the k-medoids algorithm performs reasonably better than the k-means algorithm. the characteristic feature of this algorithm is that it requires the distance between every pair of objects only once and uses this distance at every stage of iteration."
"geometrical approaches have also been studied. the idea is to detect collisions using velocity vectors, compute the minimal velocity vector change to avoid the collision and divide equally the minimal velocity vector change among the mobile entities [cit] . one algorithm in particular has been successfully applied to multi-robot [cit], and some adaptations for aircraft have been made [cit] . most of them have not been tested in dense situations. the last ones however are interesting in the way they decentralize the problem among the different entities and give interesting results with dense situation as long as the constraints on maneuvers are light."
"mixed-integer linear programming (milp) solvers are studied as well, in particular for solving a minimum weight maximum clique model [cit] . results are interesting, but they use important instantaneous heading or speed changes. constraint programming has also shown some interesting results [cit], finding solutions proved to be optimal, but also scale badly with the number of airplanes."
"for the slight scenarios (10 and 20 airplanes) presented in table 1 caamas solves all the collisions, while the comparative algorithm is not able to solve some of them. when the number of airplanes increases too much (from 100 to 120), the number of collisions increases significantly for both, but [cit] do not give delays since the algorithm cannot bring any aircraft to its destination. the last scenario with 120 airplanes could be considered as very overloaded because the collision number increases significantly. the max delay is quite high (from 50 to 140%) compared to the durand et. al. [cit] or reality, and represent isolated aircraft. nevertheless, the mean flight delay is not so high (10%)."
"this produces a separation of the objects into groups from which the metric to be minimized can be calculated. although it can be proved that the procedure will always terminate, the k-means algorithm does not necessarily find the most optimal configuration, corresponding to the global objective function minimum. the algorithm is also significantly sensitive to the initial randomly selected cluster centers. the kmeans algorithm can be run multiple times to reduce this effect. k-means is a simple algorithm that has been adapted to many problem domains. first, the algorithm randomly selects k of the objects. each selected object represents a single cluster and because in this case only one object is in the cluster, this object represents the mean or center of the cluster."
"we also plan to test our algorithm when some communications are lost, and adding non-cooperative obstacles in the scenarios, such as airplanes or weather, to prove it resilience and to experiment the benefits of having a percentage of cooperative airplane in the traffic. testing other means of perception, such as radar, would also be an interesting case study."
"clustering can be considered the most important unsupervised learning problem; so, as every other problem of this kind, it deals with finding a structure in a collection of unlabeled data [cit] . a loose definition of clustering could be \"the process of organizing objects into groups whose members are similar in some way\". a cluster is therefore a collection of objects which are \"similar\" between them and are \"dissimilar\" to the objects belonging to other clusters. unlike classification, in which objects are assigned to predefined classes, clustering does not have any predefined classes. the main advantage of clustering is that interesting patterns and structures can be found directly from very large data sets with little or none of the background knowledge. the cluster results are subjective and implementation dependent. the quality of a clustering method depends on:"
"in this section, separated results for the random and the roundabout experiments are presented. the tests were performed on a computer equipped with a 2.50 ghz i7-6500 processor and 8 gb of ram. we implemented the algorithms in java 8. due to the decentralized and distributed nature of multi-agent systems that introduce some that, for the benchmark with 120 airplanes, the algorithm takes less than 12 s to compute the solution. in average, it takes 212 ms, 418 ms, 1305 [cit] ms, 2826 ms, 5072 ms, 7660 ms and 11891 ms to compute the algorithm with, respectively, 12, 20, 40, 52, 60, 80, 100, 120 airplanes. note that caamas can be deployed on a distributed computation network which can drastically reduce the computation time."
"finally, the exact integration technique yields accurate solutions at lower computational time compared to the analytical solutions. however, this technique is applicable for monomials only. the exact integration technique is suitable for fem based on stiffness matrices which consist of simple polynomials, such as virtual node method and strain based elements. this technique provides an alternative for existing numerical integration techniques, especially gaussian quadrature which requires high number of integration points and weights to integrate higher order monomials."
"if several actions can be done to help most critical agents or no collision is detected, the agent decision is based on the trajectory criticality as defined above."
"note that, the two parameters d coll and t tr were decided with atm experts to underline the nature (critical or not) of a situation."
"integration of a function over a certain domain is frequently encountered in engineering computations. finite element method (fem) is one of the computational methods which require such integration technique to obtain the stiffness matrix, k according to the formula below (for solid mechanics):"
"we propose to address the collision avoidance management problem using a decentralized distributed approach: the adaptive multi-agent system (amas) theory. it aims at solving problems in dynamic environments by a bottomup design of autonomous agents, where cooperation is the driver of the self-organization process [cit] . amas has been successfully used to solve different problems, such as anomaly detection in maritime environment [cit], control and optimization of heat-engine [cit], or context learning [cit] ."
"the remainder of this paper is structured as follows. section 2 briefly reviews the related work. section 3 presents our general approach to collision avoidance. section 4 describes its application to atc, the experiments and the results. finally, sect. 5 summarizes our findings and concludes this study."
"where c denotes the class of the identity, and mi(.) [cit] denotes the function to compute the mutual information of the input pair of matrix."
"from the experimental results on lfw, ytf, and cacd, we can observe the availability and reliability of the proposed loss function. by integrating the shared features from the input image pairs to the proposed cnn model (the non-occluded face and occluded face) and unique information from each input facial image, the presented loss function could provide the constraint from a pair of input facial images on the original softmax loss and have shown its performance in the face verification tasks. meanwhile, similar to human being's visual system, our proposed approach can extract the global and local features of the images of faces. the global features combined with local features form a layout for each identity."
"face recognition has become the primary biometric technique used for personal authentication and identification in various fields, including finance [cit], public security [cit], and education [cit] s and could derive the low-dimensional representation under specific priors on the features in the facial images. however, these techniques would result in limited performance while the presented assumptions might not be compatible with the practical scenarios."
"recently, the angular loss and its different modifications were proposed. instead of the employment of euclidean space, the angular loss functions could realize the separation of the output features with angular distance."
"in general, the previously proposed deep face recognition approaches differ from each other in at least one of the following aspects: the network architecture and the loss function."
"most of the previously proposed cnns have been exploited to implement the face recognition applications. according to the input channels used in the network architectures, these cnn networks could be roughly categorized into two types as follows."
"as shown in table 2, the proposed approach outperforms the state-of-the-art face verification methods, while the number of images used in the training set to train our model is less than most of the other techniques like facenet, which exploits 200 millions of images in its training process."
"to evaluate the performance of the proposed method, we conducted comparison experiments on several publicly available facial benchmarks between state-of-the-art approaches and ours. experimental results demonstrate that our mutual information constrained cnn framework learns occlusion-invariant representation and outperforms the state-of-the-art face verification techniques."
"where conv denotes the feature extraction function with the proposed cnn, x i is the corresponding output feature, and θ denotes the feature maps in the cnn architecture that need to learn in the training phase. one fundamental prior used in the proposed method is that there should be shared component between the nonoccluded image and its occluded counterpart. accordingly, we introduce three different matrices (u, v p, and v q ) to represent the shared information of the features and the unique feature, which can be formulated as follows."
and the initial value of the θ could be obtained from the trained cnn model; u and v i are initialized with random feature maps.
"where ux i denotes the shared feature, and vx i denotes the unique feature. due to the characteristics of mutual information in the shared feature and unique feature, we impose the mutual information as a regularization term on the commonly used softmax loss function, which can be formulated as follows."
two parameter-sharing channels (as shown in fig. 1 ) are exploited to process the non-occluded and occluded faces. and the output feature layer is exploited to obtain both the shared features from the input face pair and the unique feature from each input single image. the details for each cnn channel are listed as follows.
"as shown in table 3, the proposed approach outperforms most of the state-of-the-art face verification methods except the vgg-face [cit] . the entries in the \"images\" column represent the number used to train the face verification methods. the 'time\" column represents the execution time for single input image"
"the proposed cnn architecture and the corresponding parameters are firstly trained on the publicly available facial dataset lfw [cit] and vgg face [cit] . then, the initialized model is fine-tuned with the manually collected 6178 images including the non-occluded faces and the corresponding occluded images. as shown in fig. 1, for each input facial image, 8 convolutional layers with corresponding maxout operators and 3 fully connected layer were utilized."
"alexnet [cit] and has shown its great performance in different machine vision systems. it contains five convolutional layers combined with rectified linear unit (relu), dropout operator, and three fully connected layers."
"to assess the performance of the proposed face verification technique, we conducted the comparison experiments between the state-of-the-art methods (including deepid3 [cit], l 2 softmax [cit] 48m) ) and ours on three publicly available datasets. following the protocol \"unrestricted with labeled outside data, \" we firstly performed the experiments on 5000 pairs of facial frames in ytf, and the experimental results are shown in table 3 ."
"on the one hand, data imbalance in the prevailing facial datasets should be one possible explanation for this phenomenon. despite most of the face recognition training datasets contain large amount of identities, they still suffer from the deficiency of difficult facial images with partial occlusions such as sunglasses, hats, and hairs. an intuitive solution to this problem is that more occluded facial images should be included into the training process of the cnn framework."
"the rest of this paper is organized as follows. firstly, we briefly reviewed the related work on face verification methods in section 2. then, in section 3, we describe both the materials that we used and the details of the proposed approach. section 4 presents the experimental results and the discussion. finally, we draw the conclusion and presents our future work in section 5."
"extensive experiments were conducted to assess the performance of our proposed face verification method on several publicly available face recognition benchmarks, including lfw [cit], youtube faces (ytf) [cit], and cross-age celebrity dataset (cacd) [cit] . both the experimental results and the analysis are demonstrated in this section."
"to assess the performance of the proposed face verification technique, we conducted the comparison experiments between the state-of-the-art methods (high-dimensional lbp [cit], hidden factor analysis [cit], lf-cnn [cit], center loss [cit], and marginal loss [cit] ) and ours on three publicly available datasets. following the protocol \"unrestricted with labeled outside data, \" we firstly performed the experiments on 4000 pairs of facial frames with different types of occlusion in cacd, and the experimental results are shown in table 4 . notably, there are only several methods have reported their performance on cacd."
"to implement the partially occluded face verification, we propose a deep learning strategy-based two-channel cnn architecture and a newly presented loss function. in the proposed cnn architecture, two parameter-sharing cnn channels are exploited to respectively process a pair of face images: the non-occluded facial image and occluded facial image. at the end of the network, both the shared feature and the unique feature could be obtained in a feature layer. the mutual information regularized softmax loss is iteratively optimized through the alternating minimization algorithm. to evaluate the performance of the proposed approach, we conducted comparison experiments between the state-of-the-art methods and ours on several publicly available face image datasets. experimental results show that the proposed approach outperforms the state-of-the-art methods in accuracy. this paper offers several contributions. first of all, a novel deep cnn is proposed to implement the face verification task. secondly, this is probably the first attempt to introduce the novel loss function in the cnn architecture. meanwhile, it is also an early application of the shared information between the non-occluded image and occluded image into the same cnn model. thirdly, our approach performs with superiority to the state-of-the-art face verification techniques."
"in our future works, we will continue to implement more applications [cit] of the presented cnn architecture. for instance, we would use more practical images, e.g., the blurry images, and evaluate the accuracy of the proposed cnn on these images. to achieve this objective, we will continue to collect more face images and create a publicly available dataset."
"benefiting from the success of cnn in recent years, face verification has obtained significant progress. in this section, we propose the cnn-based approach and the novel loss function. since the structure of cnn has been presented in a great deal of studies [cit], we focused on the network architecture of the proposed cnn."
"on the other hand, the loss function could also significantly affect the training of cnn-based face verification systems and lead to poor performance while it might be biased to the data distribution. for instance, softmax loss, which was not specifically designed for complicated samples, would neglect the faces with occlusion by enlarging the conditional probability of the entire samples. to address this issue, numerous loss functions and different constraints on the traditional loss functions have been presented [9, [cit] ."
"to evaluate the performance of the proposed face verification technique, we conducted the comparison experiments between the state-of-the-art methods (including deepid3 [cit], l 2 softmax [cit] 48m) ) and ours on three publicly available datasets. following the protocol \"unrestricted with labeled outside data, \" we firstly performed the experiments on 6000 pairs of facial images in lfw, and the experimental results are shown in table 2 ."
"with p( ) the prior distribution function for which correlates to a set of constraints for the parameter vector. this allows further utilizing the a priori information regarding the rotation ranges of the joints. to achieve the human body pose estimates, the parameter vector est will be estimated by minimizing the negative logarithm of the posterior probability function using an unconstrained non-linear minimization function:"
"with r a 3d-rotation matrix given by the body parts' parent rotation defined by α, β and γ and a 3d-translation vector v defined by the size of the body part d. in the example of p w kr the point in world coordinates will be defined by the joint of the node p w hr, the combined rotations of the pelvis and upper right leg (which corresponds to the product of the matrices r α,β,γ b and r α,β,γ ur ) and the translation vectors v ur . this vector equals to a negative displacement d ur in the zaxis that corresponds to the actual length of the upper right leg. this set of equations is the same for the left portion of the body and can be expanded for the other body parts."
"as a third sensor modality a bosch bno055 [cit] imu sensor was used in this hardware architecture. it can also be found in other rehabilitation and binaural hearing research [cit] . this specific imu integrates a triaxial gyroscope, accelerometer and magnetometer. it also features a small on-board microcontroller that can fuse this specific sensor data into absolute orientations (e.g. quaternions or euler angles) using recursive bayesian filtering techniques. due to its ease of use, high-accuracy and data fusion capabilities this sensor can provide absolute orientation information of the sensor nodes at an increased data rate. because of its intrinsic operation it can also alleviate non-line-ofsight (nlos) situations of the optical and acoustic sensors that either yield no or inaccurate sensor data."
"in order to achieve the proposed low-cost wireless body sensor network nodes, a hardware topology was defined that offers sufficient capabilities for ongoing and future research purposes. in the design, we took into account the deficiencies of earlier measurement hardware we developed."
"the chosen hardware topology has to meet a number of prerequisites. in the first place, the overall system cost should remain low, i.e. approximately e 1.000 for a wbsn that can provide full body pose estimates. secondly, since the wireless sensor nodes need to be attached to various parts of a human body these devices need to be relatively small and light. thirdly, every node needs to be able to function autonomously. in this case, this third prerequisite implies that the node should function as a stand-alone member of the wireless network as well as function without any (extra) cables for power or data transfers to ensure the test subject's normal movement ranges."
"although we have established a simulation environment that allows us to produce realistic sensor data in combination with the measured noise distributions, experiments where a ground-truth reference can also be recorded are needed for benchmarking purposes and further validation of the developed algorithms. this will give us a better insight into the real accuracy and precision of our proposed human body pose estimation system. we believe this can be achieved using our qualisys motion capture system in combination with active ir led markers, instead of passive markers, placed at known positions on the various wbsn sensor nodes. by using these active markers, the ir emissions of the qualisys cameras can be disabled thus allowing us to capture the htc vive ir sweeps and pulses to obtain the wbsn lighthouse sensor data and simultaneously track the position and orientation of the sensor nodes with the qualisys system. [cit], he became a tenure track professor at the constrained systems lab, university of antwerp, where he researches sensors, sensor arrays, and signal processing algorithms using an embedded, constrained systems approach. he pursued industrial exploitation of the patented 3-d array sonar sensor that was developed in collaboration during his ph.d. degree. volume 7, 2019"
"the best known method for establishing motion capture is vision based. vision-based systems make use of specific passive or active markers that can be attached to the person or object combined with specialized cameras (e.g. vicon or qualisys) [cit] . the downside is that the cost of these systems can range from e 20.000 to e 300.000. markerless vision-based pose estimation systems using ''standard'' rgb cameras are also showing promising results when combined with machine-learning algorithms such as a deep neural networks [cit] or a convolutional neural networks [cit] . another method for motion capture applications is a bsn of nodes that are equipped with inertial measurement units (imus) [cit] and often a wireless radio. these nodes can be strapped to the various body parts or can be integrated into a specialized body suit (e.g. xsens) [cit] . since imu devices inherently suffer from integration errors [cit], these type of sensors cannot be used to achieve high-accuracy and high-precision pose estimations without making certain assumption regarding human motion patterns that compensate for these errors. therefore, using this setup in rehabilitation scenarios where normal gait patterns cannot be expected, is not feasible."
"body pose, the joint parameter vector for estimating the same articulated lower human body with an equal amount of sensor nodes would consist of 30 parameters (6dof * 5 nodes)."
"the associate editor coordinating the review of this manuscript and approving it for publication was pietro savazzi. sleeping patterns. besides these devices, targeted at ambitious yet amateur athletes, the professional sporting sector has also taken its interest in this technology. the aforementioned devices all have their professional counterparts that feature either more accurate and precise measurements, have more features or both. gathering correct information has become crucial for training efficiently. a factor that takes a great part in this is the athletes' form or technique [cit] . in order to objectively quantify the relevant technical performance metrics, motion capture technology is being used to further refine this. rehabilitation physicians and physiotherapists are currently also using motion capture technologies for full body pose estimation purposes [cit] . by using full or partial body pose estimation to objectively quantify the gait of a rehabilitating patient, the rehabilitation process can be personalized and adjusted in a timely manner [cit] . this can significantly reduce the recovery time of the patient, hence shortening the time spent in a hospital or rehabilitation center, and potentially reduce costs for all involved parties."
"modalities of the wbsn nodes, a three-element infrared photodiode array was chosen in order to capture infrared pulses and sweeps emitted by two htc vive base stations. by accurately timing the rising and falling edges of these infrared emissions, the azimuth and elevation relative to the emitting base station can be calculated for every photodiode. this sensor modality was chosen because of the demonstrated efficacy for estimating 6dof-poses of sensor nodes in our previous work [cit], the low-cost of the receiver hardware and the off-the-shelve available htc vive base stations."
"these microsecond timestamps are appended to every data point that is received from the stm32f4 before transmitting the data wirelessly through a tcp connection to a local server which collects all of the data. when we look into the collected data, it is clear that it is very unlikely to have data points between the different wbsn nodes with exactly the same timestamp. due to our implementation where the nodes will sequentially initiate a connection to the tcp server, the first and/or last data point of the nodes will not be perfectly aligned either. by using linear interpolation on the collected data of every individual node between a common start and end timestamp, a synchronous data set is created with a fixed 100-ms interval."
"as can be seen in figure 5, the ground-truth poses and estimated poses differ only slightly even when we add more sensor noise to the simulated measurements than we have established through our sensor characterization. on the one hand, the noise rejection of our proposed method can be attributed to the use of the parametric human body model (phbm) which constrains the solution space due to the kinematic chain. on the other hand, the proposed model also limits the rotation ranges of the joints. some rotation ranges can even be discarded thus reducing the parameter vector to 14 parameters for an articulated lower human body equipped with 5 wbsn nodes. in previous work where the 6dof pose for every individual node (in) was estimated to establish a figure 5. visual representation of one of the simulations that were used to verify the proposed algorithms and models. in this simulation a lower human body was outfitted with 5 wbsn nodes, the former and the latter were described in the human body model by defining its parameters and sensor placement. the simulated test subject walked from the back to the front of the defined environment, in which two lighthouses were positioned. both the estimated pose as the ground-truth pose are drawn in this representation."
"in order to quantify the efficacy of using either parameter vector phbm (consisting of 14 parameters) or in (consisting of 30 parameters), the average position and orientation errors were compared with added cumulative sensor noise. whereas the individual node approach does not benefit from the rotational constraints nor a kinematic chain, the same probabilistic pose estimation algorithm was used to approximate the human body pose. using the aforementioned simulation environment, these two approaches could be tested of which the results are shown in figure 6 where panel a) shows the average position error and b) the average orientation error. these results clearly show that a human body model with a kinematic chain approach, which reduces the parameter vector, has an overall much lower average error on both figure 7. in a) our test subject was equipped with five wbsn nodes to the upper legs, lower legs and torso. based on the node placement on the body and on the physical properties of the test subject, an estimated body pose was generated that is shown as an overlay. panel b) shows the same estimated pose of the test subject by itself from a different viewpoint. panel c) shows five snapshots of body pose estimates based on a recorded motion pattern. the first three snapshots illustrate the poses during a normal gait pattern in a straight line while the fourth and fifth snapshot display the test subject lifting its right and left leg. while a ground-truth reference could not be recorded at this time, the poses can be easily distinguished."
"the notion of body sensor networks (bsn) might still seem like a futuristic science fiction concept, the reality however is that these types of devices are becoming more ubiquitous nowadays. smartphones equipped with a wireless radio (e.g. bluetooth) can easily connect to a great number of wireless or other so called ''smart'' peripherals. the medical sector is adopting this technology for closely and continuously monitoring some of the vital body functions in a less intrusive manner [cit] . more commonly known examples of these bsns have spurred a booming market of health, sports and fitness accessories e.g. wireless heart rate monitors, pedometers, stride length monitors, cadence meters, smart or sports watches that enable the user to monitor his/her physical health, sporting performance progression and"
"next, the position and orientation of the htc vive base stations, lh a and lh b, are defined in the world coordinate system. this information is necessary since the actual measured lighthouse sensor data is relative to either lh a or lh b . these points are defined by p w lh, which describes the 3d-position (x, y and z ), and a 3d-rotation (α, β and γ ) that can be used to construct a rotation matrix r . as a last step (to translate the sensor coordinate data into lighthouse azimuth and elevation data) every column of either matrix ss r lh s, containing cartesian coordinates, can be converted to its spherical counterpart of which the range is discarded. as mentioned before, the htc vive base station's architecture deviates from the traditional spherical coordinate system definition. the htc vive lighthouse azimuth θ lh and elevation ψ lh data is more closely approximated using the following equations that make use of the four-quadrant inverse tangent function (tan −1 ):"
"since we have a priori information concerning which rotations can be expected for every joint, a number of parameters can be discarded resulting in a 14-element vector :"
"to support the autonomy of the wbsn nodes a trade-off has to be made in processing power and power consumption while maintaining a compact and viable solution. there is a great variety of micro-controllers and processors that are available nowadays ranging from tiny integrated circuits (ics) that target ultra low-power applications to so called system-in-package (sip) ics that basically embed an entire unix system. somewhere in between these two extremes resides the arm cortex m4 series microcontrollers that incorporate numerous interfaces and hardware capabilities at a reasonable power consumption. within the range of these m4-devices a great variety exists in features, clock speeds, pin-count, physical size and price. to check off all of the aforementioned prerequisites, an stm32f429 microcontroller from stmicroelectronics [cit] was chosen as the core of the wbsn nodes. the main purposes for this microcontroller are sensor data acquisition, timekeeping and performing data transfers. although the internal memory provides 128 kb of data storage, an extra external ram ic of 8 mb was chosen. using the on-board flexible memory controller, the external memory can be read or written just like the internal memory with only slightly slower transfer speeds. given the size of this external memory, a large amount of data can be buffered in worst case scenarios were data transfers to other computers might not be feasible in real-time. figure 1 illustrates how the various components of the hardware architecture are connected to each other. as one of the sensor figure 1. panel a) shows the schematic overview of the wbsn node that uses an stmicroelectronics stm32f4 microcontroller at its core. this arm cortex m4 microcontroller will perform the data acquisition of the htc vive base station infrared pulses and sweeps, of the knowles ultrasound microphones and of the bosch bno055 inertial imu. the data can be stored in an external 8 mb ram chip that serves as a data buffer before transferring the data to a central server through the espressif esp8266 wi-fi soc or wired interface provided by the ftdi ft2232h ic. the latter converts a usb data connection to two uart interfaces that can be used for data transfers and reprogramming both microcontrollers. the usb connection also provides power to the system through the voltage regulator and the microchip mcp73833t lipo charge controller ic that in turn safely charges a connected single-cell lipo battery. panel b) shows the front side of the wbsn node on which the photodiode sensor elements are marked. the pcb is mounted on the 3d-printed enclosure that incorporates the 1400 ma h lipo cell providing an autonomy of approximately 10 hours. the combination of the pcb and enclosure measures 6.8 by 7.2 by 1.85 cm and can be attached to objects or body parts. panel c) shows the back side of the wbsn node where the digital microphones are marked together with the triaxial gyroscope, accelerometer and magnetometer imu sensor which aligns to the center of the pcb."
this pose estimation technique is validated through simulation of which the results are shown in section iv. the results of the experiments that were performed with the combination of the aforementioned technology and techniques are presented in section v. in the final section we will discuss our conclusions for the proposed solution and future work. additionally new functionalities that we want to explore in the future were also included.
"despite the benefit of having wireless data acquisition capabilities, the problem of inter-node synchronization arises. the synchronization between the different nodes is crucial for the intended use-case of human body pose estimation. unsynchronized data can result in body pose estimations where individual body parts appear to be lagging behind. this might even entirely disrupt the estimation technique. in order to alleviate this issue every individual wbsn node will request a network time protocol (ntp) timestamp from one of the available ntp servers. within the specification of the ntp protocol it is stated that these timestamps are represented as a 64-bit fixed-point number, in seconds relative to 0000 ut on 1 [cit] of which there is an integer part and a fractional part [cit] . this fractional part brings about a free-running 1-ms clock on the server side that can be polled. once the ntp timestamp is received, the internal clock of the esp8266 will maintain the time with microsecond precision only to refresh its ntp timestamp at larger intervals."
"by introducing a new hardware architecture based on a wbsn topology we have created a novel approach to human body pose estimation in a low-cost yet accurate and precise fashion. the basis for our approach is powerful yet powerefficient hardware that combines three sensor modalities. these sensors (optical, acoustic and inertial) can be used to complement one another to overcome their inherent flaws, e.g. non-line-of-sight, integration drift, etc. by applying sensor fusion techniques. in this paper, we limited ourselves to only using a single modality, i.e. the optical detection of emitted ir pulses and laser plane sweeps. through its scalable architecture and wireless data transfer capabilities combined with ntp microsecond timestamps, which allows inter-node synchronization, a significant step was been taken towards attaining full body pose measurements and estimations. as a future addition to fully utilize and optimize the captured lighthouse data, the synchronization pulses can be parsed to gain additional information of the htc vive base stations. this information contains e.g. device specific calibration offsets of the horizontal and vertical rotors."
"in order to wirelessly transfer the acquired sensor data to a central processing hub, the ubiquitous ieee 802.11 wireless radio standard was chosen, commonly known as wi-fi. the one-to-many scalable network topology, high data throughput rate, possible internet access and off-the-shelve low-cost router hardware (if required) fits the envisioned system architecture. to create a wi-fi client out of a wbsn node, a low-cost and low-power espressif esp8266 [cit] wi-fi system-on-chip (soc), which runs a full tcp/ip stack, was used together with an inverted-f pcb trace antenna tuned to 2.4 ghz. since this soc features a programmable microcontroller its operation can be tailored to cooperate with the stm32f4 in a joint state machine. besides a wireless connection, a wired usb connection was also provided which serves multiple purposes. firstly, the usb data is routed to an ftdi ft2232h ic which creates two universal asynchronous receiver-transmitter (uart) interfaces that connect to both the esp8266 and the stm32f4 microcontrollers for either data transfers or reprogramming purposes. secondly, the 5 v that is provided through the usb connection will provide power to the entire system and is used by the microchip mcp73833t lithium polymer (lipo) charge controller ic to safely charge single-cell lipo batteries. the proposed system uses a 1400 ma h lipo cell to provide power to the wbsn node for approximately 10 hours of continuous operation. this satisfies the autonomous measurement prerequisite or other use-cases where a usb connection is not available or feasible. in figure 1 panel b) the actual assembled pcb is shown mounted on a 3d-printed enclosure that incorporates the lipo battery. it can be strapped to an object or body part. the pcb fits all of the aforementioned ics in a 5 by 5 cm rectangle. in combination with the volume 7, 2019 figure 2. panel a) illustrates two htc vive base stations (lh a and lh b placed in an environment together with a wsbn node that is equipped with three ir sensitive photodiodes that will receive the ir pulses emitted by lh a and lh b . b) shows three discretized photodiode receiver signals that exhibit two synchronization pulses, which are received at the same time, together with a narrow pulse that corresponds to the either the horizontal or vertical ir laser plane sweep that passes over the photodiode."
"in section ii, the hardware of the individual nodes will be presented, together with the sensor measurement and synchronization scheme. in section iii the probabilistic approach for estimating the actual pose of the individual nodes will be discussed together with the human body model that combines the individual node estimates into a body pose estimation."
"this vector can be used to generate a sensor system pose, and consequently the sensor azimuth and elevation data m c lh ( ) that is the input for our probabilistic pose estimation algorithm. this algorithm will minimize the difference between the measured lighthouse azimuth and elevation data"
"as mentioned in the previous subsection, the wbsn nodes are equipped with three sensor modalities. each sensor technology has its inherent advantages and disadvantages but can be combined to complement each other. however in this work, we will focus on a single sensor modality that will be used as the primordial source of data for establishing human body pose estimates. other modalities will be added later in an iterative design process. given the positive results of our previous work [cit], the three-element photodiode array combined with two fixed htc vive base stations in a measurement environment were chosen to start from."
"when working with spherical coordinates to represent a point in 3d-space, the htc vive system can provide the azimuth and elevation data whereas the distance can be estimated using the configuration of the photodiode array. given our previous work [cit], accurately measuring distance using ultrasound emissions is a feasible and low-cost solution. therefore, a three-element digital microphone array [cit] was fitted on the nodes. these knowles sph0641lu4h-1 [cit] digital microphones incorporate an analog amplifier together with a 1-bit analog-to-digital converter (adc) in a single cost-efficient package. when introducing an ultrasound transmitter into the measurement environment at a known absolute location or relative position in regard to the htc vive base stations, the extra information can improve the pose estimates' robustness, accuracy and precision."
"to introduce the wbsn sensor nodes attached to the body parts into the model, the node's attributes are first defined in their relative coordinate system with the center of the pcb as its origin. in this case, the configuration of the photodiode array s r n is described by as a 3x3-matrix: 3 . in order to transform these relative coordinates to the world coordinate system, information regarding the placement of the sensor on the subject's body is required. this corresponds to defining a displacement vector v s n relative to the body part's parent joint. equation 3 demonstrates how the sensors' world coordinates are calculated:"
the efficacy of our proposed pose estimation technique based on a parametric human body model that incorporates a kinematic chain to estimating a body pose by using a combination of the 6dof poses of individual nodes was quantified in a simulation that added cumulative sensor noise. as performance metrics the average error on both the position and the orientation were used. panel a) represents the average error on the position whereas panel b) shows the average orientation error. it is shown that in both cases the parametric human body model approach boasts lower average errors in general and is less susceptible to sensor noise. the region of interest is indicated with a gray background in both panels.
"in our previous work [cit], a body pose was established by directly using the position and orientation (6dof) of the individual measurement system(s). the pose of the human body or the pose of a subset of the body can be described as an interconnected chain of body parts with a given size and a point in 3d-space, that functions as the body's origin. figure 4 panel a) shows an articulated lower body with p w b as the origin to which all other body parts are connected, defined in a right-handed world coordinate system that has its origin in p w o . the superscript w indicates that this point is defined in the world coordinate system instead of a relative coordinate system, indicated with the superscript r. the body parts themselves can pivot around their respective joints that in turn affect the orientation of subsequent body parts. since we are interested in estimating human body poses, a parametric human body model was created which incorporates (for volume 7, 2019 these body parts each have a start and an end point which in most cases serve as joints, besides p w b these points are designated by an acronym derived from the name of the corresponding body part: the sternum ( p s ), the left and right hip ( p w hl and p w hr ), the left and right knee ( p w kl and p w kr ), the left and right ankle ( p w al and p w ar ). the sensors in figure 4 are designated with a capital s with the body part they are attached to as their subscript, e.g. s t is the sensor placed on the torso. each sensor is represented using a 3d-model of the pcb on which the photodiodes are indicated together with the center of the pcb and its principal axes. our human body model, which consists of the aforementioned components, is an interconnected kinematic chain that is modeled as a set of matrix equations. as an example the equations that define the lower right joints p w hr, p w kr and p w ar are given in equation 2:"
"the three ir photodiodes are positioned on the wbsn pcb in an acute triangle configuration, as shown in figure 1 panel b), and will receive the ir pulses and sweeps originating from the base stations, which is illustrated in figure 2 panel a). to extract the azimuth and elevation information, also referred to as lighthouse data in this paper, the stm32f4 microcontroller registers the timing at which the rising and falling edges are detected. with two htc vive base stations the discretized photodiode receiver signal, shown in figure 2 panel b), will exhibit a pulse train that features two sync pulses with varying pulse width and a narrow pulse that represents the passing of either the horizontal or vertical sweep. as will be detailed later, the delay between t 1 or t 3 and t 5 is affinely related to the azimuth and elevation angle under which the photodiode was swept by the laser plane. this delay differs for each photodiode due to their different positions on the pcb. the sequence of two synchronization pulses and a sweep occurs at a rate of 120 hz. the duration of the synchronization pulse flashes encodes three information bits that indicate whether (a) the device will produce an ir laser sweep, whether (b) it will be the vertical or horizontal sweep and (c) an additional data bit. the latter can be used to optically transfer additional status information, e.g. firmware version, rotor offsets, etc. the azimuth θ pd n and elevation φ pd n angle for every photodiode pd n can be calculated using the time difference t between the rising edge of one of the synchronization pulses t 1 or t 3 and the rising edge of the laser sweep t 5 as shown in equation 1. the azimuth θ pd n is the angle in the xy -plane measured from the positive x -axis and elevation φ pd n is the angle in the xz -plane measured from the positive x -axis. this deviates from the classic definition in a spherical coordinate system [cit] ."
"the position and orientation. additionally the individual node approach is more susceptible to sensor noise, especially the average orientation error quickly escalates. these results prove that our proposed parametric human body model is highly resistant to sensor noise. this feature is desirable in a system that requires high accuracy and precision in its pose estimates."
"up to this point, we have established a forward parametric human body model that can be seen as a kinematic chain which can be used to generate sensor data m c lh based on the position of the htc vive base stations p w lh, the sensor configuration s r, displacement vectors v that define either size of the body parts or the placement of the sensors, a model's origin point p w b and rotations of the body parts r α,β,γ . while a number of these parameters are dependent on the measurement setup and test subject, the origin point p w b and rotations of the body parts α, β, γ are the variables which are used to estimate the actual pose of the subject throughout a simulation or experiment."
"a ground-truth reference that confirms the realism of our estimated body poses however was not accomplished throughout our real-world experiment due to practical concerns. while a qualisys motion capture system was available to establish a ground-truth, the cameras used by such a measurement system emit infrared emissions that interfere with the infrared emissions emitted by the htc vive base stations. this ir interference disrupts the processing of the photodiode information thus rendering the wbsn nodes virtually void of the sensor information. we are currently working on a solution to alleviate the lack of a ground-truth reference."
"once the synchronous hardware architecture was developed and verified, its sensor data needed to be integrated into the processing algorithm in order to achieve the actual body pose estimates. to facilitate the sensor integration and further development of the pose estimation processing algorithm, we composed a model for the sensor node."
"while the redesigned hardware proves to be a valuable contribution, this first iteration of the parametric human body model also shows great promises for the body pose estimates. given the structure of the kinematic chain, other joints and body parts can be easily added with their respective rotational ranges and physical sizes. although currently only the sensor model for the htc vive sensor is implemented, the other sensor modalities will follow to further improve the robustness, accuracy and precision of the pose estimates. the sensor processing and pose estimation techniques could be further optimized in future work as well. in this work single snapshots of sensor data are processed to attain the body poses while time-sequence processing could be used instead. this type of processing could be achieved by applying a movingwindow technique on the sensor data or by using a particle filter in conjunction with our probabilistic pose estimator."
"3d-printed enclosure a node measures 6.8 cm by 7.2 cm by 1.85 cm. when combining the price of the pcbs with the price of all components, the hardware cost of a single wbsn node amounts to approximately e 65 (in a batch of 10 nodes). fifteen of these sensors, adding up to ±e 975, should suffice for estimating a full body pose. the e 1000 price point is slightly exceeded when a wi-fi router and two htc vive base stations are added to the system cost. however, it is reasonable to assume that the sensor cost can be decreased significantly in a larger production quantity."
"using the likelihood function a posterior probability function for the parameter vector can be calculated given a volume 7, 2019 measurement m m lh, utilizing bayes rule:"
"kw represent the new decision variables of the problem. following the decision rule approximation, the number of decision variables of the problem is polynomial in the number of time windows, categories, resources, and teams. also, it is independent of the number of scenarios. since the linear functions lie in the space of all feasible functions the decision rule results in a conservative approximation. we denote the resulting conservative approximation by (p l ). proposition 3. problem (p l ) provides a lower bound on the optimal objective value of problem (p)."
"the pulsed tof range finding method usually uses the direct distance measurement technique with a time-to-digital or a time-to-amplitude converter and can achieve a millimeter level accuracy even to a range of more than a few tens of meters [cit] . the drawback of this technique is the sensitivity to background light if the sensor is open for photons during a time window covering the whole measurement range (e.g. 100 ns for a measurement range of 15 m). with strong ambient light conditions, one solution is to use a fast electric shutter to control the sensor's sensitivity to photons by limiting the time interval during which the detectors are enabled for the photon detection [2 -5] . in this work, the width of the time gating with an electric shutter is set to the minimum and thus the imager is very tolerant to background light."
"to represent these constraints efficiently, we apply the above result to each constraint in problem (p l ) (except from (4)) and denote the resulting problem by (p l−rc ). for general uncertainty sets, we obtain a conservative approximation to problem (p l ) . the following proposition establishes that with uncertainty set ξ as, as defined as in example 2, the reformulation of these constraints is exact. proposition 5. suppose that we have the uncertainty set ξ as as defined in example 2. then, statements i) and ii) in proposition 4 are equivalent and problems (p l ) and (p l−rc ) are equivalent."
we proposed an antenna selection algorithm applied with beamforming to gain high throughput in cr networks. the proposed algorithm allows cognitive users to access the channel with no interference effect on primary users using beamforming. our proposed cross-layer algorithm is shown to be offer high throughput using low number of rf chains. our simulation results also show that the effect of imperfect (csi) and delayed estimates is not significant where the system still able to outperform other schemes.
"three dimensional imaging of the environment is important for autonomous driving systems, robotic vision and surveillance applications, for example. a low-price and small 3d-scanner would find use also in human-machine interfaces and gaming applications."
"we address a significant limitation in this area of work, where the previous unrealistic assumption of complete certainty in screenee arrival times, renders its solution unusable in realworld settings by proposing a scalable framework that provides good solution quality and works for generalized models of uncertainty."
"it is worth noting that the complexity of the proposed algorithm grows with the number of transmit antennas. we calculate the complexity of the algorithm by the required number of floating point operations (flops). one can note that determining the beamforming matrix [cit] requires only six flops, and the antenna search algorithm requires ( n t m ) operations for the combined algorithm. thus, the total complexity of the cross-layer antenna selection and beamforming algorithm is equivalent to o(( n t m )6)."
"the independent time gating signals for each of the 40 sub-arrays can be selected from the dll outputs with multiplexers. hence, time gates at each sub-array can vary with the maximum of ~24 ns in relation to each other. in the prototype, each of the time gates are produced from three independently selectable signals. the selection of the signals is made with 240-to-1 multiplexers (mux) from the dlls' 240 outputs. each of these 3*40 muxs is controlled by the external fpga with 8 bits."
"now, we investigate the effect of imperfect csi. for this purpose, we assume a timeframe consisting of l t training or pilot symbols and l d data symbols. radio devices can estimate the channel h p using a priori knowledge of these training symbols in maximum-likelihood estimation method to yieldĥ"
"at the beginning of the operation, the spad is quenched by biasing the anode node of the diode to the voltage supply of 3.3 v. the cathode of the diode is connected to a highvoltage supply of ~22 v. when the quenching switch is turned off, the anode node can be shunted to the ground. this forms the leading edge of the time gate and exposure of the sensor for photons."
"the rest of the paper is organised as follows. section 2 introduces the proposed system model where we consider both perfect and imperfect channel-state information (csi). performance evaluations and discussions are provided in section 3. finally, conclusions are presented in section 4."
"screening for threats is an important security challenge, be it inspecting cargo at ports, passengers at airports, or fans entering a stadium. given a strategic adversary capable of exploiting gaps in security measures, along with a large number of screenees, it becomes critical to optimize the allocation of limited screening resources. indeed, to improve airport screening efficiency and effectiveness, the us transportation security administration (tsa) recently launched the dynamic aviation risk management solution (darms) initiative [cit] to incorporate adaptive screening."
"in the following analysis, we choose boldface letters to represent vectors and matrices, (.) h represents conjugate transpose, e(.) denotes the expectation, . stands for 2-norm."
"however, even when restricting π to be functions of the aggregate arrival ζ, o w and u ρ are still functions of the full passenger arrival ξ w−1 . the overflow in time window w is a function of not only ξ w but all"
"the fill factor of the sensor array should be high in the focal plane approach to maximize the photon detection efficiency. the larger the spad array, the more difficult it is to get a high fill factor due to the large number of routing signals or due to in-pixel electronics. typically, large arrays with more than 1000 pixels tend to have a small, less than 10 % fill factor [cit] ."
"the 3d range imager proposed here is based on the pulsed tof technique and flexible time gating of the spads in the detector array. the detector chip collects binary type 2d cross-sectional images from predetermined distances which are defined by the time gating electronics, see fig. 1 . the chosen measurement range is scanned, and the 3d image is accumulated from the 2d cross-sectional images of the surface of the target resulting from the single shot measurements."
"it is important to mention that in the cross-layer antenna selection algorithm, antenna combination is selected from the available antennas that achieve maximum transmission efficiency at the llc sub-layer. for this purpose, a search process considers all possible antenna combinations. conversely, in the cross-layer antenna selection and beamforming, the search process considers only the combinations that can be applied to beamform the transmitted symbols. that is beamforming is sed here to cancel interference between cognitive and primary users."
"in the following, the operating principle of the proposed 3d imager is described first. then the pixel design and architecture of the chip are described. at the end of the paper, the first 3d range image measurement results are shown."
"in fig. 2, one can note that the clbf algorithm outperforms clas, bf and no antenna selection algorithms. as one can see, the use of antenna selection combined with beamforming offers larger throughput gains as the number of available antennas increases. the extra throughput gain achieved is owing to the ability of the proposed algorithm to remedy the interference effects at the primary users while maximising the throughput of the cognitive network. different from the clbf and bf algorithms, the performance with clas and no antenna selection is shown to deteriorate as the snr goes high because of the interference constraint set at the primary user."
"problem (p) can become computationally expensive to solve for realistic size instances where the cardinality of ξ is exponential in the number of time windows, see example 3. we thus propose a solution approach that results in a tractable problem even when ξ has exponentially many scenarios. in what follows, we describe our approach and main results. the proofs can be found in the appendix."
"in order to be able to place the time window of 24 ns (see fig. 2 ) to the desired time position, a control block was implemented. this control block enables the dlls' output buffers for a specific time window. the control block has a 6-bit counter which counts total dll cycles after the laser command has been sent. the control block is controlled by the fpga with 6 bits and enables a time window of 24 ns for the time gating even with a ~1.5 µs delay from the laser pulse."
"a promising technique to realize a small and robust 3d-scanner is to use the optical time-of-flight (tof) technique with the electronic focal plane scanning approach. in this technique, a laser diode source illuminates the field of view of a single photon avalanche diode array. when the detector array is located at the focal plane of a positive lens, a 3d range image of the field-of-view of the system can be produced without any rotating mirrors or other moving parts. the distance in tof systems is measured either directly from the time intervals taken by the photons to travel from the laser transmitter to the target and back to the receiver or indirectly by utilizing a phase comparison technique with a continuous wave (cw) modulated source."
the time gating signals are produced on-chip from two delay-locked loops (dlls) each of which has 120 outputs. the length of both dlls is 24 ns with an 83.3 mhz clock signal from the fpga.
"finally, the cross-layer based antenna-selection and beamforming algorithm works as cognitive source node first selects the antenna combination to achieve maximum throughput at the llc sub-layer. then precoding is applied to the transmitted symbols for zero-forcing the interference at the primary user, as presented in (19)"
"in fig. 3, we examine the effect of delayed csi on the throughput performance. from these results, one can note that performance degradation occurs because of delayed csi where degradation is more evident in the bf case than the clbf case. also as seen, in all cases, the proposed cross-layer design is shown to outperform the beamforming scheme."
"threat screening games (tsgs) [cit] have been previously introduced to model screening domains as bayesian stackelberg games. these games model situations, wherein a strategic attacker attempts to penetrate a secure area, while the screener has the opportunity to screen for threats using limited resources. optimizing the defender (mixed) strategy by means of the tsg captures the strategic behavior of attackers and thus yields more effective screening strategies. tsgs are inspired by previous research in security games [cit], where a defender protects a set of targets from a strategic adversary. however, tsgs differ significantly because they (i) do not have an explicitly modeled set of targets; (ii) include a large number of non-player screenees that must be screened while a single adversary attempts to pass through undetected; and (iii) encompass screening resources with differing efficacies and capacities that are combined to work in teams. these key differences make tsgs more appropriate for screening settings."
"we have omitted the sets of the variables κ, m, w and φ to minimize notational overhead. the variable z w κ,m is the probability of detecting an attack (m, w, κ). accordingly, the objective function corresponds to the worst-case expected utility of the screener. the expectation is taken with respect to the uncontrollable component of the attacker's category. the minimum is taken across all choices available to the attacker."
denotes the set of all marginal strategies. we note that problem (2) is equivalent to a moderately sized linear program obtained by linearizing the piecewise linear concave objective function using the standard epigraph reformulation approach.
"to address this shortcoming, our first contribution is a new model robust threat screening games (rtsg), which expresses the required uncertainty in screenee arrival times. in rtsg, we model the problem faced by a screener as a robust multistage optimization problem. we present a tractable solution approach with three key novelties that contribute to its efficiency: (i) compact linear decision rules; (ii) robust reformulation; and (iii) constraint randomization. we present extensive empirical results that show that our approach outperforms the original tsg methods that ignore uncertainty, and the exact solution methods that account for uncertainty."
"to be able to measure distances from zero distance, a selectable delay for the laser shoot command has been implemented. the selection is done from one of the 120 dll outputs with the delay grid of 200 ps. the signal for laser driver circuitry is buffered in the same way as the time gate signals to reduce the effect of temperature and supply voltage changes."
"despite promising results, previous work in tsg fails in its mission to realistically model real-world settings. its fundamental limitation is its assumption of perfect fore-knowledge of screenee arrival times (e.g., arrival times of passengers at airports). however, in the real-world there is significant uncertainty in arrival times. addressing this challenge is difficult, as it requires reasoning about all the possible realizations of the uncertainty and coming up with an optimal plan for each of those scenarios. when dealing with a large number of screenees, this result in millions of possible scenarios, making the planning problem extremely difficult."
"a 3d range image of two spheres is shown in fig. 7 . the diameters of the targets with non-cooperative surfaces are 60 cm and 22 cm and they were placed at the distances of 80 cm and 60 cm from the transceiver. the positions of the time gates at the beginning of the scan are shown in fig. 7 a) and a partial scan range of 30 cm with ~1.5 cm (~100 ps) lsb z-resolution was selected. the time gate widths were 800 ps. the shown result was achieved by using 1000 laser pulses for each of the time gate positions (i.e. 20 000 laser pulses for the 30 cm partial scan). the laser pulsing rate was 100 khz resulting in a 3d frame rate of 5 hz. the laser pulse had a wavelength of 860 nm, an energy of ~1 nj and fwhm of 110 ps. an optical bandpass filter was placed in front of the detector lens to suppress the ambient light. the measured ambient lighting at the object fov was 200 -600 lux. the current consumption of the receiver from a 3.3 v power supply is 20 ma of which 5 ma is consumed by the delay-locked loops and the time gating circuitry."
"where ν w,s κ,t denotes the number of screenees in category κ screened by team t in window w according to pure strategy s. defender mixed strategies. a mixed strategy corresponds to a distribution over pure strategies, i.e., to a choice"
"during the last decade extensive research has been conducted to improve the spectrum utilisation in wireless applications. among these activities multiple-input multiple-output (mimo) technology has shown to improve the spectrum efficiency and the reliability of the channel. despite these efforts, spectrum crisis situations still exist because of the fixed spectrum allocation policy where users are assigned portions of the spectrum permanently. owing to the unprecedent growth of wireless users, some portions of the assigned spectrum become heavily congested, while leaving other parts unutilised. to solve this problem, and to efficiently utilise the available spectrum, cognitive radios (cr) have been proposed. it is envisioned that cr will share the spectrum along with existing primary users in a dynamic and an opportunistic manner [cit] ."
"in order to get a high fill factor for the sensor part of the chip, the electronics inside the sensor array have been minimized. biasing switches, 1-bit data storage, and buffering of signals were implemented inside each pixel. a simplified schematic of the pixel is presented in fig. 3 ."
"the first constraint in the set stipulates that the number of screenees must be a non-negative integer. the second ensures that all the screenees are allocated to a team. the third guarantees that resource capacities are not exceeded. note that s has finite cardinality, i.e., there are finitely many pure strategies available to the screener. the probability of detecting an attack (m, w, κ) given defender strategy s is given by"
"while a factor f is added to f such that both f and d decay exponentially back towards 1 between spikes according to each depression variable d1,d2,d3 follows the same dynamics, but with different parameters and d. the parameters of this short-term plasticity model were fit to the dynamics of fepsp slopes during the various plasticity induction protocols in (14)(i.e. 0.5, 1, 5, 20 hz trains) and during tbs in this study. the fit was constructed to minimize the squared error between values of a and the normalized fepsp during induction using the lsqcurvefit function in matlab. see supplemental table 2 for the resulting parameters."
"we also note recent efforts to increase stimulation intensity up to 6ma in humans by distributing current across multiple electrodes (84), which can achieve electric fields of 3 v/m in the brain (81) . given our estimates here, this would generate effects on synaptic plasticity of ~3%, notably affecting a few synapses most strongly ( figure 8d ). recent in vivo rodent work suggests that a motor learning task leads to potentiation of ~1-2% of synaptic spines in a given volume of cortex (85), which is comparable to what we expect tdcs to achieve. effect sizes of tdcs on synaptic plasticity in humans are therefore likely to be in a behaviorally relevant range."
"associativity refers to the potentiation of a weak synaptic input when it is paired with strong input at other synapses to the same neuron. in this sense the weak input becomes associated with the strong input. this can serve as a cellular mechanism to bind previously unrelated information as in classical conditioning (60), and to form cell assemblies for associative learning (35) (36) (37) . here we show that dcs can further enhance this associativity, which may manifest as an increased probability of forming associations between stimuli during learning that involves hebbian plasticity."
"hebb originally proposed that coincident pre and postsynaptic firing was required for enhanced synaptic efficacy (49) . over time the concept of hebbian plasticity has come to incorporate forms of plasticity that depend on correlated pre and postsynaptic activity variables, regardless of the exact biophysical implementation (50) . while we do not directly measure or manipulate postsynaptic firing here, tbs-induced ltp at ca1 schaffer collaterals has been shown to be hebbian in that it depends on pre and postsynaptic activity and exhibits classic hebbian properties of input specificity and associativity (51) . the synaptic plasticity rule in our model is similarly hebbian in that plasticity depends on correlated pre and postsynaptic activity in the form of presynaptic spike arrival and postsynaptic membrane voltage (41) ."
"two overlap adjacent line segments have two spatial structure relationships: collinear ( figure 5 ) and noncollinear. for the former, we refit all laser points belonging to two adjacent line segments. in addition, the process of dealing with overlap region is to compare the distance from a laser pointer to a different line segments for the latter, this method is algorithm 3."
"we induced ltp in hippocampal brain slices using theta rhythms (theta burst stimulation, tbs), and confirm that this form of \"endogenous\" plasticity is pathway specific and associative. applying dcs during plasticity induction boosted the amount of ltp, while maintaining the pathway-specific and associative properties of the underlying endogenous plasticity. additional experiments and computer simulations support the hypothesized model in which dcs achieves these effects through altered neuronal excitability and subthreshold depolarization in dendrites during ongoing synaptic input."
"input specificity is a property of hebbian plasticity whereby only synaptic inputs that are coactive with the postsynaptic neuron, and presumably relevant for the current task, are strengthened (35) . the computational significance of this specificity has been recognized for some time, as it allows a network of neurons to learn sparse, non-overlapping neural representations (52) . in practice, this is implemented in the brain by molecular machinery which responds to elevated activity specifically at task-relevant synapses (53) . here we show that dcs enhances ltp in a manner that respects this input specificity. dcs only boosts the strength of synapses that are active and already undergoing endogenous plasticity. based on this observation, we make two predictions for the optimization of tdcs effects in humans."
"however, the process of split-and-merge works on the overall scan data, other than considering line segments separately. for this reason, breakpoints may appear in line segments to make them incomplete. one typical example is shown in figure 2 . meanwhile, recursive manner leads to the performance degrading in the computational efficiency."
"membrane polarization ( figure 8a,b) was calculated by simulating a single cell without synaptic input for 100 ms with varying applied electric field. membrane polarization due to dcs was calculated in each compartment as the voltage at the end of the simulation minus the corresponding voltage in the control simulation without dcs. for each simulation and each activated synapse k, we calculate the effect of dcs on plasticity where t is the duration of the simulation, is the final weight of the k th synapse at the end of the simulation with dcs, is the weight at the end of the corresponding control simulation where no dcs was applied. note that all dcs simulations have a control simulation in which all other details are identical. therefore any deviation of from 1 represents the effect of dcs on the k th synaptic weight."
"the asymmetry may result from the interaction between dcs effects on different neuronal compartments. for example, during cathodal stimulation, depolarization of apical dendrites can counteract hyperpolarization of somas so that there is no reduction in ltp (61)( figure 8a,b ). however, this mechanism cannot explain the asymmetry we observed for ltp in basal dendrites ( figure 4c,d, bottom row), as the direction of polarization is the same in both basal dendrites and somas (supplemental figure s5a,b ). while our model does predict a nonlinear dose response in basal dendrites (supplemental figure s5e ), this is more likely due to nonlinear responses of voltage-gated ion channels or the synaptic plasticity molecular machinery."
"we propose that dcs causes small changes in postsynaptic membrane potential during ongoing endogenous synaptic activity. the altered voltage dynamics in the postsynaptic neuron then modify synaptic strength via the machinery of endogenous voltage-dependent hebbian plasticity. an implication of this hypothesis is that the effects of dcs should exhibit similar properties as the endogenous hebbian plasticity that it is paired with. two of these properties, pathway specificity and pathway associativity (19, 20), support functionally specific learning of cell assemblies in neural networks (21, 22) . tdcs may therefore enhance functionally specific learning by acting through this hebbian mechanism."
"in a second experiment, the weak input is now paired with a strong input (tbs) during induction ( figure 3b ). during induction, weak pathway inputs are timed to arrive at precisely the same time as the second pulse of each theta burst. this induces ltp in the strong pathway as before ( figure 3b black;"
"the standard least square method for line fitting may yield unsatisfactory parameter estimation results, because it only minimizes the vertical distance of each point from the line. for this reason, the orthogonal linear fitting method, 36 which minimizes the perpendicular distance of each point from the line and is adopted in this article to obtain satisfactory line fitting results."
"in this article, we treat the line segment extraction problem as a special kind of region segmentation. different from existing algorithms, our proposed algorithm uses seedsegment and region growing instead of recursion and breakpoint detectors. thus, the efficiency and correctness are improved in the extraction process. in order to obtain the precise endpoints of line segments, the processes of overlap region processing and endpoint generation are proposed. in the summary, through the design of seed-segment detection, region growing, overlap region processing, and endpoint generation, the proposed algorithm achieves superior performance, in terms of efficiency, correctness, and precision."
"we are ultimately interested in understanding the effects of weaker electric fields that occur in the human brain during clinical tdcs, which are on the order of 1 v/m (45, 46) . the model presented above is able to reproduce several experimental effects of dcs ( figures 5-7, supplemental figure s3 ) and canonical synaptic plasticity results (supplemental figure s1 ) with the same set of parameters (supplemental table 1 ). because the model includes the actual morphology of ca1 pyramidal neurons, the electric field magnitude in simulations has a precise mapping to the electric field in experiments. we therefore used the model to make predictions for how weaker electric fields would influence synaptic plasticity."
"here we used a 20 v/m electric field in order resolve effects with a reasonable number of animals. electric fields in the brain during typical tdcs experiments are expected to be 1 v/m or less (45) . while we do not measure effects with this intensity, our computational model predicts a monotonic relationship between the population-mean synaptic plasticity and electric field magnitude ( figure 8c ). for a given dcs polarity, the model predicts a linear relationship between field magnitude and mean plasticity effects ( figure 8e ). to first order this implies population mean effects of ~1% for fields of 1v/m (we observe ~20% effects for 20 v/m), in line with effect sizes observed for acute effects of dcs (70) . however, experimentally we observe sublinear effects with anodal stimulation ( figure 8f ). this linear approximation may therefore underestimate effect sizes with weaker fields."
"polarization of neuronal membranes in response to extracellular electric fields has been well characterised (17) (18) (19) (20) (21) (22) (23), as has the membrane potential-dependence of hebbian plasticity (24) (25) (26) (27) . while it is straightforward to draw a connection between these phenomena, their interaction can be complex. for example, we previously observed that the effects of dcs depend on both the location of active synapses and the precise temporal patterns of activity used to induce plasticity (14) . these results suggest that the effects of dcs depend on the interaction between the induced electric field, neuron morphology, and the endogenous brain dynamics. given this complexity, it is perhaps no surprise that results from human clinical trials with tdcs have remained inconclusive (28) (29) (30) (31), or that optimization of tdcs protocols has been slow. for example, there is an ongoing debate as to whether tdcs should be applied before, during, or after a behavioral or cognitive task (32) (33) (34) ."
"after the above steps, the line parameters ða; b; cþ, start point index m, and endpoint index n of the line segment can be obtained. the endpoints of line segments can be calculated from the fitting straight line and its vertical line, this method is shown in figure 6 . according to this rule, the proposed method has good consistency for the obtained endpoints of the line segments."
"our experiments and computer simulations support a model in which dcs affects tbs-induced ltp primarily by somatic polarization and changes in somatic spiking (figures 4-6 ). however, dcsinduced dendritic polarization is also likely to contribute to plasticity, as we suspect for 20 hz tetanus experiments ( figure 6 ) (14) . our computational model can reconcile these results by considering the voltage dynamics during induction ( figure 6 )."
"conventional seeded region growing is a local method with no global view of the problem and thus it is sensitive to noise. however, laser data are more reliable and accurate compared with image data, seeded region growing using laser data presents satisfactory performance."
parameters for the plasticity model were manually selected so as to replicate classic spike-timing dependent plasticity experiments (supplemental figure s1 ) and to qualitatively reproduce the effects of dcs on ltp. we are mainly concerned with relative changes in ltp due to dcs (or spike timing/frequency in the case of replicating stdp experiments) and so do not adjust parameters to quantitatively reproduce the amount of ltp in each experiment. under these constraints we were able to use the same set of parameters for each simulation (supplemental table 1 ). numerical integration using the forward euler method (0.025 ms time step) was used to solve for wi.
"we assume that the data sets are distributedly stored in a network of n processors, i.e., each processor i knows only the pair (x i, y i )."
"to further validate the role of somatically initiated spikes in generating this dcs effect, we repeated the previous simulations, but set the voltage-gated sodium conductance to zero in the soma and axon ( figure 6a bottom). this is analogous to the local application of ttx at the soma (44), preventing the initiation of spikes there. if the strength of synaptic stimulation is increased, spikes can still be generated, but they initiate locally in the dendrite ( figure 6b taken together, the results of figure 6 suggest that dcs can enhance associativity by facilitating the initiation of somatic spikes. the additional spikes can spread to synapses in both pathways and increase ltp, leading to a stronger association between the pathways."
"as shown in table 3 and figure 9, our proposed algorithm is more stable and much faster than split-and-merge, because our method does not need to split the data iteratively."
"we present what is, to our knowledge, the first computational model of the effects of dcs on synaptic plasticity, which reconciles several experimental results. the model makes specific and testable predictions for both how tdcs should alter plasticity when paired with various endogenous brain states, and how this can inform the design of tdcs protocols. specifically, the most effective tdcs interventions should be those that pair stimulation concurrently with behavioral training and that performance gains should be specific to the learned task."
"in a previous study, we used 20 hz tetanic stimulation to induce ltp. we observed that a boost in ltp required opposite dcs polarities for apical and basal dendrites, suggesting that dendritic rather than somatic effects were dominant for this protocol (14), ( figure 7b, top two rows). this appears inconsistent with the previous claim that dcs effects are mediated primarily through somatic spiking (figures 4-6 ). however, the computational model can readily reconcile these results if we consider the different endogenous membrane voltage dynamics during 20 hz tetanus and tbs protocols. for 20 hz tetanic stimulation, inputs arriving early in the tetanus may elicit somatic spiking ( figure 7d ), but these inputs quickly become subthreshold due to short term synaptic depression ( figure 7e ). since the majority of input pulses remain subthreshold, plasticity at these synapses is dominated by the local subthreshold dendritic potential ( figure 7f ). because the dcs-induced polarization is opposite in apical and basal dendritic compartments, the effects on plasticity are also opposite there ( figure 7b the vertical axis is the average of the last ten normalized fepsp responses. the top two panels are reproduced from data in (14) . the bottom two panels are identical to figure 4d, shown again here for comparison. c) model ltp predictions qualitatively match experimental ltp results (c.f. c; same direction of dcs effect). the vertical axis (norm. weight) is the average weight of all activated synapses at the end of simulation, calculated offline using the learning rule of (41) . d) example simulated voltage traces for individual cells recorded only at activated synapses during the first four input pulses. traces are averaged over all activated synapses for the example cell. spikes that backpropagate from the soma are indicated with arrows. e) same as d, but at a later time point in the simulation (pulses 10-13 for 20 hz tetanic stimulation; pulses 13-16 for tbs simulations). note that for 20 hz stimulation synaptic depolarization is reduced due to short term depression and somatic spiking ceases very early in the simulation. during this subthreshold period, dcs causes a small shift in membrane potential and the resulting plasticity. since dcs causes opposite subthreshold polarization in apical and basal dendrites, the effect on ltp is also opposite in apical and basal dendrites (c, top two rows). for tbs simulations, recovery from short term depression between bursts allows bursts later in the simulation to produce somatic spikes. plasticity throughout the simulation is controlled by somatic spikes, and is similar in apical and basal dendrites (c, bottom two rows) f) dynamics of synaptic weights during the full simulation, averaged over the entire population of activated synapses. for tbs simulations, the weight change is approximately linear in the number of bursts, as each successive burst is equally effective at inducing plasticity. for 20 hz stimulation, the weight change saturates with the number of pulses, as each successive pulse is weaker due to short term depression. only the weight at the end of the simulation is used to predict the resulting ltp in experiments (c). gray boxes in f indicate time periods for early (dark gray) and late induction (light gray) that are plotted in d and e, respectively. a schematic of the input pulse train and relative timing of early (dark gray) and late (light gray) induction pulses are shown at the top. all data are represented as mean±s.e.m."
"we first note that the delays do influence the convergence rate of the two algorithms, that is, the greater the delay between nodes the more time the algorithms need to terminate. second, as shown by the curve for dg the number of iterations seems to increase as a cubic function of the number of delay steps, which agrees with our analysis in theorem 4.1. finally, in this example, uniform delays have a bigger impact on the performance of da, that is, da requires more iterations to converge than dg under the same number of delay steps."
"simulated neuron morphology, showing an example of how synapses are distributed in the weak (5 hz, light pink) and strong (tbs, magenta) pathways. b) probability of time delays between spikes observed in the soma and at weak pathway synapses. negative time delays correspond to spikes that occur in the soma first. due to variable propagation delays between synapses, it is possible for a spike initiated in the dendrite to reach the soma before other synapses. this produces a negative delay between the soma and these delayed synapses, even though the spike was dendritically initiated. it is not possible however, for a spike initiated in the soma to show a positive delay. c) distribution of spike times recorded at all weak pathway synapses. spike times are shown relative to the onset of the corresponding burst. d) model prediction comparing plasticity in the weak pathway when it is unpaired (weak only) and paired (weak+strong). the vertical axis (norm. weight) is the average weight of all weak pathway synapses at the end of simulation, calculated offline using the voltage-based learning rule (41) . e) experimental data (same as figure 3d ) shown again here for comparison with e. both model and experiment show that anodal dcs increases ltp in the weak pathway only when it is paired with strong pathway activation. bottom row: simulations and methods are identical to the top row, with two exceptions. first, we emulated the application of locally applied somatic ttx by setting voltage-gated sodium conductance to zero in the soma and axon, preventing the initiation of spikes in these compartments. second, the number of synapses in each pathway was doubled, increasing the likelihood of spike generation, which now occurred in the dendrite. the testable prediction of the model is that in the presence of ttx now dcs will no longer boost ltp."
"perhaps the most well characterized cellular effect of electrical stimulation is the modulation of somatic membrane potential and firing probability (18, 20, 61, (65) (66) (67) (68) (69) (70) . in human tdcs studies, it is the modulation of motor-evoked potentials, which have been linked to long-term plasticity (48, 71, 72) . here we propose a model which translates acute changes in firing probability and timing into long term changes in synaptic plasticity. in addition to several other phenomena (7, 8, 73, 74), previous studies have pointed to the effects of dcs on bdnf release (12, 13, 15, 75) . while the precise mechanisms remain unclear, bdnf appears to be released in response to postsynaptic depolarization and involved in ltp induction (76) (77) (78) . bdnf may therefore be an essential part of the molecular machinery that translates dcs-induced effects on membrane potential dynamics into changes in plasticity."
"a complete comparison of computational time of our proposed algorithm and split-and-merge is carried out in this subsection. two data sets taken from a hallway and a laboratory are used to record the computational time of two algorithms. without loss of generality, we select first 1000 frames to display the result, as shown in figure 9 ."
"the author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: this work was supported in part by national natural science foundation of china under grant nos. 61573195 and u1613210."
"as a typical image segmentation method, 31 region growing examines neighboring pixels of initial seed points and determines whether the pixel neighbors should be added to the region."
"we will compare our algorithm (currently, a video of experimental results is available at https://youtu.be/ynn9n rioobc. the source code is available at https://github.com/ ghm0819/laser-line-segment.) with the well-known split-and-merge(https://github.com/kam3k/laser_line_ extraction.) algorithm, two algorithms are implemented with c/cþþ. in addition, the system parameters are shown in table 2 ."
"schematic of the experimental setup. two synaptic pathways are monitored before and after plasticity induction. during induction, one pathway is activated with tbs (black, strong), while the other pathway is inactive (grey), and anodal dcs is applied across the slice throughout the duration of induction (3 s, red). b) plasticity is pathway specific and so are dcs effects. ltp was observed only in the pathway that received tbs (black trace), demonstrating pathway specificity. anodal dcs enhanced ltp only in the potentiated pathway (red vs black) and had no effect on the inactive pathway (light red vs. gray), upholding hebbian specificity. fepsp slopes are normalized to the mean of the 20 of baseline responses prior to induction. induction is applied at the 20 minute mark. c) summary of pathway specific effects of dcs. the mean of the last 10 normalized slopes (51-60 min after induction) are used for each slice. data are represented as mean±s.e.m."
"we propose a general principle that emerges from this result: when endogenous plasticity is primarily driven by somatic sources of depolarization (e.g. spikes), dcs-induced polarization at the soma determines effects on plasticity. when endogenous plasticity is primarily driven by dendritic sources of depolarization (e.g. subthreshold depolarization or dendritic spikes), dcs-induced polarization at the dendrite determines effects on plasticity. the relative contribution of somatic and dendritic dcs effects, and therefore the overall effect on plasticity, is not always obvious. the spatial location and temporal pattern of active synapses, as well as neuromodulator concentrations and intrinsic excitability can all shift the endogenous voltage dynamics towards somatic or dendritic dominance. computational models, such as the one presented here, can help in this regard by exploring how dcs interacts with this large parameter space of endogenous synaptic activity. this should be an important next step for future work."
"we next experimentally tested the dose response by varying the dcs electric field (-5, 0, 2.5, 5, 20 v/m; cathodal negative; anodal positive). as in the model, there was a monotonic relationship between electric field and the magnitude of ltp ( figure 8f figure 1b ). unlike the model however, the effect of anodal stimulation appears to be sublinear, perhaps reflecting saturation of ltp near 20 v/m (47) . this discrepancy can be accounted for by the unbounded synaptic weights in the model, which unlike biological synapses, do not saturate. dcs for each synapse. inset shows example voltage traces for synapses in the tail of the distribution. these synapses correspond to cases where the control simulation brought the cell to slightly below threshold, such that dcs was able to cause firing and produce a large change in the weight. e) mean of the synaptic weight change δwdcs due to tbs, averaged over all simulated apical synapses, as a function of dcs electric field. f) experimental ltp as a function of dcs electric field. all data are represented as mean±s.e.m."
"where a0 is a constant parameter, which we set to 1 at the start of simulations. at the time of each presynaptic spike, d is multiplied by a factor d such that"
"the candidate that satisfies above requirements will be considered as a seed-segment, which is the beginning of the region growing process. the whole algorithm of seedsegment detection is shown as algorithm 1."
"transcranial direct current stimulation (tdcs) studies in humans have recently exploded in number and scope (1) (2) (3) (4) . while these studies have seen varying degrees of success (1), in aggregate they suggest that stimulation with weak constant current can have long term effects on cognitive function (5) . one of the predominant theories to explain these long term effects is that stimulation affects synaptic plasticity (6), although a variety of alternatives have also been proposed (7, 8) and are being explored (9) . the synaptic plasticity theory is consistent with an array of findings from pharmacological studies in humans (10) as well as animal electrophysiology studies conducted in-vivo (9, 11, 12) and in-vitro (13) (14) (15) (16) . however, the biophysical mechanism for such plasticity effects is unknown."
"a nonlinear dose response may also result from the distribution of initial synaptic states in the cell population that we record from. for example the prior history of the recorded synapses may be such that they are biased towards an increase in strength (64) . similarly, it could reflect the distribution of cell excitability, such that cells are biased toward an increase in firing. with this in mind, we analyzed the input-output relationship between fepsp's and population spikes in our baseline recordings. indeed, we found that our experiments are run near a nonlinearity in this input-output relationship, such that population spiking could be more readily increased than decreased (supplemental figure s4) ."
"one interesting question left open in this paper is the study of asynchronous distributed gradient algorithms, that is, when communications delays are different at different nodes and perhaps change with time. in this more general case, it would be interesting to investigate whether an upper bound on the time-varying heterogeneous delays can be helpful in obtaining convergence results. in particular, a possible topic of future research would be to determine if one can obtain bounds on the error in the objective function by using an upper bound on the delays, along with our current results."
"with d i being the distance of the i th point to the line. take the partial derivatives of f ða; b; cþ with respect to a, b, and c, then set them equal to zero, we can obtain the parameter estimation results."
"can be thought of as the temporal center of mass of the somatic activity during a burst. if more neurons in the population fire earlier during the j th burst, then should decrease."
"the electric field during dcs was modeled as uniform extracellular voltage gradient. the extracellular voltage at each point in space is then conveyed to each segment of the neuron by neuron's extracellular mechanism, as has been done previously (23) . since we are interested primarily in the effect of the extracellular field, for each simulation that applies an electric field there is a corresponding control simulation in which the neuron model is identical except for the extracellular applied voltage. the effect of the applied field can therefore be compared to a precise counterfactual, where all other aspects of the model are identical."
"an l-type calcium channel was added throughout the cell as in (86) . sodium conductance in the axon was increased to replicate spike initiation in the axon initial segment (87) . synapses were set to have both ampa and nmda receptors, which were modeled as the difference of two exponentials. nmdar conductance was modified by a voltage dependent mechanism as done previously (86, 88, 89) . see supplemental tables for the full set of neuron model parameters."
"we hypothesized that the effects of dcs on tbs-induced ltp are due membrane polarization. however, dcs will alter membrane potential in both the soma and dendrites of pyramidal neurons, but with opposite polarities (18, 23) . we therefore aimed to test whether the effects of dcs on ltp were consistent with somatic or dendritic membrane polarization. to do so, we took a similar approach as in previous work (14) . ltp was induced by stimulation of schaffer collaterals with tbs in either apical or basal dendritic compartments of ca1 ( figure 4b ). dcs is expected to have opposite effects on dendritic membrane potential in basal as opposed to apical dendrites ( figure 5a ) (18, 23) . effects due to dcsinduced dendritic polarization should therefore be opposite when synapses are activated in apical or basal dendrites. however, effects due to dcs-induced somatic polarization should be the same, regardless of the location of synaptic activation (i.e. there is only one soma per neuron). therefore, observing different effects in apical and basal compartments would rule out somatic polarization as a main determinant of the plasticity modulation."
"using the computational model we then aimed to understand how dcs modulates tbs-induced ltp, while preserving specificity and associativity. pathway specificity is explicitly built into the voltagebased plasticity rule of the model (41), following well established experimental results (35), namely synaptic weights are only allowed to change at active presynaptic inputs (see methods). since dcs does not by itself cause presynaptic activity, it cannot affect synaptic efficacy of the inactive pathway. thus, the incremental membrane polarization due to dcs upholds hebbian synapse specificity."
"during tbs on the other hand, each burst in the induction is close to threshold at the soma. somatic action potentials are generated throughout the induction, and plasticity at each synapse is dominated by the back-propagation of these spikes ( figure 7d,e; bottom two rows). effects of dcs on plasticity therefore follow the effects on somatic spike generation, regardless of the dendritic location of the synapses ( figure 7b,c; compare tbs apical to tbs basal). indeed, our experimental recordings of somatic spikes and dendritic integration in the ca1 population support this notion (figure 4 ). performing a similar analysis in the model recapitulates this result ( figure 5, c.f. figure 4d -f)."
"assume that the formulas of the straight line and its vertical line are as follows, except at least one of a and b is not zero"
"this article has presented a novel algorithm of line segment extraction using 2d laser data based on seed-segment detection and region growing. the whole algorithm is composed of seed-segment detection, region growing, overlap region processing, and endpoint generation instead of recursion and breakpoint detectors. we use two data sets taken from a hallway and a laboratory for experimental evaluation, and it is shown that compared with the wellknown split-and-merge, our algorithm presents better performance in three aspects: efficiency, correctness, and precision. future work will focus on using the extracted line segments to address the global localization and loop closure with 2d laser sensors."
"an overview of existing works of line segment extraction is presented in \"existing algorithms\" section. in \"seeded region growing\" section, it is shown how seeded region growing is related to our algorithm. finally, a summary is given to justify the proposed idea."
"to help constrain our computational model, we simulated canonical stdp results in the literature (26, 43) . first we simulated stdp by pairing spiking generated at the soma with synaptic inputs on the proximal apical dendrites (5 synapses, randomly distributed). somatic spikes were evoked by a 5 ms, 1 na current injection in the soma at varying temporal offsets from synaptic input (δt), with positive δt corresponding to pre before post pairing (pre-post) and negative δt corresponding to post before pre pairing (post-pre). synaptic weights at the end of the simulation were normalized to the initial baseline value and plotted as a function of δt ( figure s1a) . the detailed neuron model with the specified plasticity parameters (table 1 ) qualitatively reproduces the canonical stdp window ( figure s1a), where pre-post pairing leads to potentiation and post-pre pairing leads to depression. we next simulated the experimentally observed frequency-dependence of stdp (26, 43) . here we performed similar simulations but with δt fixed at either -10 or +10 ms and varied the frequency of pre and postsynaptic pairings (1, 5, 10, 20, 30, 40, 50, 75, 100 hz, figure s1b ). with respect to ca1 pyramidal cells. black traces indicate control experiments, where no electric field was applied. b) dcs has no significant effect on fepsp slopes recorded during induction. c) anodal dcs enhances population spikes recorded at the soma in response to both apical and basal synaptic activity. d) same data as in c, but showing on the first pulse during each burst of the tbs protocol. the effect of dcs is most pronounced on the first pulse. e) dcs shifts average spike timing for each burst during induction (see methods \"quantifying somatic activity\" for details) f) dcs has opposite effects on dendritic integration in response apical or basal synaptic input. the horizontal axes represent either the number of individual bipolar stimulus pulses (60 in total) or bursts (15 in total) applied to activate synapses during induction. all data normalized to the mean of the 20 baseline responses before induction and are represented as mean±s.e.m."
"in this article, we propose a novel algorithm of line segment extraction using 2d laser data based on seeded region growing. different from traditional approaches, the proposed algorithm borrows the idea of seeded region growing in the field of image processing, which is more efficient with more precise endpoints of the extracted line segments. at first, using the orthogonal least square approach together with a bearing-based condition, we present a seed-segment detection approach. once a seed-segment is detected, it will be extended to be a full line segment through the proposed region growing technique, which helps improve the computational efficiency and correctness without iterative fitting process. in addition, overlap region processing and endpoint generation determine the endpoints of line segments with high precision. experimental results show that our proposed algorithm has a better performance than the well-known split-and-merge 15, 25 in terms of efficiency, correctness, and precision. the target of this work provides basis for localization, mapping, and place recognition with high-level information, which could help solve the \"kidnapped robot\" problem and loop closure. 19 the main contributions of this article include following aspects: (1) a new line segment extraction algorithm based on seeded region growing with 2d laser data; (2) more precise endpoints of extracted line segments; (3) experimental results show that our algorithm presents better performance in three aspects: efficiency, correctness, and precision."
"in this article, we apply the idea of region growing to line segmentation, and the main goal of laser line extraction is to partition 2d laser data into some line segments. the unique property of the laser points in the same line segment is the distance from themselves to the fitting straight line, which are all less than a given threshold."
"some complete line segments are got from the 2d laser data after region growing, but two adjacent line segments may have an overlap region, as shown in figure 5, which will have some negative influence on the description of environment structure."
"timing of synaptic inputs and applied electric field for both experiment and model. c) for peak tbs (red), each burst during the tbs protocol is timed to the peak of the extracellular field, such that pyramidal cell somas are depolarized when the synaptic inputs arrive. for trough tbs (blue), each burst during the tbs protocol is timed to the trough of the extracellular field, such that pyramidal cell somas are hyperpolarized when the synaptic inputs arrive. d) example voltage traces from somatic compartment of model neuron during first two bursts of simulation. e) resulting experimental ltp in each condition. as in figure 1c, fepsp slopes are averaged over the last 10 minutes of recording in each condition. f) model ltp predictions qualitatively match (same direction of dcs effect) experimental ltp results (d). the vertical axis (norm. weight) is the average weight of all weak pathway synapses at the end of simulation, calculated offline using the learning rule (41) . figure s4 . input-output curve reveals that the baseline of our experiments is set near a nonlinearity. baseline population spike amplitude as a function of baseline fepsp slope for all slices. fepsp slopes are normalized to the maximum value detected in the process of setting baseline bipolar stimulus intensity (see methods \"fepsp recordings\"'). the horizontal axis can therefore be thought of as the fraction of activated synapses in the population. population spikes are normalized to the population spike magnitude recorded when the maximum fepsp is established. the gray box highlights approximately where baseline fepsps were set before running ltp experiments (30-40% of maximum). note that experiments are run near a nonlinearity in the input-output curve, such that system is more responsive to increases in input rather than decreases in input."
"according to this criterion of precision, comparative results of split-and-merge and the proposed algorithm are shown in figure 10, wherein it is seen that some points of line segments in the yellow circles are not detected by splitand-merge. thus, the proposed algorithm presents better performance in terms of precision."
"where nummatch is the number of matches, numextline is the number of line segments extracted, truelines is the number of true line segments in the environment. an ideal line extraction algorithm should present a high tr and a low fr. the evaluation of correctness relies on human participation by manually labeling how many true line segments in the current environment. we randomly selected 10 frames from two data sets, respectively and then recorded the results of line segment extraction, as shown in table 4 and figure 10 (wherein the green rectangles show the line segments that are not extracted successfully). it is shown that the proposed algorithm achieves better performance in terms of correctness."
"another important property of hebbian plasticity is pathway associativity, which is a cellular mechanism thought to underlie the formation of cell assemblies and associative learning (35) (36) (37) . pathway associativity refers to the potentiation of separate synaptic pathways arriving onto the same postsynaptic neuron when they cooperate to drive the postsynaptic cell. for example, a synaptic input that is too weak on its own to induce plasticity can undergo plasticity if it is coactivated with a strong input that helps to drive the postsynaptic cell."
"the results of figure 7 highlight the complex interaction between endogenous synaptic input dynamics, synapse location, and dcs-induced polarization. despite the complexity, figure 7 also points to a simple and more general principle: when endogenous plasticity is primarily driven by somatic sources of depolarization (e.g. backpropagating somatic spikes), dcs-induced polarization at the soma determines effects on plasticity. this is what we observe with tbs ( figure 7 bottom two rows). when endogenous plasticity is primarily driven by dendritic sources of depolarization (e.g. subthreshold depolarization or dendritic spikes), dcs-induced polarization at the dendrite determines effects on plasticity. this is what we observe with 20 hz tetanus ( figure 7 top two rows) or when we block somatic spiking ( figure 6 bottom row)."
"author contributions g.k. and a.r. designed and executed experiments, g.k. designed and ran simulations, g.k. analyzed data, g.k., a.r., m.b., l.p. interpreted results and wrote the manuscript ."
"to further understand how changes in membrane potential due to dcs lead to the observed changes in plasticity, we turned to a computational model. we modeled a ca1 pyramidal neuron based on a previously validated biophysical model, using the neuron software package (38) (39) (40) . to simulate the effects of dcs, we applied a uniform extracellular electric field (voltage gradient) with neuron's extracellular mechanism (23) . this extracelular field is known to polarize the cellular membrane with opposite polarities in apical and basal compartment ( figure 5a ) (18) . to calculate activity-dependent synaptic plasticity, we used a voltage-based plasticity rule (41) that has been used previously to replicate a wealth of synaptic plasticity data (41) (42) (43) . here we manually selected parameters for this plasticity rule such that we could qualitatively reproduce canonical spike-timing dependent plasticity (stdp) experiments (26,43)(supplemental figure s1 ) and the effects of dcs on synaptic plasticity in our own tbs experiments (compare experiments of figure 4d -f with model results of figure 5 ). the model also reproduces the experimental results with alternating current stimulation (compare experiment of figure 1c with model results of figure s3 ). all simulation results that follow use the same parameters unless specified otherwise (supplemental tables). figure 4d ). the vertical axis (norm. weight) is the average weight of all activated synapses at the end of simulation, calculated offline using the learning rule (41) . c) effects of dcs on somatic activity qualitatively match experimental measurements (c.f. figure 4e ). the vertical axis is the average across all neuron somas of the integral of the high-pass filtered voltage envelope (see methods). d) effects of dcs on dendritic integration qualitatively match experimental measurements (c.f. figure 4f ). the vertical axis is the average across all recorded dendritic locations of the high-pass filtered envelope of the voltage (see methods)."
"in order to evaluate the performance of our proposed algorithm, some experiments are carried out. we selected two laser scan data sets taken from a hallway and a laboratory."
"main contributions. the main contribution of this paper is to derive the convergence rate of distributed gradient algorithms under uniform communication delays between nodes. in particular, we first show that under some appropriate choice of stepsizes the nodes' estimates asymptotically converge to the solution of the problem, implying that the impact of communication delays is asymptotically negligible. this step allows us to study the rate of convergence of the algorithm, i.e., the convergence occurs at rate o"
"hippocampal brain slices were prepared from male wistar rats aged 3-5 weeks old, which were deeply anaesthetized with ketamine (7.4 mg kg −1 ) and xylazine (0.7 mg kg −1 ) applied i.p., and sacrificed by cervical dislocation. the brain was quickly removed and immersed in chilled (2-6 °c) dissecting artificial cerebrospinal fluid (acsf) solution containing (in mm): choline chloride, 110; kcl, 3.2; nah2po4, 1.25; mgcl2, 7; cacl2, 0.5; nahco3, 26; d-glucose, 10; sodium ascorbate, 2; sodium pyruvate, 3. transverse slices (400 µm thick) were cut using a vibrating microtome (campden instruments) and transferred to a chamber containing a recovery acsf at 34 °c : nacl, 124; kcl, 3.2; nah2po4, 1.25; mgcl2, 1.3; cacl, 2.5; nahco3, 26; d-glucose, 25; sodium ascorbate, 2; sodium pyruvate, 3. for dose response experiments in figure 8f, the hippocampus was dissected out and transverse slices (400 µm thick) were cut with a manual tissue chopper and transferred to the recovery chamber. after 30 minutes in the recovery solution, slices were transferred to a holding chamber containing recording acsf at 30 °c: nacl, 124; kcl, 3.2; nah2po4, 1.25; mgcl2, 1.3; cacl, 2.5; nahco3, 26; d-glucose, 25; for at least 30 minutes. finally slices were transferred to a fluid-gas interface chamber (harvard apparatus) perfused with warmed recording acsf (30.0 ± 0.1 °c) at 2.0 ml min −1 . slices were allowed acclimate to the recording chamber for at least 30 minutes before recording started. the humidified atmosphere over the slices was saturated with a mixture of 95% o2-5% co2. all acsf solutions were bubbled with a mixture of 95% o2-5% co2. recordings started approximately 2 h after the animal was sacrificed."
"while electric current does reach the hippocampus and subcortical structures during stimulation (81), tdcs is thought to primarily act on neocortex. here we chose hippocampus as a model system for the wealth of studies on hippocampal synaptic plasticity and the much neater organization of input pathways. while not identical, many excitatory plasticity mechanisms are conserved in pyramidal neurons between cortex and hippocampus (82), making our observations here informative for cortex as well. indeed, the plasticity rule used here in our model has also been used to describe plasticity at neocortical excitatory synapses (26, 41, 43) . of course, further work is needed to validate this relationship with respect to dcs effects. it is also worth noting that this work, in addition to other recent studies (12, 15, 83), motivates the hippocampus as a target for tdcs."
"first, tdcs effects in humans should similarly exhibit synaptic input specificity, which would be reflected as task specificity in the cognitive domain. indeed there is good evidence for task-specific effects of tdcs, despite its lack of spatial focality in the brain (54, 55) . this property may be central to the broad application of tdcs. it implies that tdcs can be used flexibly in combination with many different tasks and with limited side effects, despite stimulation reaching large regions of the brain. second, tdcs effects may be most pronounced when paired concurrently with training that induces plasticity. again, there is evidence for this in the human literature (33, 56) . it may be possible to leverage these properties further by pairing stimulation with forms of learning that are known to rely heavily on hebbian mechanisms (57) (58) (59) ."
"the response of an individual pyramidal neuron to the bipolar stimulus in our brain slice experiments was modeled by randomly selecting a group of dendritic segments. ampar and nmdar conductances were then activated simultaneously in the selected segments. in our experiments we expect that the bipolar stimulus will elicit this synaptic input in a population of pyramidal cells, with the number and location of synapses that are activated varying between cells."
"the organization of the rest article is as follows. in the following section, we review related work. \"line segment extraction\" section describes the process of line segment extraction using 2d laser data. the comparison criteria and experimental results are described in the \"experiment\" section. finally, some conclusion and future work are outlined."
"as shown in figure 7, the hallway only contains doors, walls, and some fire fighting equipments, which are suitable for line extraction, while there are some desks and chairs in the laboratory. the laser sensor in our experiments is utm-30lx produced by hokuyo. the main parameters of the laser sensor are detailed in table 1 . the proposed line segment extraction algorithm has been implemented on the mobile robot called pioneer-3dx shown in figure 8 . the laser sensor is mounted on the mobile robot, and we obtained two data sets from the hallway and the laboratory through manual operating the mobile robot."
"as in our previous work (14, 61) and in many tdcs studies (28, 62, 63), we observe asymmetric results with respect to dcs polarity. anodal dcs enhanced ltp, while cathodal dcs had no discernible effect with the current sample sizes. of course, the brain exhibits highly nonlinear responses to changes in membrane voltage, from the level of ion channels to the propagation of activity in a recurrent network. in this sense, it is perhaps not surprising that responses to dcs are nonlinear. however, it remains a crucial topic to understand which sources of nonlinearity are most relevant for dcs, and whether these persist in human tdcs. below we speculate on some of these potential sources, although we are unable to disambiguate them here, as it is beyond the scope of the current study."
"this framework produces a number of testable predictions for clinical experimentation. first, the efficacy of clinical protocols should improve when tdcs is paired with a learning task which induces plasticity, instead of the common practice of pairing tdcs with \"rest\". second, when tdcs is paired with a learning task, we postulate that the effects should be highly specific to the trained task. finally, the pairing of tdcs with hebbian plasticity and learning can be thought of as a method for functional targeting, since tdcs should only affect synaptic pathways that are already undergoing plasticity due to the paired task. this may alleviate the prevailing concern that focal stimulation of a desired target in the brain is not possible with transcranial electrical stimulation."
"for a given dcs waveform (polarity and magnitude), we are interested in the distribution of over all k synapses in the population. figure 8e displays the mean of this distribution as a function of dcs intensity."
"in algorithm 1, n p is the total number of laser points. s num denotes the number of laser points in a seed-segment. p min denotes the minimum number of laser points contained in an extracted line segment. seedði; jþ represents a seed-segment from i th point to j th point. specific seedsegment detection process is described in lines 5-7."
"this section presents the proposed line segment extraction algorithm using 2d laser data based on seeded region growing, which is different from previous methods. at first, orthogonal least squares are explained in the \"the orthogonal least square method\" section. the \"seed-segment detection\" section describes the process of seed-segment detection. the following subsections provide an effective solution to extracting complete line segments from seedsegments, including region growing, overlap region processing, and endpoint generation. the overall block diagram is shown in figure 3 the orthogonal least square method"
"every complete line segment is obtained from a seedsegment, which consists of a small amount of successive 2d laser points. in the whole process of line segment extraction, a precise method about seed-segment detection plays a significant role. therefore, the detection of a seedsegment is very strict, which needs to satisfy some conditions. at first, several successive laser points are used to fit a straight line by the means of orthogonal least squares, parameters (a; b; c) are obtained after the line fitting process. ideal seed-segment should satisfy following two requirements."
"where is the highpassed extracellular voltage recorded in the somatic layer, and h() is the hilbert transform calculated in python using signal.hilbert from the scipy package. tij is the onset time for the i th input of the j th burst, where and . the somatic activity was calculated as the integral of this high frequency envelope in the time window 2-8 ms after tij, chosen to avoid including the bipolar stimulus artifact. somatic activity was then normalized to the mean of baseline values (20 responses prior to induction). the same method was used to calculate somatic activity in the population of model neurons ( figure 7c ), except the recorded extracellular voltage in the somatic layer was replaced with the intracellular somatic voltage averaged over all simulated model neurons."
"where is the band-passed extracellular voltage recorded in the dendrite. for each evoked burst, j, the dendritic integration was calculated as the integral of this low frequency signal in the time window 2-100 ms after the onset of the burst, tj. dendritic integration was then normalized to the mean of baseline values calculated for each fepsp (20 responses prior to induction). the same method was used to calculate dendritic integration in the population of model neurons ( figure 7c ), except the recorded dendritic extracellular voltage was replaced with the intracellular voltage averaged over all recorded dendritic segments in the simulated population of cells."
"it is less clear however, exactly how dcs is able to boost associativity between the weak and strong pathways ( figure 3 ). we hypothesized that dcs boosted associativity through a boost of somatic spikes, which propagate to both weak and strong pathway synapses. to test this in the model, we simulated the experiments of figure 3, by activating one pathway with tbs (strong) and the other pathway with the 5 hz stimulation (weak). when the weak pathway was activated alone no spikes were generated and only very weak plasticity was observed ( figure 6d, weak only, black). applying dcs in this case led to only minor changes in plasticity, as in our experiments ( figure 6d, weak only, compare red and black). however, when the weak input was paired with the strong input, action potentials were generated in the soma that back-propagated to weak pathway synapses ( figure 6b, black), and ltp was observed ( figure 6d, weak+strong, black). therefore, the weak and strong pathway become associated by cooperating to produce somatic spikes, which are then shared by both pathways."
"electric fields are also known to alter cell motility and immune responses (79, 80) . however, these effects unfold over the course over many minutes to hours. during prolonged stimulation, it is likely that various effects on cellular physiology begin to take hold simultaneously, with interactions between them. however, robust effects were generated here with remarkably short stimulation duration (3 s), which depended on stimulation polarity with sub-second timing (100 ms, figure 1c ). polarization of neuronal membranes is the only known effect of stimulation that acts on these timescales, making it a likely source of effects here. prolonged stimulation necessarily includes effects operating on both short (membrane polarization, plasticity induction) and longer (cell motility and immune responses) timescales. however, shortening the stimulation and pairing it with quicker (sub-minute) bouts of training as we have done here, could be a useful strategy to isolate effects based on hebbian plasticity induction, which operate on faster timescales."
"mirroring the asymmetric effect of dc polarity, the effects with respect to phase of ac stimulation was also asymmetric. this suggests that even in the absence of information about the precise timing of synaptic inputs when tacs is applied in humans, a net enhancement of ltp may be expected when tacs is paired with synaptic plasticity induction. notably, the boost in ltp was also larger here for acs than dcs, perhaps owing to the frequency response properties of pyramidal neuron membranes showing a peak at theta frequencies (20, 65) ."
"all animal experiments were carried out in accordance with guidelines and protocols approved by the institutional animal care and use committee (iacuc) at the city college of new york, cuny (protocol 846.3 [cit] -24)."
"if we consider the distribution of dcs effects over all apical synapses, we find that for weak fields the mean effect of dcs is predominantly driven by the tail of this distribution, where very few synapses have large changes in plasticity ( figure 8d ). for a small number of cells that are close to threshold, a weak field may cause a spike that would have otherwise not happened. this causes a large jump in all synaptic weights for a few highly sensitive cells. while most synapses see very small effects on their weights due to small effects on spike timing and subthreshold polarization, a small number of synapses experience a large effect on their weights due to the initiation of new spikes."
": schematic of experimental design. two synaptic pathways were monitored. during induction, one pathway was weakly activated at 5 hz with 15 pulses (grey), while the other pathway was inactive (black). anodal dcs was applied throughout induction (3 s, red). bottom: weak synaptic activation had no lasting effect on synaptic strength in either pathway with dcs (red, light red) or without dcs (grey, black). b) top: schematic of experimental design. again, two synaptic pathways were monitored. now during induction, one pathway was activated with a tbs protocol (strong, black). the other pathway was activated with 15 pulses at 5 hz (weak, grey). weak pathway pulses were temporally aligned to the second pulse in each tbs burst. bottom: without dcs, the strong pathway was potentiated (black) and the weak pathway was now also potentiated (grey), demonstrating associative plasticity between these pathways. with dcs, ltp was enhanced in the strong pathway (red) and the weak pathway (light red), demonstrating that the associativity between pathways was enhanced. c) summary of ltp experiments in the strong pathway. pairing with the weak pathway did not increase strong pathway ltp, and dcs had a similar effect on ltp in both cases. d) summary of ltp experiments in the weak pathway. fepsp slopes are normalized to the mean of the 20 of baseline responses prior to induction. induction is applied at the 20 minute mark in panels a,b. the mean of the last 10 normalized slopes (51-60 min after induction) are used for each slice in panels c,d. data are represented as mean±s.e.m."
"we expected that dcs would cause a shift in the average spike timing in the population during tbs (supplemental figure s2e ). to create a measure of the mean spike timing, we performed a center of mass calculation on the somatic activity envelope where is the population spike timing of the j th burst, env is the envelope of the highpassed extracellular voltage (see above), tij is the onset time for the i th input of the j th burst, where and, and sij is the somatic activity as in the previous section. again, we restrict the integrals to between 2 and 8 ms after each input pulse to avoid contributions of the bipolar stimulus artifact."
"synaptic conductances were modified by presynaptic short-term plasticity model as in (90) . specifically, ampar and nmdar conductances were multiplied by a factor a, which captures short-term facilitation and depression dynamics at presynaptic terminals. a is the product of a facilitation variable f, and 3 depression variables d1,d2,d3"
"here we found that dcs had the same effect on ltp in both basal and apical dendrites ( figure 4c,d). this result is consistent with plasticity effects of dcs being driven primarily by effects on somatic spiking. to further test this, we looked at measures of dendritic integration and somatic spiking in each condition ( figure 4a, see methods for details of analysis). indeed, we found that dcs had a similar effect on somatic spiking ( figure 4e ), but opposite effects on dendritic integration in apical versus basal dendrites ( figure 4f ). thus the effect of dcs polarity on ltp mirrors that of the effect on the soma, but not dendrites. to derive a measure of dendritic integration, the dendritic recording was low-pass filtered, and the integral of this filtered signal was taken for each burst during tbs (gray area). to derive a measure of somatic population spiking, the somatic recording was high-pass filtered, and the integral of this signal's envelope during each burst was used (gray area; excludes periods of stimulation artefacts; see methods). b) schematic of apical (top row) and basal (bottom row) experiments. c) anodal dcs (red) boosts ltp in both and apical and basal dendrites compared to control (black). cathodal dcs (blue) had no significant effect in either apical of basal dendrites. tbs was applied with or without dcs at the 20 minute mark. note that the top panel is identical to figure 1a (shown again here for comparison). d) summary of the data in c. the mean of the last ten normalized responses were used for each slice. e) population spiking measured for the first bipolar input pulse of the last burst (see supplemental figure s2c for all pulses during induction). f) population dendritic integration for the last burst of tbs (see supplemental figure s2f for all bursts during induction). all data normalized to the mean of the 20 baseline responses before induction and error bars represent standard error of the mean."
"field excitatory postsynaptic potentials (fepsps) were evoked using a platinum-iridium bipolar stimulating electrode placed in either stratum radiatum (apical experiments) or stratum oriens (basal experiments) of ca1 within 200 μm of the somatic layer. recording electrodes were made from glass micropipettes pulled by a sutter instruments p-97 and filled with recording acsf (resistance 1-8 mω). a \"dendritic\" recording electrode was placed in stratum radiatum (apical) or stratum oriens (basal) approximately 400 µm from the stimulating electrode in ca1 to record fepsps. the stimulating electrode and dendritic recording electrode were placed at approximately the same distance from the ca1 somatic layer. for all experiments, a second \"somatic\" recording electrode was placed in the ca1 somatic layer to record population spikes. for two-pathway experiments (figures 2 and 3), a second stimulating electrode was placed on the opposite side of the recording electrode. fepsps were quantified by the average initial slope, taken during the first 0.5 ms after the onset of the fepsp. the bipolar stimulus intensity was set to evoke fepsps with 30-40% of the maximum slope, which was determined at the onset of recording. baseline fepsps were recorded once a minute for at least 20 minutes before any plasticity induction was applied and only if a stable baseline was observed. for two pathway experiments, stimulation of each pathway was interleaved with an offset of 30 s. after plasticity induction, fepsps were again recorded once per minute for 60 minutes. to measure synaptic plasticity, all fepsp slopes were normalized to the mean of the 20 fepsps immediately preceding induction. the amount of ltp in each slice is quantified as the mean of the last 10 minutes of normalized responses (51-60 minutes after induction)."
"synaptic plasticity is critical for many forms of learning and tdcs has been thought to alter synaptic plasticity (6, 48) . how stimulation may interact with ongoing synaptic activity to alter plasticity remains poorly understood. here we found that weak electrical stimulation with constant direct currents can enhance ltp, while maintaining input specificity and associativity. we propose a model in which dcs boosts endogenous hebbian synaptic plasticity through modulation of pyramidal neuron membrane potential dynamics. as this model predicts, the effects of dcs also reflect the input specificity and input associativity of the endogenous hebbian plasticity."
"in this paper we have studied a continuous-time distributed gradientbased consensus algorithm for network optimization problems, with the focus on uniform communication delays. we provided an explicit analysis on the rate of convergence of the algorithm as a function of the network size, topology, and communication delays, specifically the convergence time of the algorithm grows as a cubic function of the delays."
"hough-transform, 19 and so on. a comparison of line segment extraction for indoor mobile robotics has been put forward in the work, 15 which concludes that split-andmerge provides the best performance among existing algorithms via comparative experiments."
"schematic of the experimental setup, showing the orientation of anodal (red) and cathodal (blue) electric fields. location of stimulation (stim) with tbs and recording (rec) of field excitatory postsynaptic potentials (fepsp) are indicated relative to a ca1 pyramidal neuron soma (black triangle). bottom: membrane polarization throughout a model pyramidal neuron in response to 20 v/m anodal (red) or cathodal (blue) dcs. green compartments are depolarized due to dcs, while magenta compartments are hyperpolarized by dcs. b) constant current stimulation applied during tbs modulates the resulting ltp measured as a change in fepsp normalized to baseline. c) alternating current stimulation (5hz) was applied and tbs bursts were timed to either the peak (red) or the trough (blue) of the sinusoidal alternating current. note that the applied electric field at the peak of the alternating current is identical to anodal constant current, as is the case for the trough of the alternating current and cathodal constant current. the effects of alternating currents are similar to those of the analogous constant current paradigm, indicating that plasticity modulation is consistent with the instantaneous incremental membrane polarization on millisecond timescale. ltp induction is applied at the 20 minute mark. all data are normalized to the mean of the 20 baseline responses before induction and are represented as mean±s.e.m."
"this paper proposed a postfault controller for scalar controlled asymmetrical six-phase induction machines. the proposed controller has the same structure under either healthy or fault conditions. it was proved that the machine could optimally be controlled during open phase fault condition under either open-loop or closed-loop speed control modes. the proposed scalar v/f-based controller does not require accurate information about the machine parameters while provides a robust and straightforward control in applications that do not require a high dynamic response. it also comprises a smaller number of current regulators than foc-based controllers. although the 1n connection offers a higher steady-state fault tolerant capability than the 2n connection under fault conditions, the former exhibits a sluggish starting response under open loop speed control due to the effect of the third harmonic components caused by zero sequence excitation. this effect has been explored using experimental investigation since it was commonly neglected in the available machine models. under closed-loop speed control, the machine dynamic response can, however, be assumed identical for all cases. as far as the current waveform quality is concerned, the 1n connection generally corresponds to a higher distortion in the current waveforms, which is mainly due to the third harmonic induction caused by zero sequence excitation. this effect can, however, be compensated, if needed, by employing a multiple resonant current controller structure, which will, of course, entail a higher number of current regulators."
"unlike foc-based controllers, the αβ current components are not directly controlled when scalar v/f control is employed. hence, different low order space harmonics will cause a notable distortion in the current waveform, which will indeed be higher than the foc-based control case. moreover, the current waveform distortion under 1n arrangement is relatively higher than the 2n configuration. this is mainly due to the induced third harmonic air gap flux component due to zero-sequence excitation, which is, in nature, a pulsating flux component [cit] . this component may also saturate the core and cause this notable distortion in the αβ current components. the 2n may, therefore, be preferable to avoid this notable current distortion."
"the extended multiphase version of some other controllers commonly proposed for three-phase machines has also been introduced such as direct torque control (dtc) [cit] and sliding mode control (smc) [cit] . recently, the finitecontrol set model predictive control (fcs-mpc) was shown promise to provide a fast-dynamic response with relative flexibility to include some operational constraints [cit] . it is worth mentioning that many other innovative controllers, which have been recently proposed in the literature, are still limited to three-phase machines [cit] ."
"the proposed controller is investigated under different postfault scenarios as well as neutral point configurations. the healthy case is used as a benchmark reference case. under fault conditions, phase a 1 is physically disconnected to emulate a single open line circuit fault. the machine is operated under closed loop speed mode, with the switch s set to position 2, while the reference speed is set to 1000 rpm. the constant gain matrices k xy and [k 0 ] are selected from table 1 based on the selected operating mode. under healthy case, 2n configuration is used while the reference xy and zero sequence current components are set to zero. successfully ensures the desired reference optimal currents under all cases. the corresponding sequence current components are given in fig. 6 . the obtained current waveforms agree with a large extent to their optimal reference currents given in table 1 ."
"in this paper, a postfault controller based on conventional scalar v/f control is proposed for an asymmetrical six-phase induction machine. the proposed controller offers the following merits over standard foc-based postfault controllers:"
"in induction machines, distributed windings are typically employed. hence, primitive harmonic-free models for the xy and zero subspaces are usually assumed, while the machine dynamic performance mainly depends on the αβ subspace."
"several control techniques concerning the multiphase motors in general and the six-phase machines, in particular, have been proposed in the literature. most of them were, in essence, excerpted from the conventional three-phase controllers but extended to regulate the higher phase order machines. in the standard field oriented control (foc), the torque/flux regulation is carried out using the current components of the fundamental αβ subspace [cit] . the reference secondary current components of the secondary xy subspace are commonly set to zero under normal operation to avoid excessive circulating currents [cit] ."
"the proposed postfault scalar controller is verified using the six-phase prototype machine given in table 2 . two three-phase voltage source inverters connected to the same dc-link are used to supply the machine stator. the dc-link is constructed using a programmable dc source, while conventional pwm modulation at 5khz switching frequency is employed to control the two inverters. the im is mechanically loaded using a coupled pm dc generator, while its terminals are connected to load bank resistors. the whole control algorithm is implemented using a digital signal processor board ezdsp tm based on texas instruments f28335 dsp. a can bus connected to the dsp serial port is also used for the online measurements including rms line currents, speed, and sequence current components. under the 1n arrangement, the summation of all phase currents should be zero. while under the 2n arrangement, the summation of each winding group adds to zero. the machine phase currents i b1, i c1, i a2, and i b2 are therefore measured using a digital oscilloscope, while the remaining two phases are then estimated based on the employed winding arrangement. the corresponding sequence current components are extracted using matlab. for display purposes, all measured currents are filtered out using a low pass filter with a cut-off frequency of 2khz. the pi and pr regulators are tuned via trial and error, and the controller gains are given in table 3 ."
"in standard irfoc [cit], the αβ current components are derived from the reference dq current component after applying park's transformation. the other reference sequence current components are, therefore, derived based on (9). the measured currents are transformed to their sequence components using (1) . then, the voltage components of each subspace are obtained using a pair of proportional resonant (pr) controllers; dual-pi regulators in synchronous and anti-synchronous reference frames can also be employed [cit] . volume 6, 2018 thus, the foc-based controller will entail three pairs of dual-pr controllers to ensure same controller structure under both healthy and postfault conditions while fully controlling all sequence current components."
"in the proposed scalar controller, the conventional v/f control is used to derive the αβ voltage components of the fundamental subspace, as depicted by the controller block diagram shown in fig. 2 . under healthy conditions, this will, in fact, be the same as a conventional three-phase case. this controller can, therefore, be used under either open or closed loop speed control modes. for the former, the reference frequency ω * s is set based on the desired no-load speed; as the machine is mechanically loaded, the machine speed drops [cit] . under closed loop speed control, the machine speed error is used to generate the rotor slip frequency, ω sl, which is, in turn, used to calculate the reference stator angular frequency ω * s . under healthy conditions, the αβ voltage components of the winding phase voltages will be equal to the reference αβ components of the inverter leg voltages. assuming a balanced stator winding, the corresponding αβ current components should, therefore, be balanced. during open-phase(s) conditions, if the reference αβ voltage components are kept balanced, the corresponding αβ current components can still remain balanced as long as the other non-fundamental subspaces are properly controlled [cit] ."
"in high-power adjustable speed drives, the six-phase induction machines have usually been laboring in many industrial applications such as ship propulsion, and oil and gas applications, where high-reliability standards are mandatory [cit] . although the field-oriented control proved itself optimum to obtain a high torque response performance in the electric drive arena, it entails accurate machine parameter identification and complex controller structure. on the other hand, it seems fair to admit that the simple v/f scalar control is still widely being used especially if a simple control approach is required [cit] and/or constant load is experienced such as fans, pumps, blowers, mixers, extruders, mills, hoist drives, excavators, test stands, ships' drives, conveyor belts, etc. although this control method is a rather simple while independent of machine parameters, the torque response under low-speed operation will likely be slow. nevertheless, the scalar v/f control technique is still widely adopted in different multiphasebased industrial applications [cit] ."
"the machine response under closed-loop speed control is shown in fig. 9 . the machine is initially started at 500rpm while mechanically loaded with an initial load torque of 2 nm. at 1s, the machine reference speed is step increased to 1000 rpm. rated load is then applied after a while. the machine speed profiles under step-speed change and mechanical loading are shown in fig. 9, which seem similar to the healthy case for all postfault operating modes. the corresponding profiles of the rms line currents are given in fig. 10 . clearly, by controlling the machine speed, the sluggish response shown in fig. 7 and obtained under free-running open-loop speed control due to the effect of nonfundamental subspaces is entirely avoided, while the machine dynamic response is quite similar to the pre-fault (healthy) case."
"according to the available literature, the six-phase machine is usually controlled under postfault operation to generate rated airgap flux, while backward flux component is eliminated."
"in this subsection, the machine dynamic response under either open-loop or closed loop speed controller is investigated. the proposed controller is first investigated during machine starting under free-running open-loop speed control. the machine is started at a specific constant voltage and frequency while mechanically unloaded. after the machine reaches its final no-load steady-state speed, a step load is then applied. this test can give a clear clue to the effect of the low order space harmonics of different subspaces on the machine dynamic response [cit] . under direct starting, the inrush starting currents may saturate the current measurement boards. hence, this test is carried out under a lower phase voltage to avoid this problem. fig. 7(a) shows the starting speed profiles of the machine speed under different cases. while fig. 7(b) shows the effect of step loading on the machine speed profile under the same cases. under open-phase conditions with 2n arrangement, the machine dynamic response is very close to the healthy case, as clear from fig. 7(a), which proves the effectiveness of the proposed controller. on the other hand, the machine under 1n connection exhibits a longer starting time due to the effect of the subsynchronous speed point caused by the induced third harmonic component [cit] . this represents the second notable effect of a single neutral arrangement. however, this sluggish response can be avoided by using a ramp starting or under closed-loop speed control. fig. 8 shows the rms line currents for the five considered cases. under healthy case, the line currents will be equal. however, a small difference in current magnitude between the two three-phase winding sets is noted, which is mainly due to different winding asymmetries. under open-phase case, the mt mode corresponds to approximately equal line currents with magnitudes of 1.73 times and 1.44 times the healthy line current magnitude for the 2n and 1n configurations, respectively. under ml mode, the line currents are generally different. the current magnitudes of some of these phases are within the healthy current magnitude; however, the current magnitudes of phases i a2 and i b2 are 1.8 times the healthy current magnitude case for the 2n configuration. under 1n connection, the current i a2 is the phase current which has the maximum current magnitude (1.86 times the healthy current) for same rotor speed. finally, the current distortion due to the induced third harmonic component under this connection will cause a relatively small deviation from the optimal reference currents, as clear from fig. 8 ."
"the process that divides the network into interconnected substructures, called clusters. each cluster has a particular node elected as cluster head (ch) based on a specific metric or a combination of metrics such as identity, degree, mobility, weight, density, etc. the cluster head plays the role of coordinator within its substructure. each ch acts as a temporary base station within its cluster and communicates with other chs [cit] . a cluster is therefore composed of a cluster head, gateways and members node."
"where a and g represent acceleration and gravitational acceleration, respectively. when a vertical acceleration is the maximum in the local area, we regard it as a peak. one problem is that, due to noise, there are many false peaks, which lead to overload statistics. a variance threshold is used to remove false peaks. therefore, two detection conditions, defined as follows, are presented to improve the performance of the step counting model:"
"the main drawback of the triangle model is that the double integral easily enlarges the error of the step length. another drawback is that the triangle model does not take into account the influence of the walking frequency on the step length. taking into account the influence of the walking frequency and the acceleration variance, the step length based on training model is calculated as follows:"
"in order to further prove the feasibility of the improved algorithm, a more complex experiment was carried out. the walking distance was about 400 m. the positioning result and cdf are shown in figures 13 and 14 . the experimental results show that the problem of directional drift was well solved. the average error, rmse, maximum error and cep(95%) were 3.12, 3.81, 8.97 and 7.97 m, respectively. when the position error was less than 3 m, the probability of error for the proposed algorithm was 37.7%. from the results of the walking experiment, it can be seen that the performance of the indoor positioning system had been improved."
"author contributions: all authors have made a contribution to this paper. j.c., g.o. and a.p. proposed the main idea. j.c. performed the walking experiments and drafted the paper. a.p., l.z. and j.s. were involved in the algorithm design."
"we have illustrated the benefits of using pqlseq to perform glmm analysis on rna sequencing and bisulfite sequencing data. we have shown that pqlseq is the only method currently available that can produce unbiased heritability estimates for sequencing count data. in addition, pqlseq is well suited for differential analysis in large sequencing studies, providing calibrated type i error control and more power than standard lmm methods. pqlseq is implemented as an r software package with parallel computing capacity, can accommodate both binary and continuous predictor variables, and can incorporate various biological or technical covariates as fixed effects. with simulations and real data applications, we have shown that pqlseq is a useful and efficient tool for analyzing genomic sequencing datasets that are becoming increasingly common and increasingly large."
"here, we develop a new method and a software tool to enable scalable and accurate inference with glmm for large-scale rnaseq and bsseq studies. we also perform extensive simulations to comprehensively evaluate our method together with several other existing methods in various simulation settings to give out recommendations on glmm based differential analysis and heritability estimation for practitioners. our newly developed method is based on the penalized quasi-likelihood (pql) approximation algorithm [cit], applies to glmm with two or more variance components, and with an efficient implementation, is capable of utilizing the parallel computing environment readily available in modern desktop computers. with the multiple-thread computing capability, our method can improve computation time for glmm analysis of large-scale genomic sequencing data by at least an order of magnitude, making glmm based differential analysis and heritability estimation applicable to hundreds or thousands of individuals. importantly, as we will show below, our method is currently the only available method that can produce unbiased heritability estimates for sequencing count data. we refer to our method as the penalized quasi-likelihood for sequencing count data (pqlseq). with extensive simulations and comparison with lmm or other existing glmm methods, we illustrate both the advantage and limitation of our method. finally, we apply our method to analyze a large-scale rnaseq study in the hutterites."
"(l − h) represents the height of the smartphone from the ground when the pedestrian is walking. the vertical displacement of the smartphone is h, which is calculated as follows:"
"(1) the horizontal displacement in a smartphone is estimated by using a pure ins algorithm without using a foot-mounted attachment. meanwhile, the system does not require the installation of infrastructure, greatly reducing the time and economic costs of indoor positioning systems."
"where t 1 and t 2 represent the start and end times of each step; a z is the vertical acceleration. according to the pythagorean theorem, the step length is calculated as follows:"
member node (ordinary nodes): is a node that is neither a ch nor gateway node. each node belongs exclusively to a cluster independently of its neighbors that might reside in a different cluster.
"is still unknown and it can be estimated by a representative value from the prior, which may be chosen the mean, the mode or a random draw from the prior. therefore,"
"m. anupama and b. sathyanarayana [cit], analyzed, compared and classified some clustering algorithms into: location based, neighbor based, power based, artificial intelligence based, mobility based and weight based. they also presented the advantages and disadvantages of these techniques and suggest a best clustering approach based on the observation and the comparison."
"where i 0 and β 0 represent the initial brightness intensity and initial attraction, respectively. γ represents a fixed optical absorption coefficient. r is the distance between two fireflies. low brightness firefly, i, is attracted by high brightness firefly, j, according to the following [cit] :"
"during a user's walking, the inertial sensors collect the user's acceleration and angular velocity at a predetermined sampling frequency. the motion equation for the inertial sensor error model is constructed as follows [cit] :"
"the optimal result in eq. (13) cannot be found, because the posterior is not yet available. that is why the choice of importance density remains a filter design decision that has no unique global answer, and it is addressed differently in several variants of pf [cit] . in the following, four variants of pf are developed with the ability to receive stochastic input."
"when a user walks, vertical acceleration has a certain periodicity. the steps are counted by detecting the peak of the vertical acceleration. two peaks are taken as a step. vertical acceleration is calculated as follows:"
"weight based clustering techniques use a combination of weighted metrics such as: transmission power, node degree, distance difference, mobility and battery power of mobile nodes… etc. the weighting factors for each metric may be adjusted for different scenarios. some of these algorithms are presented next."
"a mobile ad hoc network (manet) consists of a group of mobile nodes that self-configure to form a temporary network without the aid of a preset infrastructure or centralized management. such networks are characterized by: dynamic topologies, existence of bandwidth constrained, variable capacity links, and energy constrained operations and highly prone to security threats. due to all these features routing is a major issue in mobile ad hoc networks [cit] ."
"routing in a network is the process of selecting paths to send network traffic. routing can take place either in a flat structure or in a hierarchical structure [cit] . in a flat structure [cit], all nodes in the network are in the same hierarchy level and thus have the same role. although this approach is efficient for small networks, it does not allow the scalability when the number of nodes in the network increases. in large networks, the flat routing structure produces excessive information flow which can saturate the network [cit] . hierarchical routing protocols [cit] have been proposed to solve this problem among others. this approach consists of dividing the network into groups called clusters. this results in a network with hierarchical structure. different routing schemes are used between clusters (inter-cluster) and within clusters (intracluster). each node maintains complete knowledge of locale information (within its cluster) but only partial knowledge about the other clusters. hierarchical routing is a solution for handling scalability in a network where only selected nodes take the responsibility of data routing [cit] . however, hierarchical approaches undergo continual topology changes. thus, topology management plays a vital role prior to the actual routing in manet. cluster based structure (hierarchical structure) in network topology has been used to improve the routing efficiency in a dynamic network [cit] ."
"in this survey, we first presented fundamental concepts about clustering, including the definition of clustering, design goals and objectives of clustering schemes, advantages and disadvantages of clustering, and cost of network clustering. then we classified clustering schemes into five categories based on their distinguishing features and their objectives as: identifier neighbor based clustering, topology based clustering, mobility based clustering, energy based clustering, and weight based clustering. we reviewed several clustering schemes which help organize manets in a hierarchical manner and presented some of their main characteristics, objective, mechanism, and performance. we also identified the most relevant metrics for evaluating the performance of existing clustering schemes. most of the presented clustering schemes focus on important issues such as cluster structure stability, the"
"(3) using handheld motion gesture, we performed the walking experiments in three scenarios at a local university. an average positioning accuracy of 2.14 m was obtained."
"fa, derived from simulating the natural phenomenon of fireflies in nature at night, was first proposed by yang [cit] . fa shows that fireflies with high brightness attract fireflies with low brightness. by attracting low light fireflies, they move toward high brightness fireflies. in firefly activity, each firefly searches for food through the distribution of fluorescein. in general, the brighter the firefly, the stronger the appeal. eventually, many fireflies are gathered around some brighter fireflies. in the artificial firefly optimization algorithm, each firefly is regarded as a solution of the solution space. the firefly population is randomly distributed in the search space as the initial solution. three basic principles for fa are as follows: (1) ignore gender; a high brightness firefly attracts a low brightness firefly; (2) firefly attraction is proportional to brightness; for any two fireflies, the brighter one attracts the other; however, brightness decreases as the distance increases; (3) if no one is brighter than a given firefly, it moves randomly."
"to evaluate and compare performance of the developed filters, the well-known non-stationary growth model (ngm) [cit] will be utilized. we have enhanced this model with non-uniform steps ∆ and including the input for performance evaluation of the systems with stochastic inputs [cit] ."
"a flexible weight based clustering algorithm (fwca) uses a combination of metrics (with different weights) to build clusters. node degree, remaining battery power, transmission power, and node mobility are used in chs election process. the cluster size does not exceed a predefined threshold value. during cluster maintenance phase, fwca uses the clusters capacity and the link lifetime instead of the node mobility because the link stability metric affects the election of a ch with the same weight as the node mobility metric."
"a novel clusters algorithm [cit] which guarantees longer lifetime of the clustering structure. the main idea is to estimate the future mobility of mobile nodes so that the ones that will exhibit the lowest estimated mobility will be chosen as chs. combining the mobility prediction scheme with the highest degree clustering technique, the authors proposed a distributed algorithm that builds a small and stable virtual backbone over the whole network. this algorithm creates clusters highly resistant to node mobility. the node with the highest weight among its neighbors is declared as the ch. this algorithm eliminates the problem of frequently changing ch due to node mobility, by allowing a node to become a ch or to join a new cluster without starting a re-clustering phase."
"mobility-based d-hop clustering algorithm (mobdhop) [cit] divides the network into d-hop clusters based on relative mobility metric. the objective of creating d-hop clusters is to supports larger than one-hop radius clusters which reduces the number of cluster heads. the relative mobility is estimated based on the signal strengths of received packets. the distance between two nodes is estimated using the signal strengths of the received packets exchanged. the cluster formation process is divided into two stages: discovery stage and merging stage. during the discovery stage, mobile nodes with similar speed and direction are grouped into the same cluster. the merging phase is invoked in order to either merge clusters together or join individual nodes to a cluster. the cluster maintenance process is invoked when a node switches on and joins the network or a node switches off and leaves the network."
"instead of a continuous mathematical function, the posterior density in eq. 5 can be numerically represented by a set of weighted scenarios, known as particles."
"in terms of scalability, existing algorithms to fit glmm are generally computationally expensive due to an intractable highdimensional integral in the glmm likelihood [cit] . for example, the frequentist method mixed model association for count data via data augmentation algorithm (macau) relies on a bayesian strategy of markov chain monte carlo (mcmc) sampling to numerically approximate the integration in glmm. however, though accurate, mcmc based strategy is computationally inefficient for large sample size: it takes macau several days to analyze moderate-sized rnaseq or bsseq data with a few hundred individuals. to overcome the computational bottleneck of mcmc-based approaches, recent studies have started to explore alternative approximation strategies to fit glmm. for example, in bisulfite sequencing studies [cit], the mixed model association via a laplace approximation algorithm (malax) relies on a laplace approximation to improve computational speed. however, the computational improvement of malax over macau is relatively marginal (approximately 2-folds). in nongenomics sequencing settings, a score test based approximate algorithm has also been recently developed to apply glmm to analyze large-scale gwass [cit] . however, score test based strategy is not well-suited for genomic study setting where the null model varies for every genomic unit tested (e.g. gene or cpg site). therefore, scaling up glmm to thousands of individuals remains a challenging task."
"as shown in figure 1, an ins module and an fpf module compose the indoor positioning system. in an ins module, angular velocity is used to estimate the user's gesture through a quaternion vector and a direction cosine matrix. vertical acceleration is extracted from the acceleration and gravitational acceleration. if zero vertical velocity is detected, we construct kf using a pseudo-observed value with zero vertical velocity and zero change in heading. the estimated error is used to compensate for the hybrid step length and heading angle. dead-reckoning is often used to calculate the user's position. in an fpf module, heading angle and step length are used to construct a pf motion model. the floor plan in the database is designed to construct the measurement model. in the resampling model, the floor plan is designed to detect the effectiveness of the particles. if the particles cross the obstacle, fa is used to move particles that have crossed an obstacle to particles that have not crossed an obstacle. subsequently, the particle weights are updated based on an exponential function. finally, fpf is used to estimate the user's position."
"in terms of accuracy, existing algorithms to fit glmm rely on different approximations and these different approximations may work well in different settings. for example, in the field of biostatistics, it has been shown that while some glmm algorithms may produce accurate p-values for differential analysis tasks in small studies, other glmm algorithms rely on asymptotic properties of the likelihood and can only produce accurate p-values when sample size is relatively large [cit] . therefore, exploring the behavior of different glmm algorithms in different settings will be informative for practitioners. in addition, as we will show below, existing glmm algorithms in genomic sequencing studies cannot yet provide accurate heritability estimates."
"where a z (k) and a z_var (k) denote the vertical acceleration and the vertical acceleration variance at time k, respectively. thd is a threshold. if both detection results are \"1\", we believe that the real peak is detected."
"because of sensor bias, non-orthogonality and temperature drift, heading drift cannot be avoided. kf reduces a part of the error when zero vertical velocity is detected. however, the positioning error still accumulates over time without boundaries. the step length and direction for a user, which include noise during walking, satisfy the gaussian distribution with zero mean."
"note that ω 1, ω 2 and ω 3 represent weighting factors on sl tri, sl tra and sl, respectively. the thresholds sl cor τ and sl step τ are designed to determine the correlation sl cor ∆ and variation sl step ∆, respectively. figure 3 shows the cumulative distribution function (cdf) of positioning results with different step length models. the improved step length algorithm combines the advantages of different step length models and reasonably uses the triangular model and training model in different stages. therefore, the improved step length algorithm is obviously superior to the other two step length models. step length model."
"in this paper, two innovations were proposed to improve the accuracy of indoor positioning. the first was that the hybrid step length model improved the accuracy of step length estimation. the core of the algorithm was to choose the appropriate step length at different times. the second was that the improved particle filter improved the indoor positioning performance. through experimentation, the performance of our proposed algorithm was demonstrated. three walking experiments were performed in a teaching building, a study room and an office building. the errors for three walking experiments are listed as follows: 1.5, 1.8 and 3.12 m, respectively, for the average error; 1.6, 2.27 and 3.81 m, respectively, for rmse; 2.85, 5.59 and 8.97 m, respectively, for maximum error; 2.44, 4.75 and 7.97 m, respectively, for cep (95%). the four performance metrics imply that the improved algorithm improves the performance of the indoor positioning system and can meet the needs of most users."
"where α, β and γ are weighting parameters for step length estimation. f denotes the walking frequency. a and a denote acceleration and the mean value, respectively. different step length models contain errors due to the user's mobility and sensing capability. it is very important to determine the appropriate step length in order to improve the positioning accuracy. therefore, we establish a new algorithm that uses two different measurements to find a reasonable step length for a user. the core idea of the algorithm is to select the appropriate step length based on the correlation of the two step length models and the fluctuation of the step length of a triangular model. the proposed algorithm is expressed as follows:"
"1. collect acceleration and angular velocity. 2. load a floor plan information in the database. 3. fpf initialization. 4. particle state transition. 5. according to the position relationship between the particles and a floor plan, the particles are divided into crossing-obstacle particles and non-crossing-obstacle particles. 6. use fa to modify the crossing-obstacle particles. 7. update the particle weights. 8. resampling. 9. estimate the location of a user. 10. if the condition is not finished, go to step 4; otherwise, end fpf."
"we assume p l and p h are from crossing-obstacle particles, c l, and non-crossing-obstacle particles, c h, respectively. p s represents the modified particles. when a crossing-obstacle particle is detected, fa is triggered. fa is calculated as follows:"
"however, some problems must still be addressed. combined with a floor plan and fa, our proposed pf adds an external procedure, which increases the computational burden. in addition, how to improve positioning accuracy in harsh environments warrants further consideration."
"based on the principle of particle resampling, particles with high weights have high sampling probability. on the contrary, particles with low weights have low sampling probability, suggesting that the particles with high weights are closer to the real location of the user. increasing the number of particles, in addition to improving positioning accuracy, also increases the computational burden. to reduce computational complexity and improve positioning accuracy, we designed fpf."
"the first walking experiment was conducted in the hallway on the third floor of the teaching building. the walking distance was about 90 m. with the handheld gesture, the trajectories of the tester are shown in figures 7 and 8 . in ins, sensor bias, temperature drift and the tester's jitters all caused positioning errors to increase over time. according to the characteristic that vertical velocity and heading change of the tester should be zero at a particular moment, kf was used to reduce the position error. as seen from figure 7, although kf reduced the position error, the accumulated error of ins still grew over time. the walking experiment, illustrated in figure 7, suggests that additional information is needed to reduce the ins error. combined with the floor plan, an indoor localization system using fpf was proposed to reduce the position error. in fpf, through the fa operation, crossing-obstacle particles migrated to particles that do not cross an obstacle. as seen in figure 8, the tester's locations are in the hallway. compared with kf, fpf greatly improved the positioning accuracy. in order to evaluate the performance of the improved algorithm, four performance metrics (the average error, root mean square error (rmse), maximum error and circular error probability (cep)) are defined as follows:"
"kf has been successfully applied in indoor localization systems to improve positioning accuracy [cit] . in a handheld smartphone positioning system, kf is often used to estimate the vertical velocity error [cit] . when zero vertical velocity is detected, pseudo-observations are used to construct a kf. a nine-element error state vector is defined as:"
"therefore, exploring the use of other glmm algorithms may help identify algorithms that are particularly well suited for small data. studies have shown that the recently developed integrated nested laplace approximation (inla) algorithm can provide accurate parameter estimates in non-genomic settings [cit] . while inla is a bayesian method, we can pair the inla algorithm with the main idea of macau to rely on the difference of the posterior and the prior to enable frequentist estimation. by extracting the likelihood as the difference between the posterior and the prior, inference will no longer depend on the prior specification. therefore, exploring the use of inla or other glmm algorithms may facilitate the application of glmm to small datasets in the future."
"there are several algorithms in the literature for cluster heads election in mobile ad hoc networks: lowest-id [cit], highest-degree [cit], distributed clustering algorithm [cit], weighted clustering algorithm (wca) [cit] and distributed weighted clustering algorithm (dwca) [cit] ."
"the battery power of node is a constraint that affects directly the lifetime of the network, hence the energy limitation poses a severe challenge for network performance. ch performs special tasks such as routing causing excessive energy consumption. next, we discuss some existing energy based clustering algorithms."
"in both bsseq and rnaseq based simulations, we found that all three glmm methods (pqlseq, macau and malax) are more powerful than lmm method (gemma) across a range of simulation settings. the higher power of glmm methods comes from their proper modeling of sequencing count data as demonstrated in previous studies [cit] . among the different glmm methods, we found that the performance of pqlseq, macau and malax are almost identical to each other when sample size is small (n 300), regardless of heritability values ( supplementary fig. s9 ), pve ( supplementary fig. s10 ), read counts ( supplementary fig. s11 ) and over-dispersion variance (supplementary fig. s12 ). the similarity in power between macau and malax in bsseq based simulations are consistent with the previous study [cit] . however, macau/pqlseq can be slightly more powerful than malax when sample size is large (n ! 200; supplementary fig. s7 ). the power comparison results also suggest that, despite the difference in type i error control, both pqlseq and macau rank genes or sites similarly well in terms of their differential expression or differential methylation evidence, thus producing similar power at a fixed fdr for differential analysis."
there are different choices of kernel functions among which epanechnikov and gaussian functions are commonly used for regularization [cit] . the process of rpf technique for state estimation is outlined in table 3 .
"pf is independent of the system model, which is used for a variety of non-linear and non-gaussian models. pf plays an important role in many fields [cit], such as pedestrian tracking, robot navigation and process monitoring. the main bottleneck of pf is the particle impoverishment problem, caused by a reduction in particles. increasing the number of particles is one way to solve this problem. however, the computational complexity is also increased significantly. for an ins/floor-plan indoor localization system, too many crossing-obstacle particles fail to estimate the real location of a user. fa points out that high brightness fireflies attract the low brightness fireflies. with fa as an inspiration, the main idea of this paper is to move crossing-obstacle particles to non-crossing-obstacle particles."
"gerla and tsai proposed a protocol called high-connectivity clustering (hcc) [cit] based on the degree of connectivity to construct clusters. in this protocol the node with the highest number of neighbors is selected as the cluster head. if two nodes or more have the same degree of connectivity, the node with the lowest id is elected as a cluster head. hcc generates a limited number of clusters. in mobile environment, this algorithm increases the number of re-affiliations of chs because their degree changes very frequently."
"because when users walk indoors, it is impossible to cross obstacles, a floor plan is used to check whether the particles, which represent the location of a user, are effective. when particles do not cross an obstacle, we think the particles are effective and have high weights; if they cross an obstacle, the particles are invalid and have low weights."
"in the process of walking, when a smartphone is at the highest or lowest point in the vertical direction, a moving platform is stationary relative to the earth. therefore, the vertical velocity and the change of direction should be zero. if zero vertical velocity is detected, the measurement equation for the inertial sensor error model is constructed as follows [cit] :"
input: acceleration and angular velocity from the inertial sensor 1. extract the vertical acceleration. 2. detect zero vertical velocity point. 3. loop kf 4. use estimated sensor error to compensate measurements. 5. update the quaternion based on the angular velocity. 6. use the update quaternion to calculate the attitude of the navigation system. 7. update the state transition matrix. 8. calculate kf gain when zero vertical velocity is detected. 9. update the filter state covariance matrix. 10. end kf 11. calculate the step length. 12. calculate a user's position using dead-reckoning.
"where r z, v z represent the vertical displacement and the vertical velocity, respectively; roll, pitch, yaw denote the user's attitude angle; v z represents the bias for the vertical velocity; roll, pitch, yaw represent the bias for the attitude angle."
"both the above two applications of glmm for differential analysis and heritability estimation require accurate and scalable inference algorithms to accommodate the increasingly large genomic sequencing studies that are being collected today. indeed, several genomic projects have already collected sequencing data on hundreds of individuals [cit], and the recent topmed omics sequencing project further aims to sequence a few thousands of individuals in the next couple of years. compared with small sample studies, large genomic sequencing studies are better powered and more reproducible, and are thus becoming increasingly common in genomics. in addition, large-scale population sequencing studies pave ways for accurate estimation of heritability for various molecular traits. unfortunately, existing algorithms for fitting glmm in genomic sequencing studies are not scalable. in addition, as will be shown in the present study, existing glmm algorithms do not always produce calibrated p-values for differential analysis nor accurate heritability estimates."
"inspired by the above work, we propose fpf for an ins/floor-plan indoor localization system. combined with a floor plan, an improved firefly operation is designed to move low-weight particles to high-weight particles. in addition, a novel particle weight updating algorithm is designed. finally, the indoor localization system performance is greatly improved."
"the kf calculation process consists of two parts: time update and measurement update. the time update begins when the inertial sensors collect acceleration and angular velocity. the measurement update is triggered when the zero vertical velocity is detected; otherwise, the measurement update does nothing. to improve localization accuracy, kf is applied to estimate the inertial sensor error. the calculation sequence for kf is shown as algorithm 1."
"using fingerprints for indoor positioning is a popular method [cit] . commonly-used fingerprints include magnetic fields and radio frequency signals (e.g., wifi, bluetooth, radio frequency identification). in the off-line phase, a surveyor needs to collect the fingerprint signals of a predefined trajectory in advance. to build a position fingerprint database, a constant speed assumption is necessary. in the online phase, the position of a user is estimated by calculating the similarity between the measured signals and the fingerprint in the database. single-point matching [cit] and sequence matching [cit] are commonly-used techniques for computing two sequence similarities. different positioning accuracy depends on different positioning environments, sensors and testers. radar [cit] obtained a positioning accuracy of 2-5 m on the second floor of a three-story building. wls [cit] obtained a positioning accuracy of 2.03 m under calling, dangling, handheld and pocket gestures. maloc [cit] obtained a positioning accuracy of 1-2 m in four scenes (hall, conference, corridor and library). the premise of fingerprint matching is that the signal is affected by the spatial position, resulting in different signal strengths at different positions. taking into account that the changes in the indoor environment have a great impact on fingerprint positioning, researchers need to update the fingerprint database within a certain period of time. another bottleneck is that fingerprint matching costs excessive computation. limited smartphone battery power poses a challenge for continuous positioning over an extended period."
"monte carlo method has been applied to sequential bayesian state estimation framework for general class of dynamical systems with first order markovian model. the resulting filter, i.e. pf can estimate the state considering the noisy input of the systems. in addition to the generic pf, three other variants of this filter, i.e. apf, rpf and rapf were developed and presented. to verify the performance of the filters in a one-dimensional state-space, an extended form of the well-known ngm has been employed for simulating a highly nonlinear system behaviour. to verify the filtering performance with a higher dimension, the ngm was utilized such that the input, the state and the output variables were taken complex variables. results of applying the developed filters on the ngm model shows that rpf provides the highest accuracy for the simulation model under study, for both one-dimensional and two-dimensional models. it should be noted that filtering performance is depended on the individual systems, so that a filter with superior performance in one system may show poor results on another system [cit] ."
"where θ(k) and sl(k) represent the direction and step length at time k, respectively. θ(k) and sl(k) represent the estimated direction and step length at time k, respectively. g θ and g sl represent gaussian noise. according to the previous position, the current position of a user is recursively calculated as follows:"
"we provide a brief overview of the pqlseq method in the section 2, with algorithmic details available in the supplementary material. briefly, pqlseq fits a poisson mixed model (pmm) for modeling rnaseq data and a binomial mixed model (bmm) for modeling bsseq data. in both data types, pqlseq examines one genomic unit (i.e. a gene or a cpg site) at a time, produces an estimated heritability b h 2, and in the case of differential analysis, also computes a p-value for testing the genomic unit association with a predictor variable of interest, where the predictor variable can be either continuous or discrete."
"in a teaching building experiment, we showed that fpf combined with a floor plan can effectively constrain the heading drift of ins. pf [cit] and a map-matching algorithm [cit] were used to perform a comprehensive comparison. the walking distance was about 110 m."
"location services using smartphones have attracted more and more attention in recent years. how to estimate the location of a user is the core of a location service. satellite navigation systems provide reliable outdoor positioning. due to the obstruction of obstacles, such as walls, satellite signals are severely degraded in an indoor environment, leading to unreliable indoor positioning that relies on satellite signals. with smartphone sensors, some indoor positioning technologies have received the endorsement of many scholars [cit] . unlike mature outdoor satellite positioning schemes, indoor positioning techniques, using smartphones, cannot provide real-time, long-term meter-level positioning accuracy."
"compared with a fingerprint matching algorithm, our system does not need to construct the fingerprint database in advance. our system is robust to changes in the environment. likewise, the computational burden is relatively small, thereby saving a limited amount of phone power. in general, the main innovation points are as follows:"
"generic pf overlooks the role of input in the system model [cit] . inclusion of the input into the system model has just been attended in a few recent works. however, in these works, the inputs of the systems have still been treated as deterministic variables [cit] . in the real systems as illustrated in fig. 1, the inputs are noisy, and there is a need for the development of pf framework such that it can manage general class of dynamical systems including noisy inputs. in a recent research work, the authors presented a pf based framework to address the stated problems [cit] . in this paper, new 103-2 variants of pf will be developed to manage the stochastic nature of the system input. to verify performance of the developed framework, a comparative study will be presented using one-dimensional and two-dimensional generalized nonstationary growth model. a conclusive summary of the paper will be provided in the last section."
"we assume the input noise and the output noise have unknown distributions. instead, the input noise and the output noise are represented by sets of redundant measurements in each case. the model exemplifies a system with unknown noise characteristics for the sensors, where, there are several readings from the sensors at each time step."
where n is the number of individuals; y i is the number of reads mapped to the particular gene for the i'th individual; n i is the total read counts for the individual (a.k.a read depth or coverage); and k i is an unknown poisson rate parameter that represents the underlying gene expression level for the individual.
"when particles cross an obstacle, we set their weight to zero. otherwise, we set the weight to one. crossing-obstacle particles are treated as low-weight particles. high-weight particles and low-weight particles are resampled with high and low probabilities, respectively. when too many low-weight particles participate in resampling, conventional pf often suffers from the particle impoverishment problem. to mitigate the particle impoverishment problem, we use fa to modify the particles. to illustrate the effectiveness of the modified operations, the weights before and after performing fa are shown in figure 6 . as the tester walked along the teaching hallway, we selected the 20 th, 60 th, 125 th and 150 th steps for analysis. the left and right subfigures in figure 6 denote the particle weights before and after performing fa, respectively. there are 3, 7, 11 and 19 zero-weight particles in the left subfigures. there are 0, 0, 0 and 0 zero-weight particles in the right subfigures. after fa is performed, the crossing-obstacle particles are greatly reduced, demonstrating that the particle impoverishment problem is mitigated."
"pqlseq fits two forms of glmm that include the poisson mixed model (pmm) for modeling rnaseq data and the binomial mixed model (bmm) for modeling bsseq data. these two different types of sequencing data have different data structures. specifically, rnaseq studies collect one read count for each gene as a measurement of its expression level. in contrast, bsseq studies collect two read counts for each cpg site-one methylated count and one total count-as a measurement of the methylation level at the cpg site. the ratio between these two counts in the bsseq data represents approximately the methylation proportion of the given cpg site. therefore, we use two different types of glmm to model rnaseq and bsseq data. for both data types, we examine one genomic unit (i.e. gene or cpg site) at a time."
"r. agarwal and m. motwani [cit] examined the important issues related to cluster-based manets, such as the cluster structure stability, the control overhead of cluster construction and maintenance, the energy consumption of mobile nodes with different cluster-related status, the traffic load distribution in clusters, and the fairness of serving as cluster head for a mobile node."
"the rest of the paper is organized as follow: we start by introducing different clustering approaches. then, we present their advantages and disadvantages. in section 3 we present some existing works on survey of clustering in manets. in section 4, we review some clustering schemes for manets. then we compare the clustering schemas that already present. finally, in section 5, we conclude the paper."
"in the paper, we have primarily focused on illustrating pqlseq for simple glmms with two-variance components: one component models sample non-independence due to the covariance matrix k, while the other component models independent over-dispersion. however, pqlseq can easily accommodate multiple variance components. indeed, we have implemented pqlseq so that it can fit glmms with multiple variance components. glmms with multiple variance components can be particularly useful when there are multiple sources of variance that need to be accounted for [cit] . for example, one can use multiple variance components to account for population stratification, family relatedness as well as independent environmental variation. alternatively, one can use multiple variance components to account for sample non-independence due to cell composition heterogeneity across samples, batch effects as well as independent noise. exploring the use of glmm with multiple variance components in various genomic sequencing studies is an interesting future direction."
the remainder of this paper is organized as follows: section 2 introduces related work. section 3 describes the system model in detail. section 4 discusses the walking experiments to test different scenes. section 5 presents the conclusion and future work.
"generalized linear mixed model (glmm) has recently emerged as a powerful statistical tool for the analysis of high throughput genomics sequencing studies [cit] . the main application of glmm in these genomic sequencing studies is so far restricted to differential analysis, which aims to identify genomic units (e.g. genes or cpg sites) that are associated with a predictor of interest (e.g. disease status, treatment, environmental covariates or genotypes). original paper sequencing (rnaseq) studies [cit] and differential methylation analysis in bisulfite sequencing (bsseq) studies [cit] . effective differential analysis with sequencing data often requires statistical methods to both account for the count nature of sequencing data and effectively control for sample non-independence-a common phenomenon in sequencing studies caused by individual relatedness, population structure or hidden confounding factors [cit] . glmm accomplishes both tasks by relying on exponential family distributions to directly model sequencing count data and by introducing random effects terms to account for sample non-independence. in effect, glmm generalizes both the linear mixed model (lmm) that has been widely used to control for sample non-independence in association studies [cit], and over-dispersed count models (e.g. negative-binomial, beta-binomial) that have been widely used for differential analysis in sequencing studies [cit] . by combining the benefits of the two commonly used methods, glmm properly controls type i error and improves power for differential analysis [cit] . while the existing applications of glmm in genomic sequencing studies have been primarily restricted to differential analysis, the similarity between glmm and lmm begs the question on whether glmm can also be applied to estimate heritability for sequencing count data. heritability measures the proportion of phenotypic variance explained by genetics and is an important quantity that facilitates the understanding the genetic basis of phenotypic variation. the standard tool for estimating heritability is lmm, which has long been applied for heritability estimation [cit] or snp heritability estimation [cit] for various quantitative traits in the setting of genome-wide association studies (gwass). in the setting of genomics studies, lmm has also been recently applied to estimate gene expression heritability [cit], methylation level heritability [cit], as well as various other molecular traits heritability [cit] . however, lmm is specifically designed for analyzing quantitative traits. in genomic sequencing studies, the application of lmm requires a priori transformation of the count data to continuous data before heritability estimation [cit] . transforming sequencing count data may fail to properly account for the sampling noise from the underlying count generating process, and may inappropriately attribute such noise to independent environmental variation-thus running the risk of overestimating environmental variance and subsequently underestimating heritability. in contrast, glmm directly models count data, and as will be shown in the present study, has the potential to become a more accurate alternative than lmm for heritability estimation in genomic sequencing studies."
"a heuristic based algorithm [cit] called max-min dcluster builds d-clusters non-overlapping. the node id is used for ch election. the algorithm is divided into three phases. in the first phase, each node broadcasts its id to its neighbors within d-hops, collects their ids and finds the highest id which it will broadcast in the second phase. in the second phase, on receiving the highest ids, each node keeps the lowest ids among the highest. during the third phase cluster head is chosen based on the ids saved in the two previous phases. this algorithm produces a robust structure of clusters. however, the duration of cluster formation is significant and more information is exchanged before electing a ch."
where f i and f j represent the location of two fireflies before fa is performed. f s denotes the position of the new firefly. the second term of equation (25) represents the attraction of the brightness fireflies. the third term is random.
"ins is composed of three parts: attitude angle estimation, a hybrid step length module and a step counting module. we will introduce these three modules in detail."
"in many engineering problems, internal state of a system is to be estimated using sequential measurements on timevarying inputs and outputs of the system. state-space model of a dynamical system includes at least two parts: a system model that describes the state evolution with time, and a measurement model that shows the relation of the state and the measurements. sequential bayesian estimation is a rigorous approach for state estimation in dynamical systems. the optimal algorithm in closed form is called kalman filter (kf) [cit], which applies only for first order markovian systems with linear/gaussian state-space models. extended kalman filter (ekf) is a variant of kf that projects its applications to nonlinear systems [cit] . with availability of low cost computational power, monte carlo methods have been applied to simulate the state probability distribution using weighted sample scenarios, called particles [cit] . the so called particle filter (pf) is not restricted to linear/gaussian systems and therefore, its variants have found a wide range of applications in various fields of science and engineering [cit] . a dynamical system that receives time-varying input has the following state-space model:"
"structuring a network is an important step to simplify the routing operation in manets. several algorithms based on clustering techniques have been proposed in the literature [cit] . the clustering consists of dividing the network into a set of nodes that are geographically close. it is an efficient solution to simplify and optimize the network functions. in particular, it allows the routing protocol to operate more efficiently by reducing the control traffic in the network and simplifying the data routing. several clustering schemes have been proposed. these schemes have different characteristics and are designed to meet certain goals depending on the context in which the clustering is used (routing, security, energy conservation, etc.) [cit] ."
"in the topology based clustering, the cluster head is chosen based on a metric computed from the network topology like node connectivity. we present below some of the existing topology based clustering algorithms."
"we designed npcc, ppcc, and fpcc for unknown rotation angles. the proposed designs for different number of pipeline stages with different timing constraints were synthesized, and table 5 lists the synthesis results. note that 3, 4, and 6 stages are sufficient to produce greater than 100 msps, 150 msps, and 200 msps tps, which requires 31.3%, 28.1%, and 24.1% less area and 19.0%, 25.2%, and 25.6% less eps than an fpcc, respectively. from table 5, we know that remarkable savings in terms of area and eps can also be obtained over fully-pipelined designs, even for unknown rotation angles."
"in this section, we propose a recursive cascaded cordic design, namely a pipelined-recursivecascaded cordic (prcc). figure 4 shows the structure of a 2-stage prcc that performs n successive micro-rotations using two cordic units. when n is even, the first n/2 micro-rotations out of n micro-rotations are performed during the first stage, whereas the remaining n/2 micro-rotations are performed during the second stage. a pair of 2:1 muxes in the first stage selects the initial input values x 0 and y 0 during the first clock cycle of the period of the first n/2 clock cycles, and it selects outputs from two adders or subtractors in the first stage during the subsequent n/2 − 1 clock cycles. the first stage is responsible only for the first n/2 cycles of micro-rotations and passes the intermediate results the hardware-complexity of the barrel-shifter in a prcc can be effectively reduced with a simple hardwired pre-shifting scheme [cit], as shown in the second stage of figure 4 . only the (n/2) most significant bits of the input words from the registers can be loaded to the barrel-shifters, because n/2 is the minimum number of shifts in the second stage and the n/2 less significant bits would become truncated during shifting. therefore, the barrel-shifter must implement a maximum of n/2 shifts. the output from the barrel-shifters are loaded into the (n/2) lsbs of the adder or subtractor, and the n/2 msbs of the corresponding operand are sign-extended. the barrel-shifter in the second stage can be implemented using n/2 · log 2 (n/2) number of muxes, whereas n · log 2 (n) muxes are required without using hardwired pre-shifting. therefore, the area complexity of a prcc can be reduced by implementing hardwired pre-shifting."
"where t npcc (n, l) is the propagation delay of the n-th stage in an l-bit npcc. now assume that one more stage is added for a total of two stages, but there are no pipeline registers between stages. one can see from figure 1b that the input to the adder or subtractor on the right side in the second stage is obtained by right-shifting x 1 by 1. therefore, addition or subtraction in the second stage can begin only if the second least significant bit (lsb) of x 1, i.e., x 11, is available. note that x 11 is the lsb in the input of the adder or subtractor after the lsb x 10 is truncated. therefore, the propagation delay of a 2-stage npcc is the sum of the delay from sign(ω 0 ) to x 11 in the first stage and the delay from x 11 to y 2(l−1) in the second stage. the increased propagation delay due to inclusion of one more stage becomes t xor + t faac + t facs, which is much lower than the delay of one l-bit rca. similarly, when one more stage is added to n-stage npcc, yielding a total of n + 1 stages, the propagation delay increment ∆ n can be estimated as follows:"
"commitment-based protocols assume that a (notional) social state is available and inspectable by all the involved agents. the social state traces which commitments currently exist between any two agents, and the states of these commitments according to the commitments lifecycle. commitments can be used by agents in their practical reasoning together with beliefs, intentions, and goals. in particular, [cit] point out that goals and commitments are one another complementary: a commitment specifies how an agent relates to another one, and hence describes what an agent is willing to bring about for another agent. on the other hand, a goal denotes an agent's proattitude towards some condition; that is, a state of the world that the agent should achieve. an agent can create a commitment towards another agent to achieve one of its goals; but at the same time, an agent determines the goals to be pursued relying on the commitments it has towards others."
"3.3. algorithm design for a pipelined cascaded cordic based on the above analysis, we propose an algorithm for minimizing the number of pipeline stages and the locations of pipeline registers in a cascaded cordic. figure 3 shows a flowchart of the proposed search algorithm. the goal of the algorithm is to minimize the number of pipeline stages in order to minimize pipeline overhead without violating the timing constraint per stage. for a given timing constraint t, assume that"
"the clima vii gold miners scenario consisted of developing a multi-agent system to solve a cooperative task in a dynamically changing environment: a grid-like world where agents could move from one cell to a neighbouring cell if it contained no agent or obstacle. gold could appear in the cells. agent teams were expected to explore the environment, avoid obstacles and compete with another agent team for collecting as much gold as they could and deliver it to the depot. each agent can carry one gold nugget at a time (we say that an agent that is not carrying gold is free). agents had a local view on environment, their perceptions could be incomplete, and their actions could fail. listing 1.1. the gold miner agent code in jacamo+."
"the propagation delay of the barrel-shifter could also be decreased by hardwired pre-shifting. as the maximum number of shifts in the first and second stages are equal, as shown in figure 4, each stage has the same propagation delay t mux · log 2 (n/2), where t mux is the propagation delay of the 2:1 mux. therefore, the critical path of a prcc is determined by any of its stages. however, the critical path of a prcc is less than that of a conventional cordic because the maximum number of shifts with the barrel-shifters in a prcc is less than that in a conventional cordic. the propagation delay of the k-stage, l-bit prcc with n micro-rotations is"
"one of the strongest points of jacamo+ is the decoupling between the design of the agents and the design of the interaction -that builds on the decoupling between computation and coordination done by coordination models like tuple spaces. the decoupling allows us to change the definition of the artifact without the need of changing the agents' implementation. so, in the gold miners scenario, allocation can be fifo, based on the miners' position, or take into account further contextual information like day time, known differences in the equipment of the miners, difficulty in reaching the nugget location. all these different policies can be implemented in a way that des not have an impact on the miners' code."
"-d. gupta, s. sharma and seema, &quot;bicriteria in n x 3 flow shop scheduling under specified rental policy, processing time associated with probabilities including transportation time and job block criteria&quot;, mathematical modelling and theory. 1(2), 2011, 7-18."
"-l. n. van wassenhove and l. f. gelders, &quot;solving a bicriteria scheduling problem&quot;, aiie trans., 15, 1980, 84-88. -l. n. van wassenhove and k. r. baker, &quot; a bicriteria approach to time/cost trade-offs in sequencing&quot;, ejor, 11, 1982, 48-54."
"the key concept of coordinate rotation digital computer (cordic) arithmetic is that it can effectively compute trigonometric functions, vector rotation, multiplication, and division through an iterative formulation of shift and add operations. [cit] by jack e. volder [cit], a wide variety of explorations of cordic applications, algorithms, and architectures have been investigated to find low-cost, high-performance hardware solutions [cit] ."
"the hardware and time complexities of the proposed designs and conventional designs for known rotation angles are listed in table 3 . the proposed k-stage ppcc is compared with an npcc and fpcc [cit], which are two opposing cordic versions in terms of pipeline strategy in a cascaded cordic. the proposed k-stage prcc is also compared with a conventional cordic [cit] computed through recursive computation of one stage and opposing non-recursive cascaded cordic computed through n stages. the propagation delay in a conventional cordic is the sum of delays of a 2:1 mux, barrel-shifter, and adder or subtractor. fpcc has the shortest critical path, which involves an inverter and an adder or subtractor. the fpcc, npcc, and ppcc produce an output sample during every cycle when they are used in the pipelined cordic applications, whereas the conventional cordic and k-stage prcc produce the output sample every n and n/k cycles, respectively. the k-stage prcc has k cordic units, thus it involves a factor k more adders or subtractors, barrel-shifters, registers, and muxes than a conventional cordic. however, the area of a k-stage prcc is less than a factor k greater than that of a conventional cordic due to its hardwired pre-shifting. the propagation delay of a prcc can be reduced to be less than that of a conventional cordic."
"otherwise, the ppcc can never meet the timing requirement. it is assumed that the values of t inv, t faac, t xor, t facs, and t facc are known, and the values of n, m, and k are initialized. to search for the first location of the pipeline registers, t ppcc (l)(1, 2) is estimated according to equation (13) . if the value of t ppcc (l)(1, 2) is less than t, we can infer that pipeline registers are not required between the first and second stages. then, m is increased by 1 to test whether the registers should be placed before the third stage by calculating t ppcc (l) (1, 3) . in this manner, the value of m is increased by 1 until t ppcc (l)(1, m) is larger than the timing constraint. if the value of t ppcc (l)(1, m) is larger than the value of t, the pipeline registers should be placed after the (m − 1)-th stage. then, the location of the first pipeline registers (m − 1) is stored in p(0), and the algorithm continues to search for the next location of pipeline registers p(1). because the pipeline registers are located before the m-th stage, the initial position of propagation delay n is reset to m for the next search, and the value of m is increased until the next location of pipeline register is found or m reaches the end of the stage. when the algorithm terminates, the location of pipeline registers can be retrieved from p(k), and the total number of pipeline stages k is taken to be (k + 1). pseudo-code of the algorithm is given in algorithm 1."
"the shift from obligations to commitments is beneficial in many respects. first, the autonomy of the agents is better supported because they are free in deciding how to fulfill their goals. it follows that agents are deliberative, and this paves the way to self-* applications, including the ability to autonomously take advantage from opportunities, and the ability of properly reacting to unexpected events (self-adaptation). moreover, the interplay between goals and commitments opens the way to the integration of self-governance mechanisms into organizational contexts. thus, our concluding claim is that directly addressing social relationships increases the robustness of the whole mas."
"throughput and power consumption have become the major issues in cordic algorithm design, especially for implementations in embedded systems with limited resources. as cordic acts as a basic arithmetic operator in addition to adders, subtractors, and multipliers in hardware implementation, their performance greatly affects the performance of the entire system, especially in cordic applications requiring high performance, such as fast fourier transform (fft) [cit] or embedded fpga-based synthesizers, including chaotic pseudo-random number generators [cit] . in particular, the latency of cordic computation is a major issue due to the large number of iterations required to preserve sufficient precision of the output, despite its linear-rate convergence [cit] . therefore, the speed of cordic operations is limited either by the required precision (number of iterations) or the clock period. angle recoding (ar) methods [5, 6, [cit] can be used to reduce the number of cordic iterations by encoding the rotation angle as a linear combination of a set of selected elementary angles. however, selective implementation of micro-rotation carries significant scaling overhead."
"jason [cit] implements in java, and extends, the agent programming language agentspeak(l). jason agents have a bdi architecture. each has a belief base, and a plan library. it is possible to specify achievement (operator '!') and test (operator '?') goals. each plan has a triggering event (causing its activation), which can be either the addition or the deletion of some belief or goal. jacamo+ extends jacamo by allowing the specification of plans whose triggering events involve commitments. jacamo+ represents a commitment as a term cc(debtor, creditor, antecedent, consequent, status) where debtor and creditor identify the involved agents (or agent roles), while antecedent and consequent are the commitment conditions. status is the commitment state (the set being defined in the commitments life cycle [cit] ). commitment operations are realized as internal operations of the new class of artifacts we added to cartago. thus, they cannot be invoked directly by the agents, but the protocol actions will use them as primitives to modify the social state."
"it should be noted that the increased propagation delay does not depend on the input word-length but rather on the number of shifts in the cordic unit when separate pipeline stages are merged, which is much less than the delay of t addsub (l), especially in the initial cordic stages. the propagation delay of the n-stage npcc is"
note that log 2 (n/k) t mux is the delay of a barrel-shifter and another t mux is the delay of a 2:1 mux used for input selection.
"an alternative approach to speed up cordic is to reduce the clock period and increase the throughput rate of the cordic output. as the cordic iterations are identical and involve nearly the same complexity, mapping them onto pipelined architectures is very hardware-friendly. the main objective of pipelined implementation is to reduce the critical path. [cit] . pipelined cordic circuits were used thereafter for high-throughput sinusoidal wave generation, fft, adaptive filters, and other signal processing applications [cit] . given each pipeline stage performs one predetermined micro-rotation, the number of shifts required for shift-add or shift-sub operations in each pipeline stage is known a priori. therefore, shift operations can be hardwired, and barrel-shifters can be completely eliminated in the non-recursive cascaded implementation of cordic. then, the critical path of pipelined cordic amounts to the time required to execute add or subtract operations in each stage."
"where t inv and t addsub (l) are propagation delays of an inverter and l-bit adder or subtractor, respectively. sign extension to avoid overflow is not considered because the word-length is generally retained during cordic operations. assuming that a ripple carry adder (rca) is used for the adder or subtractor, we have [cit] t"
"the rest of the paper is organized as follows. a critical path analysis for a cascaded cordic with unknown rotation angles is presented in section 2. the fine-grained critical path analysis of a cascaded cordic for known rotation angles is presented in section 3. an algorithm for determining the number of pipeline registers and their locations for a given timing constraint is also proposed in section 3. a hybrid recursive cascaded cordic is presented in section 4. the performance of the proposed designs in terms of area, throughput, latency, and power consumption is discussed in section 5. conclusions are given in section 6."
"in our case, instead, the connection between the event \"commitment detached\" and the associated plan is not only causal, but the plan has the aim of satisfying the consequent condition of the commitment that triggers it (drop(x, y)), i.e. of accomplishing an explicit and shared social engagement. the signal that notifies gold allocation is not relevant to the agent, at the point that it does not even appear in the code nor in the commitment. it is the detachment of the commitment itself that causes handling the gold. there is no need of knowing or using logics, that are internal to the protocol (artifact), for programming the agent. social meanings are the key."
"fully-pipelined-cascaded cordic (fpcc) has potential for a very low clock period and very high throughput. however, such high throughput rate is usually not required in real applications. for example, when cordic is used to implement the complex butterfly operation for a fast fourier transform (fft), the throughput rate provided by cordic must match the throughput requirement of the application and the type of fft implementation, such as fully-pipelined implementation, folded-pipeline implementation, and the number of butterfly circuits. on the other hand, we find that removing a pair of pipeline registers to merge two pipeline stages does not substantially increase the critical path. the propagation delay of a pair of cordic stages is generally assumed to be the sum of delays of two adders or two subtractors. however, such an assumption is currently invalid in asic and fpga implementation, wherein adders or subtractors in each stage are not considered as discrete components. we also find that the rear stages in the cascaded cordic have longer propagation delays than the preceding stages, thus inserting pipeline registers for each is not a good pipelining strategy for a given number of stages. however, there is no systematic analysis regarding the appropriate pipeline decision in a cascaded cordic. in this paper, we discuss the fine-grained estimation of propagation delays in cascaded cordic. based on that, we derive a formulation that could be used by a designer to determine the number of pipeline stages to be considered and where to place the pipeline registers in the cascaded design."
"we coded designs-1, 2, 3, 4, and 6 in the design examples in table 2 in vhdl and synthesized those using the synopsys design compiler with the tsmc 90 nm general purpose standard cmos library [cit] . a conventional cordic [cit], 2, 4, and 8-stage prccs, and an fpcc [cit] were also synthesized and compared for known rotation angles. the values of l and n were 16. the maximum propagation delay, throughput per second (tps), latency required to obtain the first output sample, area, power consumption at 50 mhz operating frequency, energy consumed per sample (eps) required to produce a 50 mhz clock output, and area-delay product (adp) are listed in table 4 ."
"cascaded cordic involves one cordic unit for each micro-rotation, while at the other extreme, recursive cordic uses only one cordic unit for all micro-rotations. instead, we can have a hybrid of non-recursive cascaded cordic and recursive cordic, which we refer to as pipelined-recursive-cascaded cordic (prcc). prcc consist of a cascade of a few cordic units, where each such cordic unit performs a certain number of micro-rotations recursively. in this paper, we investigate the design and implementation of such recursive cascade as another design option for the cascaded cordic."
"when organizational goals are not associated with corresponding guidelines, agent deliberation is crucial for the achievement of goals. an agent has to act not only upon its own goals, but also upon what interactions could be necessary for achieving these goals. in other terms, an agent has to discover how to fulfill a goal by interacting with others. it is important to underline that when agents can fully exploit their deliberative capabilities, they can take advantage of opportunities (flexibility), and can find alternative ways to get their goals despite unexpected situations that may arise (robustness). to this aim, we present jacamo+, an agent platform, that builds upon jacamo [cit], where jason agents engage commitment-based interactions which are reified as cartago artifacts. cartago is a framework based on the a&a meta-model [cit] which extends the agent programming paradigm with the first-class entity of artifact: a resource that an agent can use, and that models working environments. the environment is itself programmable and encapsulates services and functionalities, making it active. jacamo+ artifacts represent the interaction social state and provide the roles agents enact. the use of artifacts enables the implementation of monitoring functionalities for verifying that the on-going interactions respect the commitments and for detecting violations and violators. the well-known gold miners scenario is used as an example."
"a conventional cordic is inherently sequential and involves large latency. it offers low throughput rate due to its recursive implementation of micro-rotations. in contrast, a fully-pipelined non-recursive cascaded cordic provides very high throughput rate at the cost of large area complexity. however, such high throughput is not required in many applications. for example when cordic is used for implementing the complex butterfly operation for fft calculations, the throughput rate of cordic must match the throughput requirement of the application and the type of fft implementation. on the other hand, we see that the critical path does not increase substantially when some of the pipeline stages are removed from a fully-pipelined cascaded cordic. therefore, we have explored other design choices for a cordic with varying operating frequency, throughput rate, latency, and area complexity. in this paper, we present a precise estimate of the propagation delays in cordic circuits to determine the critical paths in the pipelined and non-pipelined recursive and non-recursive cordic architectures. we have shown that the propagation delay does not increase significantly and does not depend on the input word length when adjacent pipeline stages are merged. instead, the propagation delay depends on the number of shifts in the cordic unit. therefore, more initial stages in cordic can be merged to form a single pipeline stage compared to the later stages. we proposed an algorithm to search for the locations of pipeline registers in order to minimize the number of pipeline stages and determine the desired critical path for a given timing constraint. we have also proposed a hybrid cascaded cordic and recursive cordic to increase throughput and save energy per sample. we have shown that ppcc requires less area with lower eps than an fpcc, and a prcc operates with less eps and has lower adp compared to the conventional recursive design."
"where t xor is a delay of a 2-input xor gate. t faac, t facc, and t facs are delays of the 1-bit full adder (fa) from port input-a to port carry-out, from port carry-in to port carry-out, and from port carry-in to port sum, respectively. note that the propagation delay of an adder or subtractor increases by t facc as the bit-width of the input increases by 1. if all pipeline registers marked by thick solid lines in figure 1b are removed, we can obtain a non-pipelined-cascaded cordic (npcc), and the black-dotted line in figure 1b becomes the critical path. then, the propagation delay of an n-stage, l-bit npcc t npcc (n, l) can be generalized as follows:"
"knowledge gathered from the creation of the subinteraction diagrams showed that bp4.1 and bp4.2 were the main production business processes. thus to fully understand implications of production activities on business indicators such as cost and value generation, there was the need to further create interaction diagrams describing the various flows that exist between the sub-business processes of bp4.1 and bp4.2. these further elementary interaction diagrams were called \"sub-sub\" interaction diagrams. figure 6 shows the subsub interaction diagram describing the flows that exist between subprocesses of the bp4.1 and bp4.2."
"4 bearing the forgoing in mind, three primary forms of decomposition selected and deployed by the authors were a separated development and deployment of structural and behavioural model, such that differentiation could be made between system characters that are essentially static during the time frame of modelling from other system characters that will posses dynamic changing behaviours; b during structural modelling to separately capture and visualise process, resource, work subsystem viewpoints, so that during subsequent modelling and decision making amongst alternative system configurations comprising current or possible future system components and their organisation structures that can be explicitly and visually represented and communicated, as needed with reference to various other system viewpoints; c the use of various hierarchy concepts, particularly to identify and encode boundaries, ownerships, and encapsulations embedded within actual and modelled systems and their processes, resource elements, and work structures."
"typically, for further dynamic analysis, the initially created cl models have to be redefined and \"structured\" to be able to provide useful contributions towards analytical decision making and quantification of cls. thinking about developing structured causal loop models sclms, a set of rules are defined to help reorganise the variables identified in the initial causal loops. the starting point is to identify variables with measurable and operational meanings. starting from this point enable other variables to be connected in such a way that estimation of \"operational variables\" can be determined through the \"factual analyses\" of the connecting variables. whilst doing this, care is taken to ensure that the resultant sclms consist of variables which are causal, deterministic, time variant, directed and signed."
"initial steps taken to understand acam processes involved the creation of a \"static\" enterprise model that captures relatively enduring aspects of the processes and systems used by acam."
"the cl models shown in section 2.2 were helpful in describing qualitatively the causes of dynamics in selected key business processes of acam ltd. with the view to achieving sclms of relevance to performance indicators such as cost and value, the initially created clms see figures 8, 9, and 10 were revised based on the requirements described above. figure 11 is an extension of the initial clms presented in figures 8, 9, and 10. it was derived through an extensive study of the previously created clms. as shown in figure 11, to quantify customer needs, customer stock levels are taken into consideration. a negative polarity is indicated because as customer bearing stock level reduces, customer demand increases. customer stock level is affected by a number of factors. again, for the purpose of creating a structured causal loop model, the broad range of factors such as customer bearing failure rate, machine breakdowns, preventive maintenance schedules, customer stocking policies and other influencing factors are described simply as customer usage rate. in practice, acam ltd. operates directly with most of the engineering departments of their customers and is able to predict their maintenance cycles. although acam ltd. is not in favour of stocking bearings, their historic patterns of sales are able to predict the bearing usage rate of their customers. in a more complex model, customers will have to be classified based on their usage rates, so that distinct analysis can be made for each customer. one critical thing derived from customer demand is the number of sales orders prepared by acam ltd front-end business dp3 process. it was verified that about 92% of customer enquiries become sales orders; hence the number of sales orders generated within six months can be estimated. one other critical information from customer stock level is \"payments received\" by acam ltd. although this link is not vivid, it is implied that since bearings are supplied before payments are made by customers, the quantity of bearings required to be paid by a customer is the difference between the \"paid stock of bearings\" and the \"unpaid received stock of bearings\". most often there are some delays in payments. the actual value realized by acam ltd is the total payments received from customers. but for budgeting purposes, acam ltd estimates the total sales value from the number of different sales orders received. the estimated value of sales orders almost always exceeds the actual payments received from customers, so a \"value deficit\" is created."
"many other results related to total manufactured products, despatched volumes, process cost, material storage cost, and packaging cost were collated but have not been presented in this paper for the lack of space."
"to help overcome the limitations observed in the use of cls, the authors have developed and tested an integrated em-sd methodology comprising the systematic use of cimosa, cls, and a continuous simulation modelling tool called ithink. also the fundamental purpose of harmoniously deploying a combined enterprise and dynamic systems modelling approach was to address critical issues of complexity handling observed when problem solving in many example manufacturing enterprises mes . this paper describes how a specific case selection and unification of enterprise modelling and dynamic systems modelling concepts, methods, and techniques was developed and usefully deployed in the a case study described in section 3."
"in the study case, as in many other mes, the authors and their former colleagues in the msi research institute at loughborough university extended the use of cimosa modelling concepts. those extensions were made primarily to provide support within any given me setting the rapid and effective capture, visual representation and validation of process-oriented structures used by the me being studied. the purpose of doing so was to create a \"backbone process-oriented structural model\" the detail of which can be fleshed out over time and onto which other modelling viewpoints can be attached. to enable this stage of process-oriented modelling two main extensions were developed, namely 1 to visually document the me's current and/or possible future processes and their elemental activities using the four types of diagramming templates illustrated earlier by figures 2-5 which in effect implemented cimosa function modelling viewpoint and 2 to create and deploy a simple structured questionnaire which is used as a common basis to consult with all relevant types of decision makers, in order to ensure that sufficient real case data is elicited to populate the four process-oriented modelling templates, and to provide a structural framework onto which captured real case data encoding entities related to the resource and work subsystem viewpoints could be attributed. to instrument 1 and 2 and reduce the company and modeller people times involved during modelling, the authors developed and used a combination of visio and spread sheet tools as reported in agyapong-kodua 6 . as earlier discussed, cimosa was by no means the only viable choice of enterprise modelling technique. however its process-centric approach to decomposition was found to provide a useful starting point for structural modelling onto which other viewpoint and modelling concepts such as those supported by the grai methodology, aris, and the idef suite of tools can be added."
"activity diagrams for each of the bps described in the sub-sub interaction diagrams can be created to illustrate how bps are decomposed into their elementary activities. at this stage, the creation of activity diagrams for each of the bps was considered not necessary. this is because, fundamentally, the cimosa models created are to serve as a backbone for understanding process interactions and the various flows among bps so that dynamic analysis of factors which impact on business processes can be understood and based on the understanding derived, provide solutions for managing complexities and dynamics in manufacturing processes. the sub-sub interaction diagram was adequate to provide the basis for understanding the cause and effects structure of the company."
"based on these definitions and distinctions, table 2 was created to help specify the stocks, flows and converters in the sclm shown in figure 11."
"5 also bearing in mind the forgoing, the authors perceived that various forms of \"fit for purpose\" system behaviour modelling would need to be conducted, at required levels of modelling abstraction that normally would require aggregations of process, resource, and work model viewpoints, such that qualitative and predictive decision making support can be provided as commissioned for many potential types of model users that could be supported. critically, however, the authors observed the need for \"fit for purpose\" behaviour modelling to be conducted with reference to the organisational context in which specific and collective me decisions are made, that is, with reference to previously conducted me structure modelling. the purpose of doing so would typically be to facilitate coherent decision making amongst multiple decision making groups such as me directors, multilevel managers, and plant personnel . figure 1 conceptualises the essence of the combined enterprise and dynamic systems modelling approach developed and reported upon in this paper. as shown in figure 1, having captured the process-oriented \"big picture\" [cit] of \"role requirements\" to \"competencies possessed by human, machine or it resource candidates, that is, as potential role holders\" and b map modelled flows of cognate work types through combined models of process and resource subsystems 38 . at this stage of modelling the explicit capture of many types of real case me data needs to be encoded at multiple levels of abstraction so that the validity of the captured data and the organisational structures that are represented can be assessed by relevant me knowledge holders."
"previous sections of this paper have shown example outcomes of how structural models and their encapsulated data can be reused via the use of suitable \"in context\" mental models of types outlined in sections 2 and 3. the authors have yet to find a good way of representing the visual models so created, but figure 16 has been constructed to show the types of system entity that are naturally encoded in the structural model and through processes of mental modelling are positioned appropriately into structural designs of causal loops and continuous simulation models. the reuse so enabled has proven effective in positioning and creating \"fit for purpose\" multilevel, multipurpose clm, and continuous simulation tools for clients, where the clients have anchored the understanding so generated both within me wide and with respect to specific domains of their expertise."
"based on these indicators, table 1 shows a review of 5 of the major system dynamics tools available in the public domain. from the review it can be mentioned that although cl modelling had been useful in many business analyses, it generates qualitative results and cause and effects cannot be simulated using cls alone 10, 32 . thus on its own, cl cannot facilitate quantitative prediction of outcomes. as a result, the authors are of the view that although the cl modelling technique performs better than other sd modelling techniques in some aspects, the technique requires further support for it to be suitable for systems' design and business analysis 29 . because of these limitations, the authors are of the view that, for full benefits of cl modelling in support of business decision analysis to be obtained one has the following."
"on completion of this modelling stage, generally significant new insights will be inducted into the subject me, as knowledge holders discuss the pros and cons of their best practices which are illuminated by the structural model. essentially the big picture me model so captured has proven to be an extremely valuable repository of me knowledge which can be reused in a variety of ways. those ways include helping decision makers position their thinking and then subsequent more holistic decision and action taking, reuse of the encoded organising structures to enhance decision and action processes and to underpin those processes with better design it systems, and constructing \"what if\" scenarios which can begin to justify improved policies and practices throughout the me."
"although the resulting sclms are still qualitative, all parameters will have operational and measurable indicators so that at the next stage a \"stock and flow model\" can be created by defining stocks, flows, and converter variables. the quantifications of the final stock and flow model are supported with the ithink simulation tool."
"on resumption of the modelling exercise, a series of structured and unstructured interviews and shop floor visits were conducted to enable better understanding of acam advances in decision sciences ltd. processes. a full documentation of the interview questions and their responses is provided in 6 . in addition to the data and information gathering exercises, company production data, human resource organization charts, sales, and finance data were also examined. initial understandings of the company processes were documented and described in the form of a spreadsheet and later transformed unto revised versions of the cimosa modelling templates, as described in 6, 41 . based on these earlier understandings about process decomposition, a context diagram, as shown in figure 2, was created to represent all the dms observed in the company. as can be seen from figure 2, six main domains were observed with three of them being considered to be non-cimosa domains. dm1 is used to represent the set of processes belonging to the customer domain. dm1 is therefore responsible for providing orders to acam ltd., receiving finished bearings in time and making prompt payments. dm2 is used to describe the processes performed by the suppliers of raw materials to acam ltd. a set of processes belonging to sales, planning, and designing was classified as \"front-end businesses\" and denoted as dm3. the managerial and supervisory activities needed to ensure the fulfilment of orders were termed the \"business management\" dm4 domain. dm5 refers to the \"physical processes and activities\" required to fulfil customer orders. this represents the actual material transformation processes required to convert raw materials into finished goods. finally, dm6 is used to represent the support processes required for the fulfilment of the other domains."
"3 to cater for inherent system complexities and enable multilevel of abstraction modelling, it was assumed that a number of approaches to decomposition which had been developed previously by the systems engineering and enterprise modelling communities could be deployed in a unified fashion."
"the managers of acam ltd. confirmed that the integrated modelling approach enabled understanding about their business, especially how resources and information flow from one unit to the other. this was helpful for them to understand the implication of activities in one department on the other. more critically, it was an excellent way of illustrating the factors which could be controlled and monitored to reduce cost and improve value. it was observed that the integrated method served as a strong modelling tool for capturing most of the salient factors in the company related to its \"architectural structures\" and how these structures impact on time based \"organisational behaviours\". with a base model created for analysing the performance of acam ltd., further experiments on process variables can be conducted to analyse optimal business performance in terms of process efficiencies, cost, and values generated by the company, resource utilisation, among others. essentially, the integrated models offer a means of: a replicating and understanding historic enterprise behavior, b predicting future enterprise behaviours and impact on performance indicators, c experimenting alternative decisions before implementation, to save cost and minimize errors."
"the combined approach to modelling is not tied to any specific case modelling methodology or tool. rather in several of their other complementary papers, the authors describe the use of alternative frameworks and tools, but the essence of this contribution is the conceptualisation of a methodology for creating and positioning \"fit for purpose,\" multilevel of abstraction models within the context of a host me and its environmental stimuli."
"to help verify the ithink models, efforts were made to observe how the structure of the model reflected the reality. this was done through studying the already created sclms and asking the managers of the shops to help verify if the model structure fairly represented the processes under consideration. to validate the process logics and controls of the simulation models, actual historic data in the form of production orders, interarrival times, order batch sizes, operation times and resources deployed, were inputted into the models. when the data set was inputted into the model, the operation cost, revenue generated, average throughput, process times and delivery volumes were found to conform to the real state of acam ltd.; hence the model was found to be sufficiently valid for use for further experimentation. figure 13 shows graphically the relationship between customer demand, value generated and material cost. as shown in figure 13, because of the nature of the production system, the relationship is not linear. a study of figure 13 shows that, in acam ltd., customer demand fell gradually from the beginning of the accounting year. the fall in demand had significant impact on actual value realized. because of the random nature of payments received from customers, actual value realized is largely different from \"expected value\" which is essentially dependent on number of sale orders for a given month. as shown on the graph, there is a gradual rise in value which means more payments are received at the beginning of the year with the peak of \"actual value realized\" being in the fifth month. as customer demand reduces, there is a sharp fall of \"value realized\" until it reaches its lowest level in the seventh month. another set of results showed the effect of constant \"sales orders\" on \"volumes of strip, paints and chemicals supplied\" as well as \"total storage cost\" see figure 14 . it was expected that when customer orders are steady, volumes of materials supplied will be constant, but the model assisted in understanding quantitatively the impact of material supply policies on acam ltd. production system. the graph showed that purchasing was not synchronized with customer orders. when this was verified from the managers of acam ltd., they explained that supplies of materials are forecasted based on previous production orders. as actual production orders achieved differ largely from customer orders, actual number of sale orders did not directly impact on their volume of material supply. figure 15 also shows that payments are inversely proportional to \"value deficit,\" but as \"supply\" increases, \"customer stock\" increases whilst \"despatch volume\" increases and falls over the period."
"dynamics impacting on business processes bps have been modelled using an integrated em and sd approach. following the modelling approach, complex structure and dynamics impacting on aspects of the business, especially those influencing cost and value, were captured. also the interaction between key system parameters was identified. the efficient modelling of the interactions was necessary, since it provided a thorough understanding of the system behaviour and provided basis for assessing the system performance under various operating conditions. the models supported the company in measuring their state of performance under varying conditions. in principle, the approach enabled the systematic deployment of candidate em and sd tools for assessing the impact of decisions on key performance indicators including cost and value."
"although many other factors such as lead time, quality, and innovation are necessary, it was considered at this stage of the research that the above five criteria were useful for detecting modelling techniques which were capable of supporting business analyses of complex and dynamic manufacturing systems."
"in correspondence with the main theme of the dms, a high-level interaction diagram was created to depict how respective domain processes interact see figure 3 . at the next stage of the enterprise modelling exercise of acam ltd., a decision was taken to further understand the process interactions that existed between the sub-business processes of dp3 and dp4. efforts were concentrated on further decompositions of dps 3 and 4, because discussing with the production managers of acam ltd, it was concluded that the company was essentially interested in knowing how front-end, and production activities impacted on their business. this decision matched well with the research objectives since basically the objective was to further help provide a backbone for understanding the impacts of dynamics on subprocesses. a subinteraction diagram showing how material and information flows between bp3.1, bp3.2 and bp3.3 is shown in figure 4 . instances of interaction of these bps with external dps such as dp1, dp2, dp4, dp5, and dp6 are also shown in the figure."
"to help provide in-depth understanding about the processes involved in acam ltd. and also provide a context for the application of sd models, an enterprise model em was created and used as the backbone for the creation of sd models. the ems so created show how acam processes can be decomposed into elementary activities and used to support further business analysis."
"in the \"produce bearings\" domain, the factors specified in figure 8 were simplified and reorganized to provide a background for quantitative analysis of the production requirements in the shops. the key factors influencing the production rate of the shops were observed to be the number of activities required to fulfil specific orders. taking into consideration the operation time of these activities, the number of bearings produced over time can be estimated. the operation time is a historic data which takes into account human resource and machine availabilities, breakdowns and all necessary adjustments in the shops. to help estimate the machine and labour cost, the number of machines and human resources required for the activities in the shops are shown. the labour, machine, material and storage cost influence the total production cost. if these cost components are expressed in units related to number of products realized then production cost can be deduced from the production volume."
"acam ltd. is a small-to-medium-sized bearing manufacturing company located in the united kingdom. acam ltd. makes ordering of a range of advanced composites bearings. these products are normally fibre-reinforced plastic laminates, ideally suited to highly loaded bearing applications in agricultural, marine, mechanical, pharmaceutical, and food processing environments. in addition to producing customised bearings and specialized structural bearings, washers, wear rings, wear pads, wear strips, rollers, and bushes, acam ltd. also produces semifinished bearing materials which are made available in tube and sheet forms."
"these issues were considered complex by the managers of acam ltd., and most importantly, they felt that demand variation and material supplies were critical factors impacting negatively on their business. section 2 of this paper considers the essence of the proposed combined enterprise and dynamic systems modelling approach and reflects on knowledge contributions made, whilst section 3 centres on the case application of the integrated em-sd methodology. in sections 4 and 5, the observations, recommendations, and conclusions are mentioned."
"ii big picture decomposition and representation that allows dynamic systems modellers to form effective and flexibly configured mental models of me structures that guide the specification of mental models of scenarios of system and subsystem change, along with appropriate kpi determination that will lead to qualitative analysis of relevant objective functions,"
the total material cost is estimated by the cost of the total materials supplied. there is also a difference in actual cost of materials and material cost paid by acam ltd. this is due to the payment arrangements and delays between acam ltd. and some of their suppliers.
"iii the combined reuse and transformation of effective mental models of me structures, causal and temporal relationships, and scenarios of system and subsystem change into mental models of stock and flows that can be readily encoded using stock and flow modelling concepts; which subsequently can be implemented and run using an appropriate choice of continuous simulation modelling tool."
"a body of literature related to current trends in mes has explained the enormous complexities and dynamics associated with the design and realisation of business processes 1-7 . although mes are inherently complex, traditional methods for solving problems in mes have not fully accommodated complexities and causal relationships associated with processes in mes 8, 9 . mes are inherently complex because they are composed of complex process networks which are interrelated in a way that changes made to one process thread induce dynamics in the me by having causal and temporal effects on other process threads 10 ."
"as can be seen from the sclm, efforts were made to express the otherwise descriptive variables into variables with operational and measurable meanings whilst taking care not to violate the rules for the creation of effective clms. to gradually transform the qualitative model into a quantitative model, at the next stage of modelling, the variables specified by the sclm were classified into stocks, flows and auxiliaries."
"internal sales records showed that customer demands were received through e-faxes, emails, post and telephone. about 92% of these customer requests turned out to become sales orders. as would be expected, the increase in customer demand increased the number of sales orders produced. the preparation of sales orders is performed through bp3.1.1 create sales order/job card which belongs to dp3 as shown in figure 8 . an increase in the number of sales orders created will increase the material requirements as well as the number of different bearings required. increase in material requirements implies that the number of individual material components will increase. from their material purchase records, normally four main raw materials are purchased. these are broadly classified as paints, clothes, resins and other chemicals. thus an increase in material requirements means an increase in the purchase orders pos of these components. collectively as the number of pos raised by the \"manage purchases\" business process bp5.4 increases, the total raw material demand also increases. this demand triggers the supply of the materials specified by the pos. in effect, the total supply volume increases as shown in the cl model in figure 8 . however the actual raw material stock is influenced by a number of factors which include the supply volume and supply frequency. internally, the raw material stock is negatively influenced by the consumption of material through production processes. this is expressed in the form of material required for production in the \"produce and deliver\" domain process dp4 . a more detailed description of the causal influences of bp4.1 is shown in figure 9 . a study of the sub-sub interaction diagram showing the process interactions of bp4.1 and bp4.2 shows that in the \"produce and deliver\" domain process dp4, raw materials are processed to meet the material requirements for producing flat products bp4.1.2, strips bp4.1.3 and round products bp4.1.4 . therefore the total raw materials required will be equivalent to the sum of the total raw materials for flat, strips and round products, whose quantities are grossly influenced by the total number of bearings derived from the sales orders. as shown in figure 9, the total number of flat, strips and round products is dependent on the processing rates of the production shops in charge of producing these components. the processing rates of the three shops are themselves influenced by a number of factors such as: number of activities, resource requirements, resource capabilities and competence, material availability, machine availability, among others. another initial clm created to describe the influences of process variables on the \"pack and despatch of bearings\" business process bp4.2 is shown in figure 10 . as shown in the figure, the actual numbers of flat, strips and round products realized is dependent on the processing rate of the various production shops responsible for the making of these products. other factors which influence the processing rate of the shops are described in figure 10 . in the clm for the \"pack and dispatch\" bp4.2 process, the number of products packaged is dependent on the total products finished. other factors include the availability of packaging materials and the rate of packaging. the increase in number of packaged products increase the number of bearings despatched. however, other factors such as delivery rules, availability of despatch vans and internal despatch priorities positively affect the number of bearings despatched."
"the application of the proposed methodology was tested in a rapidly growing bearing manufacturing company called acam ltd. the motivation of the company in supporting the deployment of this modelling methodology was to help better understand 1 the impact of variations in their customer demand on actual value generation and material cost, 2 the effect of constant sale orders on material supply, 3 the effect of company operations on payments or revenue generation."
"an initial causal loop model describing how customer orders influence purchases and supply of raw materials is shown in figure 8 . customer demand is influenced by a number of factors but because these factors are external to the main business domains, investigations were not carried out to establish the actual variables influencing customer requests."
"essentially the method of modelling described in the forgoing is not the prime focus of modelling science contribution reported in this paper. rather the focus is on a newly developed means of reusing big picture models of me structures that are populated with a myriad of real case me data within multipurpose dynamic systems models that can support predictive scenario analysis at multiple levels of abstraction. to maintain the paper within acceptable limits, this paper is focussed on one form of big picture structural and data model reuse. whereas other research of theauthors 2-4, 39-41, 43 reports on complementary reuses of the same structural models and data in support of a multilevel of abstraction \"fit for purpose\" 8 advances in decision sciences discrete event simulation modelling of alternative manufacturing paradigms, including the use of lean, agile, economies of scope, and scale and postponement strategies and b as a front-end to it data base and decision support system design/specification."
"it follows that the new science reported in this paper is about showing how the synergistic use of various kinds of mental, structural and dynamic systems model can facilitate complexity handling and lead to better and faster dynamic analysis of complex systems."
"system dynamics has been defined as a computer-aided approach to policy analysis and design. it has usefully been applied to dynamic systems characterised by interdependence, mutual interaction, information feedback, and circular causality 5 . research in the modelling and management of complexities in dynamic systems has resulted in the derivation and application of a number of system dynamics modelling tools and techniques. notable among these are fuzzy logics fls 11-15, neural networks nns 16-20, bayesian networks bns 21, 22, petri nets pns 7, 23-25, causal loops cls 7, 26-30 and stock and flow models 5, 7, 31, 32 . reflecting on the above-mentioned system dynamic modelling techniques and their application in managing complexities and dynamics in mes see table 1, the cl modelling technique is considered most suitable for representing, qualitatively, the cause and effects evident in dynamic systems 32, 33 . other researchers 33, 34 have mentioned that cls are useful for creating dynamic models of businesses for alternative policy verification. their unique advantage stands on their being able to be aligned with appropriate simulation software for quantitative business analysis. further basis for the support of the cl modelling technique was based on a set of performance criteria reported earlier by one of the authors 6 . in this earlier work by the first author, it was established that different assessment indicators may be considered when reasoning about suitable modelling techniques for business analysis of complex and dynamic manufacturing systems. this work showed that for a modelling technique to be relevant and provide useful inputs for multiproduct manufacturing systems' design and business analyses, it should have i the ability to analyse multiproduct flows and their associated product dynamics, ii the ability to identify and capture aspects of complexities and dynamics in mes, iii the ability to reflect causal impacts of activities in mes on performance indicators especially in financial terms, iv the ability to support business analysis especially in a virtual environment, v the capability to decompose processes into elemental activities to enhance understanding and process analysis."
"2 to achieve the purposes of each me, complex interactions need to occur between the various resource elements deployed. if the impacts of these interactions on the behaviours of the me can be better understood and predicatively quantified then potentially, desirable interactions can be enabled and undesirable ones constrained. but observations made in many different mes have shown that those understandings need to be developed from a variety of decision-making viewpoints requiring multiple and ideally coherent models of me systems at a number of levels of abstraction."
"1 mes commonly deploy a system of systems so that they conduct business, engineering, production, logistical, servicing, and other, functions in an effective and well-ordered manner. the elemental systems of mes typically comprise people, advances in decision sciences 5 machine, information, and communication resource elements onto which various kinds of organisational structures are overlaid, such that deployed resource elements function coherently as part of one or more wider systems."
"a careful study of the subinteraction diagram shown in figure 4 shows that, orders are received by the \"obtain and process orders\" bp3.1 from the external domain process belonging to the customer domain. by realizing bp3.1, sales orders are generated and transferred unto a job card which becomes the major input information for \"produce designs\" bp3.2 and \"plan and schedule production\" bp3.3 processes. bills of materials boms derived from the realization of bp3.2 are transferred to dp5 for purchases and estimated to be prepared and sent to suppliers and customers, respectively. product drawings, boms, and design specifications are also derived through bp3.2 and transferred to dp4. upon receipt of purchase orders, suppliers supply raw materials to dp4 for further processing."
"a number of researchers have provided explanations to how cl models can be quantified. key findings and recommendations on the transformation of cl to quantifiable models have been made by researchers such as 5, 27, 32, 35, 37 . these researchers from the systems modelling school have enhanced cl modelling by providing mathematical and social support to the technique. unfortunately, only little has been done towards the comprehensive use of the technique in support of manufacturing process design and analysis as well as the translation of qualitative cl models to quantitative simulation models. in many instances, cl modelling has been considered as not being suitable for modelling \"processes.\" they are used to capture factors which induce dynamics on systems."
"because of the observed complexity involved in doing so, the authors have not made any attempt to fully systemise the reuse of me big picture structural models, such as by automatically transforming them into coherent sets of multipurpose dynamic systems models; rather as also conceptualised by figure 1, the authors' focus of study has been on enabling i big picture decomposition and representation that allows dynamic systems modellers to form effective and flexibly configured mental models of me structures which can be mentally transformed by the modeller at appropriate levels of abstraction into equivalent mental models of casual and temporal structural dependencies between variables of systems that impact significantly on me behaviours of current modelling concern,"
"from the number of sales orders received, useful production and supply information can be deduced. this is reflective in the information presented on job cards and production schedules. on the production schedule the expected number of strips, round and flat products is indicated. the difference between the expected number of products and the actual manufactured products is the backlog acam ltd needs to deal with. the actual production volume is affected by real production variables such as processing rates of the production shops, materials available, human resource, machine availabilities and bearing type. based on the number of sales orders, the designers estimate the quantity of materials required. these quantities are compared with existing stock levels of materials to enable specific material orders to be raised. historic data exist for number of material orders raised over the sixmonth period. in some cases, the actual materials supplied did not match exactly with the quantity of materials ordered. reasons provided by the production managers included the unavailability of materials in the suppliers' domain, counting errors and wrong deliveries, among others."
"the identification of stocks, flows, and converters in sclm makes it possible for an ithink model to be created. referring to table 2 and the sclm presented in figure 11 and adding additional process variables which will enhance the algebraic relationship between process variables, ithink simulation model for various dps was created. a snapshot of ithink model for one frame belonging to the \"produce and deliver\" domain process dp4 is shown in figure 12 . similar ithink models for the other dps were created but not presented in this paper for the lack of space."
"further work by the authors and their colleagues has shown how processes can be classified as enterprise domains dms and decomposed into their respective domain processes dps, business processes bps, and elementary activities eas 2-4, 39-43 . in essence, enterprise domains represent functional areas of the enterprise which are decoupled from each other with clearly identified objectives which enable them to be composed of well-defined processes for achieving the objectives defined for the domain. based on the observed goals and associated processes, stand alone processes, called domain processes dps are grouped to reflect the distinctions in goals and deliverables. in a graphical form, the achieved goal of a collection of dms is modelled using suitable templates and this is termed the explicit structural model of the subject me so created is not directly computer executable in the sense that it cannot support quantitative prediction about the many possible time-based me interactions and behaviours that might lead to competitive performance given a set of external or internal change scenarios. this is because the structural models produced do not encode system dynamics, such as events and state transitions. in the authors' studies, this has proven to be both an advantage and disadvantage. it is advantageous because it has systemised and enabled decomposition, captured and explicitly integrated representations of structurally connected sets of relatively simple multiperspective, multilevel of abstraction models which collectively illuminate a big picture of the me. this picture can also be progressively detailed as needs and funds come on stream, and can cope with the presence of high levels of inherent systems of system complexity. but the disadvantage is that quantitative assumption testing is not readily supported."
1 there is the need to provide a structure around the modelling technique. this implies providing a means of specifying actual factors which influence situations in their context of application; in-effect modelling in context. 3 there is the need to develop a methodology which addresses the requirements specified in 1 and 2 .
"in the \"pack and dispatch\" business process, it was also understood that packaging technology, and availability of packaging materials were factors which affected the packaging rate. in the same way delivery rules, despatch priorities, and availability of despatch vans affected the despatch rate. packaging cost can be estimated by deriving the unit cost per product packaged from the resources and materials required for packaging. finally, the increase in the number of products despatched increase the customer stock."
"an approach to integrate the prior knowledge existing in databases with the specific insight provided by phospho-proteomics data was recently introduced and implemented in the tool cellnopt (cellnetoptimizer; www.cellnopt.org) [cit] . cellnopt uses stochastic optimization algorithms (in particular, a genetic algorithm), to find the boolean logic model compatible that can best describe the existing data. while cellnopt has proved able to train networks of realistic size, it suffers from the lack of guarantee of optimum intrinsic of stochastic search methods. furthermore, it scales poorly since the search space (and thus the computational time) increases exponentially with the network size."
"the iscb curriculum task force undertook the task of identifying some of the breadth of needs for bioinformatics education, as described in a series of reports from the task force. this effort arose first from a series of surveys of current training practice and desired training needs [cit], which identified a set of broad categories of training needs but also widespread disparities across programs in what was taught, how, and for what intended target audiences. an outcome of these surveys was the need for identifying a set of core competencies as broad categories of skills and training that cross different programs and training needs and that can provide a basis for discussing similarities and differences between programs and desired outcomes. this led to a further effort to define a set of initial core competencies [cit] that in turn led to an intensive program of community engagement to refine these competencies to better serve the breadth of needs of the bioinformatics training community."
"compared to conventional compound or protein representations using molecular descriptors or pfam domains, the encoded representations learned from novel structurally-annotated sps sequences and smiles strings improve both predictive power and training efficiency for various machine learning models. given the novel representations with better interpretability, we have included attention mechanism in the unified rnn-cnn models to quantify how much each part of proteins or compounds are focused while the models are making the specific prediction for each compound-protein pair. we have chosen sse as the resolution for interpretability due to the known sequence-size limitation of rnn models [cit] . but one can easily increase the resolution to residue-level by simply feeding to our models amino-acid sequences (preferentially of length below 1,000) instead of sps sequences."
"non-biofeedback conditions. after determination of ss-gs, five walking trials were conducted consisting of ninety seconds of acquired data in the absence of biofeedback. in each trial, treadmill speed was imposed at one of five percentages of the subject's ss-gs [80%, 90%, 100%, 110%, 120%] in a randomized order. for each gs, ss-sl was calculated as the mean sl measured at that treadmill speed and utilized for the definition of subsequent desired sl values at each gs. biofeedback conditions. after determination of ss-sl for all five gs conditions, ten additional walking trials were conducted consisting of ninety seconds of data acquisition, two for each treadmill speed value, using biofeedback to cue a desired sl. for each gs, the desired sl was set to be either 17% greater or 17% smaller than the ss-sl at that gs, in a random order. the range of change in sl values was specified based on previous studies showing feasibility of achieving distinguishable gait kinetics when sl was modulated by 17% of the ss value [cit] . the investigator initiated data acquisition for each condition when the subject sufficiently achieved the cued sl condition specified via biofeedback."
"we caution that these core competencies are not, and are not intended to be, a prescription for a specific set of curricula or curricular standards. while the competencies highlight common points of focus across training scenarios, few points escape dissent. the field is still figuring out what it means to be trained in bioinformatics or how best to provide that training. we do not expect that state of affairs to end in the near future. nonetheless, we hope that having a framework in which we can evaluate how different programs define and service their training needs will prove valuable in the maturation of bioinformatics as a discipline."
"in a custom matlab (mathworks, natick, ma, usa) script, hip, knee, and ankle joint angles and moments for the right leg in the sagittal plane were extracted and filtered with a 2 nd order low-pass zero-shift butterworth filter with a cut-off frequency of 15 hz. gait cycles were segmented between subsequent heel strike events, defined as the instants at which the vertical ground reaction force changed in value from zero to positive, and remained positive for a minimum of 400ms. due to events such as marker occlusion or subjects' foot stepping on the contralateral force plate, acquired data were manually screened and some gait cycles were excluded from the analysis. a minimum of 25 segmented gait cycles were linearly resampled in the [cit] % gait cycle domaint and averaged at each point in gait cycle to yield an average hip, knee, and ankle joint moment profile tðtþ for each of the 15 experimental conditions for each of the 20 subjects."
"when applied to case studies on drugs of known target-selectivity, our models have successfully predicted target selectivity in all cases whereas conventional compound/protein representations and machine learning models have failed some. furthermore, our analyses on attention weights have shown promising results for predicting protein binding sites as well as the origins of binding selectivity, thus calling for further method development for better interpretability."
"there were three major steps in the development of the core competencies: (1) defining the competencies needed for using bioinformatics, (2) defining a variety of user profiles describing distinct subgroups in need of training, and (3) defining how the competencies will apply to each user profile (scoring). the core competency framework was developed through an iterative process with input from multiple parties from diverse backgrounds with a connection to bioinformatics. in order to gain a broader appreciation of which competencies the bioinformatics community considers relevant for different bioinformatics user profiles, the iscb curriculum task force has run several competency workshops (discussion sessions for defining the competencies and their applications) both at iscb conferences and at other bioinformatics education venues such as the goblet (global organisation for bioinformatics learning, education and training) annual general meeting. each iteration of a competency workshop has greatly enhanced not only the competencies themselves but also the definitions of the user profiles [cit] and the competency-use case scoring mechanism."
"however, fewer investigations have focused on the joint kinetics associated with the modulation of spatiotemporal parameters such as sl or tla, and studied this effect at multiple gs values. summed joint work has been observed to be strongly correlated with sl in both young and old adults, where young adults primarily utilized swing phase hip work to modulate sl and old adults utilized ankle and knee joint work [cit] . an early investigation found stance phase peak knee extension moment and peak knee flexion moment to be strongly correlated with increasing sl [cit] . a more recent and in-depth investigation found that as sl increased, peak ankle plantarflexion moment, plantarflexion moment at 40% of stance, and peak knee extension moment all increased, while peak knee flexion moment and peak hip flexion moment decreased [cit] ."
"we have also introduced attention mechanism to unified rnn-cnn models. the goal is to both improve predictive performances and enable model interpretability at the level of \"letters\" (sses in proteins and atoms in compounds). attention models (for proteins and compounds) are jointly trained with the rnn encoder and the cnn part. learned parameters of theirs include attention weights on all \"letters\" for a given string. compared to that in unsupervised learning, the attention model here outputs a single vector as the input to the subsequent 1d-cnn model."
"subjects were exposed to a total of fifteen experimental conditions, determined as the combinations of two factors: i) gs, with five levels, and ii) sl, with three levels. factor levels were defined in terms of percent change relative to subject self-selected (ss) values to accommodate inter-subject variability in gait parameters. moreover, to account for the correlation between gs and sl [cit], we first measured self-selected stride length (ss-sl) at all speeds, and defined biofeedback-modulated sl conditions as percent changes of sl relative to the ss-sl at any given speed. this experimental setup allowed us to investigate joint kinetics underscoring an increase or decrease of sl relative to the subject's self-selected stride length, at all speeds."
"cells perceive extracellular information via receptors that trigger signaling pathways that transmit this information and process it. among other effects, these pathways regulate gene expression (transcriptional regulation), thereby defining the response of the cell to the information sensed in its environment. over decades of biological research we have gathered large amount of information about these pathways. nowadays, there exist public repositories such as pathways commons [cit] and pathways interaction database [cit] that contain curated regulatory knowledge, from which signed and oriented graphs can be automatically retrieved [cit] . these signed-oriented graphs represent molecular interactions inside the cell at the levels of signal transduction and (to a lower extent) of transcriptional regulation. their edges describe causal events, which in the case of signal transduction are related to the molecular events triggered by cellular receptors. these networks are derived from vast generic knowledge concerning different cell types and they represent a useful starting point to generate predictive models for cellular events."
"pairwise comparisons for the hip joint show that during increased sl conditions (fig 5, right), hip flexion moment during early swing and hip extension moment during late swing decreased. the first comparison reached significance in three out of five gs conditions, while the second effect was significant at all gs values. no effect of sl on hip joint moment during stance were observed in more than one gs condition. a similar pattern was observed when sl is decreased via biofeedback (fig 5, left) ."
"with the purpose of identifying location and amplitude of application of pulses of torque that would approximate the sl-specific difference between joint moment profiles measured at all speeds, we performed statistical analyses to determine if any of the outcome measures, pulse magnitude and location, were significantly modulated by any of the three independent variables (i.e., joint, direction of sl modulation, and sign of applied pulse). for the purposes of our analysis, pulse magnitude is the absolute value of pulse amplitude. four separate linear mixed effects models (sas v9.4, sas institute, cary, nc) were performed on the one and two pulse approximations for both the pulse magnitude and location data sets to test the null hypothesis that no independent variable had an effect on the outcome measures. the models included fixed effects for each of the independent variables as well as all two-way and one three-way interaction between them. heterogeneity due to trials completed under different gait speed conditions, and multiple pulses in the case of the two pulse approximation were accounted for by the inclusion of random effects. correlation between multiple measurements taken on the same subject were accounted for by the inclusion of a repeated measure effect. upon comparing nested model akaike information criterion (aic) values, the lowest aic value came from the unstructured covariance structure and was therefore selected for the final models."
"computational methods to infer and analyze signaling networks from highthroughput phospho-proteomics data are less mature than for transcriptional data, which has been available for much longer time [cit] . in particular, the infererence of gene regulatory networks from transcriptomics data is now an established field (see [cit] for a review). in comparison to transcriptomics, data is harder to obtain in (phospho) proteomics, but prior knowledge about the networks is much more abundant, and available in public resources as mentioned above."
"the results of the linear mixed effects model analyses for torque pulse magnitude are shown in tables 1 and 2 . a significant effect of factors joint and pulse sign, and a significant interaction between factors joint and sl modulation were observed for the one and two pulse approximations. the interaction between factors joint, sl modulation, and pulse sign was significant for the two pulse approximation. the tukey-kramer post hoc test for the joint and sl in the one and two pulse approximations, for both sl modulation conditions, the normalized torque pulse magnitudes of the knee joint were greater than both the hip and ankle joints. the only significant difference between hip and ankle joint magnitudes existed for the positive sl modulation condition for the two pulse approximation."
"given that sps and smiles strings are interpretable and attention models between rnn encoders and 1d convolution layers can report their focus, we pinpoint sses in proteins and atoms in compounds with high attention scores, which are potentially responsible for cpis. to assess the idea, we chose 3 compound-protein pairs that have 3d crystal complex structures from the protein data bank; and extracted binding site residues (their sses are regarded ground truth) for each protein from ligplot diagrams provided through pdbsum [cit] . based on attention scores α j 's from the single unified rnn-cnn model, we picked the top 10% (4) sses as predicted binding sites. table 5 shows that, compared to randomly ranking the sses, our approach can enrich binding site prediction by 1.6∼2.0 fold for the three cpis. we delved into the predictions for factor xa-dx-9065a interaction in fig. 3 (the other 2 are in sec. 5 of supplementary data). the red β strand was correctly predicted to be at the binding site with a high rank 2, thus a true positive (tp). the sse ranked first, a false positive, was its immediate neighbor in sequence. another sse (cyan loop with two residues labeled above the compound), although a false negative ranked 11th, is structurally close to the true positive; and along with the true positive it forms the cavity to accommodate the compound. therefore, in the current unified rnn-cnn model with attention mechanism, wrong attention could be paid to sequence neighbors of ground truth; and additional information (for instance, 2d contact maps or 3d structures of proteins, if available) could be used as additional input to reduce false negatives."
"our analyses provide concrete illustrations of the potential applications, in our opinion under-explored, of asp in this field. recently, integer linear programming (ilp) have been used to solve the same problem that we described here [cit] . in principle, ilp solvers can also provide the complete set of optimal solutions but a detailed comparison between asp and ilp for this particular problem remains to be done."
"processing. raw marker trajectories were labeled offline. marker position and force/torque data were fed into a standard visual3d processing pipeline, which included i) noise gating of measured force with a 25 n threshold ii) low-pass filtering of marker and force/torque data (butterworth filter at 6 hz and 30 hz cut-off frequency, respectively), iii) interpolation of missing marker data with a third order polynomial fit for a maximum gap size of five samples, iv) application of the subject mass, height, and standing calibration marker positions to an anatomical template to derive a 7 segment (pelvis, thighs, shanks, and feet) link-segment subject model, and v) use of an optimization algorithm applied to inverse kinetics and kinematics equations to obtain joint angles and net moments."
"figs 8 and 9 show the distribution of torque pulse magnitudes grouped by joint for both positive and negative sl modulations for the one and two pulse approximations, respectively."
"significant effects of sl modulation were also observed at the hip joint. as indicated by the main effect of sl quantified by the continuum analysis, there are two major intervals of significance: early swing and late swing. these effects in swing are visible upon inspection of the group moment profiles where an increase in sl is associated with a decrease in flexion moment during early swing and a decrease in extension moment during late swing. these findings are in contrast with a previous study which only observed a decrease in peak hip flexion moment [cit] . the statistical significance of these observations is clearly indicated by the pairwise comparisons of sl modulation at different speeds shown in fig 5. here, in all pairwise comparisons, an increase in sl is associated with a significant interval in early swing and in late swing. another indication on how sl modulates hip moment during the swing phase is provided by the torque pulse approximation histograms, which consistently depict a grouping of negative pulses in late swing for positive change in sl and positive pulses in late swing for negative change in sl. however, the pattern is less clear than the one seen at the knee joint because of the small magnitude of those pulses occurring in the swing phase, which even though they are representative of a statistically significant effect, they account for a small amplitude (see distribution of larger pulses in figs 12 and 13 ). as such, the effect in sl obtained for a change in magnitude of the applied torque is not symmetrical (tables 7 and 8), as demonstrated by the tukey-kramer post hoc tests for both pulse approximations. these groupings of pulses in late swing are likely associated with the decrease in hip extension moment during late swing associated with increasing sl."
"the continuum analysis showed an effect of gs on the normalized joint moment profiles, where a significant effect of gs was detected for a total 82.4%, 80.4% and 64.7% of the gait cycle for the hip, knee, and ankle joint respectively, as shown in fig 4. the effect of sl on hip joint moment was significant for seven short clusters in early to midstance, at push-off, and two clusters spanning the majority of swing for a total duration of a significant effect of sl on hip joint moment of 56.1% of the gait cycle. a stronger effect, both in magnitude and duration, was measured at the knee joint with the first two clusters spanning early stance, and continuing in the swing phase. the effect of sl on knee joint moment was highly significant from midstance until the end of gait cycle; for a total of 91.2% of the gait cycle with a significant effect. a strong effect of sl was detected at the ankle joint for 5 clusters; at weight acceptance, push-off and three clusters covering approximately half of swing for a total of 41.7% of the gait cycle. the interaction between the two factors was significant for the hip for four clusters; mainly during the transition from push-off and during late swing for a total of 33.9% of the gait cycle. for the knee, the interaction was significant for four clusters; during early stance, late stance, early swing, and midswing for a total of 35.9% of gait cycle. for the ankle, the interaction was only significant for two clusters, late stance and mid swing for a total of 19.1% of gait cycle. pairwise comparisons of joint moment profiles measured at nominal and biofeedback-modulated sl values are shown in figs 5-7 for all gss."
"for unlabeled compound data from stitch, we randomly chose 500k samples for training and 500k samples for validation (sizes were restricted due to computing resources) and then removed those whose smiles string lengths are above 100, resulting in 499,429 samples for training and 484,481 for validation. for unlabeled protein data from uniref, we used all uniref50 samples (50% sequence-identity level) less those of lengths above 1,500, resulting in 120,000 for training and 50,525 for validation."
"in a first run, the asp implementation allowed us to compute the minimal score of the optimization problem. afterwards, we run the asp solver again to enumerate all the models having a score lower or equal than the minimal score."
"our study has measured the effects of stride length (sl) on the lower extremity joint moment profiles at different speeds, demonstrating several consistent effects in our population. the main effects of increasing sl at the knee include an increase in knee extension moment at early stance and an increase in flexion moment at late stance. at the hip, the main effects of increasing sl are a decrease in flexion moment during early swing and a decrease in extension moment during late swing. for an increase in sl, the ankle primarily exhibits an increase in dorsiflexion moment during loading response. given the observed linear relationship between the effect of stride length on joint kinetics sl 0 and tla, pulse torque approximation patterns associated with sl modulation are also associated with tla modulation. these findings suggest that a possible joint moment assistance strategy based on pulses of torque applied primarily at the hip and the knee joint could induce modulations in both sl and tla. according to our analysis, the application of an extension pulse of torque in early stance and a flexion pulse in late stance to the knee appear to be suitable assistance strategies to support an increase of sl and tla during walking. if pulse the effect of stride length on joint kinetics torque assistance is to be applied at an additional joint, pulse torque assistance could be applied at the hip with a flexion pulse applied during late swing. this study has some limitations. the methods pursued in this paper are based on a group analysis of joint moment profiles measured via inverse-dynamics. as such, it is possible that the most successful assistance strategies may significantly change between different individuals. therefore, the group analysis based assistance strategy candidate could be best utilized as an initial estimate and assistance strategies could be iteratively optimized for each subject using human-in-the-loop optimization, like it has been done for single-joint assistance schemes [cit] ."
"structural property sequence (sps) representation. although 3d structure data of proteins is often a luxury and their prediction remains a challenge without templates, it has been of much progress to predict protein structural properties from sequences [cit] . we used sspro/accpro [cit] to predict secondary structure class (α-helix, β-strand, and coil) and solvent accessibility (exposed or not) for each residue and group neighboring residues of the same secondary structure class into secondary structure elements (sses). the details and the pseudo-code for sse are in supplementary data (algorithm 1)."
"we went on to test how well our unified rnn-cnn models could predict certain drugs' target selectivity, using 3 sets of drug-target interactions (including 1 in supplementary data) of increasing prediction difficulty. our novel representations and models successfully predicted all target selectivities whereas baseline representations and the shallow model random forest failed some."
"the biological problem that we tackle in this work is essentially a combinatorial optimization problem over the possible logic models representing a given pkn. in this section, first we introduce the graphical representation of logic models by giving a simple example that motivates our formalization. then, we give a formal definition for the inputs of the problem, we formally define a protein signaling logic model (pslm) and we show how predictions are made for a given model. finally, we define an objective function used for the optimization over the space of possible logic models."
"thereafter, we look for the truth assignment such that r ∧ s ∧ k evaluates to true and which represents the lss of the network for the given initial conditions. the biological meaning behind this concept is that the input (stimuli ) signals are propagated through the network by using the faster reactions and after some time, the state of each protein will not change in the future. thus, we say that the network is stabilized or that it has reached an steady state [cit] . finally, we can define the model prediction under a given experimental condition as follows."
"protein-tyrosine kinases and protein-tyrosine phosphatases (ptps) are controlling reversible tyrosine phosphorylation reactions which are critical for regulating metabolic and mitogenic signal transduction processes. selective ptp inhibitors are sought for the treatment of various diseases including cancer, autoimmunity, and diabetes. compound 1 [2-(oxalylamino)-benzoic acid or oba] [cit] and its derivatives, compound 2 and 3 (pubchem cid: 44359299 and 90765696), are highly selective toward ptp1b rather than other proteins in the family such as ptpra, ptpre, ptprc and shp1. ptp1b, ptprc and shp1 did not exist in the training set. and ptpra and ptprc only had 6 and 3 examples involving other compounds in the training set."
"the proposed solution encodes the optimization problem in answer set programming (asp) [cit] . asp is a declarative problem solving paradigm from the field of logic programming. distributed under the gnu general public licence, it offers highly efficient inference engines based on boolean constraint solving technology [cit] . asp allows for solving search problems from the complexity class np and with the use of disjunctive logic programs from the class σ p 2 . moreover, modern asp tools allow handling complex preferences and multicriteria optimization, guaranteeing the global optimum by reasoning over the complete solution space."
"the first and second rules simply declare which nodes have fixed truth values because they are either an input node, or an inhibited node in a particular experiment. thereafter, the third and fourth rules declare the truth assignments that are given by the experimental condition."
"to tackle the problem, we have designed interpretable yet compact data representations and introduced a novel and interpretable deep learning framework that takes advantage of both unlabeled and labeled data. specifically, we first have represented compound sequences in the simplified molecular-input line-entry system (smiles) format [cit] and protein sequences in novel alphabets of structural and physicochemical properties. these representations are much lower-dimensional and more informative compared to previously-adopted small-molecule substructure fingerprints or protein pfam domains [cit] . we then leverage the wealth of abundant unlabeled data to distill representations capturing long-term, nonlinear dependencies among residues/atoms in proteins/compounds, by pre-training bidirectional recurrent neural networks (rnns) as part of the seq2seq auto-encoder that finds much success in modeling sequence data in natural language processing [cit] . lastly we develop a novel deep learning model unifying rnns and convolutional neural networks (cnns), to be trained from end to end [cit] b ) using labeled data for task-specific representations and predictions. moreover, we introduce attention mechanism to interpret predictions by isolating main contributors of molecular fragments, which is further exploited for predicting binding sites and origins of binding specificity."
"we encode compound smiles or protein sps into representations, first by unsupervised deep learning from abundant unlabeled data. we used a recurrent neural network (rnn) model, seq2seq [cit], that has seen much success in natural language processing and was recently applied to embedding compound smiles strings into fingerprints [cit] . a seq2seq model is an auto-encoder that consists of two recurrent units known as the encoder and the decoder, respectively (see the corresponding box in fig. 1 ). the encoder maps an input sequence (smiles/sps in our case) to a fixed-dimension vector known as the thought vector. then the decoder maps the thought vector to the target sequence (again, smiles/sps here). we choose gated recurrent unit (gru) [cit] as our default seq2seq model and treat the thought vectors as the representations learned from the smiles/sps inputs. the detailed gru configuration and advanced variants (bucketing, bidirectional gru, and attention mechanism which provides a way to \"focus\" for encoders) can be found in sec. 1.4 of supplementary data."
"robot-assisted training is a promising tool under development for improving walking function [cit] . a primary indicator of gait performance improvement is gait speed (gs), which is associated with a better quality of life [cit] and overall functional status [cit] . currently, it is not well understood how the modulation of assistance provided by a robot during gait training will lead to changes in gs. the gait parameter of gs is known to be correlated with anteriorposterior ground reaction force, the propulsive force of the foot against the ground [cit] . furthermore, propulsive impulse, the propulsive force integrated over time, is associated with the posture of the trailing limb at push-off [cit] . the posture of the trailing limb at push-off is quantified by one kinematic parameter, known as trailing limb angle (tla), defined as the angle of the line connecting the hip joint center and foot center of pressure at the instant of peak propulsive force, relative to the global vertical axis [cit] . in healthy control subjects, it was observed that when increasing gs, the increase in tla contributes twice as much as the increase in ankle moment to the resulting increase in propulsive force [cit] . in older adults exposed to a biofeedback paradigm for increasing propulsive force, increased tla and decreased hip flexor power were the primary means by which they increased propulsive force [cit] . therefore, tla has been advanced as a variable of interest for robot-assisted gait training paradigms aimed at improving walking function. our research group is exploring the use of robot-assisted gait training to directly target and modulate tla, thus allowing subjects to modulate gs. a possible controller could be composed of torque pulses applied at specific instants during the gait cycle, with the advantage of not constraining gait to follow prescribed trajectories [cit] . this approach has been shown in previous studies to be a successful method of robot-assisted gait training [cit] . however, in the absence of models of the human response to a robotic input, it would be difficult to define parameters for such a controller acting on multiple degrees of freedom. a possible method of controller formulation can be derived from the principle of bio-inspiration, where a robot is controlled to apply the difference in joint moment applied by human subjects when they achieve a desired gait feature (in this case modulation of tla), relative to their normal walking condition. once the effects of the variable of interest have been identified, a rehabilitation robot could be controlled in either assistive, resistive, or perturbation mode to deliver different forms of robot-assisted training [cit] . to support the development of such a controller, we first required knowledge about the joint moments applied by healthy control subjects to modulate tla at a range of gss. since tla has not been a primary measure of interest in the biomechanics literature, we extended our search to a more common variable, likely correlated to tla, such as stride length (sl)."
"to assess how useful the learned/encoded protein and compound representations are for predicting compound-protein affinity, we compared the novel and baseline representations in affinity regression using the labeled datasets. the representations were compared under the same shallow machine learning models -ridge & lasso regression and random forest (rf). from table 1 we found that our novel representations learned from smiles/sps strings by seq2seq models outperform baseline representations of k-hot encoding of molecular/pfam features. for the best performing random forest models, using 40% less training time and 20% less memory, the novel representations achieved roughly the same performance over the default test set as the baseline ones and significantly lowered root mean squared errors (rmse) for two of the three generalization sets whose target protein classes (estrogen receptors / er and ion channels) are not included in the training set. similar improvements were observed on k i and k d predictions in tables s5 and s7 (supplementary data), respectively. these results show that learning protein and compound representations from even unlabeled datasets alone could improve their context-relevance for various labels."
"since each subset generated by the rules in (5) represents a possible conjunction clause, we can generate all possible logical formulas in dnf by considering each subset as either present or absent. the first rule is a choice rule that declares the non-deterministic generation of predicates clause/3 from a subset. a clause represents the conjunction of all the elements included in the subset. the second rule declares an integrity constraint to avoid the generation of redundant logical formulas by using the predicates generated in (6) ."
"for dx-9065a interacting with either factor xa or thrombin, we ranked the sses based on the attention scores from the unified rnn-cnn single model and assigned each segment the same rank as its parent sse. we then calculated the rank difference for each segment between factor xa and thrombin interactions and re-ranked the segments (fig. 4) . the groundtruth segment (black) was ranked the 4 th among 50 segments."
"bioinformatics short courses: european bioinformatics institute (embl-ebi) and university of cambridge. both embl-ebi (www.ebi.ac.uk/training) and the university of cambridge (ucam, http://bioinfotraining.bio.cam.ac.uk/) offer extensive programmes of short courses that enable the research community to gain competency in bioinformatics. these programmes differ from the full-time curricula described above in that they are aimed at individuals already pursuing a research career. most of the scientists attending these courses are phd students, postdoctoral researchers, or more senior researchers (in academia or in industry), who are performing data-intensive experiments and need guidance on experimental design, data analysis, and interpretation. as a proof of principle for elixir, europe's distributed infrastructure for biological data with nodes in 20 countries, embl-ebi and ucam recently performed an exercise to map their course programmes to the iscb competency framework. the goal was to identify any gaps in training provision and also to rapidly check the robustness of the competency profile-in total they looked at 50 short courses offered by ucam and 21 at embl-ebi, covering a wide range of topics aimed primarily at bioinformatics scientists and bioinformatics users. both programmes included coverage of all the competency areas, with only a very small number of courses increasing competence in a, general biology (this is already well developed in the target audience, many of whom have postgraduate degrees in the biological sciences) and a high proportion of the courses increasing competence in f, bioinformatics tools and their usage (48 courses from ucam; 20 courses from embl-ebi); d, details of the scientific discovery process and the role of bioinformatics in it (34 courses from ucam; 20 courses from embl-ebi); and n, effective communication of bioinformatics problems, issues and topics (28 courses from ucam; 20 from embl-ebi). two competency areas were identified that they felt were not adequately covered by the existing framework and that they would like to propose adding: data curation for dissemination of research data (for example, the annotation of data required when submitting data sets to public databases, and the annotation of data performed by professional biocurators who add value to these resources) and data curation for analysis of research data (for example, annotation of a newly sequenced genome to find orthologues/paralogues or to gain a functional overview of the genome). this exercise, if performed across all of the elixir nodes, will help to understand the impact of elixir's training portfolio for different target audiences and will enable them to shape our offering accordingly. mapping existing short courses to bioinformatics core competencies could also be used to help individuals along a learning path, taking them from one competency level to the next."
"as a starting point, the group used the iscb core competencies and a policy paper that defined the role of clinical bioinformaticians to draft a rough list of competencies; the group also created a list of different types of healthcare professionals likely to be impacted by the 100,000 genomes project. each member of the group then consulted with colleagues and the wider community, asking them to provide information on which competencies were required to make use of the 100,000 genomes data, and requesting participants to think about whether any additional competencies are required. at least five representatives of each profession were consulted, and all input was combined to create a consensus competency profile. this consensus view, published in a white paper, 'developing clinical bioinformatics training in the nhs' (https://www.genomicseducation.hee.nhs.uk/images/publications/developing_nhs_ clinical_bioinformatics_training.pdf), captures not only which competencies are required by the professions listed but also an indication of the level of expertise required, from no knowledge through general awareness and working knowledge to specialist expertise. the profile does not provide guidance on the evidence required to assess whether an individual has gained each of the required competencies, but this would be an obvious next step."
"our main conclusion here is as follows: in both cases, due to the use of in silico data, models with perfect fit were exhibited by both approaches. the main advantage of the formal approach is to be able to explicitly compute the minimal score, allowing us to enumerate all models with this score in a very short time. meanwhile, genetic algorithms are not able to exhibit this information and therefore cannot develop strategies to compute all minimal models. at the same time, this leads to the question about the biological relevance of optimal models and if it is possible to discriminate between them. a precise study of the biological pathways selected in each optimal model did not allow us to specifically favor one model according to biological evidences. that is why we choose to show the union of them in each case (fig. 2(c) and fig. 3(c) )."
"using the generalization sets, we proceed to explain and address our unified rnn-cnn models' relatively worse performances for new classes of protein targets without any training data. we first noticed that proteins in various sets have different distributions in sps words. in particular, the testing set, ion channels, gpcrs, and estrogen receptors are increasingly different from the training set (measured by jensen-shannon distances), which correlated with increasingly deteriorating performance relative to the training set (measured by the relative difference in rmse) with a pearson correlation coefficient of 0.84 ( fig. s4 in supplementary data) ."
"in addition to its long-standing bachelor of engineering in bioinformatics, unsw recently introduced a bachelor of science bioinformatics major emphasising the use of existing bioinformatics methods for biological discovery rather than the design of new bioinformatics methods. the core competencies were used to guide the design of the program by identifying the competencies to emphasize relative to the engineering program (b, c, and d) and those for which a lower level of achievement was acceptable (g, h, j, k, m, o). this in turn guided the choice of courses for the bachelor of science major."
"learning framework for a metacurricular resource: the coursesource bioinformatics learning framework. coursesource (http://www.coursesource.org) is \"an open-access journal of peer-reviewed teaching resources for undergraduate biological sciences\" [cit] . coursesource organizes its resources by biological disciplines (e.g., evolution, genetics, molecular biology, bioinformatics) that play integral roles in biology. each discipline has an associated framework of learning goals and objectives that undergraduate students in the biological sciences should have reached by the time they have completed their degree. the iscb curriculum and competency guidelines were used as a model to develop the bioinformatics learning framework. the framework can be viewed at http://www.coursesource.org/courses/bioinformatics. it represents a practical application of the guidelines and provides an elaboration of the guidelines to a level appropriate for implementation in classroom settings."
"first, we focused in finding minimal pslms compatible with the given pkn and predicting the generated dataset for the middle (see fig. 2 ) and large-scale (see hal-00737112, version 1 -1 [cit] fig. 3 ) benchmarks. second, general comparisons between our logical approach implemented in asp and the genetic algorithm implemented in cellnopt, were performed over the 240 datasets generated as described in section 3.2."
"all of the pairwise comparisons conducted to assess the symmetry of the effect for a reversal of sl conditions are reported in tables 7 and 8 . in the one pulse approximation, both pairwise comparisons for the hip and ankle joint yielded a relatively large differences (9-11% gait cycle duration) in mean location, with three out of the four comparisons statistically significant. on the contrary, the pairwise comparisons for the knee yielded small (1-2% gait cycle duration) and statistically insignificant mean differences. this indicates that for the knee joint, clustering of torque pulses by location was symmetrical in reversed sl conditions, with negative pulses in negative sl conditions clustering around a similar value as positive pulses in positive sl conditions, and positive pulses in negative sl conditions clustering around a similar value as negative pulses in positive sl conditions, while the same effect was not measured for the hip and the effect of stride length on joint kinetics ankle joints. however, this pattern was not observed in the two pulse approximation; in which one out of the four hip and ankle joint comparisons and one of the two knee joint comparisons were statistically significant. for the one and two pulse approximations, all knee joint mean comparisons were below 10% gait cycle, the width of the torque pulses used for the approximation."
"initially, the mapping of bioinformatics competencies to audiences considered three major user profiles: (1) the bioinformatics user; (2) the bioinformatics scientist; and (3) the bioinformatics engineer. early competency workshops quickly surmised that these user profiles were too narrow and did not adequately capture the breadth of roles requiring bioinformatics competency and curriculum. participants spent much of the workshop time defining a bioinformatics user or distinguishing a bioinformatics scientist from a bioinformatics engineer. the use case roles were subsequently expanded to better embody the breadth of bioinformatics users, including physicians, lab technicians, ethicists and biocurators, scientists (which include the discovery biologist, academic bioinformatics researcher and core facility scientist), and engineers (which may be a bioinformatician in academia, bioinformatician in research institute, or software engineer). this change allowed for subsequent workshop participants to self-select according to the category of user with which they most identified."
"our goal is to provide an asp solution for learning pslms from experimental observations under several experimental conditions (definition 9). here we provide a logic program representation of the problem described in section 2 in the input language of the asp grounder gringo [cit] . after describing the format of any input instance, we show how we generate non-redundant candidate solutions having an evidence in the given pkn, then we describe how model predictions are made and finally, we show the minimization of the objective function."
"the carnegie mellon/university of pittsburgh joint phd in computational biology offers an example at another extreme of the spectrum: a full multi-year training program for students expected to become experts in computational biology, who are expected to graduate competent to lead independent research programs in the area, teach computational biology, run bioinformatics core facilities, or pursue similarly demanding jobs. computational biology programs face a special challenge compared with more traditional degree programs, in that the lack of clear standards for training at the undergraduate level means that there is little one can assume or enforce about background knowledge of incoming students beyond basic competencies in biology, computing, and mathematics. furthermore, since a phd program is research-focused and under pressure to limit time to degree, formal training can occupy only a finite amount of a student's time, equivalent to roughly a year of full-time coursework. to a limited degree, the program can rely on admissions standards, remediation, and self-teaching to assume some basics of all students (a,f,i,j). some competencies can be handled by flexible menu-based requirements to meet a competency in ways appropriate to each student's individual needs and background (b). in others, every student needs a high level of competency and this must be met with specialized core classes designed for this population (c,d,e,g,h) . others must be met within the curriculum through specialized professional development mechanisms as well as one-on-one mentorship by the thesis advisor (k,l,m,n,o,p). nonetheless, some competencies, especially those that depend on the mentorship of the research advisor, may be acquired much more effectively by some students than others. the competencies again suggest that these topics should be flagged for consideration for more formal training in the future. furthermore, the challenges faced by this program with respect to knowledge of incoming students make clear the value that accepted standards for competencies at the undergraduate level could have in making most effective use of time in graduate school for specialists in the field."
"the work of the task force identified a pressing need for bioinformatics education but also tremendous variability in the details of this need and widespread confusion about how to meet it for diverse target user populations and training contexts. the effort to develop and successively refine a set of core competencies for bioinformatics training has sought to assist educators in this domain by providing a conceptual framework in which the field can more productively share experiences and pool our efforts in identifying best practices for bioinformatics education in the face of divergent needs and expectations. several years of community engagement efforts and subsequent refinements have brought us ever closer to that goal, leading to a broader appreciation of the range of user personas in need of bioinformatics education and a more productive language through which to identify and discuss shared needs and training mechanisms. as the use cases presented here illustrate, the core competencies that arose from this process provide a basis for the community of bioinformatics educators, despite widely divergent goals and student populations, to draw upon their common experiences in designing, refining, and evaluating their own training programs."
"structure-based methods can predict compound-protein affinity, i.e., how active or tight-binding a compound is to a protein; and their results are highly interpretable. this is enabled by evaluating energy models [cit] on 3d structures of protein-compound complexes. as these structures are often unavailable, they often need to be first predicted by \"docking\" individual structures of proteins and compounds together before their energies can be evaluated, which tends to be a bottleneck for computational speed and accuracy [cit] . machine learning has been used to improve scoring accuracy based on energy features [cit] ."
"using the novel representations we next compared the performances of affinity regression between the best shallow model (random forest) and various deep models. for both separate and unified rnn-cnn models, we tested results from a single model with (hyper)parameters optimized over the training/validation set, averaging a \"parameter ensemble\" of 10 models derived in the last 10 epochs, and averaging a \"parameter+nn\" ensemble of models with varying number of neurons in the fully connected layers ((300,100), (400, 200) and (600,300)) trained in the last 10 epochs."
"representations, random forest correctly predicted ptp1b selectivity for compounds 1 and 3 but not compound 2, whereas unified rnn-cnn models correctly did so for all three compounds."
"for the knee, during increased sl conditions (fig 6, right), knee extension moment increased in early stance, while knee flexion moment increased in late stance. during the swing phase, knee extension moment decreased in early swing, and knee flexion moment the effect of stride length on joint kinetics decreased in late swing. the effects reported were significant at the group level at all gss. a similar pattern was observed for a decrease of sl, with smaller effects for the increased knee extension at early stance (a significant effect was measured only in four out of five gs conditions, fig 6, right) ."
"clinical bioinformatics: the united kingdom 100,000 genomes project. the need for bioinformatics to infiltrate current clinical practice is urgent, expedited by programs such as the 100,000 genomes project in the uk (https://www.genomicsengland.co.uk/the-100000-genomes-project/), which will sequence 100,000 patient genomes with the goal of using the genomic data to inform clinical decision-making. many different types of healthcare professionals will be impacted by this project. for example, specialist healthcare scientists require training to handle and interpret genomic data; clinical staff involved in recruiting patients to the 100,000 genomes project require training to understand the results of genome sequencing and to counsel patients (and their relatives); and the general workforce requires training to provide awareness of genomic medicine and how it can improve patient care. [cit] health education england convened a \"task and finish group\" in clinical bioinformatics to provide recommendations on training requirements arising not only as an immediate consequence of the 100,000 genomes project but also from the increasing use of biomolecular data in medical practice as a whole. the group decided to tackle the immediate problem by defining the competencies needed by healthcare professionals to enable them to use data emerging from the 100,000 genomes project to inform clinical decision-making. as a proof of principle, the group also mapped these competencies to existing or newly designed training programmes commissioned by health education england, to inform the design of future training programmes for healthcare professionals."
"in the future, the task force plans to detail its guidelines in a manner similar to the coursesource framework. specifically, the plan is to provide an explicit mapping between the competencies and the coursesource framework, which is tailored for life scientists. the taskforce's ultimate goal is to have explicit mappings of courses to competencies for each of the personas in the iscb competency framework. this is already underway for life scientists (with the coursesource framework) and clinical practitioners (with the nhs clinical bioinformatics framework). where there are synergies with other frameworks, we see potential to map these to curricula for other personas; for example, the edison framework for data science has many elements relevant to bioinformatics engineers; the abet framework was indeed used as a basis to develop the iscb competency framework; and the curricula described in this manuscript also provide specific examples that can be generalised into a framework for bioinformatics engineers."
"the overall pipeline of our unified rnn-cnn method for semisupervised learning (data representation, unsupervised learning, and joint supervised learning) is illustrated in fig. 1 with details given next."
"the entire rnn-cnn pipeline is trained from end to end [cit] b), with the pre-trained rnns serving as warm initializations, for improved performance over two-step training. the pre-trained rnn initializations prove to be very important for the non-convex training process [cit] . in comparison to such a \"unified\" model, we also include the \"separate\" rnn-cnn baseline for comparison, in which we fixed the learned rnn part and train cnn on top of its outputs."
"hal-00737112, version 1 -1 [cit] fig. 4 . pipeline of the generation of the 240 benchmarks all together, we obtained a complete enumeration of all minimal models. below, we show the results obtained using the asp-based approach to solve the middle and large optimization problems."
"specialist track in an undergraduate bioengineering program: the university of illinois. at the university of illinois, undergraduate bioengineering majors select a track, one of which is computational and systems biology (csb). students not in the csb track get a small amount of programming experience, but do take a non-majors cs course in their sophomore year that exposes them to matlab and c programming. they also take a junior-level course, computational tools for biological data, that covers basic probability and statistics; hypothesis testing; modelling and simulation; and experimental design and applies these concepts and techniques to human genomic variation; sequence alignment; hidden markov models and gene finding; cancer genomics; and gene regulatory networks. students in the csb track take the computational tools for biological data course described above but have a more rigorous training in mathematics and computer science. specifically, csb students take courses for cs majors, including introductory programming, discrete mathematics, data structures, data mining and bioinformatics. overall, csb students have a rigorous training in mathematics, probability, statistics, and computer science, and take at least two senior-level courses in which techniques from these disciplines are applied in bioinformatics analyses. experience with this population highlights a gap remaining in the competencies, with a population not currently well represented in their use. it suggests a possible direction for future work, as the bioinformatics engineer curriculum working group might extend its guidelines to better encompass the field of bioengineering."
"we used molecular data from three public datasets: labeled compoundprotein binding data from bindingdb [cit], compound data in the smiles format from stitch [cit] and protein amino-acid sequences from uniref [cit] ."
"through unsupervised pre-training, the learned representations capture nonlinear joint dependencies among protein residues or compound atoms that are far from each other in sequence. such \"long-term\" dependencies are very important to cpis since corresponding residues or atoms can be close in 3d structures and jointly contribute to intermolecular interactions."
"from 413,635 ic 50 -labeled samples collected from bindingdb, we completely excluded three classes of proteins from the training set: estrogen receptors (er; 4,037 samples), g-protein-coupled receptors (gpcr; 1,072 samples) and ion channels (5,197 samples), to test the generalizability of our framework. and we randomly split the rest into the training (282,442 samples including 10% held out for validation) and the default test set (120,887 samples) without the aforementioned three classes of protein targets. similarly, we split a k i (k d ) labeled dataset into 124,871 (11,419) samples for training, 53,591 (4,772) for testing, 724 (14) for ers, 825 (131) for ion channels, and 165 (0) for gpcrs. all labels are in logarithm forms: pic 50, pk i, and pk d . more details can be found in sec. 1.1 of supplementary data."
"competency workshops have additionally helped to revise the scoring of competencies for each user profile. early workshops scored the applicability of a bioinformatics competency to a particular profile with a simple yes/no response, which did not allow for an appreciation of the depth of the competency necessary for a given profile. such a scoring approach, while better than no score, would not be helpful when developing a curriculum for a specific user profile. subsequent workshops used a graded scoring approach, with grades ranging from 1 (no competency required) to 4 (specialist knowledge required). this, too, proved too ambiguous to allow for meaningful discussion and classification. the scoring approach was thus revised again to the current model, which uses the bloom's revised taxonomy [cit] terms: knowledge, comprehension, application, analysis, synthesis, and evaluation. while the use of bloom's taxonomy has been useful in mapping competency levels to each of the user profiles, this change required refinement of the competency list as several of the earlier competencies incorporated bloom's taxonomy terms."
"for the ankle, during increased sl conditions (fig 7, right), ankle dorsiflexion moment increased at early stance, while no effect on plantarflexion moment was measured at push-off. a similar pattern was observed for a decrease of sl (fig 7, right), with a greater effect measured in terms of increased plantarflexion moment at early swing (significant at all gss). in two out of five gs conditions, the decreased sl condition exhibited a reduced ankle plantarflexion moment during push-off significant at the group level."
"after successfully predicting target selectivity for some drugs, we proceed to explain using attention scores how our deep learning models did so and what they reveal about those compound-protein interactions."
"the first rule declares that for each experiment, if there is at least one conjunction clause having all its positive literals assigned to true and all its negated literals assigned to false, then the complete clause evaluates to true. while the second rule declares that every node that is not assigned to true, it is assigned by default to false."
"with user profiles better defined, competency workshops then struggled with the competencies themselves and their definitions. several early competency definitions appeared to overlap. for example, \"apply knowledge of computing appropriate to the discipline (e.g., effectively utilize bioinformatics tools)\" closely resembled \"analyze a problem and identify and define the computing requirements appropriate to its solution (e.g., define algorithmic time and space complexities and hardware resources required to solve a problem).\" workshop participants helped to reduce the redundancy in our initial set of bioinformatics competencies from 20 competencies to a refined set of 16 competencies."
", where w is body weight in n, and l l is leg length in m, measured as the distance between the hip joint center and the floor during straight-leg standing."
"continuum analysis. we sought to determine if the two factors gs and sl had any significant effect on the sagittal plane moment profiles for the hip, knee, and ankle joint � tðtþ, and, if so, at which phase of a gait cycle was a significant effect of either factor measured. we conducted an analysis for the main effects of the two factors, gs and sl, by implementing a repeated-measure 2-way anova on the mean joint moment profiles measured from each subject and experimental conditions, spanning exhaustively the 15 combinations of factors. anova was conducted to test the null hypothesis that neither factor (gs and sl), nor their interaction, induce a significant effect on joint moment at any time point."
"with compound and protein representations learned from the above unsupervised learning, we solve the regression problem of compoundprotein affinity prediction using supervised learning. for either proteins or compounds, we append a cnn after the rnn (encoders and attention models only) that we just trained. the cnn model consists of a onedimensional (1d) convolution layer followed by a max-pooling layer. the outputs of the two cnns (one for proteins and the other for compounds) are concatenated and fed into two more fully connected layers."
"to have a first view of the space of solutions, we investigated the role of the model size over the optimization process. indeed, the optimization criteria moderates the choice of a model of minimal size -according to a parsimonious principleby a free parameter related to the fitting between observations and predictions. (see eq. 3). however, as we mentioned above, in all our experiments we known that there exists at least one model which predicts the whole datasets without mismatches and thus, the optimization problem is focused on finding minimal models. therefore strongly favoring the size of the model. to evaluate the impact of this for the middle and large optimization problems depicted in fig. 2 and fig. 3, we used asp to enumerate all the models with perfect fit having their size less or equal to the size of the models found by cellnopt. results are depicted in fig. 5, providing a first insight on the structure of the space of compatible pslms with perfect fit. it appears that the number of compatible pslms increases exponentially with the size of the model. therefore, optimizing over the size criteria appears quite crucial. a prospective issue is to elucidate whether the topology of the space of suboptimal models informs about the biological relevance of minimal models. fig. 6 . executions of asp and cellnopt optimizations that found all global optimal models. the total number of runs was of 240 in account of the in-silico data generated. the xaxis represents the number of global optimal models that each problem had. the y-axis, shows the number of executions where asp and cellnopt found the total number of global optimums."
"we identify three inputs to the problem: a prior knowledge network (pkn), a set of experimental conditions or perturbations and for each of them, a set of experimental observations or measurements. for the sake of simplicity, in this work we have considered only pkns with no feedback loops. they account for the main mechanisms of transmission of information in signaling pathways, but do not include feedback mechanisms that are typically responsible for the switching off of signals once the transmission has taken place [cit] . in what follows, we give a mathematical definition for each of these inputs."
"robot-assisted training is a promising tool under development for improving walking function based on repetitive goal-oriented task practice. the challenges in developing the controllers for gait training devices that promote desired changes in gait is complicated by the limited understanding of the human response to robotic input. a possible method of controller formulation can be based on the principle of bio-inspiration, where a robot is controlled to apply the change in joint moment applied by human subjects when they achieve a gait feature of interest. however, it is currently unclear how lower extremity joint moments are modulated by even basic gait spatio-temporal parameters. in this study, we investigated how sagittal plane joint moments are affected by a factorial modulation of two important gait parameters: gait speed and stride length. we present the findings obtained from 20 healthy control subjects walking at various treadmill-imposed speeds and instructed to modulate stride length utilizing real-time visual feedback. implementing a continuum analysis of inverse-dynamics derived joint moment profiles, we extracted the effects of gait speed and stride length on joint moment throughout the gait cycle. moreover, we utilized a torque pulse approximation analysis to determine the timing and amplitude of torque pulses that approximate the difference in joint moment profiles between stride length conditions, at all gait speed conditions. our results show that gait speed has a significant effect on the moment profiles in all joints considered, while stride length has more localized effects, with the main effect observed on the knee moment during stance, and smaller effects observed for the hip joint moment during swing and ankle moment during the loading response. moreover, our study demonstrated that trailing limb angle, a parameter of interest in programs targeting propulsion at push-off, was significantly correlated with stride length. as such, our study has generated assistance strategies based on pulses of torque suitable for implementation via a wearable exoskeleton with the objective of modulating stride length, and other correlated variables such as trailing limb angle. plos"
"the first rule declares that every subset contains its \"head\" element. the second rule declares that if w is included in t, and if there is another subset having t as its \"tail\", then w is also included in it."
"relatively small effects of sl modulation were observed at the ankle joint. most prominently, during loading response, dorsiflexion moment increases with an increase in sl as also can be observed in the group moment profiles. this is supported by the main effect of sl measured via the continuum analysis, where the first 10% of gait cycle shows a significant effect of gs on joint moment. this is further supported by the observation of a significant increase in dorsiflexion moment (negative change) for increases in sl, in nine of the ten pairwise comparisons at all speeds, shown in fig 7. another observed effect through the pairwise comparisons the effect of stride length on joint kinetics conducted via the continuum analysis is the increase in plantarflexion moment at early swing with increasing sl. this effect can be confirmed through visual inspection of the group moment profiles. in contrast with previous work [cit], we did not observe a consistent increase in peak plantarflexion moment with increasing sl, with only the three highest speed conditions showing a significant effect for the transition from -17% ss to ss-sl, and no significant effects measured for an increase in stride length over the self-selected value. also, increasing ankle plantarflexion moment with stride length at 40% of stance was not observed, in contrast with previous work [cit] . the effect of stride length on joint kinetics"
"to better illustrate the use of the competencies, we present here a series of brief use cases-scenarios in which the competencies have proven valuable already in defining, refining, or assessing a bioinformatics training mechanism. these use cases were selected to highlight a diverse set of training needs, user personas, types of training programs, and educational settings. in this spirit, we present examples grouped into three categories: (1) complete degree programs for which the competencies have proven valuable to overall curriculum design or refinement; (2) supplements to existing degree programs (i.e., specializations, tracks, certificates); and (3) training resources outside the context of specific degree programs."
"20 healthy adults (10 males, 10 females) were recruited to participate in this study (protocol approved by the university of delaware institutional review board, protocol no. 619724). subjects-age (mean ± std) 21.55 ± 2.50 yrs, height 1.73 ± 0.08 m, body mass 69.20 ± 8.73 kgwere naive to the purpose of the study, and free of orthopedic or neurological disorders affecting walking function. subjects were required to wear their own comfortable athletic shoes and lightweight clothing for the walking experiment."
"overall, competency workshops have been invaluable to the enhancement and refinement of the bioinformatics competencies. through these workshops, the iscb curriculum task force has been able to construct a useful set of bioinformatics competencies that curriculum developers can use to develop, compare, and assess impactful bioinformatics training programs for a wide range of audiences and ultimately help establish bioinformatics skills in such audiences [cit] . table 1 reports the current state of the competencies developed and refined through this community engagement process. tables 2-4 map these refined competencies to a broader set of personas, suggested over the course of the task force's community engagement efforts, via bloom's taxonomy terms. for reference, table 5 provides examples and definitions of the bloom's revised taxonomy terms. in the next section, we provide some examples of how the competencies have been applied in a variety of training contexts."
"the linear mixed effects model results for torque pulse location in gait cycle are shown in tables 5 and 6 . the analyses yielded a highly significant effect of joint and interactions between joint and pulse sign, sl modulation and pulse sign, and joint, sl modulation, and pulse sign for both pulse approximations. the factor of pulse sign was only significant for the one pulse the effect of stride length on joint kinetics approximation and the factor of sl modulation and interaction of joint and sl modulation were only significant for the two pulse approximation."
"we wanted to compare the asp approach with cellnopt and analyze the scalability of the methods. also we wanted to determine how the inference of the network is influenced by specific parameters of the problem. for this purpose, we generated meaningful benchmarks that covered a broad range of these influential parameters."
"nonetheless, the asp search was strongly supported by the fact that there exists at least one model with perfect fit. this considerably reduces the optimization problem to the search of compatible models with minimal size by canceling the θ f term in eq. (3). performing optimizations over real data will induce that there will no more exist models with perfect fit, which may have a strong effect over the performance of our formal approach, while for genetic algorithms performances may probably be less affected by real data, but this will have to be studied. an interesting perspective is therefore to test the efficiency of these approaches in a real case experiment. hal-00737112, version 1 -1 [cit]"
"our goal is to, realistically, predict quantitative levels of cpis (compound-protein affinity measured in ic 50, k i, or k d ) from sequence data alone and to balance the trade-offs of previous structure-or sequence-based methods for broad applicability, high throughput and more interpretability. from the perspective of machine learning, this is a much more challenging regression problem compared to the classification problem seen in previous sequence-based methods."
"our results show significant improvements, concerning computation time and completeness in the search of optimal models, in comparison with cellnopt. we note that similar features can be obtained by formulation of the problem as an integer linear optimization problem [cit] . the perspectives of this work go towards the exploration of the complete space of optimal solutions in order to identify properties such as the robustness of optimal models, and relate them to the quality of the obtained predictions."
"model predictions next, we show the representation for the input signals propagation. for each experiment, first the truth values for stimuli and inhibited nodes are fixed and then, truth values are propagated to all nodes by exploiting the fact that in order to assign true to any node, it is enough that one conjunction clause over it evaluates to true."
"thrombin was not included in the training set whereas xa was with 1,524 examples excluding dx-9065a. table 3 suggested that all three models -random forest using baseline representations, random forest using our novel representations, and our unified rnn-cnn models using novel representations -correctly predicted the compound's favoring xa."
"to improve the performances for new classes of proteins, we compare two strategies: re-training shallow models (random forest) from scratch based on new training data alone and \"transferring\" original deep models (unified parameter+nn ensemble) to fit new data (details in supplementary data) . the reason is that new classes of targets often have few labeled data that might be adequate for re-training class-specific shallow models from scratch but not for deep models with much more parameters."
"phospho-proteomics assays [cit] are a recent form of high-throughput or 'omic' data. they measure the level of phosphorylation (correlated with protein-activity) of up to hundreds of proteins at the same moment in a particular biological system [cit] . most cellular key processes, including proliferation, migration, and cell cycle, are ultimatelly controlled by these protein-activity modifications. thus, measurement of phosphorylation of key proteins under appropriate conditions (experimental designs), such as stimulating or perturbing the system in different ways, can provide useful insights of cellular control."
we used the asp implementation detailed in section 3.1 to identify the pslms compatible with the middle-scale pkn (respectively the large-scale pkn) having an optimal score with respect to the generated dataset.
"-middle-scale the optimization was run for 9.2 hours and the best score was reached after 7.2 hours (299 generations). during the optimization, 66 boolean models with perfect fit were found, with sizes going from 16 to 24. out of the 66 models, only 2 models were minimal (i.e. with size equal to 16) ."
"we have proposed a formal encoding of a combinatorial optimization problem related to the inference of boolean rules describing protein signaling networks. we have used asp, a declarative problem solving paradigm, to solve this optimization problem and compared its performance against the stochastic method implemented by cellnopt. our asp formulation relies on powerful, state-of-theart and free open source software [cit] . as main conclusion, we prove that our asp-based approach ensures to find all optimal models by reasoning over the complete solution space. moreover, in the experiments presented in this work, asp outperforms cellnopt in up to 5 orders of magnitude."
"thus far, the factorial modulation of both gs and sl and resulting hip, knee, and ankle kinetics has not been investigated; as such it is unclear how lower extremity joint moments are modulated by both gait parameters. addressing this gap of knowledge, we designed an experimental study to establish the effects of gs and sl on the resulting hip, knee, and ankle joint moments. the findings are intended to inform the design of a robotic controller that delivers pulses of torque to the lower extremity joints with optimal timing and amplitude to induce desirable modulations of gs and sl, and of associated spatiotemporal parameters."
"next, we define two properties that we use later as the constraints of the csp formulation. the first property defines for a given pkn, the conditions hal-00737112, version 1 -1 [cit] that must be satisfied by a logical formula in order to define the truth value of any node. for example, if we look the fig. 1 is quite clear that the hypergraph in (b) is not just some arbitrary hypergraph, but instead is strongly related to the graph in (a). this relation is captured by the following definition."
"middle and large-scale benchmark datasets we constructed a middle (see fig 2) and a large scale (see fig 3) optimization problem. both pkns were derived from literature and in each case we randomly selected compatible pslms or hypergraphs (middle: fig. 2(b), large: fig. 3(b) ), from which we generated in silico datasets under several experimental conditions giving place to different numbers of output measures. the main parameters used to compute the objective function for the optimization are shown in the table 1 ."
"the process starts at the whole program level, by identifying which courses in the program significantly address specific core competencies. then, for each core competency, the learning outcomes of the relevant courses are examined and refined to address this competency. assessment activities are tailored with the core competency in mind to ensure that at the conclusion of the course, students are able to demonstrate that they have achieved sufficient levels of proficiency. the process is repeated for each core competency, resulting in a matrix mapping competencies to curricula. the matrix may reveal weaknesses, which can be addressed by modifying or substituting courses. for example, in the most recent revision, the program was modified to replace generic elective courses with additional design project courses and software engineering workshops. to facilitate the evaluation of a program relative to core competencies and graduate attributes, the university's academic information management system requires each course description to include a mapping of the course's learning outcomes to both assessment tasks and core competencies. the competency mapping matrix can then be generated automatically for each course and at a whole program level. expanding the iscb curriculum guidelines by including examples of learning outcomes for each core competency would facilitate this kind of analysis and increase the usefulness of the competencies in curriculum design and evaluation."
"we have developed accurate and interpretable deep learning models for predicting compound-protein affinity using only compound identities and fig. 4 . interpreting deep model for factor xa specificity. pairwise alignment of amino-acid sequences of factor xa and thrombin decomposed both sequences into 50 segments (labeled by two sse indices with a hyphen in between) and scored by the difference in attention ranks between the two compound-protein interactions. ground truth is in black. protein sequences. by taking advantage of massive unlabeled compound and protein data besides labeled data in semi-supervised learning, we have jointly trained unified rnn-cnn models for learning context-and task-specific protein/compound representations and predicting compoundprotein affinity. these models outperform baseline machine-learning models. and impressively, they achieve the relative error of ic 50 within 5-fold for a comprehensive test set and even that within 10-fold for generalization sets of protein classes unknown to the training set. deeper models would further improve the results. moreover, for the generalization sets, we have devised transfer-learning strategies to significantly improve model performance using as few as 40 labeled samples."
"in fig. 7 we plot the computation time evolution for asp and cellnopt with respect to the number of experimental observations (i.e. output measures) included in the in silico datasets used to run the optimizations. since we generated multiple datasets which contained the same number of experimental observations, for each optimization related to these multiple datasets we obtained minimum, maximum, and average times. we observe that the asp computation times are in a range that goes from 0.02 to 0.15 seconds, while cellnopt computation times to find the best score goes from 43 minutes to 2.7 hours, which was set as the limit running time. we see from these results that asp outperforms cellnopt in 5 order of magnitude guaranteeing in all cases global optimality. as discussed in a previous subsection, the main prospective issue is to test the relevance of this conclusion when optimizing with real data instead of in silico data."
"-large-scale the optimization problem was run for 27.8 hours and the best score was reached after 24.5 hours (319 generations). during the optimization, 206 models with perfect fit were found, with sizes going from 27 to 36. note that in this case, cellnopt did not find any of the minimal models (i.e. with size equal to 26)."
"the study of the middle and large optimization problems evidenced that genetic algorithms may not find all minimal models. in order to elucidate whether this phenomenon is frequent, we used the 240 benchmark datasets generated with the method described in section 3.2. in fig. 6 we show the number of executions of the optimization process for asp and cellnopt where both approaches found the complete set of global optimal (minimal) models. recall that asp ensures finding the complete set of global optimal models (blue bars in fig. 6 ) while this is not the case for cellnopt. we observed that in 202 executions out of 240 (84%), asp and cellnopt both found all the minimal models. this is particularly clear in the 105 executions with a single minimal model, which was found by cellnopt in 95% of executions. nonetheless, in the 44 cases with more than 4 optimal models, cellnopt found all optimal models in only 47% cases. more generally, as the number of minimal solutions to the optimization problem increases, the percentage of minimal solutions identified by cellnopt decreases."
"the joint kinetics associated with gs modulation in healthy control subjects have been thoroughly elucidated in the literature, where an increase in gs is generally associated with increase in magnitude of peak joint moments. a very early investigation examining knee kinetics found increasing gs to be strongly correlated with an increase in peak knee extension moment [cit] . further work found peak hip flexion and extension moments, knee flexion and extensions moments, and ankle plantar and dorsiflexion moments to all increase with gs. however, these changes in joint kinetics to increasing speed were primarily observed at the hip, particularly in extension, and secondarily at the ankle for purposes of support [cit] . recently, an increase in gs was observed to be associated with an increase in peak hip extension moment during loading response, knee flexion moment in late stance and peak ankle plantarflexion moment [cit] . also, hip extension during loading response, hip flexion during early swing, hip extension during late swing, knee flexion during loading response, knee extension during early stance and ankle dorsiflexion during loading response all increased with increases of gs and sl [cit] ."
"moreover, the proposed assistance strategy is based on the assumption that human contribution will not change when an assistive torque is applied via a wearable exoskeleton, such that the combination of torques applied by the two agents would result in a simple summation. however, it is well known that the human neuromuscular system is non-linear [cit] and involves multiple complex feedback loops [cit] . as such, the response to a torque perturbation at a specific instant in gait cycle will be difficult to predict. given the difficulty of formulating a model of the human response to these assistance strategies, once again the results of this analysis work could be used as an initial estimate to be iteratively optimized for each subject using human-in-the-loop optimization."
"undergraduate training in an australian university: bioinformatics engineering education at the university of new south wales (unsw). the university of new south wales (unsw) (sydney, australia) offers a bachelor of engineering (bioinformatics engineering) program, which aims to empower graduates to design and implement computing systems for bioinformatics, including software algorithms as well as data management and analysis infrastructures. the be (bioinformatics engineering) [cit] and is the longest-running undergraduate bioinformatics program in australia. it is fully accredited as an engineering degree by engineers australia: graduates are recognized as entry-level engineers in all the countries that are signatories of the washington accord-an international agreement among bodies responsible for accrediting engineering degree programs [cit] . the program is revised periodically to keep it relevant and is reviewed every 5 years by an external panel of engineers to ensure that accreditation criteria are met. curriculum mapping of the program content to the iscb and engineers australia core competencies as well as to the university's graduate attributes is a crucial step in that process."
"visual feedback of sl (k) was provided in terms of the height of a bar, while the desired sl was displayed as a horizontal line with dashed lines indicating the ± 10% range. the bar indicating sl (k) was color coded to indicate whether the measured value was within ± 10% of the desired value. during biofeedback sessions, subjects were instructed to modify the length of their strides to achieve the target range, while walking at treadmill-imposed speeds."
"as discussed within the results section, several prospective issues shall now be investigated. we first have to study the robustness of our results when optimizing over real networks and datasets. second, we shall develop tools to explore the topology of the space of suboptimal models in order to gain in biological relevance in the inference process and try to elucidate whether this topology informs about the biological relevance of minimal models. finally, by considering the presence of feedbacks loops in the input pkn and by studying the effect of different discretization approaches, we hope to improve the state of the art in protein signaling network inference and offer a useful tool for biologists."
"as has been reviewed, structure-based methods predict quantitative levels of cpi in a realistic setting and are highly interpretable with structural details. but their applicability is restricted by the availability of structure data, and the molecular docking step makes the bottleneck of their efficiency. meanwhile, sequence-based methods often only predict binary outcomes of cpi in a simplified setting and are less interpretable in lack of mechanism-revealing features or representations; but they are broadly applicable with access to large-scale omics data and generally fast with no need of molecular docking."
-middle-scale the minimal score was computed in 0.06 seconds 1 . the enumeration took 0.03 seconds and found 8 global optimal boolean models with size equal to 16. the union of the 8 optimal models found is shown in fig. 2(c) . -large-scale the minimal score was computed in 0.4 seconds. the enumeration took 0.07 seconds and found 2 global optimal boolean models with size equal to 26. in fig. 3(c) we show the union of the 2 optimal models found.
"to visually illustrate the distributions of torque pulses in gait cycle, location histograms were generated. all pulses were combined across the twenty subjects and five gait speed conditions and grouped by joint and sl modulation condition for the one and two pulse approximations, separately. for representation purposes, within each histogram, the pulses are divided into positive and negative groups according to the sign of pulse amplitude d� t ðjþ � ðtþ, and then further divided into two more categories (i.e. small and large) based on whether their magnitude was smaller or larger than the group median."
"drugs are often developed to target proteins that participate in many cellular processes. among almost 900 [cit], over 80% are small-molecule compounds that act on proteins for drug effects [cit] . clearly, it is of critical importance to characterize compound-protein interaction for drug discovery and development, whether screening compound libraries for given protein targets to achieve desired effects or testing given compounds against possible off-target proteins to avoid undesired effects. however, experimental characterization of every possible compound-protein pair can be daunting, if not impossible, considering the enormous chemical and proteomic spaces. computational prediction of compound-protein interaction (cpi) has therefore made much progress recently, especially for repurposing and repositioning known drugs for previously unknown but desired new targets [cit] and for anticipating compound side-effects or even toxicity due to interactions with off-targets or other drugs [cit] ."
"note that without the assumption of no feedbacks loops in the given pkn, the existence of multiple steady states or cycle attractors should be considered. then, in order to guarantee that ρ is well defined, new constraints should be added to the csp instance defined in (6), but this is left as a future work."
"as shown in fig. 2, deep transfer learning models increasingly improved the predictive performance compared to the original deep learning models, when increasing amount of labeled data for new protein classes are made available. the improvement was significant even with 1% training coverage for estrogen receptors and ion channels (only 40 and 51 training examples, respectively) but not for gpcr with just 10 examples. notably, deep transfer learning models outperformed random forest models that were re-trained specifically for each new protein class."
results in table 4 table 4 . predicted pic50 values and target specificity for three ptb1b-selective compounds interacting with five proteins in the human ptp family.
"certificate programs and specializations: ohio university. ohio university offers bioinformatics certificates at both the undergraduate and graduate levels. additionally, computer science students at the bs, ms, and phd degree levels may specialize in bioinformatics by selecting degree tracks that contain appropriate biology and bioinformatics courses. to complete an undergraduate bioinformatics certificate, trainees take courses in the following: statistics, discrete mathematics, data structures, genetics, laboratory biology, cell biology, one elective course in biology, bioinformatics tools, and data mining. a graduate certificate in bioinformatics is earned by completing graduate level courses in biochemistry, two elective courses in genetics/molecular biology/systematics, laboratory biology, bioinformatics tools, computational genomics, data mining, or statistical foundations for bioinformatics. similarly, explicit biomedical informatics tracks within the computer science degree programs allow students to elect a structured training program."
"self-selected gait speed. a preliminary set of trials were conducted to calculate the subject's self-selected gait speed (ss-gs). subjects were asked to walk on the treadmill moving at an initial speed of 0.5 m/s, with the treadmill speed gradually increased by intervals of 0.03 m/ s, and to indicate when ss-gs was reached. the same procedure was repeated by starting with the treadmill at 1.8 m/s, and decreasing treadmill speed in increments of 0.03 m/s, until the subject indicated that ss-gs had been reached. this procedure was repeated three times and the ss-gs was calculated as the average between the six measured treadmill speed values."
"the need for bioinformatics education and training is immense, but it is also diverse. there is a wide range of audiences who are potential recipients of training, each of which has different needs in terms of what skills or knowledge they require and at what depth. for example, someone training to be a bioinformatics engineer (which we define as someone who will actively be involved in the development and application of bioinformatics algorithms) requires in-depth knowledge of existing algorithms, how they work, how to critically evaluate them, and how to interpret the results. by contrast, a bioinformatics user (which we define as someone making use of bioinformatics resources in an applied context, such as in medical practice) would need a basic level of understanding of the methods and a stronger focus on the interpretation of the outputs. in a recent publication [cit], the iscb education committee's curriculum task force described the potential for refinement and application of bioinformatics core competencies for different user groups. here, we describe the further refinement of these competencies and provide a series of use cases illustrating their applications to different bioinformatics education and training programs globally."
"undergraduate and graduate degree programs in a us research university: computational biology education at carnegie mellon. carnegie mellon university has long been active in education in computational biology and bioinformatics, providing several opportunities for considering how a general set of competencies can apply to diverse populations. these experiences include degree programs in computational biology at several levels, including a bs in computational biology [cit], an ms in computational biology [cit], a phd in computational biology [cit], and required training in computational biology as part of the core of the bs in biological sciences, the university's general undergraduate biology major. while all of these programs predate the iscb competencies, the competencies provide a basis for considering how well these programs prepare students for work involving computational biology to differing degrees. two of these programs-the bs in biological sciences and the phd in computational biology-are discussed as examples of programs with very different student populations and training needs that can be evaluated in light of the competencies."
"the rules in (11) declare the predicates that we need to give a representation of θ int . first, we declare two predicates to represent the free parameter α as a fraction of integers. then, we use a weighted sum to declare the size of the union of all logic models and we count the number of single experimental observations. the two last rules declare in which cases a model prediction does not match the corresponding output measurement. last but not least, we require the minimization of the (integer) objective function θ int simply by using the #minimize directive."
"undergraduate degrees in bioinformatics at a small liberal arts college: saint vincent college. [cit] . the program is small, with less than 20 [cit] to the present. initially, there was only one set of required courses for the bs degree, which included courses covering programming (in c++), data structures, discrete structures, introduction to databases, biostatistics, cell biology, molecular genetics, genomics, and biomedical informatics. there was also a capstone three-semester research project. roughly speaking, three types of students entered the program: (1) students who enjoyed both biology and computation and were good at both; (2) students who enjoyed biology but struggled with the programming courses; and (3) students who enjoyed programming but struggled in the upper biology courses, particularly labs. the program tended to lose students in the latter two groups from the program to biology or computer science. [cit] they split the curriculum into two tracks-biology and computation-to try to accommodate students in these groups and keep them in the major. about two-thirds of the courses are common between the two tracks, but, for example, the biology track only requires one semester of c++ programming rather than three for the computation track."
"the sps representation overcomes drawbacks of pfam-based baseline representation: it provides higher resolution of sequence and structural details for more challenging regression tasks, more distinguishability among proteins in the same family, and more interpretability on which protein segments (sses here) are responsible for predicted affinity. all these are achieved with a much smaller alphabet of size 76, which leads to around 100-times more compact representation of a protein sequence than the baseline. in addition, the sps sequences are much shorter than aminoacid sequences and prevents convergence issues when training rnn and lstm for sequences longer than 1,000 [cit] ."
"sequence-based methods overcome the limited availability of structural data and the costly need of molecular docking. rather, they exploit rich omics-scale data of protein sequences, compound sequences (e.g. 1d binary substructure fingerprints [cit] ) and beyond (e.g. biological networks). however, they have been restricted to classifying cpis mainly into two types (binding or not) and occasionally more (e.g., binding, activating, or inhibiting [cit] ). and more importantly, their interpretablity is rather limited due to high-level features. earlier sequence-based machine learning methods are based on shallow models for supervised learning, such as support vector machines, logistic regression, random forest, and shallow neural networks [cit] . these shallow models are not lack of interpretability per se, but the sequence-based high-level features do not provide enough interpretability for mechanistic insights on why a compound-protein pair interacts or not."
"subjects walked on an instrumented dual-belt treadmill (bertec corp., columbus oh, usa), as shown in fig 1, while wearing thirty-six reflective spherical markers (4 on the pelvis, 4 per thigh, 4 per shank, 2 per knee, and 6 on ankle/foot). an eight camera raptor-4 passive motion capture system (motion analysis corp., santa rosa ca, usa), for subjects 1-14, and ten camera vicon t40-s passive motion capture system (oxford metrics, oxford, uk), for subjects 15-20, were used to measure marker position in space. marker data were acquired at 100 hz, while the treadmill analog force/torque data were acquired at 2 khz. a 24-in screen was placed at approximately 1500 mm anteriorly from the center of the treadmill, and was used in biofeedback conditions. the screen provided visual feedback about the sl measured at the previous gait cycle (starting and ending at right heel strike) which was updated within 20 ms after each right heel strike. in this experiment, sl for cycle k was defined based on the right heel strike time t and anteroposterior coordinate in the laboratory frame x and constant velocity of the treadmill v:"
"carnegie mellon's bs in biological sciences illustrates one kind of bioinformatics training: for students primarily training for work in experimental biology. [cit] of making introduction to computational biology (icb) a core requirement of every undergraduate biological sciences major, providing an opportunity to explore how one would design a class to be accessible but rigorous and useful to a population of general biology students. applying these competencies, then, requires working in the context of students who are typically taking a single class on computational biology but within a full undergraduate biology curriculum. some competencies, primarily those focused on technical aspects of computational biology, can be covered reasonably well at the level needed by an experimental biologist within a single computational biology class (c,d,f,i,j; see table 1 ). other important areas, such as more conventional biological knowledge, are covered thoroughly in other areas of an undergraduate biological sciences curriculum, e.g., in more traditional core classes such as genetics, biochemistry, or cell biology (a,b). still others, such as the topics that fall broadly under communications and professional development, are covered elsewhere in the curriculum by a variety of mechanisms inside and outside the classroom (m, n,o,p). still other areas go beyond what can fit in one introductory class but are also not covered elsewhere. some of these (g,h,k) are competencies that may not be needed by this population but can be flagged for consideration in revisions of icb. the most interesting topics are those that are crucial for experimental biologists, cannot be covered sufficiently in icb, and are not covered elsewhere (e, i.e., biostatistics). icb gives this latter area enough coverage to convey the key ideas needed for bioinformatics work, but the competencies flag it as an area in need of further development in the curriculum as a whole."
"to interpret the unified rnn-cnn models' predictive behaviors in the generalization sets, we examined attention weights or scores (α j 's for protein sse j) from attention models of the single model. for estrogen receptors, α helices and coils as well as polar residues were much more focused on, which agrees with the fact that its helix 12 and neighboring loop are binding sites for cofactors and drugs. for gpcrs which represent a more diverse class, although the 7-helical transmembrane proteins' secondary structures are mostly α helices, much attention was still paid to β strands and coils which are secondary structures of known extracellular drug binding site especially for class a gpcrs. more details and results are in sec. 3 of supplementary data. more in-depth interpretability studies will be discussed on selective drugs in sec. 3.4."
"system setup figure 2 shows the custom uav platform, used for validating the proposed algorithm. the autopilot used for low level control (attitude control) is pixhawk (3d robotics, berkeley, ca, usa, https://pixhawk.org/). the pixhawk consists of an integrated imu sensor, providing the angular velocities and linear accelerations in all 3-axis. we use on board a intel realsense z200 rgb-d (intel, santa clara, ca, usa, https://software.intel.com/en-us/articles/realsense-r200-camera) camera for real-time computation of the 3d point cloud. the rgb-d camera is placed at a fixed angle of −50 deg with respect to the y-axis of the robot frame, not pointing perpendicular to the ground surface in order that the camera can be used for other future autonomous mission tasks such as object detection, avoiding the need for a dedicated sensor for altitude estimation task. all the computation is carried on board the uav on an intel-nuc 6i5syk mini-compute."
where n z is the gaussian white noise in the vertical component process model and is the only component of the robot's process model noise n r .
"barometers measure the altitude from the pressure changes in the environment, but they are very noisy and biased when used indoors and on board the uavs, due to the ceiling, ground, and wall effects. accelerometers can be used to provide an estimate of altitude, double integrating the vertical accelerations, but having huge drift in time and bias in its measurements due to the vibrations during a uav flight."
"the software modules were developed in c++ and robot operation system (ros) (willow garage, standford, ca, usa). for a complete autonomous flight the aerostack architecture (http://www.aerostack.org) is used, integrating our newly developed software modules of the proposed altitude estimation approach. a full description of the aerostack architecture can be found at [cit] ."
"in an indoor flight, normally, the flight altitude of a uav is required to be computed with respect to the ground plane, where the world frame is located on the ground at the take-off point. the obtained 3d point cloud information from a 3d sensor, can be used to extract the 3d surface information of the objects present in the environment. hence, a 3d point cloud sensor partially inclined downwards, can detect the ground surface as the most prominent 3d planar horizontal surface. clustering the 3d points relevant to the ground surface and segmenting the 3d planar information can provide us with an estimate of the flight altitude of the uav with respect to the ground surface."
"it should be noted that even though in all the performed experiments the commanded flight altitude is kept constant to 1.5 m in order to compare the algorithm with the ground truth data and the laser measurements, our algorithm can perform equally at different commanded flight altitudes during the execution of the mission, provided that the commanded altitudes are within the minimum and maximum ranges of the point cloud sensor."
"where i ranges from 1 to m, and m being the number of map elements in z map . the associated innovation variance can be given as:"
"in order to evaluate and validate our approach, we first test it with a standard dataset provided by the research community, estimating the camera altitude using the provided point cloud data."
in this section we present the results obtained from performing autonomous flights in the previously mentioned scenarios. the navigation controller uses as feedback the flight altitude estimated by the proposed algorithm.
"this section presents the current research for estimating the flight altitude of the uavs using several conventional sensors such as laser altimeters, accelerometers, barometers etc. [cit] present an autonomous uav for navigation in indoor environments. they estimate the altitude using a multilevel-slam technique for mapping the different level of the ground objects using the measurements from laser beams deflected downwards. using only 1d laser beam information pointing downwards can have lot of noise in the measurement when flying on top of complex structured obstacles, resulting in wrong mapping of the ground obstacles. the map of the ground obstacles is created depending on the x-y-axis position of the uav and hence altitude estimation is valid only in presence of static ground obstacles and would fail in presence of dynamic ones."
"the aim of this section is to provide an overview of the current research carried out in the field of uavs for estimation of the flight altitude using several on-board sensors. since the presented work in this paper estimates the flight altitude of uavs using methods related to simultaneous localization and mapping (slam) of 3d planar surfaces, we also provide an overview of the current 3d surface-based slam algorithms, used for localizing mobile robots."
"in the experiments, the camera motion is performed in a completely random manner and the new ground obstacles added for complexity are non-planer, smaller in size just occluding to some amount the present horizontal planes, thus they neither increase or decrease the accuracy of the algorithm. because of the above reasons within a given speed of the camera the rms errors do not follow a pattern (see table 1 ), and are based on the random translation motions performed. but it can be clearly seen from table 1, as the speed of these translation motions increases, the rms errors increase as well due to the fact that at high speeds during several time instances the horizontal planes are absent in the point cloud data."
"our proposed approach is based on fast segmentation of 3d planar surfaces from a 3d point cloud data computed from any point cloud sensor. we cluster the obtained 3d planar surfaces in order to segment the 3d horizontal planar surfaces. we map the vertical distances of the 3d horizontal planar surfaces, in order to provide a robust estimation of the flight altitude of the uav. our algorithm works at a fast real-time frequency of 54 hz, with the point cloud acquisition frequency of 60 hz."
"video link of the performed experiments can be found in supplementary materials. the aim of the real flight experiments, on one hand is to evaluate the capabilities of our proposed approach in presence of unknown, cluttered and dynamic ground obstacles, and on the other hand, to evaluate robustness of the estimation with fast and rapid uav movements. section 5.2.1 explains briefly the setup of our uav system as well as the configuration of the indoors real flight scenario. after which, section 5.2.2 presents the results obtained during the performed autonomous indoor missions."
"in presence of ground obstacles, the prominence of the ground surface decreases due to occlusions. in cases of the ground surface being completely occluded, the estimated flight altitude will be inaccurate, as it will be relative the ground obstacle, occluding the ground surface. in order to overcome this problem, we propose to first cluster and segment all the horizontal planes present at a given time instant. then, we continuously map the segmented planes in order to accurately estimate the flight altitude of the uav."
"several experiments have been performed in order to validate our approach. [cit] standard kinect dataset obtaining very low rms errors when compared to the ground truth data. we also perform several real indoors flights in presence of several static as well as dynamic obstacles, achieving an rms error as low as 0.037 m, when comparing it with the ground truth data."
"the remaining paper is arranged as follows: section 2 presents a detailed related work regarding the state-of-the-art techniques for estimating the altitude of uavs as well as techniques used for localizing mobile robots using 3d planar surfaces. sections 3 and 4 describe in detail first, the 3d planar clustering and horizontal plane segmentation method and second, the localization and mapping of these segmented horizontal planes for a robust flight altitude estimation. sections 5 and 6 [cit] dataset and when performing autonomous real flights using our custom uav framework, thus validating and demonstrating the usefulness of our proposed approach. finally, section 7 concludes the paper and points out some future work lines."
"much research has been carried out estimating the altitude of uavs using vision-based systems. [cit] evaluate a machine learning approach for estimating the altitude of a uav using top-down images from a single on-board camera. the algorithm learns to estimate the altitude by mapping textured information in an image to a possible altitude value, thus it always requires a textured ground surface. it also provides higher errors in presence of ground obstacles."
"autonomous unmanned aerial vehicles (uavs) are increasingly being the focus of several indoor applications requiring higher level of automation. the research community has addressed several autonomous indoor problems such as exploration [cit], search and rescue [cit], and inspection of pen-stocks [cit] ."
"the computed normal centroids of the 3d planes, comprising the matrix n c p from section 3.1, are compared with computed horizontal ground plane normal in the sensor frame, n c p from equation (1). using a given threshold, the normal centroid of the horizontal planes present in the normals matrix n c p is segmented. from this segmented normals, all their corresponding 3d points of all the horizontal planes are segmented."
"it can seen from section 5.2.2 that the average rms errors in presence of dynamic obstacles is less as compared to static obstacles, this is because in the dynamic scenario the number of ground obstacles with horizontal planes is greater as compared to static scenario. higher number of ground obstacles thus provide higher number of horizontal planes for the localization and mapping stage (section 4), improving the accuracy of the algorithm. even though the obstacles have dynamic random motion, our algorithm maps them based on their horizontal plane height and matches them accurately when they are re-encountered, making it robust against dynamic motion of the ground obstacles, and thus improving the accuracy of the estimated flight altitude of the uav."
t w c being the transformation of the sensor frame f c to the world frame f w . t w c is a composition of the transformation the of sensor frame f c to robot frame f r given by t r c and transformation of the robot frame f r to world frame f w given by t w r :
"low velocity flight: figure 7a represents the results of the low velocity (0.3 m/s) autonomous flight of the uav. it can be seen from figure 7a, when comparing the estimated altitude with the ground truth altitude we get an rms error of 0.079 m. -high velocity flight: figure 7b represents the results obtained from the high velocity autonomous flight of the uav. the rms error achieved is 0.11 m, when compared to the ground truth altitude."
"biswas and veloso [cit] also perform a depth camera-based indoor localization of a ground robot on a pre-generated map of the environment. the authors extract the planar segments and their corresponding normals from the point clouds, downsample the plane filtered points to 2d lines and match them with the existing 2d map for localization. although the authors provide very good localization results, their approach is limited to environments with a known map."
"our proposed approach can map the ground obstacles only below the flying altitude of the uav. with the current sensor setup of our uav used in the experiments, if there is a ground obstacle taller than the flight altitude, it will be discarded by our algorithm as no horizontal plane would be detected. whereas such ground obstacles would be detected and avoided using the 2d laser sensor on board the uav. obstacle detection and avoidance using the 2d laser can be found in our previous work [cit] ."
"computer vision systems based on rgb or rbg-d information provided by the cameras are also widely used for estimating numerous parameters related to the uav instantaneous state, including the altitude, because they provide more information about the environment at a given time interval as compared to laser altimeters. however, computer vision systems also suffer from several drawbacks: the monocular systems cannot recover the scale from the environment, and stereo based systems are computationally very expensive. both cases require textured environment which cannot be guaranteed in all indoors scenarios, especially regarding ground surfaces, which are often polished with high light reflectance properties."
"where p is a 3d point composed of p x, p y, p z in cartesian coordinates of x, y, and z axis respectively and n c p composed of the three components n c p x, n c p y, n c p z . a second k-means algorithm is used for clustering the height vector h, which is a vector of all the computed heights h from equation (3). the height centroids obtained from the second k-means clustering are used by the localization and mapping stage as explained in the following section 4, in order to accurately estimate the flight altitude of the uav. algorithm 1 explains briefly the implementation of this section."
"in order to deal with the problem of multiple horizontal planes, we formulate a kalman filter based localization method to estimate the flight altitude of the uav based on mapping all the segmented horizontal planes for generating a complete map."
"all the distances within the given chi-square threshold are sorted in order to obtain the minimum value, and thus the corresponding map element. this selected map element is then associated with the measurementẑ pn ."
"in this paper, we present a fast and robust point cloud-based altimeter, accurately estimating the flight altitude of uavs, in presence of several static as well as dynamic ground obstacles, in indoors environment."
"to improve the limitations of the previous works and taking into account the advantages of the previously mentioned 3d surface-based localization techniques which can provide enhanced information of the ground obstacles, we propose an innovative approach using 3d point cloud data, for accurately estimating the height of the ground obstacles and the flight altitude of the uavs even in texture-less environments and in presence of several unstructured and dynamic ground obstacles."
"in autonomous indoor missions, the uavs are required to have capabilities which allow them to navigate in unknown and unstructured environments in presence of several ground obstacles, perceiving the environment and interacting with its diverse components. to successfully execute such kind of missions, uavs require a proper estimation of its flight altitude, in order to maintain a steady flight when performing the navigation, perception and interaction tasks. in such scenarios, a fast and robust estimation of the uav flight altitude becomes a critical necessity for autonomous navigation."
"the prediction stage follows the standard kalman filter equations, where the state x is estimated in the current time instant k, using the previous estimate at time k − 1."
"as can be noted from section 2.1, robust flight altitude estimation of uavs in cluttered, dynamic, and unknown environments with ground obstacles, remains an open problem. in our previous work [cit], we addressed this problem, estimating the flight altitude of a uav along with the height of the ground obstacles using an ekf-based sensor fusion technique, fusing the measurements from a laser altimeter, barometer, and vertical accelerations from an accelerometer. nevertheless, due to the limitation of the laser altimeter, measuring the distance with respect to ground obstacles only at a single point at a given time interval, the estimation of the ground obstacle height was limited to be one dimensional instead of a 3d plane. this caused errors in the height estimation of the ground obstacles in presence of highly complex structures of the ground obstacles thus increasing the errors in the estimated flight altitude."
"in this paper, we propose an algorithm consisting of a 3d point cloud sensor, tilted downwards towards the ground surface, along with an inertial measurement unit (imu) to robustly and rapidly estimate the flight altitude of uavs in unstructured, cluttered and dynamic environments, consisting of several ground obstacles and without any requirements of textured surfaces. to summarize, we propose the following main contributions:"
"the flight altitude estimation in presence of dynamic obstacles can be appreciated from figure 8a,b. during the take-off phase in presence of obstacle number 1, the laser measurements are with respect to the obstacle 1 whereas our algorithm having the input height of the obstacle 1, estimates the correct flight altitude of the uav during the entire take-off phase. during the entire navigation stage due to the presence of several random moving dynamic as well as static obstacles, several laser measurements get referred to the obstacles instead of the ground, making it impossible to use such measurements for estimating the flight altitude fo the uav. as our approach rapidly maps all the present horizontal planes, it can accurately estimate the flight altitude of the uav in such environment at both slow as well as high uav velocities of 0.3 m/s and 1 m/s respectively. it can also be appreciated from figure 8a,b that as our algorithm maps and localizes based on all the present horizontal planes, random and dynamic nature of the ground obstacles has no effect on the performance of our algorithm."
"we use an optitrack mocap system (optitrack, corvallis, or, usa, http://optitrack.com/), providing the ground truth flight altitude of the uav, in order to compare and evaluate our estimated flight altitude. an on-board lightware sf/10 laser altimeter (lightware optoelectronics, gauteng, south africa) is also used, only to demonstrate its inaccuracies in presence of ground obstacles."
"the research community has witnessed several algorithms based on 3d planar localization and mapping techniques applied mostly to ground robots. de la [cit] describe a 3d mapping approach based on compact models of semi-structured environments in partially destroyed buildings, for carrying out rescue work using ground robots. the authors use 3d data from a laser scanner along with computer vision techniques to segment 3d surfaces, not necessarily planar. a maximum incremental probability algorithm based on extended kalman filter (ekf) provides 6d localization and produces a map using the planar patches with a convex hull-based representation."
"as it can be seen from the section 5.2.2, our algorithm performs quite robustly in presence of high complexity-shaped dynamic and static ground obstacles, as well as with high-speed movements of the uav. it can be appreciated from figure 7a,b with low as well as high velocities of 0.3 m/s and 1 m/s, our proposed algorithm efficiently estimates the flight altitude when comparing it with the ground truth altitude."
"different kinds of techniques have been introduced in the literature for altitude estimation of uavs in gps-denied indoor environments using several kinds of sensors such as: laser altimeters, ultrasonic altimeters, barometers, accelerometers (imu) and vision-based systems. laser altimeters are very accurate but, in cluttered environment with various ground obstacles, they measure the altitude only with respect to the ground obstacles. the ultrasonic altimeters are similar to the laser altimeters, generally have shorter maximum range as compared to laser altimeters and measure the altitude with respect to the ground obstacles. hence when directly using the measurements from the laser or the ultrasonic altimeters, it is not possible to measure the accurate flight altitude in presence of a cluttered environment consisting of several ground obstacles."
"the orientation of these computed normals is used for the clustering all the 3d planes present in the environment. we cluster all the computed normals using a k-means clustering algorithm, which provides the normal centroids of all the present 3d planes including all the horizontal planes. these clustered centroids of the normals constitute the matrix n c p . figure 1 shows an example containing a ground obstacle with all the computed normals with their orientations and their respective clusters."
"even though suffering from this above-mentioned limitation, in a real uav flight with rgb-d camera on board the uav, inclined downwards and with fast uav motions up to 1 m/s, the computed point cloud always contains at least one horizontal plane. in addition, as we map all the horizontal planes, the performance of our proposed approach is not affected when testing with real and fast uav flights except during the landing phase, which usually does not affect performance the autonomous mission, as it being the last and concluding phase of the mission (see section 5.2.2)."
"the segmented 3d points along with ground plane normal in the sensor frame n c p, are used for computing the vertical height of the segmented horizontal planes with respect to the sensor as follows:"
"the estimated horizontal plane centroids from equation (7), are compared with the ones already in the map vector z map by means of the mahalanobis distance. it is assumed that the ground plane is always present at the lowest location of the map, in the world reference frame. hence the map vector z map, initially contains at least the ground plane as a horizontal plane with height zero. for a given measurementẑ pn, the measurement innovation is computed as:"
"an immediate line of future work is to robustly estimate the x-axis and y-axis position of a uav along with the robust flight altitude estimation using the point cloud sensors, in dynamic and unstructured indoors environment. since obtained the point cloud, is generally very noisy in the x-axis and y-axis direction, segmenting 3d planar surfaces in these directions and achieving robust localization becomes a challenging tasks. as a second line of future work, we plan to improve the data association problem, when matching segmented planes, for a more accurate mapping stage and thus an even more robust localization."
"as shown in figure 2, the normal centroids computed from the above-mentioned section 3.1 are with respect to the sensor frame of reference f c . in accordance to the definition of the world reference system, the normals of the horizontal planes are always parallel with respect to the z-axis of the world reference frame, which is used for segmenting the horizontal planes. converting all the normal centroids from sensor frame to world frame becomes a computationally expensive and time-consuming process. in order to speed up the computation, instead of converting all the computed normal centroids to the world frame, we convert the orientation of a single horizontal ground plane normal in the world frame, which is always known and fixed, into the sensor frame as:"
"the emergence of gpus has allowed complex algorithms to be executed almost in real time. gpu is conceptualized as a set of streaming multiproccesors (sm), where each sm is gaussian random number generator doppler filter"
"moreover, knowing the behavior or performance of a mobile communication system under real conditions (in situ test) can be very expensive, owing to the transfer of the communications system and test equipment to the place under study, among other issues. additionally, the system behavior can not be tested under the same propagation conditions due to the nature of the communication channel. faced with this problem, an economical alternative is to use mathematical models, which represent the radio channels under consideration. in this sense, we can define a channel simulator as a software tool that permits reproduction of the behavior or the propagation conditions of a mobile communications channel under controlled or laboratory conditions."
"finally, an interpolation technique such as splines, polynomial, or basis expansion is used for obtaining the samples at rate. the entire process is presented in figure 1 and summarized in algorithm 1."
"the upsampler stage is responsible for generating noise samples at the rate, implemented as an interpolation. the usual interpolation available for gpus is the linear interpolation offered by texture memory; npp offers other methods for more accurate results. in this case, the nppiresize function with a cubic interpolation is used. it returns the interpolated value for a given coordinate within two known noise values."
"the implemented precoding mitigates some of the penalty resulting from the half-duplex constraint at the relays. it brings the performance very close to the one achieved when a direct continuous link is available and sfbc coding is used at the bs. actually, for the case of 2 antennas in each relay, the precoded scheme outperforms the non-cooperative one for high snr regime, due to the nonorthogonality of space-frequency codes for 4 transmitting antennas. improvements are obtained for scenarios where cooperative links have higher quality than the direct link, being more pronounced as the quality of the cooperative links increases."
"the remainder of the paper is organized as follows: in section 2, a general description of the system model considered is presented; we then describe the proposed algorithm and derive the main link equations as well as compare the proposed scheme with the equivalent distributed sfbc system, for both cases of 1 and 2 antennas at relays, in section 3; also, pairwise error probability derivation and diversity analysis is made for the proposed algorithm with 1 antenna at each relay, in the same section; then, in section 4, we present the comparison between theoretical and simulation results, and the performance of the precoded algorithm is assessed and compared with the reference cooperative and non-cooperative systems; finally, main conclusions are pointed out in section 5."
"the aim of this paper is to propose a novel dataprecoded relay-assisted (ra) algorithm, which can achieve full spatial diversity, obtaining a coding gain in comparison with the equivalent distributed sfbcs scheme, maintaining the same spectral efficiency of the non-cooperative system. two rns are used for cooperation, equipped with either 1 or 2 antennas. the simple precoding scheme proposed exploits the relation between qpsk and 16-qam, by alternately transmitting through the 2 relays, achieving full diversity, while significantly reducing power penalty. also there is no need to transmit through the direct link, in alternative to the non-orthogonal algorithms proposed previously. this is beneficial for most scenarios, since the direct link is usually strongly affected by path loss or shadowing. in this algorithm, we perform precoding of the data symbols prior to transmission and posterior decoding at the ut by using viterbi algorithm [cit] . analysis of the pairwise error probability of the proposed algorithm with a single antenna in each relay is derived and confirmed with numerical results."
"the simulation was carried out using an imac computer with the following specifications: os 10.9.4 (maverics), intel core processor i5 (3.4 ghz), 16 gb of ram, graphic card geforce gtx 780 m with 4 gb of ram, and 1536 cuda cores."
"there is a cuda kernel for computing a set of independent grn vectors. each vector corresponds to a path, which is computed in chunks by the gpu multiprocessors and then stored on device global memory. the implementation of the gnr generator is presented in the algorithm 2, where the function setup kernel initializes the threads of the same block with a different sequence number but the same seed and offset (zero offset). furthermore, generate normal kernel computes several pseudorandom values with gaussian distribution through the calling of curand normal2."
"finally, it is important to emphasize that the presented approach can deal with several path realizations. this suggests that the developed fading channel simulator can be considered for generating large mimo channels, which represents a new simulation paradigm."
"in the proposed precoded algorithm, while bs transmits data continually to the rns, relays transmit and receive alternately: rn 1 transmits in even time slots, or symbol duration, while rn 2 receives; rn 2 transmits in odd time slots, during the reception period of rn 1 (figure 2) . at the ut, we use the viterbi decoding algorithm to separate the qpsk data symbols, since it is the optimal decoding method [cit] . in the following the signal expressions and methods are presented in detail, separately for each scheme."
"from the presented results, it is clear that the proposed cooperative schemes can be used to extend the coverage mainly in scenarios where the quality of the direct link is poor, as is the case of cluttered urban environments."
"the curand normal2 function generates two normally distributed pseudorandom numbers in each call. because the underlying algorithm is based on the box-muller transform, it is suitable for generating random complex numbers; that is, each call generates real and imaginary parts at the same time."
"our previous work included a distributed spacefrequency block coding (sfbc) scheme, designed for orthogonal frequency-division multiplexing-based cellular systems, requiring a direct path [cit] . we have thereby considered the use of an antenna array at the base station (bs) and a single antenna at both the ut and relay node (rn). we observed better performances with the cooperative systems against the non-cooperative, when one has better link quality in the first hop to the relay than the direct link. however, as the other cooperative systems recently proposed, this scheme has also half of the spectral efficiency of the respective noncooperative system. because of the half-duplex constraint at the relays, transmission of a data rate equivalent to that of a modulation technique with m bits per symbol in the case the direct link would be available from the bs to the ut would require the use of a constellation with 2m bits per symbol. this would imply a penalty in the power efficiency."
"the rest of this paper is organized as follows: in the second section, the background of the wireless communication system is stated, specifically as regards the channel communication model. in section 3, how to simulate the communication channel is explained. next, in section 4, the gpu implementation of the fading channel simulator is detailed. section 5 is devoted to presenting the implementation results when a wimax scenario is considered. finally, the conclusions are presented in section 6."
"from (1), we can easily recognize that each symbol s k is a 16-qam symbol. however, the receiver will interpret it as a sum of 2 qpsk symbols, allowing, because of the fact that each qpsk symbol is received through two paths, to bring the performance close to the one that would be achieved if the qpsk symbols were transmitted continuously."
"the superscript in signals also refers to subcarrier position. assuming uncorrelated antenna channels, a diversity order of 4 can be achieved. the rate of the proposed scheme is n/(n + 1), where n is the number of symbols transmitted, which is close to 1 for large values of n."
"in the above two approaches spatial diversity can be achieved, but because of the half-duplex constraints of relays, the information has to be transmitted in half of the time that would be needed in the case of a continuous link available from the source to the destination. this means that, assuming that a modulation scheme carrying m bits per symbol could be used in the case when continuous direct link was available, one would need to switch towards a modulation carrying 2m bits per symbol (if the symbol duration was kept identical), for example, going from qpsk to 16-qam when 2 rns are available. as a major consequence, increasing modulation order leads to a decrease of power efficiency. however, as we will show in the next sections, the relation between 16-qam and qpsk defines an inherent trellis structure that can be used to bring the performance closer to the one that would be obtained with the more power efficient modulation scheme."
"some assumptions were considered for this work, such as perfect csi at the relays and at the ut, the transmitted power per time slot normalized to 1, and the distance between antenna elements of each bs and rns large enough to assume uncorrelated antenna propagation channels. the block length used in the simulations, n, is of 3600 symbols."
"in the case that original symbols are qpsk, one can use a simple precoding operation that relates qpsk and 16-qam, obtaining the symbols transmitted by the bs given by"
"our strategy for implementing the fading channel simulator is aimed at improving the overall performance by chaining software functions (called kernels) representing each communication step. in order to implement the parallel fading simulator as illustrated in figure 3, we distinguish five stages in the gpu design methodology as follows."
"therefore, the proposed scheme asymptotically achieves the performance of qpsk, in the case of high snr and when the channels have equal average power gain, that is, an improvement of 4 db relatively to 16-qam; taking ρ to 0, the performance will be that of distributed alamouti, as we have only one path, reducing to a 16-qam demodulation situation."
"where (⋅) is the expectation operator and (⋅) * represents the complex conjugate. this channel model is difficult to implement; nevertheless, some assumptions can be asserted which simplify the model. the first is the absence of correlation between the different scatters, and the second is that each scatter is a wide-sense stationary process, which together comprise the well known wide-sense stationary uncorrelated scattering (wssus) model. therefore, (4) transforms intõ"
"multiple-input, multiple-output (mimo) wireless communications are effective in mitigating channel fading, thus improving the cellular system capacity [cit] . however, there is significant correlation between channels in some environments, and using an antenna array at the user terminal (ut) may not be feasible due to size, cost, and hardware limitations. cooperative systems are promising solutions for wireless systems to overcome such limitations, when the direct link does not have good transmission conditions [cit] . it can be achieved through cooperation of terminals (either dedicated or user terminals acting as relays), which share their antennas and thereby create a virtual antenna array (vaa) or a virtual mimo (vmimo) system [cit] . these allow single antenna devices to benefit from spatial diversity without the need for colocated additional physical antenna arrays."
"for each state, there are 4 branches arriving at each symbol. the viterbi algorithm is then used to find the most probable sequence [cit], with the euclidian-squared distance given by"
"then, when the channel power gains exhibit an asymmetry with ratio ρ, the asymptotic channel coding power gain of the proposed scheme, relative to the distributed alamouti, is given by"
"we proposed a novel data-precoded relay-assisted scheme, which ensures spatial diversity for cooperative systems with 2 relays, while ensuring spectral efficiency. numerical results for the precoded scheme were computed, in scenarios with different link quality conditions, for the cases of each relay node being equipped with either 1 or 2 antennas."
"the principal result of this study is the introduction of a methodology for designing fading channel simulators via gpu devices. such a methodology permits nonspecialized users to easily implement channel simulators in parallel. as was shown, the use of gpus in the development of fading channel simulators greatly saves simulation time when channel realizations are generated for testing communication systems. moreover, a case of study for wimax systems demonstrated the functionality of the implemented channel simulator. we believe that the proposed parallel channel simulator can aid in testing mobile communication systems based on lte and wimax. additionally, the presented approach based on gpu will allow the design of more sophisticated simulators of complex channel models such as triply selective mimo fading channels (i.e., time, frequency, and space selective)."
"is the autocorrelation function with respect to the time difference variable δ for the scatter located in the delay variable . from (5), it is possible to calculate the scattering function, which is defined as the fourier transform of the correlation function with respect to the time difference variable δ, as follows:"
"we further assume that each relay node is capable of deciding whether or not it has decoded correctly. if an rn decodes correctly, it will forward the bs data in the second phase; otherwise it remains idle. this can be achieved through the use of cyclic redundancy check codes. this performance can also be approximated by setting a signalto-noise ratio (snr) threshold at both rns; the rn will only forward the source data if the received snr is larger than that threshold [cit] ."
"the path gain is implemented with a multiplication function. the resulting colored noise from the previous stage is multiplied by a scalar. this could be carried out with a specific kernel or by using a standard library, such as cuda basic linear algebra subroutines (cublas) [cit] or npp. the proposed implementation uses the nppimulc function of the npp library."
"consider a single-input and single-output (siso) communication system where the transmission of in-phase ( ) and quadrature ( ) signals modulated by orthogonal carriers ( ) and ( ), respectively, are assumed, which are mixed for obtaining ( ). this signal ( ) is propagated through the communication channel (, ), which is considered to be a causal time-varying linear system. the signal filtered by the channel reaches the receiver where a noisy version ( ) is detected. it can be expressed mathematically as follows:"
"where is an index variable that enumerates the −1 discrete scatters and ( ) is a complex variable that encloses the gain and phase shift factor of such scatter. if a wssus channel is considered, the correlation function of (7) is"
"the schemes considered in our evaluations are presented next, where the former bullet includes the proposed ones and the two last ones are used as references: the results of the cooperative and non-cooperative schemes are presented in terms of bit error rate (ber) as a function of e b /n 0 of the direct link used for reference, where e b is the received energy per bit at the ut and n 0 /2 is the bilateral noise power spectral density."
"although several works related to the use of gpus in communication systems exist, there are currently no works that describe in detail the implementation of a fading channel simulator based on gpus. in this paper, the methodology for implementing a fading channel simulator (time and frequency selective) via gpu computing is presented."
"are then object of viterbi decoding used to find the most probable sequence, similarly to the previous scheme, with the corresponding trellis code weights for time slot k given by (14), and withȗ k represented in (9):"
"where x k is the kth qpsk symbol of the original sequence information, with unitary power; α is a normalization factor, so that the average transmitted power is 1, in this case being"
"the fact that we have two independent paths from the relays to the destination allows us to achieve diversity, assuming error-free links from source to relays. we consider the case that the relays are half-duplex; that is, they cannot transmit and receive at the same time. considering the halfduplex nature, we may have several options."
"diversity order, which is an important measure that we ultimately also need to keep track of, has been defined as the absolute values of the slopes of the error probability curve plotted on a log-log scale in high snr regime [cit] . from (24) we can see that the error probability decays as γ −2, which means that our scheme achieves diversity order of 2."
"currently, the high demand for integrated services (voice, data, and video) means that new data transmission schemes have to be developed for dealing with high transmission data rates and at the same time for offering high levels of quality of service. the fourth generation (4g) of mobile communication systems is still under development; its main goal is to provide a digital communication network (land, mobile, and satellite) with peak data rates of 100 mbps for high mobility devices and high data rates of 1 gbps for users or devices in low mobility environments or stationary conditions. the main technologies used in 4g include techniques based on multiple-input and multiple-output (mimo) antennas, turbo decoding, adaptive modulation, coding schemes and error correction, and orthogonal fdma (orthogonal fdma, ofdm) [cit] . current versions of standards that incorporate 4g are lte-a (long term evolution-advanced) and ieee 802.16 m wimax (worldwide interoperability for microwave access) mobile. therefore, the new issues imposed by the standards require new processing algorithms to be tested on high mobility environments affected by doppler shifts (time-selective channels) and multipath propagation (frequency-selective channels). the temporal channel variability occurs when the characteristics of the transmission medium change over time or when there is a relative motion between the receiver and transmitter, as in communication systems such as lte and wimax."
"the proposed methodology considers the use of common gpu software libraries that permit nonspecialized users in gpu programming to easily implement the proposed simulator. on the other hand, the generation of the rayleigh fading variates is achieved using the filtering method [cit] . in this case, the filtering method is carried out in time domain by using a finite impulse response (fir) filter for coloring gaussian noise samples. furthermore, it is well known that if the filter order is increased, then the accuracy of the channel statistics can be improved, though at the cost of increasing the computational complexity. therefore, in this work, we take advantage of gpus for handling such computational complexity (multiplication and addition operations) in order to implement an accurate communication channel for siso systems. moreover, this methodology paves the way for implementing mimo channel simulators in the future."
"another case occurs when the relays receive and transmit alternately and the source is transmitting continuously, firstly sending the information to the rn 1 and then repeating it to the rn 2 . in this approach, diversity is achieved without need for any extra processing at the relays and using maximum ratio combining at the ut."
"similarly, the good performance achieved with the gpu implementation with respect to the cpu implementation can be observed in the x-fold gain reported in table 3 . this gain is calculated as the time consumption quotient of both implementations. the behavior of this gain has been reported for each of samples stated in the previous paragraph."
"ddfx outperforms the sfpu and swfp alternative in both families. moreover, since an sfpu unit is equivalent to 4 ddfx cores in terms of resources (i.e., in the case of the adder), one can state that ddfx outperforms sfpu by about 40% in the virtex ii pro case. for virtex 4, 4 ddfx cores outperform sfpu by about 80%."
"once being deployed, the bs and sensor nodes will keep stationary. 2. the location-unaware sensor nodes are uniformly distributed in the observation area, and each of them will be allocated with a unique id. 3. after detecting the strength of the signal, the sensor node can estimate the approximate distance from the sender, and adjust the transmission power adaptively to save energy according to the distance. 4. all sensor nodes are capable of data fusion. assuming data being perfect relevant, they can be fused into a plurality of packets with equal size."
"as seen in fig. 1, the title molecule (i) is nearly planar with maximum deviations of 0.197 (1) å for o3, -0.157 (1) å for c9 and 0.145 (2) å for c6. the napthalene ring system (c1-c10) makes a dihedral angle of 5.04 (6) ° with the the benzene ring (c12-c17) of the 1-fluoro-4-nitrobenzene group. the c1-c11-n1-c12, f1-c13-c12-n1, o1-c2-c1-c11, o2-n2-c16-c17 and o3-n2-c16 c15 torsion angles are -179.56 (13), -179.39 (13), 2.4 (2), -8.9 (2) and -9.7 (2) °, respectively. all bond lengths and angles are similar to those of a related structure previously reported [cit] ."
"in the case of sfpu, the multiplication is on average ≈5.5% slower than addition. in the case of software-emulated floating point, the difference between addition and multiplication is almost imperceptible for the powerpc. however, in the microblaze case, the difference is significant. this is mainly due to differences in the compiler (ways to emulate floating point) and the differences between the instructions available to each processor. one can also note that the microblaze implementation outperforms the powerpc. the reason for this difference lies in the level of integration between the processors and the fsl bus used to connect the coprocessors. the fsl bus was originally designed for the microblaze, and it is tightly integrated with this core. in the case of the ppc, a bridge is needed for adding latency to fsl operations."
"with the aim of analyzing the impact of the fft/viterbi co on the global implementation complexity, we explored the possible design scenarios considering the number of the physically instantiated butterflies in a multistandard design [cit] . then, the global complexity reduction is evaluated by comparing the number of implemented logic gates for the co and velcro based designs."
"a limitation of the reconfiguration scheme used is the sequential nature of the operations. the processor must stop what it is doing, reconfigure a functional unit, and then continue its work. the performance results presented earlier can be improved by getting the data transfer of reconfiguration memory closer to the icap data port bandwidth. figure 11 models the expected performance of the proposed scheme as we improve the reconfiguration data rate."
"in this section, the proposed energy-efficient sleep scheduling mechanism (essm) is described in detail. essm is a distributed competitive mechanism based on unequal cluster-wsn, and it makes local decisions for determining competition radius and electing cluster-heads. in order to estimate the competition radius for tentative cluster-heads, essm employs both residual energy and distance to the bs parameters. moreover, it takes advantage of fuzzy logic method to acquire optimal competition radius based on a probabilistic model, which is employed for competition between candidate clusterheads. the specific flow chart of essm is shown in fig. 1 ."
"in this paper, runtime partial reconfiguration (rtr) is used to dynamically change an arithmetic unit's precision, operation, or both. this approach requires intensive use of partial reconfiguration making it particularly important to take into consideration the time it takes to reconfigure. this time is commonly referred to as the reconfiguration time overhead."
"the paper is organized into six sections. in section 2, we provide an overview of numerical representations and provide further motivation for dynamic precision. in section 3, we provide an overview of reconfigurable computing with a particular emphasis on dynamically reconfigurable architectures. we present the proposed dynamic arithmetic architecture in section 4. a summary of testing platforms and methodology is given in section 5. results are given in section 6. concluding remarks are provided in section 7."
"consumption. the power consumed during the reconfiguration process becomes important in the context of runtime partial reconfiguration. for dynamically reconfigurable arithmetic, reconfiguration power is not dissipated only once as in standard reconfigurable systems, but many times. thus, there is a clear relationship between the reconfiguration frequency and an increase in the mean power consumption of the overall system. measurements of the reconfiguration power consumption for both the virtex ii pro and virtex 4 are presented in section 6.4.3. figures 15 and 16 figure 12 : dynamic power consumption for virtex ii pro (a) and virtex 4 (b). these measurements were taken using running hardware systems described in section 5. figures correspond to the voltage across the shunt resistor (0.1 ohms) used to measure the current (see figure 7) ."
"the data in figure 12 was obtained by physically measuring the current at the fpga core during the execution of especially crafted test programs. in both graphs, the values in the y axis are expressed as a percentage of the processor power consumption while in standby."
"implementation results for addition and multiplication are shown in tables 2 and 3 . resources and maximum operation frequency values were found by compiling the cores on an xc4vfx12 device for the virtex 4, and on an xc2vp30 device for the virtex ii pro using ise 9.2.4i with default settings. all but the ddfx table 1 : precision and dynamic range results. also see [cit] ."
"after clustering, the sensor nodes in same cluster can be partitioned into several categories based on the similarity of monitoring data. in each category, some nodes may be selected as redundant nodes and scheduled to sleep state. it will reduce the overall energy consumption of the network significantly. obviously, if the number of redundant nodes selected is less, the greater the amount of information is retained on the whole, that is, the data collected is more comprehensive."
"to explain this comparison, we illustrate in fig. 17 a simplified example of a two-standard terminal. for a classical implementation (velcro), we need to implement the maximum number of required fft butterflies and the maximal number of viterbi butterflies. on the other hand, the use of the co enables to reduce the number implemented butterflies because of the \"reuse\" across the algorithms, rather than just across standards. in this case, the use of the fft/viterbi common operator can reduce the complexity of the fft and viterbi by up to 5%, when the number of the implemented viterbi butterflies is equal to the number of implemented fft butterflies [cit] . on the other hand, the use of the dmfft operator can reduce the design complexity by up to 26% compared to classical implementations [cit] . this complexity gain can be interpreted as little, but it should be kept in mind that the main motivation of this work is to build operators of higher flexibility and that can be used in a regular architecture. for that reason, showing that this additional flexibility is not traded against additional complexity is a very promising result."
"precision fx (32 24) ≈187 db implementation were created using the provider's optimized cores (coregen). in tables 2 and 3, the term equivalent gate count refers to the number of 2-input nand gates that would be required to implement the same number and type of logic functions. the number of equivalent gates cited in these tables was obtained from the ise 9.2.4i tool after synthesis, map, place, and route had been executed. in terms of logical resources, for both families, a 32-bit single-precision sfpu adder is approximately equivalent to 4 ddfx or 8 fixed-point cores. thus, in terms of resources, for the cost of one sfpu addition, we can perform 4 ddfx additions. in the case of multiplication, the ddfx footprint is smaller than sfpu by about 3% for the virtex ii pro case. the smaller difference in size can be explained by the size of the multiplier used in every case. the single-precision sfpu requires a 24-bit multiplier (the other 8 bits are the exponent) while the fixed-point and ddfx cores require a 32-bit multiplier."
"several observations can be made from the graph. first of all, when both, precision and the operation itself, are reconfigured the performance drops faster than when only the precision is changed. this is a straightforward consequence of the larger bitstream used when both precision and operation are reconfigured. secondly, one can see a sharper drop in performance for the virtex ii pro as compared to the virtex 4. this is a direct consequence of the architectural changes that allowed smaller bitstreams on the virtex 4. finally, the graph shows that in the case of virtex ii pro, the solution is outperformed by the sfpu even with very small reconfiguration frequencies."
"by choosing a boundary value, β, as the next incremental value after the maximum positive number of the radix p 0 scaling, a continuous range of numbers can be represented as shown in figure 3 . in the rest of the document, a number with radix p 0 will be said to belong to a p 0 range while a number with radix p 1 will belong to the p 1 range."
"a logical extension of ddfx is to also allow dynamic reconfiguration for the static parts. although such action defeats the purpose of trying to reduce the amount of logic to reconfigure, it adds an extra degree of flexibility to the architecture by allowing a change in the operation. since the reconfiguration time overhead is expected to be larger, these operation exchanges will have a greater impact on the architecture's performance."
"for the broad range of biological applications of schiff bases, see, for example: [cit] . for the significance of fluorine atoms in drug structures, see: [cit] . for a related structure, see: [cit] . for hydrogen-bond motifs, see: [cit] ."
"when analyzing the block diagram in figure 4, one realizes that precision is handled only by the control blocks shown in gray. the adder itself does not deal with precision and only requires the operands' binary point to be aligned. thus, it is possible to change the adder's precision and dynamic range by dynamically changing the control blocks. the segmentation of the functional unit in a control (dynamic) and operational (static) parts is the basic idea behind the architecture."
"when a node broadcasts a candidate-ch message, the range is termed as the competition radius of candidate-ch. only the nodes within the radius of the competition can receive the message from the candidate ch. during the stage of the ch selection, the spatial distribution of chs can be restricted by setting the competition radius. in a single hop mode, the energy of the sensor node may be primarily used for sending data to the bs. therefore, the radius of competition can be regarded as the key parameter for affecting the lifetime of the network. if the radius value is larger, it will result in a lower number of clusters, and more energy consumption due to the higher signal power with respect to transmit across a larger distance. in brief, the appropriate competitive radius will be conducive to balance the energy consumption of chs and the overhead of intra-cluster communications."
"besides, the candidate-chs should acquire the location information and adjacent competitors within its communication range. each candidate-ch maintains a neighbor candidate-ch table, which includes the node's id, the remaining energy and status flag of adjacent chs. after ch's selection, chs will broadcast the message, which includes their identity and the member nodes list, and wait for adjacent member nodes to join in. according to the received signal strength, non-ch node estimates the distance of its neighboring chs, and choose to join in the nearest one. it can reduce the energy consumption of the member nodes for data delivery, and also make the nodes near the bs undertake more burden of data forwarding or aggregation to achieve balanced energy consumption of the entire network. next, the member nodes will send to the ch with the join_msg, which contains the node's id and distance between the chs. by receiving the join_msg from the non-chs, the ch will send ack message and update the cluster membership list simultaneously."
"in the case of multiplication, dfx operands do not need to be aligned with respect to the binary point before performing the operation. thus, a prescaler section as the one in addition is not required."
"similarly to the static case, a like-for-like comparison in terms of resource consumption requires us to compare performances between one sfpu core and about 4 ddfx cores. we compared performances results where the load was distributed among 4 ddfx coprocessors and reconfiguration was performed in any of them. the results depicted in figure 10 show that the maximum reconfiguration frequency has been reduced, thus more reconfiguration instances are possible while still outperforming the sfpu alternative. (7), one way to improve the overall performance is by reducing t reconf . reducing t reconf is a primary objective of our approach. it was accomplished by reducing the hardware footprint of the changes required to vary precision and operation, and also by improving the overall reconfiguration speed as described in section 5.1. here, 0% cannot be represented in our logarithmic scale, thus it was included as the leftmost point in the graph. these measurements were taken using running hardware systems described in section 5."
"incorporating fluorine increases fat solubility, improving the drug's partitioning into membranes and hence increasing bioavailability [cit] . fluorination can also aid hydrophobic interactions between the drug and binding sites on receptors or enzymes [cit] . further to our study in synthesis of fluorinated bioactive compounds we herein report the synthesis and crystal structure of the title compound."
"the radix-2 fft algorithm is usually applied when the fft size is a power of 2. equation (4) shows that at the k th step the results of two smaller fourier transform are needed. then, using the divide and conquer strategy, a k-point transform can be reduced to two k/2-point transforms: one for even samples, one for odd samples (fig.6 ). starting with n that is a power of 2, it is possible to apply this subdivision recursively until getting down to 2-sample transforms that can be represented graphically using the, so called, butterfly in fig.7 ."
. fourier transform defined over this specific galois field gf(f t ) known as fermat number transform (fnt) can play a leading role in the frequency processing of rs codes: the encoding and the most important tasks of rs decoding (i.e. syndrome computation and chien search) and can be performed with fnt.
"starting from the previously presented structures of the fft and viterbi butterflies, we propose in this paragraph a common operator that addresses the requirements of the two algorithms. this architecture can perform the calculation of the fft butterfly and the (bmc + acs) operations of the viterbi decoding algorithm."
β is a boolean parameter (implemented through a single bit selector) that permits the configuration of the common structure to switch between the fft and viterbi computation. in fig.15 we present the graphic representation of the previously developed equations.
"an important architecture to mention, as it is relevant to this paper, is disc: the dynamic instruction set computer. disc was first proposed by wirthlin and hutchings [cit] . it used a medium grain, configurable logic array (clay) device from national semiconductors. this computer had an instruction set made of independent hardware units that were configured into the device as needed. disc was the first attempt to implement a truly dynamic architecture. although in a different device family, the architecture's performance was also limited by the reconfiguration time overhead. a fundamental difference between disc and our approach is the way we deal with the reconfiguration time overhead. disc based its approach on a large number of resources available to add new functional units as the system required it. only when resources were exhausted the system proceeded to replace existing modules by new ones. this sort of hardware cache reduced the impact of the reconfiguration overhead by decreasing the number of instances in which reconfiguration was required. in comparison, our approach is built upon the premise that resources are limited, requiring an existing module to be replaced at every instance."
"during the phase of ch-determination, chs will be determined based on a linear combination of probability selection and local competition. once the operation is complete, all nodes may be put into the status of member, candidate-ch or ch. initially, each node is treated as common node, and will generate a certain probability for being candidate-ch in view of the distance from the bs and the residual energy. comparing with the nodes far away from the bs, the sensors that are close to the bs will have higher probability of being candidate-chs. therefore, the probability function can be defined as follows:"
"from the fft butterfly ( fig.7), we define the computation method using bit-parallel multipliers for complex-valued operations [cit] . in equation (6) we define real and complex parameters for the fft butterfly."
"the test results showed a linear relationship between the amount of resources used and static power consumption. figure 13 depicts the test results interpolated with the resources used by common peripherals in an embedded system. as in the case of dynamic power, these results suggest our dynamic approach could be exploited to tradeoff performance for power consumption. the idea is extendable to any reconfigurable system. considering the amount of time a peripheral could be idle, the savings in static power are considerable."
"it was shown beneficial to implement the common operators in a bank to form a regular architecture previously referred to as common operator bank (cob) [cit], where the cos can be mapped and used by the considered standards ( fig.3 )."
"in the fft mode these operators process complex data by performing complex multiplications and additions. in the fnt mode data are defined over finite field and the operations performing fnt are done modulo f t . so, these arithmetic operators should be re-redefined to support complex and modular operations. fig. 5 illustrates the associated butterfly operator architecture."
"the preferred approach consisted of implementing a bitstream-cache sort of memory. in this approach, partial bitstreams are loaded from cf into random access memory (ram) with the system's start-up process. their locations in memory are stored as pointers. in this scheme, the more expensive and slower part of the partial bitstream loading is performed before any algorithm is executed. when a partial bitstream is required by an algorithm, it can be fetched"
"a dynamic arithmetic architecture using runtime partial reconfiguration was presented and evaluated in terms of numerical precision, dynamic range, logical resources, performance, and power consumption. for the newer virtex 4 fpga devices, our testing results show significant advantages of the proposed approach over alternative approaches. for the same data width and for approximately equivalent precisions, ddfx's dynamic range and logical resources lie between fixed-point and floating point. ddfx exhibits a footprint up to 75% smaller with respect to floating point in the case of addition/subtraction. keeping logical resources and precision equivalent, ddfx shows a performance advantage in excess of 50% with respect to alternative floating point and software-emulated floating point when no reconfiguration is performed. the maximum reconfiguration frequency, while keeping performance comparable to a floating point unit, is 0.001% (in the case of the virtex 4). these results were obtained running actual hardware implementations of the proposed architecture. we also presented simulation results of a theoretical system where higher (but possible) reconfiguration speeds are used. these results show that, at higher reconfiguration speeds, one could reconfigure once every 5000 operations while still being as fast as a floating-point unit."
"virtex ii pro and virtex 4 devices use three power sources commonly known as vccint, vccaux, and vcco. two different testing setups were implemented to measure the current for each power source. for the first setup, we measured the average current through the fpga during operation (active power) and during standby (static power). the active power measured is equivalent to the sum of the static and dynamic power consumed by the device during operation. the measurements were taken using an ammeter as shown in figure 6 . the results of these measurements are presented in section 6.4.3."
international journal of reconfigurable computing where t op represents the time required to perform the arithmetic operations while t reconf represents the time it takes to reconfigure the precision or the operation.
"the rest of this paper is organized as follows: in \"related work\" section, we briefly introduce related work. we describe the assumptions and explain the details of our method in \"system model and analysis\" section. in \"sleep scheduling algorithm\" section, the detailed of sleep scheduling algorithm is described. at next section, the experiments method is shown and the result is discussed regarding the performance evaluation of our method. finally, we conclude this paper and discuss the future work in \"conclusions and future work\" section."
"the classical complex fft architecture is re-design in a way to enable to perform the fnt. a radix-2 fft implementation is considered because it has advantages in terms of regularity of hardware, ease of computation and number of processing elements. obviously, for a given transform length n power of 2 (or power of 4), the algorithm chosen to be applied to perform fft should be valid to perform the fnt. indeed, since the symmetry and periodicity"
"the test programs used to obtain the performance results in the next section targeted basic operations in linear algebra. linear algebraic operations can be built upon a hierarchy. dot products involve the scalar operations of addition and multiplication. matrixvector multiplication consists of dot products. matrix-matrix multiplication amounts to a collection of matrix-vector products [cit] . based on these operations, more complex linear algebraic tasks can be built. performance is measured in terms of operations per second and as a function of the reconfiguration frequency. here, we define the reconfiguration frequency as the ratio between the number of arithmetic operations and the number of reconfigurations performed. it is straightforward to infer from this methodology that performance is directly related to the reconfiguration time overhead and thus to the reconfiguration speed and partial bitstream size."
"in this paper, the energy consumed during per round can be estimated based on the energy consumption of the nodes for reception or transmission in each round. to measure the energy consumption, the first order radio model is exploited [cit] . the energy spent for transmission of l-bit packet from the transmitter to the receiver at a distance (d) can be defined as:"
"the smaller unit of reconfiguration is called a frame. in virtex 4 devices for instance, a frame corresponds to a bitwide column of 16 clbs. all virtex 4 configuration frames consist of forty-one 32-bit words resulting in a total of 1312 bits per frame. the bitstream size per each reconfigurable region in a device is calculated by the number of frames (clb columns) it contains. figure 2 shows the time it takes to reconfigure fully or partially (horizontal axis) different"
(1) analytical approach. classical numerical analysis of the algorithm and modeling of the worst-case error as a function of operand word lengths. analytical methods attempt to determine the optimal range and precision requirements for each operation based on the input variables representations.
"for instance, numerical optimization algorithms are a specially complex subgroup of iterative algorithms. they require the same increase in precision as the number of iterations increases, but they can also benefit from low precision in early iterations [cit] . thus, a dynamic precision arithmetic can improve both numerical stability (reduce quantization errors) and convergence speed. we will present a related application in the inversion of large matrices using an iterative method."
"the main idea is based on classification by fuzzy mathematics, which can group the nodes with high data similarity into the same category [cit] . according to the scheduling mechanism, a certain mount of sensor nodes will be selected from all categories. more concretely, while the perceptual data received by ch from its member nodes is accumulated to a certain extent, the fuzzy similarity matrix can be constructed to make clustering. next, in the premise of the data fusion accuracy as high as possible, some nodes can be chosen from all categories as redundant ones. finally, a specific sleep scheduling mechanism can be applied in those redundant nodes to reduce the communication costs and traffic conflicts."
"from the formula (4) and (5), it can be observed that the optimal competitive radius of each node will increases as the distance between the node and the bs. according to the optimal competition radius of each node, it can minimize local energy consumption, and come into being an uneven hierarchical structure of clustered wsns. in the region close to the bs, the distribution density of the clusters will have relatively small size."
"for the second setup, we measured the current during reconfiguration (see figure 7 ). the resistor used had different values for the virtex ii pro and the virtex 4, but in both cases it was very small (0.1 ohms and 0.2 ohms, resp.). larger resistors generate a large voltage drop and prevent the fpga from functioning properly."
"in the process of data delivery, the members make use of tdma mechanism to send data to ch. first, the ch will divide data collection time into several time slots, and attribute each interval to a member node to form a scheduling arrangement for data aggregation. then, according to the distance of its member nodes, the ch can set its transmission range and send sched_msg to all member nodes for scheduling. after receiving the sched_msg, the member nodes record it belonging time interval for data transmission according to the sched_msg respectively. for saving energy, they can turn off the wireless communication unit during the non-transmission slots."
"unlike the two previous stages, the adders and subtructors in the third stage are interconnected and are dependent on each others. in this stage, six a/s are required but interconnected by three for real and imaginary parts of the fft butterfly. as illustrated in the fig.11 and fig.13, the a/s blocks are not interconnected in the same way for the two algorithms. thus, in order to build a common structure for this stage we develop the equations below to show a common mathematical expression for the third stage of the viterbi and fft butterflies. fig. 11 and fig.13, actually perform equations (7) and (8) linking the inputs and outputs of the third stage."
"some methods [cit] make use of statistical models to estimate the readings of nodes. owing to require few readings to respond of queries, statistical models can drastically reduce the amount of data sent by nodes [cit] . however, the inadequacies lie in time cost and energy consumption being required for constructing those models in terms of massive data. in addition, they are unable to decrease the conflict and interference originated from the redundant nodes."
"static power is due to the current drawn by the device after being powered up and configured but while doing nothing. this power is due to figure 11 : performance bound curves at the theoretically maximum possible reconfiguration data rate speeds for virtex ii pro microblaze (a) and virtex 4 powerpc405 (b). here, 0% cannot be represented in our logarithmic scale, thus it was included as the leftmost point in the graph. the original speed measurement (square red) was taken using running hardware systems described in section 5. the round (black), plus (blue), and triangle (pink) curves were estimated scaling the results from the original speed measurement. the leak current in the transistors' thin oxide layers. as the number of transistors goes up, and fabrication techniques allow for smaller geometries, this current usually increases. the increase in this current will result in heat dissipation that will in turn result in more leak current. static power has been and still is a huge problem for the fpga technology."
"in the realm of embedded systems, a designer often faces the decision of what numerical representation to use and how to implement it. particularly, when using programmable logic devices, constraints such as power consumption and area resources must be tradedoff with performance requirements. floating point is still too expensive in terms of resources to be intensively used in programmable logic devices. fixed-point is cheaper but lacks the flexibility to represent numbers in a wide range. in order to increase numerical range, several fixed-point units-supporting different number representations-are required. alternatively, numerical range can be increased by a single fixed-point unit able to change its binary point position."
"for instance, a 16-bit wide fixed-point format with no binary point can represent a number as large as 2 15 -1 and as small as 1. thus, the dynamic range for this specific format is ≈2 15 ."
"in order to prolong the lifetime of the network, it is necessary to balance the energy consumption among sensor nodes [cit] . for reducing the energy consumption in the cluster, the inter-cluster communication distance should be restricted within the threshold d 0, and it will ensure to keep in touch of the sensor's energy loss by free space model. under the condition of the single hop mode, the ch can send data to the bs directly. if the bs is far away from the monitoring area, the ch needs to employ multi-path attenuation model to deal with the power amplification loss. it will increase the energy consumption of ch greatly. therefore, chs are more likely to consume energy and turn to die earlier than its member nodes, which will shorten the lifetime of the whole network badly. therefore, it is crucial to set optimal competition radius of chs and form distributed and uniform clusters to balance the energy consumption."
"we consider three types of power: dynamic power, static power, and reconfiguration power. all results presented in this section were measured using the running hardware systems described in section 5."
"in this paper, we propose an energy-efficient sleep scheduling mechanism with similarity measure for wsns (essm), which will schedule the sensors into active or sleep mode to reduce energy consumption effectively. firstly, the optimal competition radius is estimated to organize the all sensor nodes into several clusters to balance energy consumption. secondly, according to the data collected by member nodes, a fuzzy matrix can be obtained to measure the similarity degree, and the correlation function based on fuzzy theory can be defined to divide the sensor nodes into different categories. next, the redundant nodes will be selected to put into sleep state in the next round under the premise of ensuring the data integrity of the whole network."
"with a similar aim, the new co presented herein intends to address the infinite field fft and the viterbi decoding algorithms. first, the algorithms are analyzed to highlight the similarities between the treillis and the butterfly structures which are further exploited to build the new co."
"precision is the accuracy with which the basic arithmetic operations +, −, *, / are performed [cit] . in floating-point arithmetic, precision is measured by the unit roundoff u such that"
"the architecture is compared with hardware implementations of floating-point, fixed-point, and a gcc software emulation of floating point. these alternatives were chosen for comparison because of their widespread use in the industry and their availability for testing. comparisons are made in terms of resources, power consumption, performance, and precision. data width is kept constant across comparisons. when comparing resources and power consumption, implementations with similar precision are used. when comparing performance, implementations with similar precision and resource consumption are used. for performance, the architecture is evaluated in the context of linear algebra, which is widely used in scientific calculations. linear algebra operations are broken into vector operations, and performance is measured in terms of operations per second. the architecture is ported to two of the latest xilinx device families in order to compare how the families' architectural differences impact the effectiveness of the dynamic approach."
"then, the execution of every butterfly requires the branch metrics bm00, bm01, bm10 and bm11, and the whole viterbi butterfly computation (acs+bmc) can be realized as shown in fig.13 ."
"since the dynamic part represents less than 40% of the whole adder, this segmentation allows us to reduce the amount of logic required to reconfigure the precision. as a consequence, the reconfiguration time overhead is also reduced."
"since the member nodes are often distributed in the adjacent region, the data collected by those sensors tend to demonstrate spatial and temporal correlations [cit] . in order to reduce unnecessary energy consumption, some sensor nodes maybe turn into the sleep state. our work mainly focuses on the node's sleep schedule, which is generally more suitable to solve the problems of data redundancy and transmission conflict. moreover, by making some nodes enter the dormant state, the sleep scheduling strategy can save the energy consumption derived from node's active state. in this section, we will discuss how to extend the wsn's lifetime by using optimization strategy based on fuzzy clustering theory."
"we measured the performance of the arithmetic units set as coprocessors running a series of test programs targeted towards vector operations (addition, substraction, and multiplication). the average performance is measured as"
"where d max denotes the is the largest distance to the bs, and d min denotes the nearest distance. α is the constant parameter. each node will obtain the probability of being candidate-chs to ascertain whether it has competencies required to be a ch in current round. moreover, the nodes being candidate-chs will modify their own state, and calculate the optimal competitive radius according to their distance to the bs. to save energy, other nodes that fail to become candidate-chs can turn off the module of wireless communication during the phase of ch selection."
"p 0 ] and 4b 0 from a faster memory than the cf. this slight alteration to the standard procedure improved performance by 60 times, giving average data rates of 6 mbits/sec."
"starting from the previous discussion and by gathering the developed stages, the entire fft and viterbi common operator is presented in fig.16 . this common operator architecture allows switching between two different functional implementations of the viterbi and fft algorithms. the reconfiguration can be easily performed using a single parameter. the reconfigurable operators are composed by real adders and multiplexers."
"similarly, we found that, in the case of the virtex 4, the configuration process draws an additional 25 ma. this represents ≈2.5% of the overall active power consumption. if a system were constantly reconfiguring, an increase of up to 2.5% in the mean power consumption can be expected, an increase which is an order of magnitude smaller than in the case of the virtex ii pro. reconfiguring precision"
"in this paper, we proposed an energy-efficient sleep scheduling mechanism with similarity measure for wsns. according to the similarity of the data collected by the nodes, the nodes are classified and the redundant nodes can be selected by defining correlation function in fuzzy theory. therefore, essm can activate a minimum number of sensor nodes in a densely deployed environment and maintain high data accuracy. in addition, it improves network lifetime by performing optimal ch selection approach and decentralized sleep scheduling mechanism. however, for the applications with sparse distribution of nodes or low spatio-temporal correlation of data, essm will be very hard to evaluate the differences between nodes by their data similarity. and then, the selection of redundant sensor nodes within clusters may not be very effective."
"environmental studies, such as water quality monitoring [cit] and wind measurements [cit], often require coarsegrained spatial sampling with a resolution of more than hundreds of meters in large areas attributable to the spatiotemporal data correlation. the data collection network of such applications is sparse as the distance between two neighboring sampling locations could be up to more than 1 km. to build sparse wsns using low power communication modules, this demo presents a novel long-distance networking paradigm, called distanceless, which can expand the communication range of sensor motes based on rateless codes and fully exploit network diversity for sparse sensor networks deployed across large fields. by leveraging rateless codes, a sender keeps transmitting encoded data stream until the first receiver accumulates sufficient information and decodes the original data successfully. therefore, we are able to gradually lower down the effective data rates and thus significantly augment the communication distance beyond the current limit. the distanceless transmission is able to fully exploit the link capacity and automatically adjust to a suitable effective data rate for both near and far receivers. we also adapt the distanceless transmissions to low duty-cycled sensor networks and allow the first wakeup node to forward the data instead of a fixed routing strategy. permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for third-party components of this work must be honored. for all other uses, contact the owner/author. copyright is held by the owner/author(s). we implement the proposed networking system in tinyos on the tinynode platform [cit] and deploy the sensor network in a real-world project, in which 12 wind measurement stations are installed around a large urban reservoir of 2.5km * 3.0km to monitor the field wind distribution, as depicted in figure 1 . the wind distribution and some key water quality parameters at several discrete locations in the reservoir (e.g., dissolved oxygen, chlorophyll, and temperature) are used as inputs to a three-dimensional hydrodynamicsecological model. based on the calculation in the model, we can study the effect of different environmental parameters on the water quality and predict the evolution of the water quality in the reservoir. to the best of our knowledge, this is the first distanceless sensor network design that enables the data collection of sparse sensor networks deployed across large fields. extensive experiments reveal that our system significantly outperforms the state-of-the-art data collection protocols in terms of the data delivery reliability, latency and energy consumption. more detailed information could be found in our paper [cit] ."
"co4 co4 co4 in the present work we define common operators for fft and fec decoding algorithms. these algorithms are completely different in nature, if we compare their processed data and their functionality. however, when explored in the paramerization context, functional and structural similarities can be identified. in the following sections we highlight similarities between fft and fec decoding algorithms (convolutional and block channel decoding) to define a fft/fec co toolbox. one can represent this way of doing by a graph sketched in fig. 4 . the interpretation of fig. 4 is the following: performing some steps of block channel decoding (reed solomon) and complex fft can be done with dmfft operator [cit] . similarly, the proposed work intends to perform complex fft and convolutional channel decoding thanks to a common operator termed as fft/viterbi."
the systems under test (sut) were implemented in virtex ii pro and virtex 4 devices with the purpose of gaining insight on the impact that the differences between both families have over reconfiguration time overhead. the general characteristics of the suts defined are the speed of 100 mhz for the cpu and the peripheral bus was chosen to facilitate fair comparisons between the devices. virtex 4 devices in general support a larger frequency of operation than virtex ii pro. a more accurate comparison is feasible at lower operating frequencies since the level of effort the compiler requires to accomplish this frequency is similar for both devices. a major difference between the virtex ii pro and the virtex 4 suts is that a microblaze microprocessor has been used for the virtex ii pro while a powerpc is used for the virtex 4. the microblaze was picked in the case of the virtex ii pro because the powerpc 405d5 available on these devices do not support a tight coprocessor integration as the powerpc 405f6 on the virtex 4 does.
"in the future, our work will focus on heterogeneous sensor networks composed of different types of sensors and discuss the synchronization scheme and applies to application scenarios with strict coverage requirements."
"the improvements in dynamic range and precision allow this approach to find potential applications in problems where only a floating-point solution made sense before. numerical optimization algorithms are examples of such applications. the iterative nature of these algorithms makes them especially susceptible to numerical issues arising from the use of fixed-point arithmetic. furthermore, these algorithms usually require extensive calculations, making them good candidates for performance speed-up through parallelization. in that sense, the smaller hardware footprint 2 international journal of reconfigurable computing of our approach is an advantage as it allows a larger number of ddfx units as opposed to a reduced number of larger floating-point units."
"branch metrics are evaluated in the bmc block. at the output of this block, the difference between the received value and the different transitions related to it are evaluated. the computed metrics are then distributed to all the butterflies of acs module. thus the recalculation of the butterfly parameters requiring the same metrics is avoided. the branch metric computation can be designed with simple addition and subtraction operations between the decoder inputs. thus, for r soft received inputs, all possible metric consists in 2 r possible operation (addition or subtraction) which can be reduced by half, since half of the metric can be deduced from the other half by a simple change of sign. indeed, the 2 r-1 first metrics are computed from r soft inputs, and the last 2 r-1 metrics are evaluated from the first ones by a simple change of sign."
"and forwarding the data to the base station (bs). by rotating cluster-heads selection periodically, the node's energy consumption over the network can be balanced. in case of dense-deployment, the readings being collected by sensor nodes in the adjacent regions may demonstrate the features with spatial and temporal correlations. despite of providing a fault-tolerant mechanism for data aggregation, the redundant data will result in superfluous data transmission, and it will lead to collisions and undesired energy depletion to affect the network lifetime [cit] . in view of the recognizable targets, the evolutionary algorithms are proposed to arrange the nodes with similar monitoring results into the same cluster as far as possible. the amount of data that the chs communicate with the bs is maximally compressed by the fusion of similar information. finally, the sensor nodes with spatial-correlation can be organized as much as possible in a cluster. therefore, it can not only improve the accuracy of the data in the monitoring area, but also reduce the transmission cost of the ch. however, this kind of methods has high complexity and long time consuming [cit] ."
"after developing the complex operations required for the butterfly computation, equation (7) is obtained. (7) this \"direct\" form of the fft butterfly computation requires four multipliers and six adders. the number of multipliers can be reduced by rewriting equation (7) into a different form as shown in equation (8) ."
"results are presented in terms of precision, dynamic range, resources, performance, and power consumption. comparisons are made against hardware implementations of singleprecision floating point (sfpu) and fixed-point (fx) with similar precision and fixed data width (32 bits). results for gcc software emulation of single-precision floating point (swfp) are also presented when appropriate. this is a special case. it is presented throughout the results not as a fair comparison, but as an example of an alternative easily available to the designer. throughout the results in this section, the notation n p 0 p 1, previously defined in section 4.1, is used to specify the word length and binary point position of the different numerical representations used."
"the mutual support degree between node s i and s j can be defined as the confidence distance, which is expressed as function del(i, j). the smaller the value is, the closer the measurement value of the pair of sensor nodes is. conversely, it demonstrates that the monitored data collected by those nodes differ considerably. clustering method is based on fuzzy matrix to classify the observed objects. for different confidence levels, different classification results can be obtained, and then to form a dynamic clustering diagram."
"in this section, we a build a common structure for the fft and viterbi decoding algorithms starting from the architectures of the previously presented fft and viterbi decoder."
"starting from the representation of the input variables, analytical methods generate approximate representations for each operation, in a consecutive order. the goal is to then provide minimum precision and range requirements for each variable so as to guarantee a certain level of accuracy in the final result."
"we implement luby transform (lt) code [cit] in tinyos on tinynode hardware by tackling a set of challenges, such as decoding overhead reduction, fast decoding and block size adaptation. a link layer protocol is proposed to support the distanceless transmissions. we further propose the expected distanceless transmission time (edtt) metric for evaluating the link quality. edtt can be integrated into the existing routing protocols to enable network-wide data collection. we finally extend the distanceless networking to duty-cycled sensor networks. the final system includes both link and network design. it significantly enriches the connectivity of sparse sensor networks and thus improves the reliability and efficiency of data collection. figure 2 depicts the architecture of distanceless networking, which has been presented in our regular paper [cit] . in this demo, we brief introduce the main components. we implement four major modules compatible to the existing ieee 802.15.4 networking stack with the minimal modifications to current protocol implementations in tinyos. to transmit a data packet, the processed data packet is delivered to the logical link control module from the routing module to generate rateless blocks. frames are passed to the existing mac layer for transmissions using existing multiple access schemes, like lpl and csma/ca. the optimization of transmission parameters, e.g., block size and frame length, are also performed in the logical link control module. for receiving, the fast decoding module retrieves blocks from the buffer and passes blocks to the logical link control to decode while the radio is still receiving the rest of frame. the receiver with a smaller forwarding cost will relay the packet if it wakes up first."
"the title compound was obtained unintentionally in a good yield from a three components reaction by heating of 1 mmol (172 mg) 2-hydroxynaphthalene-1-carbaldehyde, 1 mmol (156 mg) 2-fluoro-5-nitroaniline and 1 mmol (188 mg) 5phenylcyclohexane-1,3-dione in ethanol for 8 h at 350 k. the solvent was evaporated under vacuum and the resulting solid was crystallized from a mixture of ethanol and few drops of acetone. yellow rods of product (m.p. 471 k) were collected (73% yield) of sufficient quality for x-ray diffraction."
"starting from (5), we can deduce the operations required by the viterbi butterfly implementation. the computation of every path metric requires tow adders and one subtractor as illustrated in fig.12 ."
"the main limitation for systems considering rtr is the performance penalty paid because of the reconfiguration time overhead. this overhead is quantified by taking into consideration the bitstream size and the data transfer speed of the configuration circuitry [cit] . selectmap and icap are the external and internal parallel reconfiguration ports for xilinx fpgas, respectively [cit] . selectmap provides an 8-bit or 32-bit bidirectional data bus interface to the virtex 4 configuration logic that can be used for configuration and readback at an operation frequency of 100 mhz [cit] . icap has 32-bit wide input and output data buses and is also set to run at a maximum frequency of 100 mhz. thus, a maximum theoretical speed at which data can be transferred into the configuration memory using icap or selectmap is 3.2 gb per second."
"next, reconfiguration is added, and the performance of a single core is measured. as mentioned before, all three operations have the same latency. also, the bitstreams to partially reconfigure both-their precision control section and the operation itself-have the same size. this might seem counter-intuitive at first, since one expects the multiplication to require more logic resources. the bitstream sizes are the same figure 9 : performance results for a single ddfx core versus reconfiguration frequency for virtex ii pro microblaze (a) and virtex 4 powerpc405 (b). here, 0% can not be represented in our logarithmic scale, thus it was included as the leftmost point in the graph. these measurements were taken using running hardware systems described in section 5."
"by dynamic precision we refer to a scheme in which a hardware implementation of an arithmetic operation changes in time to adapt its precision (change on the binary point position) according to its needs. this scheme can be accomplished by using runtime partial reconfiguration to reconfigure arithmetic modules as long as the reconfiguration time overhead is small as compared to the algorithm execution time. the reconfiguration time overhead can be decreased by reducing the amount of hardware changes required to varying precision, and by reducing the number of times precision changes are required (reconfiguration frequency). thus, we want to consider numerical representations with small hardware footprint and with a large dynamic range."
"similar to our accumulation example in section 2.3, we expect that the precision requirements for x k will grow. ideally, after each iteration is completed, we want to adjust the precision of x k, so as to maintain the maximum possible precision. on the other hand, the static matrices in (10) are best kept at a constant precision. this approach avoids the costly numerical conversion of a matrix. here, we note that the type conversion for x k is limited to n vector elements."
"similar to the adder, the blocks that control precision are shown in gray. the multiplier itself does not deal with precision. the blocks in figure 5 can be rearranged in static and dynamic parts. the section of the multiplier that one needs to change in order to change the multiplier's precision represents less than 30% of the overall logic resources used by the multiplier."
"a block diagram of the multiplier is shown in figure 5 . the diagram is divided in two stages. the first stage is a full precision 2's complement, binary multiplier. the second stage-a postscaler-takes the multiplier output and performs an analysis similar to the adder's postscaler case. however, in the case of the multiplier, no shifting is required. only bit slicing is performed."
"we experimented using partial reconfiguration to shut off different sections of a system at different times, without affecting the rest of it. for this purpose, shift registers of different sizes, built upon the srl16 primitive (basically luts) were connected to both suts as customized cores. the experiment consisted of shutting off one peripheral at a time and measuring the current at the fpga core every time. the shut-off of a core is done by reconfiguring the section that the core occupies with a \"blank\" bitstream-equivalent to having nothing programmed in that section."
"the following case may occur. assuming that competition radius of candidate-ch s i is larger than candidate-ch s j and s j can receive the comp_msg message from s i . however, the comp_msg message from s j can not be transferred to s i due to the limitation of the transmission range of s j, and it will result in that s i not be aware of the existence of candidate-ch s j . in order to acquire complete information about adjacent competitors, any candidate-ch must estimate the distance from the sender after receiving the comp_ msg message. if the distance is greater than the radius of their competition, it is necessary to reissue a comp_msg message to the sender. in this way, it can assist the senders in acquiring complete neighbor candidate-ch information and updating the neighbor candidate-ch table. after receiving the comp_msg message, candidate-ch will compare its residual energy with the sender's. if the remaining energy is less than sender's, it will quit the competition, set its status as member node, and broadcast quit_msg. otherwise, it will wait for the comp_msg from other competitors till the end of the ch selection. if still does not withdraw from the competition, it will broadcast sus_msg message to all nodes in its transmission range to declare being elected to the ch, and modify the state flag. if a candidate-ch receives the quit_msg, the node will check whether the final state has been determined or not. if already being the ch or member, it will drop this message. otherwise, once the node is still of the candidate-ch state, it will update the status of the sender to member state in the neighbor candidate-ch table, and continues to wait for the messages from other neighbor candidate-chs to decide its final state."
"after standardization, the matrix x will be transformed into a fuzzy matrix [cit] . firstly, the shift and standard deviation transformation is implemented and the element in the normalized matrix can be given as where"
(4) figure 1 shows how x's precision requirement increases with the number of iterations for a starting precision of 8 bits. this curve represents an upper bound for a full precision arithmetic operation.
"2.1.1. dynamic range. dynamic range is a quantitative measurement of the ability to represent a wide range of numbers, and it is defined by the relationship between the largest and smallest magnitudes that a numerical format can represent."
"several tools are available to estimate power consumption. for xilinx fpgas, we have the xilinx power estimator (xpe) and the xilinx power analyzer (xpa). xpe provides an estimate based on highlevel resource consumption statistics and average switching activity. xpa provides a more accurate estimate based on simulated switching activity and exact utilization statistics [cit] . however, the most accurate way to determine fpga power consumption is still through hardware measurements. moreover, there is no tool available that can provide an estimate of the power consumption during reconfiguration."
hardware realization of the common operator can be now presented to perform with the same architecture fourier transforms over gf(f t ) and over c.
"usually, runtime reconfigurable implementations involve the exchange of relatively large functional units that have large processing times. this, along with low reconfiguration frequencies, significantly reduces the impact of the reconfiguration time overhead on the performance."
"the classical architecture of the viterbi algorithm can be divided into three units, as shown in fig.9 . the branch metric calculation (bmc) unit computes the distances (branch metrics) associated to each transition of the trellis in order to evaluate the correctness of the received data for a given transition. secondly, the add compare select unit (acs) computes the accumulated metrics (called path metrics) and selects the incoming survivor path for each state of the trellis. finally, the survivor memory management unit (smm) stores the decision taken by the acs unit in order to provide the most likely decoded path at the output of the decoder."
"unlike common runtime reconfigurable implementations, the exchangeable functional units in this approach are smaller, and reconfiguration frequencies are larger. smaller exchangeable functional units are possible by using a dual fixed-point (dfx) numerical representation [cit] which provides larger dynamic range than classical fixed-point representations with little extra cost in terms of hardware. we introduce a dynamic dual fixed-point (ddfx) architecture that allows changes in precision (binary point position) based on relatively small changes at runtime in the hardware implementation. although at a higher reconfiguration time cost, ddfx also allows the dynamic swap between different arithmetic operations."
the dynamic power is due to the energy consumed by the device when it is doing useful work. dynamic power is mainly dependent on the number of transitions in the logic gates' transistors (frequency of operation) and the gate count of the design in question. some design decisions can help to keep this power in budget (including different number formats). consider the performance curves shown in figure 8 and the corresponding power consumption curves presented in figure 12 . the relationship between these curves suggests that our dynamic approach can be exploited by a system to dynamically tradeoff performance for power consumption.
"it is natural to call the discrete index i \"time', taking values on the time axis 0, 1, ..., n-1, and to call f the \"time-domain function' or the 'signal'. also, one might call the discrete index \"frequency', taking values on the frequency axis 0, 1, ..., n-1, and to call f the \"frequency-domain' or the \"spectrum'."
"the simulation results of figure 17 demonstrate the success of the approach. the actual solution was estimated using the pseudoinverse (computed using singular value decomposition) in double arithmetic. in our example, we compute euclidean distances from x k to the actual solution. in other words, we report on the root mean square (rms) of the error between x k and the actual solution. we also report distances in db using 20 log 10 (rms solution /rms error ). similarly, we report the distance between the doubleprecision floating-point arithmetic and the dynamic dual fixed-point iterate in figure 17(d) . the dynamic precision reconfiguration is shown in figure 17 (e)."
"firstly, from figure 17 (b), we can see that the dynamic dual fixed-point and the double-precision floating-point arithmetic iterations converge to the correct solution. the dynamic dual fixed-point is very close to the doubleprecision floating-point estimate as shown in figure 17(d) . as shown in figure 17(a), the static dual fixed-point fails to converge. the dynamic dual fixed-point has switched between 4 representations as shown in figure 17 clear from this example that ddfx can achieve convergence to the correct solution using significantly less resources and total energy consumption than double-precision floatingpoint arithmetic."
"by focusing on the butterfly structure of the viterbi algorithm, it is possible to analyze how it operates and then highlight the similarities with the fft butterfly. indeed, the viterbi butterfly involves not only the computation of the path metrics, but also a comparison module. the comparison module can easily be realized using a subtractor associated to a multiplexor. thus, for the implementation of the path metrics computation, two adders and one subtractor are required. equation (5) describes the computation performed by the butterfly and fig.10 illustrates its structure. the defined viterbi butterfly corresponds to the acs module in fig. 9 . (5) as shown in this section, the fft and viterbi algorithms have strong similarities if we compare their butterfly structures. these similarities can be explored to build a common structure for the two algorithms."
"we also present simulation results on the use of ddfx for inverting large matrices. for the jacobi method, for inverting large linear systems, it is shown that ddfx's results closely approximates that of double precision floating-point."
"range. for a fixed precision and data width (32 bits), dynamic ranges vary between the hardware implementations compared. table 1 shows dynamic range and precisions for each of the numerical formats chosen for comparisons purposes. the values were calculated using the definitions presented in section 2."
"in what follows, we assume that a variety of possible partial bitstreams are available to the system for reconfiguration at run time. thus, they need to be stored somewhere accessible to the system. the solution provided by xilinx, is to store the partial bitstreams in a compact flash memory (cf), which the system under test sees as a file system. this approach proved to be extremely slow, due to the cf high access latency. testing shows bitstream loading speeds of about 100 kbits/sec, way 6 international journal of reconfigurable computing below the maximum possible data rate of 3.2 gbits/sec. an alternative considered was to store the partial bitstreams in a network file system. this solution, although slightly faster than the cf solution, was more costly in terms of logical resources (an ethernet supporting core is comparatively larger than a cf supporting core). moreover, loading speed in this case would be nondeterministic and affected by the ethernet communication channel."
"when performing addition of dfx numbers, both operands must be aligned with respect to the binary point. thus, a prescaler section is required to international journal of reconfigurable computing analyze the operands and to determine in which range-p 1 or p 0 -one must perform the addition. depending on the range in which addition is performed, one may need to scale an operand to a different range. this is done by executing an arithmetical shift to the right on the operand. a set of multiplexers are then used to select between an unchanged operand or its shifted version. note that a truncation is performed when an operand is scaled. the bits discarded by the truncation are saved using a third multiplexer. these bits can later be recovered into the final result if the output range allows it. a block diagram is shown in figure 4 . the prescaler produces two operands that are handed to the second stage: a full binary adder. this adder does not allow for a carry in, but it does produce an overflow signal. the adder's output is given to the third and last stage: the postscaler. in this stage, one decides in which radix the result will be presented and scales the output if needed. scaling is again performed via shifting and multiplexers. note that the adder described in this section can easily be transformed into a subtractor by including a 2's complement block for the subtrahend operand."
"for equivalent logical resources, precision, and performance (equivalent upper bounds), the proposed solution consumes 10% less power (in the case of the virtex 4) than its alternatives. we also showed an approach that can produce power savings by dynamically managing the number of processing cores, effectively trading off performance by power consumption. our measurements show that the savings in overall energy consumption are significant. the approach is extensible to more general embedded systems, where a dynamic reconfiguration scheme can be used to shutdown unused cores. we have also quantified the power consumed during reconfiguration, showing that it is not significant. finally, we provide a simulation example where ddfx was shown to closely approximate results obtained with doubleprecision floating-point arithmetic. this suggests that ddfx international journal of reconfigurable computing provides for a promising platform for large numerical analysis problems where floating point is believed to be the only alternative."
"the candidate-chs broadcast their participation information to adjacent nodes in the range of competition radius with the corresponding transmission power. the comp_ msg message contains the candidate-ch id and residual energy. if receiving that message, the other candidate-chs will record the id of the candidate-ch to the neighbor candidate-ch table. due to different size of the competition radius from candidate-chs, ("
"taking into account that temporal correlation of the data collected by those nodes, the correlation coefficient method is employed to form the fuzzy similarity matrix."
"the same structure can also be identified in the viterbi algorithm, used for finding the most likely sequence of states in a trellis, which is the most usual representation of convolutional code state diagram. although it is not the most compact form, the trellis structure is commonly used because it easily illustrates the sequencing of decoding algorithms (fig.8) ."
"the overall power consumption is shown in figure 14 . this graph was obtained unifying the results of dynamic and static power consumption. a definite advantage of ddfx over swfp and sfpu alternatives can be observed in the case of the virtex 4 while, in the case of the virtex ii pro, no clear advantage is observable. this discrepancy can be explained by the architectural and technology differences between both families."
"furthermore, we compare the average number of nodes with different node's density in each round. as shown in fig. 5, the number of dormant nodes in high-density is more than in low-density. that is because the correlation of data collected by neighboring nodes is much higher and more dormant nodes being selected have little impact on subsequent data fusion. as a result, the number of redundant nodes that are scheduled to sleep is relatively more. in addition, we can find that the average number of dormant nodes at different densities is relatively stable, which provides the basic foundation for maintaining stability of the average energy consumption among all nodes."
"schiff bases have been widely studied due to their importance in industrial and biological applications. they serve for example as, antibacterial, antifungal, anticancer [cit] and herbicidal agents [cit] . it is well known that the introduction of fluorine atom into an organic molecule causes dramatic changes in its biological profile [cit], mainly due to the high electronegativity of fluorine."
"are verified, every radix-2 algorithm applied to fft can be applied to the fnt. the heart of this algorithm known as the \"butterfly\" was redesigned. here re-designing means taking into account the reconfiguration of the operators constituting the butterfly as well as the connection between those operators. the switching from fft mode to fnt mode should be accompanied by the replacement of the twiddle factor w by the primitive element  of the given galois field."
"this decomposition allows the pooling of the fft and viterbi operators by suggesting a common structure for each stage. the first stage for each operator requires independent add/subtraction operations, four for the viterbi bmc (or two for a more optimized form) and three for the fft. then the first stage of the common operator consists of three a/s blocks. the second stage is dedicated to the fft butterfly, it consists in three multipliers."
"because the partial reconfiguration regions allocated for these sections are similar in area. this is independent of the percentage of resources used in each region. however, note that the architectural changes between the virtex ii pro and the virtex 4 families made possible for the areasthus the bitstreams-to be smaller in the case of virtex 4. this, as figure 9 shows, represents a significant difference in the way performance drops for both families, when reconfiguration is included in the measurements. dynamic reconfiguration can be triggered by the arithmetic cores asserting a signal that will be treated as an interrupt by the microcontroller. in our examples, reconfiguration was hardwired in the testing code to occur after a fix number of operations. this allowed us to emulate different reconfiguration frequencies."
