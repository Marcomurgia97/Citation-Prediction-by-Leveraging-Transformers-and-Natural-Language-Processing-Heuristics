text
"in the backbone of large-scale networks, origin-to-destination (od) traffic flows experience abrupt changes which can result in congestion, and limit the quality of service provisioning of the end users. these so-termed traffic volume anomalies could be due to external sources such as network failures, denial of service attacks, or, intruders which hijack the network services [cit] . unveiling such anomalies is a crucial task towards engineering network traffic. this is a challenging task however, since the available data are usually high-dimensional noisy link-load measurements, which comprise the superposition of unobservable od flows as explained next."
"it is worth noting that not only s, but also the position of the nonzero entries in a 0 plays an important role in satisfying i) and ii). this is manifested through k, for which a small value indicates the entries of a 0 are sufficiently spread out, i.e., most entries do not cluster along a few rows or columns of a 0 ."
"the proof bears some resemblance with those available for the matrix completion problem [cit], and pcp [cit] . however, presence of the compression matrix r gives rise to unique challenges in some stages of the proof, which necessitate special treatment. in what follows, emphasis is placed on the distinct arguments required by the setting here."
"extensive numerical tests with algorithm 1 suggest that the convergence rate can vary considerably for different choices e.g., of the matrix r. the ad-mom algorithm on the other hand exhibits less variability in terms of performance, and only requires tuning c. it is also better suited for the constrained formulation (p1), since it does not need to resort to a relaxation. white represents exact recovery (er ≈ 0), while black represents er ≈ 1."
"the ln solution is an attractive choice, since it facilitates satisfying c2) and c3) which require norms of vec(x) to be small. substituting the ln solution (11) into the left hand side of c2) yields (define"
"] is in the column [row] space of x 0 for some (i, j). small values of γ r (u) and γ(v) imply that the column and row spaces of x 0 do not contain the columns of r and sparse vectors, respectively."
"theorem 1), which limits the extent to which r and s can be increased. if the correlation between any two columns of r is small, then higher rank and less sparse matrices can be exactly recovered."
"this three-step procedure implements a block-coordinate descent on the augmented lagrangian, with dual variable updates. the minimization (36) can be recast as (28) while not converged do"
"the ad-mom is an iterative augmented lagrangian method especially well-suited for parallel processing [cit], which has been proven successful to tackle the optimization tasks encountered e.g., in statistical learning problems [cit] . while the ad-mom could be directly applied to (p1), r couples the entries of a and it turns out this yields more difficult ℓ 1 -norm minimization subproblems per iteration. to overcome this challenge, a common technique is to introduce an auxiliary (decoupling) variable b, and formulate the following optimization problem"
"to measure the incoherence among subsets of columns of r, which is tightly related to the second condition in lemma 1, the restricted isometry constants (rics) come handy [cit] . the constant δ k (r)"
"the class of admissible compression matrices can be extended to matrices which are block diagonal up to row and column permutations. let π r (π c ) denote, respectively, the row (column) permutation matrices that render r block diagonal. instead of (1) consider"
"which is equivalent to (p1). to tackle (p3), associate lagrange multipliersm andm with the constraints (31) and (32), respectively. next, introduce the quadratically augmented lagrangian function"
"one can envision several extensions to this work, which provide new and challenging directions for future research. for instance, it seems that the requirement of an orthonormal compression matrix is only a restriction imposed by the method of proof utilized here. there should be room for tightening the bounds used in the process of constructing the dual certificate, and hence obtain milder conditions for exact recovery. it would also be interesting to study stability of the proposed estimator in the presence of noise and missing data. in addition, one is naturally tempted to search for a broader class of matrices satisfying the exact recovery conditions, including e.g., non block-diagonal and binary routing (compression) matrices arising with the network anomaly detection task."
"let k i denote the maximum number of nonzero elements per 'trimmed' column of a 0, the trimming being defined by the block of rows of a 0 that are multiplied by r i when carrying out the product ra 0 ."
"note that a ′ ω (a ω a ′ ω ) −1 is the pseudo-inverse of the full row rank matrix a ω (cf. lemma 3), and thus"
"this results in α max ≈ f/l and β max ≈ (ω −1 max − 1) −1 when l ≪ f . satisfaction of i) and ii) then requires o(1) summands in the left-hand side of ii), which gives rise to"
"there are two key aspects to the success of apg algorithms. first, is the selection of the points t[k] where the sequence of approximations q(s, t[k]) are formed, since these strongly determine the algorithm's con-"
"two iterative algorithms for solving (p1) are developed in section vi, which are based on the accelerated proximal grandient (apg) method [cit], and the alternating-direction method of multipliers (ad-mom) [cit] . numerical tests corroborate the exact recovery claims, and the effectiveness of (p1) in unveiling traffic volume anomalies from real network data (section vii). section viii concludes the paper with a summary and a discussion of limitations, possible extensions, and interesting future directions."
"the corresponding task arises with network traffic monitoring, brain activity detection from undersampled fmri, and video surveillance tasks, while it encompasses compressive sampling and principal components pursuit. to estimate the unknowns, a convex optimization program is formulated that mininimizes a tradeoff between the nuclear and ℓ 1 -norm of the low-rank and sparse components, respectively, subject to a data modeling constraint. a deterministic approach is adopted to characterize local identifiability and sufficient conditions for exact recovery via the aforementioned convex program. intuitively, the obtained conditions require: i) incoherent, sufficiently low-rank and sparse components; and ii) a compression matrix that behaves like an isometry when operating on sparse vectors. because these conditions are in general np-hard to check, it is shown that matrices drawn from certain random ensembles can be recovered with high probability. first-order iterative algorithms are developed to solve the nonsmooth optimization problem, which converge to the globally optimal solution with quantifiable complexity. numerical tests with synthetic and real network data corroborate the effectiveness of the novel approach in unveiling traffic anomalies across flows and time."
"for multiple solution pairs. however, the next lemma asserts that a slight tightening of the optimality conditions i)-iii) leads to a unique optimal solution for (p1). see appendix a for a proof."
holds with probability at least 1 − n −cπλτ if δ k (r) and the right-hand side of (19) do not exceed one. consider (19) when λ is small enough so that the quantity inside the square brackets is close to one.
"in words, (1) is locally identifiable if and only if the subspaces φ and ω r intersect transversally, and the sparse matrices in ω are not annihilated by r. this last condition is unique to the setting here, and"
"among other implications, matrices x 0 and r with small γ r (u) and ξ r (u, v) are such that the columns of r (approximately) fall outside the column space of x 0 . from a design perspective, this suggests that the choice of an admissible x 0 (or in general an ensemble of low-rank matrices) should take into account the structure of r, and vice versa. however, in the interest of simplicity one could seek conditions dealing with x 0 and r separately, that still ensure γ r (u) and ξ r (u, v) are small. this way one can benefit from the existing theory on incoherent low-rank matrices developed in the context of matrix completion [cit], and matrices with small rics useful for cs [cit] . admittedly, the price paid is in terms of stricter conditions that will reduce the set of admissible matrices."
". the second key element stems from the possibility of efficiently solving the sequence of subproblems (27) . for the particular case of (p2), note that (27) decomposes into"
"alternative sufficient conditions for exact recovery, expressible only in terms of the aforementioned basic parameters, can be obtained by combining the bounds of this section along with i) and ii) in theorem 1."
"moreover, no restriction is placed on the magnitude of these entries, since as seen later on it is only the positions that affect optimal recovery via (p1)."
"where in (a) and (b) it was used that the rows of r are orthonormal, and the maximum singular value of a projection matrix is one. substituting (49) and the bound of lemma 3 into (48), leads to (4)."
"where d is the distance from the lens to the iris plane, w img is the width of the iris as measured in the image, and w world is the actual width of the iris. variables d and u are related by the lens equation"
"the newest wave of vr and ar devices include integrated eye tracking devices, and are susceptible to identity theft and spoofing attacks. in this section we describe the threat model that puts the user at risk, and propose defocus as a solution to enable secure eye tracking configurations. we provide a theoretical basis of a solution and evaluate it with respect to degrading the iris biometric and errors in gaze estimation while viewing on-screen targets."
"1) biomarker prediction network: we employ the choroidal thickness as a prior knowledge, which has been found to be related to various ocular diseases as mentioned in the introduction, to assist the segmentation. the choroidal thickness denotes the average distance between the upper boundary (bruces membrane, bm) and the lower boundary (csi). the biomarker prediction network trained a thickness fig. 3 . illustration of the bio-net for the choroid segmentation. firstly, the biomarker prediction network is trained to predict the biomarker, and its parameters are fixed after that. then, we follows the anatomical hierarchy of the retinal oct and employ the global multi-layers segmentation module to segment the oct image into 12 layers. finally, the global multi-layered result and the original oct image are concatenated and fed into the local choroid segmentation module to segment the choroid region, where the biomarker information is infused and applied as a regularization."
"stimuli generation the animation renders from study 1 were used for study 2. however, instead of two animations being presented only one animation was shown at a time in the center of the screen."
"we implement optical defocus to create a secure eye tracking configuration. we use a pupil labs pro glasses-based eye tracker with an adjustable telescoping arm to increase camera distance. example eye images from in-focus and out-of-focus configurations are shown in figure 2 . eye trackers for xr devices use similar cameras, and this form of eye tracker is readily available to researchers and consumers. this is one instance of a secure eye tracking configuration. an example alternative configuration would be using an eye camera with an adjustable focus lens."
"as shown in figure 3, the distance between the lens and camera sensor, u, is constant. the same process was followed to set up the in-focus eye tracker configuration for each participant. we compute u as follows: (1) for each participant eq. 3 and eq. 4 are used to solve for d, (2) the average distance for all participants, d, is computed, (3) d is substituted into eq. 4 to compute u. this process assumes that the measured values of d are within the depth of field of the camera for which the iris region is in-focus, and can be be estimated with the average distance, d."
"to evaluate the effectiveness of the global multi-layers segmentation module and the biomarker prediction net, we combine them with the u-net respectively. table iii and fig. 11 illustrate the effectiveness of the biomarker prediction network and the global multi-layers segmentation module. in the experiment, we take the u-net as a baseline, the table demonstrates that the infusion of the biomarker prediction network can lead to an improvement on the choroid segmentation task, as the di increases from fig. 11 . ablation study of the bio-net. from left to right: the input oct b-scans, ground truth, the segmentation results using the global multi-layers segmentation module (gms), the u-net baseline (base) and the gms, the baseline and the biomarker constraint (bio), and our bio-net, respectively. 88.36% to 90.27% and the ausde decreases from 8.01 pixels to 6.46 pixels. on the other hand, the performance of the u-net added by the global multi-layers segmentation module makes the iou increases from 79.14% to 81.34% and ausde decreases from 8.01 pixels to 6.54 pixels. meanwhile, the ausde is 3.50 pixels lower and sen 1.79% higher than only gms module employed, which demonstrates that the globalto-local network works better than a single global multi-layers segmentation module or a single local choroid segmentation module."
"the objective of this paper is to examine, fundamentally, and assuming the csi transfer problem can be solved, under what conditions non-orthogonal sharing can provide gains as compared to conventional operations. specifically, 1) we model non-orthogonal spectrum sharing via a miso ic, and the reference scenario as a miso broadcast channel (bc); 2) we summarize known upper and achievable bounds on the sum-rate for these models in secs. 3 and 4, respectively; 3) we evaluate the bounds for flat and frequency selective fading in sec. 5; 4) we draw conclusions on what operating points where non-orthogonal spectrum sharing is likely to work at all. sec. 6 summarizes these conclusions."
"we obtain the upper bound by performing power allocation using water-filling. we note that if h 1,k and h 2,k are orthogonal, then these trivial upper bounds will coincide with the achievable zf bounds."
"an iris authentication procedure is used to evaluate the increase in security from an in-focus configuration to out-of-focus configuration. utility is measured using a target viewing task in a typical eye tracking setup, with error calculated between the estimated gaze positions and on-screen targets. ideally, a secure configuration will degrade iris authentication while preserving the accuracy of gaze estimation."
"of optic nerve head [cit] . this algorithm was then employed in the calculation of the attenuation coefficients of retinal tissue [cit], enhancing the visibility of lamina cribrosa [cit], and improving the contrast of the choroid vasculature and the visibility of the sclera-choroid interface [cit] ."
"eye tracking will transform virtual and mixed reality. major hardware companies are integrating eye trackers into head-mounted displays (hmds) to enable applications ranging from intuitive gaze-based interfaces [cit], foveated rendering [cit], and streaming optimization [cit] . foveated rendering is driving eye tracking within vr headsets due to the potential to both optimize resources and reduce simulator sickness [cit] . for social virtual reality with hyper-realistic virtual avatars [cit], eye tracking is required to transfer non-verbal social cues from the user to his or her conversational virtual avatar."
"iris authentication infrared images of the eye with sufficient resolution capture iris patterns unique to the individual. iris recognition places in the top tier of biometrics as it is universal, distinct, permanent, and robust against spoofing attacks [cit] . it is important to keep the iris pattern secure, as recognition methods are robust to poor lighting [cit], off-axis imaging [cit], occlusion [cit], and distance [cit], making the biometric accessible at times when the user may not consent. iris authentication has been long established through the work of john daugman [cit] and many others 1, as a statistically valid method for recognition of an individual. as a result, iris patterns have been trusted for identification at voting booths [cit], border customs [cit], schools [cit], and in hospitals [cit] . these applications highlight the sensitivity of information that could be accessed if a hacker is able to steal identity through a biometric. thus, the presence of a user's iris within a dataset or application places the user's identity at risk."
"future work it would be interesting to investigate an optimization framework for security and utility. it would also be useful to create implementations of secure eye tracking configurations that apply to different camera form factors. additional perceptual experiments with a smaller distance from an avatar, and more realistic features on the avatar that include blinks, eyelid movements, and pupil diameter would provide further insight as well as implementing a similar evaluation within an immersive vr environment. our work motivates active research in these directions before eye tracking in xr becomes ubiquitous and users are at risk to malicious attacks."
"to obtain an upper bound, we optimize the power using water-filling. 4 in order to simplify notation, we omit the subchannel index when we focus on a single subchannel."
"participants nineteen participants (14 male, 5 female) with age ranging from 19 to 39 were recruited from the university community under an irb approved protocol. all participants reported normal or corrected-to-normal vision. participants were ineligible if they had previously participated in study 1, to ensure they had not previously seen the animation stimuli."
"here, we first give the system model for the miso ic using linear beamforming. then, by using such a transmission scheme we obtain lower (achievable) bounds on the sum-capacity of the miso ic. the matched-filtered symbol-sampled complex baseband data received by the rxs on subchannel k for the miso ic is modeled as"
"the goal of study 1 is to answer rq 1 . we designed a same-different experiment where naive viewers are presented with a reference avatar with unmodified eye tracking and a stimulus avatar with modified eye tracking, and they are tasked with reporting whether the two avatars are identical or different from each other. we compute psychometric curves from the participant responses and report the point of subjective equality (pse) and detection threshold (dt). these values clarify the level of defocus at which viewers are able to perceive a difference in the eyes of the virtual avatar."
"we further analyse and discuss the details of the proposed method in this section, including the ablation study of the bio-net and the comparison of different inpainting methods."
"we compare the proposed shadow elimination pipeline with the a-line based ac algorithm [cit] and our previous implementation of this shadow localization and elimination pipeline, which used the cti as the inpainting algorithm [cit] . figure 9 demonstrates the visual examples of the shadow elimination. from left to right: the original en face choroid image, the shadow mask, and the shadow elimination results using different methods. the second and third rows are the zoom-in views inside the blue and yellow boxes in the first row, respectively. the last row are the corresponding vessel maps."
"camera distance the out-of-focus configuration was implemented by increasing distance between the eye and camera to degrade iris authentication. first, the in-focus configuration was set up by placing the eye camera as close as possible to the user's eye, while keeping the eye in the center of the eye image frame. then, to create the out-offocus configuration the experimenter adjusted the telescoping arm to the farthest point, again orienting the camera such that the eye stayed within the frame. we compute the distance between the camera lens and the eye to quantify the impact of this process on gaze accuracy and iris authentication. camera distance is computed by modeling an imaging system with a thin lens. figure 3 illustrates such a system. the distance between the iris and lens, and the lens and camera sensor are related by"
"abstract-the choroid provides oxygen and nourishment to the outer retina thus is related to the pathology of various ocular diseases. optical coherence tomography (oct) is advantageous in visualizing and quantifying the choroid in vivo, because it does not suffer from the information contamination of the outer retina in fundus photography and scanning laser ophthalmoscopy and the resolution deficiency in ocular ultrasound. however, its application in the study of the choroid is still limited for two reasons. (1) the lower boundary of the choroid (choroidsclera interface) in oct is fuzzy, which makes the automatic segmentation difficult and inaccurate. (2) the visualization of the choroid is hindered by the vessel shadows from the superficial layers of the outer retina. in this paper, we propose to incorporate medical and imaging prior knowledge with deep learning to address these two problems. we propose a biomarker infused global-to-local network, for the choroid segmentation. it leverages the thickness of the choroid layer, which is a primary biomarker in clinic, as a constraint to improve the segmentation accuracy. we also design a global-to-local strategy in the choroid segmentation: a global module is used to segment all the retinal layers simultaneously for suppressing overfitting, then a local module is used to refine the segmentation with the biomarker infusion. the u-shape convolutional network is employed as the backbone in these modules. for eliminating the retinal vessel shadows, we propose a deep learning pipeline, which firstly use anatomical and oct imaging knowledge to locate the shadows using their projection on the retinal pigment epthelium layer, then the contents of the choroidal vasculature at the shadow locations are predicted with an edge-to-texture twostage generative adversarial inpainting network. the experiments shows the proposed method outperforms the existing methods on both the segmentation and shadow elimination tasks on a oct dataset including 1280 labeled oct b-scans and 136 oct volumes. we further apply the proposed method in a clinical prospective study for understanding the pathology of glaucoma, which demonstrates its capacity in detecting the structure and vascular changes of the choroid related to the elevation of intraocular pressure."
"the vessel shadows could be treated as the real vessels in clinical assessment, which would cause the overestimation of the vd. the calculated vd values of the vessel maps in the last row of fig. 9 are: 0.510 for the original choroid, 0.504 for the ac, 0.501 for the cti, and 0.500 for the proposed method. we also calculate a vd of 0.499 without including the shadow areas. the results are in accordance with the overestimation assumption, in which the original image has the highest vd. the ac method could eliminate part of the shadows thus lower the vd. the cti and proposed method could further lower the vd because they remove the shadows completely. besides, their vds are very close to that of the masked vessel map, which indicate the effectiveness of this shadow localization and elimination pipeline. we checked the vds of other testing datasets,which follow the exactly same trend. however, because the variation of the vds among different eyes are much larger than that of the shadow elimination, we did not include their average values and standard deviations here."
"an approach to protecting eye images is to only stream gaze data that is relevant to the application [cit] . image data is encapsulated within a processing unit, reducing the chance that a malicious user can gain access. however, this also restricts applications that may utilize the iris for improved gaze estimation [cit], realistic rendering of the user's eye [cit], and iris authentication in cases where it is desired, such as logging into the microsoft hololens 2."
"the choriod, lying between the retina and the sclera, is the vascular layer which provides oxygen and nourishment to the outer retina [cit] . because traditional imaging modalities like fundus photography and scanning laser ophthalmoscopy acquire 2d overlapping information of the outer retina and the choroid, the pathological changes of the choroid could not be precisely retrieved and evaluated. on the other hand, ocular ultrasound is able to do 3d imaging, but it needs to touch the eye and has a low spatial resolution."
"in this paper, we have developed an automatic method for the segmentation and visualization of the choroid, which combined deep learning networks with prior medical and oct imaging knowledge. we have proposed the bio-net for the choroid segmentation, which is a biomarker infused globalto-local network. it outperforms the state-of-the-art choroid segmentation methods on the arod dataset. for eliminating the retinal vessel shadows, we have proposed a deep learning pipeline, which firstly locates the shadows using anatomical and oct imaging knowledge, then removes the shadow using a two-stage gan inpainting architecture. compared with the existing methods, the proposed method has superiority in shadow elimination and morphology preservation. we have further applied the proposed method in a clinical prospective study, which quantitatively detected the changes of the choroid in response to iop elevation. the results show it is able to detect the structure and vascular changes of the choroid efficiently."
"in this study, channels were randomly drawn but given. this corresponds to the case of delay constrained traffic. for future studies, multi-user diversity, i.e., the possibility of selecting two users from a larger pool of users per subchannel, should be incorporated. however, results not shown herein indicate that this does not change the main conclusion."
"as shown in the figure, the original choroidal vasculature is conterminated by the retinal vessel shadows at the locations shown in the shadow mask. inside the zoom-in views, the ac could enhance the contrast of the choroidal vessels and minimize small vessel shadows but could not get rid of the large vessel. using the localization and elimination strategy, both the large and small vessel shadows could be thoroughly eliminated, but as shown in the zoom-in views, the cti introduces unnatural artefacts compared with the proposed method."
"eye tracking data was collected at 30 hz using a pupil labs pro glasses-based eye tracker (ca. 2016) with an eye image resolution of 320x240 [cit] . we calibrated the fixed focus pupil labs eye camera using a checkerboard pattern and matlab's single camera calibrator app to compute a focal length, f, of 338.04 pixels (1.014mm). prior to analysis, frames containing blinks or motion blur were removed."
"authentication by hd i j has the highest accuracy when comparing in-focus images with in-focus images. as expected, in-focus images did not create any matches with out-of-focus images. however, outof-focus images did create matches with other out-of-focus images from the same individual, albeit less frequently. only three participants produced a hd i j less than hd auth for the out-of-focus images. table 1 reports the crr values, with an average in-focus crr of 78.6%, while the out-of-focus images had a rate of 7.1%."
"1) shadow localization: the idea of using rpe to locate the vessel shadows is inspired by two medical and imaging knowledge. (1) the retinal layers below the outer plexiform layer and above the bm are avascular [cit], so any vessel-like structure appears on these layers are the projected shadows."
"a series of recent activities have promoted the idea that operators of cellular wireless networks would benefit from sharing spectrum resources. activities reported in e.g., [cit], suggest inter-operator sharing of spectrum on small timescales (comparable to the fading coherence interval), using advanced scheduling and multiple antenna techniques. the sharing concept makes technical sense since spectrum is a finite resource, so whenever one operator does not need all of its resources, another one may borrow from it. in the long run both could gain from doing so. conceivably, since spectrum is expensive, larger revenues could be generated by better spectrum utilization."
"defocus-based identity preservation rana and colleagues presented a systems argument for why applications that process images and videos do not necessarily need access to the raw image feed [cit] explored adding blur to increase privacy of a tele-conference video feed. they found that there is no general purpose blur level that preserves utility across all scenarios in this context. for example, the participants specified a much higher amount of blur in video that captured embarrassing activities such as picking their nose or changing clothes, compared to daily computer work. participants were asked to identify the activities being performed in each video, with the level of blur being decreased until they could confidently classify the activity. the computed blur thresholds and classification rate determine that blur is effective at increasing privacy while retaining utility, but that the trade-off must be evaluated across applications and sensors. [cit] investigated various image filters such as masking, blurring, and pixelation with respect to their effectiveness in obscuring specific features of the content as well as retaining the utility and aesthetics of the photograph. they reported that blur was effective at obscuring the gender of the photographed person, though not so much the ethnicity or expression. ultimately they determined that there is no 'one size fits all' solution for every scenario, and object size or security context can influence the optimal method. pittaluga and koppal [cit] have implemented a similar blur-based privacy approach within the context of micro-scale image sensors. a hardware-based approach is used to add blur, as opposed to a software-based gaussian blur. the use of optics to scatter light before the image is captured creates blur on the camera sensor. applications like head tracking, person tracking, and facial recognition are explored with several types of camera sensors (thermal, ir, rgb) imaging the user. each camera configuration and application must be optimized and designed to balance the trade-off between security and utility. our work investigates adding blur to eye images pre-capture, however the goal is to do so without modifying the stock hardware or optics. this allows consumers to control their own privacy, as current consumer technology would lack any specialized privacy hardware."
"for each txi where p i,k is the power sent by txi on subchannel k. for the non-sharing scenario, each operator serves two users. each operator can spend power p over the ns subchannels. hence, the average power per user and subchannel is p/2ns. 2 in order to meet the power constraint for the non-sharing scenario, we must have"
"there is a growing concern in keeping eye movement data private and secure in both real-time applications [cit], and published datasets [cit] . publicly available datasets release de-identified gaze data from individuals viewing vr videos [cit], the social interactions of children with asd [cit], and individual responses to emotional content such as nude imagery and faces [cit] . sensitive information, such as personality traits [cit] and neurological diagnoses [cit], could be linked to individuals that contributed to the aggregate data. to protect against this type of attack, differential privacy techniques have been proposed for securing heatmaps and other gaze-based features [cit] . however, they are constrained to dealing with already recorded gaze data and not real-time streams."
"non-orthogonal sharing, by contrast, refers to permitting operators to transmit concurrently in the same time-frequency resource. fundamentally, a scenario with two transmitters (tx1, tx2 herein, belonging to the two operators) using the same time-frequency resource to send data to two receivers (rx1, rx2 herein, associated with tx1 and tx2 respectively) constitutes an interference channel (ic) [cit] . for the tx1-rx1 and tx2-rx2 pairs to achieve good performance, it is required that they coordinate their operations. multiple antennas at the txs is thereby a fundamental enabler. with multiple antennas at the txs but a single antenna at the rxs, we have a multiple-input single-output (miso) ic. it has been argued that non-orthogonal spectrum sharing can bring substantial benefits in terms of increased sum-rate. the paper [cit], for example, reported gains close to a factor of two."
"due to their clinical significance, the automatic segmentation and visualization of the choroid have drawn numerous research interests recently [cit] . however, the majority of the choroid segmentation methods are based on graph search [cit], which is restricted by the choice of a suitable graph-edge weight model [cit] . the inferior choice of the edge weight or the variation of oct image features would cause inaccuracy in the choroid segmentation [cit], so tedious manual inspection and correction are still required for clinical usage [cit] . the existing methods for eliminating the vessel shadows are based on the compensation of vessel-induced light attenuation [cit], but the effectiveness of this kind of a-line based method is limited to small vessels and capillaries in oct retinal imaging. the large vessel shadows still have residue on the choroid [cit] ."
"the scene camera feed [cit] . once calibrated, the average error between projected gaze and the center of each target was computed in terms of visual angle. we computed the precision as the root mean square deviation between successive gaze locations while the targets were present to measure the stability of the gaze at each target."
"spectrum sharing comes in two basic forms; orthogonal sharing and non-orthogonal sharing [cit] . orthogonal sharing means that each operator owns and operates in an independent piece of spectrum, but that they can borrow spectrum from each other on a need basis. the borrower can use the borrowed piece of spectrum exclusively."
"to perform authentication the bit values of these codes are compared using hamming distance to determine if the source and target match. hamming distance is defined as the number of bits that disagree between source and target binary codes,"
"optical coherence tomography (oct) is a high-resolution non-invasive 3d imaging modality that could precisely separate the information of the underlying choroid from the outer retina, thus has been becoming a powerful tool to understand the role of the choroid in various ocular diseases [cit] . it has been shown that the thickness of the choroid layer extracted from oct, is directly related to the incidence and severity of predominate ocular diseases, such as pathological myopia [cit], diabetic retinopathy (dr) [cit], age-related macular degeneration (amd) [cit], and glaucoma [cit] ."
"zero-forcing (zf) beamforming: zf is a linear transmit beamforming strategy that cancels out the interference to the unintended receiver. the zf beamforming vectors are [cit],"
"our first contribution is to discuss the theoretical basis of this problem and a proposed solution, provide a novel hardware mechanism to achieve the solution, and evaluate its ability to reduce accuracy of iris authentication. our second contribution is to determine detection thresholds for the amount of image defocus that can be applied before a difference in eye animations is perceived. our third contribution is a study to determine how image defocus impacts perceived eye contact, attentiveness, naturalness, comfort, and truthfulness of the conversational avatar. based on this work, it is possible to recommend to a user how to create their preferred level of security for eye tracking, and how much impact this setting will have on the perceived characteristics of their virtual avatar. more broadly, this work motivates the need to investigate the security-utility tradeoff for a wide range of xr applications and develop eye tracking configurations that prioritize security."
eye tracking utility was measured in terms of gaze accuracy during the five target viewing task. the pupil labs software was used to identify frames with circular targets. these frames were used to calibrate a gaze mapping model that predicts the 2d gaze point-of-regard within id table 1 : security and utility results for in-focus and out-of-focus eye tracking configurations. on average there was a difference of 8mm between in-focus and out-of-focus configurations. defocusing the camera caused a decrease in crr without an appreciable impact on gaze accuracy. gaze error (°) s01 s02 s03 s04 s05 s06 s07 s08 s09 s10 s11 s12 s13 s14 s15 correct recognition rate (%) s01 s02 s03 s04 s05 s06 s07 s08 s09 s10 s11 s12 s13 s14 s15 25 30 35 40 distance (mm) correct recognition rate (%)
"participants eye tracking data and images were collected from fifteen participants (8 male, 7 female) in an irb approved user study. participant demographics were 20% asian, 13% hispanic, 13% african american, 27% indian, and 27% caucasian."
"the global multi-layers segmentation module is to segment the global oct image into several layers following its anatomical characteristics. as a multi-task network, the tasks of segmenting different layers are constrained with each other, which can reduce overfitting and improve robustness. it is trained by a multi-class loss, which aims to optimize the global classification loss. this module is mainly used to obtain the global structure information of the retinal oct."
"3) evaluation metrics: we employ dice index (di), intersection-over-union (iou), average-unsigned-surfacedetection-error (ausde), accuracy (acc), and sensitivity (sen) to quantitatively evaluate the performance of the bio-net. the di and iou show the proportion of the overlap between the segmented choroid region and the ground truth (larger is better). the ausde [cit] represents the pixel-wise mismatch between the segmented choroid boundary and the ground truth (smaller is better). acc and sen represent the accuracy and sensitivity of the segmentation compared with the ground truth (larger is better)."
"note that the data throughout this paper was collected from topcon oct systems, so we did not consider to remedy the domain discrepancy caused by manufacturers in the proposed method. for using it in the scenerios that the oct systems are from different manufacturers, domain adaptation methods [cit] have been used for achieving superior segmentation and shadow elimination performance."
"where f is the focal length of the camera in mm. we estimate w world as the average width of a human iris, 11 mm [cit], measure w img within the image, and compute f ."
"to simulate the state of high iop, after taking the baseline scans in a normal sitting position, each of the volunteers was asked to take scans in upside-down position. the average iop was increased to 34.48 ± 5.35 mm hg because of the upsidedown, compared with the average iop of 15.84 ± 1.99 mm hg at the normal position. a total of 136 oct volumes were acquired (34 volunteers, 68 eyes, normal and high iop)."
"2) implementation: the shadow localization and elimination pipeline is also implemented using the pytorch library [cit] . for training the u-net in the shadow localization, we employ the adam optimizer [cit] for training. the initial learning rate is set to 0.0001. then we gradually decrease the learning rate with a momentum of 0.9. we further enhance the segmentation result of the u-net with six iterations of dilation and erosion."
"the biomarker prediction network b is trained to predict the biomarker, and the parameters are fixed after that. the biomarker prediction network outputs a vector (x i, h) for each input, where i denotes the coordinate of the a-line and h is the corresponding predicted thickness. the mean of h is the biomarker value b pred, whose ground-truth is b gt . it is trained with mean absolute error loss, denoted as l bio,reg ."
"3) inpainting/object removal: after locating the vessel shadows, we propose to use inpainting techniques, which is also referred as object removal. here the object to be removed is the vessel shadows. inpainting techniques have been extensively studied and applied in various computer vision and pattern recognition related fields (see [cit] and the references therein). early inpainting techniques primarily filled the targeted area with information from similar or closest image parts, such as exemplar-based inpainting (ebi) [cit], or used higher-order partial differential equations to propagate the information of surrounding areas into the targeted area, such as coherence transport inpainting (cti) [cit] ."
"method the study structure and apparatus was identical to study 1, except participants rated each animation. the prompt provided to participants was: \"in each trial you will be shown a video of an animated virtual avatar for 12 seconds. only the eyes are animated. imagine you are having a conversation with the avatar. your task is to respond to several prompts about the animation after each trial.\". based on prior work we evaluate each interaction in terms of truthfulness [cit], naturalness [cit], attentiveness [cit], comfort with the avatar [cit], and eye contact [cit] . after watching each animation the participant used a mouse to respond to the following prompts, using a five point likert scale from 'strongly disagree' to 'strongly agree'(1-5):"
"maximum sum-rate using linear beamforming: here, we maximize (4)- (5) with respect to the beamforming vectors, assuming equal power allocation over the subchannels. that is, for each subchannel, we solve"
"the user was asked to look directly at the eye tracking camera for five seconds, simulating a \"stop-and-stare\" interface for iris authentication [cit], prior to target viewing and directly afterwards. only images from this part of the data collection were used for authentication, ensuring that the pupil and iris are on-axis with the camera. on-axis images increase the reliability of iris segmentation and matching [cit] . each user logged around 300 frames during this procedure."
"to address these two problems, and inspired by the recent success of deep learning in medical image processing [cit], we propose an automatic segmentation and visualization method for the choroid in oct via knowledge infused deep learning. the main contributions of our work include:"
"social virtual avatars have eye animation with the goal of increasing social presence and immersion. a large body of work has established that the animation of eye movements impacts viewer perceptions of avatar attributes, such as truthfulness and attentiveness [cit] . the goal of this section is to determine how the noise introduced by secure eye tracking impacts the perception of animated virtual avatars."
"(2) as shown in fig. 5, the rpe layer has the highest oct light reflectance and best shadow contrast compared with other avascular layers including outer nuclear layer (onl) and photoreceptor layer (prl)."
"due to the absent of the ground-truth choroidal vasculature, the quantitative comparison of the inpainting methods can not directly implemented. thus we created artificial retinal vessel mask with the vessel widths slightly wider than the real shadows. the artificial mask combining with the repaired choroid images were used as the input to evaluate the inpainting algorithms. the widely-used image similarity measures including structure similarity index (ssim), peak signal to noise ratio (psnr), and mean squared error (mse) [cit] are employed as quantitative metrics. we used the masked images as the baseline."
"primary angle-closure glaucoma (pacg) is prevailing not only in east asia but also in overseas chinese and eskimos [cit] . the patients with pacg were found to have higher iop and thicker choroids than normal controls [cit] . previous studies shown the changes of the choroid thickness and blood flow might be associated with the pacg [cit], but the fig. 9 . the visual examples of the shadow elimination. from left to right: the original en face choroid image, the shadow mask, and the shadow elimination results using the ac [cit], our previous work using the cti [cit], and the proposed method in this work, respectively. the second and third rows are the zoom-in views inside the blue and yellow boxes in the first row, respectively. the last row are the corresponding vessel maps. initial mechanism underlying angle closure has not been fully understood. here we applied the proposed method in a clinical prospective study, which quantitatively detected the changes of the choroidal in response to iop elevation [cit] ."
"here, we give two upper bounds on the miso ic sum-capacity. the first bound is a trivial bound, where interference is ignored. the second bound is more sophisticated and hence tighter."
"different from the previous shadow elimination methods that could not eliminate the shadows from large vessel [cit], we propose a novel method that is able to remove the shadow without the limitation in vessel caliber. it firstly locates the vessel shadows then uses image inpainting techniques to repair the shadow-conterminated areas. as shown in fig. 4, we segment the retinal vessel shadows from the en face rpe image with the u-net [cit] . the generated shadow mask could be used to locate the shadows in a oct volume. then the shadow mask in combination with the en face choroid image are fed into the shadow elimination module, namely the deshadow-net, to get a shadow-free choroid image."
"because of the networked nature of social platforms and the use of cloud-based rendering techniques for vr [cit], it is expected that xr devices will follow an 'always on and connected' model. streaming eye tracking data makes it susceptible to attacks. most critically, the iris image of the user is vulnerable. the iris image is a gold standard biometric that is used in high security applications, such as border customs [cit], and is recognized as such by headset manufacturers [cit] showed that typical eye tracker eye images, if stolen, could be used to biometrically identify as a user. they presented a proof of concept solution that blurred the eye image to remove the high frequency patterns that form each person's unique iris signature. they evaluated this solution for a target viewing task. however, for such a solution to be impactful, it is also necessary to determine the consequences of a security mechanism for specific applications. we focus on the application of eye tracking to animate the eyes of virtual avatars, as eyes are critical to realism and naturalness of avatars, gaze is a crucial social cue in conversations, and inadvertently altering a user's gaze may result in unintended changes in how he or she is perceived."
"regularizer by a set of manually labeled choroid images to learn the distribution of choroidal thickness. since the upper boundary is easy to detect, the thickness regularizer will be helpful to segment the lower boundary."
"iris patterns present in eye tracking data streams can serve as a password, and are continuously streamed when an eye tracker is in use. this data stream is subject to a man-in-the-middle attack if images are sent over a network. in configurations where images are not streamed over a network, they are still subject to attacks when data is transferred at the hardware level."
"however, the application of the choroidal biomarkers in clinic is still quite limited, which may be attributed to two primary reasons. (1) the lower boundary of the choroid (choroid-sclera interface, csi) in oct is fuzzy, which makes the automatic segmentation difficult and inaccurate. (2) the visualization of the choroid is contaminated by the vessel shadows from the superficial layers of the outer retina. figure 1 is a demonstration of the csi and the retinal vessels and their projection on the underlying layers. the position above the orange dashed line shows the fuzzy csi in a bscan. the anisotropy of the red blood cells inside the vessels cause strong forward attenuation of the probe light, thus bring shadow-like dark tails to the underneath layers extending to the choroid and the sclera (white arrow). the center part of fig. 1 is a segmented oct volume, which could further be used to generate the en face images of each layer in the right side. the ganglion cell layer (gcl) possesses the retinal vessels (black arrows) and has high light reflectance (green box). the depth-projected vessel shadows (black arrows) turn dark on the vessel-absent retinal pigment epithelium (rpe) layer (pink box) and the choroid layer (orange box). it is evident that the shadows bring difficulties to the extraction of the choroidal vasculature."
"limitations the stimuli used for our perceptual evaluation has limitations. particularly, our evaluation does not consider the impact of defocus on eye movement characteristics such as the blinks, the dynamics of saccades with large amplitudes, or estimated pupil diameter. these characteristics play an important role in complex social interactions and are more prominent the closer the user is to the avatar. the stimuli also did not include head or mouth movements. the defocus solution presented in this paper leveraged the telescoping arm of a popular eye tracker. more generally, a defocus solution applies to configurations where the eye camera is readily accessible, though future work might investigate clip on optics similar to pittaluga and koppal [cit] . our findings with respect to the fall in correct recognition rate are based on the daugman method of iris recognition. if the iris recognition module were to be replaced with upcoming deep network based approaches, such as one proposed by proenca and neves [cit], the fall in correct recognition rate as a function of hardware parameters might need to be re-assessed. our work provides a foundation for developing an automated system that continually optimizes the security-utility trade-off even as new methods of eye tracking and iris recognition are invented."
"by using the duality property of the mimo bc and mimo multiple access channel (mac) [cit], we can include power control across the subchannels. the trick is to write the multi-channel system as an equivalent mimo system. however, for finding the capacity for this case, numerical methods are needed. average rate [bpcu and user] no sharing, (19), no water-filling no sharing, (19), water-filling sharing, (7), no water-filling sharing, (7), water-filling fig. 2 . achievable sum-rate bounds using zf beamforming, with and without water-filling. fig. 3 . achievable sum-rate bounds using linear beamforming and equal power allocation across the subchannels."
the inner sum of (4) is the ergodic rate for the ith user while the outer sum yields the average rate of the two users.
"discussion using hd auth we computed crr for each participant, comparing every eye image with every other eye image. for the infocus configuration we found on average 78.6% of frames were a match. this indicates that a login procedure that matches only one input image to another may not be robust enough for a consistent user experience. using a larger hd auth would create a smoother process, but compromise security. instead, collecting a small set of ideal onaxis images and computing the average hamming distance from the reference may be a more dependable approach."
"where p i,k is the power spent on user i on subchannel k. as performance measure we use the average rate over users and subchannels. maximizing the average rate is the same as maximizing the sum-rate. the motivation for this choice of performance measure is that it gives the highest possible throughput of the operators."
"method we created a same-different task where each trial consists of a stimulus and a reference presented simultaneously, and participants are asked to indicate if they are the same or different [cit] . the miss rate is computed as the proportion of 'same' responses. the point of subjective equality (pse) corresponds to the 50% miss rate, i.e., the stimulus level at which participants are as likely to detect as they are to miss the difference between the stimulus and the reference. in other words, the pse clarifies when a viewer can discriminate the presence of the stimuli at the same rate as chance. the detection threshold (dt) provides an upper bound on the amount of defocus that can be applied before a difference is perceived by a viewer."
"we recruited 34 healthy volunteers with the ages ranging from 18 to 30 years old, with no previous history of iop exceeding 21 mm hg. the participants were volunteers recruited mainly from the zhongshan ophthalmic center at sun yat-sen university medical school, and nearby communities in guangzhou, china. the study was approved by the ethical review committee of the zhongshan ophthalmic center and was conducted in accordance with the declaration of helsinki for research involving human subjects."
"we have analyzed the potential gains of non-orthogonal spectrum sharing by using first principles based information theoretic models. in conclusion, the gain of non-orthogonal sharing appears to be very limited. the intuition behind this result is that without sharing, each operator has a bandwidth of b and two spatial degrees of freedom (dof) per bandwidth unit. by contrast, with sharing each operator has a bandwidth of 2b and two spatial dof per bandwidth unit of which one can be used for transmission and the other must be used to suppress interference."
"in this work, we employ the gan inpainting method [cit] for the shadow elimination task. we have compared it with our previous implementation [cit] using the cti [cit] as shown in fig. 9 . it shows both of the inpainting methods could completely eliminated the shadows and fill the shadow areas with neutral extensions of surrounding contents. we also could notice the cti create some locate artefacts. here we further compare the performance of these inpainting method quantitatively. we also include the ebi [cit] in the comparison."
"the potential gain might be larger if, in the non-sharing scenario, highly suboptimal transmission schemes are used (not fully utilizing all spatial dof); however, in this case, better utilization of spatial dof per operator appears to be more attractive than nonorthogonal sharing between operators. taking into account the extra csi needed for non-orthogonal sharing, it is questionable whether non-orthogonal sharing can give any gain in practice."
"iris authentication for our experiment, iris segmentation was performed using an open source implementation of irisseg [cit] . our authentication procedure applies a bank of 1d log-gabor filters to the resulting iris pattern to generate a binary code that captures the identifying features of the iris pattern [cit] ."
"stimuli generation naturalistic gaze data was recorded with the pupil labs pro glasses-based eye tracker in a conversational scenario. we selected an english as a second language instructional video from youtube 2 . the details of the video are shown in table 2 . the video had an instructor speak conversational sentences in english for the student to pause and repeat back to them. the topic of the conversation was a technique for learning english grammar. one of the authors watched the video, and acted out the part of the student by repeating sentences back as appropriate while being eye tracked. we extracted six 12 second segments from different parts of this dataset, resulting in six eye animations. gaze directions from these segments were transferred on to a virtual avatar."
"as shown in fig. 3, the bio-net is a cascade of biomarker prediction network, global multi-layers segmentation module, and local choroid segmentation module. firstly, the biomarker prediction network is trained to predict the biomarker, and its parameters are fixed after that. then, we follows the anatomical hierarchy of the retinal oct and employ the global multi-layers segmentation module to segment the oct image into 12 layers. finally, the global multi-layered result and the original oct image are concatenated and fed into the local choroid segmentation module to segment the choroid region, where the biomarker information is infused and applied as a regularization."
"in parallel, a chromatin extraction step was performed using approximately 5-15 million cells (10-20% of the total number of cells used per experiment) according to 85 ."
"60-70 ug of protein per fraction were reduced with tcep, alkylated with mmts, digested with trypsin and labelled with isobaric tagging reagents as previously described 85 . for our hyperlopit samples, each tag from a tmt10plex kit (thermo fisher scientific) was split in half (essentially making the labelling scheme a 20plex) and used to label all the membrane fractions (some were pooled to ensure adequate protein amounts) as well as the cytosol-and chromatin-enriched samples. three tmt10plexes were used to label three biological replicates."
"to the best of our knowledge, there have been only few attempts in computational argumentation that go deeper than analyzing argument structures (e.g., [cit] ) mentioned above). [cit] model argument strength in persuasive essays using a manually annotated corpus of 1,000 documents labeled with a 1-4 score value."
"aiming to create a simpler alternative to hyperlopit, we developed a second mass spectrometrybased technique for the study of protein subcellular localisation which we named localisation of organelle proteins by isotope tagging after differential ultracentrifugation (lopit-dc, figure 1 ). for the development of this method we combined the strengths of our hyperlopit protocol with elements of other subcellular fractionation methods that employ differential centrifugation 59, 67, 68 ."
"the authors of the paper have selected and assessed methods and tools related to the bpmn modeling and simulation based on resource properties. it can be seen from the presented evaluations that there does not exist tool or method that allows one to model resources, provides the ability to specify particular settings and has a graphical notation."
"finally, we also explored the distribution of protein isoforms in our data. as seen in figure 6d, we could identify six examples where two different isoforms of a protein were present in both of our datasets. in all of these cases, each isoform is mapped to the same subcellular niche in the lopit-dc and hyperlopit data. for example, both q96ae4 isoforms (yellow) were assigned to the same unique subcellular location, the nucleus, in our two datasets. indeed, this protein is a dna-binding transcription factor which regulates the expression of the c-myc gene 78 and is listed as a nuclear resident in the uniprot and cell atlas databases."
"the results from a svm classification on the hyperlopit dataset showed that the majority of unlabelled proteins were assigned to the nucleus (765 proteins), mitochondrion (426 proteins) and pm (267 proteins). a smaller number of proteins was assigned to the ga (7 proteins), peroxisome (9 proteins), proteasome (23 proteins) and ribosomes (23 proteins for the ribosome 40s and 28 proteins for the ribosome 60s), akin to the lopit-dc classification results. we found that approximately 42% of the proteins in our hyperlopit data were classified as part of a single subcellular niche (table s2, figure 2c )."
"recent years can be seen as a dawn of computational argumentation -an emerging sub-field of nlp in which natural language arguments and argumentation are modeled, searched, analyzed, generated, and evaluated. the main focus has been paid to analyzing argument structures, under the umbrella entitled argumentation mining."
"bpmn -this is a graphical notation intended to model business processes. this notation has four graphics categories. the first category consists of flow objects: activities, events, and gateways. the second category consists of connection objects: sequence flow, message flow, and association. the third category -swimlanes: a pool and a line. the last group is artefacts: data objects, groups, and annotations. bpmn allows defining business processes graphically. it also allows specifying roles using a pool and a line. these objects are used to specify resources graphically in some way. however, bpmn has the limitation: by using a pool and a line one cannot specify the resource quantity or it is difficult to represent many resources that are used in one task of business process. if one task uses more than one resource, then the pool and lines should be represented for all resources. apart from that, some information resources can be presented using data objects that are input or output of a task."
"the final supernatant was precipitated with five volumes of cold acetone overnight at -20 °c. the obtained precipitated pellet and membrane pellets were resolubilized in 8 m urea, 0.15% sds and 50 mm hepes ph 8.5. protein concentration was measured using the bca protein assay according to the manufacturer's instructions."
"business process models and simulation resources are used when the system performs the entity. resources include fixed equipment, mobile equipment, containers, personnel, supplies used and any other technical and material supplies that are needed to process an essence. the resources are divided into human and technical ones. they are also divided into quantitative, schedule availability, size, price, and cost management. resource modeling describes employed or utilized resources in the business process. the resource modeling describes the usage of resources."
"lopit-dc was applied to u-2 os cells. this cell line was chosen as it is a well characterised model that has been used for a variety of research purposes. more importantly, a large amount of immunofluorescence-based protein subcellular localisation data obtained using this cell line is publicly available as part of the cell atlas project 72 . therefore, this database is an excellent source of information for the validation of our mass spectrometry-based spatial proteomics observations and has served as such in the past (more details in next section)."
"the u-2 os human osteosarcoma cell line was a generous gift from professor emma lundberg (scilifelab stockholm and school of biotechnology, kth). the cells were grown at 37 °c and 5% co2 in mccoy's 5a medium (sigma) supplemented with sodium bicarbonate, 10% foetal bovine serum (biosera) and 1% glutamax tm (life technologies), without antibiotics."
"the main difference between hyperlopit, where the subcellular fractionation part of the protocol is based on density gradient ultracentrifugation, and lopit-dc is that fractionation in the latter relies on subsequent ultracentrifugation steps. as a modification of our hyperlopit protocol, lopit-dc utilises sequential differential centrifugation steps to fractionate the cell lysate into 10 fractions (table 1) . some of the centrifugation speeds in our lopit-dc workflow are similar to those described in 67 with the addition of 4 extra steps. in contrast to the dynamic organellar maps pipeline, lopit-dc is an all-in-one method meaning that all subcellular niches are analysed in a single preparation, thus avoiding any variation arising due to membrane damage and protein leakage. 70, novelty detection using semi-supervised learning 51 and transfer learning 71 . in addition, the prolocdata package 44 provides a variety of readily available, annotated and preformatted datasets generated using lopit or hyperlopit over the years and originating from different species. these open-source, open-development r packages constitute our full data analysis pipeline. they are interactive, user friendly and are accompanied by working examples, full documentation, tutorials and videos allowing users to follow this analysis workflow step-by-step 69 ."
"what makes a good argument? despite the recent achievements in computational argumentation, such as identifying argument components [cit], finding evidence for claims [cit], or predicting argument structure [cit], this question remains too hard to be answered."
"the qsep function which is freely available as part of the proloc package 44 was used to quantify the resolution of the lopit-dc and hyperlopit datasets (for more details see 73 ). qsep calculates cluster separation by comparing the average euclidean distances within and between subcellular clusters. these distances always refer to one specific organelle marker cluster and the distances within clusters are usually smaller than the ones between clusters, except in cases of overlapping subcellular niches. to enable reliable comparison of such distances within a single experiment but also across different studies qsep further divides each value by the reference within-cluster average distance, as follows:"
"we do not find an approach that could visualize all resource parameters. despite that, there is not an approach that takes into account and could model all aspects of the resource. this approach could be implemented in business process simulation tools using a business process simulation metamodel of bpsim framework. this approach also takes into account the resource concurrency that is important when resources are limited."
"lately, advances in large-scale proteomics technologies [cit] have led to spatial proteomics studies which have provided useful insights regarding organelle composition, dynamics and function in health and disease and across a range of different species and cell types 4, 28 . elaborate subcellular fractionation protocols coupled to differential or density-gradient centrifugation and downstream mass spectrometry-based proteomics methods have been the gold standard for protein subcellular localization analysis for many years and hundreds of subcellular proteomics studies have been published, aiming to characterise all major organelles, macromolecular structures and multiprotein complexes in eukaryotic cells [cit] 16, [cit] . methods for subcellular fractionation alternative to centrifugation have also been developed and applied to subcellular proteomics research with variable success 1, 34, 35, 41 ."
"samples for hyperlopit were treated with nuclease and then fractionated using an iodixanol density gradient as described in 85 and 72 . briefly, a cell lysate from approximately 280 million cells per average experiment was first separated into a cytosol-enriched and a crude membrane fraction using 6% and 25% (w/v) iodixanol-containing solutions and centrifugation at 100,000 x g, 90 min, 4 °c. the supernatant was stored and the membrane fractions situated at the interface of the iodixanol layers collected and centrifuged to get rid of any residual cytosolic contamination. the samples were then resuspended in 25% (w/v) iodixanol, underlaid beneath a linear gradient of 8%, 12%, 16% and 18% (w/v) iodixanol solutions and fractionated by centrifugation at 100,000 x g, 8 h, 4 °c. after ultracentrifugation approximately 20-22 fractions were collected, pelleted several times at 100,000 x g, 1 h, 4 o c to wash away the iodixanol and stored at -80 °c."
"we experiment with two machine learning algorithms on two tasks using the two new benchmark corpora (ukpconvargstrict and ukpconvargrank). in both tasks, we perform 32-fold cross-topic cross-validation (one topic is test data, remaining 31 topics are training ones). this rather table 2 : properties of resulting gold data."
"within the hyperlopit dataset the smallest normalised pairwise distances are those between the two ribosomal subunits and the nucleus as well as between the cytosol and the proteasome ( figure 3a, right). in the case of the ribosome 40s/nucleus and ribosome 60s/nucleus pairs this result is due to differences in cluster size, as also observed in the case of the lopit-dc data. concerning the cytosol/proteasome pair, the low distance value reflects the fact that these two clusters partially overlap in principal components 1 and 2; another example of small normalised distances due to overlapping clusters along these dimensions is the case of the pm/ga pair. on the contrary, the largest normalised distances in the hyperlopit dataset correspond to various organelles paired with the cytosol. this demonstrates that the cytosol exhibits the best separation from the rest of the organelle clusters in our hyperlopit data."
"in hyperlopit, an additional nuclear chromatin preparation aids in separating the nucleus and chromatin clusters. to investigate if the same is true for our differential centrifugation-based workflow, we prepared and added this chromatin-enriched fraction to our lopit-dc analysis. as can be observed in figure s7, this addition did not significantly improve the resolution of our u-2 os lopit-dc dataset but in turn led to a slight decrease in overall subcellular resolution ( figure s7b ), therefore we excluded this chromatin-enriched fraction from our downstream data analysis (see figures s7b and s7c for details)."
all mass spectrometry runs were performed on an orbitrap fusion™ lumos™ tribrid™ instrument coupled to a dionex ultimate™ 3000 rslcnano system (thermo fisher scientific) with parameters from 85 .
"raw files were processed with proteome discoverer v1.4 (thermo fisher scientific) using the mascot server v2.3.02 (matrix science). the swissprot sequence database for homo sapiens (canonical and isoform, 42,118 sequences, downloaded on 04/11/2016) was used along with common contaminants from the common repository of adventitious proteins (crap) v1.0 (48 sequences, adapted from the global proteome machine repository). precursor and fragment mass tolerances were set to 10 ppm and 0.6 da, respectively. trypsin was set as the enzyme of choice and a maximum of 2 missed cleavages were allowed. static modifications were: methylthio (c), tmt6plex (n-term) and tmt6plex (k). dynamic modifications were: oxidation (m) and deamidated (nq)."
"our three u-2 os hyperlopit experiments required on average 280x10 6 cells each. this way we consistently obtained at least 70 µg of protein in each fraction with the exception of the first 5-7 fractions which were pooled for further analysis ( figure s3b ). while the lopit-dc workflow focuses on simplicity and speed, our hyperlopit approach aims at achieving the maximum overall resolution possible leading us to pursue a different tmt labelling strategy. in more detail, we included all density gradient fractions, together with the cytosol-and chromatin-enriched samples, in our hyperlopit analysis, ending up with 20 tmt channels per replicate and 60 tmt channels for our merged dataset as opposed to our 10-channel lopit-dc dataset and previous hyperlopit reports 56 ( figure 1, supplementary quantitation table). due to the large number of samples analysed during our hyperlopit experiments the amount of missing values which arose throughout the analysis was higher compared to the lopit-dc dataset, leading to the final combined hyperlopit dataset being smaller than the combined lopit-dc one; following quantitative lc-sps-ms 3 analysis of all three of our hyperlopit replicates we identified 9558 protein groups which were reduced to 4883 after filtering and concatenating replicates (table s1 ). three of the 60 tmt channels present in our final hyperlopit dataset possessed extremely low ion intensity profiles so they were excluded from downstream data analysis to minimise background noise to the data."
"moreover, a large number of publicly-accessible organelle databases and web-based resources have also been developed, some of which link subcellular proteomics data to functional datasets as well as disease relevance and animal model information; two important such resources are uniprot 45 and the human protein atlas 46 and some others are reviewed in 37 ."
"the cytosol-enriched supernatant was precipitated with five volumes of cold acetone overnight at -20 °c. the obtained precipitated pellet and membrane pellets were resolubilised in 8 m urea, 0.2% sds and 50 mm hepes ph 8.5. protein concentration was measured using the bca protein assay kit (thermo fisher scientific) according to the manufacturer's instructions."
bpmn language is a graphical modeling notation widely used for modeling business processes in organizations [cit] . this notation is focused on process sequences [cit] . a particular action or set of actions with certain resources are displayed graphically using an item called swimlanes in the bpmn notation. this element presents the participants of the process. a participant may be a specific business entity or a general business rule. resources are not necessarily allocated during the design in this notation. one of the major drawbacks of this notation is the lack of ability to model the resources [cit] .
"according to results (table i), it is safe to state that there is not a suitable way to model resources and adequate graphical representation. therefore, this article suggests a method, which attempts to collect advantages from each analyzed method and adapt these advantages to the reviewed approaches."
"as shown in figure 2b, 12 major subcellular niches were successfully resolved during our hyperlopit experiments: the cytosol, nucleus, chromatin, mitochondrion, peroxisome, lysosome, er, pm, ga, ribosomal subunit 40s, ribosomal subunit 60s and proteasome. unlike our lopit-dc observations, our hyperlopit experiments accomplished separation between the two ribosomal subunits as well as the nucleus and nuclear chromatin (figures 2b, s2b). furthermore, similarly to the lopit-dc data, the organelle classes present in the hyperlopit dataset seem to be arranged in four larger groups: the first group contains the cytosol and proteasome, the second the nucleus, nuclear chromatin and ribosomal subunits, the third the mitochondrion and peroxisome and the fourth the membranous organelles of the secretory pathway (lysosome, pm, er, ga). notably, while in dimensions 1 and 2 the ga and peroxisome seem to partially overlap with the pm and mitochondrion, respectively, these organelles become entirely separated from each other along principal components 1 and 8. our three hyperlopit replicate experiments also exhibit excellent reproducibility and, importantly, complementary subcellular resolution ( figure s1b, s2b, s3b, s4)."
"all 16,927 argument pairs were annotated by five workers each (85k assignments in total). we also allowed workers to express their own standpoint toward the topics. while 66% of workers had no standpoint, 14% had the opposite view and 20% the same view. this indicates that there should be no systematic bias in the data. [cit] plus two weeks of pilot studies. in total, about 3,900 workers participated. total costs including pilot studies and bonus payments were 5,520 usd."
"we have previously shown that transfer learning is particularly useful for organelle classes which are not optimally resolved in the primary experimental data. given that and in order to explore whether we can improve protein subcellular localisation assignment and thus gain new information by making use of the unique features and strengths of each of our two subcellular fractionation methods, we next proceeded to apply transfer learning on the lopit-dc and hyperlopit u-2 os cell datasets. since our prior analysis, described above, indicated that hyperlopit achieved higher overall resolution than lopit-dc during our experiments and aiming to maximise subcellular resolution after classification, we used the hyperlopit data as the primary information source and the lopit-dc dataset as the auxiliary data. figure 4a shows the distribution of the class-specific weights selected over 100 test partitions of the transfer learning algorithm applied to the two datasets. as evident in this figure, the weight distributions corresponding to each dataset closely reflect the resolution achieved by either hyperlopit or lopit-dc during our experiments. in more detail, the distribution of the best identified weights is skewed towards 1 for just under half of subcellular compartments suggesting that the proportion of neighbours to use during protein subcellular location classification to these organelles should be predominantly primary and indicating their better resolution in hyperlopit. however, this is not true for all subcellular niches: the cytosol was assigned best weight of 0 in 78% of runs, signifying that auxiliary data should be used to classify to it. this observation reflects the overlapping distributions exhibited by the cytosol and proteasome in the hyperlopit dataset and, in turn, their superb separation from each other and all other organelles in the lopit-dc data. furthermore, half of the subcellular compartments were assigned weights of 0.5 indicating that each dataset should contribute equally to the classification of those subcellular organelles. finally, the macro f1 scores obtained after weight optimisation and classification of our unannotated proteins demonstrate that including the auxiliary data in the classification leads to an increase in classifier prediction relative to the generalisation accuracy acquired using the hyperlopit dataset alone (figure 4b ). importantly, our findings highlight the merit of integrating our two spatial proteomics methods in order to achieve optimal classification of proteins to organelles."
"since this task is a binary classification and the classes are equally distributed (see table 2 ), we report accuracy and average the final score over folds [cit] ."
"without any modifications, we use the same svm and features as described in section 4.1. regarding the blstm, we only replace the output layer with a linear activation function and optimize mean absolute error loss. table 4 shows that svm outperforms blstm. all correlations are highly statistically significant."
"the simulation is not sufficiently developed for the usage, as there is an important step in the business process management approach. the main idea of the simulation is that the model has to be executed several times repeatedly. this process is done to obtain more than one value, resulting in the range of values [cit] . usually the beginning of the simulation starts with filling business process model up with data. once it is filled with data, the calculations would start. the simulation is rarely used in business decision-making; however, it is often applied to analyze the initial business process model. while many organizations try to use the simulation to analyze the changes in the business, only a few organizations use the simulation effectively. this situation occurs due to the fact that the development of simulation and supervision is a timeconsuming process. often the simulation results may not correspond to the reality. moreover, the organizations have a need to deal with issues that are relevant to the present time rather than abstract future problems [cit] ."
the business process models are often excluded from the simulation in the current modeling and simulation tools. models and simulations are often being developed separately. most of the simulation tools provide only the number of resources used. it is also common that organizations model resources incorrectly in the business process models and simulations. the problematic areas regarding a couple of the main resource modeling cases are identified below [cit] :
"after exploring the similarities and differences between our lopit-dc and hyperlopit datasets regarding subcellular resolution at the marker level, we expanded our characterisation to the level of protein subcellular localisation prediction. as mentioned above, in order to assign the unlabelled proteins in our data to a unique subcellular location we performed svm-based supervised machine learning using 10 organelle classes for the lopit-dc dataset and 12 for the hyperlopit data. as a first step in assessing classifier performance we examined the macro f1 scores 69 (harmonic mean of precision and recall) obtained after svm parameter optimisation for each of our datasets. macro f1 score values range from 0 to 1 and a high score suggests that the marker proteins in the test dataset are consistently assigned to the correct subcellular location by the algorithm 69 . as shown in figure 3c, the average f1 scores acquired during svm parameter optimisation using our core marker set were optimal for both of the datasets as both values were very close to 1. at the level of individual organelle scores the classifier performed best for the hyperlopit dataset ( figure s5b ) and slightly worse for the lopit-dc data, specifically in the cases of the lysosome and pm ( figure s5a ). figure 2c shows the u-2 os lopit-dc (left) and hyperlopit (right) datasets after svm-based protein subcellular location classification followed by 5% fdr filtering. a larger number of proteins was assigned to a unique location in the lopit-dc data compared to our hyperlopit dataset but the proportion of classified proteins was slightly higher in the hyperlopit dataset (42%) as opposed to the lopit-dc data (35%) (table s2, figure 2 ). we proceeded to a comparison between the svm predictions obtained for each dataset in the form of contingency matrices and heatmaps, aiming to visualise and explore the level of agreement achieved by our two distinct workflows and the potential emergence of method-specific biases towards particular organelles. strikingly, our two datasets exhibit an outstanding level of agreement as the majority of the proteins which were assigned to a unique subcellular compartment in one dataset were classified to the same location in the second dataset (figure 3d) . furthermore, the vast majority of the \"chromatin\" and \"nucleus\" as well as the \"ribosome 40s\" and \"ribosome 60s\" hyperlopit classifications were assigned to the \"nucleus/chromatin\" and \"ribosome\" niches by lopit-dc, respectively. importantly, the very few mismatches which can be identified between the lopit-dc and hyperlopit organelle assignments are either false positives resulting from the 5% fdr filtering process or proteins that could be labelled as residents of either of the predicted locations according to published evidence. furthermore, it is apparent from figure 3d that the majority of classification disparities between the lopit-dc and hyperlopit data stem from cases where a protein was assigned to a unique subcellular niche in one dataset but remained unlabelled in the second dataset. in this case, the highest number of proteins which were labelled as \"unknown\" in the lopit-dc dataset was assigned to the nucleus in the hyperlopit dataset and vice versa. finally, the heatmap presented in figure 3d is color-coded according to the percentage of intersection between the lopit-dc and hyperlopit svm-based organelle assignments; the intersection in this case is calculated by dividing the number of matching lopit-dc and hyperlopit classifications by the union of the lopit-dc and hyperlopit assignments for that same organelle. additional comparisons such as the ones including missing proteins or comparing the results of svm-based protein subcellular localisation classification using 12 instead of 10 organelle classes for the lopit-dc dataset are presented in figure s8 ."
"memory (blstm) neural network for end-to-end processing. 9 the input layer relies on pre-trained word embeddings, in particular glove [cit] trained on 840b tokens from common crawl; 10 the embedding weights are further updated during training. the core of the model consists of two bi-directional lstm networks with 64 output neurons each. their output is then concatenated into a single drop-out layer and passed to the final sigmoid layer for binary predictions. we train the network with adam optimizer [cit] using binary crossentropy loss function and regularize by early stopping (5 training epochs) and high drop-out rate (0.5) in the dropout layer. for both models, each training/test instance simply concatenates a1 and a2 from the argument pair."
"we also plotted several large protein complexes upon the pca plots of the lopit-dc and hyperlopit data with similar results. as shown in figure 6b, the majority of complexes we examined exhibit identical distributions in our two datasets. for example, the sumo-activating enzyme complex and cop9 signalosome are both located within the cytosolic cluster in both the lopit-dc and hyperlopit data. similarly, the integrator complex and ssu processome are positioned on top of the nucleus, the nadh dehydrogenase complex and mitochondrial ribosomes are situated within the mitochondrial cluster and the signal peptidase complex is found upon the er cluster in both of our datasets."
"since the main goal of argumentation is persuasion [cit] we take a pragmatic perspective on qualitative properties of argumentation and investigate a new high-level task. we asked whether we could quantify and predict how convincing an argument is. if we take argument 1 from figure 1, assigning a single \"convincingness score\" is highly subjective, given the lack of context, reader's prejudice, beliefs, etc. however, when comparing both arguments from the same example, one can decide that a1 is probably more convincing than a2, because it uses at least some statistics, addresses the health factor, and a2 is just harsh and attacks. 1 we adapt pairwise comparison as our backbone approach."
"although the \"traditional\" svm with rich linguistic features outperforms blstm in both tasks, there are other aspects to be considered. first, the employed features require heavy languagespecific preprocessing machinery (lemmatizer, pos tagger, parser, ner, sentiment analyzer). by contrast, blstm only requires pre-trained embedding vectors, while delivering comparable results. second, we only experimented with vanilla lstms. recent developments of deep neural networks (especially attention mechanisms or gridlstms) open up many future possibilities to gain performance in this end-to-end task."
"here, we introduce localisation of organelle proteins by isotope tagging after differential ultracentrifugation (lopit-dc), a novel spatial proteomics pipeline based on differential centrifugation which, unlike previous methods, allows for cell-wide sampling in a single experiment. this workflow requires less starting material than hyperlopit and is a simpler and quicker protocol. we compare the protein subcellular localisation maps produced by the two methods using the u-2 os cell line and also utilise qsep, a recently developed, freely available tool which aims to objectively and robustly quantify subcellular resolution in spatial proteomics data. importantly, we evaluate the impact of employing a differential centrifugation-based workflow on global, experiment-wide resolution and compare the results side by side with the equivalent hyperlopit output, detailing the strengths and weaknesses of each method. using both approaches, we highlight suborganellar resolution and protein isoform-specific subcellular niches as well as the locations of large protein complexes and proteins involved in signalling pathways which play important roles in cancer and metabolism. we also showcase an extensive analysis of the multilocalising proteomes identified via both methods. in summary, we present the most comprehensive mass spectrometry-based spatial proteomics map of a human cell line to date and deliver a flexible set of subcellular proteomics protocols from sample preparation through to data analysis."
"principal component analysis (pca) revealed underlying data structure and the quality and identity of these clusters were further explored by overlaying a collection of manually curated organelle markers on this dataset. as figures 2a, 3a and 3b show, lopit-dc offers superb resolution concerning most major subcellular niches. in more detail, our lopit-dc experiments were able to resolve the following 10 major organelle clusters: cytosol, nucleus/chromatin, mitochondrion, peroxisome, lysosome, endoplasmic reticulum (er), plasma membrane (pm), golgi apparatus (ga), ribosomes and proteasome. in the merged dataset the chromatin is only partially resolved from the non-chromatin nuclear compartment and the two ribosomal subunits are not separated (figure 2d, figure s6 ). moreover, the various organelle clusters seem to be organised into three larger groups separated from each other by greater distances: the first group contains the membranous organelles excluding the nucleus, the second group includes the nucleus/chromatin and ribosomes and the third group consists of the cytosolic cluster and proteasome. importantly, subcellular niches that seem to overlap in principal components 1 and 2 are separated in other dimensions. for example, the ga and pm exhibit overlapping distributions in pcs 1 and 2 but are separated along dimensions 1 and 4. similarly, the nucleus/chromatin, ribosome and proteasome clusters seem to overlap in dimensions 1 and 2 but these structures are separated from each other along principal components 1 and 3. lopit-dc also offers good reproducibility between replicates based on protein yield per fraction ( figure s3a ) and the fact that all three of our u-2 os lopit-dc experiments exhibit similar subcellular resolution ( figure s1a, s2a, s4)."
"if we take the hyperlopit dataset, visualise it by pca and then annotate it with the localisations determined by the lopit-dc method (figure 2d, right) we see that these localisations (as highlighted by coloured points) form similar clusters to what we observe in the original hyperlopit plot (figure 2c, right). the main difference is that the \"nucleus\" and \"chromatin\" hyperlopit classes from the original hyperlopit experiment now correspond to one \"nucleus/chromatin\" class and, similarly, the \"ribosome 40s\" and \"ribosome 60s\" classes correspond to a single \"ribosome\" group as dictated by the 10-class lopit-dc classifications. similarly, if we take the original lopit-dc dataset, visualise it by pca and annotate it with the localisations determined by the hyperlopit data (figure 2d, left) we find that the localisations form similar clusters to those found in the original lopit-dc dataset (figure 2c, left) . interestingly, the distributions of the two hyperlopit subnuclear class assignments in this plot indicate that our lopit-dc experiments achieved at least partial separation between the chromatin and nucleus clusters without the need for a separate chromatin enrichment step. in conclusion, the above observations demonstrate that the svm-based protein subcellular localisation classifications acquired for our lopit-dc and hyperlopit data are transferable between the two datasets, indicating their extremely high agreement."
"in order to apply a quantitative metric to our observations concerning the overall resolution of our experiments at the organelle marker level we utilised qsep, a tool which aims to objectively and robustly quantify subcellular resolution in spatial proteomics data and is freely available as part of the proloc package. for a detailed description of this function the reader is referred to the materials and methods section and 73 ."
"transfer learning using the hyperlopit and lopit-dc datasets as the main and auxiliary data sources, respectively. a) visualisation of the transfer learning parameter optimisation step: each row shows the frequency of observed weights, along the columns, for a specific organelle class, with large circles representing higher observation frequencies; b) pca plot of the hyperlopit dataset after classification, with point size being proportional to classification score; c) f1 scores of the lopit-dc dataset, the hyperlopit datasets and the combination of the two."
"according to the uniprot database q9upn3 isoform 2 has been found in multiple subcellular locations including the plasma membrane, golgi apparatus, cytoskeleton and cytoplasm and its localisation is dependent upon its phosphorylation state. similarly, p06753 is a tropomyosin chain component, another cytoskeletal element which is listed as a resident of both the cytoskeleton and cytosol in the uniprot and cell atlas databases. on the other hand, p29692 isoform 1 (chosen as the canonical sequence) is an elongation factor which regulates the function of ef-1-alpha and therefore the transfer of aminoacyl-trnas to the ribosomes 82 . this protein is described as a resident of the nucleus and cytosol in the uniprot and cell atlas databases and indeed, in both of our datasets, it is found between the nuclear and cytosolic clusters and, in the case of the hyperlopit data, also close to the two ribosomal subunits which is in agreement with its molecular function. lastly, q03001 and q03001-13 are positioned on top of the uncharted area and close to the plasma membrane cluster, respectively, in our pca plots corresponding to both datasets. similarly to q9upn3 and p06753, q03001 is a dynamic cytoskeletal linker protein which regulates the organisation and stability of intermediate filaments as well as microtubule and actin cytoskeleton networks by acting as an integrator 83 . this protein is also involved in the docking of the dynactin/dynein motor complex to vesicle cargos during retrograde axonal transport. according to uniprot, q03001 isoform 1 has been found at various cytoskeletal structures throughout the cytoplasm as well as focal contact attachments at the cell membrane and q03001 isoform 8 has been observed at the plasma membrane, cell cortex and several other cytoskeletal formations."
"the qsep metric was applied to the lopit-dc (10 organelle classes) and hyperlopit (12 organelle classes) datasets in order to quantitatively assess the levels of subcellular resolution our two methods accomplished. the quantitative cluster separation heatmaps in figure 3a demonstrate that both datasets feature exceptional subcellular diversity and thus spatial resolution, as both heatmaps contain similar colour patterns with a majority of average (light blue) and large (dark blue) normalised pairwise distances across all subcellular clusters. the higher overall resolution afforded by hyperlopit is supported by the average normalised pairwise distances describing each dataset, shown in figure 3b . according to this figure hyperlopit displays the highest global, experiment-wide subcellular resolution."
"as described in 71, transfer learning can be used for the meaningful integration of heterogeneous data sources in order to improve overall protein subcellular location classification given an optimal combination of the datasets provided. our transfer learning approach is based on the integration of a primary experimental spatial proteomics dataset and an auxiliary dataset and, as we have previously demonstrated, results in the assignment of proteins to their respective subcellular niche with higher generalisation accuracy than standard supervised machine learning workflows using a single information source 71 . the aim behind implementing such an approach is to support and complement the primary data with secondary annotation features without compromising their integrity, with the user possessing complete control over the amount of auxiliary data to incorporate into the learning process."
"all, where the lopit-dc dataset is classified using 10 marker classes and the colour code is based on the percentage of intersection (i.e., the number of intersecting proteins is divided by the total number of proteins assigned to that organelle in the lopit-dc and the hyperlopit data)."
"samples were prepared as described in 85 and 72 . u-2 os cells were trypsinised, washed and resuspended in a gentle lysis buffer (0.25 m sucrose, 10 mm hepes ph 7.4, 2 mm edta, 2 mm magnesium acetate, protease inhibitors). they were then lysed using a ball-bearing homogeniser and spun at 200 x g, 5 min, 4 °c to remove unlysed cells."
"data analysis was performed using the r 87 bioconductor 88 packages msnbase 89 and proloc 44 as described in 69 . briefly, 579 manually curated marker proteins were used to define 12 subcellular locations: cytosol, proteasome, nucleus, chromatin, 40s ribosome, 60s ribosome, peroxisome, mitochondrion, lysosome, golgi apparatus, plasma membrane and endoplasmic reticulum (supplemental quantitation table). these constitute our \"core organelle markers\", proteins known to localise to one specific subcellular niche. supervised machine learning using a support vector machine (svm) classifier with a radial basis function kernel was employed in order to predict the localisation of unlabelled proteins. in the case of the lopit-dc data classification was performed using both 12 and 10 marker classes: in the latter case the pairs nucleus/chromatin and ribosome 40s/ribosome 60s were merged to form single classes. following the protocol in 85, one hundred rounds of fivefold cross-validation was employed (creating five stratified test/train partitions) to estimate algorithmic performance. this protocol features an additional round of cross-validation on each training partition to optimise the free parameters of the svm, sigma and cost, via a grid search. based on the best f1 score (the harmonic mean of precision and recall), for the hyperlopit dataset the best sigma and cost were 0.01 and 8, respectively. the best sigma and cost for the lopit-dc data annotated with 12 marker classes were 0.1 and 16 and for the dataset with 10 classes they were 0.01 and 16. all proteins assigned to a specific subcellular niche by svm-based classification were ordered according to their svm scores and a threshold was set to achieve a 5% fdr based on agreement with the uniprot and gene ontology databases."
"in the current article, we have only slightly touched the annotated natural text reasons. we believe that the presence of 44k reasons (550k tokens) is another important asset of the newly created corpus, which deserves future investigation."
"bpmn language is not sufficiently adapted to model the resources. resources are the important aspects of business process simulation. it can be checked only the workflow logic without aspect of resources using only bpmn model. the modeling of a resource aspect gives an opportunity to simulate the real world processes without performing the process in reality. in this way, there is a way to get the results and adapt them to the optimization and improvement of a real process."
"the well-established hyperlopit workflow allows for the proteome-wide, high-resolution tracking of protein subcellular localisation where multiple organelles are analysed during a single experiment and has been applied to the study of many different biological systems. since this method provides high-quality, global spatial maps its application can be time-consuming as well as labour-and resource-intensive. aiming at researchers who do not necessarily seek a maximum resolutionyielding protocol we developed a simpler alternative to hyperlopit which we named lopit-dc. during the systematic study presented in this manuscript we applied both methods to a human osteosarcoma cell line using identical cell culture and lysis conditions as well as protein identification and quantitation pipelines and data analysis strategies. we compared the results produced via each workflow using a variety of approaches including qsep, a tool which enables the robust quantification of subcellular resolution in spatial proteomics datasets. our findings indicate that the data generated using hyperlopit exhibit the best overall resolution while the dataset obtained using lopit-dc closely follows. our analysis further suggests that the non-chromatin nucleus and chromatin as well as large protein complexes such as the ribosomes and proteasome are not wellseparated from each other in our lopit-dc data, while the same structures display distinct distributions in the hyperlopit dataset. despite that, the data produced by the two approaches showed excellent agreement regarding protein subcellular localisation prediction which increased even further when the strengths of these methods were integrated using transfer learning. importantly, both workflows were able to retain crucial information corresponding to suborganellar resolution as well as the localisation of protein complexes and components of important signalling pathways, with extremely high agreement regarding the distribution of individual proteins. furthermore, the two methods yielded comparable results related to protein isoform-specific subcellular niches with particular isoforms exhibiting very similar distributions in the lopit-dc and hyperlopit data. moreover, using information available in the cell atlas and gene ontology databases we could assign the lopit-dc-and hyperlopit-specific unlabelled, multilocalising proteomes to subcellular compartments with almost identical results for the two datasets."
"a method based on differential centrifugation alone, called dynamic organellar maps, was recently applied to silac-labelled hela cells 67 . in the context of this technique cells are processed according to two schemes: a) after removing the nucleus, silac-light cells are fractionated into membraneenriched pellets and b) silac-heavy cells are split into nucleus-, organelle-and cytosol-enriched fractions. each silac-light sample is then pooled with a \"reference\" silac-heavy membraneenriched fraction and analysed on a mass spectrometer to obtain light/heavy ratios from which a protein's membrane location is inferred. in parallel, all three silac-heavy fractions are analysed to obtain information on global protein distribution between the nucleus, cytosol and other organelles. this workflow has since been updated to include a full label-free (lfq) and a tmt-labelled option 68 in which no reference organellar fraction is used. in the lfq option the nucleus is included in the protein membrane distribution analysis and for tmt labelling only the five post-nuclear fractions are used and two replicates labelled with one tmt10plex set."
"we automatically selected debates that contained at least 25 top-level 4 arguments that were 10-110 words long (the mean for all top-level arguments was 66 ± 130 and the median 36, so we excluded the lengthy outliers in our sampling). we manually filtered out obvious silly debates (e.g., 'superman vs. batman') and ended up with 32 topics (the full topic list is presented together with experimental results later in table 3 ). from each topic we automatically sampled 25-35 random arguments and created (n * (n − 1)/2) argument pairs by combining all selected arguments. sampling argument pairs only from the same topics and not combining opposite stances was a design decision how to mitigate annotators' bias. 5 the order of arguments a1 and a2 in each argument pair was randomly shuffled. in total we sampled 16,927 argument pairs."
"finally, we applied supervised machine learning using a svm-based classifier in order to predict the subcellular localisation of the unlabelled proteins in the merged u-2 os lopit-dc dataset. we performed classification using 12 (cytosol, nucleus, chromatin, mitochondrion, peroxisome, lysosome, er, pm, ga, ribosome 40s, ribosome 60s and proteasome) or 10 organelle classes, where the nucleus and chromatin were merged into one cluster and the same was done for the two ribosomal subunits. in both cases, after classification the majority of proteins were assigned to the nuclear, cytosolic and mitochondrial clusters whereas the least populated niches were the proteasome, peroxisome, ga and lysosome. based on information available in uniprot and the literature we manually set svm classification thresholds for each subcellular niche allowing for a 5% false discovery rate. this way, approximately 35% of the merged dataset proteins were assigned to their respective subcellular location (table s2, figure 2c ). finally, as demonstrated by table s2 and figure s6, as expected, using 10 organelle classes rather than 12 improved classification numbers and quality regarding subcellular niches not optimally resolved in lopit-dc, such as the ribosomes."
"as demonstrated in the above example, the resulting distance value is informative of how much the average distance between two clusters is greater than the average distance within a cluster, the reference within-cluster distance here being a measure of how compact a cluster is. the resolution metric used by qsep is not influenced by the number of classes used for its computation and performs consistently well when provided with different organelle marker annotation. however, subcellular marker definition does affect the resolution assessment scoring with low quality marker lists yielding suboptimal results 73 . resolution measurements acquired via the qsep function can be visualised using quantitative cluster separation heatmaps and boxplots (see figures 3a and 3b and 73 )."
"apart from the proteins unambiguously classified to a unique subcellular niche, more than half of the proteins in each of our datasets remained unlabelled after svm-based subcellular location prediction and subsequent 5% fdr filtering. more specifically, 65% and 58% of the total number of proteins initially present in our analysis remained unclassified in the lopit-dc and hyperlopit data, respectively. these proteins might: 1) reside in more than one subcellular compartments, 2) associate with dynamic components, 3) be active traffickers between different organelles or/and 4) belong to subcellular structures for which no known markers were included in the analysis. importantly, these unassigned proteins constitute an important part of any spatial proteomics experiment as they most likely represent the dynamic effectors of protein (re)localisationdependent changes within the cell. furthermore, many of these multilocalising proteins are also multifunctional with protein function in these cases often depending on a specific subcellular niche and it is such potential translocators which have in many instances been identified as responsible for causing disease in cases of aberrant protein trafficking or/and aggregation [cit] 8 . in many of these cases, early stages of disease can be identified by protein translocation events which precede changes in gene expression. these changes often do not result in overall protein abundance alterations and can therefore only be studied at the subcellular level. examples of multilocalising proteins include, among others, signalling molecules, transporters, cytoskeletal components, transcription factors, proteins associated with vesicles or junctions, secreted factors and moonlighting proteins. due to the significance of the diverse roles these molecules play in the cell we proceeded to further explorative analysis of the proteins which were labelled as \"unknown\" in both of our datasets."
"the aspect of resources is critical in the business process simulation. each of the resources is associated with an activity, which is indicated in the processes. resources perform the work related to the following resources [cit] . there are no business processes without resources. business process needs to have described resources and workflow in order to be simulated. it is necessary to ensure that various activities use necessary resources or activities, which are carried out by the necessary resources (examples: personnel, worker, and equipment). excessive activity automation and poorly planned distribution of work are critical moments of projects, which are based on a workflow [cit] . therefore, it is necessary to indicate resources for both: the business level and information (technical) level. bpmn 2.0 allows assigning resources, but does not allow for graphical representation of resources in a way that can be understood by non-experts [cit] ."
"importantly, major developments in bioinformatics including approaches to interrogate spatial proteomics data 43, 44 and achieve sequence-or annotation-based prediction of protein subcellular localization (reviewed in 37 ) have also contributed to the evolution of spatial proteomics methods."
"one of the methods that allow for the simultaneous analysis of multiple subcellular structures in complex biological mixtures is termed localisation of organelle proteins by isotope tagging (lopit). lopit was first developed more than a decade ago to globally identify, quantify and assign cellular proteins to their respective subcellular niche 47, 48 and does not rely on absolute organelle purification (thereby circumventing the problems associated with it) but is based on the measurement of the distributions of cellular proteins across multiple density gradient fractions. in the context of this technique, protein localisation is assigned by comparing the distributions of unlabelled proteins to those of known, well curated organelle markers using quantitative mass spectrometry coupled to multivariate statistical analysis and machine-learning approaches 44 . finally,"
"we examined about fifty random false predictions to gain some insight into the limitations of both systems. we looked into argument pairs, in which both methods failed, as well as into instances where only one model was correct. blstm won in few cases by properly catching jokes or off-topic arguments; svm was properly catching all-upper-case arguments (considered as less convincing). by examining failures common to both systems, we found several cases where the prediction was wrong due to very negative sentiment (which might be a sign of the less convincing argument), but in other cases an argument with strong negative sentiment was actually the more convincing one. in general, we did not find any tendency on failures; they were also independent of the worker assignments distribution, thus not caused by likely ambiguous (hard) instances. table 4 : correlation results on ukpconvargrank."
"our newly created corpus of annotated pairs of arguments might resemble recent large-scale corpora for textual inference. [cit] introduced a 570k sentence pairs written by crowd-workers, the largest corpus to date. whereas their task is to classify whether the sentence pair represents entailment, contradiction, or is neutral (thus heading towards a deep semantic understanding), our goal is to assess the pragmatical properties of the given multiple sentence-long arguments (to which extent they fulfill the goal of persuasion). moreover, each of our annotated argument pairs is accompanied with five textual reasons that explain the rationale behind the labeler's decision. this is, to the best of our knowledge, a unique novel feature of our data."
"we address this problem as a regression task. we use the ukpconvargrank data, in which a realvalue score is assigned to each argument so the arguments can be ranked by their convincingness (for each topic independently). the task is thus to predict a real-value score for each argument from the test topic (remember that we use 32-fold cross validation). we measure spearman's and pearson's correlation coefficients on all results combined (not on each fold separately)."
"bonitasoft tool is an open source tool and is freely available to everyone. apart from that, the source code is available. we have compiled the bonitasoft source code and made the first changes in the source code. further, we are planning the practical implementation of the proposed method by modifying the bonita source code and developing a business process model for testing purposes."
"to make sure that subcellular fractionation was conducted successfully in both cases, sds-page 86 and western blotting were performed using the antibodies from 85 . proteins were separated on mini-protean tgx precast gels (bio-rad) and transferred to nitrocellulose or polyvinylidene fluoride (pvdf) membranes using the trans-blot turbo transfer system (bio-rad). signal was detected using the ecl prime western blotting detection reagent (ge healthcare) kit according to the manufacturer's instructions."
"observations first, let us compare the different sorting algorithms for each sampling strategy. as table 1 shows, on average, 158 pairs are ignored in total when all pairs are used for sampling (26 removed by mace and 132 by the graph building algorithm), while 164 pairs are ignored when only non-equivalent pairs are sampled (129 had already been removed apriori-26 by mace and 103 as equivalent pairs-and 35 by the graph algorithm)."
"argumentation quality has been an active topic among argumentation scholars. [cit] elaborate on criteria for practical argument evaluation (namely relevance, acceptability, and sufficiency). yet, empirical research on argumentation quality does not seem to reflect these criteria and leans toward simplistic evaluation using argument structures, such as how many premises support a claim [cit], or by the complexity of the analyzed argument scheme [cit] ."
"igrafx is a complete and easy to use process analysis and simulation tool that is specifically designed to help six sigma professionals to better understand and improve the process diagrams. this tool offers process design, analysis, and optimization and management capabilities. igrafx allows modeling resources and assigning them to process activities. there is a worker tab, which allows manipulating the resources. however, the simulation with igrafx does not use bpmn. instead of bpmn igrafx uses its own notation that is similar to bpmn. igrafx notation represents resources by using swimlines and pools. it could be assigned by various parameters of resources, like time and cost. availability parameter could be defined only as a percentage value and cannot be changed or influenced during simulation. the definition of resource roles is also limited, since resources cannot be assigned to different roles. apart from that, the priority of resources cannot be defined."
"we next employed qsep in order to assess the overall subcellular resolution of our lopit-dc and hyperlopit data compared to a variety of publicly available spatial proteomics datasets. in the context of this analysis we applied minimal data post-processing and used, whenever possible, the annotation provided by the original publications. moreover, we only considered organelle classes defined by at least 7 subcellular markers. in the cases where multiple replicates of a dataset were available, we used the combined dataset as opposed to individual replicate experiments and, in the cases where a combined dataset was unavailable, we used just the first (or only) replicate experiment provided by the authors. furthermore, any missing values present in the datasets were retained during our qsep analysis. figure s9 displays the distributions of the global average normalised distances stemming from all subcellular clusters for each dataset, with the datasets ordered according to experiment-wide median between-cluster distance. interestingly, as demonstrated by this figure, our lopit-dc and hyperlopit data exhibit the best overall, experimentwide resolution compared to all the other datasets, with the u-2 os hyperlopit data being topranked. this result demonstrates the superb quality of both of our datasets. a detailed description of the datasets used for this analysis as well as a more extensive comparison using additional datasets are presented in 73 ."
"regarding go molecular function (mf) terms, the proteins with an unknown localisation in both the lopit-dc and hyperlopit data are enriched with terms related to (ras) gtpase-, cadherin-/cell adhesion molecule-, actin-/cytoskeletal protein-and enzyme-binding (figures 5c and 5d) . here, the lopit-dc-specific unlabelled proteome displays an additional overrepresented term associated with molecular function regulation and the hyperlopit unclassified proteins are enriched with an extra term related to purine nucleotide-binding, which possibly refers to transcription factors or rnabinding proteins. these results provide additional validation and insights on the molecular nature of the multilocalising proteins identified by our two spatial proteomics methods, revealing the presence of protein function regulators as well as interactors of structural components, signalling molecules and nucleic acids among the lopit-dc and hyperlopit \"unknowns\"."
"after labelling, peptides were pooled into 10plexes, cleaned with c18 seppak cartridges and fractionated using high-ph reverse phase chromatography. the resulting fractions corresponding to each tmt10plex set were orthogonally combined into 18-22 samples for downstream ms analysis."
"argument pair weights by building argument graph from all pairs, introducing cycles into the graph seems to be inevitable, given a certain amount of noise in the annotations. we asked the following question: to which extent does occurrence of cycles in an argument graph depend on the quality of annotations?"
"let us extend our terminology. worker is a single annotator in amazon mechanical turk. reason is an explanation why a1 is more convincing than a2 (or the other way round, or why they are equally convincing). gold reason is a reason whose label matches the gold label in the argument pair (see figure 2 )."
"after labelling, peptides were pooled into 10plexes, cleaned with c18 seppak cartridges and fractionated using high-ph reverse phase chromatography. the 11 th tag was added to each 10plex just before rp-hplc. the resulting fractions corresponding to each tmt10plex set were orthogonally combined into 18 samples for downstream ms analysis."
"in conclusion, both workflows presented in this study can achieve high overall resolution and display excellent reproducibility. the choice regarding which one to use depends on the biological question in mind as well as the amount of starting material, time and resources available. if such matters are of no concern then hyperlopit can provide maximum subcellular resolution as the method of choice but in cases of starting material, time or financial constraints the simpler and quicker lopit-dc protocol can offer a great all-in-one alternative. as our findings demonstrate, both methods yield reliable, comparable results and can be utilised in the context of dynamic studies or for the mapping of features such as post-translational modifications, protein interactions or isoform behaviour. importantly, our u-2 os lopit-dc and hyperlopit data are the highest-resolution mass spectrometry-based spatial proteomics maps created using human cells to date; these datasets provide a snapshot of the structural organisation of u-2 os cells and can serve as a reference for future studies on human protein subcellular localisation and its relationship to protein function."
"we propose a novel task of predicting web argument convincingness. we crowdsourced a large corpus of 16k argument pairs over 32 topics and used global constraints based on transitivity properties of convincingness relation for cleaning the data. we experimented with feature-rich svm and bidirectional lstm and obtain 0.76-0.78 accuracy and 0.35-0.40 spearman's correlation in a crosstopic scenario. we release the newly created corpus ukpconvarg1 and the experimental software under free licenses. 11 to the best of our knowledge, we are the first who deal with argument convincingness in web data on such a large scale."
"there are some other aspects that are necessary for business process simulation [cit] . the simulation requires a more detailed business process analysis, investigation in the duration of activities and incoming workflow, which is not described in the standard business process model. it is necessary to investigate the duration of the activity and to explore a workflow. activities of programmatically submitted simulation models need to be represented as processes with certain parameters, resources, attributes, and relationships. it must contain all relevant information about the activities because simulation is a representation of the mathematical calculations to a certain level. it requires the simulation engine and tool or software code, which can use this engine in order to simulate the model. this paper analyzes the suitability of bpmn for modeling and simulating business processes with a particular emphasis on the resource aspect. this paper also analyzes the methods that allow proceeding the modeling of resources with bpmn based tools. it presents the evaluation of resource modeling in the business processes, highlighting the drawbacks of bpmn, when bpmn is used for business process modeling and simulation. finally, the paper presents significant properties for resource modeling and proposes a method for resource modeling in bpmn models."
"main contributions of this article are (1) large annotated dataset consisting of 16k argument pairs with 56k reasons in natural language (700k tokens), (2) thorough investigation of the annotated data with respect to properties of convincingness as a measure, (3) a svm model and end-to-end blstm model. the annotated data, licensed under cc-by-sa license, and the experimental code are publicly available at https://github.com/ukplab/ [cit] -convincing-arguments."
"the first step during transfer learning is free parameter optimisation for the classifier. our transfer learning approach uses a k-nn classifier and requires optimisation of two different sets of parameters: the first set is the k's necessary for the nearest neighbour calculations for the primary and auxiliary datasets and the second is the organelle class weights, one per class, which determine the proportion of primary and secondary data to be used for learning and range between 0 and 1. a weight of 1 implies that all weight is given to the primary data source, meaning that the final result relies exclusively on the primary experimental dataset and ignores the auxiliary data source provided. conversely, a weight of 0 indicates that all weight is given to the auxiliary data, representing a situation where the primary source of information is completely ignored and only the secondary dataset is considered. a weight of 0.5 implies that both data sources are equally used during learning and so contribute equally to the final result. the optimal combination of subcellular class-specific weights for a given primary and auxiliary data pair is identified during the algorithm's optimisation task and the results can be directly plotted as a bubble plot illustrating the proportion of best weights observed during the optimisation phase for each organelle class."
"it is well established that the level of complexity of the human proteome extends far beyond the number of gene products expressed by the genome in a cell 1, 2 . protein subcellular localisation is an important aspect of this complexity since the compartmentalisation of eukaryotic cells and the dynamic distribution of proteins between different organelles are intertwined with the regulation of cellular function 3 . perturbations in protein subcellular location have serious clinical implications and there are many human diseases caused by abnormal protein expression in combination with aberrant localization [cit] . subcellular proteomics studies have led to novel discoveries regarding disease mechanisms, generating new models to link mutations to certain disorders 2, [cit] . therefore, creating a complete and comprehensive organelle map for each tissue type or cell line under each possible physiological or pathological condition has the potential to significantly benefit drug discovery programs."
"even aristotle claimed that perceiving an argument as a \"good\" one depends on multiple factors (aristotle and kennedy (translator), 1991) -not only the logical structure of the argument (logos), but also on the speaker (ethos), emotions (pathos), or context (cairos) [cit] . experiments also show that different audiences perceive the very same arguments differently [cit] . a solid body of argumentation research has been devoted to the quality of arguments [cit], giving more profound criteria that \"good\" arguments should fulfill. however, the empirical evidence proving applicability of many theories falls short on everyday arguments [cit] ."
"initially, we utilised the immunofluorescence-based, u-2 os cell-specific protein subcellular localisation information available as part of the cell atlas database in order to investigate the subcellular distribution of our unlabelled proteins. as illustrated in figures 5a and 5b, the location which harbors the majority (300/200+ occurrences) of the proteins which remained unclassified in both the lopit-dc and hyperlopit datasets is the cytosol according to the cell atlas u-2 os data. this is expected as many known translocators are soluble cytosolic proteins capable of migrating towards different organelles to exert their function(s). the rest of the unassigned proteins in both of our datasets are mostly (50+ occurrences) distributed to the nucleoplasm or vesicles or are shared between the nucleoplasm and cytosol or cytosol and plasma membrane based on the cell atlas database. strikingly, these distribution patterns reflect the fluid nature of the above compartments: transcriptional regulators and many other kinds of proteins constantly travel between the nucleoplasm and the cytosol; proteins traffic to all organelles as well as the extracellular space and are also led to the degradation pathway via many types of vesicles; the plasma membrane, being the site of secretion and endocytosis as well as intercellular communication, possesses an exceptionally dynamic protein composition. interestingly, there are obvious differences between the distribution patterns of the proteins which were identified as \"unknown\" as part of our lopit-dc or hyperlopit data across the locations defined by the cell atlas. in more detail, the subcellular niche that contains the second largest number of unlabelled proteins in the case of the lopit-dc dataset is the nucleoplasm, followed by, in order, the nucleoplasm/cytosol combination, vesicles and cytosol/plasma membrane combination. on the other hand, the location harboring the second highest amount of unclassified proteins regarding the hyperlopit data is the vesicles, followed by the nucleoplasm as well as the nucleoplasm/cytosol and cytosol/plasma membrane combinations. this discrepancy might indicate that, while both methods primarily identify translocators associated with the cytosol, hyperlopit is able to capture the vesicle-associated dynamic proteome more effectively than lopit-dc which in turn most efficiently covers the nucleus-associated multilocalising proteome."
this section analyzes the resource modeling with bpmn tools and proposed extensions. there is a table that shows specific properties in the selected tool or extension (table i) . table i shows a set of resource properties:
"furthermore, recent advances in the field of quantitative proteomics have greatly contributed to the evolution of spatial proteomics studies. one of the most powerful such strategies has been the development of isotope-labelling methods which allow for the simultaneous analysis of many different biological samples in the same experiment 27, 42 . in vitro chemical labelling multiplexing up to 11 different tandem mass tags (tmt) is now possible leading to significant reduction in factors which previously limited subcellular resolution, such as experimental variation arising due to separate mass spectrometry runs and biological sample preparations 42 as well as missing values resulting from the stochastic nature of mass spectrometry."
"50 ug of protein per fraction were reduced, alkylated, digested and tmt-labelled as previously described 85 . one tmt10plex kit was used to label all the membrane and cytosol-enriched fractions in three biological replicates. the tmt11-131c tag was used to label the chromatin-enriched fraction."
following this calculation the normalised distance matrix ceases being symmetric and the normalised distance ratios are proportional to the tightness of the reference cluster (along the columns).
"bpsim (business process simulation) [cit] -is proposed for the business process simulation specification. bpsim provides a framework or a standardized specification for capturing business process models while using bpmn or xpdl. bpsim defines the parameterization and interchange of process analysis data that allows structural and capacity analysis of process models. the bpsim framework distinguishes a resource parameter class. this class defines availability, quantity, selection and role parameters. in addition, it is distinguished by a cost parameter class. however, this class is not related to resource and the cost should also be defined for the use of resources. the selection parameter is a property for selecting the desired resource and it is similar to a priority parameter. however, this framework does not define how to represent resources in the business process model."
"we next sought to investigate the location of distinct components of signalling pathways in our lopit-dc and hyperlopit datasets. a comprehensive illustration of several pathways which are important for many essential cellular functions is presented in figures 6c and s12-s21 . interestingly, we observe that the majority of these individual pathway components are found in the same subcellular niche in both datasets. one of the pathways we inspected is the p53 signalling pathway, presented in figure s12 . this pathway plays a crucial role in the control of dna replication and cell division as well as in cellular responses to different types of stress and has been implicated in many cancers 76 . as exhibited in figure s12, two components of this pathway can be found in the cytosol, two other of its constituents are classified as pm and er, one p53 pathway element is situated within the mitochondrion and five additional proteins which belong to this signalling cascade are unassigned in both the lopit-dc and hyperlopit datasets. furthermore, since our experiments were performed using the u-2 os cell line which is a cancer (osteosarcoma) cell line, we also explored pathways which have been found to play critical roles in cancer. for example, we looked at proteins that have been shown to be involved in transcriptional misregulation in cancer, presented in figure s16 . as demonstrated by this figure, twenty-four such proteins can be found in our two datasets. of these, five are classified to the pm, one is in the ga and fifteen overlap with the nuclear as well as the ribosomal clusters in both the lopit-dc and hyperlopit datasets. the remaining three components of this pathway visually from our pca plots are positioned close to the cytosol in the lopit-dc dataset and are slightly shifted towards the unassigned area of the plot in the hyperlopit data. importantly, we observed similar results for all the pathways we examined which showcases the outstanding agreement between our two datasets and reflects the functional organisation and networks of the cell. all pathways presented here were plotted according to information available in the kegg pathway database 77 ."
"we propose creating a graphic element such as bpmn \"comment\". this new element should have new functionality that is designed to model resources. the element must have attribute, whose value can be selected from a drop-down list. drop-down list contains names of the existing resources of the business process. collapsed display represents only selected resource name (fig. 1) ."
"for cell lysis, lopit-dc utilises the same approach as hyperlopit as it involves a gentle isotonic lysis buffer which keeps organelles as intact as possible while cells are lysed in a ball-bearing cell cracker. this lysis step shows excellent reproducibility and can be optimised for a variety of cell or tissue types. as in hyperlopit, cell lysis in lopit-dc is followed by a whole cell pre-clearing step which is necessary to remove unlysed cells that could confound downstream analysis. the cell lysis stage in both of our methods is critical as inefficient cell lysis can result in suboptimal organelle recovery which would lead to low protein yield during later steps, reducing overall efficiency. for lopit-dc, inefficient lysis would also mean generation of large microsomal particles which would sediment during the initial centrifugation steps. on the other hand, excessive cell lysis can damage sensitive membranes and lead to release of organellar content to the soluble part of the preparation."
"web discourse as a data source has been exploited in several tasks in argumentation mining, such as classifying propositions in user comments into three classes (verifiable experiential, verifiable non-experiential, and unverifiable) [cit], or mapping argument components to toulmin's model of argument in user-generated web discourse [cit], to name a few. while these approaches are crucial for understanding the structure of an argument, they do not directly address any qualitative criteria of argumentation."
"since assessing convincingness of a single argument directly is a very subjective task with high probability of introducing annotator's bias (because of personal preferences, beliefs, or background), we cast the problem as a relation annotation task. given two arguments, one should be selected as more convincing, or they might be both equally convincing (see an example in figure 1 )."
"seeing how such approaches suffer from low resolution relatively to our hyperlopit approach ( figure s9 ), but also acknowledging that the hyperlopit workflow is a relatively expensive and long protocol which requires a large amount of starting material, we modified our method towards the creation of a workflow which would be faster, cheaper and less resource-intensive than hyperlopit while retaining the highest subcellular resolution possible. in this article, we present our lopit-dc method as a \"best-of-both-worlds\" scenario aimed at scientists whose questions do not necessarily require obtaining maximum organellar resolution, those who seek to reduce experimentation time in the context of dynamic studies or those who are limited in starting material amount, resources or funding and therefore are after a more economical solution than hyperlopit (table s3) ."
"we performed three biological replicates of lopit-dc on the u-2 os cell line using on average 70x10 6 cells per replicate (figures s1a, s2a, s3a, s4). this way we obtained at least 60 µg of protein in each fraction with p6 being consistently the lowest-yield sample ( figure s3a ). lc-sps-ms 3 analysis of the u-2 os lopit-dc fractions resulted in identification of 9386 protein groups after replicate merging and, following initial processing and missing value removal, 6837 protein groups with a full reporter ion series remained (table s1 )."
"we thus compute a weight for each argument pair. let e i be a particular annotation pair (edge). let g i be all labels in that pair that match the predicted gold label, and o i opposite labels (different from the gold label). let v be a single worker's vote and c v a global worker's competence score. then the weight w of edge e i is computed as follows:"
"methods as a \"traditional\" method, we employ svm with rbf kernel 8 based on a large set of rich linguistic features. they include uni-and bi-gram presence, ratio of adjective and adverb endings that may signalize neuroticism [cit], contextuality measure [cit], dependency tree depth, ratio of exclamation or quotation marks, ratio of modal verbs, counts of several named entity types, ratio of past vs. future tense verbs, pos n-grams, presence of dependency tree production rules, seven different readability measures (e.g., ari [cit], coleman-liau [cit], flesch [cit], and others), five sentiment scores (from very negative to very positive) [cit], spellchecking using standard unix words, ratio of superlatives, and some surface features such as sentence lengths, longer words count, etc. the resulting feature vector dimension is about 64k. we also use bidirectional long short-term 8 using lisbvm [cit] ."
"lopit has been applied for the study of the subcellular proteomes of the hek293 human kidney cell line, a. thaliana roots, d. melanogaster embryos, s. cerevisiae cells and the dt40 lymphocyte cell line [cit] ."
"in order to gain additional insights into the multilocalising proteome captured by our two distinct workflows we next performed functional gene ontology (go) 74 term enrichment analysis on the proteins which remained unassigned in both our lopit-dc and hyperlopit datasets. regarding go biological process (bp) terms, both the lopit-dc-and hyperlopit-specific unclassified proteomes are enriched with terms related to vesicle-mediated transport and the endomembrane system (figures 5c and 5d) . similarly, concerning go cellular component (cc) terms, the unlabelled proteins in both datasets are enriched with terms associated with the cytosol/cytoplasm, endomembrane system, vesicles and endosome (figure 5c and d) . here, the lopit-dc unassigned proteome exhibits an additional overrepresented term related to the plasma membrane and the hyperlopit-specific unclassified proteome is enriched with extra terms associated with the extracellular exosome and extracellular vesicles. importantly, our go bp and cc term overrepresentation analysis results corroborate the cell atlas comparison findings presented in the previous paragraph, according to which the majority of the unlabelled proteins in both of our datasets is annotated as cytosolic in the cell atlas and also a large portion of these proteins is assigned to the vesicles and endomembrane system as part of the same database. moreover, the enrichment of both the lopit-dc-and hyperlopit-specific unassigned protein pools with general terms referring to macromolecule subcellular localisation might refer to presence of factors which regulate the establishment of protein localisation (or the localisation of molecules other than proteins) within the cell."
"quantification at the ms 3 level was performed within the proteome discoverer workflow using the centroid sum method and an integration tolerance of 2 mmu. isotope impurity correction factors were applied. each raw peptide-spectrum match (psm) reporter intensity was then divided by the sum of all intensities for that psm (sum normalisation). protein grouping was carried out according to the minimum parsimony principle and the median of all sum-normalised psm ratios belonging to each protein group was calculated as the protein group quantitation value. only proteins with a full reporter ion series were retained. finally, proteins identified as crap were removed for downstream analysis."
"the author of the paper [cit] suggests that the bpmn element called swimlanes should be portrayed as the role and that each of these elements represents a certain role. however, while using such a method, business process model complicates a number of activities."
"within the lopit-dc dataset the smallest normalised distances correspond to the peroxisome/er, er/pm and lysosome/mitochondria pairs ( figure 3a, left); this is in concordance with our pca plot observations according to which these subcellular niches are positioned close together in pca space, forming a continuum of clusters. smaller distances in the heatmap are attributable to differences in cluster size: for example, the nucleus is a much larger cluster than the ribosome and so the average normalised distance between the two is greater than the distance within the nuclear cluster. however, when examining in reverse, the distance between the ribosomes and nucleus is 6.06 times greater than the within-ribosome distance. on the other hand, the largest normalised distances in this dataset are those between various organelles and the proteasome or ribosome. this is expected, as both the ribosome and proteasome are clearly very well separated from the membrane-bound organelles of the secretory pathway as well as the mitochondrion along dimensions 1 and 2."
other popular process modeling methods are idef0 and idef3. these languages originate from idef (integration definition). idef is a set of modeling languages in the field of systems and software engineering. idef0 was designed for creating a function model of a new or existing system of application domain. idef3 process description capture method is a method for creating a dynamic model of the system [cit] .
"recently, an improved version of lopit called hyperplexed lopit (hyperlopit) has been developed and applied to the study of the e14tg2a mouse embryonic stem cell subcellular proteome 56 . the hyperlopit protocol integrates novel approaches for sample preparation, mass spectrometry data acquisition and multivariate data analysis to create high resolution protein subcellular localisation datasets. its application results in subcellular location assignment for thousands of proteins and functional complexes with excellent resolution and enables the novel classification of proteins whose subcellular distribution was previously unknown. it also returns information on proteins that demonstrate intermediate distributions between multiple subcellular compartments, which in turn reflects protein functions involved in important aspects of cell biology. importantly, hyperlopit provides information on a cell-wide scale, unlike proximity tagging methods 57 designed to identify proteins associated with discrete subcellular niches which only return limited data per experiment and do not readily scale to reveal proteins with multiple locations."
"sampling large sets of arguments for annotation from the web poses several challenges. first, we must be sure that the obtained texts are actual arguments. second, the context of the argument should be known (the prompt and the stance). finally, we need sources with permissive licenses, which allow us to release the resulting corpus further to the community. these criteria are met by arguments from two debate portals. 3 we will use the following terminology. we use topic to refer to a subset of an on-line debate with a given prompt and a certain stance (for example, \"should physical education be mandatory in schools? -yes\" is considered as a single topic). each debate has two topics, one for each stance. argument is a single comment directly addressing the debate prompt. argument pair is an ordered set of two arguments (a1 and a2) belonging to the same topic; see figure 1 ."
"(which is 1.5 is our example). the average transitivity score is then an average of transitivity scores for each pair of nodes from the graph that are connected by two or more paths. analogically, the maximum transitivity score is the maximal value. we restrict the shortest path to be a direct edge only."
"the proposed method has several interconnected advantages in comparison with existing methods and tools. the method offers to graphically display resources with essential parameters of resources like role of resource, availability, cost, duration, cost, quantity and concurrency. these improvements helps better understand the process that is designed for simulation."
"in a slightly different case, the q9upn3 (pink), p06753 (green), p29692 (orange) and q03001 (dark blue) isoform pairs presented in the same figure all remained unclassified in both the lopit-dc and hyperlopit data and therefore overlap with the \"unknown\" area of our pca plots, but each isoform of every pair is positioned close to the same organelles in the two datasets. interestingly, these proteins seem to possess important regulatory roles which could justify their identification as dynamic translocators. in more detail, q9upn3 isoform 2 is a well-studied actin-binding protein which crosslinks actin to other cytoskeletal components, binds to and stabilises microtubules and is involved in the control of focal adhesion assembly and dynamics as well as cell migration and vesicle transport through the trans-golgi network 79, 80 . this protein also acts as a positive regulator of the wnt receptor signalling cascade as it has been shown to be involved in the translocation of the axin1/apc/ctnnb1/gsk3b complex from the cytoplasm towards the plasma membrane 81 ."
"aiming to further explore the quality of our data we also examined the clusters present in both the lopit-dc and hyperlopit datasets in terms of suborganellar resolution. as demonstrated in figure 6a (and in more detail in figures s10 and s11 ), the distributions of several suborganellar structures mapped upon our data exhibit a superb level of agreement between the two datasets. for example, the er lumen and er membrane are located at slightly different positions on top of the er cluster and the ergic-cis golgi is positioned between the er and ga clusters in both the lopit-dc and hyperlopit data. interestingly, these suborganellar structures are better resolved from each other in the lopit-dc rather than the hyperlopit dataset. additionally, as expected due to the broad connectivity of the cytoskeleton with most subcellular structures, actin-binding proteins are distributed mainly in the \"unknown\" area of our plots in both datasets, with some proteins being located close to a variety of organelles. similarly, our endosomal markers are distributed on top of the plasma membrane and lysosome clusters as well as the unassigned area of the hyperlopit pca plot, while the same proteins exhibit a slightly shifted distribution in the lopit-dc data where they are located closer to the er and in the \"unknown\" area of the plot. both distributions are justifiable since the endosome, as part of the endocytic membrane transport pathway, is a very dynamic organelle which recycles between the ga, plasma membrane and lysosomes and is also in contact with the er 75 ."
"we also release the full dataset ukpconvargall. in this data, no global filtering using graph construction methods is applied, only the local pre-filtering using mace. we believe this dataset can be used as a supporting training data for some tasks that do not rely on the property of total ordering. along the actual argument texts, all the gold-standard corpora contain the reasons as well as full workers' information and debate meta-data."
"traditional scientific programming languages generally lack support for object-oriented programming (oop), which has increasingly been adopted by the community. oop codifies the discipline of data and procedure encapsulation, thereby facilitating the development of re-usable software. the large set of supported languages, shown in figure 1, emphasizes languages of interest to the community. support for traditional scientific programming languages is important due to the significant amount and inherently long lifetimes of legacy codes written in those languages."
"babel, which bridges the gap among the different programming paradigms and languages, enables software written in classical imperative programming languages, such as fortran and c, to interoperate with interpreted scripting languages, such as python. to accomplish this feat, babel must deal with different binary representations, symbol length limitations, and inconsistent rules for identifier declarations (e.g. case sensitivity or reserved symbols)."
"true if all elements in array u fall within the specified tolerance, t, of r low ::r high . size (u) allocated size of array u."
"for various reasons, (d)com is hardly a candidate for high-performance computing. one major problem is portability to platforms other than windows. it also lacks the necessary abstractions for parallel data organization and scientific data types. furthermore, com uses a very limited object model with no support for polymorphism."
reference counting is implemented using atomic compare-and-swap operations instead of global locks. babel uses non-standard compiler intrinsics if available and small chunks of inline assembler otherwise.
"multi-disciplinary, multi-physics, and multi-resolution applications are far too complex to be developed by a single organization. hence, the parts (models, libraries, and solvers) are developed by code groups with relevant expertise. each team often relies on different programming languages and development platforms. some critical codes may have even been developed by experts long retired. these differences exacerbate the integration challenges that must be overcome to successfully create large-scale applications."
"this study aims at implementing a recommender system for special data that the general recommender systems described above cannot be used for, i.e., real estate property. we predict users' preference for properties, which is the first step of property recommendation, using the dataset that includes users' evaluations for properties possessed by ietty 3, which is a rental company of real estate properties. generally, users search for desired properties on property search websites and contact the real estate companies possessing the properties. however, ietty recommends properties to the users on its website, and the users evaluate the properties online, after which they preview the properties or lease them. thus, if an effective method for prediction of users' preference for properties using their evaluation data is available, property recommendation can be performed more easily."
"distributed reference counting can be a challenging problem. babel keeps partial reference counts in the local stubs. only when the local reference count reaches zero can communication occur to adjust the reference count on the object itself. this approach has many advantages since it significantly reduces the amount of network traffic. however, it may also lead to resource leakage in case clients are physically disconnected. as for communication protocols, more fault-tolerant schemes can be provided by the user if necessary."
"the numerically intensive computations performed in scientific applications are heavily dependent on the use of array-based and numerical data types. of particular importance are dynamic, multi-dimensional arrays, array strides, single-and double-precision complex numbers, and structures. arrays with all of these features are not generally native types in modern, general-purpose languages."
"another babel feature mainly implemented in skeletons are so-called hooks. hooks provide a simple form of aspectoriented programming and allow the user to execute code right before or after method invocations. they can be dynamically enabled or disabled. hooks are useful for a variety of applications such as logging or profiling. the skeleton executes pre-hooks immediately prior to the usual method dispatch. likewise, if no exceptions are encountered, posthooks are invoked immediately after returning from an implementation."
"ietty, a rental company of real estate properties, has attribute data of real estate properties and users, and recommends several properties to the users by the rule-based algorithm on its website. further, the users can evaluate each recommended property, selecting \"want to see the property, \" \"register as favorite, \" or \"no interest. \" [cit] which contains 220,094 cases with the floor plan images, and divided them into train set, validation set, and test set at a ratio of 3: 1: 1. thus, the dataset includes attribute data of 19,538 users, and attribute data and floor plan images of 131,947 properties."
further adding to the challenges faced by the scientific computing community is the need for software tools to run on commercial as well as one-of-a-kind platforms. computational scientists often develop their codes on desktop platforms then port them to top 500 1 machines for highperformance runs. machine and language idiosyncrasies present unique portability challenges requiring a detailed understanding of binary interfaces and linkage conventions.
"additional features not natively available across languages are typically required by the numerical libraries that dominate scientific applications. specifically, dynamic, multi-dimensional arrays, array strides, single-and doubleprecision complex numbers, and structures are common. the heavy reliance on arrays for managing numerical data is a critical aspect of these applications and it can have a significant impact on performance."
"packages are used to define name space hierarchies. all sidl entities are required to be part of a package. they consist of a combination of types, primarily in the form of interfaces and classes, and allow for software composition. packages may be nested and versioned, with the latter (identified through major and minor numbers) being inherited by class and interface declarations. types defined within an external package may be referenced by importing the package. package version restrictions can be enforced by import statements."
"next, for any object of a given class, the entries of the entry point vector are exactly the same. consequently, dispatch tables are initialized only once and require only a constant amount of memory. each object, however, carries a reference to the entry point vector of its dynamic type."
"objects can be passed remotely either by reference or by copy. a call by reference requires the existence of a socalled babel object server (bos). call by value, on the other hand, requires an object to be serializable and creates an additional local copy on the remote end. the user is responsible for implementing (de)serialization functionality for a particular class by implementing the sidl.serializable interface. for derived types such as structs, babel provides automatically generated support routines."
"the babel compiler translates sidl descriptions into wrappers used to map between programming-languagespecific types and the common representation layer. native language features, such as built-in data types and method overloading, are leveraged in the generated code, whenever possible, with reasonable alternatives used in the remaining cases. babel maps features to the particular native language ecosystem in order to impose minimal requirements for existing software packages. there is no major feature in babel that cannot be supported in all existing language bindings. to the user, a component using babel always appears to be implemented in the particular native language, no matter which combination of languages is actually used. the common representation layer depends on features of the sidl runtime library to support the object model and contains interface contract enforcement features, when needed. the layer is also instrumented with interface contract enforcement checks, when contracts are defined, that also depend on features in the runtime library."
"experiments for course-grained interface descriptions [cit] clearly show that the overhead of babel is well within measurement imprecision and usually below 1%. a more detailed performance study [cit] shows that the call overhead is usually very small for most data types. large overheads can only be observed for a small number of types such as strings or arrays with explicit order specifications, as babel is forced to allocate new memory and physically copy data."
"swig [cit] ) is a software development tool that connects c and cþþ libraries to a large set of scripting languages (e.g. perl, python, php, ruby, or tcl). recent versions also include bindings for nonscripting languages such as c#, lisp, or java."
stubs are pieces of glue code generated by babel for each member function serving two main purposes: (a) they convert arguments and return values between the native language representation and babel's ior and (b) they dispatch to the skeleton code via an object's epv. there is a strict separation between client and server code in order to ensure that components can be distributed in binary form together with the corresponding sidl file. the method entry points stored in the epv are the only way to 'cross' the barrier between client and server code.
we predicted users' preference for properties by the hybrid filtering of 3.5 and compared the same three types of input as 4.3. table 2 shows the result of the prediction.
"the opaque type is a language-independent way to declare language-dependent interfaces. babel only guarantees that opaque arguments are preserved between caller and callee. in practice, opaque types are sometimes used for pointers to resource handles or language-dependent data structures. however, their use is strongly discouraged as opaque types have meaning only within a particular process and introduce limitations in combination with rmi."
sidl interfaces can also be used to automatically generate documentation using a html backend similar to javadoc [cit] . babel itself uses these capabilities to generate consistent documentation for its runtime library. user-provided doc-comments are properly maintained.
corba mainly targets distributed systems communicating over a physical network. communication is managed by a so-called object request broker (orb). an abstract protocol (giop) is used to communicate among different orbs. implementations exists on top of plain tcp/ip as well as higher-level protocols such as ssl or http.
"several things are important to note. first, the memory layout is such that the beginning of an object of a derived type is always also the beginning of a valid object of its base class. thus, up-and down-casting can be implemented without pointer adjustments. casting to an interface, however, will usually return a base address different from the original object. the only exception is sidl.baseinterface, which is treated differently."
"true if all elements in array u fall within the integer range n low ..n high lower (u, d) lower index of the d th dimension of array u."
"sidl supports the fortran concept of array strides, which means that data is not necessarily densely packed. each dimension can be strided arbitrarily. one-dimensional arrays can be declared row-major or column-major in order to specify dense arrays. otherwise, row-and column-major one-dimensional arrays are identical."
"finally, the often lengthy execution times (or the order of days to weeks) of scientific simulation runs result in the need to minimize the introduction of additional overhead. babel was initially designed for fast, in-process communication to address this important issue. the project won the prestigious r&d 100 award [cit] for 'the world's most rapid communication among many programming languages in a single application'. while babel's primary focus is efficient interoperability within a single address space, it also fully supports transparent remote method invocation (rmi)."
babel supports traditional and experimental contract enforcement through runtime options combining the classification of contract clauses to be enforced with the frequency of their enforcement. this approach was considered to be the most flexible for not only enforcement but also classification purposes [cit] . table 3 lists the more common options for each criteria.
"large-scale, multi-physics, and multi-resolution computational science and engineering applications of today face significant integration challenges due to their use of numerically intensive, long-running codes written in different (including legacy) programming languages for deployment on one-of-a-kind platforms. babel addresses the functional and performance needs of the community through a high-performance interoperability toolkit. the motivation for and approach taken to develop the technology is described in sections 2 and 3, respectively. details of the toolkit are provided in sections 4-6. applications of babel are presented in section 7. section 8 covers the most relevant related work. directions for future work are presented in section 9."
"all methods inherited from an abstract base class or interface remain abstract unless they are re-declared in the class definition. in order to keep class declarations concise, the implements-all keyword can be used to revert this behavior for interfaces."
"raw arrays provide for low-level data access but suffer a series of limitations compared with regular sidl arrays. in particular, raw arrays may only be passed in mode in or inout and cannot be used as return values. they must be contiguous and are implicitly stored in column-major order. implementations are not allowed to change the shape of the array or return an array different from the one being passed. also, raw arrays are restricted to fundamental numerical types. as for structs, a major motivation for raw arrays is to match existing legacy apis, requiring less or no code to translate from and to sidl interfaces."
"arrays are more interesting as they are defined for a particular base type. they are represented by a more complex data structure containing meta-information and a pointer to the allocated memory. meta-information includes lower and upper bounds, strides, a reference count, and a simple dispatch table for a small number of support routines. structs are recursively defined on top of these types and may contain arrays, other structs, or interfaces/classes. however, there is no support for arrays of structs or arrays of arrays. by far the most interesting types are classes and interfaces. in babel jargon, each object or interface carries a reference to an epv that defines the set of member functions supported by the corresponding type. figure 8 shows the memory layout for a babel object with a simple inheritance structure. solid lines denote generalization while dashed lines stand for the implementation of interfaces."
"in this study, we predict preference by content-based filtering using similarities of both users and items, and mlp that adds the deep features extracted from floor plan images using floornet into the input. because these methods are available without past evaluation data by the users to be recommended items, they are robust to sparse data. we also propose a system called hybrid filtering that combines these two methods and predict users' preference by it."
"multimedia for retech'18, june 11, 2018, yokohama, japan the input. as a comparison among the three types of input with hybrid filtering, the characteristics of the prediction performance is similar to 4.3. when hybrid filtering is compared with the mlp, we can see that all the prediction performances are considerably improved. therefore, it is considered that by using content-based filtering and mlp in combination, there is an effect of improving generalization performance, like that of an ensemble in machine learning."
"sidl methods define the calling routines of interfaces and classes through method signatures specifying the arguments and return type, if any. standard object oriented mechanisms for controlling method inheritance, such as virtual, final, and static, are either implicit or defined with method qualifiers. parameter declarations must indicate the mode. finally, rmi is supported through additional modifiers identifying communication semantics."
"the cca specification itself is written solely in sidl. thus, each of the languages supported by babel is a firstclass citizen and can be combined with components written in any other language. components interact with each other via ports, following the provides/uses design pattern. each component specifies which ports it uses from others and for which it provides an implementation. several cca compliant frameworks focusing on different programming models are currently available. ccaffeine [cit] focuses on single program multiple data (spmd)-style programming and supports a native cþþ interface; xcat 4 [cit] ) focuses on distributed programs; scirun [cit] features bridging technologies between cca, corba, vtk, and shared-memory models. babel itself ships with a simple reference implementation called decaf [cit] ."
"babel is under active development and serves as testbed for a number of exciting research projects. we actively work on reducing the overhead of language interoperability to allow for more fine-grained interfaces. more challenging is support for emerging parallel pgas (partitioned global address space) languages such as chapel [cit], unified parallel c (upc) [cit], or co-array fortran [cit] . we are also considering support for gpgpu-oriented languages such as cuda and opencl."
"among all the prediction systems mentioned above, the best performance was achieved by hybrid filtering that added deep features extracted from floor plan images by the fine-tuned model into"
"we define \"want to see the property\" and \"register as favorite\" as positive evaluation and \"no interest\" as negative evaluation of properties by the users, designate these as users' preference for properties, and predict it using methods available for sparse data."
"babel bridges the gap among different scientific programming paradigms and languages, combining native support for key scientific data types, object orientation, and interface contracts, to produce a tool tailored for scientific language interoperability. the approach involves the generation of source code wrappers from specifications of the calling interface, in sidl. the generated code is based on an object model (defined in sidl and implemented in the sidl runtime library) with built-in scientific data types and optional interface contracts. the toolkit, therefore, combines the benefits of object orientation and programming by contract [cit] ) with scientific language interoperability."
"the ability to enforce interface contracts, described in section 4.6, at runtime requires library support. minimally, there are contract-specific exceptions raised when clauses are violated. support for a variety of enforcement options across programming languages also requires option management. [cit], 2005), the goal of trying to better control enforcement overhead lead to the current release containing a centralized enforcement manager [cit] ."
"as with all of the remaining approaches, corba lacks essential features for high-performance computing such as fortran-style arrays, strides, or complex numbers. [cit] . corba also provides a rather limited object model with no support for polymorphism. microsoft's component object model (com) and its distributed version dcom [cit] are windowsbased interoperability frameworks. their main focus is on business and internet applications."
"the remainder of this study is organized as follows. in section 2, we discuss the related works about recommendation systems and analysis of floor plan images. in section 3, we describe our proposed method in detail. in section 4, we show metrics of the main experiments and the results. in section 5, we conclude this study."
"a reference to a remote object can either be obtained by creating a new remote object via _createremote or by connecting a stub to an existing remote object via _con-nect. remote object stubs differ from local objects mainly in that their epv will not directly point to the skeleton, but to a babel-generated function that marshals and unmarshals arguments and performs the necessary network transfer. all objects are identified by a protocol-specific url. babel itself ships with a simple tcp/ip-based protocol. [cit] ). the same mechanism can be used to provide compatibility with related systems such as corba."
"in reality, objects optionally carry some additional state for profiling, contract enforcement, or 'hooks', which are user-supplied methods invoked before and after a particular method invocation. the example also omits implicit base classes and interfaces for simplicity."
"w/o image feature: using only attribute data of users and properties using attribute data of users and properties, and deep features extracted from floor plan images by the fine-tuned model in 3.4 table 1 shows the result of the prediction."
"raw arrays provide a lower-level alternative to numeric arrays in some languages (e.g. a one-dimensional raw array may appear as a double pointer and a length parameter in c). to highlight the contrast, a normal sidl array is represented by a struct in c, a template class in cþþ, a 64-bit integer in fortran 77, and a derived type in fortran 90/95. in higher-level languages such as java or python 3, raw arrays appear like regular arrays."
"a relatively recent addition to sidl are structural data types (struct). these types provide a natural way to group semantically related data together. figure 6 shows an example sidl specification. structural types are more efficient and require less development effort than regular classes. they also often allow for sidl specifications of existing interfaces and provide compatibility with related systems such as corba or wsdl [cit] . a sidl struct may contain fields of arbitrary type, including (raw) arrays and structs. however, there is currently no support for arrays of structs. structs are fully compatible with rmi and provide for a very efficient way to pass data among various programming languages."
"the ior is a well-defined intermediate representation for each of the fundamental sidl types listed in table 2 . for basic algebraic types, babel uses the corresponding equivalent in c (e.g.the sidl type int is represented by the c99 type int32_t). likewise, basic types such as bool or string are mapped to their c equivalents. complex numbers are pairs of single-or double-precision floating point numbers."
"contract clauses, which are optional, define constraints on properties of objects as well as argument and return values. babel supports three clauses: preconditions, postconditions, and class invariants [cit] ). the syntax is borrowed from eiffel [cit] . figure 5 shows the sidl specification of precondition and postcondition clauses associated with a vector dot product method. as illustrated, clauses consist of a list of assertions optionally preceded by individual labels used for documentation."
"contract clause classification options are based on either the type of clause or clause contents. traditional enforcement options are either all-or-nothing for precondition, postcondition and/or invariant clauses. babel supports those as well as options based on clause contents (e.g. presence or absence of method calls, output and return arguments, or the complexity of assertions). determination of the complexity of actual contract clauses is inferred by the babel compiler based on the complexity of array arguments in the corresponding assertions. the clause is tagged according to the highest complexity encountered."
"similar to corba idl, each parameter declaration is preceded by an explicit mode specifier, which may either be in, out, or inout, to declare whether a parameter will be read-only or also written to by the method. implementations are explicitly allowed to return an object different from that being passed for inout parameters, except for special cases such as raw arrays."
"true if real values x and y are equal within the specified tolerance, t. nearequal (u, v, t) true if the corresponding elements in arrays u and v are equal within the specified tolerance, t. none(expr)"
"although the layout type of floor plans, such as one bedroom + one bathroom is included in the attribute data of properties, it is expected that the accuracy of preference prediction is improved by considering the floor plans. for example, in the same layout type, some users desire properties that prevents a direct connection between an entrance and a child's room so that young children do not come out outside without their parents' permission, while other users desire convenient properties that allows walking out into the corridor from each room. therefore, we propose adding deep features of floor plans into the input of mlp to consider such floor plans. the details of each method are described in the following sections."
"skeletons are the counterpart to stubs. they convert the ior to the particular native language representation and transfer control over to the user-supplied member function implementation. for return values and out arguments, the inverse operations are applied, i.e. they are converted from their native language representation to babel's ior. most technical considerations discussed in the context of stubs also apply to skeletons. except for cþþ [cit] /2008, they are implemented in c and use some form of native language interface for argument conversion."
"all implementations of the method are not (supposed to) have any side effects. at this point, there are no tools in the babel toolkit to verify this property. babel interface contracts support the specification of eiffel-inspired precondition, postcondition, and class invariant clauses. each clause, when specified, contains one or more assertions. assertions can be as simple as basic variable expressions consisting of arguments, operators, and numeric or boolean literals. alternatively, methods (built-in as well as user-defined) may be used in the expressions. overall, babel supports a rich variety of assertion expressions within sidl interface contracts."
"exceptions are used to indicate errant behavior. all methods implicitly throw a sidl.runtimeexception should there be errors in the babel generated code or communication. additional exceptions have to be specified explicitly using a throws clause, as shown in figure 5 . exceptions are mapped to native language features whenever possible; otherwise, they are communicated using an additional generated out parameter that must be explicitly checked by the user."
"interoperability between languages involving incompatible programming paradigms and type systems is inherently difficult. for example, dynamic memory management may be a feature of one language but left to the programmer in another. errors may be reported explicitly versus via dynamic exceptions. arrays may be represented in columnversus row-major order and their indices start at 0 versus 1. these incompatibilities can make building, debugging, and maintaining associated software systems extremely challenging."
"the studies on floor plans of properties before the development of deep learning were based on analyzing floor plans graphically. [cit] analyzed floor plans using adjacency graphs with rooms, corridors, etc., as labeled nodes and four sampled datasets that divided the floor plans according to their square measures. they examined patterns and their numbers of the adjacency graphs, and classified them into six types according to the distances from each node to other nodes of the adjacency graphs. in addition, [cit] analyzed the rent of properties using adjacency graphs with rooms, corridors, etc., as labeled nodes and doors, glasses, etc., as labeled edges, and floor plans of 3ldk, 3k, or 3dk apartment in kyoto, japan. they extracted subgraphs from the adjacency graphs and effectively estimated the rent from the presence/absence of common subgraphs. however, the cost of creating adjacency graphs of floor plans is very high."
"babel was born out of a larger approach to manage the rising software complexity of scientific applications. the common component architecture (cca) [cit] ) is a joint effort by researchers from both academia and us national laboratories to establish and adapt component technology for high-performance scientific computing. the cca mediates how components interact with each other and with the underlying framework, using babel as its language interoperability framework. babel can be used standalone or as part of the full cca framework."
"under such circumstances, a real estate technology called real estate tech (retech) began to grow rapidly, and the ministry of land, infrastructure, transport and tourism (mlit) 1 in japan conducted social experiments to deregulate activities on the internet in the real estate industry 2 . therefore, property recommendation on websites has become an important task."
"both scientific and general-purpose programming languages lack support for interface contracts, which are a well-known software engineering technique for improving testing and debugging [cit] . interface contracts define and enable the automated enforcement of software behaviors at call boundaries. they are also a logical extension of language interoperability solutions for the support of cross-language debugging."
"rmi adds three additional modifiers: local, oneway, and nonblocking. caller and callee of a local method have to share the same address space (i.e. they cannot be invoked on a remote object). a method declared oneway can only have in arguments and be implemented using unidirectional network messages. non-blocking methods implement asynchronous call semantics. these methods return a so- called ticket that can be used to retrieve return values and out arguments."
"specifications into client-server language interoperability source code using the babel compiler. there are actually four layers generated: stub, intermediate object representation (ior), skeleton, and implementation."
"babel supports a variety of traditional and experimental contract (clause) enforcement options. traditional all-ornothing approaches are supplemented with selective and sampling-based enforcement techniques. selective enforcement options can be used to gather data on the nature of clauses actually encountered during testing and, potentially, deployment. sampling-based enforcement techniques are intended to enable reduced contract enforcement during deployment in performance-constrained environments."
"accordingly, we proposed a prediction system combining contentbased filtering and multilayer perceptron (mlp) to predict users' preference for properties. moreover, we used deep features of floor plans as input of the mlp to improve accuracy. consequently, we succeeded in predicting users' preference for properties with an accuracy of 60.7%."
"there are three interface contract exceptions supported in the runtime library, one per contract clause. violations in the assertions within a clause result in a clause-specific exception being raised at runtime. for example, an assertion evaluating to false within a precondition clause results in a sidl.previolation exception. the contracts are checked and exceptions raised automatically by the generated middleware. the client side needs to be aware of and appropriately handle these exceptions."
"each epv contains a set of built-ins provided by babel in addition to user-defined methods. built-ins always start with an underscore and provide casting, reference counting, and support routines for object construction and destruction. the user invokes the static _create built-in in order to create new sidl objects. this will dynamically allocate memory for the new objects and initiate recursive initialization of the epv for each class in the inheritance hierarchy. babel will call a user accessible constructor (_ctor) once basic object initialization is complete. object destruction is implicit once the reference count goes to zero. again, a destructor (_dtor) is provided to the user for implementation-specific de-allocation."
"in the three types of input used in this study, the performance of the mlp was the best when we used deep features extracted from the floor plan images by the fine-tuned model in 3.4 as a part of the input. the performance of the mlp is improved by adding deep features of floor plans to input. therefore, considering floor plans seems to be effective in predicting the users' preference for properties. in addition, the performance of the mlp is higher with the fine-tuned model than that with the pre-trained model of imagenet [cit] as the extractor of deep features of floor plans. hence, it is considered that better deep features can be extracted by finetuning so that feature of the floor plans can be expressed well."
"it is important to note that implementations of sidl objects can be stateful. this is achieved via the data pointer shown in figure 8 . for procedural languages such as c or fortran, this pointer is exposed to the user and can be used to hold a private data structure. however, for objectoriented languages such as cþþ, java, or python, the user implements a regular class that may contain private data members. in these cases, babel implicitly manages a reference to an object of the user-provided type using the data pointer. the skeleton is responsible to cast this reference to the appropriate type and invoke the implementation within its context."
"babel's runtime library introduces some support for reflection, in particular runtime type information (rtti). this allows the programmer to determine the dynamic type and sidl version of a given object. types can be identified by name or via a class info data structure."
"there is also extensive support for dynamic loading and symbol resolution. symbols can be resolved ahead of time or as needed (lazy). the sidl runtime systems manages a library search path and keeps track of all libraries loaded through its interface. dynamic sidl libraries are identified by a sidl class file (scl). these files contain meta-data for an arbitrary number of dynamic libraries, allowing the loader to locate and identify them as needed. there is optional support for md5 and sha1 message digests to verify that libraries have not been modified or replaced."
"as most approaches, swig lacks the support for fortran and scientific data types. its primary purpose is to provide language-specific bindings for c/cþþ libraries. thus, there is a strong asymmetry between the native implementation and the supported client languages. swig uses so-called 'interface files' that contain c/cþþ style declarations as its input. swig can also be used to package structures and classes into proxy classes for the particular target language, exposing data structures in a more direct way."
enumerations (enum) provide a way to declare types with a limited range of values that can be referred by name instead of hard-codded values. c/cþþ developers will find the sidl syntax very familiar. concrete numeric values can either be provided in the declaration or are assigned automatically in a meaningful way.
chasm [cit] ) is a research effort to automatically create components from existing fortran modules. chasm generates adapter classes that mediate among the two languages. babel is a more general and flexible tool but builds on a stripped-down versions of chasm for some language backends.
"comments are optional annotations for adding documentation to the sidl and generated files, where appropriate. sidl supports basic java and javadoc/doxygen-style comments. two cases of the former appear in figure 4, while examples of the latter appear in figures 3 and 5 . these annotations embed class, interface, and method documentation directly in the sidl file. javadoc-style comments can also be used to create interface documentation and are automatically replicated in the generated files, where appropriate, whenever they directly precede the corresponding declaration."
"a key challenge to providing efficient runtime contract enforcement is minimizing performance overhead. this is accomplished in part by the generation of a second function pointer table. while the primary epv table contains pointers to functions defined in the skeleton layer. the second, or contracts, epv table contains pointers to the corresponding check routines."
"enforcement decisions are actually made by an enforcer class based on information about the clause under consideration and the enforcement policy options in affect. if an assertion within a clause is determined to be violated, then the appropriate exception is raised. statistics on enforcement decisions and violations are maintained. this is all handled automatically by the ior making the necessary calls to the enforcer class at the appropriate time."
"while some overhead is often unavoidable, converting to and from the ior is unnecessary if caller and callee are implemented in the same language. recent versions of babel implement an experimental feature that allows us to bypass the ior in this special case. experiments for cþþ show that the costs for native babel calls can be reduced to roughly the costs of native virtual function calls, effectively eliminating the overhead for native method invocations."
"babel is a programming language interoperability toolkit for high-performance scientific computing. it was designed to address specific functional and performance needs in the development of large-scale, multi-physics simulations involving the integration of multiple mathematical models, libraries, and solvers implemented in different programming languages. the inherent complexity of the resulting systems requires the aid of software tools for their development, evolution, and maintenance."
"interoperability solutions at the time babel was initially conceived, such as corba (common object request broker architecture) [cit] and com (component object model) [cit] ), tended to be geared more for general and commercial interests. that is, they generally lacked support for the legacy programming languages and native data types commonly used by computational scientists and engineers. features for aiding cross-language debugging were also missing."
"sidl provides a declarative description of the public methods of the calling interface as extensions of the scientific object model. the model is defined through base classes, interfaces, methods, exceptions, and built-in types. sidl, like the corba interface definition language (idl) provided by the object management group (omg) 2 [cit], is programming language neutral. both idls support the modular packaging of full method definitions specifying the type (e.g. integer, float) and mode (i.e. in, out, inout) of each parameter. both also support enumerations, arrays, and multiple inheritance of interfaces. unlike corba idl, sidl provides basic types for numeric complex and multi-dimensional, multi-strided arrays. another distinguishing feature is complete support for polymorphism across programming language boundaries. for example, python may be used to overload a specific method of a fortran module, throwing an exception implemented in cþþ. interface contract clauses with a rich set of expressions are also supported as an aid to testing and debugging."
"protocol buffers 7 are google's approach to object serialization and is used for remote procedure calls in googles internal software. protocols are specified in a file format similar to sidl and a compiler automatically generates the (de-)serialization code. they do not support fortran and also do not include a remote procedure call (rpc) mechanism. apache thrift [cit] ) is a very similar product developed by facebook. it includes an rpc mechanism but also lacks support for fortran. the main difference to babel is that babel is highly optimized for in-process language interoperability, where code in another language is directly called without having to serialize the arguments to transmit them over a network. both formats are, however, very interesting alternatives to the babel rmi protocol."
"to allow for more flexible interfaces, sidl supports the notion of generic arrays, which are array declarations with no type or dimension information. meta data such as length and dimension is available using an array application programming interface (api) in each of the language bindings. generic arrays are useful to handle a wide range of arguments (e.g. for (de)serialization or logging)."
"frequency options further restrict enforcement of the clauses satisfying the classification option. the traditional all-or-nothing approach is supported through the always and never options, respectively. simple sampling strategies are reflected in periodic and random sampling, where the period (or number) is specified at runtime. the final options focus on performance-driven adaptive enforcement with the goal of reducing the contract enforcement overhead. the two basic adaptive strategies are: adaptive fit and adaptive timing. both rely on execution time estimates for contract clauses and methods. adaptive fit checks clauses whose execution time will not result in exceeding the desired overhead limit when compared with the execution time of the method. adaptive timing, on the other hand, factors in the cumulative estimates of time spent in methods and contract checking, enforcing contracts so long as the clause does not result in exceeding the desired limit."
"each clause corresponds to a different set of enforcement points. precondition and postcondition clauses, indicated by the require and ensure keywords, respectively, apply to methods. a precondition declares constraints on invocation of a method while a postcondition constrains its effects. in some cases, there may be properties that need to hold throughout the life of an instance of a class. rather than require the assertions be specified in the precondition and postcondition clauses of every method, the class invariant, which is specified at the interface and class level, is used for these properties. it is important to keep in mind that interface assertions only need to hold at the method call boundary. depending on the nature of the algorithm, it may be necessary for the implementation to temporarily violate the contract during method processing. however, as long as the corresponding assertions hold at the call boundary, the contract is not technically violated."
"in order to support assertions on object properties, which are not visible to the interface, function calls may be specified in contract clauses. it is very important that such methods be side-effect-free because the implementation cannot assume contracts will actually be enforced during runtime. the functions may be user-defined methods that are in scope or any of 20 built-in functions listed in table 1 . built-in array accessor and simple numeric value comparator function operate in constant-time. other array-based operations, including existential and universal quantifiers, are linear in the size of the arrays. contracts may also include user-defined functions (i.e. methods returning a value) in scope whose contract includes the is pure annotation in its postcondition clause. the annotation indicates figure 5 . sidl fragment illustrating precondition and postcondition clauses for a vector dot product operation. the preconditions for vudot require the two normal sidl arrays, u and v, both be non-null, one-dimensional arrays of the same size. the tolerance value must also be non-negative. the postcondition clause indicates all implementations of the method must ensure the following, assuming the preconditions are satisfied: (1) if u and v are equal then the result of calling vudot will be non-negative; and (2) if u and v are both zero vectors then the result will be within the specified tolerance of o.o. the is pure assertion within the postcondition clause indicates implementations should be side-effect-free so they can be specified within the contract clause of another method; however, the assertion is not executable since babel does not statically analyze associated implementation(s)."
"the sidl runtime library essentially provides languagespecific implementations of language-independent sidl constructs. it provides basic operations and capabilities associated with sidl types, such as casting, reference counting, reflection, and implicit exceptions. the runtime also supports object and library management as well as interface contract enforcement."
"classes define a set of methods a caller can invoke on an object. unless explicitly abstract, classes have implementations of each member function. abstract classes, however, have at least one unimplemented method in order to preclude their instantiation. figure 3 illustrates the definition of one abstract class, algorithm, and two concrete classes, list and quicksort. data members cannot be declared in sidl; therefore, the only way to pass data in or out of objects is through methods."
"sidl also supports the usual notions of static methods (i.e.conceptually part of the class but do not execute in the context of a concrete object) and overloaded methods (i.e. methods with the same (base) name but different arguments). the specification of the method name must include an extension, as shown in figure 4, to disambiguate names in languages that do not support overloading. for languages with built-in support for method overloading, such as cþþ and java, the short name is used while for the remaining languages, such as c, fortran, or python, the userspecified suffix is appended to provide an unique identifier."
"the scientific computing community needs a tailored, high-performance language interoperability toolkit to facilitate the development of complex, large-scale, multiphysics, and multi-resolution applications. traditional scientific and modern programming languages must be supported to accommodate legacy and new codes. dynamic, multi-dimensional arrays are critical to the numerically intensive codes that must interoperate. ensuring these multiple programming language applications work correctly requires mechanisms, such as interface contracts, for cross-language debugging."
"we predict preference by content-based filtering using similarity of attributes for both users and properties. we define u and i as the attribute data of a user and a property, respectively, and let the positive evaluation value be w (the ratio between the number of the negative evaluations and the number of the positive evaluations in the train set) and the negative evaluation value be −1. using a pair (u, i) in the train set whose evaluation value r ui is known and a pair (u t est, i t est ) in the test set, we calculate the cosine similarity cos(u t est, u) for each u, and let u c bf be the set of top k u % of all u by similarity. similarly, we calculate the cosine similarity cos(i t est, i) for each i, and let i c bf be the set of top k i % of all i by similarity. further, we calculate the predicted evaluation value v c bf for the pair (u t est, i t est ) in the test set as (5) ."
"true if the real value x falls within the specified tolerance, t, of the range r low ..r high . range(u, r low, r high, t)"
keeping implementations up to date with evolving interface specifications can be a challenging problem. babel provides some assistance therefore via so-called splicer blocks. splicer blocks are structured comments that mark the beginning and end of user-modifiable sections within a file. babel will preserve these sections across repeated invocations. all changes outside these splicer blocks may be lost. babel automatically adds new methods and changes prototypes of existing methods as necessary.
"interfaces define a set of methods a caller can invoke on an object of a class implementing the methods. figure 3 illustrates the definition of two interfaces: comparator and container. interfaces are akin to java interfaces or pure abstract base classes in cþþ. all member functions of an interface are implicitly abstract. like java and c#, only single inheritance and multiple implementation of interfaces is supported to avoid major complications and ambiguities caused by multiple inheritance."
"babel is an idl-based tool similar to industry's corba/ ccm [cit], microsoft's (d)com [cit] and .net 5, mozilla's xpcom 6, and sun's javabeans [cit] . there are several existing projects providing language interoperability among a limited set of languages: the most important are briefly discussed in the following."
"methods define routines available for invocation by a caller. they represent the public interface of an object, therefore all methods implicitly have public visibility. there is no notion of private or protected member functions. sidl methods are by default virtual. this means that the actual method being called is determined at runtime, based on the concrete type of an object. methods may be declared final to prevent them from being overridden."
babel supports an alternative tool-friendly xml representation for sidl interfaces. there is both a xml frontend and backend. this means that xml can be used as an input language for babel. babel can also be used to convert generic sidl files into xml specifications. xml files retain references to the original sidl file such as line number information. both formats are largely equivalent and have uniform support for doc-comments.
"babel is an open-source scientific language interoperability toolkit, distributed under the lgpl license, with open bug tracking and version control systems. the latest stable babel release is version 2.0. babel is written in java using the javacc parser generator for the sidl frontend and xerces for xml parsing. the build and configure system is based on autoconf, automake, and libtool."
"rmi in babel is fully transparent to the user. this means that the user's code stays exactly the same, no matter whether an object is local or remote. babel provides the built-ins _isremote and _islocal to distinguish among the two cases."
"thomas gw epperly, phd, is the project leader for the components project in the center for applied scientific computing (casc) at the lawrence livermore national"
"babel and/or the cca has been successfully used in a large number of projects in various areas such as chemistry, geomagnetics, sparse linear algebra, fusion, or nuclear plant simulation. discussing these projects is beyond the scope of this paper. a recent overview can be found elsewhere )."
"there are 76,871 positive data (35%) and 223,223 negative data (65%) in the total 220,094 evaluation data. there is a bias in the dataset; thus, we used the matthews correlation coefficient (mcc) [cit] as evaluation metric to evaluate equally. mcc is calculated as (7) when the number of true positive in the prediction result is t p, the number of true negative is t n, the number of false positive is f p, and the number of false negative is f n . the maximum value of mcc is 1, the minimum value is -1, and a larger value represents that performance is better."
"sidl is a programming-language-neutral specification language used to define the calling interface. sidl specifications are formed from eight main elements: packages, interfaces, classes, methods, exceptions, contract clauses, types, and comments [cit] . figure 3 shows an example sidl specification. the remainder of this section describes each element."
"babel features a complete set of fundamental types, listed in table 2, including single-and double-precision complex numbers. basic types such as bool, int, or float are fixed size with a native equivalent in most target languages. this is different from languages such as c where only a hierarchy among types is defined (e.g. the actual size of type int in ansi c is completely implementation dependent but has to be at most the size of long). whenever possible, more complex types, such as fcomplex or string, are mapped to native language equivalents. otherwise, a sensible data structure is provided with a language-dependent runtime library. strings often involve a high runtime overhead as they are represented differently by practically every programming language."
"in the presence of contract clauses, the babel compiler adds check routines to the ior to support enforcement. individual assertions within a contract clause results in the generation of a corresponding check in the routine. preconditions, if any, are grouped together under a single (compound) if-statement to ensure they are only executed when allowed by the current policy. any invariants are grouped in the same manner before the call to the original skeleton method. finally, postcondition and invariant checks are generated. hence, all specified contracts are translated into enforcement checks within the new routines, with enforcement decisions being made based on runtime options."
"by performing such scaling, we obtain the predicted evaluation value v h f, which emphasizes the two methods to the same extent, and like other methods, we predict preference as positive when the predicted evaluation value v h f is larger than zero, and predict it as negative otherwise."
"apart from glue code generation, babel provides assistance in developing and maintaining implementations for sidl specifications. this includes the generation of makefile templates as well as a set of implementation files, which are the only babel generated files the user is expected to modify. implementation files serve as a starting point for developers and contain all of the necessary declarations and prototypes."
"in many respects, the design considerations are very different from the requirements of the scientific computing community. the main differences and limitations of each approach will be discussed individually."
"babel supports virtual function calls even on top of procedural languages such as fortran 77. consequently, it cannot rely on language features. instead, it implements its own virtual function table and generates the necessary dispatch code for the various supported languages in the client stubs. dynamic dispatch using virtual function tables was first introduced in simula [cit] and is today the preferred technique for widely used languages such as cþþ [cit] ."
"in most language bindings, stubs are implemented in c. the c backend itself is a special case as the ior is already implemented in c. thus, no argument conversions are necessary. the stub directly dispatches calls to the usersupplied implementation. for cþþ, stubs are member functions of a wrapper class closely resembling the sidl declaration. wrapper classes are also used for java and python. however, the implementation uses the particular c native interface, i.e. jni in the case of java and c extension types in the case of python. fortran 77 and fortran 90/95 bindings are technically challenging as there is neither a native c interface nor a well-defined binary representation. instead, it depends on the particular compiler how data structures are laid out in memory. babel uses a strippeddown version of chasm [cit] stubs are also used by babel in order to expose built-in sidl features, such as reference counting and up-/downcasting, in a way that integrates nicely with the native language ecosystem. for example, wrapper classes for cþþ and python implement smart pointer semantics freeing the user from error-prone explicit reference counting."
"restricting our approach to the least common denominator across supported languages would result in the loss of critical features in both traditional and modern programming languages. combining scientific data types with the discipline of object-oriented encapsulation led to the development of technologies based on a scientific object model. the resulting babel toolkit consists of three parts: a programming language-neutral interface specification language, compiler, and runtime library. the scientific interface definition language (sidl), developed at llnl, is the specification language. the babel compiler translates sidl specifications into language-specific glue code, prototypes, and documentation. the supporting library is referred to as the 'sidl runtime library'. figure 2 shows the major parts of the toolkit in relation to their use in a scientific software artifact."
"furthermore, there is a private data pointer in each object that can be used to hold state. the meaning of this pointer depends on the particular implementation language. in simple cases such as c or fortran, it is just an opaque data pointer provided to the user. for higher-level languages such as cþþ, it is usually a reference to an object of a user-modified class."
"another very common use case for babel is to provide bindings to scientific libraries for various languages. one example therefore is hypre: a suite of scalable parallel linear solvers and preconditioners for sparse linear equation systems [cit] . the user is not even necessarily aware that he is using babel. languageindependent sidl specifications have also proven to be a valuable tool for stable interfaces. furthermore, hypre developers were able to consolidate multiple versions of algorithms that only differed in implementation details for matrix multiplication using polymorphic interfaces. this shows that babel can be used to provide benefits of object-oriented design while avoiding portability problems of real object-oriented languages such as cþþ."
"xpcom is a similar approach used by mozilla to connect javascript and cþþ components for their products, most notably the firefox web browser. owing to its high overhead for data marshaling, due to the selection of supported languages, it is also not well suitable for highperformance computing."
"there are n stages and m rows in a tcro puf cell that can be used to generate the frequency as illustrated in figure 10 . assume that k tristate inverters are selected from the n-th stage of the delay matrix, the permutation and combination for selecting the tristate inverters can be represented as"
"for future works, we will focus on the finite-time threshold calculation and the finite-time synchronization for complex networks with other external disturbance such as time-varying delays, mixed delays, parameter uncertainties and so on. in addition, the applications of the synchronization for the complex networks in secure communication and image encryption fields are also the focus of future research."
"hence, all selection possibilities of the tristate inverters in n . in order to generate a 1-bit response bit, two identical tcro matrices over l matrices are utilised and compared. therefore, the number of selection possibilities of the tristate inverters over n stages can be defined as equation 5 ."
"therefore, for convenience, we denote x i (t) and x i (t − τ ) as x i and x i (τ ), the complex-valued network can be written as an equivalent real-valued system"
"a comparison of the tsram puf and proposed tcro puf with other pufs is listed in table 1 . the tsram and tcro puf designs achieve better uniqueness and reliability results from both asic simulations and fpga platforms than previous memory based pufs and delay based pufs. particularly, with the configurable bits, both the tsram puf and tcro puf offer numerous configuration options, which increases the number of crps exponentially. also, in term of hardware consumptions, the tsram puf and tcro puf use a much lower number of resources to generate a one bit response compared with the previous best puf designs [cit] ."
"the uniformity of a puf design measures the proportion of one and zero bits in a response, from which the likelihood of each value can be derived. if a design returns responses that are ideally random, then the distribution of bit values will be equal between ones and zeros. having this property is essential from a security perspective to prevent an attacker from guessing if a response of a particular device is biased towards a particular value. to estimate the uniformity, it is simply a matter of finding the hamming weight (hw) of a response, which will give the ratio of ones and zeros, as well any biases in the design of the puf cell itself as each bit is independent."
"to validate the functionality and performance of the proposed tcro and tsram puf designs, simulations using umc 65nm technology and practical implementation on a xilinx virtex-ii field programmable gate array (fpga) device are evaluated. experimental results show that the proposed designs use the smallest number of hardware resources compared with previous work. reliability experiments under temperature and voltage variations demonstrate good robustness of the proposed designs."
"where r(i, t) is the t−th sample of r i . the percentage figure of merit for reliability can be defined as equation 3 ."
"in contrast to current existing improvements that either employ a ro to generate puf crps, the quantity of tristate inverters can be flexibly selected and utilized in a ro. the main advantages of the proposed tcro puf design are as follows: 1) high efficiency in term of hardware cost. the proposed design improves the efficiency of every single transistor used to compose the tcro puf structure. due to the tristate inverters, the configurable signal of the tcro puf design enables every inverter to contribute to the crps. hence, the proposed tcro puf design, based on the same amount of the tristate inverters, has a large number of crps; 2) high flexibility. more than two tristate inverters are connected in parallel to form a new architecture whose output is non-linear; 3) low cost and lightweight. the proposed tcro puf design achieves the same number of crps by using less transistors compared to the previous designs. hence, the proposed tcro puf is very lightweight, and is suitable for resource constrained applications, e.g. iot devices."
"in this section, illustration examples are performed to show the effectiveness of the proposed schemes obtained in the previous section. we consider the complex-valued networks with 6 nodes, and choose node dynamics being the following complex-valued lorenz chaotic system [cit]     ẋ"
"as the output response of a puf will be used for security applications, e.g. device authentication and key generation, the response of every chip should be unpredictable. uniqueness evaluates how easily the responses of different puf implementations can be differentiated when the same challenge input is used. a percentage measurement for uniqueness based on average inter-chip hamming distance (hd) can be defined according to equation 1. two chips i and j among k devices implement the same puf circuit and derive two n−bit responses, r i and r j, from the same challenge c."
"a puf design should always produce the same response to the same challenge. however, variations in the supply voltage and temperature can affect the response. reliability assesses the robustness of a puf design under different environmental conditions. we use the percentage of the number of unstable bits to measure a puf's reliability, which can be defined by finding the average intra-chip hd of s n−bit responses as in equation 2"
"tristate inverters are very common in cmos designs and can be easily implemented in asic designs. in order to evaluate the performance of the proposed puf structure in asic, cadence 6.1 is employed to carry out simulations with umc 65 nm technology assuming a 1.1v supply voltage. monte carlo simulation is utilised to simulate the process variation. the output responses are processed with matlab to evaluate the puf metrics, e.g. uniqueness. reliability cannot be evaluated from the simulation results."
"the rest is organized as follows. network model and mathematical preliminaries are given in section ii. main results are deduced in section iii. simulation examples are given to verify results in section iv, conclusion and future works are presented in section v and section vi,."
"remark 2: many complex-valued chaotic systems, such as the complex-valued chen system, complex-valued lorenz system, complex-valued chua system, satisfy the lipchitzlike condition, and we will give detailed illustration in the simulation example."
"the 1-bit tsram puf design proposed in our previous work [cit], consists of two identical cross-coupled tristate inverter arrays, is shown in figure 7 . each array contains n parallel tristate inverters. when none of the tristate inverters is enabled, the output of the tsram puf circuit is in a state of high impedance. when the challenge signal contains one or more enable signals, tristate inverters are selected from the two arrays to form an effective sram puf cell, forcing the circuit to settle down to one of the two stable states, i.e. '0' or '1'. for an identical cross-coupled loop, the same number of tristate inverters, at least one in each array, should be enabled at the same time in each challenge part. when only one tristate inverter is selected from each array, the tsram puf operates like a conventional sram puf. once the tristate inverters are enabled, the tsram puf cell produces a 1-bit response. ideally, the tsram puf cell with two physically identical inverters is logically undetermined. due to the physical mismatch and the electrical noise in a practical implementation, the cell will converge to one of the two stable states. if two or more tristate inverters are selected from each array, the additional current generated could enhance the uniqueness of the response. this is demonstrated in the next section. the number of tristate inverters selected is determined by the challenges. this reconfigurable architecture enables the tsram puf to generate effective crps without the need for additional auxiliary processing, such as a oneway function or stream cipher [cit], as required by the conventional sram puf aiming to obtain multiple crps. the tsram puf can efficiently reduce the hardware resource consumption and the system complexity. a general tsram puf architecture is shown in figure 8, where c-i, c-ii and rare used to represent challenge part i, challenge part ii and the response. the challenge signal determines the number of tristate inverters that are selected to form the metastable loop. the challenge input for each tsram puf cell can be the same or different. if the application demands more crps, then different challenges can be applied to c-i and c-ii. otherwise, the challenges for each puf cell can be the same."
"in this paper, consider the complex-valued network with coupling delay consisting of n nodes, in which all nodes are n dimensional and have identical dynamics. they can be described aṡ"
"during the past decades, the theory of complex networks has gained a lot of interests from a variety of directions including spreading dynamics, community structure, evolution game, control synchronization, and so on [cit] . amongst all the directions, the investigation of control synchronization has attracted considerable attentions because of its promising applications in the secure communication, image encryption, information processing and other fields [cit] . synchronization refers to a process that all the nodes seek to adjust a certain property of their motion to a common behavior as time evolves. specially, many kinds of synchronization patterns have been put forward such as exponential synchronization [cit], cluster synchronization [cit], projective synchronization [cit], and robust synchronization [cit] . even though many results on synchronization of complex networks have been obtained, most of them are to realize the synchronization as time goes to infinity. in practical engineering, however, to realize the synchronization of complex networks in a desired finite time, rather than merely asymptotically synchronize, is more valuable and greater significance."
"configurable bits are used to enable the tristate inverter in the proposed design. according to the configurable bits, different tristate inverters of the tcro puf cell are selected to feed into the circuit and contribute to the output frequency. the configurable bits of each column in the cell should have at least one bit equal to 1 to ensure the signal can propagate to the final output. due to the process variations in devices, the responses of each cell will exhibit differences when applying the same configurable bits. when only one tristate inverter is activated in each column, the tcro puf cell is equivalent to a conventional ro puf. in the proposed tcro puf design, the activation of a tristate inverter provides an additional charging (discharging) current to the capacitive load to effect the time delay on each column stage. hence, it introduces a reduction of the oscillation period and thus increases the frequency of the tristate ro, which makes the tcro puf more reliable."
"remark 3: from the proof of theorem 1, we will give the method of selecting parameters and do some illustrations. on one hand, the choice of parameters α r and α i in controller (5) are decided by the inequalities (6), and α r e r i (t) and α i e i i (t) can make response-network (3) synchronize to drive-network (2) . on the other hand, the convergence time is closely related to the parameters β and γ, and they can make the synchronization achieved under settling time shown as (7). theorem 3.1 provides a finite-time synchronization criterion for the complex networks with coupling delays. in the following results, we derived the synchronization conditions for the complex network without coupling delay by extending theorem 3.1"
"motivated by the above discussions, we develop a fully complex-valued network with coupling delay. an equivalent real-valued system is built by decomposing the complexvalued network into real and imaginary parts. meanwhile, finite-time synchronization of the proposed complex-valued networks is explored. by designing the simple linear feedback controllers, some sufficient conditions are rigorously induced to guarantee the synchronization in a given finite time. numerical examples are shown to verify the effectiveness of our results. compared with the existing works, there are mainly two contributions in our paper. on the one hand, the complex network model is new. the complex network is extended to the fully complex-valued network, in which the state variables, system function, inner coupling matrix and outer coupling matrix are considered as complex-valued. the couplings between three complex-valued terms (node states, inner coupling matrix and outer coupling matrix) increase the complexity of its structures, while there are couplings of no more than two complex-valued terms, see [cit] and references therein. on the other hand, under the effect of designed controllers, the synchronization between the driveresponse complex-valued network with or without coupling delays can be achieved in a given finite time, rather than merely asymptotically synchronize."
"for the cro puf [cit] as shown in figure 2 (a), assuming that it has m cro cells and n stages on each cell, the number of response bits is calculated as described in m 2 · 2 n−1 · 2 n−1 . based on this, the ce is computed as described in equation 8 ."
"a cross-coupled tristate inverter based sram puf, tsram puf, was proposed in our previous work [cit] . the tristate inverters introduce a mechanism that can reconfigure the sram cell, which produces effective crps without using any additional auxiliary processing. in this paper, we propose a novel configurable puf architecture based on a tristate inverter matrix, which reframes the design of the conventional ro puf design. a configurable delay unit composed of a tristate inverter matrix is used to replace the ros in the ro puf and the memory cell in the sram puf. the configurable bits are able to select a subset of the tristate inverters in the delay unit. by using this strategy, every tristate inverter in the puf design can be fully used to enhance the flexibility of the puf design. the new scheme can generate more crps at an exponential order compared to the conventional ro puf designs while significantly reducing the hardware area consumption."
"in the numerical simulations, we assume the outer coupling matrix and inner coupling matrix are then the real and imaginary parts of the complex-valued matrices are respectively"
"ro puf is one of the most promising designs due to its reconfigurability, high uniqueness and reliability. ro puf is composed of ro pairs based on the basic ro unit [cit] . to generate a single bit response of a conventional ro puf design, two symmetrical and route-balanced ros are used to produce two different frequencies and one 1-bit response is decided by comparing two frequencies. a counter and a comparator are used to generate one bit output, either '0' or '1'. this architecture incurs large power and area overheads. configurable ro pufs have been proposed to improve the reliability and hardware resource usage of ro puf [cit], where multiplexers (muxs) are used to select one of two inverters and thus the number of crps increased and the hardware consumption is decreased."
"in order to evaluate the performance of the tcro puf in fpga, a tristate matrix of 3 columns and 3 rows, as shown in figure 20, is created based on the inverter and the internal tristate gate on a xc2vp30. to avoid routing mismatch, the tristate matrix is well balanced by manual routing. a hardmacro of the tristate matrix is also created using xilinx fpga editor. in each xc2vp30, 128 tristate matrixes are built to generate a 64-bit response. figure 21 shows the evaluation results of the proposed tcro puf, in which figure 21 (a) exhibits the uniqueness result that from the xilinx virtex ii fpga. it can be seen that the proposed tcro puf achieves the uniqueness result of 48.30%. figure 21(b) shows the reliability of the proposed tcro puf design over different operating conditions. it can be seen that the average reliability results of the proposed design are approximately 95.27%. moreover, the responses also have a good uniformity of 37.43%."
"in the conventional ro puf [cit], the ro is composed of n inverters, and two of m ros are selected to generate a 1-bit puf response. typically, one inverter consists of two transistors. hence, the ce of a conventional ro puf can be depicted as equation 7 ."
"in this paper, we develop a fully complex-valued network with the coupling delay, complex-valued state variables, complex-valued system function, complex-valued inner coupling matrix and complex-valued outer coupling matrix. unlike the existing works, the inner coupling of the referred complex-valued network is linear complex-valued coupling, which leads to the coupling between three complex-valued terms (node states, inner coupling matrix and outer coupling matrix) and greatly increases the complexity of its structures. meanwhile, finite-time synchronization of the referred complex-valued network with or without coupling delay are explored. based on the finite-time stability theory, suitable controllers are designed to guarantee the synchronization realized in a given finite time, and several synchronization protocols are also rigorously induced."
"an inverter is a common component in a puf structure, e.g. inverters are used in conventional ro puf designs and the sram puf designs. in our proposed designs, a normal inverter is replaced by a tristate inverter. every tristate inverter, as shown in figure 6 (a), has an enable signal to activate the operation. figure 6 (b) shows the transistor level of a tristate inverter. when the signal en is set as '0', both input related transistors are disabled, which leaves the output floating, producing a high impedance output. in contrast, when en is '1', both input related transistors are enabled, and the tristate inverter is equivalent to a common inverter."
"in this section, some sufficient conditions of finite-time synchronization for the fully complex-valued networks with coupling delay or without delay are derived by strict proof."
"the three main contributions of this paper are summarized as follows: the rest of the paper is organized as follows. section ii provides background on related puf structures. section iii gives the preliminaries of the design. section iv presents the detailed structure and circuit of the tsram puf and proposed reconfigurable tcro puf. section v evaluates the simulation performance of the proposed puf in cadence with umc 65nm technology. the implementation of the tsram puf and proposed tcro puf on fpga is given in section vi. finally, a conclusion is provided in section vii."
"theorem 1: suppose the assumption 1 holds, and under the effect of controllers (5), the finite-time synchronization between the drive-network (2) and the response-network (3) can be realized if the controller parameters α r and α i are properly chosen to satisfy"
"the transmission delay of a tristate inverter is higher than a normal inverter, and consequently the output frequency of the tristate matrix will be slower compared with a conventional ring oscillator. figure 15 shows the monte carlo simulation for the tcro puf, where the output frequency changes with the process variation. it is clear that the frequency of the proposed puf structure is much slower than a traditional ro. at the same time, the output frequency of the proposed structure will change with the process. after obtain a sufficient number of crps, the uniqueness is calculated in matlab. the uniqueness result is shown in figure 16 . it can be seen that the average uniqueness value is 49.69%, very close to the ideal value of 50%, indicating that the tcro puf also performs well in differentiate devices. based on the simulation responses, the uniformity is calculated and the value is 52.45%."
"the cost efficiency (ce), introduced here to measure the efficiency of the proposed tcro puf design, is defined as the number of gates (n gate ) per response bit (n bits ) as shown in equation 6 ."
"nowadays non-volatile memory (nvm) based security mechanisms are widely used in conventional security systems, in which binary encrypted keys are stored and authenticated to access stored secret information. however, with the development of attacking techniques, e.g. side channel analysis (sca), the keys stored in nvm are vulnerable to adversaries [cit] . to address this issue, puf designs have been investigated by researchers to improve hardware security [cit] . a puf is a security primitive that utilizes unpredictable fabrication variations to encrypt integrated circuits (ics) to provide unique identifying information. the random variations in chips that are produced under the same fabrication process can lead to different unique responses when presented with the same input challenge. when an input (challenge) is sent to a puf circuit, a unique output (response) will be generated. pufs can use these crps to authenticate devices and distinguish genuine devices from fake ones. hence, pufs can be applied to key generation [cit], radio-frequency identification (rfid) security [cit] and ip protection [cit] . commonly, pufs are categorized into delay-based pufs and memory-based pufs [cit] . delay-based pufs focus on extracting the differences in the propagation delay of signals and memory-based pufs detect the instability in memory cells when powered up. to date, a number of delay-based puf designs have been proposed to exploit the various types of fabrication variations in ic, e.g. arbiter puf [cit] and ro puf [cit] . memory-based puf designs have been proposed including static ram (sram) puf [cit], butterfly puf [cit], fpga id generator [cit], etc.."
"for the cro puf [cit] as shown in figure 2 (b), based on m cro cells and n stages on each cell, the ce can be described as equation 9 ."
"suppose the state variables, system function, outer coupling matrix and inner coupling matrix are complex values including real and imaginary parts. for every node state x i (t), denote"
"recent research has been devoted to study integral sliding mode (ism) methods, which enables to generate an ideal sliding mode of the controlled system starting from the initial time instant t 0 . a sliding mode is defined integral sliding mode if the system, while sliding, is of the same order as the original system [cit] . ism requires to split the control variable into two parts"
"in this section, we evaluate the performance of of-fl and compare it against the performance of the two other objective functions, namely of0 and mrhof."
"the present paper is organized as follows. in section ii, higher order sliding modes and integral higher order sliding modes are reviewed with reference to a siso uncertain dynamical system. in section iii, the integral high order sliding mode control approach is extended to the suboptimal control approach, and the new algorithm is presented. in section iv, the kinematical and dynamical models of a three joints planar robot manipulator are introduced, and the proposed motion control scheme is described. the final part of the paper is devoted to present simulation results. some conclusions (section v) end the paper."
"therefore, the whole system is reduced to the union of three decoupled uncertain smooth siso systems. now we design the controller c relying on the previously described integral suboptimal second order sliding mode control approach. according to this latter, a sliding variable for each siso system is selected as"
"given an audit log of -who‖ did -what‖ to -which‖ data -when‖ and -how‖ and an effective means of processing the log, auditing can answer -why‖. by answering the question of -why‖, it provides one of the means to detect intrusions into the system [cit] . malicious access to protected data can cause major damage to organizations and also problems for the public. [cit], 29% of the incidents of intrusion of database system are done by individuals inside organizations that have privileges to freely access data. therefore, auditing systems must include the ability to prevent and detect internal threats to the system [cit] ."
"to feedback linearize the nonlinear system (27), the classical inverse dynamics control approach [cit] has been adopted. the inverse dynamics of a rigid robot manipulator can be written in the joint space as a non linear relationship between the plant inputs and the plant outputs, relying on (27) and (28), so that the control law results in being"
"let us consider the following example to understand the importance of metrics combination. first, the combination of the delay and the hop count allows assessing the number of hops from the router node to the dag root while taking into account its delay. now, assume that a node whose path to the root has the lowest delay and at the same time the nearest to the dag root. this situation makes that node the best candidate for the parent among the set of the other neighbors. however, using this parent node in a repetitive way may rapidly deplete its battery level. consequently, it is also crucial to consider the node energy in the selection of the best parent in order to avoid overusing a node until it vanishes its energy, and thus, extending the node's availability and the network lifetime in general."
"3) end to end delay: figure 4 shows the average end to end delay of the three objective functions of0, mrhof with path etx and of-fl based on the number of hops to the dag root. the delay is measured as the time between sending a frame and the reception of its acknowledgement. according to the figure, the three objective functions keep an average endto-end delay under 4 seconds. however, of0 based network induces more latency than the two other schemes, although it allows to minimize the hop count. this result demonstrates that the shortest path does not mean the path that induces less latency, as some nodes may be congested in the shortest path."
"the use of artificial intelligence techniques to support the decision making process is in fact widely used in the recent research works in relation with low power and lossy wireless sensor networks. artificial intelligence techniques reinforce the efficiency and performance of routing protocols, by combining data from nodes and their interactions in order to make decision to improve the global network performance. since we aim to decide about the best parent among the list of neighbors depending on the application requirements in an efficient way, our proposal makes use of fuzzy logic as it plays an important role in decision making. there are many advantages of fuzzy logic:"
"linear dimension reduction techniques have been applied over r i, ∀i, in order to further reduce its dimension and class separability. a linear discriminant analysis has been conducted to obtain a final 30 feature feature set assigned to each subject, f i . classification of an unknown subject, f u, has been performed using a minimum distance criterion: min"
"a user is defined as a human being. although the concept of a user can be extended to include machines, networks, or intelligent autonomous agents, the definition is limited to a person in this document for simplicity reasons."
"-hop count: this metric refers to the number of hops between the neighbor node (i.e. the candidate parent) and the root. for real-time applications, it is effective to reach destination through the smallest possible number of hops. however, it may happen that some shortest paths experience larger delays and higher energy dissipation due to congestion/overload. thus, combining this metric with the delay will be more effective for real-time applications by optimizing the selection process of routes."
"semantic web is not a separate web but an extension of the current one, in which information is given well-defined meaning, better enabling computers and people to work in cooperation [cit] . semantic web having well structured data provides a new solution for producing new knowledge based on existing knowledge."
"dsd relations differ from ssd relations by the context in which these limitations are imposed. this model component defines dsd properties that limit the availability of the permissions over a user's permission space by placing constraints on the roles that can be activated within or across a user's sessions. dsd properties provide extended support for the principle of least privilege in that each user has different levels of permission at different times, depending on the role being performed. these properties ensure that permissions do not persist beyond the time that they are required for performance of duty. this aspect of least privilege is often referred to as timely revocation of trust. dynamic revocation of permissions can be a rather complex issue without the facilities of dynamic separation of duty, and as such it has been generally ignored in the past for reasons of expediency [cit] ."
"(3)meta-synthestive knowledge processing: the three main elements of knowledge processing system -content, machine and man-can be integrated seamlessly in web2.0 environments. the increasing popularity of internet technology highlights limitations of human brain in the speed, accuracy, strength, storage capacity, storage time and standardization of knowledge processing. as a result, computer is becoming an alternative tool of knowledge processing. the abilities of human brain and computers in knowledge processing are complementary with each other so that man-machine collaborative knowledge processing will be one of the basic models for computing knowledge on the web. table 1 conducts a comparison between the semantic web and web2.0 from a knowledge processing perspective. there is a growing awareness that integrating semantic web with web2.0 is reasonable and practical [cit] .web3.0, which is the third wave to hit the web in future, should be an integration of semantic web into web2.0 [cit] . the semantic web and web2.0 are complementary with each other [cit], for example the semantic web can be used for linking and reusing data across web 2.0 communities [cit] . table 2 . a comparation between the semantic web and web2.0"
"the expected transmission count (etx) membership function is an important routing metric that indicates the reliability of a path. as shown in figure (1c), the etx path metric belongs to the range [0..100]. we concluded this range from a set of simulations performed with different scenarios [cit] ."
"five time series [cit] have been be used to evaluate our methods. sequential and \"shuffle\" ways to obtain train and validation subsets are evaluated into the system. forecasted values are compared with real values (i.e. test set) and two error values are used: mse (mean squared error) and smape (symmetric mean absolute percent error) [cit] . the results are shown in table 1 ."
"-link quality: this metric is measured by the expected number of retransmissions (etx) as an indicator of the link quality between the node and its neighbor. a good link should have etx values close to zero. high etx values mean that the link is unreliable as the number of retransmissions goes high. this metric should have significant weight for applications with reliability requirements. other link quality metrics such as received signal strength (rss) or link quality level (lql) may also be considered as link quality estimators [cit] . with respect to lql, it is not recommended to consider it as link quality estimator as it maps the link quality to seven values (from 1 to 7), which only gives a rough classification of the current link metric."
so a model is needed that not only provides features of rbac but also should be invulnerable to sql injection attacks. also it should provide some mechanism to keep an eye on the activities that an individual user perform in system. in this paper a model -authentication & auditing enabled role based access control (aarbac) is presented that integrates features of rbac with auditing and authentication mechanisms invulnerable to sql injection attacks. the rest of the paper is organized as follows: in section 2 standard rbac model is discussed. in section 3 model aarbac is discussed along with description of its various components. sections 4 and 5 discusses about how authentication and auditing can be integrated in simplest ways with rbac respectively. section 6 concludes the paper.
"knowledge life cycle (klc) is one of the most important concepts in new generation knowledge management (ngkm). the ngkm for the first time allows for the production of new knowledge in knowledge management, while the first generation knowledge management concerns itself mainly with the distribution, sharing and use of existing knowledge. existing knowledge can be categorized into knowledge in human brain and knowledge stored on the web. in this paper, the topic is just limited to the existing knowledge, namely knowledge stored on the web. the production of new knowledge form existing knowledge mainly relies on the technologies of artificial intelligence (ai) to date. however, the ai hasn't made much progress with knowledge production yet."
"the current article presents a robust solution to person recognition using information provided by multiple views. in order to overcome appearance variations due to perspective, a 3d voxel reconstruction of the scene is obtained. information provided by a tracking system allows estimating the centroid and orientation of the subject under study and translating and rotating its associated volume to a common spatial reference frame. the obtained set is scale, translation and rotation invariant. time alignment is achieved by estimating the walking cycle out of the obtained invariant volume data. the hyperspherical radon transform is introduced as a robust technique to analyze spatio-temporal data by integrating the aligned set through a set of hyperplanes that integrate information from both space and time. a first dimension reduction is performed through a variance analysis for feature selection and the lda algorithm is applied afterwards. finally, effectiveness of the proposed algorithm is assessed by mean on quantitative metrics over an annotated multi-camera dataset. real-time performance of the proposed algorithm is also achieved proving its validity for real systems. figure 1 . multi-camera input data sample. in (a), a sample of the original images. in (b), the foreground segmentation of the input images employed by the shape-from-silhouette algorithm and, in (c), projection of the binary 3d voxel reconstruction."
"a possibility to evaluate the extremal values ξ max can be to use again a levant's differentiator. in practice, this means that, even if the stateς is not available for measurements, it can be estimated by the differentiator, the structure of which is reported in (22) and (23), and the extremal values of σ(t) can be stored at the time instant whenς(t) changes its sign. in alternative, one can deduceς relying on the definition of σ and on the estimate ofṡ obtained through (22) and (23) ."
"the term \"semantic web\" [cit] and defined as not a separate web but an extension of the current one, in which information is given welldefined meaning, better enabling computers and people to work in cooperation [cit] . the layer cake framework of the semantic web implicates that the development of semantic web technologies proceeds in steps and each step building a layer on the top of another. it www.intechopen.com mainly includes seven different layers, namely unicode and namespace, xml, rdf(s), ontology, logic, proof and trust. two principles, those are downward compatibility and upward partial understanding, were usually recommended to build adjacent layers [cit] . there has been noticeable improvement in the studies of semantic web technologies over the past ten years. some of them, especially the technologies at lower levels of the layered cake, such as xml, rdf(s), owl and sparql, have been standardized by w3c knowledge representation, knowledge searching, knowledge mining, semantic web services, semantic grid, application integration and social network analysis are becoming research hotspots in knowledge processing. project10x examined over more than 270 companies providing semantic products and services and published a semantic web wave report [cit], including google search options, rich snippets, open calais 4.0,bbc's semantic music project, freebase, and data.gov. it is one of revolutionary innovations in the semantic web that human-centric knowledge representation, which has been widely used in traditional web, is substituted for machine-centric knowledge representation. therefore, knowledge on the semantic web is machine-readable. machine readability of semantic web knowledge representation is implemented by:"
"in previous work, train and validation sets are obtained in a sequentially manner (train first 70%, validation last 30%). but, in this new approach, \"shuffle\", the process of splitting the patterns set will consist of obtaining train and validation sets in a random way from time series data, see (fig. 1 ). so it will let different parts of the time series to train the ann and also different parts of the time series to validate the ann, in order to obtain better generalization ability. cross-validation has been used to forecast time series. in this study, the total pattern set will be split into n complementary pattern subsets (n from 2 to 8). fig.2 shows an example of cross-validation with three pattern subsets. so that a individual in ga is an ann topology, applying cross-validation to this individual gives n different ann architectures (i.e. topology plus connection weights) and it n different fitness values depending on which patterns are used to train and validate the topology. so, applying cross-validation, the final fitness value for an individual will be the average of all its fitness values from each of its architectures."
"where w i is the domain value corresponding to the rule i, n is the number of rules triggered in the inference engine, and µ a (w i ) is the predicate truth of that domain value."
"according to the seven basic principles of web2.0 and main outcomes of organizational knowledge management, we can propose a novel collaborative knowledge management model by the holistic systems approach. the model has three different layers: knowledge chain management, knowledge base management and knowledge ecosystem management, as shown in figure 3 . these three levels correspond to the three different objectives of organization knowledge management respectively: accumulating or creating organizational knowledge, mining or utilizing organizational knowledge and building a knowledge ecosystem for the organization. the theoretical foundation of knowledge chain management is web2.0 based organizational knowledge management. knowledge chain management plays a role of knowledge provider for organizational knowledge base. the management of knowledge base in turn could promote the further development of the knowledge chain management. knowledge base management is a prerequisite for building a knowledge ecosystem and a well developed knowledge ecosystem can provide a better environment for construction of knowledge base. organizational knowledge management should cultivate its knowledge chain, knowledge base and knowledge ecosystem at the same time. figure 4 introduces a new framework that integrates semantic web with web2.0 to make full use the mutually complementary natures of them. the framework consists of following five layers in top-down direction: user layer, application layer, computing layer, knowledge layer and networking layer. the building activities of the five different layers should also follow the two basic principles of semantic web layered cake -downward compatibility and upward partial understanding-has been discussed in previous section."
"in addition, the figure shows that of-fl has a low average end-to-end delay as compared to of0 and mrhof with etx mainly when the distance from the root becomes significant (beginning from 6 hops) and the maximum delay that can undergo a data packet is 3.4 seconds for 10 hops versus 4 seconds for of0. this result is mainly due to the average hop count that is minimized, and the end to end delay that is considered when deciding about the next hop. these results are promising as of-fl allows to minimize the end-to-end delay as compared with the existing objective functions."
"the most basic indoor localisation comes from the proximity-based tracking idea. whenever a user and a base station can communicate, the user's location is determined as the location of the station. this method is further enhanced by dividing the tracking space into grids. the stations are strategically placed in such a way that each grid block is overlapped by the signal from as many different stations as possible ( figure 2 ). thus, instead of coarsely predicting the user's location to be somewhere within the station's broadcasting range, the accuracy is improved by interpreting the user's location to be the overlapped portion of the stations the user sees. however, many stations must be deployed to have good signal coverage. overall, coarse-grained tracking idea is great for localising user at the roomlevel resolution. the next section discusses how to identify the user at much finer-grained sub-room resolution."
"the integral 2-sliding mode is kept on a suitably modified sliding manifold from the initial time instant (this time instant being the time instant when the adopted levant's differentiator [cit], involved in the scheme, converges) and from that time instant the robustness of the controlled system is proved. the effectiveness of the proposed approach has been assessed in simulation, relying on a model identified on the basis of the data collected on a comau smart3-s2 anthropomorphic industrial robot manipulator, and experimentally, using the actual robotic system which is present in our lab. beacuse of space limitations, only simulation results are hereafter reported and discussed."
"in this paper, we are interested in improving the quality of service contract of various application areas of llns. in the rpl specification, the component that is responsible of selecting paths is called objective function (of). the of allows to select the preferred parent among the set of neighbors to be the next hop to the dag root. the rpl specification did not impose any constraint in selecting the routing metrics to consider when choosing the best paths to the root, and thus offers a great flexibility for supporting various application requirements."
"as discussed in the previous section, existing objective functions rely either on a single metric or on the combination of two metrics in order to optimize the paths to the dag root."
"2) membership functions: the membership functions are used to quantify the linguistic terms. in figure 1, the membership functions of battery level, hop count, delay and etx variables are plotted. we have chosen the trapezoidal form for the membership functions as it is largely used in fuzzy logic systems [cit] . all the metrics are binned into three fuzzylogic semantic levels and thus have computation complexity. the membership is computed by a membership function that relates a value of a discourse of universe to a real value in the interval [0 − 1]. as shown in figure (1a), the end-to-end delay is a member of three sets which are \"low\", \"average\" and \"high\". following several simulations, we concluded that the end-to-end delay of a path is in the range between 0 and 15 seconds. we have chosen reasonable values for the thresholds, and we considered for instance that when the end-to-end delay is less than 3 seconds, the neighbor node fully belongs to the fuzzy subset of nodes with low latency."
"-node energy: this metric represents the remaining battery level in a rpl router. with this metric, it is possible to avoid selecting routers with low energy, and thus will allow to extend the network lifetime. it is essential to consider this metric for applications with energy-efficiency concerns."
"in this section, we propose an improvement of rpl with the use of a holistic objective function based on the combination of the aforementioned metrics using a fuzzy logic approach. we call the objective function as of-fl. in the proposed objective function, we consider the aforementioned link and node metrics, end-to-end delay, hop count, etx and battery level in the design of our objective function. each metric describes an important link or node property, as previously discussed. the set of selected link and node metrics will be used to assess the quality and compare the set of neighbor nodes to select the best parent that will represent the next hop to reach the dag root. in order to assess the best neighbor to be the preferred parent, we resort to fuzzy logic."
"the transient trajectory is realized as in (15)- (17) . more specifically, an appropriate choice (see [cit] ) for the transient function ϕ(t) is the following"
"in this section, we identify four routing metrics to be considered in the design of of-fl. each metric describes an important property of the neighbor node that is eligible of acting as a parent, thus contributing to forming a route towards the root. we consider a subset of link and node metrics that will be used to express the goodness of a given neighbor. first, we identify the properties of a good route. in general, a good route in llns shall satisfy the following properties:"
"having discussed the problem of indoor localisation, this section explains and applies the conformal prediction (cp) algorithm to address the specific indoor problems outlined previously. particularly, we show why cp is a suitable algorithm for our purpose. the next section will compare its performance with other traditional algorithms."
"core rbac includes sets of five basic data elements called users (users), roles (roles), objects (obs), operations (ops), and permissions (prms). the rbac model as a whole is fundamentally defined in terms of individual users being assigned to roles and permissions being assigned to roles. as such, a role is a means for naming many-to-many relationships among individual users and permissions. in addition, the core rbac model includes a set of sessions (sessions) where each session is a mapping between a user and an activated subset of roles that are assigned to the user [cit] . core rbac model element sets and relations are defined in figure 2 .1 which also illustrates user assignment (ua) and permission assignment (pa) relations. the arrows indicate a many-to-many relationship (i.e., a user can be assigned to one or more roles, and a role can be assigned to one or more users). this arrangement provides great flexibility and granularity of assignment of permissions to roles and users to roles."
"due to the high dimension of feature spaces, classification techniques employed in this field aim at a dimension reduction and/or feature selection. linear techniques such as pca, mda and lda have been thoroughly used [cit] . in some cases, fusion of classifiers allowed integrating information from multiple views at feature level [cit] or combining information from multiple modalities such as face and gait [cit] ."
"(2)self-organized knowledge processing: web2.0 applications allow users to cooperate with each other rather than controlled by other. wiki, for example, allows anyone not only to contribute his or her own knowledge but also to edit the knowledge provided by others therefore, a self-organized knowledge processing platform forms on web2.0 and facilitates ongoing knowledge sharing and innovation activities in organizational knowledge ecosystems."
"time series forecasting is an essential research field due to its applications in several research, commercial and industry areas, and can be performed by statistical methods or artificial neural networks (ann) [cit] . the ann have the capability, without any information but the data, of extracting the nonlinear relationship between the inputs and outputs of a process. there are, in the literature some \"state of art\" by abraham [cit] and yao [cit] about automatic methods to design ann based on evolutionary computation (ec)."
"core rbac defines a minimum collection of rbac elements, element sets, and relations in order to completely achieve a role-based access control system. this includes user-role assignment and permission-role assignment relations, considered fundamental in any rbac system. in addition, core rbac introduces the concept of role activation as part of a user's session within a computer system. core rbac is required in any rbac system, but the other components are independent of each other and may be implemented separately."
"-average number of parent changes: it is the number of times a node has changed its parent. this metric is an indicator of the topology stability. too many parent changes lead to unstable topology, but improve the quality of routes and the routing performance (e.g., pdr and rtx) [cit] ."
"as it can be observed applying shuffle method to these time series does not achieve better forecasting in passengers and dow-jones time series. it could be explained because of the few elements of those time series (less than 200). if train and validation pattern subsets obtained are split in a random way, then all the patterns used to adjust the connection's weights does not correspond to consecutives time series values. so the relationship between inputs and output could be harder to learn if there are few patterns for learning and they are not consecutive (i.e. mixing up the training and validation patterns). on the other hand, the same experiment was also carried out with quebec and mackey-glass time series, larger than previous ones (about 730 elements) applying shuffle to these time series gets better results, specially for mackeyglass."
"the increasing popularity of internet technology highlights limitations of human brain in the speed, accuracy, strength, storage capacity, storage time and standardization of knowledge processing. as a result, computer is becoming an alternative tool of knowledge processing. however, the human-oriented knowledge representation, which is a common feature of traditional knowledge representation technologies, makes the machines' knowledge processing activities complex. the semantic web provides a new platform for man-machine cooperative knowledge processing because of its computer-readable knowledge representation technologies. therefore, man-machine cooperative knowledge processing is becoming one of the hot research topics in knowledge management. this chapter discussed new features of the semantic web-based knowledge processing, designed a model to combine the semantic web with web2.0 for knowledge processing and proposed a unified framework of organizational knowledge ecosystems and knowledge processing software systems, after in-depth studies on the semantic web, web 2.0 and knowledge ecology. research methods such as literature review, case study, system study, and knowledge engineering methodologies are used to conduct the research."
"in this section, the results of the verification and validation of the proposed algorithm based on simulation are reported. simulations have been run using a model of the actual robot identified on the basis of real data. in order to formulate the model of a n-joints rigid robot manipulator, kinematical and dynamical aspects have to be considered. during our tests, for the sake of simplicity, only vertical planar motions of the robot manipulator were enabled, by locking three of the six joints of the robot (see fig. 1 ). fig. 1 ."
"gait analysis is a promising research direction towards contact-free biometrics for person recognition. automatic gait recognition is attractive because it enables the identification of a potentially uncooperative subjects from a distance, with a variety of possible applications. moreover, these algorithms must be robust to pose variations, perspective changes, low resolution and noisy input images."
"from these simulation results, one can see that the packet loss is low in low node density; and this problem becomes more serious under a high node density. we also observe that with a throughput of 1 packet/min, a rpl based network with of-fl has a much lower packet loss ratio than that with of0, and almost the same packet loss as an etx based network. the reason of high packet loss in the of0 based network is that of0 does not promote high quality links, and when choosing the parent with minimal rank, this parent may be congested and thus will drop the data packets. an etx based network has a comparable packet loss ratio as of-fl, and this result can due to the use of the same link quality estimator which is etx. minimizing the etx when selecting the next hop will imply a path with low packet loss ratio [cit] . for the three schemes, we notice that the packet loss ratio is high mainly when the number of hops increases. these results raise the question about the effectiveness of the etx and it is necessary to promote the reliability by choosing another link quality estimator such as f-lqe [cit] . when sensor transmission rates becomes large (6 packets per minute), the packet loss ratio increases. this result is mainly due to network congestion and packet collision. we note also that the performance of of-fl becomes better than mrhof with etx as it allows dropping a lower number of data packets. thus, of-fl outperforms mrhof with etx when the amount of data packets is high, which demonstrates the effectiveness of of-fl in high throughput."
"conflict of interest in a role-based system may arise as a result of a user gaining authorization for permissions associated with conflicting roles. one means of preventing this form of conflict of interest is through static separation of duty, that is, to enforce constraints on the assignment of users to roles. the static constraints defined in this model are limited to those relations that that place restrictions on sets of roles and in particular on their ability to form ua relations. this means that if a user is assigned to one role, the user is prohibited from being a member of a second role. an ssd policy can be centrally specified and then uniformly imposed on specific roles. from policy perspective, static constraint relations provides a powerful means of enforcing conflict of interest and other separation rules over sets of rbac elements. static constraints generally place restrictions on administrative operations that have the potential to undermine higher-level organizational separation of duty policies [cit] ."
"where to log: oracle permits the administrator to choose the location of audit trail in either an operating system file or a database. oracle uses an audit trail database named sys.aud$ and data dictionary in this example, there is a database ‗bank' which contains a table named ‗account'. that table has a column ‗balance'. if administrator wants to monitor when a user tries to access records whose balance is more than $11000, oracle logs this event to the fga audit trail."
"this contribution reports two methods, \"shuffle\" and cross-validation, to obtain the pattern sets used for ann learning algorithm in a previous approach [cit] based on genetic algorithms (ga). \"shuffle\" refers to the way the whole pattern set will be split between train pattern set and validation pattern set. cross-validation will be used for time series with few elements, so that cross-validation will be used to obtain several pattern subsets which will help to evaluate more accurately every specific ann obtained in the ga."
"in this paper, we have presented of-fl, a new objective function for rpl-based llns. in comparison with the existing objective functions that rely only on one or two metrics and thus do not consider the applications requirements in the process of path optimization, of-fl combines four node and link metrics (etx, hop count, end-to-end delay and battery level) by using fuzzy logic, as we proved that this technique is an appropriate strategy for combining different and heterogeneous metrics. the best parent is specified as a set of if-then rules that combines the four linguistic variables. the evaluation of the fuzzy rules returns the membership of the neighbor in the fuzzy subsets of neighbors with good qualities. the neighbor that has the greatest quality will be selected as the preferred parent. of-fl has been implemented with only minor addons, ensuring backward compatibility with the rpl standard specification. simulations show that of-fl has achieved a great improvement in the rpl-based network especially in term of end-to-end delay, network lifetime and packet loss ratio. thus, our solution intent to support multiple concurrent applications with antagonistic requirements simultaneously."
"a key concept in fuzzy logic is that a variable could belong to a set that is between true and false. linguistic variables are input or output variables whose values are words or sentences instead of numerical values. for instance, the linguistic variable of the hop count is a member of three sets according to its position to the dag: \"near\", \"vicinity\" and \"far\". these sets are called linguistic variables. as shown in figure 1b, for values of hop count below 2, the membership to the fuzzy subset of nodes that are \"near\" the dag is of 1. starting from 7 hops, the node is considered totally out of the fuzzy subset of neighbors that are near the root. for values of hop count between 2 and 7, the membership decreases linearly from 1 to 0. the same reasoning holds for \"vicinity\" and \"far\" hop count. in same manner, we classified the etx, the end-to-end delay and the battery level into three linguistic variables as shown in figure 1 ."
"i.e. it is required that g(·) has constant known sign. since the information about the bounds of f(·) and g(·) are assumed to be available, the original dynamical system (1) implies the differential inclusion [cit]"
"role based access control models are widely in organizations for access control. in rbac access control decisions are often based on the roles individual users take on as part of an organization. a role specifies a set of transactions that a user or set of users can perform within the context of an organization. rbac provide a means of naming and describing relationships between individuals and rights, providing a method of meeting the secure processing needs of many commercial and civilian government organizations. various forms of role based access control have been described under ansi standardization. core rbac defines features that are minimally required of all rbac systems. hierarchical rbac add requirements of role hierarchy. hierarchies are a natural means of structuring roles to reflect an organization's lines of authority and responsibility. constrained rbac adds separation of duty relations to the rbac model. separation of duty relations are used to enforce conflict of interest policies that organizations may employ to prevent users from exceeding a reasonable level of authority for their positions. rbac simplifies the access control but access control is considered to be only a partial solution to provide security. this paper integrates auditing and authentication modules to rbac model and enhanced its capability to provide security. this model is called aarbac. authentication is different from access control. authentication is a process of signing on to a computer system by providing an identifier and a password. so, correctly establishing the identity of the user is the responsibility of the authentication service. most of the authentication procedures are affected by sql injection attacks. sql injection attacks are code modifying attacks. prepared statements can be used to prevent these attacks. these statements use bind variables that make sql injection attacks impossible. auditing is a process of finding whether a security policy is being violated or not by logging and analyzing events. oracle provides feature of fine grained auditing using dbms_fga policy. these two modules can be integrated with any system incorporating rbac mechanism to enhance its functionality."
the network behavior. it is therefore reasonable to think about accounting for all these metrics simultaneously -as much as possible -to optimize the selection process of good routes that can provide the best possible balance among all of them.
"the computing layer bridges the gap between the computing layer and the application layer and be responsible for knowledge retrieving, inferring, extracting and mining of the whole framework. therefore, its knowledge processing technology depends on the two other layers. some semantic web knowledge processing technologies, including ruleml,sparql and sparql-update, are used for knowledge computing in the computing layer because of its lower layer and some web2.0 technologies, such as p2p,b/s,ajax and soa, are also used to keep align with its upper layer."
"in this scenario, we considered a dag composed of one dag root and 100 rpl routers. through our simulations, it was observed that the effect of the objective function on the average power consumption is less important. for this reason, we have observed the nodes remaining energy of all the nodes to compare the network lifetime of the rpl routers with the three objective functions and we varied the throughput from 1 to 6 packets per minute. we have run these simulations for 2 simulation days. figure 5 shows the results of these simulations."
"3.1 klc in next generation knowledge management klc shown as figure 1 was developed by members at the knowledge management consortium international (kmci), especially by joseph m. firestone and mark w. mcelroy [cit] . in that figure, the life cycle of knowledge is a continuum regime of knowledge process and it can be divided into three fundamental phases: knowledge production, knowledge validation and knowledge integration. the theoretical foundation of klc is complex adaptive system theory (cas theory). this theory views a system as a fluidly changing collection of distributed interacting components that react to both their environments and to one another. therefore, cas theory makes klc possible to study organizational knowledge management from a new perspective, in which the complexity of managing knowledge comes from not only the external environment, but also the internal adaptive components. the most significant contribution of klc to knowledge management is for the first time arguing that the knowledge process is one kind of natural process of living systems. the first generation knowledge management theory places too much emphasis on supply side of knowledge process (such as the sharing and use of existing knowledge), while neglecting the importance of demand side knowledge process (including the production, validation and integration of knowledge).the introduction of klc changes the conventional ways of thinking on knowledge management into a more effective way -next generation knowledge management (ngkm)."
"}, also regarded as a gait template for a determinate subject. this set is invariant to scaling, rotation, translation and has been properly aligned and timely averaged within the walking cycle thus being suitable as a person's gait template for the forthcoming classification algorithm. an example of this set is shown in fig.4 . for the sake of notation simplicity in the next sections, ordinal time instants are assigned to volumes within g w, centered at the neutral pose c n, thus becoming"
man-machine collaborative knowledge processing is one of most protruding features in the framework which inherits from web2.0. it should be put on top of the list to maximize the complementary advantages of man and machine when developing or selecting methods and technologies for each layer.
"(2)knowledge interconnection: another main purpose of semantic web is to build a web between data. today's web is not a web of data, but a web of computers or applications. knowledge on the current web doesn't connect with or be related to each other. uniform(uri) and namespace (ns) are most common used technologies to build connection between semantic knowledge. (3)knowledge reasoning: semantic web's strength lies in its ability to knowledge reasoning. it is very difficult for today's web to reason knowledge because of lacking metadata and rules. semantic web makes knowledge reasoning possible by adding semantic metadata and a rule system to semantic data. by adding new rule system to the semantic web, new knowledge can be inferred and existing knowledge can be validated. rule system may be monotonic or nonmonotonic. monotonic rule system, which is a special care of predicate logic, can be combined with semantic web by semantic web rules language (swrl) or description logic programs (dlp). no monotonic rules are useful in situations where the available information is incomplete [cit] . through ruleml, nonmontonic rules can be represented easily and priorities to resolve some conflicts between these rules can also be added. (4)knowledge retrieving: knowledge can be retrieved with high precision on semantic web. the process of semantic knowledge searching can be divided into following steps: searching for semantic web document and searching for semantic knowledge in a web document found. intelligent agent and search engines are the most frequently used tools to search for semantic web documents. after the document is located, addressing and querying languages, such as xql, xquery, x-path, rql and sparql, can be used to further search for semantic web parts. (5)knowledge validation: as knowledge on semantic web may be redundant, out-of-date, incorrect, or distorted, it is necessary for semantic web to validate the result set of knowledge retrieving. knowledge validation in ngkm refers to the process by which new \"knowledge claims\" are subjected to peer review and a test of value in practice [cit] . the process of validating semantic knowledge can be carried out based on its authenticity and integrity. digital signatures, encryption, certificate authority technology are the most prevalent technologies for semantic web to validate its knowledge. technologies to integrate knowledge with business process including individual and group learning. though knowledge management on semantic web is only in the initial phase, it looks quite promising. that knowledge life cycle on semantic has three features: 1) the life cycles of knowledge on semantic web is a continuums regime of knowledge process, 2) six distinct stages in semantic knowledge management are representation, interconnection, reasoning, retrieving, validation and integration, and 3) semantic knowledge management requires different methodologies or technologies for different stages. [cit] 4. modelling collabrative knowledge management on the semantic web 4.1 web2.0 and its implications the concept of \"web 2.0\" began with a conference brainstorming session between o'reilly and medialive international [cit] . web 2.0 is not a new technology, but a new shift in the application model of the world wide web. design principles behind web 2.0 include: the web as platform, harnessing collective intelligence, data is the next intel inside, end of the software release cycle, lightweight programming models, software above the level of a single device and rich user experience [cit] .there are some typical applications of web 2.0 such as blog,rss,wiki,tag,sns,p2p which has been widely used on existing web. the reasons why web2.0 has been successfully accepted are as follows: users create value, networks multiply effects, people build connections, companies capitalize competences, new recombines with old, and businesses incorporate strategies [cit] . in contrast to web1.0, web2.0 extends the coverage of knowledge management to the long tail of organizational knowledge chain so that it can nurture knowledge ecosystems for contemporary organizations. some of the key implications of web2.0 on knowledge processing are as following:"
constrained rbac adds separation of duty relations to the rbac model. separation of duty relations are used to enforce conflict of interest policies that organizations may employ to prevent users from exceeding a reasonable level of authority for their positions. ansi rbac allows for both static and dynamic separation of duty as defined within the next two subsections.
"the objective of this simulation study is to demonstrate how of-fl can provide a better performance with respect to the application requirements in a rpl-based lln as compared to the objective functions of0 and mrhof. we simulated of-fl under cooja, a well-known emulator available under contiki operating system [cit] . in all the simulations, we consider the default simulation testbed which consists in a large scale dag containing up to 100 rpl routers and one dag root spanning over a squre space (600m by 600m). the nodes are spread randomly in the space so that they form a connected network. the radio channel is set to 26 and the transmission power to 0 dbm, which is the maximum available radio output power level for the simulated nodes (tmote sky). in each simulation, each rpl router periodically sends one data packet to the root each 60 seconds. the nodes begin the transmission of data packets after 60 seconds to enable the topology establishment. we run each simulation for a sufficient time (60 minutes) to ensure that simulations converge to a steady state. each result is measured with a 90% of confidence interval."
"a user can be coarsely identified at room-level or precisely localised at sub-room level. the finest-grained systems offer up to 3cm accuracy, 95% of the time, using intensive hardware implementation [cit] . however, they demand massive investment to install and maintain. balancing between fine-grained tracking and affordable deployment is desirable for recent approaches, which concentrates purely on software solutions. this section outlines the recent approaches with the indoor localisation, as a background to apply conformal prediction (cp) in the next section."
"the meaning of knowledge atom is declared in domain ontology which is written by xmlbased rdf schema or owl language. therefore, the semantic web allocates a unique specific meaning to each of knowledge atoms in a semantic web document and can avoid the two wellknown semantic problems: homonymy and polysemy. [cit] 0808 [cit] 0808 represents a student could be described as follows:"
in sql injection attacks the attacker attempts to modify the existing sql statement by adding elements to the where clause [cit] . most of the sql injection attacks occur during the login authentication. a simplistic web application may check user authentication by executing the following query:
"the knowledge layer is located between the networking layer and computing layer and provides the upper layer with machine readable knowledge representation services. this layer involves two kinds of knowledge: domain knowledge and non-domain knowledge. the latter can be described, retrieved, inferred and validated by the former. unlike domain knowledge, non-domain knowledge can be maintained by collaborative efforts of domain experts and grass root users. xml-based rdf and owl language is two most prevailing technologies in the knowledge layer."
"-reliability: a route is reliable if it provides a high delivery ratio. this property is much correlated with the quality of the links forming the route. thus, the reliability property can be assessed through link quality estimators, such as the packet reception ratio (prr), the received signal strength (rss), the expected number of retransmissions (etx) and others [cit] ."
"the battery level is measured with an integer encoded in 8 bits. thus, the energy ranges from 0 (no energy) to 255 (full energy). as shown in the figure, the battery is a member of three sets during its life (see figure (1d ). the choice of these thresholds can be tuned according to the application requirements. for instance, for applications with energy efficiency concerns, one can argue that the battery level begins to be full at 70%. this will have an impact on the choice of the best parent giving more weight for the neighbor node that has the highest remaining energy."
-packet loss ratio: it is the ratio of the total number of delivered packets (at the dag root) to the total number of sent packets (by all the router nodes): this metric is an indicator of the end-to-end reliability of the routing protocols.
"having shown how to apply cp into the indoor localisation context, we perform experiments to evaluate the performance of cp with k-nn, and that of the w-knn alone."
"the main challenge is that rssi measurement at the same location can vary from time to time, and distinct locations might have a similar rssi combination due to human movements, humidity, furniture re-arrangement, as well as the multipath fading of the indoor environment."
"the application layer is the upper layer of the computing layer and provides the user layer with web2.0 application environments, such as wiki, blog, rss and im. the layer combines web2.0 with semantic web so that improves the effects of knowledge processing by manmachine collaboration. at the same time, the technologies of application programming interface are usually required in this layer for keeping its independence with computing layer."
"shown as figure 1, it is evident that trust and security are common challenge for all the layers and can't be ignored by any of the layers. therefore, a holistic security and trust solution is required in the framework. in general, the trust of two top layers-user layer and application layer -could be implemented by interaction between agents, at the same time, the security of the other three layers should use information security technologies, including encryption or certificate authentication."
"in this section, we're going to make an in-depth study of applications built on the top of foaf project and to provide insights into the semantic web based collaborative knowledge management. the friend of a friend (foaf) is a project about creating a web of machine-readable homepages describing people, the links between them and the things they create and do [cit] ."
"access control is different from authentication. authentication is a process of signing on to a computer system by providing an identifier and a password. so, correctly establishing the identity of the user is the responsibility of the authentication service. on the other hand, access control assumes that authentication of the user has been successfully verified prior to enforcement of access control. authentication process is vulnerable to sql injection attacks mostly in case of web applications. in these attacks attacker tries to add malicious code to legitimate sql query and try to gain access to resources or database."
"what to log: it is better to log more and more information about an event but this is not always feasible. it is good to keep track of username, name of the schema object accessed, date and time stamp etc."
"the use of a single routing metric in the of encompasses some advantages and limitations. first, it can be noted that relying on a single metric in an objective function may be inefficient and could degrade the performance of the dag as it may not fully satisfy the application requirements. for example, while the hop count routing metric allows to choose the shortest path, it may lead to the failure of one or more nodes due to battery depletion, as the battery level is not considered in the decision process. in addition, considering etx as a single routing metric may lead to high latency in routing messages. in fact, while selecting parents with low etx make the network more reliable, it cannot reduce the latency in routing the messages. thus, the etx metric alone is not adequate for real time applications, as it does not consider the timing requirements of the applications."
"our approach does not seem to achieve an improvement using \"shuffle\" with short time series (i.e. passengers, temperature and dow-jones), so cross-validation, usually used when not too many data are given, have been tried for these time series. the number of subsets in which the total pattern set has been split goes from two to eight. all forecasted values, obtained from the ensemble of the ann architectures are compared with real values and smape error is shown. results are shown in table 2 . we can observe that applying cross-validation to these time series obtain different results depending on the time series and the number of subsets the total pattern set has been split. the problem now arise in which is the optimum number of subsets which should be used to forecast a time series using cross-validation. the results disclose that shuffle only improves forecasting for not short time series. an issue arises at this point: how the positive/negative effect of shuffle depends on the number of time series elements (i.e. size of training/validation subsets). on the other hand, cross-validation let us improve the result for short time series, but another issue arise, the optimum number of subsets to split the total pattern set. as it is a totally automatic method, it will not be necessary any previous knowledge from the user. the user just have to give the time series he wants to forecast and the number of future elements he wants to be forecasted to the system; and this method will give these forecasted values as result to the user. this approach got 6 th position in nn5 forecasting competition [cit] ."
"the current knowledge processing models can be classified into two categories-man's knowledge processing model and machine's knowledge processing model-according to literature reviews of knowledge processing studies in knowledge management and artificial intelligence. man's knowledge processing model is based on knowledge management theory, especially the second generation knowledge management (sgkm), and focuses on processing tacit knowledge by human brains. machine's knowledge processing model is based on artificial intelligence or first generation knowledge management (fgkm), and engages in processing explicit knowledge by computers. furthermore, there are two challenges faced by current research of knowledge processing. one of these challenges is how to break through bottlenecks in the two knowledge processing model by lowering the cost of knowledge sharing and innovation and adopting machine-readable knowledge reorientation technology; the other one is how to make full use the complementary advantages of human and computer through combining the two models [cit] . in this chapter, we carry out in-depth study of knowledge life cycle on the semantic web and propose the model for collaborative knowledge processing and its implementation framework. the remainder of this paper is organized as follows. in section 2, we review the development of semantic web technologies and discuss machine readability of semantic web knowledge representation. in the next part, section 3, we describe the knowledge life cycle on the semantic web. then, section 4 proposes a model for collaborative knowledge management on the semantic web and section 5 discusses how to implement the model. section 6 provides a case study by analyzing the foaf project. in the conclusion (section 7) some topics that should be further studied are proposed."
"future research lines involve applying the presented scheme on sequences with moderate occlusions yielding to noisy 3d reconstructions. the extension of other integral transforms to the spatio-temporal domain is also under study. as a contribution for further comparison, the authors will release the employed multi-camera dataset."
"it must be noted that hyperplane π(η, ρ) spans over time and space hence the transformed coefficients will encode information from both domains and is robust to variations due to spurious noisy voxels. this integrating hyperplane can be understood as a 3d plane that shifts along time (see fig.5 ) and intersects with w(x) to produce r(η, ρ). the radon transform is very suitable for gait representation and recognition. during the walking cycle, there are two noticeable variations: the appearance changes among different time instants produced by the limbs movement and the variations among subjects when performing a walking cycle. this means that the radon transform over a properly space and time aligned set guarantees some specific coefficient will vary considerably through time and among subjects. therefore, the study of these coefficients will allow distinguishing among different gait templates."
"sliding mode (sm) control is a widely used control methodology which ensures good performance of the controlled system even in presence of a significant class of uncertainties [cit] . yet, because of the discontinuous nature of the sliding mode control law, it can produce the so-called chattering effect [cit], i.e. high frequency oscillations of the controlled variable, which can be disruptive for the controlled plant or significantly limit the life cycle of the actuators. this is the reason why the use of sliding mode control in robotics is quite limited. spong and hutchinson in [8, subsection 8.4.11] suggested, in order to control robotic systems, to implement a continuous approximation to the discontinuous control, which however could only guarantee the uniformly ultimately boundedness of the tracking error system. this, in practice, diminishes the efficacy of sliding mode control, since a pseudo-sliding mode is generated, rather than an ideal sliding mode, and the robustness features of the methodology are lost."
"this model component provides the capability to enforce an organization-specific policy of dynamic separation of duty (dsd). dsd allows a user to be authorized for two or more roles that do not create a conflict of interest when acted in independently, but produce policy concerns when activated simultaneously. for example, a user may be authorized for both the roles of cashier and cashier supervisor, where the supervisor is allowed to acknowledge corrections to a cashier's open cash drawer. if the individual acting in the role cashier attempted to switch to the role cashier supervisor, rbac would require the user to drop the cashier role, and thereby force the closure of the cash drawer before assuming the role cashier supervisor. as long as the same user is not allowed to assume both of these roles at the same time, a conflict of interest situation will not arise. dynamic separation of duty relations are defined as a constraint on the roles that are activated in a user's session (figure 2 .4)."
"the purpose of indoor localisation is to identify and observe a user inside a building. global positioning system (gps) has long been an optimal solution for outdoor localisation, yet the indoor counterpart remains an open research problem, because the complex building infrastructure hinders the gps signal. in this paper, we applied the conformal prediction (cp) algorithm to enhance an effective indoor tracking solution known as \"fingerprinting method\" [cit] . we designed a new nonconformity measure with the w-knn as the underlying algorithm. to the best of our knowledge, we are the first to apply cp for the localisation purpose in general, and for the indoor localisation context in particular. the logical progression of the paper is graphically depicted in figure 1 ."
"nowadays, a well-established method to perform chattering alleviation is that consisting in confining the discontinuity to a derivative of the control variable, so that the control signal actually fed into the system is continuous. this approach, called higher order sliding mode (hosm) control [cit], after a transient phase, enforces a sliding mode, involving not only the sliding variable but a. ferrara and g.p. incremona are with the dipartimento di ingegneria industriale e dell'informazione, university of pavia, via ferrata 1, 27100 pavia, italy (e-mail: antonella.ferrara@unipv.it, gp.incremona@gmail.com)."
"the hyperspherical radon transform is not invariant to scaling, translation and rotation thus the preprocessing of input data v 1:nt ensures that transformed coefficients from different subjects will be comparable."
auditing is the recording and analyzing of events to provide information about system use and performance in a clear and understandable manner. the goal of an auditing system is to be able to determine if security policies are being violated.
"this model component introduces role hierarchies (rh) as indicated in figure 2 .2. role hierarchies are commonly included as a key aspect of rbac models and are often included as part of rbac product offerings. hierarchies are a natural means of structuring roles to reflect an organization's lines of authority and responsibility [cit] . role hierarchies define an inheritance relation among roles. inheritance has been described in terms of permissions; i.e., r1 -inherits‖ role r2 if all privileges of r2 are also privileges of r1. ansi rbac recognizes two types of role hierarchiesgeneral role hierarchies and limited role hierarchies. general role hierarchies provide support for an arbitrary partial order to serve as the role hierarchy, to include the concept of multiple inheritances of permissions and user membership among roles. limited role hierarchies impose restrictions resulting in a simpler tree structure (i.e., a role may have one or more immediate ascendants, but is restricted to a single immediate descendent)."
"in general, cp is determined by some nonconformity measure [cit] . the nonconformity measure is a real-valued function a(b, z) measuring how different a sample z is to the training database b. whenever a new sample needs to be classified, we exhaustedly test every possible label recorded in the training data, and cp tells us whether to accept that label or not by a test of randomness for a given significance level. cp uses existing machine learning algorithms such as nearest neighbours, svm and neural network as the underling algorithms to compute a \"nonconformity measure\" and generate the prediction region [cit] . regardless of the chosen nonconformity measure, the set of locations predicted by cp is always valid in the online setting. however, the efficiency, in other words, the tightness of the prediction region is affected. ideally, it is more preferred to have as few predictions in the prediction region as possible, without sacrificing the confidence level too much. each problem requires a customised nonconformity measure to fit the purpose, thus opening more research opportunities. in the next part, we design a new nonconformity measure for the indoor localisation problem."
"in the paper the good features of the integral sliding mode control approach are extended to the so-called suboptimal algorithm, also ensuring chattering alleviation and robustness with respect to matched uncertainties. a new version of the suboptimal algorithm named integral suboptimal second order sliding mode control algorithm has been formulated. some theoretical results have been first discussed: the finite time regulation of the auxiliary system state, the reduction of the reaching phase, as well as the robustness of the proposed approach guaranteed since the initial time instant to. then, the proposed algorithm has been used to design a motion control scheme for robot manipulators. the scheme has been tested in simulation, relying on the data from a real industrial robot manipulator. the effectiveness of the proposed algorithm in terms of convergence and robustness is confirmed by the satisfactory simulation results."
"for a given frame in the video sequence, a set of n c images are obtained from the n c cameras (see a sample in fig.1(a) ). each camera is modeled using a pinhole camera model based on perspective projection with camera calibration information available [cit] . then, foreground regions from input images are obtained using a segmentation algorithm based on stauffer-grimson's background learning and subtraction technique [cit] as shown in fig.1(b) ."
"in the last few years, routing in llns is considered as one of the key issues that are worth investigation. llns stand for networks with very limited resources in terms of energy, computation and bandwidth turning them highly exposed to packet losses. rpl (ipv6 routing protocol for llns), which is the main candidate to act as the standard routing protocol for ipv6 based llns such as wireless sensor networks (wsns), has been recently promoted from an ietf draft to an internet request for comments (rfc) [cit], and thus, it is gaining a lot of maturity. although rpl has been recently released, several research works have been devoted to investigate the issues that are left open by the working group. however, their goals were to propose some broadcast and multicast mechanisms [cit], some security countermeasures [cit] or mobility mechanisms [cit] and did not consider the application requirements in their proposals."
"rbac is a proven technology for large-scale authorization. however lack of a standard model results in uncertainty and confusion about its utility and meaning [cit] . because of its relevance in products and applications for the management of enterprise security, rbac always has been the focus of standardization activities. the american national standard institute (ansi) [cit] and provided a consistent and uniform definition of rbac [cit] . this ansi rbac standard consists of two parts: a reference model and a functional specification. the reference model defines sets of basic rbac elements and relations, and the functional specification specifies the operations and functions an rbac system should support. the rbac model and functional specification are organized into four rbac components: core rbac, hierarchical rbac, static separation of duty (ssd) relations, and dynamic separation of duty (dsd) relations (constrained rbac)."
"a novel trend of the research on sliding mode control has led to the formulation of a joint approach which can be named integral higher order sliding mode (ihosm) control. this approach consists in defining an auxiliary sliding variable as in (13), with the function ϕ(t) suitably chosen to fulfill some restriction on the transient time during the reaching phase, so that, thanks to the existence of a sliding mode on the integral sliding manifold since the initial time instant, one has"
"as a conclusion, single routing metric or a simple combination of two routing metrics cannot provide a good assessment on the quality of the routes from the source to the destination in the dag. as such, there is a need to design a holistic objective function that combines several representative metrics to be able to characterize the route quality in a more efficient way. in order to better choose the best parent as the next hop, we advocate combining several important routing metrics, to get a holistic characterization of the neighbor."
"once all the rules are established, the next step is the defuzzification, which consists in producing one output metric value from several membership values to produce a crisp control action. we have chosen a simple and frequently used defuzzification method which is the centroid defuzzification method [cit] that finds the balance point of the solution fuzzy region by calculating the weighted mean of the fuzzy region. mathematically, the crisp output domain value r, from the solution fuzzy region a, is given by:"
"edited by dr. due to the development of mobile and web 2.0 technology, knowledge transfer, storage and retrieval have become much more rapid. in recent years, there have been more and more new and interesting findings in the research field of knowledge management. this book aims to introduce readers to the recent research topics, it is titled \"new research on knowledge management technology\" and includes 13 chapters. in this book, new km technologies and systems are proposed, the applications and potential of all km technologies are explored and discussed. it is expected that this book provides relevant information about new research trends in comprehensive and novel knowledge management studies, and that it serves as an important resource for researchers, teachers and students, and for the development of practices in the knowledge management field."
"as described above, fuzzy logic represents a widely used approach to combine several logical information that are of different nature, using the concept of membership functions [cit] . the first question that arises when using fuzzy logic is: what are the metrics to be combined ?"
one more benefit of prepared statement is to prevent sql injection by making them impossible in database through the use of bind variables [cit] . an example is shown below [cit] here -?‖ is the bind variable that binds username to variable usern and password to passw.
"conformal prediction is a recently developed machine learning algorithm, which uses experiences in the past to confidently and precisely predict the outcome of a new sample [cit] . however, what differentiates cp from other similar machine learning algorithms is the ability to produce prediction region for the given confidence level parameter. it has been mathematically proved that the prediction region generated by cp is valid in online setting [cit] . in other words, for a confidence level at 95%, the correct estimated position is expected to be included in the prediction region at least 95% of the time. achieving the prediction result in such a powerful sense, however, cp demands a relatively weak assumption that the training database and the new sample to be classified are generated from the same distribution independently. cp has been successfully used in many applications such as medical diagnosis and network traffic classification [cit] ."
"there is a number of datasets intended for vision-based gait but, although some of them datasets contain video data from multiple cameras, few provide both synchronization among cameras or calibration information. in order to test the validity of our algorithm we have recorded a dataset containing 28 people, 22 men and 6 women, walking in a 4x5 meters room surveyed by 5 calibrated and sync cameras at 25 fps with 768x576 pixels. the training part of this dataset presents the different subjects walking straight in the scenario while the testing part includes several covariates of the walking cycle (carrying a bag, wearing slippers or no shoes, wearing a coat). invariance of the proposed algorithm to scaling, rotation and translation has been tested by three particular covariates that involved walking in diagonals, zig-zag or randomly."
"the ability to analyze the data in the audit logs contributes to the effectiveness of the auditing system. since oracle uses tables as a location of logs, users can easily access and investigate them using standard sql command. generally the analysis work is done by administrator or the person with highest privileges."
"-real-time: a good route should be able to provide low end-to-end delays, in particular for real-time data flows. the real-time property can be measured through the offered endto-end delay from one source to a destination (i.e. dag root) through a particular route."
"semantic web makes it possible for ai to manage the knowledge on the web effectively. semantic web is an evolving extension of the world wide web in which the semantics of information and services on the web is defined, making it possible for the web to understand and satisfy the requests of people and machines to use the web content [cit] . therefore, this new technology will definitely introduce a new research domain into current knowledge management theory. contrary to knowledge in human organizations, the knowledge on semantic web is created, processed, stored, and transferred by machine agents, not directly by human brain. semantic knowledge management turns much attention to sharing and reusing the knowledge while organizational knowledge management places more emphasis on the continuous production of new knowledge through enhancing the conditions in which innovation and creativity naturally occur and organizational learning happen. in other words, knowledge management on semantic web belongs to supply side knowledge management while organizational knowledge management belongs to demand side knowledge management. currently, klc is limited to life cycle of knowledge in human organizations, not considering the knowledge management on semantic web. the different stages in semantic klc are representation, interconnection, reasoning, retrieving, validation and integration ( figure 2 ). (1)knowledge representation: the main purpose of knowledge representing is changed on semantic web. traditionally, web content is formatted for human readers rather than computer applications. as a result, the machines hardly find, organize, integrate or validate knowledge on the traditional web without man's intervention. we have been tended to believe that artificial intelligence is the only way to manage the web data by machines or applications. ai hasn't been, however, made much progress with data management yet and therefore many scholars of knowledge management have to grant a higher value on humancentric knowledge management instead of machine-centric knowledge management. semantic web, for the first time, make it easier for machines to manage web knowledge because data are represented to be machine readable. the main semantic technologies to represent data semantically are unicode, xml (extensive markup language), rdf/rdfs (resources description framework / resources description framework schema), and owl (web ontology language) [cit] ."
"the problem of making the r-sliding manifold associated with (6) finite-time attractive, generating a sliding mode of order r (r-sliding mode), can be solved by any r-sliding mode controller of the type"
"at the top of the framework, we can find user layer including agents, men and machines. men and machines can use web2.0 application of application layer by intelligent agents. men, machines and agents at the long tail of organizational knowledge chain are encourage to take part in knowledge processing activities in order to build organizational knowledge ecosystems which support mass-collaborative, self-organized, and meta-synthesized knowledge processing."
"where u 0 (t) is generated by any suitably designed high level controller, and u 1 (t) is a discontinuous control action designed to compensate the uncertainties affecting the system. a particular sliding manifold is defined, named integral sliding manifold, as"
"the goal of rbac systems is to provide a model and tools to help manage access control in complex environment with a very large number of users and even larger number of data items [cit] . a recent study by national institute of standards and technology (nist) demonstrates that rbac addresses many needs of the commercial and government sectors [cit] . most of the organizations base their access control decisions on the roles that individual users take on as part of the organization. other evidence of strong interest in rbac comes from the standards arena. roles are being considered as part of the emerging technologies. rbac is also well matched to prevailing business trends. a number of products support some form of rbac directly, and others support closely related concepts, such as user groups, that can be utilized to implement roles."
"a straightforward observation is that with of0 and mrhof with etx, the remaining energy distribution is not balanced. we note for instance that for of0 with 1 packet/minute, 23% of the nodes have a remaining energy under 80%, 31% of the nodes have a remaining energy between 81% and 85%, 26% of the nodes between 86% and 90%, and 10% of the nodes between 91% and 92%. this result will have an impact on the network survivability, as some nodes will undergo a completely battery exhaustion early. this observation is emphasized with 6 packets/minute as the nodes will exhaust their batteries much faster. the simulation results also demonstrate that in the case of of-fl and in both experiments, the energy expenditure is well balanced among the nodes (86% of the nodes have a remaining energy between 84% and 87%, and 14% of nodes have 90% of their battery), which is an important property for the network lifetime. it is clear also that in the cases of of0 and etx based schemes, there were less-powered nodes when compared to those in the of-fl based network. thus, we estimate that of-fl will delay the battery depletion of the first nodes."
"some recent works came up with some solutions to overcome the single metric problem. however, combining two metrics is insufficient to efficiently satisfy all the application requirements as the objective of each lln application may differ from an application to another. in addition, considering two routing metrics may improve the performance in the dag, but at the cost of the degradation of other performance parameters. for instance, choosing the latency and the etx metrics may help the rpl routers to use more reliable paths with a minimal latency to reach the dag root, but may lead to the overuse of some rpl routers leading to battery depletion."
"rpl is a distance-vector and a source routing protocol that is designed to operate on top of several link layer mechanisms including ieee 802.15.4 phy and mac layers [cit] . it mainly targets collection-based networks, where nodes periodically send measurements to a collection point. a key feature of rpl is that it represents a specific routing solution for low power and lossy networks. the protocol was designed to be highly adaptive to network conditions and to provide alternate routes, whenever default routes are inaccessible."
"discussion: it is clear that each metric provides an individual perspective of the notion of a good route. in addition, the aforementioned routing metrics have an important impact on"
after analysis it is the duty of analyzer to notify the result. if the analysis represents any flaw in dbms it should be notified to all the users of dbms by the analyser else if the analysis shows any breach the analyser should take appropriate actions.
obtained recognition results for this scenario are shown in fig.6 . it can be seen that for standard cases such as straight walking the recognition rate is the highest and this performance decreases gradually as the complexity of the path grows. the abrupt changes in the trajectory of the subject yields to noisy angle estimations therefore producing a misalignment of the input data fed to the hyperspherical radon transform.
"some important issues related to logging include what, when, where, how, and how often a database should be logged. the ways in which the databases address these issues have an impact on other aspects of the system."
"when and how often to log: audit module should monitor every command to detect qualified events. if a particular event is met with the predefined condition by users, it logs necessary information to the database. this is the fundamental principle of the database audit. it is also necessary to keep an eye on events every time to avoid breach of information. periodic auditing is also useful depending upon the needs of an organization."
"we have demonstrated the application of conformal prediction into the indoor localisation context. to the best of our knowledge, we are the first to implement conformal prediction algorithm into the localisation problem in general, and the indoor localisation in particular. we also designed a new nonconformity measure with the weighted k-nearest neighbours as the underlying algorithm."
"the new framework is built on the top of traditional world wide web technologies and provides the knowledge layer with the services of data interconnecting and transferring. some current web technologies, including unicode, uri and namespaces, form part of the basic structure of this layer."
"rpl is based on the topological concept of directed acyclic graphs (dags). the dag defines a tree-like structure that specifies the default routes between nodes in the lln. however, a dag structure is more than a typical tree in the sense that a node might associate to multiple parent nodes in the dag, in contrast to classical trees where only one parent is allowed. more specifically, rpl organizes nodes as destination-oriented dags (dodags), where most popular destination nodes (i.e. sinks) or those providing a default route to the internet (i.e. gateways) act as the roots of the dags."
"coarse-grained tracking is improved further by analysing the wireless signal strength, based on the fact that the radio signal attenuates and gets weaker as it travels in the air. there are two popular measurements to roughly represent the distance between a user and a station: the received signal strength indication (rssi) and the link quality (lq). however, two distinct locations might not have a linear relationship in terms of rssi/lq and the distance between them. this phenomenon is caused by the signal being blocked by the indoor objects, known as the multipath issue. an elegant solution -the \"location fingerprinting\" method has been widely adopted for its simplicity and efficiency [cit] . this method utilises the built-in wireless signal of the building to survey a signal-to-position mapping database beforehand, which is known as the off-line stage. by surveying the whole signal variation at each position, location fingerprinting does not calculate the distance from the user to the station based on the signal strength. instead, the system applies pattern-matching algorithms of the real-time signal and the database record to estimate the most probable position during the on-line stage."
"this paper introduces a novel approach to gait recognition and two main contributions are presented. first, a unified space-time representation of input data in the form of a time evolving volume set associated to a walking cycle. this representation is invariant to scale, rotation and translation changes. in order to analyze this input, the hyperspherical radon transform is introduced as an effective algorithm to produce a sparse set of features through the integration of the spatio-temporal volume over a set of hyperplanes. further dimension reduction using lda yield a high class separability. results over an annotated dataset containing 28 subjects with a number of covariates proved that the proposed method is effective for gait recognition tasks. the proposed method is not straighforward applicable to the sequences of the widely known gait challenge problem [cit] due to issues with the calibration of the cameras and the adequateness of the data. the combination of the 3d representation and the ability to analyze spatio-temporal data through the proposed radon transform might produce high recognition rates on large datasets. future steps aim at proving this afirmation."
"to measure the reliability of the network with of-fl, we have compared of-fl with of0 and mrhof in term of packet loss ratio. we also varied the throughput from 1 packet to 6 packets per minute. figure 6 shows the packet loss ratio as a function of the network size."
"prior to compute any transformation, it is required to preprocess the original input data v 1:nt in order to obtain a representation invariant to spatial scale changes, rotations and translations. some vision approaches to gait recognition achieve this spatial invariance by constraining the user to follow a determinate trajectory [cit] to get frontoparallel images. other techniques using information provided by multiple cameras generate invariant representations as synthetic views [cit] or body part trajectories [cit] ."
"another advantage of cp is that it can work at the on-line learning setting, in which cp learns from its previous predictions and updates its training database to improve the accuracy of the next prediction. this scheme perfectly fits the purpose of tracking an indoor mobile user, since the user cannot jump a long distance in a short period of time, his next movement is within a certain radius of his current position. a challenge to apply cp, however, is the low latency of many tracking systems. the environment does not always response immediately. especially with bluetooth tracking, the expected delay can be as high as 1.28 seconds, and the user might have moved away within that time-frame. this is the case of slow teacher, where immediate feedback cannot be guaranteed. a possible solution is to evaluate the trustworthy of the received signal at a particular moment. for example, a signal received in less than 0.5 second would have higher reliability than that of 2 seconds. we only add the samples with reliable signal measurement into the training database, and discard the others. it is our on-going work to inspect the improvement of cp for tracking a mobile user, considering the user movement speeds."
"1) average hop count: figure 2 compares the average hop count of of-fl, mrhof with etx and of0. it is clear that the three networks are almost identical in the case of a sparse network composed of less than 50 nodes. this can be justified by the relatively limited number of neighbors which makes the choice of another parent very restricted. however, it is clear that of-fl distinctly allows a lower average hop count, as compared with mrhof with etx, in case of dense networks composed of more than 50 rpl routers. it is obvious that the average hop count of of-fl becomes closer to that of of0 under a high node density (more than 80 nodes). this confirms the tendency of of-fl to minimize the number of hops within the dag. figure 3 depicts the number of parent changes of the three objective functions. the first observation is that rpl based networks with of0 and mrhof with etx experienced an average number of 0.2 and 0.252 respectively for each node every hour, which is considered as low. this result is expected as these ofs aim only to minimize the rank without considering any optimization in the process of parent selection. we note also that the of-fl based network experienced a slightly higher number of parent changes per node per hour, which is equal to 0.275. this relatively high number of parent changes is an indicator of topology instability but has the advantage to improve the quality of routes and the routing performance as we will explain in the next sections."
"the project accumulates various kinds of data, such as text, photo and records, from real practices and defines relations between different data source by social relations [cit] .knowledge life cycle of a typical foaf application is as follows: (1 from the long tails of web2.0 applications have good domain knowledge and can be used to make up for deficiencies in knowledge validation by computers. (6) knowledge integration: the knowledge validation could be followed by integration of knowledge from different sources. a semantic web-based collaborative knowledge processing system built on foaf project has a good capability for integrating personal information with tacit knowledge, explicit knowledge or application by integration the semantic web technologies with web2.0 principles. while foaf is a simple application of the semantic web, it has been widely used in social network analysis [23, search engine [cit], e-commerce partner query [cit] .the success of foaf application not only stress the necessity for studying human-machine collaborative knowledge management, but also highlights the importance of human intervention in knowledge processing by computers."
"the machine readability of knowledge atom is a necessary condition for that of web knowledge and not the sufficient condition of it. there is another necessary condition which is machine readability of relations between knowledge atoms. semantic web makes it possible to represent the knowledge relations in machine readable syntax. [cit] 1881 [cit] 0808, the machine readable relation between them can be shown as follows in xml-based rdf(s) syntax."
"second order sliding mode (sosm) control is a particular case of hosm control. several algorithms, such as the twisting, the super-twisting [cit] and the suboptimal algorithm [cit], have been proposed in the last years. in this paper, we refer to the suboptimal approach, and we assume that the sliding variable is expressed as"
"-energy efficiency: a route is energy efficient if it uses nodes that have more energy than the others. as a consequence, an efficient route selection must consider the battery levels of the nodes to extend the network lifetime. nodes with low battery levels should be avoided in the routing process as much as possible. the energy-efficiency can be measured through the total consumed energy, or the network lifetime, or the remaining energy, etc."
"(1)mass-collaborative knowledge processing: it can be easily inferred from some principles behind web2.0 such as harnessing wisdom of crowds, lightweight programming models, software above the level of a single device and rich user experience, that knowledge processing activities on web 2.0 environments are mass-collaborative. all volunteers who located on long tail of organizational knowledge chains would be encouraged to participate in organizational knowledge intervention and can save the cost of knowledge sharing and innovation in organizations."
"the remainder of this paper is organized as follows: in section 2, we give a brief overview on the rpl routing protocol. in section 3, we provide a literature review of relevant and recent works around the proposed objective functions in rpl. we introduce our fuzzy logic objective function (of-fl) in section 4. details of simulation scenarios and results are given in section 5. finally, section 6 concludes the paper and discusses future work."
future work will address the application of of-fl in a heterogeneous wireless sensor networks where different applications can be deployed. our aim is to fine tune the fuzzy parameters designed in of-fl in order to select the paths that are suitable for each application.
"the rbac reference model is defined in terms of four model components-core rbac, hierarchical rbac, static separation of duty relations, and dynamic separation of duty relations [cit] ."
"the radon transform [cit] and its variants [cit] have been found useful for dimension reduction and to obtain informative features in image classification problems. within the scope of this paper, this technique has been used for monocular gait recognition using template images [cit] and to process 3d data for search and retrieval tasks [cit] . the usual approach to define the transformation variables of the radon transform is through circular or spherical coordinate systems. in this paper, an extension of the radon transform in hyperspherical coordinates is presented to deal with the aligned spatio-temporal gait template set g w ."
"in this section, the ihosm control methodology is coupled with the suboptimal sosm control approach, giving rise to a new algorithm, herein named integral suboptimal second order sliding mode (issosm) algorithm. this new control approach maintains the good properties of the original suboptimal algorithm in terms of capability of stabilizing in finite time a perturbed chain of integrators with bounded control, as well as in terms of chattering alleviation. moreover, the reaching phase is reduced to a minimum, as will be clarified in a moment, by the introduction of a transient dynamics with a prescribed time."
"this paper proposes applying the iot to dehumidifier, which is one of the industry 4.0 techniques, and the method to improve the performance of dehumidifier using the thermoelectric element. to dehumidify using the thermoelectric element, the temperature of the heat absorption side in the thermoelectric element must remain under the dew point temperature. this condition is very closely connected to the dehumidification performance of the dehumidifier."
"therefore, each has temperature is measured and the temperature control is performed based on the higher has temperature. a block diagram of this system is shown in figure 4 . tdp is the dew point temperature according to humidity and current temperature. δt is the setting temperature to control the temperature below the dew point temperature. tref is the reference temperature for dew point temperature control. tf1 and tf2 are the temperature of heat absorption side in the thermoelectric element. if the thermoelectric element is over two, the input value of the t dp is the dew point temperature according to humidity and current temperature. ∆t is the setting temperature to control the temperature below the dew point temperature. t ref is the reference temperature for dew point temperature control. t f1 and t f2 are the temperature of heat absorption side in the thermoelectric element. if the thermoelectric element is over two, the input value of the controller is the temperature of the heat absorption side of the thermoelectric element that has high temperature."
"as a baseline reference, the average top-1 and top-5 accuracy reported in the mobiledeeppill 29 are 26.0 ± 1.2% and 53.2 ± 1.9% for the single-cnn version, and 53.1 ± 1.0% and 83.1 ± 0.9% for the multi-cnn one based on a retrieval scheme. these metrics are computed over fivefold cv partitioning by pills. thus, they are not directly comparable with the numbers in table 1, since the authors partition image data by pill, thanks to the retrieval-based approach they propose; meanwhile, we only split sets by image. in addition, they use the appearance-based identifiers defined in the challenge, while we directly use ndcs as identifiers, which for some pills, this means combining multiple appearances under the same code, making our task more complex. still, the mobiledeep-pill metrics work as an identification performance baseline when considering which type of approach to apply for a real-world deployment. figure 1 contains the evaluation precision-recall (pr) curve plots of each model calculated using per-class micro-averages. these plots illustrate well the differences in the overall evaluation performance between models. resnet50 and inception v3 have practically identical pr curves and average precision scores. mobilenet has a slightly lower identification performance due to lower precision averages at a low-recall range."
"our pill localization approach consists of a blob-detection neural network and morphological post processing. for the blob detector, we trained a fully convolutional network (fcn), 48 which is a pixel-segmentation algorithm. since the nih competition dataset does not provide the pill localization data, e.g., bounding boxes, we generated synthetic images and trained fcn only with the synthetic images. the segmentation performance was evaluated indirectly through its effect on the overall classification accuracy as a proxy."
"the experiments and data analysis were carried out using python 3.5 with the following openly available libraries: tensorflow 1.3.0, keras 2.0.8, numpy 1.15.0, opencv 3.4.2, and sklearn 0.19.0. the code to fine-tune the models was based on the keras neural network library available at https://keras. io. 57 the tuning code is proprietary and might be available upon request and under a nondisclosure agreement. fig. 6 an overview of our pill detection and classification approach. a fully convolutional network (fcn) is used to detect the pill blobs, followed by some standardization steps, including the rotation alignment and scaling. the cropped single-pill images are passed to the cnn blocks to generate the feature vectors and the probabilities"
"in the power consumption comparison of tables 7, 9, and 11, the difference between the pi controller and the sf-pi controller is 1.79 wh, 2.4 wh, and 0.5 wh, respectively. the difference between these values seems to be very small from a typical numerical comparison. however, this is due to the small capacity of the system, and a percentage comparison shows a 7.6%, 11.9%, and 2.3% reduction in power consumption, respectively. therefore, if the system capacity is increased or the used number increases and the usage time and duration are increased, the effect will be increased. figure 20 shows the consumption power of sf-pi and pi controller under condition of figure 17 . because the sf-pi controller has a low temperature error, its consumption power is also low under conditions of changing temperature and relative humidity. table 11 shows analysis of figure 20 . the sf-pi controller reduces consumption power by about 2.3% compared to pi controller even in condition of figure 17 ."
"energy consumption is increasing globally and energy demand is expected to increase by 30% [cit] to cope with the rapidly changing climate [cit] . the energy consumed by buildings is about 80%-85% in high temperature and high humidity regions, and 39% and 40% in europe and the usa, respectively [cit] . the co 2 emissions from these buildings account for 30%-40% of all industries [cit] . in particular, the air conditioning system occupies 10% of the total energy and in japan, the united states, and korea, 91%, 90%, and 86% of residents have air conditioners [cit] . such an air conditioning system is used for heating, ventilating, and cooling the room air, and it represents a financial burden for consumers since it is continuously used even in the peak load time of the building. the load on these air conditioning systems accounts and temperature control experiment result by sf-pi controller are presented and the validity of the proposed method is analyzed. finally, section 5 presents the conclusion and future work of this paper."
"the first term is the power by the seebeck effect, and the second term is the power by the electric resistor in equation (3) . the sum of these powers becomes the total input power of the thermoelectric element."
"the typical pi controller calculates the control value using an error signal, a proportional (k p ) and integral gain (k i ) or integral time (t i ). the optimal gains of pi controller change by the operating state. therefore, gain adjustment is necessary, and the adjustment has to consider the control characteristics suitable to the system. the characteristics requested to the pi controller are as follows."
"the k p and k i (t i ) must be adjusted within a stability performance range. in generally, the increasing of gain k p and k i (decreasing of the integral time constant t i ) brings the system the quick response. however, excessive increase or decrease of these values causes vibration of the system. therefore, the system becomes a divergent system at worst."
"we now focus on the resnet50 model, which displayed the best evaluation performance in our setup, and which we already use in our proof-of-concept implementation."
"pills are identified employing two deep-learning models in series. first, we perform image segmentation isolating the pill from the background with a blob-detection cnn and define a bounding box to crop a smaller image that centers on the pill. second, we employ a deep-learning-based classifier to return a ranked list of drug codes based on matched likelihood by the pill in the cropped image output by the initial stage. figure 6 shows the system architecture overview. in the following sections, we detail the learning setup for the models of each stage. next, we present the results of the experiment we carried out, comparing multiple cnn models to select which one we will use in our pill identifier implementation. we include additional results deepening the performance evaluation of the selected model."
"therefore, each has temperature is measured and the temperature control is performed based on the higher has temperature. a block diagram of this system is shown in figure 4 . tdp is the dew point temperature according to humidity and current temperature. δt is the setting temperature to control the temperature below the dew point temperature. tref is the reference temperature for dew point temperature control. tf1 and tf2 are the temperature of heat absorption side in the thermoelectric element. if the thermoelectric element is over two, the input value of the the purpose of this paper is to improve the temperature control performance of a dehumidifying system using a thermoelectric device. in this paper, we design a system to control the input value of the pi controller because the pi controller has a limitation in the performance improvement due to the fixed gain value. control of input values of pi controller uses fuzzy control that does not require mathematical modeling and robustness to a nonlinear system."
"therefore, each has temperature is measured and the temperature control is performed based on the higher has temperature. a block diagram of this system is shown in figure 4 . the purpose of this paper is to improve the temperature control performance of a dehumidifying system using a thermoelectric device. in this paper, we design a system to control the input value of the pi controller because the pi controller has a limitation in the performance improvement due to the fixed gain value. control of input values of pi controller uses fuzzy control that does not require mathematical modeling and robustness to a nonlinear system."
the amount of heat absorption (q c ) on the low temperature side and radiation of heat (q h ) on the high temperature side are as follows [cit] :
"for the first proof-of-concept implementation of our pill recognition service, we went with a resnet50 45 model as the final classifier. both mobilenet and squeezenet are simpler deep cnn-based classifiers aimed at running in mobile devices, which provide an interesting contrast with our initial resnet50 selection. on the other hand, inceptionv3 has a network structure of comparable depth and number of parameters. the results of this evaluation are the basis from which we will select the type of model to power the next iteration of our recognition service. current results indicate that resnet50 continues to be a good the table also includes average precision of the model over all classes along with its total number of parameters to assess its complexity as future work, expanding the dataset for different camera angles and configurations, and lighting conditions would be required to ensure model performance in practice. the consumer images in the competition dataset are challenging with various lighting conditions; however, they are still relatively controlled with the same layout and the camera angle. expanding the dataset with more pill types would be also important to support more use cases. we plan to look into extreme classification 46 and metric-learning 47 techniques to handle a much larger range of pills. although our identification approach performed quite well with 924 target pills, giving coverage to the tens of thousands fda-approved pills will be challenging. in addition, as the pills with low average-precision score indicate (fig. 4), it is necessary to employ advanced ocr \"in the wild\" techniques (i.e., from photographs of signs and lettering instead of documents) in order to improve identification accuracy since many of these lowprecision pills are all but identical with the exception of their hard- the columns indicate the predicted-pill characteristic, while the rows the true-pill ones. the left matrix adds by color of the pill prediction and its true color and the right aggregates based on shape. to reduce crowding, zero counts are not shown to-read engraved imprint. these areas constitute two rich and interesting avenues for future work."
"2. integral time (t i ) & k p adjustment. set the integral time t i, which is the time between the over-shoot and the under-shoot of the step response, with the proportional controller. decrease t i until the rising time achieves a satisfactory state. if the control state is unstable during the t i setting, decrease k p ."
"zone b in figure 18b is a region wherein the has temperature is higher than the ct. in this condition, the performance of dehumidification weakens because of insufficient cooling. therefore, to improve the dehumidification performance, the has has to cool quickly. to meet this requirement, the atc is decreasing, can obtain fast cooling of has. figure 19 shows a temperature error of the pi and the sf-pi controller under figure 17 condition. the sf-pi controller can also achieve accurate temperature control under conditions of changing temperature and relative humidity by adjusting pi's input value by fuzzy control. table 10 shows the result of figure 19 . the temperature error of the sf-pi controller is reduced by about 50% compared to the pi controller. figure 20 shows the consumption power of sf-pi and pi controller under condition of figure 17 . because the sf-pi controller has a low temperature error, its consumption power is also low under conditions of changing temperature and relative humidity. table 11 shows analysis of figure 20 . the sf-pi controller reduces consumption power by about 2.3% compared to pi controller even in condition of figure 17 ."
"the fuzzy control is performed by inputting the error (e) and the changing error (ce) values by the reference temperature (t ref ) and the heat absorption side temperature of the thermoelectric element. if the dew point temperature (t dp ) is used as the reference temperature for the thermoelectric device heat absorption side (has) temperature control, the change in temperature and relative humidity due to the ambient environment will greatly affect the dehumidification performance. therefore, the reference temperature is set using the band gap temperature (∆t), and the has temperature is controlled below that temperature."
"if two or more thermoelectric elements are used in the dehumidifier, the temperature of each has is different. the has temperature is most closely related to the dehumidification performance, and if the has temperature is higher than the dew point temperature, dehumidification is not performed."
"the results shown in our survey of the existing cnn technology applied to pill identification demonstrate that recent advances in ai make it feasible to mostly automate this task involving pharmacies, patients, first responders, and care providers. in general, this work serves as an example that one of the initial impacts of ai in healthcare will be to streamline tasks, before getting into supplanting care staff in complex and high-stake tasks, such as diagnosing and prognosticating, if ever. on the other hand, ai-powered tools like ours will allow care teams to focus more on the patient increasing their productivity and deceasing the risk of error. it is through these mechanisms that our system can have an effect in the quadruple aim."
"we also created confusion matrices for this model's predictions on the hold-out consumer images. first, we had created a confusion matrix of ndc predictions as it is standard in reporting classification performance, but given the model's high performance and the experiment's high pill count, it was hard to make out the details of it. thus, we include two confusion matrices that group the hold-out images by color and by shape of the predicted-pill ndcs (fig. 3 ) that have few images outside the diagonal, since most of the mistakes made by the model are intrashape and color. this is also clearly shown when looking at the pills in which the model obtained its lowest average precision scores."
"to obtain the models for pill classification, we fine-tune starting from model weights pre-trained on imagenet. in this paper, we provide a comparison between results of applying resnet50, squeezenet, 52 mobi-lenet, 53 and inceptionv3 54 models as the final identifier. we modify the original structure by removing the classification layers at the top, e.g., until the last average pooling for resnet50. we connect the output of this layer with two sequential blocks, each consisting of batch-normalization, dropout, and dense layers. the rate of these new drop-out layers is left as a hyper-parameter of each model training process. we use an adam optimizer 51 whose initial learning rate constitutes the second hyperparameter whose value we will find by search. the rate is decreased to 0.2 every time learning stops after an epoch, as indicated by the validation loss not decreasing. the weights for each model are all learned with the same fine-tuning strategy starting from the pre-trained weights."
"t c is the reference value adjusted by the fuzzy control, e c is the input value of pi controller, and k p and k i are the proportional gain and integral gain used in pi controller, respectively. the rc value is obtained in equation (10) the rc value is obtained in equation (10)"
"the third most common cause of death is not disease, but medical error, with 250-400 k or more mortalities per year. [cit] the epidemic of medical error gained attention in reports from the institute of medicine, 3, 6 which found that the most common type of preventable medical error is medication error, which results in over 1.5-m injuries and over $3b in complication costs alone. [cit] report further provides guidelines on reducing the high frequency and unacceptable cost of medication error, including greater use of information technology, which could be implemented at each stage from prescribing and dispensing through to monitoring the patient's response. technology solutions have been applied to drug reference information, drug-drug interactions, drug allergies, and threshold warnings for high doses. despite the common occurrence, there is little research funding in medical error, particularly when compared with other leading causes of death, such as heart disease and cancer. however, there are clear cost benefits; computerized medication systems have the potential to reduce errors by 84% and save hospitals over $500 k/year in direct costs. 7 [cit], the triple aim: 8 patient experience, outcome, and cost, became the quadruple aim to include care-team experience. 1 personalized medicine squarely addresses the outcome, with pioneering advances in research; 9,10 however, personalized medicine is nascent years or generations from generalizing in earnest in clinical settings. [cit] for example, we show how deep learning can more immediately address the quadruple aim by providing tools that improve task efficiency and seamlessly fit into clinical workflows. this improves the patient and care-team experience while improving quality and cost, and furthermore, as demonstrated here, these technologies already generalize beyond specific settings or use cases, here on real-world, mobilegenerated images."
"where α (v/k) is the seebeck coefficient, r (ω) is the inner resistor of thermoelectric element, and k (w/m·k) is the coefficient of thermal conductivity. the first term is the influence of the peltier effect, the second term is the influence of the joule's effect and the last term is the amount of thermal transfer by conduction from the high temperature side to the low temperature side in equations (2)-(3)."
"mode selection for dehumidification/exhaust -auto: automatically control using set humidity, current humidity and temperature -start: forced starting -stop: forced stopping 7 9 adjusting set humidity value adjusting set humidity value figures 8-10, respectively show the changing of the dehumidifier operating state according to set a web page for remote control. figure 11 shows a system structure and a circuit diagram used in this paper for experiment. table 5 shows the major parts used in figure 11 ."
"in this paper, anti-windup is applied using output limit. the problem of windup is that the output of the accumulator integrator reduces the influence of the proportional controller in the next state, resulting in delayed control. therefore, the method used in this paper limits the output value from the pi controller and continues to use the limited value when calculating the next state value. therefore, even if the state changes to the next state, the influence of the proportional controller output can be maintained because the output of the pi controller has a limited value. the following figure shows the the purpose of this paper is to improve the temperature control performance of a dehumidifying system using a thermoelectric device. in this paper, we design a system to control the input value of the pi controller because the pi controller has a limitation in the performance improvement due to the fixed gain value. control of input values of pi controller uses fuzzy control that does not require mathematical modeling and robustness to a nonlinear system."
"the implementation of our pill identification service is composed of two web-based apis that handle segmentation first and then identification. the apis are hosted in separate azure vms with a python implementation based on the flask framework, and using models with a tensorflow 55 back end. the segmentation service locates pills in the image it receives as a parameter. it responds with a list of bounding-box and confidence-score pairs for all the pills it has found. the client is then responsible for cropping the source image based on the received bounding boxes to obtain pillcentered images. requests to the identification service also require an image as a parameter. these are expected to be generated from the first api, since the identification model is trained and validated on images generated with it. in particular, the second api works on the assumption that there is only one pill per image, roughly centered and covering a big portion of its area. the service response consists of a ranked list of the top-5 pill identity predictions of the model based on confidence. these predictions consist of an ndc and a confidence-score pair for each of the pill image possible medications. the ordered list of ndcs are combined with related information from nlm pillbox 18 for the client-side convenience, although it is optional. in terms of the runtime speed, api response time from a 4g mobile network is estimated to be 0.14 s and 0.05 s for the detection api and the identification api, respectively. api end-to-end performance was measured using simulated 4g (upload 3.5 mb/s, download 4.0 mb/s, 20 round-trip time) and 3g (upload 500 kb/s, download 750 kb/s, 100 rtt) mobile networks. an azure vm with nvidia tesla v100 gpu and intel xeon cpu e5-2690 v4 (2.60 ghz) was used for running the service. the details are shown in table 2 ."
"the pi controller controls the pulse width modulation (pwm) for the thermoelectric device control, and the fuzzy control controls the input value of the pi controller. in this paper, fuzzy control and pi controller are arranged and connected in series to construct a system that operates like this."
"the fuzzy control is performed by inputting the error (e) and the changing error (ce) values by the reference temperature (tref) and the heat absorption side temperature of the thermoelectric element. if the dew point temperature (tdp) is used as the reference temperature for the thermoelectric device heat absorption side (has) temperature control, the change in temperature and relative humidity due to the ambient environment will greatly affect the dehumidification performance. therefore, the reference temperature is set using the band gap temperature (δt), and the has temperature is controlled below that temperature."
"the input value of fuzzy control is an error (e) and a changing error (ce), and its output is a value (rc) to adjust the input value of pi controller. the fuzzy rule base is as table 3, and figures 5 and 6 show the membership function for input and output value of fuzzy control. the membership functions in fuzzy control are formed using straight or curved lines. the triangular membership function and trapezoidal membership function in straight membership function are the most common methods. due to their simple formulas and computational efficiency, these methods have been used widely. the performances of the membership functions have compared about vary control and the triangular and trapezoidal types can obtain benefits to real time control. the advantages of the triangular and trapezoidal types are proposed by many studies [cit] . furthermore, since the triangular type has robustness in terms of the response speed and the error in steady-state, it has used extensively in the related region [cit] . therefore, the fuzzy control used in this paper has applied the triangular membership function."
the pi controller is widely used in industrial fields because it has advantages in that the structure is simple and the response between the control and the control value is clear.
"here, we presented a prescription-pill identification method based on a fully convolutional network (fcn) employed as a blob detector. one of the benefits of using a fully convolutional network is that the background textures can be removed from pill images using the predicted segmentation masks. another advantage is that our approach can locate multiple pills in an input image. this method is accurate, scalable, and rapidly deployed as an api, and has direct applicability in medication reconciliation in addition to other use cases not limited to administration of medications, adherence, and counterfeit detection, the latter estimated to be a $75b challenge. 56 while we believe that training and testing a model on mobile images will likely result in a method that generalizes well, further investigation is needed to study the efficacy of a similar technology in a pharmacy, er, or other healthcare (or clinical) setting. we demonstrate with this use case the application of deep learning to empower patients and care teams with tools that streamline tasks and directly impact all four points of the quadruple aim: 1 improving the patient experience, outcome, cost, and care-team experience."
"this paper improves the temperature control performance of dehumidifier using a thermoelectric element by sf-pi controller. the sf-pi controller performs switching operation more frequently than a conventional pi controller for accurate temperature control. this results in a reduction in power consumption. therefore, research to improve performance of the power loss and temperature control by optimized switching operating is necessary."
"the pill image recognition challenge dataset consists of 7000 pill images of 1000 pill types specifically designed for the contest. [cit] reference images and 5000 consumer-quality ones. reference images have controlled lighting and background, while the consumer-quality ones have variable conditions. consumer images additionally vary in focus and device type. they imitate the pictures taken by users that would be sent to an automated pill-recognition system."
"our research revisiting automated recognition of pills is motivated by current trends in healthcare relating to the increasing administration of prescription medications in pill form; the emphasis is on medication reconciliation as a quality and safety initiative (the centers for medicare and medicaid services, the institute for healthcare improvement, and the joint commission's national patient safety goals), and the great advances in image classification in the last few years, thanks to new developments in deep learning. we employ the data afforded by the pill recognition challenge to perform a series of experiments that create and evaluate image classifiers powered by different deep convolutional neural network (cnn) models in the task of recognizing pills from images. the results of these experiments allow us to select the best model and parameter configuration to create a proof-of-concept pill identification service that already provides high-certainty predictions on the set of pills that it was created. our results, when considered for a real-world pill identification implementation, dramatically outperform those of the deep-ranking-based approach of the challenge winner. thus, the experimetal results presented below made us decide to continue with a traditional multi-class approach, as we aim to increase the number of supported drug codes. even so, we are fully aware that at some point, we may need to consider approaches based on extreme multi-label prediction or ranking to support a much larger number of codes (currently several tens of thousands)."
"the pi controller consists of a proportional control and an integral control. among them, the integral operation causes the cumulative error due to the steady state. this problem causes saturation problems in the system. therefore, the pi controller results in a control delay when the system is reversed until the cumulative error is removed. this phenomenon is called the windup. the avoiding method of the windup is called the anti-windup [cit] ."
"this paper proposes the sf-pi controller consisting of fuzzy control and pi controller connection. the conventional pi controller is not easily satisfied in both transient-state and steady-state because it is controlled using a fixed gain. however, the sf-pi controller can be expected to achieve satisfactory control performance compared to the conventional pi control because it controls the temperature using the adjusted value by fuzzy control according to the operating conditions."
"therefore, the sf-pi controller proposed in this paper has excellent control performance compared to pi control, and therefore can improve the performance of a dehumidifier using a thermoelectric element. in addition, the iot, part of industry 4.0, of the dehumidifier is performed according to remote control by a web page. by applying industry 4.0 to the dehumidifier system, it is possible to provide remote control and monitoring function to the user, and it is possible to analyze the usage pattern of the user and provide control operation tailored to the user. this paper presents the improvement of the performance of the dehumidifier system using thermoelectric devices and the application of industry 4.0. the most important performance in a dehumidification system is the ability to maintain the temperature for dehumidification. in this paper, we propose a sf-pi controller and confirm that the temperature control performance is improved compared to the conventional pi controller, and the power consumption of the dehumidification system is also reduced. recently, the use of dehumidifying systems has been increasing because of the growing interest in the indoor environment. the system proposed in this paper has improved the performance and power consumption of the dehumidification system and can be used as a system that can provide convenience to users through the application of industry 4.0."
"there is a close relationship between system performance and these gains, so the gains have to be adjusted suitably. these gains generally have the following influence on the system (table 1) . table 2 shows the various ways to adjust the gain of the pi controller [cit] . this paper adjusts the gain of the pi controller through the trial and error method, which is one of the manual methods that does not need mathematical expression and can control the gain value online. the adjusting method of k p and k i (t i ) is as follows. first, after the command value is changed, if the response time is very slow, increase k p, and if the response time is fast but unstable, decrease k p ."
"inception v3 had the best average top-5 accuracy during cv, closely followed by resnet50 and surprisingly mobilenet. the two leading models swapped the performance lead with the hold-out dataset. it is also notable that mobilenet displayed performance levels very close to these two top models that have five times the number of parameters. squeezenet is the smallest and least complex type of model that we evaluated for this domain, and it clearly showed in its much lower level of performance. though not displayed in these results, squeezenet also had the largest variance in cv accuracy during the hyper-parameter search with one configuration only achieving 34.35% of top-5 accuracy."
"the main goal of this work is to reliably identify pills from images under any imaging condition in order to provide accurate medication information. as such, we chose classifiers that optimize obtaining the highest accuracy rate with robustness to these challenges when users freely take pictures."
"the atc is decreasing, can obtain fast cooling of has. figure 19 shows a temperature error of the pi and the sf-pi controller under figure 17 condition. the sf-pi controller can also achieve accurate temperature control under conditions of changing temperature and relative humidity by adjusting pi's input value by fuzzy control. table 10 shows the result of figure 19 . the temperature error of the sf-pi controller is reduced by about 50% compared to the pi controller. in figure 18b, zone a is an area where a heat absorption side (has) temperate of thermoelectricity is lower than the ct. in this case, to reduce unnecessary cooling time, adjusted control temperature (act) increases, and because of this, the power feeding to thermoelectricity can stop quickly."
"humid air entering the interior by the fan contacts the surface of the cooler and condenses into water [cit] . in general, a dehumidifier removes moisture in the air by this method and discharges the dehumidified air. dried exhaust air is used to cool the temperature of the refrigerant used to drop the temperature of the cooler. as a result, the temperature of the output air is raised. figure 1 shows the principle of the dehumidifier. figure 2 also shows the structure of the thermoelectric element. when a current flows through a thermoelectric element, one side of the thermoelectric element made of p-n semiconductor absorbs heat and the other side outputs heat. the joule's heat caused by the current (i) flowing into the thermoelectric element each enters the top and bottom side. therefore, the top side has low temperature (tc), and bottom side has high temperature (th). the temperature humid air entering the interior by the fan contacts the surface of the cooler and condenses into water [cit] . in general, a dehumidifier removes moisture in the air by this method and discharges the dehumidified air. dried exhaust air is used to cool the temperature of the refrigerant used to drop the temperature of the cooler. as a result, the temperature of the output air is raised. figure 1 shows the principle of the dehumidifier. figure 2 also shows the structure of the thermoelectric element. when a current flows through a thermoelectric element, one side of the thermoelectric element made of p-n semiconductor absorbs heat and the other side outputs heat. the joule's heat caused by the current (i) flowing into the thermoelectric element each enters the top and bottom side. therefore, the top side has low temperature (tc), and bottom side has high temperature (th). the temperature differential is as follows:"
"second, if the feedback value does not track the command value, decrease t i, and if the feedback value unstably tracks the command value with vibration, increase t i ."
"humid air entering the interior by the fan contacts the surface of the cooler and condenses into water [cit] . in general, a dehumidifier removes moisture in the air by this method and discharges the dehumidified air. dried exhaust air is used to cool the temperature of the refrigerant used to drop the temperature of the cooler. as a result, the temperature of the output air is raised. figure 1 shows the principle of the dehumidifier. figure 2 also shows the structure of the thermoelectric element."
"the pi controller controls the pulse width modulation (pwm) for the thermoelectric device control, and the fuzzy control controls the input value of the pi controller. in this paper, fuzzy control and pi controller are arranged and connected in series to construct a system that operates like this."
"we create t-sne 44 visualizations (fig. 2) of the final highdimensional hidden layer of the resnet50 model of the hold-out consumer images. by visualizing in 2d the model output, we see that the cnn model detects groupings of similar pill categories of color and shape, without explicitly including these features in the model training."
"we believe that current developments in ai make it possible to create systems that will have an immediate impact in multiple pharmacy and first-responder processes throughout the healthcare industry. these ai-powered systems and services have the potential to change for the better how patients and healthcare teams communicate and interact with each other, as well as, how staff and practitioners are empowered by smart tools that help them to be more efficient and accurate."
"average temperature error (°c) 0.45 0.1 figure 16 shows the consumption power and the average consumption power of the pi controller and the sf-pi controller under the conditions of table 4 . the consumption power of thermoelectricity is generated in a cooling period. therefore, to decrease its consumption power, the cooling time has to be reduced. the sf-pi controller has a shorter cooling time than pi because the temperature error is low. therefore, the sf-pi controller has a short cooling time, and the consumption power is reduced. table 9 shows the average consumption power in figure 16 ."
"the coefficient of performance (cop) of the thermoelectric element is the ratio of heat absorption to the input power, and is as follows [cit] :"
"if two or more thermoelectric elements are used in the dehumidifier, the temperature of each has is different. the has temperature is most closely related to the dehumidification performance, and if the has temperature is higher than the dew point temperature, dehumidification is not performed."
"the fuzzy control is performed by inputting the error (e) and the changing error (ce) values by the reference temperature (tref) and the heat absorption side temperature of the thermoelectric element. if the dew point temperature (tdp) is used as the reference temperature for the thermoelectric device heat absorption side (has) temperature control, the change in temperature and relative humidity due to the ambient environment will greatly affect the dehumidification performance. therefore, the reference temperature is set using the band gap temperature (δt), and the has temperature is controlled below that temperature."
"if two or more thermoelectric elements are used in the dehumidifier, the temperature of each has is different. the has temperature is most closely related to the dehumidification performance, and if the has temperature is higher than the dew point temperature, dehumidification is not performed."
"deep cnns have been proven successful in multiple tasks identifying object categories in pictures taken with realistic conditions of varying illumination, focus, and perspective. for this reason, we decided to employ a classification framework instead of a similarity-based one, as it was encouraged in the experimental setup of the pill recognition challenge where we got the data for these experiments."
"raspberry pi and the arduino were used in this paper for the implementation of the iot, one element of industry 4.0. raspberry pi builds the apache http server (version: 2.2.22, apache software foundation, asf, forest hill, md, usa) and configures the database with mysql (version: 5.6.23, mysql ab, cupertino, ca, usa) to save the data needed for control. arduino measures the temperature inside the dehumidifier, stores it in the database (db), and controls the thermoelectric elements inside the dehumidifier. figure 7 shows a web page for the remote control. in addition, table 4 shows the explanation of remote web page. exhaust control set humidity humidity setting for auto exhaust control 6 8 dehumidification/ exhaust control mode setting"
"the amount of heat absorption (qc) on the low temperature side and radiation of heat (qh) on the high temperature side are as follows [cit] : when a current flows through a thermoelectric element, one side of the thermoelectric element made of p-n semiconductor absorbs heat and the other side outputs heat. the joule's heat caused by the current (i) flowing into the thermoelectric element each enters the top and bottom side. [cit], 7, 98 4 of 23 the top side has low temperature (t c ), and bottom side has high temperature (t h ). the temperature differential is as follows:"
"to provide appropriate healthcare and avoid medication errors, it is paramount to know which medications a patient is taking. 15 discrepancies in medication are common, over half of patients in one study at the time of hospital admission, with 39% capable of causing injury, and the most common type of discrepancy is errors of omission, leaving out medications a patient is taking. 16 it is frequently a challenge for consumers to identify pills when pills are transfered to different containers, combined to a single container for convenience, or portioned into day-of-week pillboxes to simplify medication management. while generally well intended, when patients separate medications from their original bottles or packaging, this presents a challenge to their healthcare teams. pharmacies often host brown bag consultations, 17 where patients are encouraged to bring in their unknown pills in brown-paper bags for pharmacists to identify. a reference search by hand-entering physical characteristics (color, shape, and imprint) of over 10,000 fda-approved medications 18 is a slow, tedious, and error-prone process that requires dexterity to handle small pills, vision to read small writing, and some degree of health literacy."
"finally, we present the pills that had the lowest average precision in fig. 4 . it is easy to understand the reason why these particular pills are worsening the model performance. in fact, groups of these pills in combination are the hardest for the model since they are easy to confuse. most of them have an engraved imprint, which is more difficult to read. these pills are a clear example of the intrashape and color mistakes indicated by the confusion matrices above, since they all are either white round tablets or capsules."
"the pi controller controls the pulse width modulation (pwm) for the thermoelectric device control, and the fuzzy control controls the input value of the pi controller. in this paper, fuzzy control and pi controller are arranged and connected in series to construct a system that operates like this."
"in the power consumption comparison of tables 7, 9, and 11, the difference between the pi controller and the sf-pi controller is 1.79 wh, 2.4 wh, and 0.5 wh, respectively. the difference between these values seems to be very small from a typical numerical comparison. however, this is due to the small capacity of the system, and a percentage comparison shows a 7.6%, 11.9%, and 2.3% reduction in power consumption, respectively. therefore, if the system capacity is increased or the used number increases and the usage time and duration are increased, the effect will be increased."
"labels based on national drug codes. the national drug code (ndc) is a unique 10-or-11-digit, 3-segment number. it functions as a universal product identifier for human drugs in the united states. our implementation goals for recognizing pills require providing medication information based on the predicted identity of the pills. thus, we decided to use the ndc label and product codes of the pills available as part of the challenge dataset identifiers instead of the original labels provided by the nlm dataset creators. some pills in the dataset were put into different classes, given that they are versions of the same ndc with different appearances. thus, as we changed into ndc-based labels, some of these pills merged into a single group producing a 924-class dataset for our experiments."
"the following recognition results abide to an experimental setup that finds an optimal hyper-parameter configuration to learn each model, which also provides a realistic and fair evaluation. this is a very important consideration, given our desire to power a pillidentification service with the resulting model. details about the dataset we employ and the differences between reference and consumer pill images can be found in the methods section. our protocol is comprised of using a hold-out set formed by 20% of the consumer images set combined with a hyper-parameter search consisting of a fourfold cross-validation (cv) on different parameter value combinations for each cnn model-configuration setup. the cv and final model learning are carried out with the remaining 80% consumer images and all the reference ones. table 1 contains the top-1 and top-5 accuracy results obtained in the best cv average (cv avg.) in the configuration search for each model and (evaluation) with the best configuration in the hold-out consumer images. the cv results shown are for the configuration with the highest top-5 cv avg of each model. a final instance is learned with this configuration, which was then applied to the hold-out test to obtain an estimate as close to the performance of a service using any of these models in the real world."
"average power consumption (wh) 22.5 20.1 the condition of table 6 is a constant indoor environment. to analyze changing environments, figure 17 shows an indoor environment condition where the dew point temperature is increasing according to decreasing relative humidity caused by indoor temperature increasing."
"the average temperature error of sf-pi controller between the reference temperature and the thermoelectric element temperature is 22% of traditional pi's value. it also decreases the consumption power by about 10% by prevention of overwork cooling. despite temperature and humidity variation, the temperature error and the consumption power of the sf-pi controller is respectively reduced by about 50% and 2.3% compared to the pi controller."
"raspberry pi and the arduino were used in this paper for the implementation of the iot, one element of industry 4.0. raspberry pi builds the apache http server (version: 2.2.22, apache software foundation, asf, forest hill, md, usa) and configures the database with mysql (version: 5.6.23, mysql ab, cupertino, ca, usa) to save the data needed for control. arduino measures the temperature inside the dehumidifier, stores it in the database (db), and controls the thermoelectric elements inside the dehumidifier. figure 7 shows a web page for the remote control. in addition, table 4 shows the explanation of remote web page."
"suppose the user u post a comment at time ct, then we can get the total number of comments tc during the fixed time window δt during [ct-δt, t]. among these comments, sc indicates the number of comments internet water army had posted. so we derived the impact factor p like:"
"in fig 3, the internet water army's retweet ratio is much lower than legitimate users'. the legitimate users prefer to post tweets by themself to present their opinions or feelings instead of focusing retweeting to spread others' opinions on purpose."
"we crawled some popular tweets whose comment number is over 7000. we picked up 307 tweets from them with all comments and reviewers' personal information and relationships in sina microblogging service. we collected 4,000,000 comments, more than 100 million unique users and personal relationships in total. then we manually labeled the microblogging users to legitimate user or internet water"
"firstly, we defined the similarity between different comments with the jaccard coefficient. we calculated similarity between all comments with length over five chinese characters [cit] ."
"queue.pop_front() queue.push_back(c(i)) end if end while algorithm 1. meiwa online algorithm according to table 2, we can see that the rank and bilateral group has a high accuracy relatively, and these two features doesn't need to be calculated in advance in which is easy to get in a constant time. so we proposed an algorithm using these two features to measure a person's credibility comment. the algorithm 1 ensures the higher reliability comments more likely to remain in the user's visible time window. at the beginning of the algorithm a queue is used to save the trusted comments in current time window. whenever a new comment is generated, it will calculate the ratio of user's rank and the average user rank in the queue, prank as well as the ratio of user's bilateral friends and the average of the queue's pbiraito. then we calculate the value p as the square root of both parameters. the value p is the possibility of the comments whether it should be pushed in the comments queue to be viewed by uses. algorithm 1 is an online algorithm, which ensure the order of time and effectively reduced the influence of internet water army. table 3, under the meiwa algorithm, the influence coefficient of internet water army reduced from 0.738 to 0.122, reduce the effect to one sixth. meanwhile, the algorithm in linear time guarantees the reviews ordered by time which is consistent with the users' reading habits. as the new meiwa algorithm treats the quality of tweet by each user as the main reason to keep the tweet in the watching queue. so uses are more likely to see tweet posted by legitimate uses as queue's property first in first out keep the sequence of posting."
"in sina microblogging service, pr companies often hire internet water army to retweet or reply tweet to support or against some specific incident [cit], attack competitors, cover up the truth and obtain public's supports and sympathy. in psychology theory, people have a conformity mentality [cit], the personal behavior in the crowd affected by the outside world, people prefer to the same performance with majority of people around them in their perception, judgment and cognitive. some pr companies take advantage of the people's psychological law to hire internet water army to create majority of a fake people's point in outward seeming, to stop legitimate users expressing their real voice and affect their point of specific event. the contributions of our work can be summarized by as followings:"
"(1) the internet water army also likes to forward the tweets to expand their influence on microblogging service. with forwarding behaviors are more widely viewed by people, we can proceed with a more comprehensive understanding of the internet water army's behavior."
"by calculating the impact factor p we can measure how many comments posted by legitimate users. as the factor defined, the more sybil comments user have viewed, the factor is larger."
"nowadays these users are called internet water army. the internet water army refers to someone are often hired by pr companies, to post specific content or replies in network and get reward. in social network, the internet water army compared to traditional one is more widely spread and influential."
"by looking up the data in the microblogging service, some comments between the users or the user self are repeated or quite similar. in the paper [cit], the author also mentioned some conclusions, which indicated these similar comments posted by different users are very suspicious. so we define a comment similarity feature."
"the experiment results showed the classification algorithm worked well. but we can only detect one tweet if it is attacked by internet water army after the attack. during this time, the legitimate users had been affected by the internet water army."
"in our work, we focus on the review posting behavior on microblogging service. we majorly do three aspects of the work. firstly, the user behavior measurement is done between legitimate user and internet water army. secondly, the internet water army detection is conducted, and we propose a new online algorithm to reduce the influence of internet water army eventually on microblogging service."
"in previous work, spammer detections are mostly focused on the publication of malicious urls [cit], irrelevant advertising user detection or measuring user behaviors [cit] ."
"firstly, we measure the both legitimate users and internet water army's behavior from different perspectives. we use a variety of machine learning-based classification algorithms to detect internet water army with a precision more than ninetytwo percent. finally we proposed a new model to measure how internet water army affects the legitimate users. we reduced internet water army's influence to legitimate users one-sixth in average with a new online algorithm"
"in fig 2, internet water army's microblogging user rank is lower than legitimate user. because the internet water army usually doesn't care about what they had posted, the account online time and so on to get higher user rank."
"in some related works, the words like spammer and sybil refer to the users who have malicious attacks, distorted the true, disseminating irrelevant advertising and so forth [cit] ."
"generally people take some methods to deal with related problems like machine learning [cit] algorithms for detecting spammers [cit], detecting the publishing time distribution to find out abnormal situations [cit], take advantage of the relationships between users in social network data for pattern discovery or community detection [cit] ."
"as shown in the above formulas, s(c) is collection of ratings c which was generated by the method of bigram segmentation. and then we used the hash function for fast matching [cit] to calculate both set intersection and union. jaccard (c1, c2) indicates the jaccard coefficient between comments c1 and c2. v (c) represents the maximum similarity among others comments. sim (u) represents the user u's maximum similarity among his all comments' similarity m(c)."
"secondly, we calculate the ratio of retweets over all tweets. the retweetrio(u) represents whether a user prefer original tweet over retweeting thirdly, according to the experimental results, when a lot of comments are published in a short period of time, it means internet water army is doing their works in social network site [cit], posting a massive number of comments. so we defined comrio(u) which indicates the average of the number of comments posted by user u during the time."
to the best of our knowledge few work is done on internet water army detection in social network. we are also the first to propose a method to measure the influence of the internet water army and a new algorithm to reduce the influence.
"by feature extraction and machine learning could achieve a high precision classification to distinguish internet water army and legitimate users. however, the text similarity calculation is often essential to wait until the end of the event calculated off-line, and the high complexity is unbearable."
"fifthly, we collected users' bilateral relationship as bilateral(u) which indicates if the user builds regular relationship in social network sites as people often build their social networks from real world relationships."
"after above features were normalized, we adopted a data analysis tools named weka [cit] for a classification experiment using cross-validation method to verify the classification results' precision and recall. as shown in table 1, when we adopted all features for classification, the precision and f value are both over ninety percent in average with four kinds of classification algorithms. which means the features what we had chosen are appropriate for the classification problem."
"based on manually labeled data, we measured users' behaviors through the ratio of users' bilateral friends, the ratio of users' retweet and users' microblogging ranks to see what difference lie between legitimate users and internet water army. army's bilateral friend ratio is significantly lower than legitimate users'. the reason is that the feature of bilateral friends is usually based on relation in real world. people already know each other offline are more likely add friends with each other in social networks. but internet water army doesn't care about the real relationship in social network. this is the main reason why internet water army's bilateral friend ratio is much lower."
"fourthly, we collected user rank from user information from microblogging service as rank (u). microblogging user rank is officially defined which indicates the user online time and daily continuous login time. in order to get a higher rank, user must login microblogging account for a long time or post original tweet. obviously legitimate users have higher ranks than internet water army from the figure 2."
"army. we labeled the data by looking up the personal information, home page, photos albums, comments content to determine whether the user is internet water army or not. judgment is based on whether the publication is exaggerated, self-contradicted [cit], personal user information is fake or user rating is low. a total number of 212 internet water army and 732 legitimate users were labeled."
"to prove the different features bring the different results in the classification, we made different combinations of the features. in order to control variables, all combinations used logistic algorithm for classification experiments. as shown in table 2, with only rank, bilateral, sim three features in the classification algorithm can achieve 0.91 accuracy and recall. we will use this combination for the new linear complexity algorithm's features."
"therefore, we proposed a new method to measure the how much internet water army had impacted the legitimate users and a new online algorithm to reduce the impact. as the comments are ordered by time, so people always see the latest comments about one tweet. based on the situation, we proposed a model to evaluate the internet water army's impact on legitimate users."
"as social network websites are popular around the world, more and more people use the social network product like twitter [cit], sina microblogging, facebook and so forth. people spend a lot of time on them to obtain news and information whatever they want. according to the official announcement, the number of sina microblogging user has exceeded 500 million, daily active users reached 46.2 [cit] . with huge number of users and active days, microblogging service attracts a large number of users, but also some others who were specifically hired to publish or disseminate some specific information on social network."
"since spammers affect the normal use of the internet users badly. for example, they may make some fake review and fake scoring [cit], overstate the effect of product, post some malicious comments against the competitors [cit] or duplicate comments in the forums, publishing articles and irrelevant reviews, disseminate irrelevant advertising [cit] . in social networking sites, they also spread malicious links, publish irrelevant ads and content [cit] ."
"(2) with the deep into internet water army's more detailed behaviors, we found that there are much more classifications in themselves. for example, some water army is only responsible for supporting some super stars. and some of them were devoted to the social phenomena event to post some tweets or give a review. for the different kinds of internet water army, we would like to do some research about how much influence they really have."
"in this paper we measured the internet water army's behavior from multiple dimensions. then we selected several effective features as the training model and use machine learning methods for classification. based on the behavior of users viewing the comment, we proposed a model to measure the influence of internet water army. in order to reduce the influence coefficient, we proposed a new linear time complexity online algorithms named meiwa. the new algorithm results showed that the influence is reduced to one sixth of the sequence strategy which is used by default with ensuring users' viewing comments habits. in this paper, there are still a lot of future works to do:"
"the dll loop filter was chosen to be similar to (6), with a noise-equivalent bandwidth b n,dll hz. the output of the dll loop filter v dll (in s/s) is the rate of change of the sss code phase. assuming low-side mixing, the code start time is updated according tô"
"where g j is the number of the pozs for unit j; p l j,k and p u j,k are the lower and upper bounds of the k-th pozs of unit j."
"in the de algorithm, the scaling factor and crossover rate have an important influence on the global search ability and convergence speed of the algorithm. the control parameters in the traditional de algorithm are selected from fixed values, so the diversity of parameters is relatively poor, for some special problems, the effect of fixed parameters is not very good. this paper proposes a method of dynamically adjusting parameters, in which the control parameters are converted from fixed to dynamic."
"for the experimental setup, as shown in figure 5 (b), zurich impedance instrument [cit] has been used to measure the aircore sensor induced signal response -mutual impedance/inductance of the sensor influenced by the tested samples. the working frequency range of the zurich instruments is from 1 khz to 500 khz. the amplitude of the excitation current is 10 ma. for the simulation modelling, both the finite-element method (fem) and analytical solution (dodd deeds) have been used to calculate the mutual inductance. table ii and table iii illustrate the parameters and modelling element dimensions for the brass and copper samples. at the end of this part, the eddy current distributions for both structures are presented and discussed."
step 5: obtain cr t m according to formulas (18) and (19) . perform crossover operation and generate the trial vector. modify the trial vector that violates equality constraints and inequality constraints. evaluate the fitness value of the individual according to formulas (20) and (21) .
"3) pcfich decoding: the ue first obtains the control format information (cfi) from the physical control format indicator channel (pcfich). the cfi indicates the number of res dedicated to the downlink control channel and can take the values 1, 2, or 3. to decode the cfi, the ue first locates the 16 res dedicated to the pcfich. then, it demodulates the obtained symbols by reverting the steps in fig. 7, which results in a sequence of 32 bits. finally, this sequence, which can be only one of three possible sequences, is mapped onto a cfi value."
"in this paper, a differential evolution algorithm based on multi-population (mpde) is proposed, the mpde algorithm improves the traditional de algorithm in the two aspects."
"software-defined receivers (sdrs) have been recently proposed in the literature for navigation using lte signals [cit] . however, there are several challenges associated with navigating with these sdrs, which rely on acquiring the primary synchronization signal (pss) transmitted by the lte base station (also known as enodeb). the first challenge results from the near-far effect created by the strongest pss, which makes it impossible for the receiver to individually acquire the remaining ambient psss. a simple solution would be to track only the strongest psss (up to three). this raises a second challenge: the number of intra-frequency enodebs that the receiver can simultaneously use for positioning is limited [cit] . to circumvent this problem, other cell-specific signals can be tracked, in which case the receiver must obtain high-level information of the surrounding enodebs, such as their cell ids, signal bandwidths, and the number of transmitting antennas. the literature on lte-based navigation assumes this information to be known a priori, which raises the third challenge associated with the published sdrs. in practice, it is desirable to have a receiver that is capable of obtaining this information on-the-fly in unknown environments."
the actual operating range of all the online units is restricted by their corresponding ramp rate limits. the ramp-up and ramp-down constraints can be written as follows: (24) where p 0 j is the previous generation of unit j; dr j and ur j are down and up ramp rate limits.
"for em simulations with the fem method, the sensor's response, i.e. mutual inductance is not easy to be computed especially under the high frequency. an extremely fine mesh is required to accurately simulate eddy current skin effects especially at high frequencies, and this could cause an extremely large total mesh for the modelling. in this paper, an equivalent-effect phenomenon is found, in which an alternative thicker structure but with less conductivity can produce the same impedance value as the original structure if a reciprocal relationship between the electrical conductivity and the thickness of the structure is observed. since the equivalent structure has fewer mesh elements, the calculation burden can be significantly relieved when using the fem method. the proposed equivalent-effect phenomenon has been validated from the measurements, analytical and fem simulations for several types of structures."
"after obtainingĥ (u) (n), the method proposed in section iv can be exploited to determine the first peak ofĥ (u) (n), which represents n (u)"
"both the fem and dodd deeds solver were scripted by matlab, which are computed on a thinkstation p510 platform with a dual intel xeon e5-2600 v4 processor and 32gb ram."
"in order to test the comprehensive performance of the improved algorithm, we tested six cases of 13-, 40-, 80-and 140-unit test systems with this algorithm. all cases are coded in c++ [cit], which are tested on a pc with intel i5 2.3ghz processor, 4gb of ram and windows 10 professional, each case runs 50 times independently and we compare them with the results of other intelligent algorithms."
"t he global positioning system (gps) has been at the core of virtually all navigation systems over the past few decades, providing accurate positioning and timing information for both military and civilian applications. however, gps signals are severely attenuated indoors and in deep urban canyons and are susceptible to unintentional interference, intentional jamming, or malicious spoofing [cit] . recent approaches to overcome gps drawbacks aimed at exploiting ambient signals of opportunity (sops). sops are radio frequency (rf) signals that are not designed for navigation purposes and are freely available when gps signals are unusable [cit] ."
"3) comparison with other methods: prior solutions on navigating with lte signals include: (1) detecting the first peak of the cir using a constant threshold [cit] or an adaptive threshold [cit], (2) estimating the cir using estimation of signal parameters via rotational invariance techniques (esprit) and kalman filter, i.e., ekat algorithm [cit], and (3) tracking the crs [cit] ."
"case 3 was run independently for 50 times with the mpde algorithm. figure 6 shows the convergence characteristics of the mpde algorithm when solving case 3. table 10 shows the output of each generator unit at the lowest total generation cost. table 11 shows the results of the mpde algorithm and other intelligent algorithms to test case 3, including the minimum cost, mean cost, maximum cost, standard deviation value, time and nfe. it can be seen from table 11, for dataset1 (40−unit) ), only ema, mcsa, nseo and mpde can get the lowest cost, mcsa takes less time than mpde, but mpde has good performance in terms of the mean cost, the maximum cost and the standard deviation. relatively speaking, mpde is relatively stable. hde, de/bbo, fapso-vde and cba are better than mpde in time, but in other respects, mpde is superior to them. for dataset2 (40−unit) ), dhs and mpde can get the lowest cost and dhs is better than mpde in terms of time, but the mpde has the best performance in terms of the maximum, minimum and mean value. by comparing the results of the mpde algorithm with other intelligent algorithms, the proposed algorithm outperforms other intelligent algorithms in solving case 3. table 12 depicts the ranks computed through the friedman test. as can be seen from table 12, for dataset1 (40−unit) and dataset2 (40−unit) the mpde has the smallest mean ranking so it ranks the first."
"the frequency reuse factor in lte systems is set to be one, which results in high interference from neighboring cells. under interference and dynamic stress, flls have better performance than plls. however, plls have significantly higher measurement accuracy compared to flls. an fll-assisted pll has both the dynamic and interference robustness of flls and the high accuracy of plls [cit] . the main components of an fll-assisted pll are: a phase discriminator, a phase loop filter, a frequency discriminator, a frequency loop filter, and a numericallycontrolled oscillator (nco). the sss is not modulated with other data. therefore, an atan2 discriminator, which remains linear over the full input error range of ±π, could be used without the risk of introducing phase ambiguities. a thirdorder pll was designed to track the carrier phase, with a loop filter transfer function given by"
"the obtained ofdm signals are arranged in multiple blocks, which are called frames. in an lte system, the structure of the frame depends on the transmission type, which can be either frequency division duplexing (fdd) or time division duplexing (tdd). due to the superior performance of fdd in terms of latency and transmission range, most network providers use fdd for lte transmission. hence, this paper considers fdd for lte transmission and for simplicity an fdd frame is simply called a frame."
"where s c rs denotes the set of subcarriers containing the crs, which is a function of the symbol number, port number, and the cell id; and d (u) i (k) represents some other data signals. assuming that the transmitted signal propagated in an additive white gaussian noise (awgn) channel, the received signal in the i -th symbol will be"
"in the de, the mutation operation is the creation of a mutation vector for each individual of the current population by a random perturbation method. the differential evolution algorithm maintains the diversity of the population through mutation operation. the most widely used de mutation strategies are shown as follows [cit] : de/rand/1:"
"in the second step, channel coding is performed using a convolutional encoder with constraint length 7 and coding rate 1/3. the configuration of the encoder is shown in fig. 5 . the initial value of the encoder is set to the value of the last 6 information bits in the input stream. the method illustrated in fig. 6 is used to decode the received signal [cit] . in this method, the received signal is repeated one time. then, a viterbi decoder is executed on the resulting sequence. finally, the middle part of the sequence is selected and circularly shifted."
"there are multiple individuals in the population, and each individual can be considered as a solution in the search space. if there are n p individuals in the population, the population can be expressed as:"
"in this paper, based on the transverse electric (te) propagation algorithms through medium with different material layers, an equivalent-effect phenomenon is discovered, in which a reciprocal relationship is found between the electrical conductivity and thickness of the thin tested piece for the same sensor output signal responseimpedance/inductance. the fundamental principle of the proposed equivalent-effect phenomenon is using an alternative thicker structure but with less conductivity to have almost the same impedance value as the original structure. with the proposed equivalent-effect phenomenon, the impedance computation burden will be significantly reduced, which can be explained by two aspects. firstly, under the same frequency, a conductive metallic structure with much lower conductivity will be less affected by the diffusion/skin effect. consequently, fewer elements are needed for meshing/modelling the less conductive structure. secondly, the thinner structure requires finer element size in order to stay at the same accuracy. as a result, much intensive and more overall mesh elements are needed for the eddy current computation of the original structure's each layer. most important of all, the measured signal -mutual impedance is almost immune to the altering of the structure's electrical and geometric properties controlled by the found equivalent-effect phenomenon. the detected sensor response signal -mutual impedance at the sensor's terminals is usually treated as the basic parameter for the flaw inspection especially in the nondestructive testing/evaluation (ndt/nde) applications [cit] . overall, based on the proposed equivalent-effect phenomenon, an alternative thicker structure with less conductivity can be equivalent to the thin metallic layer modelling, which can significantly reduce mesh size without affecting the detected mutual impedance."
"in recent years, interest in long-term evolution (lte) signals as sops has emerged. lte has become the prominent standard for fourth-generation (4g) communication systems. its multiple-input multiple-output (mimo) capabilities allowed higher data rates to be achieved compared to previous generations of wireless standards. the high bandwidths and data rates employed in lte systems have made lte signals attractive for navigation as well."
"here, e r denotes magnitude of the electrical field on the coils position; indicates the region of the sensor coil; r is the radius of the sensor coil."
"throughout the paper, italic small bold letters (e.g., x) represent vectors in the time-domain, italic capital bold letters (e.g., x) represent vectors in the frequencydomain, and capital bold letters represent matrices (e.g., x)."
"a frame is composed of 10 ms data, which is divided into either 20 slots or 10 subframes with a duration of 0.5 ms or 1 ms, respectively. a slot can be decomposed into multiple resource grids (rgs) and each rg has numerous resource blocks (rbs). then, an rb is broken down into the smallest elements of the frame, namely resource elements (res). the frequency and time indices of an re are called subcarrier and symbol, respectively. the structure of the lte frame is illustrated in fig. 1 [cit] ."
"in the timing information extraction stage of the receiver, the toa is estimated by detecting the first peak of the cir. the toa estimate is then fed back to the tracking loops to improve sss tracking. fig. 10 shows the block diagram of the timing information extraction stage. a method for estimating the toa is proposed in section iv."
"the mpde algorithm is sensitive to parameters. the parameters of the mpde algorithm for solving the ed problems are shown in table 1 and table 2 . the brief introduction to the test cases is also shown in table 2, including the valve-point effects (vpe), transmission losses (tl), prohibited operating zones (pozs) and ramp rate limits (rrl)."
"economic dispatch (ed) is a typical optimization problem of economic operation and optimal dispatch, which aims to improve the operation economy and reliability of power system effectively. the purpose of ed problem is to optimize output power of each unit and minimize power system generating cost. when considering the valve-point effects [cit], the characteristic curve of generation unit becomes nonlinear. in addition, there are a great many power generation units in the power system., which can increase the difficulty of the calculation and easily fall into local optimum solutions. because economic dispatch has a series of problems such as non-linearity, non-convexity and multi-dimensionality, some mathematical methods cannot solve ed problem well, for example, linear programming algorithm (lp) [cit], quadratic programming algorithm (qp) [cit] and dynamic programming algorithm (dp) [cit] ."
"it is worth mentioning that in a conventional timing acquisition, all enodebs must be acquired and tracked separately. in the proposed approach, only the main enodeb needs to be acquired and tracked, and toa estimates from neighboring enodebs may be obtained by using timing and neighboring cell id information obtained from the main enodeb. the parameter n d depends on the enodeb clock as well as on the distance between the enodeb and the receiver, and it must be calculated for every frame, regardless of the enodeb clock."
"after acquiring the lte frame timing, a ue needs to keep tracking the frame timing for two reasons: (1) to produce a pseudorange measurement and (2) to continuously reconstruct the frame. the pss and sss are two possible sequences that a ue can exploit to track the frame timing. the pss has only three different sequences, which causes two main problems in choosing the pss for tracking: (1) the interference from neighboring enodebs with the same sector ids is high and (2) the number of enodebs that the ue can simultaneously track is limited. the sss is expressible in 168 different sequences, hence does not suffer from the same problems as the pss. therefore, the sss will be exploited for tracking the frame timing. in this section, the components of the tracking loops are discussed, namely a frequency-locked loop (fll)-assisted phase-locked loop (pll) and a carrier-aided delaylocked loop (dll)."
"the de algorithm uses the greedy selection method to perform the selection operation. by comparing the fitness function values corresponding to the x t m and the v t m, the vector with better fitness value is selected as the next generation of individual. therefore, the selection operation can be defined as:"
"step 1: parameters preparation. the total number of power generation units (m ).the cost coefficients of the generator units. the maximum and minimum capacity constraints of all generator units. the total load demand (p d ). the maximum and minimum vales of α t, µ t and q t . the number of individuals in one population (n p ). the maximum iteration number (t max )."
"the parsed dci provides the configuration of the corresponding physical downlink shared channel (pdsch) res. the pdsch, which carries the sib, is then decoded, resulting in the sib bits. subsequently, these bits are decoded using an abstract syntax notation one (asn.1) decoder, which extracts the system information sent on sibs by the enodeb."
"fan and zhang have proposed differential evolution algorithms for a variety of different mutation strategies [cit] . different mutation strategies have different search characteristics. based on this, this paper sets up three equal-sized populations and each population adopts different mutation strategies to perform the mutation operation, as shown in figure 1 . pop1, pop2 and pop3 contain the same number of individuals, and each population has n p individuals. the mpde algorithm assigns different mutation strategies to the corresponding population. pop1 uses de/rand/1 (3) mutation strategy that is the most commonly used, all individuals in the mutation strategy formula are randomly generated, so there is no specific search direction. the de/rand/2 (4) mutation strategy is applied to pop2. the de/rand/2 strategy has relatively good stability for disturbance, and strong global search ability. pop3 adopts de/rand-to-best/1 (8) mutation strategy, which has a relatively balanced performance in global search and local search. the de/best/1 (5) and de/best/2 (6) mutation strategies are based on the optimal individual vector to perform mutation operation, so it is easy to fall into local optimum for complex problems. the de/current-to-best/1 (7) mutation strategy is not stable enough to solve complex problems and has poor robustness."
"2) sib decoding: when a ue performs acquisition, it obtains the cell id of the ambient enodeb with the highest power, referred to as the main enodeb in this paper. for navigation purposes, the ue needs access to multiple enodebs' signals to estimate its state. one solution is to perform the acquisition for all the possible values of n (2) i d . however, this method limits the number of intra-frequency enodebs that a ue can simultaneously use for positioning. the second solution is to provide a database of the network to the ue. in this method, the ue needs to search over all possible values of the cell ids to acquire the right ones unless the ue knows its current position, which is not a practical assumption. the other solution, which is more reliable and overcomes the aforementioned problem, is to extract the neighboring cell ids using the information provided in the sib transmitted by the main enodeb. since other operators transmit on different carrier frequencies, the same approach can be exploited to extract the cell ids of the neighboring enodebs from other operators. knowing the enodebs' cell ids, the receiver only needs to know the position of the enodebs using a database or premapping approaches."
"in the lower half-space, however, only a transmitted wave is present; hence a general expression is, indicate the electrical conductivity and magnetic permeability of upper half-space; 0 is a spatial frequency constant, which is solely affected by the sensor. 0 is defined to be 1 over the smallest dimension of the coil [cit] ."
"if the generator units of the new individual vector generated by the algorithm are in prohibited operating zones, the output power of each generator unit is adjusted as follows:"
"where r(n) is the received signal, s p s s (n) is the receivergenerated pss in time-domain, n is the frame length, (·) * denotes the complex conjugate, (·) n denotes the circular shift operator, and n represents the circular convolution operation. taking the fft and ifft of (3) yields"
the following derivations are associated with the solution of electrical field term using transverse electric (te) wave propagation algorithms through medium with different material layers.
"compared with the results of the analytical solution, the fem is verified to be accurate enough for the calculation of sensorstructure mutual inductance from the analysis of the equivalent-effect phenomenon performance on the flat plates as shown above. moreover, the analytical solution can only be used to calculate the mutual inductance for the flat plate. therefore, for the curved plate structures, the following mutual inductance is only computed by the fem."
"in this section, a toa estimation method is proposed. this method is a first-peak estimation algorithm in which the threshold adapts to the environmental noise."
"previously, massive works have been proposed on the electromagnetic eddy current evaluation techniques. and basic simulation methods can be summarized as method of auxiliary sources (mas), boundary-element method (bem), and finite-element method (fem) [cit] . the fundamental principle of mas is introducing a source within and near the surface of the structure that can scatter the same electromagnetic field as that around the structure. then each step of the electromagnetic field change can be equivalent to a change or addition of a new source. the merit of mas is simplifying the eddy currents computation procedure, as therefore, increasing the efficiency of the calculation. bem is essentially only modelling or meshing the boundary region of the structure, which can significantly reduce the elements and calculation amount. however, both mas and bem are not commonly used or even cannot evaluate the eddy currents of the structure with sophisticated geometry. for instance, the mas method is hard to compute the eddy currents of the structure with the rough surface; and bem cannot do the eddy current computation of structures with non-linear geometry. although fem needs to mesh/model the whole structure or even the space surrounded by the structure (considering the eddy current skin/diffusion effect), it is the most widely used and can solve eddy current evaluations for almost all types of structures including non-linear geometry and material properties."
this paper is organized as follows: section ii describes the traditional differential evolution algorithm. section iii proposes the mpde algorithm. section iv implements mpde algorithm to solve the ed problem. section v is dedicated to simulation results and analysis. section vi summarizes the conclusion of this work.
"the literature on sops answers theoretical questions on the observability and estimability of the sops landscape for various a priori knowledge scenarios [cit] and prescribes receiver motion strategies for accurate receiver and sop localization and timing estimation [cit] . moreover, a number of recent experimental results have demonstrated receiver localization and timing via different sops [cit] . cellular sops are particularly attractive for navigation purposes due to their abundance, geometric diversity, high transmitted power, and large bandwidth [cit] ."
"the first step in acquiring an lte signal is to extract the transmitted frame timing and the enodeb's cell id [cit] . these two parameters are obtained by the pss and the sss. to detect the pss, the ue exploits the orthogonality of the zadoff-chu sequences and correlates the received signal with all the possible choices of the pss according to"
"in this subsection, the proposed lte sdr and navigation framework are employed to navigate a uav exclusively with 1) uav experimental setup: when a uav flies high enough, it can be assumed that the received signal to the uav does not experience multipath from the surrounding environment, except from the uav's body. in this paper, a uav with body size less than 1 m was used; therefore, the effect of multipath from the uav's body is neglected. in this case, tracking the sss only yields good results; hence, the crs was not used to improve the navigation solution. this will significantly decrease the computational cost of the receiver. it also reduces the need for high sampling rate, which results in lower hardware cost as well. low sampling rates also allow for lightweight hardware, which is critical for uavs with limited payload. fig. 16 shows the experimental setup used in performing the experiment with a uav. in this experiment, a dji matrice 600 was equipped with:"
"where n c p is the set of cp indices and t s is the sampling interval [cit] . upon estimating the doppler frequency, the acquisition of the lte signal is complete. fig. 3 summarizes the lte signal acquisition process."
"for the fem applied in the electromagnetic area, considerable research works have been published on the eddy current testing theory under low frequency or even the static electromagnetic field. however, little has been discussed on the high-frequency eddy current computation especially for the metallic structure with high conductivity (hc), which will encounter some computation issues especially the eddy current skin/diffusion effects [cit] . as more intensive induced eddy currents are distributed in the structure surface underneath the sensor under the eddy current skin/diffusion effect, significantly refining the mesh around the surface region especially underneath the sensor area is necessary to maintain the simulation accuracy when using the fem method. however, models/meshes with intensive elements will result in a mass of computation burden."
"a differential evolution algorithm based on multi-population (mpde) is proposed to solve the ed problem with valve-point effects. the mpde algorithm applies a multi-population strategy to the traditional differential evolution algorithm. in order to enhance its search ability, the mutation strategy and algorithm parameter in each population are also different. during the evolution process, individuals in different populations can also learn from individuals in other populations, this method of information exchange enhances the diversity of a single population. moreover, the mpde algorithm adopts the normal distribution function to dynamically adjust the scaling factor and crossover rate. the mpde algorithm is tested on the 13-, 40-, 80-and 140-unit test systems. statistical results are compared with the reported results in literature, the mpde algorithm has better accuracy and robustness than other intelligent algorithms, and can provide satisfactory global optimization solutions. therefore, the mpde algorithm is a very suitable tool for solving the ed problem with the valve point effects."
the received signal model in the i -th symbol was presented in (2). the subscript i will be dropped in the sequel for simplicity of notation. the estimated cfr of the u-th enodeb is given bŷ
"otherwise (29) 3) equality constraints handling this paper takes a new method to deal with equality constraints. the difference is distributed to each generation unit by calculating the difference between the current total output and the demand output. in this way, not only the generator unit can be corrected to satisfy the equality constraints, but also the output power of the generator unit can be changed less, and the influence due to the variation of the output power can be reduced. the specific steps are analyzed below:"
"in this section, the performance of the proposed sdr is evaluated. first, the output of each block of the receiver processing real lte signals is provided. then, experimental results for a uav and a ground vehicle navigating exclusively with real lte signals are presented. in each case, the details of the exploited hardware and software are provided. finally, key concluding remarks are discussed."
"when a ue receives an lte signal, it must first convert the signal into the frame structure to be able to extract the transmitted information. this is achieved by first identifying the frame start time. then, knowing the frame timing, the receiver can remove the cps and take a fast fourier transform (fft) of each n c symbols. the duration of a normal cp is 5.21 μs for the first symbol of each slot and 4.69 μs for the rest of the symbols [cit] ."
"of the ekf, which was also implemented in matlab. the lte navigation solution was obtained by the ekf using the pseudoranges produced by the lte sdr, and the lte and gps navigation solutions were compared to calculate the estimation error."
"the ue can identify the res associated with the physical downlink control channel (pdcch) and demodulate them by knowing the cfi. this results in a block of bits corresponding to the downlink control information (dci) message. the dci can be transmitted in several formats, which is not communicated with the ue. therefore, the ue must perform a blind search over different formats to unpack the dci. the right format is identified by a crc."
"this paper studied the exploitation of lte signals for navigation purposes. a discussion of relevant signal models was presented and an sdr design for navigating with lte signals was discussed. a method for timing information extraction was proposed. in addition, a method for tracking multiple enodebs by only tracking one reference enodeb was proposed. experimental results were presented demonstrating a uav and a ground vehicle navigating exclusively with lte signals via the proposed sdr. the rmse between gps and lte navigation solutions was calculated to be 8.15 m (with 3 enodebs) and 5.80 m (with 6 enodebs) for the uav and the ground vehicle, respectively."
"in this section, the architecture of an lte frame is first discussed. then, the structure of three main lte signals which can be used for navigation, namely the pss, sss, and crs is explained."
"the sib contains information on (1) the enodeb to which it is connected, (2) inter-and intra-frequency neighboring cells from the same operator, (3) neighboring cells from other networks [cit], and (4) other information. the sib has 17 different forms called sib1 to sib17, which are transmitted in different schedules. sib1, which is transmitted in subframe 5 of every even frame, carries scheduling information of the other sibs. this information can be used to extract the schedule of sib4, which has the intrafrequency neighboring cell ids. to decode sib1, the ue has to go through several steps. in each step, the ue needs to decode a physical channel to extract a parameter required to perform other steps."
"in the third stage, the received signal is tracked using the architecture discussed in subsection iii-c. the pll, fll, and dll noise-equivalent bandwidths were set to 4, 0.2, and 0.001 hz, respectively. to calculate the interference-plus-noise variance, the received signal was correlated with an orthogonal sequence that is not transmitted by any of the enodebs in the environment. then, the average of the squared-magnitude of the correlation was assumed to be the interference-plus-noise variance. fig. 15 shows the tracking results. since the receiver was stationary and its clock was driven by a gps-disciplined oscillator (gpsdo), the doppler frequency was stable around zero."
"if the individual m belongs to the population pop3, the formula is expressed as follows: mechanism enhances the search ability of the single population. figure 2 shows the schematic diagram of the exchange among populations. this information exchange mechanism has two characteristics. on the one hand, it makes the single population have certain independence, and exchanging information too frequently among populations will cause a population to have too much influence on the other two populations, resulting in rapid convergence of three populations to local optimum. on the other hand, the greater the number of consecutive failures of individual continuous update, which indicating that the individual may fall into local optimum or that the paternal individuals in the population to which the individual belongs cannot meet the requirement of the individual to perform the mutation operation, and from other populations, it is possible to find paternal individuals suitable for the individual to perform the mutation operation. the information exchange mechanism enhances the diversity of individuals in the single population and facilitates the individual to escape from the local optimum."
"the transmitter-receiver mutual inductance changes caused by the tested piece can be obtained by applying the equation presented by dodd and deeds [cit] . for this article, a generalized equation of calculating induced voltage that could be applied to any coil sensor is present: 2"
"although the equivalent-effect phenomenon is verified to be accurate especially under the high operating excitation frequencies, the performance of this phenomenon on thinner specimens is worth to be analysed further."
"the new individual vector generated by the algorithm after the crossover operation may not satisfy the inequality and ramp-rate constraints. when this happens, the modified output power of each generator unit is calculated as:"
"by comparing figure 16 (a) and figure 16 (b), the eddy current distributions for the original mesh modelling shows a more intensive and broader padding area than that for the equivalent structure. hence, in order to get the accurate value of the sensor-sample mutual inductance, a more fine mesh is needed."
"the purpose of the ed is to optimize the power output of generator unit while satisfying the constraints of the power system, thereby minimizing the total power generation cost. the cost function when considering the valve-point effects can be formulated as follows:"
"the output power of each generator unit should meet the inequality constraint (22), it shall not be greater than the maximum of power output of the generator unit and not less than the minimum of power output of the generator unit. (22) where p max j is the maximum power of the generator unit j."
"since both the presented fem and the analytical solution are verified to be accurate by comparing with the measured results, the following further validations only focus on the fem and analytical solutions."
"case 6 was run independently for 50 times with the mpde algorithm. the lowest generating cost is 1559708.807042$/h. figure 9 shows the convergence characteristic of the mpde algorithm when solving the case 6. 1559708.807042$/h is the minimum value that can be found so far. it can be seen that the mpde algorithm has a good performance in solving the case 6. table 17 shows the output of each generator unit at the lowest total generation cost. table 18 shows the results of the mpde algorithm and other intelligent algorithms to test cases 6, including the minimum cost, mean cost, maximum cost, standard devia-volume 7, 2019 tion value, time and nfe. as can be seen from table 18, mpde can get the lowest cost. the maximum, minimum, and mean of the mpde algorithm are the best. although owgo, ccpso, kgmo, and gwo have an advantage in terms of time and ogwo is better than mpde in the standard deviation, they are not as good as mpde in terms of accuracy, mpde has a strong competition in accuracy. the proposed algorithm has great advantages in solving case 6."
"the stored lte signals were processed by the proposed lte sdr, which was implemented in matlab. the stored gps signals were processed by the generalized radionavigation interfusion device (grid) sdr whose accuracy is consistent with the standard positioning service gps signal [cit] . the gps navigation solution was used to initialize the states fig. 16 . uav experimental hardware and software setup. the lte and gps antennas were connected to an ettus e312 usrp driven by a gpsdo. the stored lte and gps signals were processed with the proposed sdr and grid sdr, respectively. the lte navigation solution was obtained from an ekf and compared to the gps navigation solution."
"evaluating the proposed receiver in different environments over longer trajectories will be addressed in the future, upon having access to a database of the enodebs' positions."
"step 4: information exchange among populations. q t is adjusted by formula (12) . judge the population to which the individual m belongs and according to formulas (13)- (15) the parental individuals and mutation strategy are selected to perform the mutation operation, so as to realize information exchange among populations. the f t m is obtained by using (16) and (17) ."
the total of all power of the generator unit is equal to the sum of load demand and the transmission loss. the equality constraints can be formulated as follows:
"firstly, an experiment has been carried out. the motivation of the experiment is to check the performance of the found phenomenon when using a real sensor. the experimental data has its noise more or less. we want to check whether the error caused by the noise is negligible for equation (27) . once equation (27) is validated by the experiment, further parts of section are all solely calculated by the analytical solution (dodd deeds) and fem."
"the initial population needs to cover the entire search space as much as possible, so the algorithm uses a uniformly distributed random function to generate the initial solutions. x 0 m.n is calculated as follows:"
"b 0j p j + b 00 (27) where p d is the total load demand; p loss is the transmission losses; b ji, b 0j, b 00 are the losses coefficients; p i is the power output of unit i."
"parameters relevant for navigation purposes include the system bandwidth, number of transmitting antennas, and neighboring cell ids. these parameters are provided to the ue in two blocks, namely the master information block (mib) and the system information block (sib). in this section, the decoding of each block is discussed."
"the second demonstration considers a ground vehicle in an urban environment in which the received lte signal suffered from severe multipath. to alleviate the effect of multipath, the proposed method for detecting the first peak of the cir is employed. the navigation solution from 6 lte enodebs is compared to the gps solution. the rmse between the trajectories is shown to be 5.80 m with a standard deviation of 3.02 m and a maximum difference of 14.96 m. the proposed method is also compared to other methods from the literature."
"in an urban environment, the pseudoranges received by a ground vehicle will suffer from more multipath-induced error compared to pseudoranges received by a uav with los conditions. however, this comparison can be made as long as the ground vehicle and uav are navigating in the same environment, using the same enodebs, and following the same trajectories, except for one being on the ground while the other being airborne. in this paper, the ground vehicle was equipped with a better usrp than the one on the uav, due to payload limitations. the usrp on-board the ground vehicle was capable of sampling two different lte channels at a sampling rate of 20 msps, whereas the usrp on-board the uav could only sample one lte channel at 3 msps. consequently, the lte receiver on-board the ground vehicle was able to listen to more enodebs than the receiver onboard the uav, providing the former with more measurements at a better geometric diversity than the latter. moreover, the ground vehicle-mounted receiver was able to produce more accurate toa measurements, since it was sampling at more than six times the rate of the uav-mounted receiver. these aforementioned factors resulted in the position rmse of the ground vehicle being less than the position rmse of the uav."
"intuitively, in order to get the same inductance value, we might think the relation between the thickness and electrical conductivity of two different samples should be the same as that between the skin depth and electrical field. however, the sensor detected signal is not solely determined by the eddy current (density) within the samples, which is controlled by the electrical conductivity; the detected signal also depends on the attenuation distance of the electromagnetic waves during the propagation, which is related to the thickness of the sample. therefore, it is necessary to do a step-by-step analyzation of the decay during the propagation (including transmissions and reflections) of the wave field. currently, there are three wave modes to investigate the propagation of the electromagnetic wave -transverse electric and magnetic (tem) mode, transverse electric (te) mode, and transverse magnetic (tm) mode. in this paper we have utilized te wave mode to analyze the induced voltages between the two coupling coils of the sensor."
"which is used to compare the desired cell's value to the noise floor. to improve the probability of detection while maintaining a constant p f a, a non-coherent integration can be used. for this purpose, it is proposed to integrate squared envelopes ofĥ (u) (n) at different slots and for different transmitting antennas (assuming that they have the same los path) in one frame duration. defining n i as the number of non-coherent integrations, averaging is performed over n i n t training cells. therefore, after integration, the threshold will have a noncentral chi-square distribution with 2 n i n t degrees of freedom. by taking the average of the probability of false alarm given the threshold presented in (9) over the new pdf of this threshold, it can be shown that [cit]"
"i d . the cell id is used for data association purposes. the crs is an orthogonal sequence, which is mainly transmitted to estimate the channel frequency response (cfr). the transmitted ofdm signal from the u-th enodeb at the k-th subcarrier and on the i -th symbol can be expressed as"
"this strategy promotes information exchange among populations to increases the diversity of individuals in the single population, and avoid the single population falling into local optimum solution."
the remainder of this paper is organized as follows. section ii provides an overview of lte signals. section iii presents the lte sdr architecture. section iv discusses the proposed method for detecting the first peak of the cir. section v proposes a method for tracking multiple enodebs by tracking only one enodeb. section vi presents the framework to obtain the navigation solution. section vii shows the experimental results. concluding remarks are given in section viii.
"in practice, the ue may not have access to estimates of its clock bias due to unavailability of gps signals. other approaches synchronize the receiver and transmitter through cables in the lab [cit] . this paper extends [cit] to address these issues and makes the following contributions:"
"step 4: check all the modified p j, if there is any violation of the inequality constrains, perform formula (28) and (29), back to step 1."
"where x 0 m.n is the value in the n-th dimension of the individual m; rand(0, 1) is a uniformly distributed random number in the interval (0, 1); x max n is the maximum boundary value of the n-th dimension of the individual and x min n is the minimum boundary value of the n-th dimension of the individual."
step 2: initialization. the power output of each individual vector is derived from formula (32) . modify the individual vector that violates equality constraints and inequality constraints. evaluate the fitness value of the individual according to formula (20) and (21) .
"even if the equivalent-effect phenomenon is valid for the flat plates geometry, its performance on the structures with other geometries such as curved plates is worth investigating, as shown in the following."
"for the edge-element fem solver, a software package based on the presented fem solver has been built, which uses the bi-conjugate gradients stabilised (cgs) iterative method to solve the matrix. compared with the canonical em simulation solver, the novelty of this fem software package is that it has assigned the solution for the previous frequency to be the initial guess of the next frequency. i.e. the presented fem solver is more efficient than the conventional em simulation solver on the multi-frequency spectra calculations [cit] . this"
"where h (u) i (k) is the cfr, u is the total number of enodebs in the environment, and w i (k) is a white gaussian random variable representing the overall noise in the received signal."
"although it is not possible to obtain a closed-form expression for the probability of detection, numerical solutions for p d have been tabulated and can also be computed with software packages [cit] . fig. 11 demonstrates the receiver operating"
"in the next step, the convolutional coded bits are ratematched. in the rate matching step, the obtained data from channel coding is first interleaved. [cit] -bit long array [cit] . next, the output of the rate matching step is scrambled with a pseudo-random sequence, which is initialized with the cell id, yielding unique signal detection for all enodebs. subsequently, quadrature phase shift keying (qpsk) is performed on the obtained data, resulting in 960 symbols which are mapped onto different layers to provide transmission diversity. to overcome channel fading and thermal noise, space-time coding is utilized. this process is performed in the precoding step. finally, the resulting symbols are mapped onto the predetermined subcarriers for mib transmission [cit] ."
"further, for a three-layer medium, a series of the reflection and transmission coefficients during the te wave propagation are shown in fig.2 . normally, multiple reflections occurred in region 2. since the te wave would decay significantly after several reflections, here only the te waves prior to the third reflection within region 2 are analysed."
"case 5 was run independently for 50 times with the mpde algorithm. the lowest generating cost is 242794.729463$/h. figure 8 shows the convergence characteristic of the mpde algorithm when solving case 5. table 15 shows the output of each generator unit at the lowest total generation cost. table 16 shows the results of the mpde algorithm and other intelligent algorithms to test cases 5, including the minimum cost, mean cost, maximum cost, standard deviation value, time, and nfe. as can be seen from table 16, the minimum cost, mean cost and the maximum cost of mpde are superior to the results calculated by other intelligent algorithms. the stability and accuracy of mpde are stronger than all intelligent algorithms in the table. some intelligent algorithms take less time than mpde, but mpde is more accurate than them in terms of minimum cost, mean cost and maximum cost. the statistical results obtained by mpde are highly competitive compared to these by the other intelligent algorithms."
"a rayleigh distribution with a probability density function (pdf) given by a neyman-pearson test is formulated to obtain the decision threshold, denoted η, where the probability of false alarm p f a is set to a desired constant and is given by"
where t is the current number of evolutions; p t is the population at the t − th generation; x t m is the m-th individual vector; d is the number of dimensions of the individual vector.
"over the course of the experiment, the uav was flying at the height of 40 m. the receiver was listening to 3 enodebs, each of which had 2 transmitting antennas with 20 mhz transmission bandwidth. the cell ids of the enodebs were 300, 398 and 364, respectively. the positions of the enodebs were mapped prior to the experiment with approximately 2 m accuracy."
"to estimate the position of the receiver in a two-dimensional (2-d) plane using a static estimator, the pseudoranges to at least three enodebs are required and can be obtained by tracking the signal of each enodeb. however, tracking all signals is computationally involved and could prohibit real-time implementation. besides, the received signal from an enodeb may be highly attenuated; therefore, it may not be possible to track all ambient ssss. in this section, a new method is proposed that exploits the frequency reuse factor of six in the lte crs signals to extract the pseudorange of multiple enodebs while tracking only one enodeb. in this approach, the receiver may obtain a list of the neighboring enodebs by decoding the sib of the main enodeb. once the neighboring enodebs cell ids are known, the receiver may generate the crs sequence transmitted by each neighboring enodeb. with some assumption on the relative delay (including distance and clock bias) between enodebs, which will be discussed in this section, the receiver may be able to estimate the cir of the neighboring enodebs in reference to the main enodeb. then, relative delay is calculated from the cir for each new frame, which alleviates the need to track the sss of the neighboring cell ids."
"implementing the proposed receiver in hardware (e.g. digital signal processors (dsps) and field-programmable gate arrays (fpgas)) is one of the remaining challenges that needs to be addressed in the future. in this realm, the delay introduced by each part of the system, i.e., hardware and software, must be evaluated to analyze real-time feasibility."
"in this paper, the value q t that allows the individual to continuously fail is set. information exchange among populations can neither be too frequent or too less. it is important to choose the suitable q t for the population to evolve. in the early stage of evolution, due to the better individual diversity of the single population, the number of consecutive failures of individuals is small. in the later stage of evolution, the individual differences of the single population become smaller, and the number of consecutive failures of individuals will increase. this paper adopts a nonlinear incremental method to dynamically adjust q t . q t is computed as follows:"
"considering the magnetic flux may penetrate thinner metallic plates, the fitting performance between the multi-frequency inductance curves for a thinner original structure and the corresponding equivalent structure, may differ from that of the thicker metallic plates."
"if the value of the scaling factor is larger, the global search ability of the algorithm is stronger, if the value of the scaling factor is smaller, the convergence speed of the algorithm is faster. in the early stages of evolution, a larger scaling factors enhance global search ability of the algorithm, in the later stages of evolution, a smaller scaling factor speeds up the convergence of the algorithm. this paper uses a nonlinear decrement method to dynamically adjust the scaling factor. the scaling factor is given by:"
"case 4 was run independently for 50 times with the mpde algorithm. figure 7 shows the convergence characteristics of the mpde algorithm when solving case 4. table 13 shows the output of each generator unit at the lowest total generation cost. table 14 shows the results of the mpde algorithm and other intelligent algorithms to test case 4, including the minimum cost, mean cost, maximum cost, standard deviation value, time, and nfe. as can be seen from table 14, for dataset1 (40−unit) ), the minimum cost, mean cost and the maximum cost of mpde are superior to the results of other intelligent algorithms. the stability and accuracy of mpde are stronger than all intelligent algorithms in the table. some intelligent algorithms take less time than mpde, but mpde is more accurate than them in terms of minimum cost, mean cost and maximum cost. for dataset2 (40−unit) ), mpde is superior to agwo in all respects. the statistical results obtained by mpde are highly competitive compared to these by the other intelligent algorithms."
"to the authors' knowledge, the second issue has not been addressed in the literature. to overcome the third issue, some approaches assume that the receiver has access to estimates of its own clock bias (from gps signals), enabling the receiver to estimate the difference between its clock bias and the clock bias of the enodeb in a post-processing fashion [cit] ."
"where f is the total power generation cost of the power system; f j (p j ) is the power consumption characteristic of the power generation unit j; m is the total number of the power generation units; p j is the output power of the power generation unit j; a j, b j, c j, e j, f j are the cost coefficients of the generator unit j; p min j is the minimum power of the generator unit j."
"in order to exploit the high-bandwidth crs signal, which improves the navigation performance in multipath environments or in the presence of interference, the ue must first reconstruct the lte frame from the received signal. to do so, the actual transmission bandwidth and number of transmitting antennas, which are provided in the mib, must be decoded. the mib is transmitted on the physical broadcast channel (pbch) and consists of 24 bits of data: 3 bits for downlink bandwidth, 3 bits for frame number, and 18 bits for other information and spare bits. the mib is coded and transmitted on 4 consecutive symbols of a frame's second slot. however, it is not transmitted in res reserved for the reference signals. fig. 4 shows the steps the mib message goes through before transmission [cit] ."
"two types of positioning techniques can be defined for lte, namely network-based and user equipment (ue)-based positioning. the network-based positioning capabilities were enabled in lte release 9 by introducing a broadcast positioning reference signal (prs). in positioning with the prs, the dedicated resources to the prs are free from the interference and the expected positioning accuracy is on the order of 50 m [cit] . however, prs-based positioning suffers from a number of drawbacks: (1) the user's privacy is compromised since the user's location is revealed to the network [cit], (2) localization services are limited only to paying subscribers and from a particular cellular provider, (3) ambient lte signals transmitted by other cellular providers are not exploited, and (4) additional bandwidth is required to accommodate the prs, which caused the majority of cellular providers to choose not to transmit the prs in favor of dedicating more bandwidth for traffic channels. to circumvent these drawbacks, ue-based positioning approaches that exploit the cell-specific reference signal (crs) have been explored, where several advanced signal processing techniques exploited to achieve a performance similar to the prs [cit] ."
"the sss is an orthogonal length-62 sequence, which is transmitted in either slot 0 or 10 in the symbol preceding the pss and on the same subcarriers as the pss. the sss is obtained by concatenating two maximal-length sequences scrambled by a third orthogonal sequence generated based on n (1)"
"sections iii-v discussed how toa estimates can be extracted from lte signals. by multiplying the obtained toa for the u-th enodeb,t (u) s, by the speed-of-light, c, pseudorange measurements are formed as"
"the section iii proposes the mpde algorithm. this section introduces the application of mpde algorithm in solving ed problems. its main contents include the objective function, constraints and constraints handling of ed problem. in addition, the specific steps of volume 7, 2019 applying the mpde algorithm to the ed problem are introduced."
"the core of the mpde algorithm is the multi-population co-evolution. in this paper, a multi-population strategy is designed for the de algorithm, and each population uses different mutation strategies. in order to increase the diversity of the single population, multiple populations can learn from each other by exchanging information."
"3) the normal distribution function is applied to dynamically adjust the scaling factor and crossover rate to accelerate convergence speed. 4) in the test of 13-, 40-, 80-and 140-unit test systems, the mpde algorithm can converge to the optimal value and has a smaller standard deviation."
"from real lte signals in a multipath environment at a given time instant (blue). the proposed method in section iv was used to obtain the threshold, η (red). then, d"
"for the symbols carrying the crs, y follows the definition in (1) . therefore, the cfr of the main enodeb can be obtained fromĥ (1)"
shown in black dashed lines. it can be seen that the proposed method was able to isolate these peaks from the noise floor. fig. 23(a) shows the environment layout as well as the true and estimated receiver trajectory. it can be seen in fig. 23(b) that the navigation solution obtained exclusively by lte signals using the proposed lte receiver and navigation framework follows closely the gps solution. the navigation performance of the ground vehicle is summarized in table ii . fig. 24 shows the distance estimation error and the experimental cdf of the error indicating a 95-th error percentile of 10.41 m.
"more integrative approaches utilize meta-predictors, which combine multiple predictors into a single output score. meta-ppisp [cit] combines cons-ppisp [cit], promate [cit], and pinup [cit] via a linear regression model taking local environment into account."
"the percentage of interacting surface residues is not constant with respect to protein size; rather, it has long been known that it follows a non-linear distribution (e.g. exponential regression line) [cit] . however, this information has only seldom been applied to ppis prediction [cit], or even treated as a linearly changing proportion [cit], despite results showing that this \"size bias\" carries significant predictive power on its own [cit] . one potential application to ml-based predictors is dynamically setting the prediction threshold such that the proportion of predicted active residues matches the estimated prior distribution for the query protein. such intelligent biasing of the learner might even be extended by looking for patterns in the interface-surface residue ratio in different types of proteins as well."
"objective evaluation of ppis predictor performance is made difficult by the varying definitions of interaction sites and accessible surface residues in the literature, the lack of available servers for all predictors, the adoption of varying training and testing datasets, and the different metrics used for evaluation [cit] . we partially circumvent these problems by considering the performance of each predictor across a variety of test sets based on literature values."
"while it is difficult to draw conclusions from the differing performance of the predictors, we can nevertheless observe some trends that may be explained by the biological theory discussed previously. for example, while transient datasets (such as transcomp_1) generally garner lower scores than permanent ones (such as planedimers), this is not perfectly followed (table 4), possibly due to the difficulty in defining a threshold on the transientpermanent continuum. some sets (e.g. s149) may be intrinsically more predictable, as evidenced by higher scores across all predictors; others achieve better results only on certain types of predictors (e.g. db3-188 on structural homology-based predictors). to achieve high scores on specialized testing datasets, predictors often require either specializations of their own, or inherent characteristics that permit accurate classification (e.g. anchor's [cit] specialization for disordered proteins and homppi's [cit] lack of requirement for structural information allow them both to successfully predict on the s1/2 disordered sets). theoretically, unbound structures are more difficult to predict on than bound monomers (due to the conformational disparity between the two sets); this is largely confirmed by differing results on the ds56b/u sets, as well as generally lower scores on unbound sets (table 4) . overall, we find that there has been significant progress in the predictive abilities of the predictors over the last decade across diverse interaction types and datasets."
"another interesting application is in assisting computational protein-protein docking. docking without prior knowledge of the interaction sites of the proteins in question (i.e. ab initio docking) has been shown to be more difficult due to the staggering search space dimensionality involved [cit] . information-driven docking [cit] can mitigate this problem by utilizing mass spectroscopy and interaction data, the latter of which is often difficult to obtain, but can be provided by ppis prediction. this approach has already proven successful in both highresolution docking [cit] and coarse mass docking experiments [cit] . additionally, ppis-driven coarse docking studies could assist in large-scale ppi network creation [cit], as well as alignment of such networks for functional ortholog identification [cit] ."
"the classification of interfaces between transient, permanent, and obligate, as well as within interfaces as core and rim structures, has been extensively studied from a theoretical standpoint. however, with some exceptions [cit], this information has been algorithmically underutilized for ppis prediction. focusing on datasets of a particular complex type, combining interface classification with interaction likelihood prediction, and integrating learning of the different properties of the interface core and rim into ppis prediction are just a few examples of promising areas that are currently under investigation."
"the main issue with the use of structural homologs, or with predictors requiring structural information in general, is the paucity of usable structures [cit], particularly when considering the relatively small size of the pdb (∼80,000 structures, including redundancy) compared to the number of sequences known (∼17 million non-redundant sequences) [cit], though this can be partly circumvented by using local (rather than global) structural homologies [cit] . however, studies on the properties of the interfaces themselves have found that their structural space is degenerate [cit], that templates for the majority of known interactions exist [cit], and that interfaces are conserved across structure space [cit] . this bodes well for structure-based ppis prediction, as it suggests that numerically limited interface examples can cover most potential queries, despite incomplete structural [cit] and interaction type (or quaternary fold) [cit] coverage. the potential contribution of homology models [190, [cit] and growing structural coverage [cit] further justify using structure."
"in other interactions [cit] . in response to these shortcomings, computational methods for the prediction of ppiss have been developed, starting with jones and thornton's pioneering analysis of surface patches [cit], and many predictors have since been published, utilizing a wide variety of algorithmic approaches to the problem. this review will first provide a systematic analysis of the features and datasets used in ppis prediction, from both a theoretical and application-oriented standpoint. an examination of the algorithms used in a selected set of the most recent ppis predictors is also given, showcasing the diversity of the latest methods employed in this endeavour, as well as the potential for combining or extending them. this is complemented by a comparative evaluation of the performance of these predictors. because it most accurately simulates the missing information inherent in real-world applications, we focus on the general case of ppis prediction: using only a single unbound protein structure, without knowledge of an interacting partner, to predict the binding site of that protein at the amino acid scale. finally, the applications of ppis prediction and promising areas for future improvements are also discussed."
"the prise (predictor of interface residues using structural elements) algorithm addresses several limitations of using whole-protein structural similarity, including the coarseness of global homology measures and the requirement for sufficient numbers of structural neighbours, via local structural conservation information [cit] ."
", can then be computed for every surface residue, combined and recast into z-scores, and finally thresholded to determine whether a given residue is interacting or not."
"where n is the number of neighbouring residues (within a sphere of 15å, based on previous work [cit] ), v i is the feature value of the ith neighbour, d i is its distance from the target residue, and rsa i is its relative solvent accessibility. finally, feature selection was performed by (1) removal of attributes with high linear correlation and (2) pca, with sufficient principal components to permit 95% of the variance to be explained (see feature selection), resulting in a feature space with low dimensionality and redundancy."
"similarly, secondary structural characteristics have been used in several predictors [cit], but have also elicited dispute regarding their utility and biological interpretation. specifically, some studies [cit] have found that β-sheets are favoured in interface sites while α-helices are more prevalent over the rest of the protein surface, though others disagree [cit] ."
"most importantly, knowledge of ppiss and hss can be used for rational design of therapeutics and biomolecules by serving as a template for the de novo creation of small molecules with enhanced efficacy and selectivity. mimetics of the interaction sites of well-known molecules have been successfully built [cit], the process of which could be significantly expedited by the knowledge of putative interaction sites, as it would allow rational construction of a mimetic compound without requiring mass screening. the design of novel interface sites has also shown promise for the construction of new functional biomolecules [cit] ."
"as with much biological data, protein structural datasets are non-standardized and virtually all structure-based predictors have varying criteria for processing and filtering this data. the types of proteins that are the least predictable, or the difference between large but heterogeneous training sets versus smaller but cleaner ones, have not been comprehensively examined. specialized training sets per query protein, whether by structural, sequence-level, or functional data, are worth exploring as well. potentially, unsupervised learning could be applied to extract hidden patterns within the data, possibly contributing to further analysis of the relation between interaction types, protein categories, and the feature space. further, methods accounting for systemic biases in the pdb (e.g. under-representation of membrane/disordered proteins) may also improve robustness [cit] ."
"similarly, the receiver operator curve (roc), which is a plot of sensitivity versus 1 − specificity derived by varying the classifier prediction threshold, can be used to compute the area under the roc curve (auroc/auc) [cit], which is especially useful for identifying artificially \"inflated\" performance (e.g. higher sensitivity at the expense of specificity) and for being decision threshold independent [cit] ."
"linear discriminant analysis (lda) was then used to derive a hyperplane capable of separating input vectors based on the class labels of the training data [cit] . importantly, the authors used amino-acid specific classifiers (i.e. a different lda classifier was built and then applied for each of the canonical amino acid types), leading to higher predictive ability."
"in addition, the method discriminates and removes outliers, as residues with high probability of ppis participation are generally in contiguous patches and not surrounded by low probability residues. this is accomplished with the use of a 2-step rf ensemble classifier, each instantiation of which consists of several hundred discrete decision trees, with the first rf utilizing structure-, energy-, and evolutionary-based features for each surface residue, as well as environmental information, and the second rf making use of the scores from the previous step, along with further environmental score-derived metrics."
"while there exist previous reviews on ppis prediction [cit], these did not have access to the most recent algorithmic advances. as such, we provide a systematic overview of these techniques, in the hope that future predictors can employ these methodologies as a foundation for the creation of more sophisticated predictors."
"the packing preferences and geometries of atomic protein structures have long been studied as a characterizing feature of association [cit], via probability density maps (pdms) describing likelihoods of contacts [cit] . advantageously, pdms can be derived from the intramolecular contacts in the protein interior and are hence less limited by the structural information available [cit] . while contacts in the protein interior differ from those in interfaces (e.g. artefactual interactions from structural constraints [cit] or greater contribution of electrostatics to folding than binding [cit] ), these differences appear to be relatively minor [196, [cit], particularly when the interface core is mainly considered [cit] . recently, 3d pdms were used as input features for ppis prediction [cit] . by projecting onto a previously described coordinate system [cit], interacting contacts can then be added to the density map, allowing preservation of both magnitude and direction. based on a similar method applied to protein folding [cit], \"co-incidental\" interactions (due to proximal atoms forced together by structure constraints) [cit] were filtered out."
"the class imbalance problem, caused by the numerical disparity between ppis and nppis training examples, was addressed by resampling the high number of noninteracting examples to match the number of interacting ones in a 1:1 ratio, based on empirically testing different ratios of positive-to-negative results over a variety of machine learners. the resulting set of feature vectors can then be used to optimize a given machine learner by grs, which will find the feature subset optimally discriminative of ppis participation for a given learner and dataset."
"all interfaces have distinctive \"core\" and \"rim\" regions, with core regions exhibiting lower sequence entropy (higher conservation) than rim [cit], as well as reduced tolerance for water and decreased polarity [cit] . the core of interfaces may be more readily predictable than the rim [cit], likely for the same reason (i.e. stronger characterizing signal) that permanent ppiss are easier to predict than tis."
"the final ensemble of atom-specific, bagging anns could then predict the interface atoms of a given query protein surface. to turn these into residue-level predictions, high-confidence atomic predictions were treated as \"seeds\", and any surrounding atoms of even moderate confidence were assigned to be part of an atomic interacting patch. any residues with a high proportion of its atoms being part of such a patch were considered interacting."
"the majority of predictors based on machine learning (ml) rely on sets of structural information to train their learners, mainly curated from the pdb [cit] . however, in the process of mining this database, it is necessary to filter out molecules that are not of sufficient quality or utility for use in the training set [cit] . an overview of these filters is presented in table 1 ."
"though the field of ppis prediction has been steadily improving in accuracy and sophistication over time, challenges remain before scores sufficient to permit its many potential applications can be achieved."
"where s j is the score of the neighbour residue, l is one of the 20 residue types, and residue a j has type l. once calculated, these scores are added to those from the firststep rf for each residue as input to the second-step rf, generating a revised prediction with reduced outliers, better environmental accounting, and improved prediction performance."
"to apply ifs to the mrmr ranking, [cit] then built subsets of features by iteratively adding attributes in the order of the mrmr ranking and chose the feature subset with the maximal mcc score via ten-fold cross-validation."
"the prescont algorithm [cit] combines local residue features with environmental information as input to an svm. the creators of prescont note their belief that interface prediction will not benefit from the use of a large number of noisy features, and thus make use of just a few important attributes, namely sasa, hydrophobicity, propensity and conservation. additionally, the weighted average of each of these features over the neighbouring residues using a euclidean distance cutoff is used. the features are scaled to the range [cit] and used as input to an svm with the radial basis kernel function, which constructs a hyperplane capable of optimally separating ppis and nppis feature vectors."
"given the disparate approaches and information sources applied to ppis prediction, it is natural that combining multiple methods should increase scores. for example, using amino acid-specific [cit] and atom-specific [cit] classifier ensembles permits the reduction of noise and better separation between residues/atoms that likely have different properties differentiating them in interface sites. similarly, the combination of local and global structural homology-based predictors in prise [cit] and the two-step rf of vorffip [cit] both increase scores over the independent counterparts. some predictors also find increased scores upon combination with previous methods, as in whiscymate [cit] and combined bindml [cit] ."
"physicochemical properties have long been used for interface prediction [cit] . the observed trend of increased hydrophobicity in interfaces compared to non-interacting surface patches [5, [cit], as well as the proposed pattern of a highly hydrophobic central region surrounded by polar residues [cit], has been used with success in several recent predictors [cit] . additionally, electrostatic potential [cit] and energy of desolvation [cit] have shown utility as discriminative properties of ppiss [cit] . b-factors (debye-waller temperature factors) [cit] and disorder measures [cit] in prediction software are also useful, with interface sites shown to be more disordered [cit] yet also appearing to have lower b-factors than other surface patches [cit] . further, the decreased flexibility implied by decreasing b-factors is confirmed by the observation that interface residues minimize the entropic cost of complex formation [cit] by avoiding the sampling of alternative side-chain rotamers [cit] ."
"while the above methods are largely dependent on sequence-based measures of evolutionary information, structural conservation has also been shown to be useful for distinguishing protein binding sites [cit] li [cit], jet [cit], bindml [cit], rad-t [cit], prescont [cit], vorffip [cit] zhou & shan [cit], rad-t [cit] depth index dpx [cit] in psaia [cit] sikic [cit], li [cit], vorffip [cit] sikic [cit] protrusion cx [cit] in psaia [cit] sikic [cit], li [cit], vorffip [cit], rad-t [cit], chen [cit] jones & thornton [cit] hydrophobicity psaia [cit], quite [cit], fauchère & pliska [cit] sikic [cit] residue contact frequencies predus [cit] predus [cit] predus [cit] atomic probability density map features yu [cit], x-site [cit] chen [cit] chen [cit] energy of solvation fernandez-recio method [cit], fiorucci method [cit] using apbs [cit] fiorucci [cit], rad-t [cit] fiorucci [cit], rad-t [cit] and even general functional sites [cit] . importantly, structure evolves more slowly than sequence and may have more powerful signals for conservation [cit] . predictors using structural homology have verified this proposition [cit] ."
"each feature set (\"individual\") is represented as a bitstring, and the fitness of the individual is defined as its matthews correlation coefficient (mcc) [cit] after leave-one-out-cross-validation (lcv). a population of these individuals is then iteratively altered by three operations: mutation (with preference for less fit individuals), selection, and crossover (preferentially choosing higher scoring individuals) [cit] ."
"greater integration of ppis prediction with other areas of computational biology also holds promise. the relatively small number of crystallized structures suggests ppis predictions based on structural characteristics may not be helpful when such information is not present; however, the use of molecular modeling could prove useful in mitigating this problem. other areas of bioinformatics are also being applied to assist ppis prediction, such as molecular docking [cit] ."
"characterizing features have been used to predict ppiss since the founding of the field [cit], and have since been combined with ml algorithms of increasing sophistication. while no single feature appears to possess sufficient information to allow prediction on its own, certain attributes have been consistently favoured, such as conservation and hydrophobicity. recently, the use of databases of precalculated features, instead of on-the-fly calculations, has gained popularity, via databases such as aaindex [cit] and sting [cit] . the most popular features, as well as references detailing their utility and procurement, are presented in table 3 ."
"first, the mutual information i(x, y ) between two features x and y, which serves as a measure of non-linear correlation, is defined as follows:"
"there are a variety of criteria for selecting the appropriate number of eigenvectors to use, including the widely used method of accounting for an arbitrarily chosen amount of variance sufficient to cover the majority of information required for the prediction task [cit] . the ppis predictor by de [cit], for example, chooses the number of pcs necessary to account for 95% of the variance of the data, after removing variables with excessively high linear correlation to each other."
"where all β i s were chosen to correlate best with the correlation coefficient above. this ic nps score was used to rank homologs of the query protein by their predicted conservation, of which the top ten were chosen to undergo a form of majority vote, where each residue was given a score based on the ratio of positive to negative votes. a threshold for this score was used to determine the interacting residues on the query."
"due to the wide range of techniques used by existing predictors, an objective performance evaluation requires the use of standardized datasets that encompass as much of the diversity of proteins and interfaces as possible [cit] and updated 3 times since its inception [cit], which has seen significant use among ppis predictors in its original, unedited form [cit], as well as in modified forms [cit] . all widely used modern testing sets are presented in table 2 ."
"for ml, rad-t then uses an alternating decision tree (adtree) [cit], an extension of the classical decision tree that integrates across multiple paths in the tree and makes use of boosting, in which multiple weak learners are used to build a single strong one."
"the limited information contained in single residues has often been supplemented with environmental or neighbourhood information [cit] . the vorffip (voronoi random forest feedback interface predictor) algorithm [cit] makes use of atom-resolution 3d voronoi diagrams [cit] to identify spatially neighbouring surface residues, for which features are assigned using structural, energetic, evolutionary, and experimental data. the use of voronoi diagrams may be better than other approaches as it is based on an implicitly defined \"visibility\" between residues (avoiding choice of falloff rates and threshold values), and it allows weighting environmental contributions with greater resolution [cit] ."
"more comprehensive and standardized benchmarking, as noted in other reviews [cit], is essential to advancing the field. additionally, to facilitate comparative evaluation of performance, as well as to ensure that improvements are not statistical anomalies, the authors suggest that significance testing be applied to future published work."
"hence, given n measurements, μ is within ofx with (1 − δ)% certainty. notably, hoeffding bounds do not rely on a particular underlying probability distribution and are thus widely applicable to different fitness measures with high conservatism (though reducing δ can mitigate this)."
"virtually all cellular machinery is composed of proteins, whose functions are mediated through biomolecular interactions; these serve to transmit signals and traffic molecular materials throughout the cell, as well as to form larger multimeric complexes capable of more complex behaviour [cit] . these interactions occur predominantly at conserved interfaces on the surfaces of the folded protein structures, often resulting in allosteric changes in the flexible conformations of the partners that alter their functions [cit] . the potential biomedical utility of interface identification makes the prediction of ppiss a critical endeavour, which necessitates theoretical knowledge of the various types of protein-protein interaction sites."
"this has, however, been disputed in the literature [cit], with the prevailing opinion appearing to favour shape complementarity, in which one of the proteins in a complex contains a concave binding site, while the interaction site on its partner exhibits convexity, in order to bind \"snugly\" [cit] . interestingly, nooren and thornton [cit] showed that transient ppiss not only tend to be more planar than their permanent counterparts, but that there is a gradient even within transient sites, with \"stronger\" sites exhibiting greater curvature than those in \"weaker\" transient interactions."
"feature selection is an indispensable part of ml, in which redundant and irrelevant attributes are removed from the feature set to ensure predictor efficacy [cit] . redundancy provides no new information (but potentially creates noise), is computationally inefficient, and overweights the contribution of that information, leading to overfitting and thus lower prediction scores."
"to test the utility of accounting for core-rim differences, the authors used the intervor [cit] algorithm (which computes distance to the interface rim via voronoi shelling order) with prescont, detecting differences in propensities (as found previously [cit] ), but ultimately not recommending use of core residues alone for training when the full ppis is desired."
"the authors note that even the more balanced measures should not be solely relied on (e.g. mcc may favour overprediction in ppis prediction [cit] and underprediction elsewhere [cit] ) and that predictor performance should be viewed holistically across as many metrics as possible, as balancing performance metrics is domain-dependent [cit] . when considering ppis prediction for mimetic drug design, slight underprediction may be desirable, as it will likely find the better discriminated core residues [cit], from which the remaining ppis can be inferred (rather than \"guessing\" which of many allegedly \"active\" residues is even interacting)."
"residue propensity, which measures ppis amino acid composition, has been used in the characterization of interface types (e.g. homodimers versus heterodimers [cit], permanent versus transient [cit], biological versus non-biological [cit] ) or subtypes (e.g. core versus rim [cit] ), in hotspot prediction [cit], and evolution [cit], as well as in ppis prediction [cit] . in general, polar amino acids are statistically disfavoured in interfaces sites, with the exception of arginine [cit] . further, while propensity differences are relatively minor between complex types, greater favouring of cysteine and leucine has been observed in permanent (but not transient) interfaces [cit] ."
"several areas of ppis prediction could benefit from the use of more sophisticated computational learning techniques. for feature selection and extraction, current methods can be combined (e.g. mrmr or pca with grs) or extended (e.g. empirical bernstein bounds [cit] with grs, nonlinear component analysis [cit], or autoencoders [cit] ). the class imbalance problem has been recently circumvented via bagging [cit], but semi-supervised learning could also be applied, as seen for hotspot [cit] and pairwise protein interaction prediction [cit], with minimal changes to the features used. indeed, as \"non-interacting\" residues may be mislabelled (since every possible protein complex is certainly not known), semi-supervised learning methods for handling this problem are even more applicable [cit] ."
"these vectors were then used for ml via a feed-forward artificial neural network (ann) with a sigmoid transfer function [cit] and the resilient back-propagation algorithm [cit] . separate ann classifiers for each protein atom type were trained and their outputs combined, with training designed to maximize mcc. further, the ensemble-based bootstrap aggregation (bagging) algorithm was employed to counter the class imbalance [cit] ."
"the foundation of the homppi predictor family is the evolutionary conservation of interface residues, derived solely from sequence information. the two homppi predictors [cit], nps-homppi (non-partner specific) and ps-homppi (partner specific), depend on the correlation between conservation of interfaces and several blast alignment statistics of sequence pairs, discovered by pca analysis. the conservation is calculated as the correlation coefficient of a prediction made by assigning all interacting residues of a protein in the pair to the corresponding residues on its sequence homolog. the blast statistics log(eval), positive score and log(lal) were found to be highly correlated with conservation in non-partner specific interfaces, where eval is the expectation score, lal is the local alignment length, and the positive score is the number of positive matches in the alignment."
"in general, similar to the lack of consensus in interface definitions and datasets, there is no standard criteria for performance assessment [cit] . given that some false positive predictions may be correct (due to the paucity of crystallized complexes), patch-specific performance metrics (i.e. assessing the correct answer in a local patch around an interface in question, such as by the sørensen-dice index [cit] ) may be used, though this poorly accounts for false positives. while other evaluation methods have been devised [cit], computing the statistics above per residue and averaging across the dataset appears to be the most objective and easily comparable method."
"alternatively, principal component analysis (pca), a method of information-preserving dimensionality reduction, can be used [cit] . the number of principal component (pc) vectors required to account for any amount of the original variance in the data can be calculated (via the pca eigenvalues), allowing control of the trade-off between high dimensionality and relevance to the predicted class. further, the pcs are orthogonal and hence linearly uncorrelated, greatly reducing feature redundancy. the features of this new space (with the pcs as basis vectors) can then be used as input to a machine learner."
"the computational power required to check all possible combinations of features renders such an approach impractical. to efficiently search this space, a method of feature selection termed genetic-race search (grs) [cit] was recently used, combining a genetic algorithm [cit] with race search [cit] ."
"the field of protein-protein interaction site prediction has grown significantly since the pioneering work of jones and thornton, and is now poised to bring great benefit to other problems in biomedical science, particularly rational drug design. this growth has, however, brought several issues to the forefront, including the need for standardized testing sets and evaluation metrics to ensure that objective comparisons of performance can be carried out. the field has seen numerous algorithmic advances over the past few years, building on decades of theoretical biology, and we expect that combining and extending these algorithms will be a considerable source of improvement in the future. as such, we have undertaken an exhaustive analysis of the state-of-the-art algorithms presently in use and their performance, as well as an exploration of the datasets and features employed by current predictors. we believe that future advances will bring predictors capable of significantly contributing to biomedical and pharmaceutical science."
"ppiss have long been thought to posses distinct 3-dimensional characteristics that allow them to be distinguished from the rest of the protein surface [cit] . in particular, curvature has been singled out as an important 3d structural characteristic [cit], with interface sites thought to be significantly more concave than the rest of the protein surface, lending stability and specificity to the interface [cit] ."
"the residues on alternating decision trees (rad-t) algorithm combines supervised ml with representative characteristics from all major feature types [cit] . training data was produced from monomers mapped back from their complexes, but separately crystallized as monomeric structures. this was done to train the learner on proteins in their monomeric conformation, rather than their complexed one, as ppis predictors will tend to be run on monomeric proteins of interest, produced by crystallization or by modelling, for which the partners or complexes are not known."
"recently, methods for accounting for neighbourhood information have become more prevalent, including averaging across the local environment [cit], voronoi diagrams [cit], and feature concatenation [cit], each providing gains in predictive ability. this suggests that methods for multi-scale machine learning could prove effective [cit] . machine learning techniques for multi-class classification [cit] (e.g. separating core vs. rim vs. surface) also hold potential for improvement."
"the identification of interacting residues on protein surfaces holds potential for use in diverse fields across biology and medicine. one of the most related problems is the elucidation of the residues inside ppiss that account for the major change in free binding energy upon complex formation, known as hotspots (hss) [cit] . ml-based ppis predictors have already been successfully used for hs prediction by simply altering the training set [cit] . interface predictions may be used to narrow the search space of hs predictors, as hss tend to localize to the interface core [cit] . for the same reason, putative hs residues could be used to \"seed\" interface site predictions. similarly, knowledge of a ppis can be used to guide mutagenesis experiments to more promising sites, reducing the expense and time required for a whole-protein analysis."
"the binding site prediction by maximum likelihood (bindml) approach is based on sequence-derived evolutionary information, though it does use an input structure to choose patches of the query protein to target [cit] . the first step involves the construction of two amino acid substitution matrices: one describing ppiss or protein binding interfaces (pbis), and the other describing non-protein binding interfaces (npbis) or non-ppiss (nppiss), via msas from ipfam [cit] . these matrices, m pbi and m npbi respectively, are computed by counting substitutions with pairwise alignment sets [cit], followed by construction with the blosum method [cit] ."
"to provide a benchmark for evaluating the performance of alternative bilevel solution methodologies, we solve the above described test problems using a nested scheme. as discussed below, the method relies on a steady state single objective real coded genetic algorithm to solve the problems at both levels. the underlying algorithm at both levels is a modified version of the procedures [cit] based on the single objective parent centric crossover (pcx) [cit] ."
"finally, we use the energy per output coefficient (eoc) as power metric which amounts to the average of energy required to compute one value of 2d dct output. eoc is calculated by multiplying the act by the power consumption and dividing the product by 64. it is shown in table 6 that the design based on xilinx's multiplier ip involves more than twice eoc compared with all the other designs. moreover, the proposed 2d dct with csd-cse technique needs less energy compared with csd-based design and xilinx's multiplier-based design. it can be further observed that the proposed csd-cse and quantization technique has 46% to 65% of eoc compared to csd-based design."
"the transparent execution of computations on the rpu entails several steps, summarized in fig. 7 . the injector is responsible for interfacing the cpu with the rest of the system, as well as for starting the reconfiguration process."
where the function ψ may be a single-vector valued or a multi-vector valued function depending on whether the lower level function has multiple global optimal solutions or not.
"for the evaluation, parameter was set to 500, and each kernel is compiled with mb-gcc 4.1.2, using the flag and additional flags which enable specific units of the microblaze processor (e.g., -mxl-barrel-shift for barrel shifter instructions)."
"in order to estimate the speedups for this scenario, it is assumed that a cr now contains only 4 fixed instructions versus the previous 20. these are absolute branches back to the starting address of the megablock and take 4 cycles to execute. in addition, there are as many put/get instructions as inputs/outputs. each completes in one clock cycle instead of the 12 cycles required to write successive values over the plb."
"in this paper, we propose a model for combined optimization of dct and quantization to implement them in the same architecture to save the computational complexity for image and video compression."
"the prototype uses a digilent atlys board with a xilinx spartan-6 lx45 fpga [cit] and 128 mbyte ddr2 memory. the cpu is a microblaze processor optimized for speed, clocked at 66.7 mhz. the same clock signal is used for all modules, including the rpu. two kernels (mpegcrc and usqrt) required lowering the clock frequency of the system to 33.3 mhz, due to delays in the generated rpu. the rm was implemented as another microblaze with the reconfiguration data in its local memory. this additional microblaze can also be used for monitoring purposes. table i summarizes the characteristics of the megablocks used in the evaluation. most kernels present similar values for the number of instructions executed per megablock call; the most notable exception is fibonacci, which executes from 3 to 16 times more instructions per call than the other benchmarks. included in table i are values for maximum instruction level parallelism (ilp), percentage of instructions covered by the megablocks (column cov.) over the total executed instructions, and average number of instructions per cycle (ipc), assuming each instruction takes one clock cycle to execute. table i also summarizes the characteristics of the rpus generated for each kernel. due to the interconnection scheme used, most of the fus are pass-throughs. the fu column indicates the total number of fus, and op ratio gives the percentage of fus that implement actual operations (as opposed to passthroughs). the rpu depth (i.e., number of rows) ranges from 3 to 8. the number of bits required for configuration of the inter-row switches grows with the number of fus in each row and with rpu depth: the largest individual benchmark (usqrt) requires 352 bits (44 bytes)."
"in this scenario, replacing the fu array by the kernels generated with catapult c achieves higher speedups, since the communication overhead has a lower impact on the overall performance. the average estimated speedup is, with speedups for individual benchmarks between and . on average, the implementations generated by catapult c are faster than our proof-of-concept implementation, while using about one-tenth of the resources. note that in a system with specialized hardware versions, the injector module can still be used to transparently move the computation from the cpu to the dedicated hardware."
"considering that fpgas are becoming more common of-theshelf components in embedded systems across several fields [cit], it has become important to fully exploit their processing capabilities. this also entails integrating fpga-related development tasks in the product development flow so as to limit toolchain complexity and reduce engineering costs over the whole product lifetime [cit] . one particular research topic has been the use of reconfigurable fabrics (e.g., fpgas) to accelerate execution of general purpose applications in embedded scenarios [cit] . some approaches include the warp processor [cit], the amber architecture [cit], the configurable compute array (cca) [cit], the dim reconfigurable system [cit], and the megablock approach [cit] ."
"we applied the same estimation approach to an extended set of 62 integer benchmarks (including image processing algorithms [cit] and commonly used algorithms [cit] ), which produce megablocks with memory operations. the estimations indicate an average speedup of . those results assume the use of dual-port brams, which support up to 2 memory operations per clock cycle [cit] . this extended set of benchmarks includes more complex examples than the ones presented earlier in this section. those benchmarks and their individual speedups include: md5"
"in the above equations, each of the levels contains three terms. a summary on the roles of different terms is provided in table i . the upper level and lower level variables can be seen to be broken into two smaller vectors (see panel a in table i ). the vectors x u1 and x l1 are used to induce complexities at the upper and lower levels independently. the vectors x u2 and x l2 are responsible to induce complexities because of interaction. in similar fashion, we decompose the upper and lower level functions such that each of the components is specialized for a certain purpose only (see panel b in table i) . at the upper level, the term f 1 (x u1 ) is responsible for inducing difficulty in convergence solely at the upper level. similarly, at the lower level, the term f 2 (x l1 ) is responsible for inducing difficulty in convergence solely at the lower level. the term f 2 (x l1 ) decides if there is a conflict or a cooperation between the upper and lower levels. the terms f 3 (x l2, x u2 ) and f 3 (x l2, x u2 ) are interaction terms which can be used to induce difficulties because of interaction at the two levels. term f 3 (x l2, x u2 ) may also induce a cooperation or a conflict. finally, f 1 (x u1, x u1 ) is a fixed term for the lower level optimization problem and does not induce any convergence difficulties. it is used along with the lower level interaction term to create a functional dependence between lower level optimal solution(s) and the upper level variables."
"the presence of an optimization problem within the constraints leads to a multi-fold increase in complexity for solving bilevel optimization tasks as compared to common optimization problems. in order to create realistic test problems for bilevel optimization, the construction procedure should be able to induce difficulties at both levels independently and collectively, such that the performance of algorithms in handling the two levels is evaluated. moreover, the problems are expected to be scalable in terms of number of decision variables to evaluate the performance of the algorithms against increasing number of variables. some of the desired properties in the test problems along with the ones already mentioned are:"
". therefore, to emphasize the nature of the lower-level problem as a parametrized constraint to the upper-level problem, an equivalent formulation of the bilevel optimization problem is obtained by replacing the lower-level minimization problem with a set value function which maps the given upper-level decision vector to the corresponding set of optimal lower-level solutions. in the language inspired by stackelberg's games, such mapping is often called the rational reaction of the follower on the leader's choice x u ."
in this test problem there is a conflict between the two levels. the difficulty is in terms of multi-modality at the lower level which contains the rastrigin's function. the upper level is convex with respect to upper level variables and optimal lower level variables.
"the implemented system is fully functional, runtime-reconfigurable and complete. the support tools successfully generate a hardware description that combines several megablocks with dfgs of varying depths along with the configuration and communication information required at runtime. the loose coupling between the injector and the remaining system modules allows the system to be easily adapted to other cpus, as well as other types of memory interface. the use of the injector module avoids the need to recompile the programs, while ensuring controlled behavior modification of the cpu by altering its instruction stream."
"in this subsection, we examine the quality of reconstructed image using an fpga prototype of the proposed dct-based image compression unit. the test images are saved in a rom in order to avoid the transmission time between the pc and the fpga. it is important to mention that in the final design we need to use a 2-bit word to indicate 4 available compression ratios. to measure the visual quality of the reconstructed image and to validate the proposed dct design, we use the xilinx's integrated logic analyzer (ila). this module works as a digital oscilloscope and enables to trigger on signals in the hardware design."
"the paper is organized as follows. in the next section, we explain the structure of a general bilevel optimization problem and introduce central notation that is used throughout the paper. section iii presents our framework for constructing scalable test problems for bilevel programming. thereafter, following the guidelines of the construction procedure, we suggest a set of six scalable test problems. a summary of the problems is given in section iv. to create a benchmark for evaluating different solution algorithms, the problems are solved using a simple bilevel evolutionary algorithm which is a nested scheme described in section v. the results for the baseline algorithm are discussed in section vi."
"control-flow instructions (e.g., branches) make hardware implementation more difficult and prevent optimizations, due to the multiple paths they introduce. instead of considering multiple paths, each megablock represents a single path of a loop across several branches. thus, the megablock includes an exit point for each instruction able to change the control flow. fig. 2 shows c code for a loop which contains an inner loop and a possible infrequent path for the common execution scenario, and fig. 2 shows the cdfg of the outer loop. depending on the execution, the trace produced by running the code can form two different megablocks with the same start address, \"abbd\" and \"acd\", as shown in fig. 3(c) . if a megablock is formed for path \"acd\", its iteration count may be too low to warrant implementation in hardware. alternatively, a megablock may be formed for the repetitive path \"abbd\" (with unrolled inner loop), with the branch to the infrequent path considered as an exit point."
"in order to evaluate the performance difference between specialized hardware implementations of each kernel and our approach, all benchmarks except merge1 and merge2 [cit] . this approach generates one dedicated hardware module per kernel. for the synthesis we used loop pipelining with an iteration interval of 1 for all kernels. for the specialized architectures, on average only 0.53% of luts and 0.21% of the fpga registers are needed. we consider here that the hardware cores generated with catapult c replace the fu array and the same data communication scheme is used. the average estimated speedup obtained over the software solution is (from a minimum of and a maximum of ) for a 66.7 mhz system clock. although the generated kernels are individually faster than the fu array (average clock frequency is 237 mhz), system performance is constrained by the communication overhead. therefore, only a modest speedup over the rpu version is achieved (less than 3%)."
"the work described here explores an alternative approach: loops in an execution trace are automatically identified and mapped to a reconfigurable processing unit (rpu), consisting of a specialized reconfigurable array of functional units (fus), and whenever one of those loops needs to be executed the array is transparently invoked and configured at runtime. the loops chosen for hardware implementation correspond to a special type of repetitive instruction traces called megablock [cit] . the megablock is a type of loop detected in the execution trace of a program. all iterations of a megablock type loop have the same execution path. the megablock has been proposed as a structure to be used for mapping execution traces of cpu instructions to hardware accelerators [cit] . the detection of megablocks is done by identifying repeating patterns of elementary units (e.g., basic blocks) in the streams of instructions forming the execution trace of a program."
"situations may occur where the cpu fetches instructions which will never execute. this occurs if a megablock starts after a mispredicted branch instruction. the implementation correctly identifies this situation: if the starting megablock address is followed by the next address in the megablock, this means the cpu did not discard the instruction, and is attempting to execute the code region mapped to the rpu."
"as mentioned before, the transfer of control flow performed by the injector is triggered by the detection of an instruction address that corresponds to the start of a megablock. although two or more megablocks may start at the same memory address, the current approach considers for megablock implementation the one with the highest code coverage measured during profiling."
"the set of selected megablocks is processed by two tools. one generates the hdl (verilog) descriptions for the rpu and routing information to be used at runtime by the rm. the other generates the cpu instructions for the communication routines and a verilog header file with the megablock addresses for the injector. both the routing information and the crs are included into the program memory of the rm. the rm copies the crs to ddr2, so that they can be executed by the cpu. the rpu description generation tool processes megablock information provided by the graph extractor tool [cit], determines fu sharing across megablock dfgs, assigns fus to rows, adds pass-through units, and generates verilog header files that characterize the placement of fus. as only one megablock will be executing on the rpu at any given time, the tool generates a description which reuses fus between megablocks."
"as discussed above, other functions can be chosen with desired complexities to induce difficulties at the lower level and come up with a variety of lower level functions."
"the architecture of the rpu was heavily influenced by the structure of the megablocks. specifically, the rpu was designed to execute loops with one path and multiple-exits. the rpu does not need to know the number of iterations of the loop before execution; instead, it keeps track of the possible exit conditions of the loop and detects when an exit situation occurs (the bne fu in fig. 6 performs exit detection in this example). for an rpu supporting multiple megablocks, the configuration also determines which exit conditions are to be considered to terminate execution. the results of the iteration that triggers an exit, i.e., the last iteration of the loop, are discarded, and the last iteration of the loop is re-executed in software. the cpu resumes execution at the megablock address so it can follow the correct control-flow path, which leads to the execution of the branch corresponding to the triggered exit."
"future work will address the support of caches memory and floating-point operations, as well as the reduction of the amount of resources needed to implement an rpu. possible further developments include the implementation of online megablock extraction and rpu synthesis, in order to have an autonomously adaptable embedded system."
"for y (2, v) and y (4, v) also the csd-cse techniques are used. y (2, v) is calculated as in (8), and the common subexpression of y (4, v) calculation is determined by increasing 8 vlsi design the number of common subexpressions shared between y (2, v) and y (4, v) ."
"the tasks performed by the cr include loading values into the rpu (in slot ), polling the rpu for completion (during slot ), checking the exit status and recovering values (slot )."
"t he challenging requirements of designing and implementing high-performance and flexible industrial control systems at low cost have made the use of field programmable gate arrays (fpgas) an attractive option [cit] . these modern, high-capacity devices are being used as platforms for the implementation of complete systems-on-chip, including one or more central processing units (cpus) connected to application-specific accelerators. however, the required design efforts to implement those systems are high. the design-flow combines software development and hardware design, the latter usually starting from a specification in a hardware description language (hdl) such as verilog [cit], and thus requiring hardware design expertise. one way to a faster design process is to use high-level synthesis tools, such as catapult c, which translate c code to hdl [cit] . this often requires rewriting the source code to fit the translator's requirements and limitations. implementing the interface between the generated hardware and the software is also necessary, a task which might require additional, manually-developed hardware, and further source code modifications."
"the megablocks identified are transformed into cdfgs (control-data flow graphs), which are then used as input to an offline tool chain that generates a specialized rpu for accelerated execution of the specific set of loops represented by the megablocks. the rpu is runtime-reconfigured to execute each megablock. in this paper, the program execution traces are obtained using a cycle accurate cpu simulator. the synthesis of the rpu is done offline, but the reconfiguration of the rpu occurs at runtime without changes to the executable binary."
"where the upper level variables (x u1, x u2 ) act as parameters for the optimization problem. the corresponding optimal-set mapping is given by"
this is a simple test problem with cooperation between the two levels. the lower level optimization problem is a convex optimization task. the upper level is convex with respect to upper level variables and optimal lower level variables.
"the last column shows the communication overhead (oh) relative to total execution time. the effective speedups measured (including communication overhead) range between and ( on average). for the merge benchmarks the speedup is lower than the average of the individual benchmarks they merge ( and for the benchmarks in merge1 and merge2, respectively) due to the overhead of rpu reconfiguration."
"where cs1, cs2, and cs3 denote 3 common subexpressions. in fact, the identification of common subexpressions results in significant reduction of hardware and power consumption reductions. for example, cs2 appears 4 times in x (2) . this subexpression is implemented only once and resources needed to compute cs2 are shared. an illustration of resources sharing is given in figure 3 . symbols n denote left shift operation by n-bit positions. it is important to notice that nonoverbraced terms in (12) are potential common subexpressions which could be shared with other dct coefficients such as x(4), x (6), and x (8) . according to this analysis, x(2) is computed by using 11 adders and 4 embedded multipliers. if csd encoding, is applied 23 adders/subtractors, are required. the proposed method enables to compute x(2) by using only 16 add/subtract operations. this improvement allows to save silicon area and reduces the power consumption without any decrease in the maximum operating frequency."
"step 1: selection of upper level parents. given the current population, we randomly choose 2µ number of members from the population and perform a tournament selection. this produces µ number of parents."
"in order to highlight the effect of subexpression sharing, 1d dct structure of loeffler algorithm is implemented with different multiplier designs. the power-delay product in nj is computed as the product of the dct computation time (ns) and the power dissipation (w) for the proposed design and the xilinx core. the power-delay product for different number of dct coefficients is calculated and plotted in figure 8 . the csd-based design and the proposed design using cse involve nearly 43% and 33% of power-delay vlsi design"
"the dfg represents one iteration (the kernel) of the megablock and contains operations (oval nodes), constants (white square nodes), input values (\"livein\" square nodes) and exit points (\"exit\" square nodes). all connections in the dfg represent dataflow between the nodes. for illustrative reasons, the labels in the connections in fig. 4 indicate output and input indexes, respectively. for instance, \"0:1\" means \"output 0 of source node connects to input 1 of destination node\". the connection labeled \"control\", between source node 4:equalzero and destination node exit:0 establishes a 1-to-1 relationship between the boolean output of an operation (e.g., true or false) and an exit point. the output determines if an exit is activated or not, according to the exit rule of the destination node."
"the results are reported in table ii for best, median, and worst values of function evaluations at upper and lower levels. the accuracy achieved and the number of times lower level optimization was performed in a single execution of the bilevel optimization run are reported in table iii ."
"communication with the rpu is done directly via a fast simplex link (fsl) interface [cit] . when a megablock start address is detected, the injector directly inserts the stream of instructions, which are adapted crs held in its own memories. the load/store instructions previously used by the cpu to communicate with the rpu via the plb bus are now replaced with instructions (get/put) that send/receive a single value over the fsl. once all operands are sent to the rpu, the injector sends a start signal to the rpu. the cpu then stalls until the rpu delivers the results as fsl get instructions are blocking. the rpu sends back a done signal which reports the exit status once calculations are done. the injector then continues the cr, recovering results. crs are now fetched at the same speed as regular bram accesses, which is 1 instruction per cycle."
"step 0: initialization. the algorithm starts with a random population of size n, which is initialized by generating the required number of upper level variables, and then executing the lower level optimization procedure to determine the corresponding optimal lower level variables. fitness is assigned based on upper level function value and constraints."
"since the cpu has no cache, the microblaze requires 23 clock cycles to fetch each instruction from the external ddr2 memory. for this scenario, the observed speedup is mainly due to avoiding this instruction fetch delay by executing operations on the rpu instead. each cr is also in external memory. therefore, the time required for executing the cr overwhelms the execution time of the rpu. the high communication overhead is also aggravated by the relatively low number of average instructions executed per call (see table i )."
"to incorporate this difficulty in the problem, we have chosen only the second functions at the upper and lower levels. the design of other functions are chosen to be done in the same way as suggested before. given that the term f 2 (x l1 ) is responsible for causing complexities only at the lower level, we can freely formulate it such that it has multiple lower level optimal solutions. from this it necessarily follows that the entire lower level function has multiple optimal solutions."
"according to the algorithm illustrated in figure 2, for a given column, y (1, v) and y (5, v) are calculated using adders and subtractors while y (3, v) uses a multiplicative constant and is given by:"
"the graph extractor tool [cit] was used to do an offline extraction of the megablocks from execution traces. for the detection, inner loop unrolling was disabled, basic blocks were used as the pattern element, and the maximum pattern size considered was 32. for each kernel (except the merge ones), the megablock with the highest coverage was implemented. on average, the selected megablocks cover 90% of the executed instructions."
"since the dct computation and quantization processes are computation intensive, several algorithms are proposed in literature for computing them efficiently in dedicated hardware. research in this domain can be classified into three parts. the first part is the earliest and concerns the reduction of the number of arithmetic operators required for dct computation [cit] . the second research thematic relates to the computation of dct using multiple constant multiplication schemes [cit] for hardware implementation. some other works on design of architectures for dct make use of convolution formulation. they are efficient but can be used only for prime-length dct and not suitable for video processing applications [cit] . finally, the third part is about the optimization of the dct computation in the context of image and video encoding [cit] . in this paper, we are interested in the last research thematic."
a test problem with a conflict between the two levels can be created by simply changing the signs of terms f 2 and f 3 on the right hand side in (4):
where f 1 affects only the value of the function without inducing any convergence difficulties. the corresponding optimal set mapping ψ is reduced to an ordinary vector valued function
"a megablock [cit] represents a sequence of executed instructions (trace) forming an iteration it, which repeats at least 2 times until one of its exit conditions is true. fig. 1 shows a portion of the instruction trace from the calculation of the fibonacci sequence on a microblaze processor. the code has a single loop which, when executed, repeats the same sequence of six instructions. this sequence of six instructions represents the iteration it of the megablock. it contains five arithmetic instructions (using addk, addik, and rsubk) and one branch instruction (bneid). whenever the branch instruction is executed, the value of register is compared to zero, and if different from zero, the control flow jumps back four instructions and repeats iteration it (as the branch has a single delay slot, it will execute an additional instruction before jumping). if register is equal to zero, the repetitions stop. this branch instruction represents the only exit point of this megablock."
"the average computation time (act) is the time interval after which we get a set of 2d dct coefficients. act is the product of the number of clock cycles required for the 2d dct computation and the duration of a clock cycle. the 2d dct computation requires 86 cycles, which is comprised of 8 cycles for register inputs, 7 cycles for the first stage 1d dct, 64 cycles for transpose memory, and 7 cycles for the second stage of 1d dct."
"the test-problem framework allows a number of ways to induce difficulties in the convergence of the optimization problem while retaining sufficient control. to demonstrate this, let us consider the structure of the lower level minimization problem."
"next, we consider the formulation of the upper level function such that a desired difficulty level in interaction between upper and lower level problems can be achieved. after having designed the lower level problem, the upper level optimization task is defined as a minimization problem over the graph of the optimal solution set mapping ψ, i.e."
"bilevel optimization is a branch, which deals with optimization problems containing an additional optimization problem within the constraints. such problems arise in many practical contexts, such as transportation (network design, optimal pricing), economics (stackelberg games, principalagent problem, taxation, policy decisions), management (network facility location, coordination of multi-divisional firms), engineering (optimal design, optimal chemical equilibria) etc [cit] . inspite of a large number of applications, the real life implementations are scarce [cit] due to the lack of efficient algorithms which could handle generic bilevel problems. moreover, there does not exist a test-bed on which some of the existing procedures could be evaluated systematically."
"where f 1 does not appear due to its independence from x l . since all of the terms are independent of each other, we note that the optimal value of the function f can be recovered by optimizing the functions f 2 and f 3 individually. hence, as the following example shows, calibration of the desired difficulty level for the lower level problem boils down to the choice of functions f 2 and f 3 such that their optima are known."
in this test problem there is a conflict between the upper level and lower level optimization task. the lower level optimization problem is a convex optimization task. an inaccurate lower level optimum may lead to upper level function value better than the true optimum for the bilevel problem. the upper level is convex with respect to upper level variables and optimal lower level variables.
"the proposed architecture and tools were tested and evaluated with 14 integer benchmarks from embedded computing, as well as commonly used algorithms [cit] . each benchmark contains a kernel that operates on 32-bit integer values. in order to ensure a fair comparison, each benchmark executes the corresponding kernel times, where is a compile-time constant. nine of the kernels have a fixed number of loop iterations: count, even ones, ham dist, pop cnt, reverse, divlu, isqrt2, sqrt and usqrt. the number of iterations of the remaining kernels depends on the values of the inputs."
"two additional benchmarks were considered, each one combining a set of 6 kernels: merge1 consists of count, even ones, fibonacci, ham dist, pop cnt and reverse; merge2 consists of compress1, divlu, expand, gcd2, isqrt2 and maxstr1. these benchmarks are used to validate the transparent support of multiple megablocks with different characteristics on a single rpu. in addition, they are used to verify that the tools correctly identify resources that can be reused between megablock dfgs."
"the rpus for the merged benchmarks exploit resource sharing. therefore, the total number of resources used is less than the total sum of the resources of the individual benchmarks. due to fu sharing, the merge1 and merge2 implementations use 32% and 51% of the total number of fus (operations and pass-throughs), respectively."
"the paper provides a test problem construction procedure for unconstrained single objective bilevel optimization. using the construction procedure, unconstrained test problems with controllable difficulties can be constructed. a test suit of six bilevel test problems have been proposed which may be used to evaluate the performance of any bilevel algorithm. as a benchmark for comparison, we provide the results from a simple bilevel evolutionary algorithm, which uses a nested strategy to handle bilevel problems. the procedure uses a parent centric crossover based global optimizer at both levels, which successfully solves the test problems. however, the function evaluations required by the algorithm are high, particularly at the lower level, because of the nested nature of the approach."
"example 2: consider a bilevel optimization problem where the lower level task is given by example 1. according to the above procedures, we can produce a test problem with a conflict between the upper and lower level by defining the upper level objective function as follows:"
"the system was conceived for an fpga environment: instead of using a single all-purpose rpu, the tool chain generates the hdl description of an rpu tailored for a single or a set of programs to be executed in the system. this step is done offline and automatically, as detailed further in this section. fig. 6 shows an example of a possible arrangement of the array of fus in the rpu. the rpu is organized as a set of rows with a variable number of single-operation fus. if there is an operation with one constant input, the rpu generation process tailors the fu to that input, as is the case with the bra fu in fig. 6, which represents an arithmetic shift right by 13 operation. the current implementation supports arithmetic and logic operations with integers, including carry operations. floating point operations, integer divisions, and memory accesses are currently not supported. crossbar connections are included between adjacent rows, and are runtime reconfigurable. connections spanning more than one row are established by pass-through fus (labeled pass in fig. 6 ). runtime reconfiguration changes the connections established by the inter-row crossbar switches. if the rpu is already configured for the megablock to be executed, reconfiguration is skipped, reducing its overhead. fig. 6 also depicts one possible interconnection arrangement for a megablock. according to its configuration, the rpu executes any one member of the set of megablocks it was designed to support. configurations applied at runtime are generated offline along with the rpu description."
(i) no two consecutive digits in a csd number are nonzero; (ii) the csd representation of a number contains the minimum possible number of nonzero bits and thus the name canonic.
"the results from each megablock iteration are passed to the following iteration via feedback connections that feed data back to the \"liveins\", as dictated by the detected dataflow."
"the algorithm uses a variance based termination criteria. when the value of α, described in the following equation becomes less than α stop, the algorithm terminates."
"for y (4, v) calculation, the common subexpression cs1 and cs3 defined for y (2, v) calculation is used. two new subexpressions cs4 and cs5 are introduced to further reduce the arithmetic operators. it is important to mention that the equations listed before are expressed to create several occurrences of common subexpression such as cs1, cs3, and cs4 those are used for y (2, v) calculation. the signal flow graphs of y (2, v) and y (4, v) are shown in figure 7 ."
"megablocks are found by detecting a repetitive pattern of up to instructions in the execution trace. since each individual instruction has an associated address value, the problem is equivalent to detecting patterns in the instruction addresses. fig. 1 has a pattern of size 6, which corresponds to the sequence of addresses 0x194, 0x198, 0x19c, 0x1a0, 0x1a4 and 0x1a8. the size of detected patterns can be reduced by considering coarser elements other than instruction addresses [cit] . for instance, pattern detection can be done over basic block addresses. in fig. 1, the list of addresses from 0x194 to 0x1a8 forms a single basic block. using basic block addresses instead of instruction addresses as pattern elements decreases the size of the detected pattern from 6 to 1. a study using a representative set of benchmarks has shown that to maximize the coverage of detected megablocks, the maximum size of the pattern can be as high as 32, even when considering coarser elements (e.g., basic blocks) [cit] ."
"this paper presented a novel approach for transparently moving computations from a cpu to reconfigurable processing units (rpus). execution traces are analyzed offline to identify loops that can be valuably moved to specialized rpus implemented in an fpga. the loop identification process targets megablocks, structures representing repeating patterns of instructions. an rpu is then synthesized to support the selected set of megablocks. at runtime, the execution the megablocks is moved transparently from the cpu to the rpu, which is reconfigured to carry out the computations of each megablock. the implemented system prototype shows that this is a feasible and promising approach for transparently accelerating the execution of embedded programs."
"in this test problem there is a conflict between the two levels. the problem contains infinitely many global solutions at the lower level, for any given upper level vector. out of the entire global solution set, there is only a single lower level point which corresponds to the best upper level function value."
"step 3: lower level optimization. for an offspring member, the closest upper level member is determined. from the closest upper level member, the lower level optimal member is copied. thereafter, a lower level optimization run is performed. the lower level is called with a population size n, the copied lower level member from the closest upper level member is included as one of the members in the population. this step is executed for each of the offspring members produced. fitness is assigned based on upper level function value and constraints."
"the choice of f 2 and f 3 suggested here is a special case, and there can be many other ways to achieve conflict or cooperation using the two functions."
"the aim of this paper is to propose a systematic framework for constructing bilevel test problems with controlled difficulties. such test problems are necessary to evaluate the performance of any algorithm, or make a comparison between the performance of different algorithms. the test problems should be able to represent the difficulties which practical application problems might have in store for the algorithms. moreover, the difficulties should be controllable, in order to assess the performance on different difficulty frontiers. past studies [cit] on bilevel optimization have introduced test problems where the difficulty level of the problems cannot be controlled. in most of the studies, the problems are linear [cit], or quadratic [cit], or non-scalable with fixed number of decision variables, or too complex such that the true optimal solution is not known. these drawbacks pose difficulties in algorithm development, as the performance of the algorithms cannot be evaluated on different difficulty frontiers. the contribution of this paper is to propose a collection of scalable bilevel test problems along with a simple test problem construction framework. the construction procedure allows to control the difficulties at the two levels independently, and also allows to control the difficulty caused by the interaction of the two levels. the problems generated by the framework are such that the optimal solution of the overall bilevel problem along with the optimal solution(s) for lower level problem are known for any set of upper level variables. this makes interpretation of the results easier and helps the algorithm developers to debug their procedure during the development phase."
"in this paper, we have presented a low-complexity dctbased image compression. we presented a novel common subexpression sharing of intermediate signals of the dct computation based on csd representation of loeffler's 8point dct algorithm. finally, we have combined the quantization process with the second stage of dct computation in order to optimize the bit width of computation of dct coefficient according to the quantization of different zones."
"to create a tractable framework for test-problem construction, we split the upper and lower level functions into three components. each of the components is specialized for induction of certain kinds of difficulties into the bilevel problem. the functions are determined by the required complexities at upper and lower levels independently, and also by the required complexities because of the interaction of the two levels. we write a generic bilevel test problem as follows:"
"using an external memory for cpu code without cache support imposes a significant overhead. an alternative is to consider an implementation which executes the kernels from on-chip memory implemented with brams (block rams), and use a point-to-point interface between the cpu and rpu. the injector interfaces with the rpu in the same manner. this scenario ensures the fastest possible execution of the software version. the injector is attached to the dedicated bus (local memory bus, lmb) that connects the cpu to the local memory (see fig. 9 )."
"common subexpressions elimination product of the xilinx's multiplier-based design, respectively. it can be seen in figure 8 that the computation of only the first 1d dct coefficient involves the same powerdelay product since this coefficient does not require any multiplier. note that the computation of 4th dct coefficient requires nearly the same power-delay product as that for 5th dct coefficient. indeed, the computation of the fifth dct coefficient requires only one more subtrator. for the 2d dct architecture using the loeffler algorithm a performance analysis is presented in table 6 in order to highlight the effects of csd coding, subexpression sharing, and quantization. the multiplier-based structures considered in the comparison are the xilinx's embedded multiplier synthesized as multiplier block ip and in luts. it should be indicated that the input bit width is of 8 bits, the dc coefficient bit width of the first and second 1d dct stages are 11 bits and 14 bits, respectively and the constant cosine coefficient bit width is 8 bits. the implementation of 2d dct is realized by decomposing the 2d dct into two 1d dct computations together with a transpose memory. it can be observed in table 6 that the area-delay complexity of xinlinx's multiplier-based 2d dct design (synthesized in block) is nearly the same as that of the combined csd-cse design but has nearly twice the power consumption. on the other hand, when the xilinx's multipliers are synthesized as lut, the 2d dct structure has less power-delay product but involves twice the area compared with the combined csd-cse structure."
in this test problem there is a cooperation between the two levels. the difficulty introduced is in terms of multimodality at the lower level which contains the rastrigin's function. the upper level is convex with respect to upper level variables and optimal lower level variables.
"this approach changes the execution flow of the cpu without overwriting the original instructions of the program or interfering with the original software toolchain. the execution of the communication routine (cr) may introduce a significant overhead (slots and ). each cr contains 20 fixed microblaze instructions, plus one instruction per input or output value."
"when the fifo is full, each following element can signal up to matches for squares with different sizes. for instance, by feeding the pattern aaaaaa to the algorithm, after processing the last element 3 matches are detected, for squares with sizes 1, 2 and 3, respectively. an arbiter must select the most relevant match. for instance, in order to consider only inner loops, priority is given to the match with the smallest substring size; to detect patterns with unrolled inner loops, but only when they appear inside outer loops (e.g., ), priority is given to the match with the longest pattern, but only if there is no match of a shorter pattern simultaneously in the current and in the previous set of matches (to prevent unrolling in cases such as ). because of the repetitive nature of the megablock, any address in the pattern can be taken as the start address. as the start address can influence single-pass optimizations, we chose the lowest address of the megablock which appears only once as the start address. for the example in fig. 1, the start address according to this heuristic is 0x194."
"in the mapping approach described in this paper, megablocks are transformed and then mapped into a dataflow graph (dfg) representation. fig. 4 represents the graph obtained from the execution trace of fig. 1 ."
"moreover, we have used the xpower tool of xilinx ise suite to estimate the dynamic power consumption. the power dissipation of the proposed 1d dct design and xilinx's core is about 39 mw and 62 mw respectively."
"in this paper, we utilize the ψ function in the test problem construction procedures to provide a convenient description of the relationship between the upper and lower level problems. to illustrate the behavior of ψ mapping, figures 1 and 2 show the two scenario where ψ can be a single vector valued or a multi-vector valued function respectively. in figure 1, the lower level problem is a paraboloid with a single minimum function value corresponding to the set of upper level variables x u . on the other hand, in figure 2, the lower level function is a paraboloid sliced from the bottom with a horizontal plane. this leads to multiple minimum values for the lower level problem, and therefore, multiple lower level solutions correspond to each set of upper level variables x u ."
we would like to point out that a prior detection of zeroquantized coefficients along with the proposed techniques could be used to further reduce the complexity of dct computations.
"the number of iterations in the loop of a program might not be known a priori. thus, to widen its applicability, instead of a traditional loop control with a fixed number of iterations, the megablock exclusively uses \"exit points\" to determine when computations end. by definition, a megablock always has at least one exit point."
"as the software execution from brams represents the best possible case for the microblaze processor, hardware speedup is significantly reduced, because there is no longer a high penalty for accessing memory. however, estimations indicate that speedups (including all overheads) for individual benchmarks between and ( on average) are achievable."
"by adhering to the design principles introduced in the previous section, we now propose a set of six test problems which we call as the smd test problems. each problem represents a different difficulty level in terms of convergence, complexity of interaction and lower level multimodality."
"in this alternative, it is the injector that performs the rpu reconfiguration, instead of the rm, via its own fsl connection. this can be done at the same time as the cpu sends the operands to the rpu. table iii shows the estimates calculated for this model. the expression was used to calculate the number of clock cycles for execution in the rpu alone (hw no oh), where is the number of iterations and is the depth of the rpu. the total execution time with hardware acceleration (sw+hw) includes the communication overhead, given by the expression to calculate the number of overhead clock-cycles, where is the fixed number of cycles needed to execute the cr (4, in this case), is the number of rpu registers (inputs and outputs) to communicate, and is the bram access delay (1 clock cycle, in this case). since all benchmarks (except merge1 and merge2) implement a single megablock, we did not include the rpu reconfiguration overhead, which is only incurred once. for merge1 and merge2, this overhead was considered. the estimates indicate an average overhead of 11% in individual benchmarks when using the fsl interface, versus 89% measured for the ddr case."
"megablocks are useful if they represent a significant part of the execution of a program. previous work [cit] shows that for many benchmarks, megablocks can have coverage similar to or greater than other runtime detection methods, such as monitoring short backward branches (used by warp [cit] )."
"the bilevel evolutionary algorithm designed to handle single objective bilevel problems is a simple minded strategy where the lower level problem is solved for all given upper level points. whenever the lower level optimal member is to be determined for a new upper level member, the information is utilized from the nearest upper level member for which lower level optimal solution is known. the main steps in the algorithm are summarized as follows:"
"in the current implementation, the feedback routing of results to the next iteration is configured through a single register. this limits the maximum number of input/outputs of the rpu. it depends on a combination of the number inputs and outputs, e.g., a maximum of 10 inputs for 8 outputs. a similar limitation imposes a maximum number of 32 exit conditions. however, these limitations were not an obstacle for implementing the megablocks found in the benchmarks being used."
"the architecture of a hardware prototype supporting transparent hardware acceleration is shown in fig. 5 . in order to avoid modifications of the cpu or its interfaces, or of the software toolchain, as in other approaches [cit], the implementation uses standard interfaces such as the plb (processor local bus) [cit] . modifications of the cpu instruction streams are made at runtime without rewriting instruction memory and thus ensuring full transparency. the hardware prototype system consists of five modules as shown in fig. 5 . the reconfigurable processing unit (rpu) is a loosely coupled hardware accelerator connected to the plb. an injector module monitors the instruction address bus and triggers the use of the rpu by modifying the contents of the instruction stream. the injector acts as a pass-through for all the instructions to be executed by the cpu. in this prototype, the cpu loads program instructions from external memory (ddr2). the reconfiguration module (rm) is responsible for managing the runtime configuration of the rpu."
"1) controlled difficulty in convergence at upper and lower levels. 2) controlled difficulty caused by interaction of the two levels. 3) multiple global solutions at the lower level for any given set of upper level variables. 4) clear identification of a relationship between the lower level optimal solutions and the upper level variables. 5) scalability to any number of decision variables at upper and lower levels. 6) possibility to have conflict or cooperation at the two levels. 7) the optimal solution of the bilevel optimization should be known. in this paper, we propose a bilevel test-problem construction procedure, which should be able to incorporate all the above mentioned features. the current procedure is designed for an unconstrained set of test problems, and therefore we have omitted the discussion on constraints in our procedure. the construction procedure can also be extended to constrained problems, however, we leave it for a future work."
"in the current implementation of the rpu, all operations complete within one clock cycle. each loop iteration takes as many clock cycles as the number of rows in the rpu, as intermediate results are registered at the outputs of the fus. the rpu acts as a bus slave communicating by means of a standard plb interface. data transfer of operands and results is done through memory mapped registers. configuration is performed by writing to a set of configuration registers, whose number depends on the size of the array. these registers control the routing of the operands through the rpu and define which exit conditions are active."
"the dfg is built by taking the start address of the megablock (i.e., 0x194 in fig. 1 ) and adding nodes to the graph according to the sequence of instructions of the megablock. each cpu instruction is transformed into one or more platform-independent instructions. when building the graph, additional information-not represented in fig. 4 -is generated, including a counter for each type of graph node, tables which map each output of the graph to its last definition in the graph, and a table with all non-data dependencies between operations. this information is needed for implementing megablocks, for instance, to indicate which nodes produce the output values that must be sent to the cpu on termination of the megablock, which cpu registers are the destinations of these values and what restrictions need to be imposed when scheduling operations. optimizations such as constant folding and propagation, and algebraic simplifications are then applied to the dfg."
"the merge2 benchmark demonstrates an rpu that supports megablock dfgs of different depths. the rpu has a depth equal to the maximum depth of the individual dfg kernels, and smaller dfgs are supported by using pass-throughs in the additional rows. resource utilization for the spartan-6 lx45 fpga ranges from 5.3% to 27.8% for luts, and from 1.3% to 5.4% for flip-flops. the implementation of merge1 requires 55% of the luts and 27% of the ffs that would be needed if the rpu were generated with no sharing of fus. for merge2, these values are 81% and 38%, respectively. table ii summarizes the results measured with the prototype. the execution times were measured using dedicated timers (counting clock cycles). for each benchmark, the table includes table ii performance results for ddr-based prototype the execution time for the software version (sw), for the hardware-accelerated version (sw+hw), and the associated speedup. also included is the running time of the hardware version without the communication overhead (hw no oh)."
"intermediate results e22 and e24 in (9) are shown in figure 2 for a given column. now, constants √ 2 cos(6π/16) and √ 2 sin(6π/16) are converted to csd format and given, respectively, by 0 + 000 + 0+ and 0 + 0 + 0 + 00− . y (1, v), y (3, v) and y (5, v) calculations are given in figure 6 ."
"the remainder of the paper is organized as follows: an overview of fundamental design issues is given in section 2. proposed dct optimization based on csd and subexpression sharing is described in section 3. an algorithm based on joint optimization of quantization and 2d dct computation is proposed in section 4. finally, the experimental results are detailed in the section 5 before the conclusion."
"in this test problem there is a conflict between the two levels. the difficulty introduced is in terms of multi-modality and convergence at the lower level. the lower level problem contains the banana function such that the global optimum lies in a long, narrow, flat parabolic valley. the upper level is convex with respect to upper level variables and optimal lower level variables."
", bilinear, fft, fir, and fdct . using the same approach, we applied our mapping technique to an airborne collision avoidance application, known as 3d path planning, provided by honeywell. it consists of 841 lines of c code, distributed over 10 files and 48 functions. a step of the application requires 50 601 067 microblaze clock cycles. we use our target architecture with megablocks mapped to a 2d cgra based on the structure of the rpu depicted in fig. 6, extended with 2 load/store units per row. this implementation considers 9 megablocks responsible for almost 87% of the total execution time. the speedup achieved by the execution of the megablock sections of code in the rpu is around, resulting in an estimated overall application speedup of ."
"(10) here, we observe that f 2 (x l1 ) induces multiple optimal solutions, as its minimum value is 0 for all x, one of the solutions is best at upper level."
"the second contribution of the paper is an introduction of a new schema of image compression where the second stage of 1d dct (dct on the columns) is configured for joint optimization of the quantization and the 2d dct computation. moreover, tradeoffs between image visual quality, power, silicon area, and computing time are analysed."
"is a sum of three independent terms. once again, our primary interest is on the last two terms f 2 (x l1 ) and f 3 (x u2, x l2 ), which determine the type of interaction there is going to be between the optimization problems. this can be done in two different ways, depending on whether a cooperation or a conflict is desired between the upper and lower level problems."
the inside of rotor blocks abc to dq0 transform include mathematical modeling of induction generator and system is shown by figures 7 and 8 .
"step 1. determination of the system dynamic behavior and characteristics in this step, the controller input and output variables and their variations range considering load-frequency control problem should be determined. input signals are considered to create the base rules as the if part and the output signal of the fuzzy controller are considered as the then part."
"when a collection of e. coli cells is located in the center of a semisolid agar with a single nutrient chemo-effecter (sensor), they shift out from the center in a traveling ring of cells byshifting up the nutrient gradient formed by consumption of the nutrient by the group. in addition to that, if high levels of succinate are used as the food, then the cells let loose the attractant aspartate so that they assemble into groups and, hence, move as a patterns of groups with high bacterial density. the spatial order results from outward movement of the ring and the local releases of the attractant; the cells provide an magnetism signal to each other so they swarm together. the mathematical representation for swarming can be represented by wherej cc (θ,p(j,k,l)) is the cost function value to be added to the real cost function to be minimized to present a time changeable cost function, s is the total number of bacteria, p is the numeral of parameters to be optimized which are present in each bacterium, and d attract,w attract,h repellent,w repellent are different coefficients that are to be chosen properly."
ac/dc and dc/ac converters create harmonics from ac and dc side that may have no suitable size for the source or load. the filters are used to reduce the harmonics. filter that is used in the ac side are to reduce the current harmonics and on either side of the dc is to reduce the voltage harmonics amplitude.
"there has been a significant amount of research on svo particularly focusing on how to extract its favorable features without any outliers. stereo odometry algorithm relying on feature tracking (soft2) [cit], which has been known to perform optimally, implemented simultaneous localization and mapping (slam) by performing pose estimation and mapping in parallel. it utilized blob and corner masks to extract features and the essential matrix to estimate the pose. it also considered the loop closing for feature and keyframe management. as features are extracted depending on the depending on the rotation, there is a disadvantage that the performance may degrade depending on the state of the viewpoint. the rotrocc+ [cit] method studied the characteristics of the optical flow and reprojection error for odometry and eliminated outliers by decoupling the optical flows of motion and exploiting the characteristics of the flow using a restrictive motion model. therefore, there was a problem that outliers were not able to be accurately removed when the estimated vehicle's motion deviated from the model. gradient-based direct visual odometry (gdvo) [cit] method used a dual jacobian optimization with a multiscale pyramid scheme for outlier removal. this method also applied gradient feature representation to respond to the lighting changes. however, it did not apply bundle adjustment, and therefore, the coordinates of the features were incorrect. elbrus [cit] applied the multiple pyramid kanade-lucas-tomasi (klt) method to track the feature and selected inliers using 2d track average motion and rate of disappearance. this method searched for features on multiple scales. it did not use depth information for elimination outliers. circular fast retina keypoint (freak)-oriented fast and rotated binary robust independent elementary feature (orb) visual odometry (cforb) [cit] detected features based on freak-orb and repeated the process 50 times to perform random sample consensus (ransac) [cit] for outlier elimination. this process was carried out using the concept of circular matching. other methods used various ways to determine inliers [cit] . it is notable that vo is based on features, and the accuracy of a method is highly related to the state of the feature. most of the previous methods mentioned above optimized features based on pixels. in this study, we try to perform feature optimization using image geometry. based on the photogrammetric analysis, we aim to apply image geometry for feature optimization and pose estimation."
"in this scheme, bfa( bacteria foraging algorithm) is used to improve the energy efficiency in wireless sensor networks. problem size, cells num, n ed, n re, n c, n s, step size, d attract, w attract, h repellent, w repellent, p ed are given as a input. algorithm output produced the best cell value where it can be a cluster head. at initial step cell number, problem size are given to find the initial population. based on this population the high energy node is calculated. there are four basic steps, such as chemotaxis, swarming, reproduction, removal and dispersal."
"wireless sensor networks (wsns) consists of networked sensors that work together in hundreds of thousands of numbers for collaborative signal processing, monitoring, sensing and control tasks. wsns offer extensive benefits and versatility to low-power and low-cost rapid deployment for many applications that can be automated without any human supervision. many of the applications include disaster recovery, military surveillance, health administration, environmental & habitat monitoring, target tracking. wsns can selforganize and self-configure independently by inter-node communications possible through multi-hop wireless paths. while the communications of each node is possible via the transceiver unit, each sensor node also consists of a sensing unit, processing unit, and a power basis unit. each of these units is included together with the help of ics with integrated signal processing and micro-sensing components. these nodes that form the wsn can be sited far from the actual occurrence, and can still be used for data aggregation and collection from a remote location far away from the point of event-occurrence."
"due to changes in the parameters in studied system, using the fixed gain classic controllers is not reasonably fixed. because this controller is not capable to maintain system stability in conditions of parameters variation. to solve this problem, the adaptive controller can be used. therefore, this controller is used in wind farm. the overall block diagram of the adaptive controller is shown in fig. 3"
"the fractal dimension, obtained after modifying the signal by multiplying its amplitude by the multiplier, is no longer the fractal dimension of the original signal but rather the fractal dimension of the modified signal only, used solely for the purpose of decision making and for classifying or quantifying the intensity of events occurring throughout the time series signal."
"it is evident that the multiplier of the normalised signal amplitude must be the same in all related signals for comparative reasons. for example, this applies to parts of a signal which are compared, as well as to signal data related to a therapeutic period (of one or more patients) or to a specific pathology across a cohort of patients. this is not the case in sevcik's unit square method, as different multipliers result if the amplitude range and/or time periods of signals to be compared are not identical. if is determined as a function of normalised time with a running average analysis, then the window width has to be taken care of. if the window width is widened, then the of the signal is less noisy, the valleys become narrower (and the peaks wider) along the time axis, and the valleys become shallower (and the peaks flatter) along the amplitude axis. the optimal multiplier has to be determined at the same window width (e.g., figure 18 ) as subsequently used for the optimised fractal dimension analysis (e.g., figure 19 ). for the optimisation procedure, the averaged over time or over the window width is used (e.g., figure 18 ). different recording devices are likely to deliver different if their sensor specifications do not match. optimisation of the signal can mitigate the problem."
"the optimisation method developed in this study is essential for decision making, particularly when the decision hinges on the condition that two different signals (or parts of a signal) have different fractal dimensions. the optimisation method is applied to calculating the maximal fractal dimension differential, as a function of the amplitude multiplier. alternatively, the fractal dimension differential between maximal dimension and average dimension can be maximised. this optimisation method enhances fractal dimensions above average and suppresses those below average. maximising the ratio of maximal to average dimension provides a further optimisation effect."
"if there are only two different events (e.g., physiological and pathological) embedded in a signal, with the latter being rare, then the average of the entire signal, avg, is compared to of the rare event ( max if the rare event produces a higher than the rest of the signal). of the rare event should not significantly influence the average . δ then results from"
"a torque which is estimated by an adaptive controller for generator and is applied as input to it, is obtained by following equation [cit] :"
"the flowchart of the proposed real-time visual odometry technique is shown in figure 2 . first, we extracted features from images and searched for corresponding points by matching. this process is important because it takes a significant amount of time in the whole process, and the number of features and the matching result affect the accuracy of the estimation. therefore, we compared the processing time and the number of corresponding points of several candidate methods. next, we optimized the corresponding points. as mentioned, this process is necessary because the degree of the outlier affects the accuracy. in this study, we applied photogrammetry-based and computer vision-based optimization. in the photogrammetry-based part, we checked the reprojection error and the distance between the calculated and projected model points. this part is performed after the second frame because the geometry information between the previous and current image is needed. in the vision-based part, the outlier filtering in multiple images was based on ransac. finally, we estimated the pose using the optimized corresponding points. it was based on the absolute orientation of the photogrammetric bundle adjustment. finally, the relative positions of the platform were calculated by continuously accumulating the estimated pose. the detailed explanations are as follows."
"the fuzzy controller membership functions are shown in step 3. description of inference engine in this step, fuzzy rules are formed by the control rules that system performance is based on that rules."
"in vision-based optimization, the ransac-based outlier filtering over multiple images was performed as in figure 6 . this method extracts random samples from the data and creates a model. then, it selects the appropriate model while inputting the remaining data. in this process, outliers are removed. within the next stereo pair (l t and r t ), we first extracted features corresponding to the features classified as inliers through photogrammetry-based optimization at (t -1). then, we eliminated the outliers by applying ransac while combining two images. we applied ransac in order from 1 to 3 in figure 6, and the features recognized as inliers in 4 images were saved for pose estimation."
"remote sens. 2019, 11, x for peer review 7 of 16 we estimated the platform's pose through absolute orientation using the collinearity condition. the collinearity condition is a condition that the three-dimensional coordinates of the object existing in the image, the image coordinates, and the camera projection center must be on the same straight line as shown in figure 7 . first, we determined the model points defined as (pn) between o1 and o2. then, we established the relationship between pn and pn based on the collinearity equation as in equation (7)."
"finally, we experimented with 00 to 11 (except 01) sequences provided by kitti. figure 15 for ten sequences, the average rotation error was 0.0175 deg/m, the average translation error was 3.5520%, and the processing time per frame was 0.0554 s on average."
"for comparative reasons, was set to 1.5, was limited to 50, and (, ) was calculated for one second at a frequency of 1 khz. the results are shown in figure 7 (c). the mafdm has the same accuracy as the mrbc method and performs better than higuchi's method. ) and is the number of data generated. as the brownian motion function is stochastic, the shapes of the curves are not identical, and therefore ten different fractal curves were tested for each (from 1 to 2, in 0.1 increments). the results are shown in figure 7(d) . the mafdm has roughly the same performance as the mrbc method and higuchi's method."
"we checked the trajectory result by the path shape. we experimented with the sequence acquired in the area with fewer curves, a large number of curves, and a sharp curve. in figure 14, the red line indicates the ground truth provided by kitti, and the blue line indicates the trajectory estimated by the proposed method. as shown, the trajectory was more sensitive to the number of curve appearances rather than the degree of the curve. for three cases, the rotation error rate was 0.0156 deg/m, the translation error rate 2.8727%, and the processing time per frame was 0.0313 s on average."
"let j be the chemotactic step, k be the reproduction step, l be the elimination and dispersal event, s be the bacteria. chemotactic is a process of finding a nearby nodes. let nc be the distance end to end of the lifetime of the bacteria as deliberate by the number of chemotactic steps they take during their life."
"we estimated the platform's pose through absolute orientation using the collinearity condition. the collinearity condition is a condition that the three-dimensional coordinates of the object existing in the image, the image coordinates, and the camera projection center must be on the same straight line as shown in figure 7 . first, we determined the model points defined as (p n ) between o 1 and o 2 . then, we established the relationship between p n and p n based on the collinearity equation as in equation (7)."
"chemo taxis: this process in the control system is achieved through swimming and tumbling via flagella. each flagellum is a left-handed coil configured so that as the support of the flagellum (i.e., where it is connected to the cell) rotates counterclockwise, as viewed from the free end of the flagellum looking in the direction of the cell, it produces a force against the bacterium so it pushes the cell. on the other hand, if they rotate clockwise, each flagellum pulls on the cell, and the net result is that each flagellum operates relatively independently of others, and so the bacterium tumbles about. therefore, an e. coli bacterium can go in two different ways; it can run (swim for a period of time) or it can tumble, and exchange flank by these two modes of operation in the entire lifetime. to represent a tumble, a unit distance end to end arbitrary direction, say( j), is generated; this will be used to define the way of movement after a tumble. in particular where j+1,k, l represents the i th bacterium at j th chemotactic step, k th reproductive step and l th elimination and dispersal step. c(i) is the size of the step taken in the random direction specified by the tumble (run length unit)."
"we performed experiments with ten sequences in the kitti dataset. the specifications of the computer used were windows 10 64 bit, cpu i5-6600 3.30 ghz, ram 16 [cit], microsoft product in the united states. this section shows the results of corresponding point optimization and pose estimation. then, it described the performance of the proposed method."
"to explain how chemotaxis motions are generated, we must simply explain how the bacteriadecides how long to run,since from the above discussion we know what happens during a tumble or run. first, note with the intention of if an e. coli is in some substance that is neutral in the sense that it does not have food or noxious substances, and if it is in this medium for a long time (e.g., more than 1 min), then the flagella will simultaneously alternate between moving clockwise and counterclockwise so that the bacterium will alternately tumble and run. this alternation between the two modes will move the bacterium, but in arbitrary directions, and this enables it to \"search\" for nutrients. for instance, in the isotropic homogeneous environment described above, the bacterium alternately tumbles and runs with the mean tumble and run lengths given above and at the speed that was given. if the bacteria are placed in a homogeneous concentration of serine (i.e., one with a nutrient but no gradients), then a variety of changes occurs in the characteristics of their motile behavior. for instance, mean run length and mean speed increase and mean tumble time decreases.they do still produce, however, a basic type of searching behavior; even though the bacterium has some food, it persistently searches for more. suppose that we call this its baseline behavior. as an example of tumbles and runs in the isotropic homogeneous medium described above, in one trial motility experiment lasting 29.5 s there were 26 runs,the maximum run length was 3.6 s, and the mean speed was about 21 next, suppose that the bacterium happens to meet a nutrient gradient (e.g., serine). the change in the concentration of the food triggers a reaction such that the bacterium will spend large time swimming and less time tumbling.."
"finally, it has to be pointed out again that the fractal dimension, obtained after modifying the signal by multiplying its amplitude by the multiplier, is no longer the fractal dimension of the original signal but rather the fractal dimension of the modified or transformed signal. serves only for comparing two or more signals (or parts of the same signal) for the purpose of decision making and for classifying or quantifying the intensity of events occurring throughout the time series signal. thus, does not serve for accurate evaluation of the fractal dimension of the original signal. yet, fractal dimensions of signals are often compared in order to draw conclusions, the practical applications of which ultimately result in decision making. the latter is optimised with the method described and presented in this study."
"in this paper, a fuzzy controller is designed for stabilization of the amplitude and frequency of the variable voltage caused by the uncertainty of the wind nature. a fuzzy adaptive controller was designed as direct supply to use maximum wind power at each moment. as is clear from the results, by applying the error associated with changes in wind speed, adaptive fuzzy controller employs wind maximum power and then voltage was created in induction generator. induced voltage has variable frequency and amplitude which were kept constant in amplitude and frequency by fuzzy controller and power electronics circuits. according to the results output voltage is within the standard in point of view of total harmonic disorder."
"finally, we experimented with 00 to 11 (except 01) sequences provided by kitti. figure 15 is a graph showing the error occurrence per mileage with sequence 00, where (a) is about rotation and (b) is about translation. figures 16-24 are graphs for sequences 01 to 11."
"in vision-based optimization, the ransac-based outlier filtering over multiple images was performed as in figure 6 . this method extracts random samples from the data and creates a model. then, it selects the appropriate model while inputting the remaining data. in this process, outliers are removed. within the next stereo pair (lt and rt), we first extracted features corresponding to the features classified as inliers through photogrammetry-based optimization at (t -1). then, we eliminated the outliers by applying ransac while combining two images. we applied ransac in order from ① to ③ in figure 6, and the features recognized as inliers in 4 images were saved for pose estimation. figure 4 shows the proposed feature optimization concept. as shown in the figure, we performed photogrammetry-based optimization using the previous and current images and vision-based optimization using the current and new images. the photogrammetric optimization was performed from the second image because it needed the image geometry. figure 5 explains photogrammetric optimization process in detail. suppose that we have exterior orientation parameters estimated for the image pair (l t−1 and l t−2 ). the features on the previous images can be projected onto the current images (l t−1 and r t−1 ) through the estimated exterior orientation parameters (eop). when the accurate image point is projected, the projected model point has a small separation from the calculated model point. also, this model point is re-projected onto the previous image; it has a small separation from the corresponding image point. however, in the case of the inaccurate image point on the current image, when projecting or re-projecting it on the previous image, the differences are large. based the separation distance, we selected optimized features."
"in order to assess the influence of scaling a signal on its fractal dimension, has to be determined in a 2-dimensional space rather than in a mono-dimensional one. it is therefore required that the length of a signal is calculated from the euclidean distance between data points of time series. this makes variable when scaling the amplitude [cit], to be shown subsequently. the main question in this case is not \"what is the correct method of calculating fractal dimensions?, \" but rather \"how can we maximise the information to be obtained from fractal dimensions?. \" the term \"maximising the information\" is seen from an engineering point of view, that is, using the difference in of two signals (or parts thereof when using a sliding window method) for practical application, for example, for automated decision making. this would apply to classifying sleep stadia from eeg signals, or identifying arrhythmias from ekg signals, by setting off an alarm at the onset of the latter."
"equation (12) is divided by two as is the average of two solutions (figure 4(b) ), resulting from taking every other even datum or every other odd datum (insert in figure 4(b) ). both solutions are embedded in σ in (12) . the terms 0.5 −1 and 0.5 + −2+1 are due to the fact that the first green in figure 4 (b) starts only at point no. 2 (insert in figure 4(b) ), and the last green ends at the point preceding the last one of the shown window. this means that half-segments were missing before point 2 (↔ in figure 4 (b)/insert) and after the second last point, if they were not included in the first place."
"feature matching is divided into pairwise matching and sequential tracking. in pairwise matching, feature description, and matching are performed. after feature extraction, we calculated feature descriptors as shown in table 1, and compared the resemblance to determine a corresponding point according to the matchers listed in table 1 . in sequential tracking, we set a window around a tables 1 and 2 summarize various techniques for pairwise matching and sequential tracking applied in this study."
"we need to enhance the performance of the proposed optimization process further as there were some remaining outliers after optimization. in this paper, we considered photogrammetric analysis between a stereo pair of current and previous frames. we need to accumulate the results of incoming frames to remove outliers with better accuracy. also, we need to consider preprocessing multiple stereo pairs of previous frames to generate a list of reference features for incoming frames. the major contribution of this paper is that we showed the feasibility of real-time outlier removal by photogrammetric analysis."
"a fuzzy system is described by a set of if-then rules and uses a number to define the degree of membership in its membership functions. to solve the problem in fuzzy controller, controller inputs, error signal and its derivation, and its output are considered as the control signal. the wind turbine system with fuzzy controller structure is shown in fig. 9 . design of fuzzy controller has 4 steps that are presented as follows [cit] :"
"the studied system data related to the induction generator is presented by table 2 [cit] . table 2 . simulation parameters [cit] value parameter overview of system simulated in simulink is shown in fig. 12 . in this section, a three phase fault is applied to the system according to fig. 12 and with design of fuzzy controller is attempting to fix and maintain the voltage and frequency fluctuations. output voltage curve of the studied system is presented in figure 17 . as shown as in fig. 17, output voltage of system is sinusoidal. the thd of system output voltage is shown in fig. 18 . thd of system output voltage in this stage is 4.65 percent ant it is within standard ranges. thd of system output voltage based on step r-l load is shown in fig 19. thd in this condition is equal to 4.91 percent in standard range. so the total harmonic distortion of system is in standard domain. the results showed that fuzzy controller in power system stability are robust and suitable for damping application in power system. according to obtained results the amplitude of frequency and voltage oscillations are kept constant and in standard range by fuzzy controller application."
"through equation (4), the model point on the model space at (t − 1) is re-projected onto the image at (t − 2). through equation (5), the distances between the re-projected and the actual image points are calculated on the image space and classified as a threshold2 as in ② of figure 5 . as in equations (3) and (6), if both threshold1 and threshold2 are satisfied, the feature is extracted as an inlier. also, while checking the number of features, this process is repeated using the previous images."
"favorable feature extraction and outlier removal are key to visual odometry techniques. in this paper, we proposed photogrammetric feature optimization applicable to stereo odometry. using the estimated poses of previous frames, we repeated the process of projecting and re-projecting the corresponding points extracted from the current frame onto the previous ones. then, we removed the outliers by confirming the projection and re-projection errors. in addition, we optimized the feature on the new input image through multi-image filtering. through the experiments, we were able to confirm the applicability of the proposed photogrammetric feature optimization process to stereo visual odometry technology."
author contributions: all authors contributed in the developing method and editing of the paper. s.-j.y. is the main author who designed whole experiments and wrote the manuscript.
"feature matching is divided into pairwise matching and sequential tracking. in pairwise matching, feature description, and matching are performed. after feature extraction, we calculated feature descriptors as shown in table 1, and compared the resemblance to determine a corresponding point according to the matchers listed in table 1 . in sequential tracking, we set a window around a feature of one image and tracked the corresponding point from the next image in"
"the three different optimisation methods, (35)-(37) facilitate that the of the different signals are clearly separated by applying a zooming effect to the signals. it will be shown in section 8, that this zooming effect also provides a filter effect."
"the corresponding point extraction was carried out in the order of feature extraction and feature matching. first, in feature extraction, we extracted features such as the corner points or edges on the image. we selected representative feature extractors, scale invariant feature transform (sift) [cit], speed-up robust feature (surf) [cit], and features from accelerated segment test (fast) [cit], shi-tomasi [cit], provided by opencv."
"our research is ongoing and the performance shown here needs further improvements, particularly compared to known optimal algorithms. for example, soft2 [cit] achieved a rotation error of 0.014 deg/m, a translation error of 0.65%, and a processing time of 0.1 s/frame. nevertheless, our results support our intention of using photogrammetric analysis as an alternative outlier removal method. we showed that the proposed photogrammetric processing could enable successful outlier removal and that real-time processing was feasible even with photogrammetric iterative estimations. it is notable that we adopted the concept of circular matching proposed in cforb [cit] and enhanced its performance by photogrammetric optimizations. cforb achieved a rotation error of 0.0107 deg/m, a translation error of 3.73%, and a processing time of 0.9 s/frame [cit] ."
"to perform social foraging an animal needs communication capabilities and it gains advantages that can exploit essentially the sensing capabilities of the group, so that the group can gang-up on larger prey, persons can obtain safety from predators while in a group, and in a certain sense the group can forage a kind of intelligence. bfa is based on the foraging performance of escherichia coli (e. coli) bacteria present in the person intestine."
"the first contribution of the scheme is related to use of bacteria foraging algorithm firstly for wsns for enhancing network lifetime of sensor nodes. to validate the algorithm, simulations had been carried out using ns2. simulation results showed better performance of bfa as compared to other clustering protocols like leach,in terms of performance metrics like number of alive nodes and total energy dissipation in the system. bfa provides better lifetime for nodes compared to leach. it is also seen that bfa is able to provide 100% live nodes for maximum duration. leach provides a considerably higher lifetime compared to k-means clustering. in addition to dipping energy dissipation, leach successfully distributes energy-usage among every nodes in the network such that the nodes die arbitrarily and at essentially the same rate. bfa is self organizing and each node works independently. provide efficient and scalable energy reduction."
"in the figure above, the turquoise lines indicate the feature motion vector. figures 8 and 9 show the feature tracking results with and without optimization. in the figures, circles indicate the feature motion vector for the moving vehicle. as shown, it can be seen that this feature has a different motion vector from the surrounding points. we confirmed that the abnormal features indicated by circles were eliminated through optimization. different motion vector from the surrounding points. we confirmed that the abnormal features indicated by circles were eliminated through optimization. figures 10-12 shows the photogrammetric feature optimization result within sequence 09. in the top images, the red point indicates an outlier removed by vision-based optimization, and the orange point indicates an outlier removed by photogrammetry-based optimization. we confirmed that the features for the moving object were removed in two steps. figure 13 and table 3 show the results with or without photogrammetric feature optimization. the rotation error rate decreased by 8.9383 deg/m and the translation error rate decreased by 0.0176%. as shown in figures 10-12, we confirmed that the accuracy was improved by not using dynamic objects as features. figures 10-12 shows the photogrammetric feature optimization result within sequence 09. in the top images, the red point indicates an outlier removed by vision-based optimization, and the orange point indicates an outlier removed by photogrammetry-based optimization. we confirmed that the features for the moving object were removed in two steps. table 3 show the results with or without photogrammetric feature optimization. the rotation error rate decreased by 8.9383 deg/m and the translation error rate decreased by 0.0176%. as shown in figures 10-12, we confirmed that the accuracy was improved by not using dynamic objects as features. figure 13 . estimated trajectory based on whether or not optimization is performed."
"this result corresponds to the fractal dimension of a straight line (as, and thus the amplitude, is 0), which has a of 1. note that"
"finally, we experimented with 00 to 11 (except 01) sequences provided by kitti. figure 15 for ten sequences, the average rotation error was 0.0175 deg/m, the average translation error was 3.5520%, and the processing time per frame was 0.0554 s on average."
"pollution is one of the most important issues in the use of renewable resources which grows increasingly its importance. so that it has forced international organizations like the iea (international energy agency) to take serious decisions to protect the health of the planet and reducing atmospheric pollution [cit] . this can be a great help to the serious forecasting to use renewable energy sources. wind energy is currently the fastest energy from point of view of spread in the world. today, wind energy is growing at a 30 percent growth in world [cit] . in terms of environmental, wind power not only reduces the production of carbon dioxide which is the main cause of greenhouse gas emissions, but it hasn't other pollutions resulting from the use of fossil fuels [cit] . in terms of wind power generation costs, these costs are decreasing day by day so that the cost of each kwh power generated by wind turbine had decreased 20 percent during 5 years. also in terms of size wind turbines come to market with higher powers while these turbines have an output of 5 mw [cit] . the use of wind power plants with variable speed has advantages compared to the fixed speed wind power plants. although wind power plants with a constant velocity, can be connected directly to the network, however, a wider range of energies is covered by the variable speed wind power plants and has less mechanical stress and noise. today, with advances in power electronics, all speeds control is possible and effective [cit] . in turbines with variable speed, in fact, the rotary part of turbine absorbs the mechanical power fluctuations with changes in its speed and output power curve is flatter [cit] . this can help to improve the quality of power. however, wind is a variable quantity and every moment is changing. so the voltage amplitude and output frequency of the wind turbine is changing that variable voltage is not suitable for consumer. so a controller must be designed to be capable to deliver maximum voltage and power with constant amplitude to consumer from turbine. in last research in frequency and voltage control of wind farm based on induction generators the simple proportionalintegral controller (pi) is applied that the pi controller parameters can be adjusted by trial and error method. pi controllers have a gain with fixed values. hence against the wide variation of operating conditions and non-linear factors, are not suitable to dynamic performance. furthermore, the fuzzy controller based on fuzzy logic is one proper approach to control the power system stability so that the controllers are able to adapt themselves to the conditions of the system."
"through equation (4), the model point on the model space at (t − 1) is re-projected onto the image at (t − 2). through equation (5), the distances between the re-projected and the actual image points are calculated on the image space and classified as a threshold 2 as in 2 of figure 5 . as in equations (3) and (6), if both threshold 1 and threshold 2 are satisfied, the feature is extracted as an inlier. also, while checking the number of features, this process is repeated using the previous images."
"the optimisation method serves for improved decision making. for example, for automated distinction between a physiological signal and a pathological one (possibly due to a life-threatening condition), their must not overlap, that is, must not result in false positive or false negative diagnoses. therefore, the of the physiological signal must be as small as possible and the one of the pathological on as large as possible (or vice versa). this is achieved by optimising the amplitude multiplier . the optimal multiplier is neither selected arbitrarily nor subjectively but rather follows engineering optimisation methods, namely, (i) maximising the difference between maximal and minimal of signals or parts thereof;"
"this paper is structured as follows. section 2 describes the material and proposed method. the experimental results are introduced in section 3. then, section 4 shows a discussion of the results describing the strengths and weaknesses. finally, section 5 concludes."
"equation (15) is divided by four as is the average of four solutions (figure 4(c) ). the terms 0.75 −1, 0.75 + −4+1, 0.5 −2, 0.5 + −4+2, 0.25 −3, and 0.25 + −4+3 are required because three-quarters of the preceding blue have to be added to the first blue (which starts at point 4; figure 4 (d)), half of the preceding green has to be added to the first green (which starts at point 3), and a quarter of the preceding yellow has to be added to the first yellow (which starts at point 2, whereas the first red starts at point 2)."
"it is probable that in the local environment, the lives of a population of bacteria changes either step by step (e.g., via consumption of nutrients) or unexpectedly due to some other influence. actions can occur such that all th bacteria in a area are killed or a group is isolated into a new part of the environment. they have the effect of perhaps destroying the chemotactic progress, but they also have the effect of support in chemotaxis, since spreading may place bacteria near good food sources. from a wide perspective, elimination and dispersal are parts of the population-level long-distance motile behavior. design fig 3.1 swimming, tumbling, chemo taxis behaviour of bacteria."
"the smaller the amplitude multiplier, the stronger is the filter effect on the fractal dimension. large multipliers result in a very noisy fractal dimension signal. the filter effect even reveals periodic events of a signal and enhances larger fractal dimensions."
"in this study the membership functions inputs and output are considered same. for inputs and output, the 5-segments triangular membership functions are applied. each input has five membership functions, so the number of base fuzzy rules is 25. the fuzzy rules of fuzzy controller are presented in table 1 . fuzzy rules refer to conception of fuzzy controller inputs and relation of input signals error and its derivation."
"estimation of a platform's pose using a sensor is a technology that has attracted attention in various fields, such as robotics and the automobile industry. typical sensors include the global positioning system (gps), light detection and ranging (lidar), and the camera. the gps is the most popular method, and sub-meter accuracy is possible. however, accurate gps equipment is very expensive, and accuracy is greatly reduced in some environments where satellite signals are blocked, such as downtown or in tunnels [cit] . the method using lidar is very accurate and stable. however, since it requires expensive equipment, its application is limited. the method using a camera has a great advantage that the construction cost is relatively low. this technique is called visual odometry (vo). vo is divided into monocular visual odometry (mvo) and stereo visual odometry (svo). the mvo is slightly cheaper because it uses one camera, but there is a scale uncertainty problem in pose estimation [cit] . it also has relatively unstable image geometry [cit] . svo has an advantage that camera localization and generation of 3d maps around the vehicle can be achieved simultaneously. for both mvo and svo, accuracy and performances are highly dependent on the image processing algorithms applied. in this study, we focus on svo."
"we checked the trajectory result by the path shape. we experimented with the sequence acquired in the area with fewer curves, a large number of curves, and a sharp curve. in figure 14, the red line indicates the ground truth provided by kitti, and the blue line indicates the trajectory estimated by the proposed method. as shown, the trajectory was more sensitive to the number of curve appearances rather than the degree of the curve. for three cases, the rotation error rate was 0.0156 deg/m, the translation error rate 2.8727%, and the processing time per frame was 0.0313 s on average."
"through the comparison of figures 8 and 9, we could see that the features on the moving object were eliminated by the proposed optimization scheme. in order to confirm the effectiveness of the optimization, the experiment was performed with sequence 09. through the comparison of figures 10-12, the features on moving objects were eliminated, and the rotation error rate decreased by 8.9383 deg/m, while the translation error rate decreased by 0.0176%. then, we experimented with three different zones with different numbers and degrees of curves. the rotation error rate was smaller than the translation error rate. also, we observed that the error generally occurred in the curved road rather than the straight road. in table 4, the average processing time per frame was 0.0313 s. for ten sequences provided by kitti, the average rotation error was 0.0175 deg/m, translation error was 3.5520%, and the running time per frame was 0.0554 s. the rotation error tended to decrease with the moving distance, but the translation error tended to increase. through all experiments shown, we confirmed that the proposed feature optimization scheme worked successfully and that real-time processing was possible. our research is ongoing and the performance shown here needs further improvements, particularly compared to known optimal algorithms. for example, soft2 [cit] achieved a rotation error of 0.014 deg/m, a translation error of 0.65%, and a processing time of 0.1 s/frame. nevertheless, our results support our intention of using photogrammetric analysis as an alternative outlier removal method. we showed that the proposed photogrammetric processing could enable successful outlier removal and that real-time processing was feasible even with photogrammetric iterative estimations. it is notable that we adopted the concept of circular matching proposed in cforb [cit] and enhanced its performance by photogrammetric optimizations. cforb achieved a rotation error of 0.0107 deg/m, a translation error of 3.73%, and a processing time of 0.9 s/frame [cit] . cforb performed slightly better in translation errors compared to ours. this is because cforb utilized the time-consuming ransac process repeatedly by 50 loops. one can check this by the large processing time of cforb. however, such extensive ransac-based outlier removal may not bring accurate pose estimation, which is supported by the superior angular estimation performance by our method. the proposed photogrammetric processing method could effectively remove outliers and estimate the pose correctly within a very small processing time."
"where is the length of the yardstick (e.g., in km), σ is the sum of all yardsticks covering the total length, and the exponent characterises the irregularity of the coastline or the frontier. it was mandelbrot [cit] who recognised that this exponent corresponds to a fractal dimension, to be calculated from the gradient of log against log 1/ . rewriting richardson's equation (2) in the form of"
"in the above equation e (t) is a scalar quantity, h(s) is the real transfer function, k is the constant with given sign, is the 1*m vector function of time and v(t) is the measurable vector of 1*m. if is changed as follows:"
"the principle of the mrbc method is to derive the number of boxes covering the signal amplitude change δℎ per time step δ by making the ratio of δℎ to δ integer with a ceiling function and summing up the number of boxes. the principle of higuchi's method is to sum up the change in amplitude δℎ normalised to the time step δ . this principle is still comparable to box-counting methods, however, by using \"boxes\" with a \"noninteger\" height or side length. if a string of data consists only of 0 and 1 (positive and negative), with a device resolution of 1, that is, zero signal with a slight noise, then the number of boxes is zero if two consecutive data are equal. in the mrbc method, relatively small changes in amplitude (with respect to δ ) always deliver a single box."
"the problem for both methods is that a constant signal still has a length-in time direction, but not in amplitude direction. nevertheless, both methods return zero δℎ in such cases. this accounts for zero number of \"boxes\" (integer or non-integer ones) despite the apparent length of the signal. this problem is not only imminent in longer segments with consecutively identical data, but also influences the if only two consecutive data points are of equal value."
"computational and mathematical methods in medicine figure 4 (c); the signal starts at 14.5 s; red, yellow, green, and blue lines start at points 1, 2, 3, and 4, respectively; therefore, 1/4, 1/2, and 3/4 of the preceding yellow, green, and blue segments, respectively, have to be included such that each coloured signal starts at 14.5 s exactly."
"through the comparison of figures 8 and 9, we could see that the features on the moving object were eliminated by the proposed optimization scheme. in order to confirm the effectiveness of the optimization, the experiment was performed with sequence 09. through the comparison of figures 10-12, the features on moving objects were eliminated, and the rotation error rate decreased by 8.9383 deg/m, while the translation error rate decreased by 0.0176%. then, we experimented with three different zones with different numbers and degrees of curves. the rotation error rate was smaller than the translation error rate. also, we observed that the error generally occurred in the curved road rather than the straight road. in table 4, the average processing time per frame was 0.0313 s. for ten sequences provided by kitti, the average rotation error was 0.0175 deg/m, translation error was 3.5520%, and the running time per frame was 0.0554 s. the rotation error tended to decrease with the moving distance, but the translation error tended to increase. through all experiments shown, we confirmed that the proposed feature optimization scheme worked successfully and that real-time processing was possible."
"we performed experiments with ten sequences in the kitti dataset. the specifications of the computer used were windows 10 64 bit, cpu i5-6600 3.30 ghz, ram 16 [cit], microsoft product in the united states. this section shows the results of corresponding point optimization and pose estimation. then, it described the performance we set equation (8) by differentiating partially equation (7) for the unknown. then, we estimated geometric elements through the iterative least squares method."
"this method dates back to felix hausdorff who coined the term \"fractal dimension\" (\"gebrochene dimension, \" [cit] ) by extending carathéodory's [cit] -dimensional measure to noninteger values of . it was reinvented by richardson [cit], for investigating the complexity and ruggedness of coastlines with yardstick methods, who empirically found the following equation:"
"for ten sequences, the average rotation error was 0.0175 deg/m, the average translation error was 3.5520%, and the processing time per frame was 0.0554 s on average."
". they recorded the acceleration signal of a rugby wheelchair at different activities (collisions, pushing, coast down, and zero activity) at 100 hz with three different sensors mounted on the frame of the chair (3g apple iphone, 4g apple ipod touch, and minimax by catapult). for the smart phones, it is evident that the generation number (3g and 4g) was decisive and not whether the device was an iphone or an ipod touch. subsequently, the signal was reduced to 50 hz by taking every other point. both signals were range optimised (figures 14 and 15), the results of which are shown in table 1. the unoptimised of the 100 hz signals (figure 15(b) ) showed pronounced differences between the values. this could be due to the different device resolutions: 3g iphone: 0.018112, minimax: 0.006, and 4g ipod touch: 0.000015 . the high resolution of the ipod touch could be a result of data averaging or filtering. the of the 3g iphone signal is higher, on average, compared to the two other signals. after optimisation, the of the 3g iphone signal dropped and fell within the range of the of other two signals. reducing the frequency to one half merely increases but does not markedly change the trend. furthermore, the optimisation diagram ( figure 14 ) shows an interesting result: the two -spectrum curves intersect, as the of minimal activity at maximal is higher than the one of maximal activity at maximal . after the intersection point, the difference is negative. this does not only explain the high values of higuchi's method (figure 15(d) ) at near-zero values of the acceleration signal, but also does this justify the optimisation procedure: select those parts of the signal which should provide different and increase the differential to a maximum through optimisation. higuchi's method would have been useless in figure 14 and table 1 )."
"where r is the radius of the wind turbine,  is the turbine rotation speed and  is the ratio of tip speed to the wind linear speed."
"the fuzzy controller uses the fuzzy logic rules to obtain control applications. fuzzy rules have been established based on control rules. fuzzy logic systems are not designed based on mathematical models. fuzzy controllers implement the human logic using fuzzy logic that is planned by membership functions, fuzzy rules and membership rules [cit] . in this study, the output voltage is sampled and is compared with the reference voltage level and the generated error signal and its derivative is applied to the controller as the controller inputs."
"case 3 (acoustic signature of a ball impact). fuss [cit] recorded the impact sound of golf balls at 11.025 khz and correlated it to the hardness of the balls. in addition to fft (for determining the power spectrum and the frequencies of the impact sound), the fractal dimension of the impact sound can be calculated. figure 16 shows the range optimisation diagram with the optimal at 0.2. reducing the original amplitude to one-fifth results in a smoother signal, spanning a larger range ( figure 17) . again, the filter effect is apparent."
"at the original signal amplitude suggests a periodic change of only in the 31 hz excitation signal, whereas the 27 and 35 hz excitations signals are far from periodic and rather noisy and chaotic in nature. after applying the optimised value, all signals are clearly periodic."
"as there is no rule for scaling-otherwise sevcik's and higuchi's methods would stand in contrast to each otherany scaling method can be applied, provided that it is neither arbitrary nor subjective. the method introduced in this paper follows an engineering optimisation approach, by maximising the difference between the smallest and largest values across signals to be compared, or across parts of a signal. this is achieved by applying an optimised multiplier to the normalised amplitude of the signals, and this multiplier must be the same in all signals (or parts thereof) to be compared. in most cases, higuchi's method narrows down the difference between the smallest and largest values and therefore is not suitable for decision making. in most cases, the optimal multiplier of the normalised amplitude is smaller than 1 (depending on the original unit if the recorded data); the smaller is, the less noisy the dataset is (obtained from a sliding window method) and the more regular the of periodic signals is due to the filter effect of small -values. the proposed optimisation technique enables the researcher to customise the normalised signal amplitude such that maximal information is obtained, specifically for improved decision making and this with an additional filtering option."
"in order to overcome this problem, a fractal dimension optimisation method is proposed that computes the fractal dimension of a normalised (dimensionless) and modified time series signal with a robust algorithm and a running average method, and maximises the difference between two fractal dimensions, for example, a minimum and a maximum one."
"finally, we experimented with 00 to 11 (except 01) sequences provided by kitti. figure 15 is a graph showing the error occurrence per mileage with sequence 00, where (a) is about rotation and (b) is about translation. figures 16-24 are graphs for sequences 01 to 11."
"in summary (figure 8 ), the mafdm did not show any disadvantage compared to the mrbc method and higuchi's method, delivered comparable results, and is therefore considered sufficiently accurate. the multiplier was set to 1 in order to compare of the unscaled signal to of higuchi's method."
"given the above steps, adaptive controller simulation model is presented in fig. 4 . in figure 4, adaptive control equations and mathematical modeling related to its inputs are presented."
"the basic principle of optimising the amplitude of signals is to find the largest possible difference between of different signals or parts thereof. the optimisation methods are exemplified by, and explained in, four different cases in section 8. the is calculated with the mafdm in all four cases."
"step 2. determination of the fuzzy sets and membership functions in this step, the degree of fuzzy membership functions related to each input and output signals are determined and the fuzzification processing is completed."
"we checked the trajectory result by the path shape. we experimented with the sequence acquired in the area with fewer curves, a large number of curves, and a sharp curve. in figure 14, the red line indicates the ground truth provided by kitti, and the blue line indicates the trajectory estimated by the proposed method. as shown, the trajectory was more sensitive to the number of curve appearances rather than the degree of the curve. for three cases, the rotation error rate was 0.0156 deg/m, the translation error rate 2.8727%, and the processing time per frame was 0.0313 s on average."
"the comparability problem when transforming a signal into a unit square could be overcome when using a normalisation factor that is common to all signals that are to be compared in terms of their fractal dimensions. such normalisation factors could be as follows: (a) the resolution of the recording device ( -axis); (b) standard deviation of the signal; and (c) the window width ( -axis) used for a running average method. the disadvantage when using the resolution of the recording device is that different research teams might use recording devices with different resolution, or a company releases the next generation of recording devices with better resolution. in these cases, the fractal dimensions of signals are no longer comparable. the disadvantage when using the mrbc difference 310 320 330 340 350 360 370 380 390 400 410 420 315 325 335 345 355 365 375 385 395 405 415 avg"
"a signal (figure 4 (a)) with amplitude (dimensionless) is recorded at frequency 0 (dimensionless) over a window width of data points. for modifying the signal, the magnitude of the amplitude is multiplied by the multiplier . this method will be subsequently referred to as modified amplitude fractal dimension method (mafdm)."
(c) figure 14 . (a) estimated result with fewer curves area; (b) the result with a high number of curves area; (c) the result with sharp curve.
"case 1 (assessment of emotional reactions with eeg). fuss and kulish (unpublished data) recorded the eeg of test persons during watching short movies with unexpected scary events (the so-called prank videos or screamers) in order to measure the intensity of the emotional pressure. by quickly plotting the at different, it becomes evident that the correlates with the emotional pressure during the aftermath of startling. it is therefore advisable to suppress the magnitude of the small, by keeping the one of the maximal . the aim is therefore to maximise the range between the highest and the average (figure 12 ). the average is calculated across the entire signal and is, in this example, smaller than the average of the highest and smallest . this can be achieved by maximising the differential of highest, and average whereas keeping the differential of average and smallest as small as possible. alternatively, the ratio of highest minus 1 to average minus 1 can be calculated according to (37). figure 13 shows the at the original signal amplitude, the range optimised and the optimised to the maximal range between the highest and the average . the effect of the latter is that the magnitude of the highest peak is kept, whereas the magnitude of smaller is reduced. a further effect is that the noise level of the decreases, specifically at small . the optimisation method therefore provides an additional filter effect. the ratio increases as decreases and finally asymptotes at small . the optimal is located at the beginning of the asymptotic segment:"
"the corresponding point extraction was carried out in the order of feature extraction and feature matching. first, in feature extraction, we extracted features such as the corner points or edges on the image. we selected representative feature extractors, scale invariant feature transform (sift) [cit], speed-up robust feature (surf) [cit], and features from accelerated segment test (fast) [cit], shi-tomasi [cit], provided by opencv."
"the minimum healthy bacteria die and the other healthier bacteria each split into two bacteria, which are located in the same location. this makes the inhabitants of bacteria constant."
"wireless sensor networks have been widely applied in the field of intelligent agriculture, and experiments were conducted in a modern vegetable planting base in city. whether the irrigation of vegetables was needed is mainly determined by the detection of air humidity. for the comprehensive analysis of the data fusion scheme, it was necessary to analyze the data accuracy, security, and data transmission overhead."
"(2) when the node receives the encryption seeds, it uses the sharing key to decrypt with neighbor nodes and obtain seed . then, the obtained seeds are added with data recorded by themselves, and deg is added by one. the pseudodata obtained is + seed . then it starts step (1) jumped from step (2)."
"however, security is often based on the overhead of the additional overhead, and too much overhead will reduce the efficiency of data fusion mechanism. therefore, building an effective guaranteed security of data fusion, calculation, and transmission is important. meanwhile, reducing resource consumption and prolonging the network life are necessary, too. it can meet the accuracy and privacy requirements of data fusion through mppdf-qos scheme and has lower energy consumption, too."
"wireless sensor network (wsn) data fusion technique has broad application prospects, which is one of the key technologies in the internet of things. data fusion technology is the process of processing multiple copies of data and it gets more effective data to users. but wireless sensor network is vulnerable to various attacks for its open and selforganizing features. the fusion data attacks may cause bad results, which not only makes the wireless sensor network lose the original construction objective but also may cause much more damage. therefore, it is very necessary to provide transmission of security and low energy consumption for sensitive data fusion in wireless sensor network."
"(1) the energy consumption. in the mppdf-qos scheme, nodes communicated only within the same group, in addition to transmitting preprocessing data to the cluster head. however, in the other scheme, all nodes in the cluster could have communication. therefore, the mppdf-qos scheme saved data overhead. figure 3 was comparison of data traffic of the cpda scheme, smart scheme, and mppdf-qos scheme."
"(3) accuracy. in order to reduce the probability of data privacy exposed, a random number would be generated in the course calculating of cluster by nodes. disturbance data would be added in the transmitted data. data perturbation could reduce the probability of data exposed, but it often affected the accuracy of the results. smart needed to send more count data to its neighbor node; thus, it would cause more collision to reduce the accuracy. data collision chance was avoided in mppdf-qos scheme, so its data accuracy was ideal. there are 10 simulations in each numerical value for each epochd and the means of these simulation results are as our result. accuracy analysis was shown in figure 5 ."
"in data fusion of classification privacy protection, in order to reduce the communication, according to the barrel principle of information security, privacy level packet contains a minimum of three node groups. secondly, next packet contains four nodes, and so on. if all nodes in cluster are divided into a group, this situation is defined as the highest level of privacy."
"data fusion is a subset of information fusion, which can be used for processing multiple copies of data or information. more effective and useful data to user can be combined [cit] . for wsn, the data fusion technology can greatly reduce the amount of data transmission to wsn and reduce the data conflict. it can reduce the network congestion and save energy costs effectively, too, which will prolong the lifetime of network."
"(1) nodes firstly verify their values of deg, and determine its size relationship with mindeg. if the value is smaller than mindeg, they only need to receive data within the comd, and it is not necessary to send collusion data. after comd, it goes directly to third step. if the value is larger than mindeg, node is chosen from node, and seed is sent to node . meanwhile, one is added to deg, and seed is subtracted from the node data. seeds are produced by node using a random number, range of data, and calculation of mindeg:"
"in the model of wireless sensor network, a node can establish a communication link with any other node in same clusters through different sharing keys. each cluster of wireless sensor network has n nodes, one of which is a cluster head, and the others are ordinary sensor nodes [cit] . this paper proposes a sequence flow diagram of the calculation procedure of data fusion, which is shown in figure 1 ."
"model. network experiment model was as shown in figure 2, and humidity sensor was am1001, for sensing the humidity in the greenhouse. the fusion node first did security data fusion to the received information, and then it transmitted the information to the base station. the traditional methods of data fusion and mppdf-qos data fusion methods were used to do data fusion of perceived humidity data and the final results of the analysis were as a basis whether to irrigate vegetable."
"a data fusion algorithm of privacy protection based on qos and multilayers hierarchically was put forward, which divided the required privacy protection levels according to the different safety requirements, and hierarchical network models were set up, too. in data fusion process, delay constraint was added to guarantee service quality of qos. this scheme has great advantages in energy consumption and safety. in the next period of time, data accuracy will be the key point to study. wireless sensor network data fusion technology is an important part of the internet of things, and it has wide application prospect in real life. how to provide security transmission to sensitive data fusion in wireless sensor network and how to protect its privacy are important technical support issues for the practical application."
"based on the problems above, a data fusion algorithm of privacy protection based on qos and hierarchical multilayers was put forward, which divided the required privacy protection levels according to different safety requirements and set 2 international journal of distributed sensor networks up hierarchical network models. in data fusion process, delay constraint was added to guarantee service quality of qos, and this would reduce energy consumption overhead. meanwhile, it would guarantee the accuracy of the data and reduce the probability of the whole network information exposed.the main structure of this paper is as follows: the first part gives the current situation at home and abroad; the second part puts forward the system structure of data fusion in wireless sensor networks; the third part puts forward the multilayers hierarchically data fusion approach to privacy protection; the fourth part gives the using method of the qos applied in multilayers hierarchically data to the privacy protection; the fifth part is the experiment of a modern vegetable planting base to validate the effectiveness of the method."
"in wsn, multitiered privacy protection data fusion protocal based on qos (mppdf-qos) includes three kinds of nodes, which are qos node, fusion node, and leaf node. in the traditional technology of data fusion, qos node is the root of data fusion structure tree to get the final result of data fusion. the merging data of fusion node receives from its child nodes and collected data by itself and sends to the parent node. leaf node is responsible for collecting data and sending it to the parent node."
"according to the characteristics of data fusion in wireless sensor network, matlab simulation platform was used to analyze the data. in the region of 50 m * 100 m for vegetable greenhouse, thirty sensor nodes were deployed, and the transmission radius of each node was 15 m. ten vegetable bases like this were selected and tested."
"in the above expression, max depth represents the maximum depth of the tree data fusion in wireless sensor networks and mindeg is the minimum required entry node degrees."
"sensor nodes are divided into ordinary nodes, fusion nodes, and sink nodes, by wsn in data fusion process. effect of data fusion technique was studied through two kinds of methods with theoretical analysis and simulation test. the results show that the minimum energy consumption ratio of using data fusion technology and not using data fusion technology is as follows:"
"in this equation, is the data transmission frequency of data fusion and is the data transmission frequency of no data fusion. meanwhile, is distance between the sensor nodes and the sink node and is the quantity of the data source node. this equation shows that the more data source network node has, the more energy will be saved in data fusion process. meanwhile, if the size of the network is larger, the energy saving of data fusion is much more significant. this shows that the data fusion plays an important role in saving energy of sensor node."
"step: fusion process. in the fusd time, fusion steps of tag algorithm are used to do fusion process from the bottom node to the top node based on the data fusion tree set up in the first step, and data fusion result will be obtained at qos at last. at this time, the node cycle number will be plus one. if loop is smaller than loop, step two will be executed. otherwise, the algorithm will end."
"in this algorithm, we assume that qos is a time delay (tid), and node is assigned a communication delay (comd) and a fusion delay (fusd). all the nodes communicate in comd time, and do transmission and fusion process in the fusd time period:"
"wireless sensor network consists of a large number of sensor nodes, which are deployed in the monitoring region. sensor nodes are divided into three categories. it includes base station, cluster head, and the common sensor nodes. the base station has enough energy and rich resources, and cluster head has less of them. however, energy and resources of ordinary sensor node are the least [cit] ."
"(2) security. in figure 4, privacy protection performances of the mppdf-qos scheme and cpda scheme were compared when there were 30 nodes in a cluster. from the figure, it can seem that it would increase with the increasing rate of node capture in three schemes, and the probability of exposure of data information had been improved. however, it could provide privacy protection based on different privacy levels required in mppdf-qos scheme. even if it was intercepted for the same data as cpda, the probability is that the whole information exposed was much lower than in the cpda."
"our first feature set consists of word n-grams of the tweets. a word n-gram is a sequence of contiguous n words in a text segment, and this feature enables us to represent a document using the union of its terms. we use 1-, 2-, and 3-grams as features."
"esp8266 is typically employed in two ways. one way is to select the working parameters and modes of the chip using the attention (at) command, typically requiring another mcu. when employed in this manner, the esp8266 must communicate with the mcu via the serial port. in contrast, the other way for this chip to be utilized is in secondary development of an official sdk, not only implementing data transmission but also processing sensor data collection. however, the esp8266 chip has a built-in 32-bit central processing unit (cpu) with significant storage and onchip processing capabilities and can be alternatively utilized to control sensors and applications through general-purpose input output (gpio). when the esp8266 chip is the only processor in the system, it can be started directly from a flash drive using minimal memory resources. the esp8266 in this system is set to operate in the second way, which allows it operates independently. since the entire system is designed to minimize the occupied printed circuit board (pcb) space, the circuit design is very simple, along with achieved low-power control."
"the third step was optimizing a membership function (msf), which defines how the total reaction rates of the fm should smoothly change between hsm and lsm. the msf calculates the membership degree (md) of each model based on the total amount of reactive solids (rs). rs is the sum of cellulose and hemicellulose concentrations. the md hsm is calculated with the linear fuzzy rule, presented in equation (10) ."
"data sets 1 and 2 data were first used to test the prediction capacities of the lk and mmk models structures. the optimized models' fitting for these kinetics are presented in figure 2 . both models present the same fitting trend: overestimation of final glucose and xylose concentrations in hsb (data set 1), and with similar predictions for product final concentrations in the lsf (data set 2). however, a deviation from initial sugar concentration occurs. this behavior may indicate that both model sets are being compromised when their parameters are forced to cope with slower reaction rates in the lsf and higher rates in the hsb."
"second generation biofuels are produced from non-food feedstocks, usually lignocellulosic biomass and biowaste [cit] . one well-studied feedstock is sugarcane bagasse. it is the byproduct of sugarcane milling, which extracts sugarcane juice to produce first generation ethanol and edible sugar. a part of the bagasse is currently used as fuel in boilers that cogenerate power for the plant and frequently sell bioelectricity to the grid [cit] . however, an excess of bagasse, which cannot be used in the boilers, can be used to increase ethanol land area productivity and produce additional high value products inside a biorefinery [cit] ."
"throughout the modeling stages, reactions 1, 2 and 4 were considered to be heterogeneous, since they represent the breakdown of cellulose (reactions 1 and 2) and hemicellulose (reaction 4). both substrates are solids that were hydrolyzed into soluble sugars."
"in these equations, α i (g·l −1 ·min −1 ) are reaction rates, where the subscripted \"i\" denotes which reaction is used, the same nomenclature is used in the subsequent variables, k i are kinetic constants (min −1 ), [e i ] are enzyme concentrations (g·l −1 ), [s i ] are substrate concentrations (g·l −1 ) and km i are the michaelis-menten constant (g·l −1 ) for substrate (equation (2)) or for enzyme (equation (3)). k p,i is the competitive inhibition constant of products (g·l −1 ), and [p i ] are product concentrations (g·l −1 )."
"the comparison between models, with 6 and 5 reactions, as well as the parameters and standard errors of the 6 reactions hsm and lsm fitting are presented in the supplementary material."
"values for the training and the test sets for the two set of experiments. the first row of the tables present the results when all the features are used, and the following rows show the results when a specific feature is removed or when a single feature is used. the tables illustrate that the most important feature set is n-grams, and there is a large drop in the evaluation score when that feature is removed (in table 2 ). for all the other feature sets, the drops in the evaluation scores shown in table 3 are very low, meaning that their contribution to the final evaluation score is quite limited. table 3 suggests that the sentiment score feature is the second most useful feature after n-grams. the experiments suggest that the classifier settings (i.e., the parameter values and the class weights) play a more important role in our final approach, as greater deviations from the scores presented can be achieved by fine tuning the parameter values than by adding, removing, or modifying the feature sets. further experimentation is required to identify useful features and to configure existing features to be more effective."
"thus, the previously-described models may not fully describe the system behavior change during the whole long-term process (in fed-batch operations, for instance). yet, adding complexity to the model, by acknowledging other effects during its mathematical formulation, will demand more parameters, which, experience shows, can be very correlated when estimated from the same previous empirical data. as a result, the parameters frequently loose physical meaning."
"the . the 7 by 6 matrix on the right-hand side of the equation is the pseudo-stoichiometric matrix and the vector (α i ) are the reaction rates of reaction 1 to 6, as previously described."
"it is important to notice that the mean squared error (mse) for the lk kinetics was 27.77 g·l −1, using 23 parameters, while the mmk model set, containing 13 parameters, obtained an mse of 8.90 g·l −1 after optimization. in this case, the model that contained almost 60% more parameters showed no improvement over the simpler model."
"to guarantee a smooth transition between models, a takagi-sugeno (ts) fuzzy system was implemented. the ts fuzzy model may be composed by several models, all connected via a set of fuzzy membership rules. in other words, each model represents part of the system behavior and the degree of membership varies within a set of established rules [cit] . fuzzy logic has been shown to improve the estimation of lignocellulosic material in a hydrolysis process with different combinations of substrates in a robust and reliable manner [cit] ."
data sets 1 and 2 data were first used to test the prediction capacities of the lk and mmk models structures. the optimized models' fitting for these kinetics are presented in figure 2 .
"the distinct feeding profiles generated different situations within the reactor, as expected. in data set 1 (hsb), the amount of substrate added in the beginning of the process generated a very high viscosity medium, where there was little visible free water within the reactor before the initial solids liquefaction. as the hydrolysis occurred, the viscosity of the media decreased rapidly. in data set 2 (lsf), the feeding of substrate occurred sparsely enough as not to build a load of solids within the reactor that could cause a significant visual change in the reactive medium, and the amount of visible free water remained constant. in data set 3 (mpf), the initial substrate concentration was not enough to change the medium pseudo-viscosity greatly; however, the subsequent small intervals between feedings modified the medium towards a high-solids state. as the hydrolysis continued, the reactor once again returned to a low solids state. thus, data set 3 is a strong validation test, since the path of the reaction system was very different from the two first data sets. experimental data are presented in the supplementary material."
"firstly, two different sets of mmk parameters were obtained: the hsm set was obtained using data from data set 1 only, and the lsm set from data set 2. the parameters obtained from these models, however, were highly correlated to the degree that most standard errors had the same magnitude of the parameter values (results available in the supplementary material). these results reflect the structure of the models, the number of parameters and the conditions used to fit the parameters, which were based on the meaningful region for the industrial process. this doesn't necessarily represent a problem, since the aim here is to demonstrate the effect that fuzzy modeling can achieve, regardless of whether the parameter sets are the best one. nevertheless, to diminish this issue, and improve subsequent model comparisons, a new set of models was fitted, but at this time reaction 2 was not considered in the system, as this is considered a secondary reaction [cit] and its suppression decouples the production of glucose from two parallel routes with three reactions to a single route with two sequential reactions. this alteration did not severely hinder the model fitting, and thus, a 5 reactions system was used from here on. the modeling results are presented in figure 3 and the parameters resulting from the optimization of hsm and lsm are presented in table 2 . figure 3a shows lsm only, to enable a comparison between this model and the hsm. as described, the data in this figure was not used to fit the lsm. the same occurs in figure 3b, but this time for hsm presence. the very narrow confidence interval obtained in the new fitting, less than 1% in most cases, was mostly due to the f/(n-m) value, which was very small, around 1.0 (2.9 for lsm and 0.9 for the hsm), which was expected since the model variance approaches the noise variance."
"in fact, all these lumping structures and simplifications generated a widely usable and adaptable model. however, some of their underlying assumptions may not hold throughout the entire process, since significant changes in the reaction media occur during the liquefaction of biomass. for instance, the rheology of the reaction medium may change drastically throughout the process [cit] ."
"it is important to notice that the mean squared error (mse) for the lk kinetics was 27.77 g.l −1, using 23 parameters, while the mmk model set, containing 13 parameters, obtained an mse of 8.90 g.l −1 after optimization. in this case, the model that contained almost 60% more parameters showed no improvement over the simpler model."
"it is important to emphasize once again that any model here presented is a strong simplification of the phenomenology behind the saccharification of lignocellulosic materials. several other reactions occur within the reactor. particularly when using new enzymatic complexes, with improved activity of β-glucosidases and the addition of new cellulose oxidizing enzymes such as lytic polysaccharide monooxygenases [cit] . furthermore, different molecules not considered here are generated during the hydrolysis, specially from hemicellulose [cit] ."
"using one model alone for a wide range of solids concentrations was not enough to take into account the reactor medium change during liquefaction. here, the fm was built using the mmk models, since they have fewer parameters (13) than the lk (23), when all equations are considered."
"the fuzzy model extrapolation ability is also superior in the validation data sets. mses for the validation data sets, predicted by fm, hsm and lsm were respectively 14.48 g 2 ·l −2, 37.86 g 2 ·l −2 and 28.12 g 2 ·l −2 ."
"thus, the fm methodology can be used to predict the trajectory of the reactor for operational conditions different from those used to train the algorithm. furthermore, this methodology requires little alterations in software development and can be applied to small datasets."
"muscle fatigue is a problem encountered quite frequently in our daily life. to assist with alleviating muscle fatigue, we designed an adaptive muscle fatigue detection and recovery system based on pwm and esp8266. note that esp8266 is not only a wi-fi adapter, but also a processor that can run independently. our system's main function is to prevent the muscle fatigue through the adoption of adaptive technology. experiments on system performance have indicated that our system has low power consumption and simple hardware composition. furthermore, our system can be effectively employed to prevent muscle fatigue, has simple and stable operations, and well meets our design requirements. in short, the system introduced in this paper exhibits an innovative application of the iot technology in modern medicine, with clear potential for an effective management of muscle fatigue. in our future work, we will explore how to further improve the system performance by integrate additional sensors so as to determine muscle fatigue in a more comprehensive manner."
"that said, this report describes a highly innovative muscle fatigue detection and treatment platform that seamlessly combines the advantages of previous systems in order to integrate semg signal detection and human muscle fatigue release into a comprehensive, user-friendly iot system. in this platform, semg signals are acquired using a myoelectric sensor then sent to an esp8266 where they are converted from analog to digital semg signals. the digital value is then used to control an adaptive vibrating motor to help relieve muscle fatigue. the application (app) charged with monitoring and controlling unit functions can also be used to manually turn vibrations on or off, making the unit more user-friendly."
"our system achieved moderate performance on the semeval sentiment analysis task utilizing very basic settings. the f-scores were particularly low for the negative class, which can be attributed to the class imbalance. considering that the performance of our system was achieved by very basic settings, there is promise of better performance via the utilization of various feature generation and engineering techniques."
"the android application is a mobile device system that provides operations through the user interface (ui). following common practice, the android application development has been divided into ui design and activity function implementation. at first, the developer performs ui design by coding the extensible markup language (xml) file and then uses the layout combined with a multi-layout manager interface to make the design more aesthetically pleasing. finally, according to the layout of the user interface, the main function of the android application is realized by writing the main-activity file."
"high performance liquid chromatography was used to determine glucose and xylose concentrations. samples were filtered (0.2 µm) into autosampler vials. they were analyzed in a shimadzu scl-10a chromatograph, with refraction index detector rid10-a, column animex hpx-87h bio-rad, mobile phase sulfuric acid 5 mm at 0.6 ml·min −1 . sample values were compared to previously established standards."
"the first approach was to evaluate which model type is more suitable to predict experimental data. this was performed since there was no clear consensus, in the literature, on which is the most appropriate. the first type of kinetic model used was based on michaelis-menten kinetics (mmk), where the reactions involving solid substrates (reactions 1, 2, 4) were represented by modified michaelis-menten models with product inhibition (equation (2))."
"a design element of the system is both the wi-fi module and intelligent mobile terminal communicate via tcp/ip protocol. as a result, users can utilize the intelligent mobile terminal for connection to the wireless hotspots provided by the wi-fi module for accessing the muscles' rehabilitation management control via the app."
"as mentioned earlier, the software design of the system (see fig. 6 ) is specifically implemented in two major components: the server and the client. first, the esp8266 is developed into server side platform, and a multi task scheduling mechanism is assumed. the esp8266 collects data from the perceptual layer and sends it to the client. in the process of esp8266 sdk's development, the server's internet protocol (ip) address and port are set, and the service set identifier (ssid) and password are set. data transmission between the client and the server occur through the socket for communication between the two sides. second, an android-based app is used as a client for the system to set an ip address to port the app for receiving data implementing control and detection functions of the mobile device."
"the transducer measures the real world degree of muscle activation through measuring electrical potential. the semg transducer can output both raw and pulse emg signals. the system does not relay the original semg signal rather, it outputs, through the transducer, an amplified, rectified, and integrated pulse signal. signal filtering occurs in the output pulse signal, with both low and high pass included. after signal amplification, the esp8266 performs analog-to-digital conversion, and the digital emg signal can then be intuitively and easily detected. a schematic diagram of the connection among the esp8266, the emg transducer, and the infra-red transducer is shown in fig. 4 ."
"both models present the same fitting trend: overestimation of final glucose and xylose concentrations in hsb (data set 1), and with similar predictions for product final concentrations in the lsf (data set 2). however, a deviation from initial sugar concentration occurs. this behavior may indicate that both model sets are being compromised when their parameters are forced to cope with slower reaction rates in the lsf and higher rates in the hsb."
"prior to testing, identical massagers were used to relax the biceps brachii for 2 min. all testers stood in a standing position with arms naturally hanging by their sides and their right hand holding a 1.5 kg dumbbell. testers then repeated identical wrist flexion and extension exercises 10 times. experimental results represent the average of 10 repeated series of 10 flexions and extensions. experimental testing was performed indoors (see fig. 7 ) at 26°c."
"these results indicate that the use of fuzzy logic to coordinate a consortium of simple models is a powerful methodology when applied to enzymatic saccharification of sugarcane bagasse, an extremely complex reaction system. figure 6 demonstrates the change in reaction rates for the training and validation data sets for the fm and its parent models, hsm and lsm, as a function of the solids concentration."
"notice that different models here are actually the same set of model structures fitted in different conditions. therefore, with a different set of parameter values. the model structure was chosen based on the standalone modeling results. fuzzy modeling was carried out in three steps. firstly, a high solids model (hsm) was fitted using the best reaction rate model type with only data from data set 1 (hsb). second, a low solids model (lsm) was fitted in the same manner, but only using data from data set 2 (lsb). both models generated independent reaction rates (α hsm and α lsm ) for every equation in a same reaction instant. figure 1 illustrates how the fm weighs the two models. with the hsm and lsm membership degrees, the reaction rate for the fm was calculated with equation (11) and it is the output of the takagi-sugeno system [cit] ."
"the complete system is composed of a power supply, an emg transducer, an infra-red transducer, an esp8266 wi-fi module, vibration motors, and a motor drive module. the motor drive module is used to control the vibration motors. the esp8266 wireless wi-fi module is both mcu and wi-fi transmitter/receiver for the system and charged with using minimal power to process sensor data and relay signals."
"in order to better evaluate the anti-jamming index and antijamming performance of our adaptive system in a more realistic environment, we also conducted a group of identical interference tests outdoors (see fig. 8 ). outdoor experiments were performed at 6°c with only minimal wind."
"one of the key steps in the production of bioethanol from lignocellulosic biomass is the saccharification process. using an enzymatic complex to depolymerize pretreated lignocellulosic biomass is the most common method. enzymatic hydrolysis is conducted in mild conditions, and generates higher yields and less inhibitors when compared to other technologies [cit] . however, due to the high cost of the biocatalyst, the operation has to be very well designed in order to find economically feasible operation windows [cit] . moreover, a high concentration of the hydrolyzed product must be reactions was modeled using a langmuir-type isotherm for the enzymes' adsorption, followed by a first order reaction with competitive inhibition by the hydrolysis products (glucose, cellobiose and xylose). the homogeneous reaction was modeled using a michaelis-menten mechanism, with competitive inhibition by glucose and xylose. the authors also introduced a \"substrate reactivity\" parameter, related to the amount of substrate that can be hydrolyzed within the system. this is an empirical parameter that correlates to the degree of polymerization of the lignocellulosic material, and to other transport phenomena hindering interactions. indeed, the degree of polymerization may be considered an important parameter for modeling reaction rates, since different biomasses or pretreatment methods may generate different crystallinity indexes. these directly correlate to the amount of cellulose that is available to the enzymatic complex [cit] . thus, using a \"reactivity\" parameter, or alternatively, evaluating the amount of available substrate, can improve model prediction."
semg signal is a one-dimensional time-series voltage signal superimposed on the surface of the skin by the action potential sequence generated by the motor unit when the muscle reaches the excited state under the control of the nervous system [cit] . the emg signal formation process is shown in fig. 3 .
"moreover, using a fuzzy logic addendum to the kinetic model can improve the model extrapolation capability. a consortium of models, coordinated by the fuzzy logic layer, can increase the overall robustness of the predictions, spanning regions of the state variable regions that were not used to fit each model of the consortium. this approach may be useful for a system that undergoes drastic physical transformations along process time. in short, our purpose is to describe a methodology in which simple models are used to represent the behavior of the enzymatic hydrolysis of sugarcane bagasse in reactors under batch and fed-batch operations. initially, semi-mechanistic michaelis-menten and langmuir-based models were evaluated as a basis to predict batch and fed-batch data. when the utilization of only one model is not accurate enough, a novel modeling methodology can be applied; a consortium of simplified kinetic models coupled to a fuzzy logic membership rule, which combine the responses of the simple standalone models fitted to different conditions."
"our adaptive system was constructed to deliver real time muscle information. iot architecture consists of 3 parts: the perceptual bottom-level layer, the network middle-level layer, and the upper level application layer [cit] . each of these layers is supported by corresponding systems and middleware [cit], as illustrated in fig. 2 . in this system, the perceptual layer consists of an emg transducer that receives the emg signal and an infrared transducer that detects the infra-red signal. the network layer processes and sends the data obtained by the esp8266 to intelligent mobile terminals. at the application layer, the information obtained by the perceptual layer is processed and displayed. the intelligent terminal has control and management functions."
"this work proposes including a model layer that can smoothly drive the model switches. this idea comes from the previously-described fact that the hydrolysis process goes through different stages of liquefaction of the biomass. besides, when fresh material is fed to the reactor, two mechanisms may co-exist: one driven by the enzymatic attack of the solid by the enzymes, and the other reflecting the fact that part of the substrate present in the reactor has already been at least partially hydrolyzed. therefore, only one simple class of model may not be sufficient to describe the reaction kinetics. using computational intelligence, different classes of models can be combined in certain regions where no clear mechanism prevails. instead of just an on-off shifting between kinetic models, their action will be combined."
"an alternative approach is using a simpler structure, with few parameters fitted in different regions. these regions can be a process where all the substrate is added prior to its initiation, generating a high solids content at the beginning, or a more liquefied medium with scattered solids feeding. interpolation between these two models to fit the mid-range solids concentrations may be used. another alternative is using different models in the same process. some authors proposed to separate the process into liquefaction and saccharification reactors/reactions [cit] . this methodology is fairly straightforward for a batch reactor. however, during a fed-batch process liquefaction and saccharification occur simultaneously, since new solid material is added throughout the process (eventually together with new enzymes). this is probably the reason for the lack of fit when model predictions are extrapolated to operational conditions of the reactor different to those used for parameter fitting."
"the power supply offers 3.3 v, 5 v, and 12 v direct current (dc) positive and negative poles. the infra-red transducer detects the infra-red signal to determine whether someone is using the system. the emg transducer determines muscle activation through potential then transmits an emg pulse signal. an esp8266 serves as the sole communicator and processor processing monitored data retrieved from the sensor. one of 3 gears can be employed by the vibration motor depending on the extent of muscle activation represented as a digital value. soft access point (ap) mode is adopted when the esp8266 wi-fi module communicates during which the esp8266 serves as a wireless access point. the mobile device app functions as a station to allow connection to the wi-fi module issued hotspot. our system utilizes the tcp/ip protocol suite (tcp/ip) for transmitting data through socket matching. overall system structure, including the representation and connection of hardware fig. 1 . the overall structure provides a basis for classifying equipment inside the 3-tier architecture of iot."
"the fitting behavior of both models demonstrated that they are not capable of representing such a wide range of components' concentrations. hence, the whole concept of how the modeling is conducted should be questioned. finding a single simplified kinetic model that would fit results for batch and fed-batch operations equally well is a tough task, if not unfeasible in the present situation. path of the reaction system was very different from the two first data sets. experimental data are presented in the supplementary material."
"a summary of the fitting errors for all data sets and models are presented in table 3 . table 3 provides the mse for the models used here. the use of mse to compare models should be done carefully, since the value may mask hidden behaviors in the data. nevertheless, it can be used to help visual comparison among models."
"after the determination of the most suitable model to describe the data obtained in the saccharification experiments, the fitting procedure for the fuzzy model (fm) was carried out. the fm used was a ts fuzzy system [cit] . the fuzzy system is used to interpolate between different models. notice that different models here are actually the same set of model structures fitted in different conditions. therefore, with a different set of parameter values. the model structure was chosen based on the standalone modeling results."
"where cov(θ) is the parameters covariance matrix, x is the linear sensitivity matrix, with derivatives approximated via finite differences, q is the diagonal weight matrix. f is the objective function at the optimum value, n is the number of data sets (data points minus the number of replicates) and m is the number of parameters in the model. the parameters standard error was estimated by the square root of the parameters' matrix main diagonal. all fitting procedures were implemented in scilab 6.0.0. to integrate the state variables, with the generated model parameters, scilab's default ordinary differential equation solver was used, with the option for stiff systems enabled in a computer with an amd fxtm-8350 and 15,7 gb of random access memory running, as the operating system, 64-bit linux mint 18.3."
"to estimate the different models' kinetic parameters and md hsm lower and upper bounds, a levenberg-marquardt algorithm was used. the sum of weighted squares errors (f) was used as the cost function; it was calculated via equation (12) ."
"in large part, this manuscript constitutes an extension of our previously published [cit] initial description of our system. that said, this report describes upgrades to our platform including the addition of an infra-red transducer that can determine when someone is using the system, and also describes our system software in significantly greater detail."
"the fitting behavior of both models demonstrated that they are not capable of representing such a wide range of components' concentrations. hence, the whole concept of how the modeling is conducted should be questioned. finding a single simplified kinetic model that would fit results for batch and fed-batch operations equally well is a tough task, if not unfeasible in the present situation."
"however, modeling such complex interactions can be strenuous and the complexity of the generated model can compromise future studies, such as applications in reactor monitoring and control. the consortium of simple models here proposed is intended to have enough complexity to predict the concentrations of the main compounds, while retaining enough simplicity and flexibility to be applied to engineering problems."
"when the fm is visually compared to standalone models trained with the same data, it is clear that fm gives a much better fitting; this is supported by comparing the mses. fm fitted with data from data sets 1 and 2 obtained a mse (1.29 g 2 .l −2 ) much smaller than a lk model (27.77 g 2 .l −2 ) and mmk model (8.90 g 2 .l −2 ) trained with the same data. of course, the fm is much more complex than lk and mmk models alone. however, the fm helps to predict the behavior of the system for conditions outside the training data, as presented in figure 5 . the fuzzy model extrapolation ability is also superior in the validation data sets. mses for the validation data sets, predicted by fm, hsm and lsm were respectively 14.48 g 2 .l −2, 37.86 g 2 .l −2 and 28.12 g 2 .l −2 ."
"3 and the parameters resulting from the optimization of hsm and lsm are presented in table 3 . figure 3a shows lsm only, to enable a comparison between this model and the hsm. as described, the data in this figure was not used to fit the lsm. the same occurs in figure 3b, the models performed well within their fitting data, however, their prediction capacity with validation data was subpar. the lsm greatly overestimated the concentration of products in the end of data set 1, and the hsm could not describe the reaction rate of data set 2."
"these results indicate that the use of fuzzy logic to coordinate a consortium of simple models is a powerful methodology when applied to enzymatic saccharification of sugarcane bagasse, an extremely complex reaction system. figure 6 demonstrates the change in reaction rates for the training and validation data sets for the fm and its parent models, hsm and lsm, as a function of the solids concentration. the first row in figure 6 displays the reactive solids concentration for each data set. it was calculated as the sum of the concentrations of cellulose and hemicellulose, which were evaluated using the values of the hydrolysis products. reactive solids concentration was the variable used to calculate the membership degree for each parent model used in equation (10) to generate the fuzzy reaction rate. this relationship explains the correlation between the solids concentration and the most pertinent reaction rate. for instance, for data set 1 the reactive solids concentration remains above the first row in figure 6 displays the reactive solids concentration for each data set. it was calculated as the sum of the concentrations of cellulose and hemicellulose, which were evaluated using the values of the hydrolysis products. reactive solids concentration was the variable used to calculate the membership degree for each parent model used in equation (10) to generate the fuzzy reaction rate. this relationship explains the correlation between the solids concentration and the most pertinent reaction rate. for instance, for data set 1 the reactive solids concentration remains above the upper bound of the high model membership function until halfway through the process, thus, up until this point, the fuzzy reaction rate (frr) is equal to the hsm reaction rate. after this point, the solids concentration remains between the thresholds in the second half of the experiment, and thus the fm is an interpolation of both hsm and lsm."
"enzymatic inactivation or inhibition during hydrolysis can occur due to several effects, the most responsible is usually thermal inactivation [cit] . however, in recent research, several authors have been emphasizing the non-productive binding with lignin as a major source of activity loss [cit] . here, reaction 6 represents a generic inactivation of the enzymatic complex. using such a simple mechanism may not fully elucidate how the several effects affect each enzyme during hydrolysis. however, the underlying idea is to use as few parameters as possible, while retaining a robust model."
"following the pioneering work on sentiment analysis by pang et. al. (2002), similar research has been carried out under various umbrella terms such as: semantic orientation [cit], opinion mining [cit], polarity classification [cit], and many more. [cit] utilized machine learning models to predict sentiments in text, and their approach showed that svm classifiers trained using bag-of-words features produced promising results. similar approaches have been applied to texts of various granularities-documents, sentences, and phrases."
"it has been shown in past research that certain terms, because of their prior polarities, play important roles in determining the polarities of sentences [cit] . certain adjectives, and sometimes nouns and verbs, or their synonyms, are almost invariably associated with positive or non-positive polarities. for each adjective, noun or verb in a tweet, we use wordnet 3 to identify the synonyms of that term and add the synonymous terms as features."
"the most integrated wi-fi chip, esp8266, is based on the library for www access in perl (lwp) protocol, and offers three modes from which to select: ap, station (sta), and ap + sta. in the ap mode, the wi-fi module is a wireless access point that functions as a router, and the chip is the creator of a wireless network. in the sta mode, it connects to the terminal as an ap and does not accept wireless access itself. ap + sta is owned by both modes and can be used as an ap or a station to connect to other hotspots. in our system, the esp8266 is utilized in ap mode, and its operation diagram is shown in fig. 5 ."
α fuzzy are the reaction rates for each hydrolysis reaction. the msf optimization stage was performed to determine where lower and upper bounds of the md hsm should be placed. these parameters were optimized via levenberg-marquardt algorithm using the data from data sets 1 and 2. the optimization procedure is described in section 2.4.4.
"bioethanol has been a reliable and extensive energy source used for several decades, with several countries committing to further expanding the utilization of this fuel in their energetic matrix, in order to comply to c footprint reduction targets. the technology for bioethanol production from sugar cane juice or corn is well established, but the industrial production of second generation (2g) ethanol has still not been consolidated, despite the fact that 2g ethanol is an interesting alternative which would reduce land use [cit] ."
"enzymatic complex celic ctec 2 was donated by novozymes latin america (araucária, paraná, brazil). the activity of the enzyme complex was 203 fpu.ml −1 and its protein concentration was 75 mg protein .ml −1 ."
"stanford grammatical dependencies have been designed with a view to provide a simple and usable analysis of the grammatical structure of a sentence by people who are not (computational) linguists [cit] . in this schema, each relation between words of a sentence are encoded as binary predicates between two words. a semantic interpretation which uses the notions of traditional grammar are attached to the relations to facilitate their comprehension. for example, from the sentence i love the banner, we expect in the analysis the relations nsubj(love, i), det(banner, the), dobj(love, banner) denoting subject, determinant and direct object roles, respectively. based on previous research [cit], our intuition is that dependency relationships maybe useful for polarity classification. we used the stanford parser integrated in the stanford corenlp 3.4 suite, 5 and computed collapsed and propagated dependency trees for each tweet."
"the third step was optimizing a membership function (msf), which defines how the total reaction rates of the fm should smoothly change between hsm and lsm. the msf calculates the membership degree (md) of each model based on the total amount of reactive solids (rs). rs is the sum of cellulose and hemicellulose concentrations. the md hsm is calculated with the linear fuzzy rule, presented in equation (10) . saccharification experiments, the fitting procedure for the fuzzy model (fm) was carried out. the fm used was a ts fuzzy system [cit] . the fuzzy system is used to interpolate between different models."
"reducing the number of reactions to fit the standalone (lk and mmk) models clearly would not improve their fitting results (see supplementary material), since the simplest model is embedded in the complex one. nevertheless, lk and mmk models were refitted using only 5 reactions. the procedure did not alter the results greatly, and thus the conclusions from the standalone model fittings were maintained. the mse for the lk kinetics was 27.98 g 2 ·l −2, while the mmk model set obtained a mse of 14.91 g 2 ·l −2 ."
"muscle fatigue routinely occurs in daily life and as the detection and evaluation of musclar fatigue clearly has practical significance has become a highly active area of research. [cit], d. tkach [cit] examined time domain stability with surface electromyography (semg) pattern recognition and successfully extracted frequency domains and features from these time domain analyses which facilitated more efficient evaluation of muscle fatigue. that same year, wang kui [cit] similarly utilized a combination of frequency domain, time domain, and time-frequency allowing a more comprehensive evaluation of muscle fatigue. [cit], wang fenjuan [cit] described a muscle fatigue detection system integrating an arm microcontroller core combined with a agcl surface electrode allowing for the effective identification of semg signal s through both digital and analog filters. notably, this constituted the first time a micro-controller unit (mcu) was incorporated into a muscle fatigue detection system; that said, although the mcu provided a suitable baseline signal for research, its hardware was complex and operation cumbersome, and ultimately proving unsuitable for clinical application. also [cit], wan sha [cit] created a multi-channel semg detection system utilizing labview. while an improvement over exisiting technologies since the semg signal could be acquired in real time and visualized on a personal computer (pc), labview carried significant delays when switching between channels. [cit], zhu anyang [cit] generated a stm32-based semg acquisition system in which the semg signal is transmitted through a usb interface to the host computer then analyzed and processed directly. importantly, however, while each of these [cit] semg signal detection systems were innovative in the time, none of these have intelligent terminals, significantly hindering their usability."
"we derive a set of lexical, semantic, and distributional features from the training data. a brief description of each feature and preprocessing technique is described below."
"our experimental design was as follows. ten groups of randomly aged men (10 men in each group, 100 men in total) were examined. the average physical condition of the ten groups is shown in table 2 . no participants reported any musculoskeletal or neurological diseases. the right biceps brachii was examined by placing one emg transducer electrode on skin above the muscle center and another along the extended direction of the same biceps brachii. a reference electrode was also placed in a position without underlying muscle."
"considering the nature of the user posts in twitter, it is common to observe rarely occurring or unseen tokens in the test data. in order to address this issue, we use embedding cluster features introduced in . we categorize the similar tokens into clusters, and as a result, each token in the corpus has an associated cluster number. therefore, every tweet is represented with a set of cluster numbers, with similar tokens having the same cluster number. the word clusters are generated based on k-means clustering of the token representative vectors (known as embeddings). the embeddings are meaningful real-valued vectors of configurable dimensions (usually, 150 to 500 dimensions) learned from large volumes of unlabeled sentences. we generate 150-dimensional vectors using the word2vec tool. 6 . our corpus includes a large number of unlabeled sentences from the provided train/test tweets plus an additional 860,000 in-house set of collected tweets about user opinions on medications. the vector and cluster dimensions are selected based on extrinsic evaluation of different configurations for the embedding clusters, generated from the same in-house twitter corpus in our previous study. word2vec learns the embeddings by training a neural network-based language model, and mapping tokens from similar contexts into vectors that can then be clustered using vector similarity techniques. more information about generating the embeddings can be found in the related papers [cit] ."
"fuzzy modeling was carried out in three steps. firstly, a high solids model (hsm) was fitted using the best reaction rate model type with only data from data set 1 (hsb). second, a low solids model (lsm) was fitted in the same manner, but only using data from data set 2 (lsb). both models generated independent reaction rates (α hsm and α lsm ) for every equation in a same reaction instant. figure 1 illustrates how the fm weighs the two models."
"to obtain the standard errors of the optimized parameters, equation (13) was used [cit] . the equation is based on the linearization of the model in relation to the parameters at their optimum values."
"in data set 3, the fm starts in the lsm rate and quickly changes to the hsm rate with the close feeding time periods. as the solids are liquefied, the model becomes a halfway interpolation of the two precursor models, and continues to approach the lsm rate at the end of the process. this improves validation data prediction greatly, as presented in figure 5 . this is a very interesting capability, as the model can be adapted quickly to situations not present in the training data using only knowledge of apparent reactive solids concentrations."
"our experiments clearly indicate that environmental factors have little effect on the accuracy and response time of our system. combined with tables 3 and 4, it is apparent that response time was stable between 1 and 2 s. as indicated through the consistent change of digital value, muscle fatigue was clearly diminished when employing this system."
"the internet of things (iot) refers to interconnecting computing devices embedded in everyday objects and enabling them to transfer and receive data over the internet. the iot is quickly spreading to virtually all aspects of people's lives including the integration of the iot into medical treatment which is highly beneficial to both doctors and patients and increasing everyday [cit] . muscular fatigue is defined as a reduction in a muscle's force-generating capacity due to exercise [cit] . muscle fatigue due to repetition incurs a loss of functional ability [cit] and, as evidenced in sports, directly relates to muscular strength [cit] . massages can help alleviate muscle fatigue [cit] through improving local blood circulation and nutrition and helping to accelerate lactic acid discharge."
"we perform standard preprocessing such as tokenization, lowercasing and stemming of all the terms using the porter stemmer 2 [cit] . our preliminary investigations suggested that stop words can play a positive effect on classifier performances by their presence in word 2-grams and 3-grams; so, we do not remove stop words from the texts."
"in this work a fuzzy model (fm) for reaction rates was proposed to describe the enzymatic saccharification of sugarcane bagasse. simple models, such as those based on michaelis-menten kinetics (mmk) fit well to the data for a particular feeding policy. however, the same model could not be used to accurately predict the process behavior when the feeding policy was changed. the use of a fuzzy rule to weight between two simple models, each one fitted for different solids concentrations, has greatly improved the bioreactor trajectory prediction for different operation modes. this approach seems to be a good trade-off between the phenomenological and empirical-driver models, and was able to describe a very complex system. of course, the methodology can be applied to different systems, for example, the enzymatic liquefaction of other lignocellulosic materials."
"twitter, for example, has over 645,750,000 users and grows by an estimated 135,000 users every day, generating 9,100 tweets per second 1 ). users often express their views and emotions regarding a range of topics on social media platforms. as such, social media has become a crucial resource for obtaining information directly from end-users, and data from social media has been utilized for a variety of tasks ranging from personalized marketing to public health monitoring. while the benefits of using a resource such as twitter include large volumes of data and direct access to enduser sentiments, there are several obstacles associated with the use of social media data. these include the use of non-standard terminologies, misspellings, short and ambiguous posts, and data imbalance, to name a few."
"for data set 2, the experiment begins with a small solids concentration, and thus, the frr is equivalent to the lsm rate. the subsequent solids addition does not amount to a solids concentration that can cause the frr to deviate from the lsm reaction rate."
"the models were then used in the fuzzy optimization, as reaction rates generators. the lower and upper bounds of the fm (see figure 1b) were then optimized. after optimization, the low and high thresholds for the linear fuzzy rule were 75.35 g·l −1 and 61.52 g·l −1 respectively. this is an interesting result, since the threshold of the upper bond agrees with the empirical observations during the experiments: visually, around this load of solids, the reactor seems to change its behavior from an almost semisolid process to one with high free water content. the resulting fitting of the fm, alongside the hsm and lsm for comparison, are presented in figure 4 ."
"analysis of table 3 demonstrates several interesting aspects of the fuzzy modeling methodology, especially its flexibility under different reactor operation policies. this is presented in figure 4, where a poor adherence from the hsm to the data in data set 2 and from the lsm to the data in data set 1 is clear. the fuzzy model, through its model interpolation, generates a much better fit in both data sets simultaneously using only one model. and upper bounds of the fm (see figure 1b) figure 4 . a summary of the fitting errors for all data sets and models are presented in table 4 . when the fm is visually compared to standalone models trained with the same data, it is clear that fm gives a much better fitting; this is supported by comparing the mses. fm fitted with data from data sets 1 and 2 obtained a mse (1.29 g 2 ·l −2 ) much smaller than a lk model (27.77 g 2 ·l −2 ) and mmk model (8.90 g 2 ·l −2 ) trained with the same data. of course, the fm is much more complex than lk and mmk models alone. however, the fm helps to predict the behavior of the system for conditions outside the training data, as presented in figure 5 ."
"our system has 2 operating modes: a fully automatic mode and a semi-automatic mode. users can select a mode via the intelligent mobile terminal. when in the fully automatic mode, the system works according to a preset program. vibration motor vibration strength changes in sync with indiviual changes of the electromyographic signal. vibration strength of the motor is calculated by the duty of pwm [cit], and the following table 1 gives the corresponding relationship. in semiautomatic mode, users can control, through the intelligent mobile terminal, if the vibration motor vibrates or not. when the \"close\" button is selected on the app side, the vibration motor stops working immediately and no longer changes with the emg signal."
"the samples were analyzed for glucose and xylose in accordance to previously published recommendations [cit] . method validation, with matrix effects evaluation, has been performed by the authors' research group (data not published). matrix effects were negligible."
"to describe the lignocellulosic material hydrolysis, six reactions were considered. in the reaction scheme, γ are the pseudo-stoichiometric mass relations between substrates and products for each reaction."
"to assess the contribution of each feature towards the final score, we performed leave-one-out feature and single feature experiments. tables 3 and 2 show the"
"the application layer has two major sets of functionality. first, the application layer is responsible for processing and displaying the information obtained from the perceptual layer. second, the application layer, realized through the intelligent terminal, is responsible for the system's control and management functions. the android terminal consists of a server and a client [cit], and was implemented using the java language and the development environment (ide) of android studio [cit] . the software design primarily focused upon network communication, data transmission, and the man-machine interface [cit] . during the process of software development, the full degree of integration between software design and hardware was comprehensively considered [cit], and the features of reliability and modularity fully employed to enhance the intelligence of the overall system [cit] ."
"reaction 5 is included, despite lignin being inert. thus, this \"reaction\" just reflects the accumulation of lignin in the reactor when it operates in fed-batch mode."
"to achieve network communication, the mina framework was incorporated. apache mina server constitutes a communication framework based on the tcp/ip, udp/ip protocol suite (udp/ip) stacks. it not only assists developers with quickly creating high-performance, expandable network communication applications, but it also provides an event driven, asynchronous operation model. the mina framework also solves common problems including, but not limited to, network management when first contacted by a client, the server first configures ap parameters, like ip addresses and server ports, through the tcp/ip protocol. these are then written into the wi-fi protocol to establish a connection with the client, and client and server socket channels simultaneously generated to ensure data transmission. following this, the server will listen to the client's request and call the accept method to finally establish the connection. after the connection with the client is established successfully, data transmission will be performed using the transceiver function according to the requirements of the control flow."
"delta-recv: mpi delta recv turns on the page protection for the receive buffer, creates a shadow buffer of the same size, and then lets the receiver task continue its execution. when the receiver accesses the message data, it incurs a page fault if the page containing the data is not yet received. the page fault handler calls delta wait data and blocks until the page (or pages if the delta size is more than a page) is received. the handler copies the pages from the shadow to the receive buffer, unprotects them, and resumes the receiver. the receiver can access data in any order in the receiver buffer, not just in sequential order. furthermore, there is no need for a wait operation to follow the delta receive. any message data not accessed are not needed (by the program)."
"kernel tests: we use a message size of 400kb in these tests. the increment size is 16kb. we will evaluate different message and delta sizes later. with the os triggering, pair shows 45% performance improvement. the base version of ring is usually considered highly efficient since the communication fully overlaps when each task sends and receives at the same time. still, delta send-recv improves the performance by 13%, 11%, 14%, 27%, and 11% for 2, 4, 8, 16, and 32 tasks respectively. the baseline execution time of ring is 2.19s, 2.3s, 2.27s, and 2.31s for 2, 4, 8, and 16 tasks, but it increases to 17.8s when there are 32 tasks. thus, the low improvement at 32 tasks does not show a limitation of delta send-recv. it reflects the limitation in our machine performance, such as the saturation of network bandwidth. we observe a similar drop at 32 tasks in all the tests on the pc cluster."
"computation and communication overlapping is a basic method in optimizing distributed programs. a straightforward way is non-blocking send and receive, which overlaps communication with unrelated computation. for dependent computation that either produces the outgoing message or consumes the incoming data, overlapping still can be done in a finer grain, known as pipelining. pipelining cannot be easily automated for complex code because it requires exact send-receive pairing to perform matching transformations in both the sender and the receiver code. manual transformation, on the other hand, makes code harder to understand and maintain. in addition, a static solution is not sufficient if the send-receive relation is not completely known at compile time."
"a compiler can insert delta send and wait calls directly instead of leveraging page protection. the sender can write to the message data in any order, and there is no paging overhead."
"in the general case, the message is managed and communicated as a set of data sub-ranges. each call of delta send/wait data adds a sub-range. dynamic messaging allows sub-ranges to be sent and received out of order. the bookkeeping of these data sub-ranges can help to detect misuse of delta send where a program may write to a memory location that has already been sent. the run-time support can abort the program and notify the user."
we show the interface by the use of the communication primitives in the context of computation. the computation has two parts: the dependent computation that produces and consumes the communicated data and the independent computation that does not use the communicated data.
"prior work has used loop strip-mining and tiling to enable senderreceiver pipelining [cit] . automatic transformation requires precise send-receive matching, a difficult problem for explicitly parallel code [cit] . in comparison, dynamic pipelining does not need static send-receive matching."
"on the receiver side, the program calls delta wait data before it uses a piece of message data. the pseudo code is also shown in figure 3 . the function keeps waiting for the next increment until the waited data has arrived."
"we have presented the delta send-recv interface and the two faceted implementation by os triggering and compiler annotation. an mpi program can create pipelines dynamically between two or more tasks. we have evaluated the design using different communication topology, computation intensity, message and increment sizes, real benchmark kernels, different machines, networks, and mpi libraries. the results show that the best size is as small as 8kb for the message and 1kb for the delta increment. the improvement is up to 2.9 times for 16-task mpi reduce and 7.9 times for 16-task cascade."
"virtual memory support: virtual memory support has been used for incremental receive for messages in mpi [cit] and bulk transfers in upc [cit] . early release changes only one side of the communication. our work adds incremental send to create dynamic pipelining. delta receive, when using os triggering, is similar to early release, except for the use of multi-page increments to amortize the paging overhead. sbllmalloc, a user-level memory allocator, uses virtual memory support to make mpi processes share read-only data (when they are executed on a single machine) and consequently reduces the memory consumption [cit] ."
"mpi library design: modern mpi libraries support non-blocking send-recv and also non-blocking collectives in libnbc [cit] and the upcoming mpi-3 standard [cit] . nonblocking communication yields significant benefits in largescale production mpi code [cit] . it is generally useful for messages of all sizes. delta send-recv is complementary, and the implementation is effective for mainly large messages."
"although not shown in figure 5, we have evaluated some other combinations of larger delta sizes and message sizes. the general trend stays the same. the only difference is the performance drops to almost no speedup once the delta size reaches 32 pages. after looking into mpich2 nemesis channel implementation, we find that a threshold on the cluster is mpidi ch3 eager max msg size, which defines the switching point from eager protocol to rendezvous protocol, and its default value is 128kb. when a message is larger than this threshold, the communication will use the rendezvous protocol, and now in order to start a data transfer, the sender needs to wait for the acknowledgment from the receiver. as a result, when the delta size is equal to or larger than 32 pages, the latency almost doubles, and the effect of dynamic pipelining is diminished."
"since there is no need to transform send-recv pairs in tandem, a user can optimize sender and receiver code separately. we call it a one-sided transformation. a user can transform an mpi send without knowing all the possible matching receives. as a result, one-sided transformation may improve performance more than previously possible or reduce the amount of programming time."
"send-receive de-coupling and one-sided transformation: delta-send can be implemented in a way that the message can be properly received by any type of receive as non-blocking sends can. similarly, delta-recv can support messages from any type of sends."
"delta-send: mpi delta send begin places the send buffer under page protection except for the first delta increment. a delta is a group of consecutive memory pages. the operation installs a custom page fault handler. when a write to the send buffer triggers a page fault, it invokes the signal handler. since the fault address signals the completion of the previous delta, it takes the address range and calls delta send data (shown in figure 3 ), which initiates dynamic messaging. the handler unprotects the pages in the next delta, and resumes the execution of the sender. in this fashion, the page fault handler sends all deltas except for the last one, which is sent out when the sender reaches mpi delta send end. finally, mpi delta wait waits for all (non-blocking) delta sends to finish. to be correct, deltasend requires sequential write, which has to be guaranteed by the user or compiler analysis."
"the send/wait functions in figure 3 form the core of the run-time support. the interface calls are implemented based on them. the delta send/receive calls are used to initialize the internal parameters needed by the send/wait. it is possible that a sender does not write the entire message. mpi delta send end is used to inform the run-time system that there will be no more calls to delta send data. the runtime system then sends all the remaining data, if any, in one (last) message. mpi delta wait is a blocking operation to ensure that all delta sends are finished. delta send-recv may be implemented inside an mpi library or as our prototype be built as a user-level library over the standard mpi interface. like standard mpi, its interface can be used by c/c++/fortran programs. next we describe two solutions for access monitoring."
"we let the kernel program compute a trigonometric operation, in particular, sin(i) * sin(i) + cos(i) * cos(i) for each element of the integer array. we run each test 100 times and take the average as the result. the performance variance is negligible, thus we don't show it on our graphs."
"the test reduce is an mpi collective and similar to mpi bcast, mpi scatter, and mpi scatterv in that all implement one-to-many communication using a logical tree structure. an mpi library can internally pipeline the communication in a collective operation, as it was done in mpich [cit] . this test shows the effect of overlapping computation with communication, which cannot be implemented by current mpi collectives. in theory, the speedup increases as a logarithmic function of the number of tasks. on the cluster, we observe increasing speedups of 1.4, 1.9, 2.4, 2.8, and finally a drop to 1.1 due to insufficient bandwidth."
"for example, delta send-recv supports efficient coarsegrained reduce. pipeline cascading reduces the cost by o(log k) for k tasks in a tree topology, compared to using mpi non-blocking send-recv."
"compiler annotation is equally or more efficient. pair shows 70% improvement. ring is improved by 9%, 12%, 13%, 23%, and 7%, which are comparable to os triggering. reduce also shows similar speedups: 1.4, 2.0, 2.5, 2.9, and 1.1. cascade shows higher improvements: 1.8, 3.2, 5.4, 7.9, and 3.5 times respectively."
"delaware tests: not all task numbers are permitted in the hycom and lu kernels. with the os triggering, the improvement for hycom is 10% for 4 and 8 tasks, 62% for 16 tasks, but a slowdown of 46% for 32 tasks due to the adverse effect of bandwidth contention. the improvement for lu is 68% for 16 tasks and 42% for 32 tasks. the improvements for mg are 2%, 3%, 4%, 5%, and 4%. the reason for the marginal improvement is that only 2 out of 6 communications can use delta send-recv."
"test suite: we use kernel and simplified application benchmarks. the kernel tests are as follows: 1) pair: the sender computes and sends a data array to the receiver, which performs identical computation (and compares the two results). 2) cascade: p tasks connected as p − 1 pairs. 3) ring: a virtual ring of p tasks, each computes and sends data to the right, gets from the left, and repeats. 4) array reduce: a virtual tree of p tasks, each gets data from each child (if any), adds them and its own, and forwards the result to its parent. equivalent to n mpi reduces, where n is the array size. as the base line, we use blocking send-recv in all kernel tests. for instance, in pair, after the computation, the sender calls mpi send to pass the data array to the receiver, and the receiver calls mpi recv to get the data before its computation starts. the only exception is ring, which needs non-blocking send-recv to avoid deadlock."
"cascading: delta send-recv may be chained together between more than two tasks. if we extend the example in figure 2 such that when it finishes processing, the second task sends the data to a third task. then the second and the third tasks form a pipeline in the same fashion as the first two tasks do. with enough computation, all three tasks will execute in parallel after an initial period. the benefit of chaining is important for mpi aggregate communication such as broadcast and reduce. such communication is often carried out on a tree topology so it takes o(log n) steps to reach n tasks. cascading happens between all tasks on the same path from the root."
"the implementation has two parts. the first is access monitoring. the second is dynamic messaging. in the general case, the message data can be produced and consumed in any order. to simplify the presentation, we first show a limited design which assumes the sequential order and then discuss the extensions needed to remove the limitation. simplified algorithms for the delta send/receive. access monitoring is done by calling these two functions, which instigates dynamic messaging."
"the run-time effect is illustrated by an example in figure 2 . the left-side figure shows the effect of non-blocking communication, which overlaps the independent computation and communication. the right-side figure shows that delta send and receive improves processor utilization by pipeline parallelism and network utilization by incremental communication."
"another benefit is adaptive control. for example, based on the amount of computation, the size of message, and the running environment, delta-send-recv can dynamically choose different increment sizes to maximize performance."
mpi collectives such as irregular all-gather (mpi allgatherv) make use of communication pipelining [cit] . it requires no changes to the user code but the effect is limited to a single mpi operation. delta send-recv requires code changes but extends the benefit of pipelining beyond a single mpi call to include the user computation.
"using compiler annotation, the performance improvements are similar or greater, as in the kernel tests. the largest enhancement happens at the 16-task run of hycom, 16-task ! and 32-task runs of lu. the improvements are 90%, 116%, and 74%, higher than 62%, 68%, and 42% by os triggering. we used different delta sizes and observed similar results except for lu. lu computes on a matrix and communicates the boundary data. the computation-to-communication ratio is high. we obtained a 2.52x speedup for 16 tasks when using 1kb delta versus 2.16x speedup in figure 4 ."
"delta-send: the compiler analyzes the computation between mpi delta send begin and mpi delta wait to identify the dependent computation and annotate the write statements by calling delta send data. standard dependence analysis can be used [cit] . if the write to a send buffer happens in a tight innermost loop, direct annotation will incur a high run-time cost. the compiler can strip-mine the loop and insert the annotation in the outer loop to amortize the cost."
"in terms of programmability, delta send-recv enables pipelining without having to reorganize the computation code. it supports variable-size communication, where the size is unknown until the complete message is generated. in addition, it can be implemented using virtual-memory support which does not need access to program source code. it allows communication optimization for interpreted languages such as matlab and r, whose use of separately compiled or dynamically loaded libraries makes manual transformation impractical."
"the delaware tests were created to evaluate compiler techniques for overlapping communication with independent computation. in figure 6, we compare the 16-task performance of their optimized code (tested on our machine) with delta send-recv."
"our current delta send data function uses a range tree data structure to merge data ranges. if a new range is merged with existing ones, and the combined size exceeds the threshold, the function calls mpi isend to send out the data chunk."
"the test platform is an 1gb-ethernet switched homogeneous pc cluster with 40 nodes. each node has two intel xeon 3.2ghz cpu and 6gb memory, installed with fedora 15 and gcc 4.6.1.. we use mpich2 1.4.1 [cit] as the underlying mpi library. we also tested on an older 32-processor ibm p690 multiprocessor machine and observed similar improvements but will not include ibm machine results for lack of space. table i shows the performance characteristics of the pc cluster. communicating a 400kb message is 2.45 times faster than communicating 100 4kb messages, showing the overhead of incremental communication. in os triggering, a page fault costs about 9.9 microseconds. in addition, the table shows the time per page for the trigonometric computation used in the kernel tests. our test machine represents a commodity cluster rather than an up-to-date hpc system, however, delta send-recv should be applicable to newer systems as long as the communication time is not negligible. figure 4 shows the improvement using delta send-recv over mpi send and receive when running 2 to 32 tasks."
"we should note that their code was not optimized for our machines, so the performance may not represent the full capability of their techniques. in addition, they can improve communication when there is only independent computation but delta send-recv cannot. this happens in one of the delaware tests that we do not include here."
"the purpose of these primitives is to enable pipelining by the sender, where a chunk, i.e. a delta, of message is produced and sent while the next chunk is being computed. the production of the message data may be sequential or not depending on the implementation which we will describe next."
"pipelining is standard in compiler parallelized code, initially through message strip-mining [cit] and later more systematically through give-n-take to place send as early as possible and the matching receive as late as possible [cit] . the give-n-take framework uses a system of dataflow equations similar to those used to solve for lazy code motion in scalar compiler optimization [cit] . it has been extended to consider the resource constraint [cit] . these techniques are not designed for explicitly parallel code."
"as the sender computes the message data, it calls delta send data when it finishes computing a piece of the data. the pseudo code is shown in figure 3 . the function waits until the finished pieces amount to a threshold, deltasize, and sends these data in a message. the code shows the connection between access monitoring, done by calling delta send data, and dynamic messaging, done by executing the function. the message size can be determined and adjusted at run time."
"implementation: our system is implemented as a userlevel library over the standard mpi interface. it is written in c and provides an interface for use by c/c++/fortran programs. to optimize, we manually insert delta send-recv functions in a way that can be automated with the compiler support. the insertion of delta send end and wait calls is a matter of replacing the original send and wait calls. delta send begin and delta receive require knowing the start of the dependent computation. loop unrolling is need by compiler annotation (but not by os triggering). for regular loop code, such analysis and transformation can be implemented with existing techniques. the run-time library uses a pre-set delta size."
"in all three cases, pipelining communication with dependent computation is more profitable than overlapping with independent computation. the improvements by the two delta send-recv implementations and their code are 56%, 90% and 11% for hycom, 68%, 116% and 28% for lu, and 6%, 5% and -2% for mg."
"on the sender side, as in figure 1 (a) shows, mpi delta send begin is called before the dependent computation that produces the sent message. it has the same parameters as a normal mpi nonblocking send, which includes 7 parameters including the buffer address, size, data type, destination task, tag, mpi communicator, and request handle. mpi delta send end is called after the dependent computation. it has just one parameter, which is the request handle. mpi delta wait is called after the independent computation, taking the request handle and returning a status pointer. mpi delta wait implies mpi delta send end, so the latter can be omitted if there is no independent computation before the wait."
"in the three tests, we use array padding to avoid mixing message array with other data on the same page. there are two other kernels in the delaware suite. one is the nas lu btls pre-conditioning loop. the kernel has no dependent computation, so delta send-recv has no benefit. the other is a simplified hycom xcaget kernel. the communication is mostly redundant, so it is unclear what to optimize."
"comparison with message splitting: in current mpi libraries such as openmpi, a non-blocking send, if it sends a large message, is divided into \"fragments\" to avoid flooding the network [cit] . the library-level message splitting does not interleave communication with dependent computation, but delta send-recv does. in implementation, delta send-recv may adjust the increment size based on not just the network but also the program computation."
"cascade shows the largest speedups due to pipeline chaining, 1.8, 3.0, 4.7, 6.7, and 3.3. this test shows the best possible case for delta send-recv, in which the improvements increase linearly with the number of tasks in theory."
"delta-recv: similar compiler analysis and loop transformation can be performed on the receiver code following mpi delta recv and insert calls to delta wait data (shown in figure 3) . initially, the receiver posts non-blocking receives for the maximal number of delta messages and each of them receives data into its own shadow buffer. since the actual delta size may be larger, and the specified and actual size of the communication may differ, the receiver cancels all remaining receives by calling mpi cancel after the last delta message (marked by the sender) has arrived."
"overheads: dynamic pipelining incurs two additional costs. the first is the increased number of messages. more messages require processor time in sending, receiving, storing meta data and acknowledging. delta messages must be non-blocking, which is more costly to manage than blocking communication because of simultaneous transfers. the second cost is access monitoring. os triggering incurs one page fault for each increment. compiler annotation invokes the run-time library. we must control the two costs so they do not outweigh the benefit of pipelining."
"in this paper, we present delta send-recv, an extension of the mpi send/receive interface and its run-time support. it divides a data message at run time into pieces which we call deltas or increments. on the sender side, the communication starts as soon as the first increment is computed. on the receiver side, the data can be used as soon as the first increment arrives. delta receive is similar to early release [cit] . when combined with delta send, it forms pipelining dynamically. multiple senders and receivers may be dynamically chained to produce cascading in a task group, improving performance by a factor linear to the number of tasks."
"we evaluate delta send-receive in more detail using the pair test. we show the improvement as 3d plots as we vary the delta size from 1 page to 10 pages (4kb to 40kb) for the original message size from 8kb to 1mb, in figure 5 compiler annotation has two advantages. first, the speedup is higher in small message sizes: 9% vs 0% for 8kb messages, and 39% vs 10% for 16kb. second, more combinations show high speedups, 19 vs 5 in the 1.6x-1.8x range."
"on the receiver side, as figure 1 (b) shows, mpi delta recv is called before the message data is used. the parameter list is the the same as a mpi recv, including the buffer address, size, data type, source task, tag, mpi communicator, and status. data may be received out of order. there is no need for an mpi wait. figure 1(c,d) show the use of non-blocking send/receive as a comparison. non-blocking communication can overlap communication with independent computation. mpi delta wait is like mpi wait for a non-blocking send. it blocks the sender until the message has been delivered. mpi delta send begin and mpi delta recv take the same list of parameters, but they are placed before (rather than after) the dependent computation. as a result, delta-send/receive combines goes one step further and overlaps the communication with both the dependent computation and the independent computation."
"where k is the total number of clusters, z n ϫ 1 is the partition of the first n ϫ 1 objects into clusters, p(x n ͉ z n ϭ k, z n ϫ 1, x n ϫ 1 ) is the probability of x n under cluster k, and p(z n ϭ k, z n ϫ 1 ͉ y n ϭ j, x n ϫ 1, y n ϫ 1 ) is the joint probability of generating a new object from cluster k and the partition of the previous n ϫ 1 objects. then this joint probability is given by,"
"where we take into account the fact that this new observation belongs to category y n . the second term on the right hand side is given by equation 4. this defines a distribution over the same k clusters regardless of j, but the value of k depends on the number of clusters in z nϫ1 . substituting this expression into equation a3 provides the relevant mixture model for the rmc. in general, the probabilities in the second term on the bottom line of equation a3 will never be precisely zero for any combination of cluster k and category j, so all clusters contribute to all categories. the rmc can therefore be viewed as a form of the mixture model in which all clusters are shared between categories, but the number of clusters is inferred from the data. the dependency of the rmc between both features and category labels means that the prior over y n depends on x nϫ1 as well as y nϫ1, violating the (arguably sensible) independence assumption made by the other models and embodied in equation 1."
"a second class of monte carlo algorithms, particle filters, are specifically designed to deal with sequential data. particle filters are underpinned by a simpler algorithm known as importance sampling, which is used in cases in which it is hard to sample from the target distribution but easy to sample from a related distribution (known as the proposal distribution). the basic idea of importance sampling is that we generate samples from the proposal distribution and then assign those samples weights that correct for the difference from the target distribution. samples that are more likely under the proposal than the target distribution are assigned lower weights, since they should be overrepresented in a set of draws from the proposal distribution, and samples that are more likely under the target than the proposal are assigned higher weights, increasing their influence."
"the rmc with the local map and single-particle particle filter algorithms were fit to both the nonlinearly separable and linearly separable conditions in the first three experiments of j. d. [cit] . to fit the models, a grid search was performed over model parameters, with values of .01, .1, .5, and 1 for the ␤ prior parameters. independent ␤ prior parameters were used for the physical dimensions, ␤ p, and for the label, ␤ l . the coupling parameter was varied with the values .1, .3, .5, .7, and .9. each simulation was repeated 1,000 times with the stimuli rerandomized within block on each simulation, which was the same randomization scheme used for the human participants."
lethality: this represents the sum of the severity of all incidents performed by this group. target type: the target type that the group attacked most. attack type: the attack type that the group used most. weapon type: the weapon type that the group most commonly used.
"in addition to the network visualization, gephi calculates some measures of centrality for each node in the networkdegree, weighted degree, betweenness, clustering coefficient and eigenvector centrality. these centrality measures give us a notion about the importance of each node in the network. table v shows the five centrality-measures for the 17 nodes of the network sorted in alphabetic order. removing the groups that are not included in the network analysis, we find a big resemblance between the clusters created by the cluster analysis and the communities created by the network analysis; however, network analysis gives the researcher far more detailed and accurate results including centrality measures and weights of linkages between groups."
"rational models of cognition provide a way to understand how human behavior can be explained in terms of optimal solutions to problems posed by the environment. the promise of rational process models is that they can link the platonic world of ideal forms and ideal learners to the less lofty reality of inexact representations and limited resources. by linking these two levels of analysis more closely, we can build models that more completely characterize both the why and the how of human cognition."
"when considering richer representations than prototypes and exemplars, it is necessary to have a method for learning the appropriate representation from data. use of equation 2 to make predictions about category labels and features requires summing over all possible partitions z n . this sum rapidly becomes intractable for large n, since the number of partitions grows rapidly with the number of stimuli according to the bell number introduced earlier. consequently, an approximate inference algorithm is needed and anderson (1990 anderson (, 1991 developed a simple inference algorithm to solve this problem. we refer to this algorithm as the local map algorithm, as it involves assigning each stimulus to the cluster that has the highest posterior probability given the previous assignments (i.e., the maximum a posteriori or map cluster). the algorithm is a local implementation of the map because it makes an assignment for each new stimulus as it arrives, which does not necessarily result in the global map."
"there are a large number of categorization paradigms on which we could compare the algorithms-we chose to compare the algorithms on several data sets for which the local map algorithm performs well, including several cases from anderson's (1990 anderson's (, 1991 original evaluation of the model. testing our algorithm against data on which the local map is known to perform well provides a strong test of the particle filter algorithm. we examine the effect of specific instances with binary [cit] and continuous parameters [cit] and show that the algorithms predict a similar correspondence with human data. next, we explore paradigms that have been chosen to highlight differences between the local map algorithm and the particle filter. the effects of trial order [cit], [cit] task [cit] are used to illustrate the advantages of using the particle filter to approximate the rmc."
"prediction performance and the range of predicted probabilities both increase if the model is trained with the same number of blocks that human participants were trained with (10) instead of just a single block. across coupling parameters, the best correlation with human ratings were high for the local map (r ϭ .95), the particle filter with m ϭ 1 particles (r ϭ .90), and the particle filter with m ϭ 100 particles (r ϭ .93). overall, the results in figure 8 look accurate for all of the models, except for a serious disagreement between the human data and model predictions for 1110, the seventh stimulus from the left. human ratings for 1110 diverged from the ratings of 0111 and 1101, the fourth and fifth stimuli from the left. however, these three stimuli are the same distances from the training stimuli, so the models tended to give these three stimuli the same probability of category 1 as a result."
"in addition to these two desiderata, we are concerned with how these algorithms might introduce new order effects into a model. often statistical models, such as the dpmm, are invariant to the order in which observations arrive. however, the approximations used in practical applications of these models tend to introduce order effects as a side effect of limited computation. people show effects of the order of presentation of stimuli [cit], and to have a psychologically plausible algorithm, the cumulative order effects of the model and those introduced by the approximation should match the order effects displayed by people. in the remainder of this section, we summarize the psychological plausibility of the local map, gibbs sampling, and particle filters. we relate these algorithms to the properties of incrementalism, a single interpretation of how the data arise, and the order effects introduced by the algorithms, which are summarized in table 1 . anderson (1990 anderson (, 1991 introduced the local map algorithm to satisfy his two desiderata for psychological plausibility. the first desideratum is satisfied because the local map is updated incrementally. in addition, the second desideratum is satisfied because only a single partition of the stimuli into clusters is available to the algorithm in order to make judgments about new stimuli. however, as a result of the single interpretation and its maximization operation, the local map algorithm is extremely sensitive to the figure 7 . results of the approximation algorithms compared with the exact posterior. the five bar groupings correspond to the five possible partitions of the three stimuli in figure 1 . the bars within each grouping correspond to the approximation algorithms outlined in the text. standard error bars are provided for the gibbs sampling, multiparticle particle filter, and single-particle particle filter algorithms. from \"nonparametric bayesian models of categorization,\" by t. [cit] ) showed that the predictions of the local map algorithm depended strongly on the order the stimuli were introduced in their clustering experiment. for one type of order, the local map always predicted one partition of the stimuli, but for the other order, it always predicted a second partition. we explore how the local map can be led down the garden path when we compare the algorithms quantitatively."
"considering the processes by which human minds might approximate optimal solutions to computational problems thus provides us with an opportunity not just to address a challenge for rational models of cognition but also to consider how one might develop a general strategy for bridging levels of analysis. in this article, we outline a strategy that is applicable to rational analyses of probabilistic inference tasks. in such tasks, the learner needs to repeatedly update a probability distribution over hypotheses as more information about those hypotheses becomes available. due to the prevalence of such tasks, our strategy provides tools that can be used to derive rational approximations to a variety of rational models of cognition."
"although the standard particle filtering algorithm follows this schema, this only scratches the surface of possible sequential monte carlo techniques. even within particle filtering, there are many options that can be used to improve performance on certain problems. for example, in some cases it is possible to enumerate all possible values of h t given the values of h tϫ1 represented by the current particles and combine this with the likelihood p(d t ͉ h t ) to obtain a more accurate proposal distribution. since the quality of the approximation depends on the match between the proposal and the target, as with other importance sampling methods, improving the proposal distribution directly improves the performance of the particle filter."
"where c is a parameter called the coupling probability, m k is the number of objects assigned to cluster k, and k is the total number of clusters in z n . although this distribution appears unwieldy, it is in fact the distribution that results from sequentially assigning objects to clusters with probability"
"step 1 of our algorithm, the proposed computer program calculated the needed group attributes for each terrorist group. table ii shows the calculated categorical attributespeak year, attack type, weapon type and target typefor each group and table iii shows the calculated numerical attributes of these 22 groupsinclusion criteria, location, lethality and average lethality. table ii shows that there are differences between terrorist groups in terms of the peak year, the most common attack type, the most commonly used weapon type, and the most common target type. for instance, it is obvious here that:"
"for all 80 settings of the parameters, the combined likelihoods over all conditions and experiments was compared. the singleparticle particle filter produced a higher likelihood than the local map algorithm did for each of the 80 settings. to better understand how well the two approximation algorithms fit the outlier table 3 presentation order of anderson and matessa training stimuli [cit] 3 0000 1111 3 0000 1111 3 0100 1010 3 0001 0111 3 1011 1000 stimuli, we recalculated the likelihoods for each parameter setting with only the outlier stimuli. for these stimuli, the single-particle particle filter produced a better fit to the data on 76 of the 80 parameter settings. the best fitting parameters for the local map were ␤ p ϭ .1 for the physical dimensions, ␤ l ϭ 1 for the label dimension, and c ϭ .7 for the coupling parameter. for the m ϭ 1 particle filter, the best fitting parameters were ␤ p ϭ 1, ␤ l ϭ .5, and c ϭ .5. the maximum likelihood fits for the local map and single-particle particle filter are shown in figure 9 . the local map algorithm produces a crossover of the average of the outliers: going from both misclassified to both classified correctly over blocks, at least for experiments 1 and 2. however, the results of the individual runs show that the local map does not produce crossovers on individual runs of the algorithm. instead, examination of the bar plots of individual runs show that the local map crossover is an artifact of averaging. unlike the local map, the single-particle particle filter produces both average crossovers and individual crossovers, as seen in the changing bar plots of individual runs."
"the plan of the article is as follows. in the first part of the article, we describe the general approach. we begin with a discussion of the challenges associated with performing probabilistic inference, followed by a description of various monte carlo methods that can be used to address these challenges, leading finally to the development of the rational approximation framework. in the second part of the article, we apply the rational approximation idea to categorization problems, using anderson's (1990 anderson's (, 1991 ) rational model. we first describe this model and use its connection to nonparametric statistics to motivate new approximate inference algorithms. we then evaluate the psychological plausibility of these algorithms at both a descriptive level and with comparisons with human performance in several categorization experiments."
"monte carlo algorithms provide efficient schemes for approximating probabilistic inference and come with the asymptotic guarantee that they can produce an arbitrarily good approximation if sufficient computational resources are available. these algorithms thus seem like good candidates for explaining how human minds could be capable of performing probabilistic inference, bridging the gap between the computational-level analyses typically associated with rational models of cognition and the algorithmic level at which psychological process models are defined. in particular, gibbs sampling and particle filters provide solutions to the challenges posed by probabilistic inference with large numbers of variables and updating probability distributions over time."
"to examine this question, the researcher used network analysis methodology in addition to database management using sql language codes, and an algorithm to calculate similarity and generate network data from regular incidents' data. the researcher developed a c# computer software to extract data from sql database, and apply the proposed algorithm."
"however, the office of the united nations high commissioner for human rights in factsheet 32 states that terrorism refers to \"acts of violence that target civilians in the pursuit of political or ideological aims\" (un office of the high commissioner for human rights (ohchr), fact sheet no. 32, 2019) ."
"the problem of category learning is to infer the structure of categories from a set of stimuli labeled as belonging to those categories. the knowledge acquired through this process can ultimately be used to make decisions about how to categorize new stimuli. several rational analyses of category learning have been proposed [cit] . these analyses essentially agree on the nature of the computational problem involved, casting category learning as a problem of density estimation: determining the probability distributions associated with different category labels. viewing category learning in this way helps to clarify the assumptions behind the two main classes of psychological models: exemplar models and prototype models. with exemplar models, one assumes that a category is represented by a set of stored exemplars, and categorizing new stimuli involves comparing these stimuli with the set of exemplars in each category (e.g., [cit] . with prototype models, one assumes that a category is associated with a single prototype, and categorization involves comparing new stimuli with these prototypes (e.g., [cit] ) . these approaches to category learning correspond to different strategies for density estimation used in statistics, being nonparametric and parametric density estimation [cit] ). anderson's (1990 anderson's (, 1991 rational analysis of categorization takes a third approach, modeling category learning as bayesian density estimation. this approach encompasses both prototype and exemplar representations, automatically selecting the number of clusters to be used in representing a set of objects. unfortunately, the inference for this model is extremely complex, requiring an evaluation of every possible way of partitioning exemplars into clusters, with the number of possible partitions growing exponentially with the number of exemplars. anderson (1990 anderson (, 1991 proposed an approximation algorithm in which stimuli are sequentially assigned to clusters, and assignments of stimuli are fixed once they are made. however, this algorithm does not provide any asymptotic guarantees for the quality of the resulting assignments and is extremely sensitive to the order in which stimuli are observed, a property that is not intrinsic to the underlying statistical model. as a result, evaluations of the model are tied to the particular approximation algorithm that was used."
"to illustrate the local map algorithm, we show in figure 3 how it would be applied it to the simple example of sequentially presented stimuli in figure 1 . each stimulus is parameterized by three binary features and the likelihood p("
"as discussed above, both gibbs sampling and particle filters are monte carlo methods. this means that they provide ways of approximating the intractable sum over partitions numerically with a collection of samples. specifically, to compute the probability that a particular object receives a particular category label, a monte carlo approximation gives"
"in anderson and matessa's experiment [cit], participants were presented with a set of 16 stimuli in one of two orders, shown in table 3 . these stimuli were designed to emphasize either the first two features (front-anchored stimuli) or the last two features (end-anchored stimuli) in the first eight trials. participants were trained in one of the two orders. following the training phase, participants were shown the full set of stimuli on a sheet of paper and asked to divide the stimuli into two categories of eight stimuli each. eleven of 20 participants presented with the front-anchored order split the stimuli into groups along one of the two features emphasized by the front-anchored ordering. fourteen of 20 participants presented with the end-anchored order split the stimuli along the features that were emphasized by that ordering. overall, there was a significant result, as 25 of 40 participants (62.5%) produced the anticipated order effect."
"\"ajnad misr\" is the most central terrorist group in terms of the number of links it has, the betweenness of its location in the network, and its connectedness to other central and powerful groups. in addition, the linkage between \"ajnad misr\" and \"ansar bayt al-maqdis (ansar jerusalem)\" is the most important link in the network, as it acts as a bridge between two cliques (cliques 2 and 3 in the results)."
"where p(z n ) is a distribution over possible partitions of the n objects into clusters. it is important that the number of clusters k in the partition z n is not assumed to be fixed in advance but is rather something that the learner infers from the data. the rmc provides an explicit form for this prior distribution, namely"
"there are several ways to construct a particle filter for the dpmm. [cit] . the key idea is to treat each new observation as a new time step, with each particle being a partition z i (l) of the stimuli from the first i trials. unlike the local map algorithm, in which the posterior distribution is approximated with a single partition, the particle filter uses m partitions. summing over these particles gives us an approximation to the posterior distribution over partitions"
"the particle filter with m ϭ 100 particles and the gibbs sampler both produce a posterior distribution that is nearly indistinguishable from the exact posterior. the single-particle particle filter is an interesting intermediate case. each run of the single-particle particle filter produces a single partition, not the distribution produced by a particle filter with m ͼ 1 particles. however, averaging over runs of the single-particle particle filter gives an approximation that is much closer to the exact posterior than the local map. unlike the asymptotic performance of the gibbs sampler and particle filter with infinite particles, the approximation of the single-particle particle filter is slightly biased, as can be seen in the figure. the bias is much less than the local map because the algorithm is stochastic, but is still present because each run of the m ϭ 1 particle filter cannot correct its previous assignments by resampling."
"both the single-particle particle filter and the particle filter with m ϭ 100 particles were run with these same parameters. there were 1,000 replications of the single-particle particle filter and 10 repetitions of the m ϭ 100 particle filter. on each replication, the stimuli were presented in a new random order. the overall correlation between the human data in the two experiments and the average output of the model was r ϭ .97 for the single-particle particle filter and r ϭ .98 for m ϭ 100 particles. here again, both types of particle filters perform as well as the local map algorithm."
"the appeal of this more general class of category representations is that it allows people to use prototype-like models when called for and to move to the more flexible exemplarlike models when needed. however, by proposing category representations of this form, we introduce new problems: for a set of n objects, how many clusters k are appropriate to represent the categories, and how should the cluster assignments z n be made in light of the available data (x n, y n )? it is to this topic that we now turn."
"the particle filter for the simple example is illustrated in figure 6 . the particle filter for the dpmm is initialized with the first stimulus assigned to the first cluster for all m particles, in this case m ϭ 2. on observing each new stimulus, the distribution in equation 13 is calculated based on the particles sampled in the last trial. like the local map, the particle filter updates the partition as each new stimulus is observed, and like the local map, only new partitions that are consistent with the previous choices made by the algorithm are considered. this consistency can be seen in the potential partitions when the third stimulus is observed in figure 6 : each descendant is consistent with the partition choices made by its ancestor. intuitively, the psychological processes involved in this approximation are very similar to those involved in the local map algorithm. people update their beliefs incrementally, keeping the assignments of old items fixed and making the assignments of new items conditional on these fixed beliefs. there are two key differences between the local map and particle filter algorithms. the first is that the choice of new partitions is stochastic instead of deterministic. the particle filter algorithm samples new partitions based on their posterior probabilities instead of always selecting the partition with the maximum probability. a particle filter with m ϭ 1 particles is equivalent to the local map algorithm, except that the new partition is sampled instead of deterministically selected. the second difference is that multiple particles mean that multiple partitions can be used instead of the single partition passed forward by the local map. the m partitions are selected without regard for ancestry, allowing a partition that was selected for the early observations to die out as the descendants of other partitions replace it."
"is calculated with binomial distributions that are independent for each feature. these binomial likelihoods are parameterized by the probability of the outcome and need a prior distribution over this probability. the standard prior for binomial likelihoods is the beta distribution (see the appendix for details). for the toy example, we used a symmetric beta prior for the binomial likelihood, with ␤ ϭ 1. the symmetric beta distribution with ␤ ϭ 1 is a simple choice because it is equivalent to the uniform distribution."
"a grid search of parameters for both the local map and particle filter algorithms was done with the same grid as in the linear separability section with 1,000 repetitions per algorithm. [cit] . over the set of all parameters, the single-particle particle filter algorithm fit better than did the local map algorithm on 58% of parameter settings. in addition, the best fit of the local map was a total ssd of .31, whereas the best ssd for the single-particle particle filter was .24. the best fitting parameters were ␤ p ϭ .5, ␤ l ϭ .01, and c ϭ 0.3 for the local map and ␤ p ϭ .1, ␤ l ϭ .1, and c ϭ .3, for the particle filter with m ϭ 1 particles. these results, shown in figure 11, demonstrate that the single-particle particle filter exceeds the performance of the local map for the parameters we tested. however, the brittleness of local map algorithm in this paradigm means that there are probably very specific parameter sets that may provide a much better match to the human data."
"we should note that our simulations are not particularly constraining on the number of particles that might best be used to fit human participants. the second desideratum for psychological plausibility stated that there should be a single interpretation of which cluster generated an object. this desideratum is debatable because it may be that people can hold multiple hypotheses of how objects are generated. in simulations we did not present, we looked at a range of approximations that varied both the number of particles and distributional scaling parameter. our simulations were not particularly constraining for these parameters. for example, a 100 particles filter with ␥ ϭ 2 produces order effects in the anderson and matessa [cit] ) experiment that were approximately equal to that produced by the singleparticle particle filter. we elected to test the local map to the single-particle particle filter in most of the simulations because it provided a clean comparison between maximization and sampling. however, we do not draw the conclusion that a single-particle particle filter is necessarily the way forward. other work has successfully fit individual subject data by varying the number of particles [cit] )."
"the local map algorithm initially assigns the first observed stimulus to its own cluster. when the second stimulus is observed, the algorithm generates each possible partition: it is assigned either to the same cluster as the first stimulus or to a new cluster. the posterior probability of each of these partitions is calculated, and the partition with the highest posterior probability is always chosen as the representation. after the third stimulus is observed, the algorithm produces all possible partitions involving the third stimulus, assuming that the clustering for the first two stimuli remains the same. note that not all possible partitions of the three stimuli are considered because the algorithm makes an irrevocable choice for the partition of the first two stimuli, and the possible partitions on later trials have to be consistent with this choice. the local map algorithm will always produce the same final partition for a given sequential order of the stimuli, assuming there are no ties in the posterior probability."
"the question of how rational models of cognition can be approximated by psychologically plausible mechanisms addresses a fundamental issue in cognitive science: bridging levels of analysis. [cit] computational level-questions about the abstract computational problems involved in cognition. this is a different kind of explanation from those provided by other modeling approaches, which tend to operate at the level of algorithms, considering the concrete processes that are assumed to operate in the human mind. theories developed at these different levels of analysis provide different kinds of explanations for human behavior, with the computational level explaining why we do the things we do and the algorithmic level explaining how these things are done. both levels of analysis contribute to the development of a complete account of human cognition, just as our understanding of bird flight is informed by knowing both how the shape of wings results from aerodynamics and how those wings are articulated by muscle and bone."
"(1) centrality. centrality describes the relative importance of an individual in a network. we can measure centrality in several ways, including: jhass degree centrality is the number of other people adjacent to the individual. the higher this measure is, the more direct associates the individual has in the known network. the person may be a formal leader, a skilled networker or poor at keeping his connections secret [cit] . betweenness centrality is the number of geodesics the individual is on. the higher this measure is, the more indirect associates the individual has. he or she may be a central actor in the communications or exchange network and maybe a key individual in holding the network together [cit] . eigenvector centrality is the degree to which an actor is connected to highly connected peers, and it takes all direct and indirect network paths from the focal actor into account [cit] . (2) components and cliques. components (sub-graphs) are those that divide the network into separate parts with each having several actors. cliques are the (maximal) sub-graphs of nodes that have all possible ties present among themselves. that is, a clique is the largest possible collection of nodes (more than two) in which all actors are directly connected to all others [cit] . (3) cutpoints and bridges. a cutpoint is a single node connecting two or more components of a network. removing that node should disconnect those components. a bridge is a link between two nodes in different networks or network components, so this relationship is also the connection between the two networks or sub-networks [cit] . (4) clustering. one common way of measuring the extent to which a network displays clustering is to examine the local neighborhood of an actor (that is, all the actors who are directly connected to it), and to calculate the density in this neighborhood. after doing this for all actors in the whole network, we can characterize the degree of clustering as an average of all the neighborhoods in the whole network [cit] ."
"before we consider alternative approximation algorithms for anderson's (1990 anderson's (, 1991 ) model, we need to provide a detailed specification of the model and the original algorithm. in this section, we first outline the bayesian view of categorization, showing how exemplar and prototype models are special cases of the approach, and then describe the specific approach taken by anderson (1990 anderson (, 1991 ."
"for example, in the usa, the annual country reports on terrorism in the us code title 22 chapter 38 section 2656f discussed the concepts of international terrorism, terrorism and the terrorist group as follows."
"the differences in quality of the various approximations can be explored by a toy example with sequential observations of the stimuli in figure 1 . we compared the local map, a particle filter with m ϭ 100 particles, a particle filter with m ϭ 1 particle, and gibbs sampling with the exact posterior. for each algorithm, a symmetric beta prior in which ␤ ϭ 1 was used for the likelihood (see appendix for details). the local map was run a single time because its outcome is deterministic on a fixed stimulus order. the particle filters were each replicated 10,000 times, and the gibbs sampler was run for 101,000 iterations. for the gibbs sampler, the first 1,000 iterations were discarded, and every 10th iteration was taken as a sample, yielding 10,000 samples. the results of this comparison are shown in figure 7 . the local map algorithm selected a single partition as an approximation to the exact posterior. in this example, the partition selected by the local map is also the map of the exact posterior distribution, but the two will not always be equivalent. by taking the map partition as each stimulus arrives, the local map can be misled to choose a partition that is not the global map, if the initial trials are not representative of the whole run of trials. an example of this can be seen in the experiment by anderson and matessa [cit], discussed in a later section."
"the local map algorithm approximates the sum in equation 2 with just a single clustering of the n objects, z n . this clustering is selected by assigning each object to a cluster as it is observed. at this point, the features and labels of all stimuli, along with the cluster assignments z i ϫ 1 for the previous i ϫ 1 stimuli are given. thus, the posterior probability that stimulus i was generated from cluster k is"
where z n is produced via the procedure outlined above. the probability that a particular object receives a particular category label would likewise be computed with a single partition.
"each feature within a cluster is assumed to follow a gaussian distribution, with unknown mean and variance. the variance has an inverse 2 prior, and the mean given the variance has a gaussian prior 2 ϳ inv-2 ͑a 0, 0 2 ͒ (a-6)"
"(1) the incident must be intentionalthe result of a conscious calculation on the part of a perpetrator. (2) the incident must entail some level of violence or immediate threat of violence, including property violence, as well as violence against people. (3) the perpetrators of the incidents must be sub-national actors. the database does not include acts of state terrorism."
"gibbs sampling draws samples from one random variable conditioned on all of the rest and all of the data, thus requiring that all of the data be present before inference begins. new data cannot be incrementally added to the sampling scheme, so in order to sample from a posterior distribution when a new piece of data arrives, gibbs sampling must start from scratch. this property makes the algorithm computationally wasteful if sequential judgments are required. for other tasks, however, gibbs sampling is more psychologically plausible. in tasks in which all of the data arrive simultaneously, such as when a researcher gives participants a set of objects to sort into groups, participants do not need to make judgments until all of the stimuli are present. here gibbs sampling seems psychologically plausible."
"network analysis enables the researcher to understand the internal structure of the terrorist organization, the pattern of communication, the flow and path of information sharing, the path of the command, the sub-groups or main components of this network, the most important actors in the network and the vulnerabilities of this network. hence, network analysis has been used in counter-terrorism research as well."
"mapping terrorist groups 2.1.1 classifying terrorist groups. different terrorist groups differ in many aspects, including terms of membership, political goal and ideological foundations. however, few studies aimed at finding a metric upon which we can classify terrorist groups."
"the change to using sampling produces some important differences. averaging many runs of the same order with the local map approximation produces the same result every time. however, averaging many runs with the same order using sampling produces a much better approximation to the true posterior. though each run of a single-particle particle filter produces a potentially extreme result, the aggregate of these results resembles the optimal solution. this effect echoes the wisdom of the crowds: the accuracy of the average over individuals can exceed the accuracy of the individuals [cit] . this effect has also been found for averaging the judgments of a single individual [cit] . in addition, for a task that requires learning categories that are not linearly separable, sampling allows for the model to occasionally assign a repeated item to a new cluster, allowing it reproduce the finding that people initially categorize an outlier stimulus incorrectly but slowly learn the correct response. the single-particle particle filter shows a real advantage on this task: not only can it produce the same results as many particles at a lower computational cost, it produces realistic-looking individual differences over runs of the model."
"the introduction of these new algorithms also inspires the development of intermediate cases. it seems necessary to limit the precision of the local map algorithm in some way to create a psychologically plausible algorithm. one possible way to do this is by casting the local map as a sampling algorithm. as each new stimulus is presented, the local map algorithm computes the posterior probability, f(x), which the new stimulus belongs to each of the existing clusters and to a new cluster. the local map algorithm selects the maximum of f(x), which we can represent by sampling. if we construct a new distribution, g(x) ϰ f(x) ␥, and set ␥ ϭ ϰ, then sampling from g(x) will be equivalent to taking the maximum value of f(x). we refer to the ␥ parameter as the distributional scaling parameter. 2 the usefulness of this representation is that we can use values of ␥ that are less than ϰ. using smaller values of ␥ produces a soft-max rule, which greatly changes the behavior of the algorithm when the best two clusters for a new stimulus have nearly the same, but not exactly the same, probability. now, instead of always selecting the highest probability cluster, the adjusted algorithm will select the top two clusters with nearly equal probability, which is more psychologically plausible. at the other end of the range of the ␥ parameter, when ␥ ϭ 1, this representation is equivalent to a particle filter with m ϭ 1 particles, which selects clusters according to their posterior probability."
"each terrorist group is also a network inside a wider network of supporters, suppliers, audiences and opponents. network analysis concepts of particular value to the analysis of terrorism include:"
"in this expression, p(x n ͉ y n ϭ j, x n ϫ 1, y n ϫ 1 ) denotes the estimated probability that an element of the jth category would possess the collection of features x n observed in the novel object, and p(y n ϭ j ͉ y n ϫ 1 ) is an estimate of the prior probability that a new object would belong to the jth category. additionally, we have assumed that the prior probability of an object coming from a particular category is independent of the features of the previous objects. thus, this expression makes clear that the probability that an object with features x n should be given the label y n ϭ j is related both the probability of sampling an object with features x n from that category and the prior probability of choosing that category label. category learning, then, becomes a matter of determining these probabilities-the problem known as density estimation."
"the intuitive reason the local map algorithm does not produce humanlike crossovers for individual runs is because it becomes stuck in a pattern based on the initial ordering of the stimuli. to illustrate this idea, we make the simplifying assumption that each of the central items of category a are assigned to one cluster and that all of the central items of category b are assigned to a second cluster. the logical possibilities for an outlier are that it is assigned to the correct cluster, assigned to the incorrect cluster, or assigned to its own cluster. whatever cluster it is initially assigned to, which depends on the parameter settings and the order of the stimuli, it will likely be assigned to the same cluster in later blocks. the repetition occurs because the cluster that the outlier was assigned to initially had the highest probability of generating that stimulus, and on subsequent blocks, this cluster will contain a copy of the outlier, which increases the likelihood of assignment to this cluster. the local map algorithm always assigned stimuli to the maximum likelihood cluster, so that the initial assignment of the outlier is almost perfectly predictive of its later assignment. in fact, examining samples of 100 runs of the local map with the best parameters on each experiment's nonlinearly separable condition, we found that the initial assignment was perfectly predictive of all later assignments."
"corollary 1: from the huge similarity between \"ajnad misr\" terrorist group and \"mulsim brotherhood\" in terms of the way they perform their violent acts (incidents), we can, figure 1 ."
"in canada, terrorism is essentially defined as \"acts of violence or threats of violence motivated by ideology and intended to intimidate the public or a segment of the public\" (criminal code of canada s83.01)."
"where v 1 is the value of feature (i) in group 1, and v 1 is the value of feature (i) in group 2. for categorical feature (i):"
"evaluating monte carlo algorithms as candidates for rational process models requires exploring how the predictions of rational models of cognition vary under these different approximation schemes and examining how well these predictions correspond to human behavior. in the remainder of the article, we provide a detailed investigation of the performance of different approximation algorithms for anderson's (1990 anderson's (, 1991 rmc. this model is a good candidate for such an investigation because it involves an extremely challenging computational problem: evaluating a posterior distribution over all possible partitions of a set of objects into clusters. this problem is so challenging that anderson's (1990 anderson's (, 1991 original presentation of the model resorted to a heuristic solution. we use a connection between this rational model and a model that is widely used in bayesian statistics to specify a gibbs sampler and particle filter for this model, which we evaluate against a range of empirical data."
"the connection between the rmc and the dpmm suggests a solution to the shortcomings of the local map algorithm. in this section, we draw on the extensive literature on approximate inference for dpmms to offer two alternative algorithms for the rmc: gibbs sampling and particle filtering. these algorithms are less sensitive to order and are asymptotically guaranteed to produce accurate predictions."
"before turning to a quantitative comparison of the algorithms with human data, it is worth considering their psychological plausibility at a descriptive level, to see whether they are appropriate for human cognition. we take as a starting point anderson's (1990 anderson's (, 1991 two desiderata for an approximate inference algorithm: that it be incremental and that people see objects as arising from a single cause. these desiderata were based on beliefs about the nature of human category learning. in tasks in which people see objects presented sequentially and must judge which category they arise from, \"people need to be able to make predictions all the time not just at particular junctures after seeing many objects and much deliberation\" [cit], and \"people tend to perceive objects as coming from specific categories\" [cit] ."
"use of the local map algorithm, the rmc [cit] has successfully predicted human choices in a wide range of experimental paradigms. we introduced two new algorithms for the rmc in the above sections: the gibbs sampler and the particle filter. we have demonstrated that both of these algorithms provide a closer approximation to the underlying model than does the local map algorithm, and both share some aspects of its psychological plausibility. in this section, we compare the local map algorithm, a sequential updating algorithm, against the sequential algorithm we have introduced: the particle filter. most empirical investigations of human categorization use a sequential trial structure, so we have focused on this comparison. we compare the fits of the multiparticle particle filter, the single-particle particle filter, and the local map algorithm to show that the particle filter provides comparable fits to the human data, and for some paradigms, the particle filter algorithm actually allows the rmc to better predict human choices."
"a huge sum of research connects terrorism to crime, especially organized crime. this is based on the fact that terror acts usually inflict damage of some sort on a given target. therefore, the element of using violence in an illegal form in terrorism is the main cause of why many studies connect terrorism to criminal acts. however, the illegal use of violence is jhass only one dimension of terrorism, but not the only one. this clarifies why mafia crimes, for instance, are not considered as terrorism."
"the third most powerful link lies between \"hasam movement\" and \"revolution's brigade\" with weight 7.53, i.e. with similarity exceeding 75 per cent. the fourth most powerful links lie between \"isil\" and \"revolution's brigade\" with weight 7.3, and \"isil\" and \"revolutionary punishment movement\" with weight 7.1, i.e., with similarity exceeding 70 per cent in both links. the huge similarity between \"ajnad misr\" and \"muslim brotherhood\" can be justified by the fact that \"ajnad misr\" [cit] directly after the removal of mohamed morsi from his office after 30 june revolution. [cit], which leaded to an armed conflict between the muslim brotherhood and the military and police forces."
"gibbs sampling is the approximate inference algorithm most commonly used with the dpmm (e.g., [cit] . it provides a way to construct a markov chain that converges to the posterior distribution over partitions. the state space of the markov chain is the set of partitions, and transitions between states are produced by sampling the cluster assignment of each stimulus from its conditional distribution, given the current assignments of all other stimuli. the clustering evolves by sequentially sampling each z i from the distribution"
"having cast the problem in these terms, it is clear that exemplar and prototype models are two extremes along a continuum of possible approaches to category representation. as illustrated in the middle panel of figure 2, the learner might choose to break the category up into several clusters of stimuli, denoted z n ϫ 1, where z i ϭ k if the ith stimulus is assigned to the kth cluster. each such cluster is then associated with a simple parametric distribution, and the category distribution as a whole then becomes a mixture model (e.g., [cit] . expressed in these terms, prototype models map naturally onto the idea of a one-cluster representation, and exemplar models arise when there is a separate cluster for each object. between lies a whole class of intermediate category representations, such as the one shown in the middle of figure 2 . in this case, the learner has divided the five objects into two clusters, and the resulting category distribution is a mixture of two normal distributions."
"particle filters are designed as sequential algorithms that explicitly use incremental updating, which clearly satisfies the first property and makes this algorithm appropriate for modeling sequential judgments. for the second property, the answer depends on the number of particles. each particle is a sample from the posterior distribution, so a single-particle particle filter will provide a single interpretation of the data. with a multiparticle particle filter, the interpretation becomes probabilistic. the order effects introduced depend on the number of particles, analogous to how the gibbs sampler's order effects depend on the number of samples. with an infinite number of particles, the particle filter is a very faithful representation of the posterior distribution and thus does not introduce any order effects not present in the statistical model. however, small numbers of particles will introduce order effects, and we explore this property in detail later."
"the highest similarity measure lies between \"ajnad misr\" and \"muslim brotherhood,\" which gives an indicator that both groups are two sides of the same coin. this claim is validated by \"ajnad misr\" claim that their acts are considered as retribution for the isolation mapping terrorist groups"
"the term \"international terrorism\" means terrorism involving citizens or the territory of more than one country. the term \"terrorism\" means premeditated, politically motivated violence perpetrated against noncombatant targets by subnational groups or clandestine agents. the term \"terrorist group\" means any group practicing or which has significant subgroups, which practice, international terrorism\" (22 u.s.c. 38 §2656f)."
"our emphasis in this article is on one class of approximation algorithms: monte carlo algorithms, which approximate a probability distribution with a set of samples from that distribution. sophisticated monte carlo schemes provide methods for sampling from complex probability distributions [cit] and for recursively updating a set of samples from a distribution as more data are obtained [cit] . these algorithms provide an answer to the question of how learners with finite memory resources might be able to maintain a distribution over a large hypothesis space. we introduce these algorithms in the general case and then provide a detailed illustration of how these algorithms can be applied to one of the first rational models of cognition: anderson's (1990 anderson's (, 1991 rational model of categorization (rmc)."
"particle filters extend importance sampling to a sequence of probability distributions, typically making use of the relation between successive distributions to use samples from one distribution to generate samples from the next [cit] ). the particle filter was originally developed for making inferences about variables in a dynamic environmentthe problem of filtering is to infer the current state of the world given a sequence of observations. however, it also provides a natural solution to the general problem of updating a probability distribution over time. each particle is a sample from the posterior distribution on the previous trial, and these samples are updated when new data become available."
"one of the most interesting properties of the rmc is that it has a direct connection to a model used in nonparametric bayesian statistics [cit] . the rationale for the use of nonparametric methods is that real data are not generally sampled from some neat, finite-dimensional family of distributions, so it is best to avoid this assumption at the outset. from a bayesian perspective, the nonparametric approach requires us to use priors that include as broad a range of densities of possible, thereby allowing us to infer very complex densities if they are warranted by data. the most commonly used method for placing broad priors over probability distributions is the dirichlet process (dp; [cit] ) . the distributions indexed by the dp can be expressed as countably infinite mixtures of point masses [cit], making them ideally suited to act as priors in infinite mixture models [cit] . when used in this fashion, the resulting model is referred to as a dp mixture model (dpmm; [cit] ."
"the nodes of the network represent the terrorist groups included in the data. the edges (linking two nodes) of the network represent whether these two nodes or groups are similar or not. in this study, an edge is drawn between two nodes if the similarity value between these two groups exceeds a threshold value u . the weights of the edges represent the similarity value between the nodes linked by these edges."
"our analysis of anderson's (1990 anderson's (, 1991 rmc draws on a surprising connection between this model and work on density estimation in nonparametric bayesian statistics. this connection allows us to identify two new algorithms that can be used in evaluating the predictions of the model. these two algorithms both asymptotically approximate ideal bayesian inference and help to separate the predictions that arise from the underlying statistical model from those that are due to the inference algorithm. we evaluate these algorithms by comparing the results with the full posterior distribution and with human data. the new algorithms better approximate the posterior distribution and fit human data at least as well as the original algorithm proposed by anderson (1990 anderson (, 1991 . in addition, we show that these new algorithms have greater psychological plausibility and provide better fits to data that have proved challenging to bayesian models. these results illustrate the use of rational process models to explain how people perform probabilistic inference, provide a tool for exploring the relation between rational models and human performance, and begin to bridge the gap between computational and algorithmic levels of analysis."
"the rmc specifies a rational model of categorization, capturing many of the ideas embodied in other models and allowing the representation to be inferred from the data. however, the model is still significantly limited because the approximate algorithm used for assigning objects to clusters in the rmc can be a poor approximation to the posterior. in particular, this makes it hard to discriminate the predictions that result from the underlying statistical model from those that are a consequence of the algorithm being used. to explore alternative approximation algorithms, we now discuss the connections between the rmc and nonparametric bayesian statistics."
"terrorism is one of the most challenging issues facing the overall world. due to the increasing advancement in information and communication technologies, international terrorism becomes more and more a complex phenomenon. to study that complex phenomenon such as international terrorism, we need new ways of analysis. network analysis is one of the recent ways used to analyze terrorist organizations from within."
"from the previous analysis and results, we can conclude that there are shared patterns of violence acts among different terrorist groups. data about terrorism incidents reveal different levels of similarity between terrorist groups in terms of the ways they perform their acts. using network analysis proved to be beneficial not only in studying the relationships between an individual actor inside a group and the rest of the group members but also in studying the mapping of terrorist groups and organizations themselves."
"where p(z i ϭk ͉ z i ϫ 1 ) is given by equation 4. under the local map algorithm, x i is assigned to the cluster k that maximizes equation 5. iterating this process results in a single partition of a set of n objects."
"some studies tend to classify terrorist groups based on ideology, group size, location, etc. however, in this study, the researcher is interested to classify terrorist organizations or groups based on the way they perform the terrorist incident."
the result is a discrete distribution over all the previous particle assignments and all possible assignments for the current stimulus. drawing m samples from this distribution provides us with our new set of particles.
"we began with experimental paradigms on which the local map algorithm performs well [cit], and the simulations we have performed demonstrate that the particle filter algorithm, especially the single-particle particle filter, performs as well or better in these categorization paradigms. for the effects of specific instances with binary [cit] and continuous data [cit] ), the single-particle particle filter and the multiparticle particle filter performed about as well as the local map algorithm. however, in the later simulations, the local figure 10 . [cit] . the three dimensions of the cube represent the three binary dimensions of the stimuli. each vertex of a cube is labeled as part of category a or category b for each of the six problems."
"updating beliefs over time is computationally challenging because it requires the learner to draw inferences every time new information becomes available. unless the learner uses methods that allow the efficient updating of his or her beliefs, he or she would be required to perform the entire inference from scratch every time new information arrives. the cost of probabilistic inference is thus multiplied by the number of observations that have to be processed. as one would expect, this becomes particularly expensive with large hypothesis spaces, such as the combinatorial spaces that result from having hypotheses expressed over large numbers of random variables. making probabilistic inference computationally tractable thus requires developing strategies for efficiently updating a probability distribution over hypotheses as new data are observed."
"likely to have ended up with a sequence of h t values that are very unlikely. in some ways, this is a waste of computation, since those particles with very small weights will make little contribution to later probabilistic calculations. to address this problem, we can use an alternative approach known as sequential importance resampling. under this approach, we regularly sample a new set of particles from a probability distribution corresponding to the normalized weights. this increases the number of particles that correspond to good hypotheses."
"where 0 2 is the prior variance, a 0 is the confidence in the prior variance, 0 is the prior mean, and 0 is the confidence in the prior mean. [cit] : 0 2 is the square of one quarter of the dimension's range, 0 is the mean of the dimension. the parameters a 0 and 0 are described (appendix continues) in the text. with these conjugate priors, the posterior predictive distribution is student's t:"
"it should be apparent from our description of the prior distribution used in the dpmm that it is similar in spirit to the prior distribution underlying the rmc. [cit] . if we let ␣ ϭ (1 ϫ c)/c, equations 3 and 7 are equivalent, as are equations 4 and 8. thus, the prior over cluster assignments used in the rmc is exactly the same as that used in the dpmm. anderson (1990 anderson (, 1991 thus independently discovered one of the most celebrated models in nonparametric bayesian statistics, deriving this distribution from first principles. figure 4 . the relation among (a) the clustering implied by the dp, (b) the distribution over parameters that is sampled from the dp, and (c) the mixture distribution over stimuli that results in the dpmm. the clustering assignments in (a) were produced by drawing sequentially from the stochastic process defined in equation 8, and each cluster is associated with a parameter value . the x stimuli are a set of undefined stimuli in which the features influence the clusters they belong to, but we are focusing on exploring the prior in this figure. after an arbitrarily large number of cluster assignments have been made, we can estimate the probability of each cluster and, hence, of the corresponding parameter value. the resulting probability distribution is shown in (b). if each value of is treated as the mean of a simple normal distribution (with fixed variance) over the value of some continuous stimulus dimension, then the resulting mixture distribution drawn from the dpmm is the one illustrated in (c). although the applications considered in this article also use stimuli that have discrete features, the notion of a mixture distribution is more intuitive in the continuous setting. from \"categorization as nonparametric bayesian density estimation,\" by t."
"gtd now includes more than 190,000 cases; for each incident, it includes data on more than 12 variables. examples include the date and location of the incident, the weapons used and nature of the target, the number of casualties, andwhen identifiablethe group or individual responsible. jhass gtd defines terrorism as \"the threatened or actual use of illegal force and violence by a non-state actor to attain a political, economic, religious or social goal through fear, coercion or intimidation\" [cit] . to consider an incident for inclusion in the gtd, all three of the following attributes must be present:"
"standard implementations of gibbs sampling do not provide a single interpretation of the data. the algorithm gathers a set of samples from a probability distribution, and all of these samples are used to infer other properties about the data, such as category labels. however, we should note that it would be possible to implement a modified version of the gibbs sampling algorithm that would provide a fixed interpretation of the data. instead of keeping all of the iterations, we could create a very forgetful gibbs sampler that would only recall the current values of the variables when making inferences. likewise, referring to our third property, gibbs sampling is asymptotically unbiased, meaning that generating a huge number samples would not introduce any order effects not already present in the statistical model. again though, the iterations of gibbs sampling are dependent on one another, so in the forgetful gibbs sampler we would have iteration to iteration dependence. this iteration to iteration dependence would not be an effect of the order in which the stimuli were presented, but instead would be an autocorrelation of judgments made by this model."
"however, a few numbers of studies used network analysis in mapping different terrorist groups together. this is because of the scarcity of data about relationships between different terrorist groups on one hand, and because there is also very few studies who tend to classify terrorist groups on the other hand."
"a more troubling discovery is that there are clusters of the stimuli that have only slightly less probability than do the cluster with the maximum posterior probability. [cit], we found that the maximum ratio of the second-best posterior probability to the maximum posterior probability could be as high as .9997. the behavior of the local map algorithm should be very different in the case of tied probabilities and not-quite-tied probabilities, but the difference between the two cases can be very subtle and depend on the precision of the numbers used in the simulation. we found this to be the case when using these best fitting parameters: using the double precision numbers of matlab (64 bits) and assigning ties equally to the best clusters, we found that the ssd of the local map at these parameters rises to .32 and that the ordering of problem difficulty on the final block is changed."
"gibbs sampling [cit] ) is a very commonly used monte carlo method for sampling from probability distributions. this algorithm is initialized with a particular set of values for each variable, often with random values. gibbs sampling works on the principle of sampling a single random variable at each step. one random variable is selected, and the value of this variable is sampled, conditioned on the values of all of the other random variables and the data. the process is repeated for each variable; each is sampled, conditioned on the values of all of the other variables and the data. intuitively, gibbs sampling corresponds to the process of inspecting one's beliefs about each random variable conditioned on one's beliefs about all of the other random variables and the data. reflecting on each variable in turn provides the opportunity for changes to propagate through the set of random variables. a complete run through sampling all of the random variables is an iteration, and the algorithm is usually engaged for many iterations."
extract data from the database using sql database codes. calculate the similarity matrix using the proposed algorithm (that will be discussed in detail in the next section). generate a network data file using the calculated similarity matrix and export it to an excel file.
"the connection of clique 2 and cliques 3through the link between \"ajnad misr\" and \"ansar bayt al-maqdis (ansar jerusalem)\"can be justified by the fact that the founder of \"ajnad misr\", humam muhammed, was a member of \"ansar bayt al-maqdis\" militant group and then split away from it. merging the cliques appearing in figure ( 2) with the groups' attributes shown in tables ii and iii, we can find that:"
"the computational problems that people need to solve are often inductive problems, requiring an inference from limited data to underdetermined hypotheses. for example, when learning about a new category of objects, people need to infer the structure of the category from examples of its members. this inference is inherently inductive, since the category structure is not completely specified by the limited set of examples given to the learner; because of this, it is not possible to know exactly which structure is correct. that is, the optimal solution to problems of this kind requires the learner to make probabilistic inferences, evaluating the plausibility of different hypotheses in light of the information provided by the observed data. in the remainder of this section, we discuss two challenges that a learner attempting to implement the ideal solution faces: reasoning about hypotheses that are composed of large numbers of variables and repeatedly updating beliefs about a set of hypotheses as more information becomes available over time."
"recently, network analysis is mainly used to serve counter-terrorism by identifying the strengths of the network to target them, and its vulnerabilities to penetrate them. among these studies, we find"
"however, the tendency to regard terrorist organizations as networks (i.e. cellular structures rather than hierarchies) is relatively new. terrorist groups or organizations can be viewed as \"hub-and-spoke\" networksi.e. efficiently organized structures of connected cells that are resilient to disruption [cit] ."
where ␣ is the dispersion parameter of the dp. this distribution over partitions can be produced by a simple sequential stochastic process [cit] . if observations are assigned to clusters one after another and the probability that observation i ϩ 1 is assigned to cluster k is
"the rmc as described in equation 2 is defined in terms of the joint distribution of x n and y n, rather than by directly specifying the category distributions, as would be the case for the simpler mixture models such as the exemplar model. so, it may not be immediately clear how to map it onto the framework of density estimation. however, it is possible to rewrite the rmc in terms of the standard bayesian categorization model (equation 1) and thereby make the link explicit. to do so, we note that the first term of the numerator of equation 1 can be rewritten to include assignments of the stimuli to clusters,"
"(3) group name: the name of the perpetrator group that carried out the attack. to ensure consistency in the usage of group names for the database, the gtd database uses a standardized list of group names that have been established by project staff to serve as a reference for all subsequent entries. if no information about the perpetrator group is available, this field is coded as \"unknown\". (4) inclusion criteria: these are three categorical variables, namely: criterion 1, criterion 2 and criterion 3. for each variable, a case is coded as \"1\" if the criterion is met and \"0\" if the criterion is not met or that there is no indication that it is met. criterion 1: the violent act aims at attaining a political, economic, religious or social goal. criterion 2: there is evidence of an intention to coerce, intimidate or publicize some messages to a larger audience(s). in addition to the previously selected features, the researcher calculates the incident severity for each incident, using the following equation [cit] :"
"the key idea behind our approach is that efficient implementation of probabilistic inference is not just a problem in cognitive science-it is an issue that arises in computer science and statistics, resulting in a number of general purpose algorithms [cit] . these algorithms often provide asymptotic guarantees on the quality of the approximation they provide, meaning that with sufficient resources they can approximate the optimal inference to any desired level of precision. the existence of these algorithms suggests a strategy for bridging levels of analysis: starting with rational models and then considering efficient approximations to those models as candidates for psychological process models. the models inspired by these algorithms will not be rational models but instead will be process models that are closely tied to rational models and will typically come with guarantees of good performance as approximations-a kind of rational process model."
"1 if the likelihood of this type of representation is large enough, then the particles will all tend to show that particular representation. later trials that point toward a different partition of the early trials will not be able to change the partition of the early trials. as a result, early examples can have a greater influence than later trials."
"the calculated similarity matrix of the 22 terrorist groups jhass therefore, reach a conclusion that \"ajnad misr\" is just a militant group formed from members of the \"muslim brotherhood\" organization. in other words, \"ajnad misr\" can be viewed as a military wing of \"muslim brotherhood\". our generated network is a weighted undirected network that contains 17 nodes and 23 edges. visualizing the network using gephi software, we find that there are three components or communities within this network. the average clustering coefficient of the network is 0.792, and the graph density is 0.169. figure (2) shows the network with three components with three different colors with line thickness represent the weight of the link."
"(1) in this study, the researcher does not intend to classify terrorist groups based on a set of group properties or features, however, the main aim is to test whether there is a specific pattern that can be shared by different terrorist groups or not, and map the groups that share the same pattern together."
"the prediction of the single-particle particle filter stands in contrast with the prediction of a particle filter with a very large number of particles. each block contains a random ordering of all of the training stimuli, so as the number of particles becomes very large, the distribution over partitions on each run of the model after each block will be the same. unlike the single-particle particle filter, a particle filter with many particles will not be able to predict between-subjects variability with the same parameters, which is an interesting consequence of the single-particle particle filter. the number of particles needed to produce the same outcome on each block is actually quite large, as simulations with m ϭ 1,000 particles still showed between-run variability, so this may only be a problem for the ideal statistical model."
"having groups similarity matrix calculated from the dataset, social network analysis is then used to understand the relationships between different terrorist groups and to test whether there are similar patterns among terrorist groups or no. in this regard, gephi software (version 0.9.2) is used to visualize the network and to calculate some useful measures for the analysisthese measures will be discussed in the next section. the results of the network analysis are then compared to the results of a two-step cluster analysis, to validate the proposed methodology."
"summing up, he defined terrorism as \"a peculiar form of political violence based on an indirect approach. it implies a patent breach of accepted rules and enjoys a tactical advantage over defense\" [cit] . another branch of literature connects terrorism with psychology, specifically in studying the psychology of a terrorist. this is based on the idea of connecting terrorist behavior with some psychopathologies."
"based on this collective view of terrorism, another line of research emerged adopting the idea of the strategic logic of terrorist organizations. in other words, terrorist organizations follow a strategic logic in their activities and their planning of the attacks with the aim to pursue a political goal [cit] . \"in contemporary studies on terrorism, three different analytical levels are usually considered:"
"despite the importance of both the computational and the algorithmic level to understanding human cognition, there has been relatively little consideration of how the two levels might be connected. [cit] clearly stated that they were not independent, with the expectation that results yielded at one level would provide constraints on theories at another. however, accounts of human cognition are typically offered at just one of these levels, offering theories of either the abstract computational problem or the psychological processes involved. cases in which rational and process models can be explicitly connected are rare and noteworthy, such as the equivalence of exemplar and prototype models of categorization to different forms of density estimation [cit], although in recent work, researchers have begun to explore how rational models might be converted into process models (e.g., [cit] b )."
"where the counts m k are accumulated over z i ϫ 1 . thus, each object can be assigned to an existing cluster with probability proportional to the number of objects already assigned to that cluster or to a new cluster with probability determined by c. since the prior distribution is set up in a way that allows k to grow as more objects are encountered, the rmc allows the learner to infer the number of clusters via the usual process of bayesian updating. intuitively, we can look at the prior as a rich-get-richer scheme: if a cluster already contains many objects, then it has a higher prior probability for new objects. the coupling probability is the parameter that determines the severity of this scheme. for high values of the coupling parameter, larger clusters will be favored in the prior, whereas for low values of the coupling parameter, smaller clusters will be favored. the cluster sizes that actually result depend on the likelihoods as well as the prior. figure 2 . three different approaches to estimating the category distribution p(x n ͉ y n, x nϫ1, y nϫ1 ). in all three cases, the learner knows that five objects (corresponding to the marked locations x 1 through x 5 ) all belong to a category, and the solid line plots the probability (density) with which a new object sampled from that category would be expected to fall in each location. the left panel shows a prototype model, in which all objects are clustered together and are used to estimate the mean of this distribution (dashed line). on the right is an exemplar model, in which each object corresponds to a unique cluster, leading to a peak located over the top of each object. the intermediate case in the middle clusters objects 1-3 together and objects 4 -5 together (i.e., z ϭ [cit] ), with the result that there are now two peaks in the category distribution."
"network analysis has been widely used in literature to study either the internal structure of a terrorist organization or the flow of information within it. in both cases, the main aim behind using network analysis is to find the most important elements in a network from the pattern of communication within this network, and therefore, detect the weak points in any terrorist network."
"recent research has also identified correspondences between the kind of sophisticated monte carlo methods discussed above and psychological process models. [cit] showed that the basic computations involved in importance sampling are identical to those used in exemplar models [cit] . with exemplar models, it is assumed that people store stimuli in memory, activating them based on their similarity to new stimuli (e.g., [cit] . an importance sampler can be implemented by storing hypotheses in memory and activating them in proportion to the probability of observed data under that hypothesis. moreover, this interpretation of exemplars as stored hypotheses links exemplar-based learning nicely to previous rational analyses of exemplar-based decisions as a form of sequential analysis [cit] . that is, the importance sampling method allows people to efficiently learn and store a posterior distribution, and the sequential analysis method allows efficient decisions to be made on the basis of this stored representation. this thus constitutes a natural, psychologically plausible scheme for approximating some probabilistic computations."
"to understand the international terrorist landscape that performs attacks in egypt, we have to construct a network of terrorist groups. however, we need first to extract network data from the gtd data. [cit] to calculate a similarity matrix between different terrorist groups."
"in this experiment, the local map algorithm predicts that participants will always produce the anticipated ordering effect. we ran the single-particle particle filter for 1,000 repetitions and the m ϭ 100 particle filter for 10 repetitions in this experimental design to compare it with the local map. the single-particle particle filter produces the anticipated order effect on 63% of trials, whereas the particle filter with m ϭ 100 particles produces the order effect only 52% of the time. in this experiment, the particle filter with a single particle is closer to the human results than either 1 we should note that users of particle filter algorithms in computer science and statistics fight garden path effects through the use of particle rejuvenation or particle jittering [cit] . in rejuvenation, variance is added to the particles, possibly with different sampling algorithms such as markov chain monte carlo. the additional sampling over partitions of stimuli would allow for the particle filter to explore possible clusterings of earlier stimuli that are irretrievable under our scheme. however, particle rejuvenation comes at the cost of additional computation, so we have not implemented it in order to keep the simplest possible algorithms. figure 8 . probability of choosing category 1 [cit] . the transfer stimuli (listed in order of human preference) are along the horizontal axis. in the first row, only the first six trials are presented, whereas in the second row, 10 blocks of six trials each are presented. the two lines in each panel correspond to two different coupling parameters: for the triangles, c ϭ .1 and for the circles, c ϭ .3. pearson correlations between the human data and the simulation data are displayed on each plot for each value of the coupling parameter. the local map algorithm or the particle filter with a large number of particles."
"\"as terrorists generally challenge the monopoly of violence of the state and its ability to protect its citizens, terrorist acts obtain political significance even when the motivation for them is not primarily political\" [cit] )."
"equation 10 is extremely similar to equation 5, although it gives the probability of a cluster based on the all of the trials in the entire experiment except for the current trial, instead of just the previous trials. the statistical property of exchangeability, briefly noted above, means that these probabilities are actually computed in exactly the same way: the order of the observations can be rearranged so that any particular observation is considered the last observation. hence, we can use equation 8 to compute p(z i ͉ z ϫi ), with old clusters receiving probability in proportion to their popularity and a new cluster being chosen with probability determined by ␣ (or, equivalently, c) . the other terms reflect the probability of the features and category label of stimulus i under the partition that results from this choice of z i and depends on the nature of the features."
"our analysis of the rmc provides a good example of how this idea can be put to good use. the rmc is an example of a successful bayesian model of cognition. it provides a reasonable explanation of how objects should be grouped into clusters, and the result of this clustering can be used to explain many categorization experiments. as a purely rational analysis, however, the rmc runs into difficulties because the complexity of the computational problems involved makes inference difficult and because of the fact that the underlying statistical model cannot produce order effects. approximation algorithms address both issues by simplifying inferences and inducing order effects. however, the original local map approximation produces some order effects that could be considered too strong, and unlike people, it learns by deterministic assignments rather than probabilistic ones. using the monte carlo figure 11 . [cit], along with the best fitting local map and single-particle particle filter algorithms to these data. each line is a separate problem type."
"the gibbs sampling algorithm for the dpmm is straightforward [cit] and is illustrated for the simple example in figure 5 . first, an initial assignment of stimuli to clusters is chosen, with a convenient choice being all stimuli assigned to a single cluster. unlike the local map algorithm, gibbs sampling is not a sequential algorithm; all stimuli must be observed before it can be run. next, we choose a single stimulus and consider all possible reassignments of that stimulus to clusters, including not making a change in assignments or assigning the stimulus to a new cluster. equation 10 gives the probability of each partition, and one of the partitions is sampled based on its posterior probability, making this algorithm stochastic, unlike the local map. the stochastic nature of the algorithm is evident in the example in figure 5 because the first circled assignment has lower probability than the alternatives. the example shows two iterations of gibbs sampling, in which each stimulus is cycled through and reassigned. in an actual application, the algorithm would go through many iterations, with the output of one iteration providing the input to the next. since the probability of obtaining a particular partition after each iteration depends only on the partition produced on the previous iteration, this is a markov chain. after enough iterations for the markov chain to converge, we begin to save the partitions it produces. the partition produced on one iteration is not independent of the next, so the results of some iterations are discarded to approximate independence. the partitions generated by the gibbs sampler can be used in the same way as samples z n (l) in equation 9. as with standard monte carlo approximations, the quality of the approximation increases as the number of partitions in that collection increases. the gibbs sampler provides an effective means of constructing the approximation in equation 9 and, thus, of making accurate predictions about the unobserved features of stimuli."
"rational models of cognition aim to explain human thought and behavior as an optimal solution to the computational problems that are posed by our environment [cit] . this approach has been used to model several aspects of cognition, including memory [cit], reasoning [cit], generalization [cit], and causal induction [cit] . however, executing optimal solutions to these problems can be extremely computationally expensive, a point that is commonly raised as an argument against the validity of rational models (e.g., [cit] . this establishes a basic challenge for advocates of rational models of cognition: identifying psychologically plausible mechanisms that would allow the human mind to approximate optimal performance."
"where p(z i ϩ 1 ͉ z i ) is given by equation 8. we can then incorporate the information conveyed by the features and label of stimulus i ϩ 1, arriving at the approximate posterior probability"
"although a complete description of the dp is beyond the scope of this article [cit], what matters for our purposes is that the dp implies a distribution over partitions: any two observations in the sample that were generated from the same mixture component may be treated as members of the same cluster, allowing us to specify priors over an unbounded number of clusters. in the case where n observations have been made, the prior probability that a dp will partition those observations into the clusters z n is"
"principle, however, we are able to derive a particle filtering algorithm that retains the strengths of the local map algorithm but fixes its weaknesses. [cit] : online updating of the representation plus a single partition of all of the stimuli into clusters. the only difference of the algorithm is that it uses sampling instead of a maximization operation in order to select new partitions."
"a network is a collection of actors (e.g. persons, groups, organizations)represented by nodesand relations between actors (connections, activities)represented by links [cit] s, primarily in the form of link analysis, a specific adaptation of network analysis to criminal intelligence and investigation."
"with that in mind, the problem facing the learner can be written in the following way: on the nth trial in the experiment, he or she is shown a new stimulus x n (e.g., the third stimulus in figure 1 ) and asked what label it should be given. if there are j possible labels involved in the task, the problem is to determine whether the nth object should be given the jth label (i.e., infer that y n ϭ j), on the basis of the information available, (x n, x n ϫ 1, y n ϫ 1 ). if we apply bayes's rule to this problem, we are able to see that"
"where ␦(z, z) is 1 when z ϭ zј, and 0 otherwise. if equation 11 is used as an approximation to the posterior distribution over partitions z i after the first i trials, then we can approximate the distribution of z i ϩ 1, given the observations x i, y i in the following manner:"
"an interesting aspect to the study of nonlinearly separable categories is exploring how category outliers are learned. the standard design is to select two category centers, with most training stimuli clustered near to the center. a small number of outliers, however, are actually very close to the center of the other category. examples of these types of structures can be seen in table 4 . in both these designs, category a consists of binary features mainly set to zero, and category b consists of binary features mainly set to one. one stimulus in each category is an outlier and is a better match to the stimuli in the other category than to the stimuli in its own category."
where b v is the number of stimuli with value v on the dth feature that z n identifies as belonging to the same cluster as x n . b. denotes the number of other stimuli in the same cluster.
terrorism is such a complex phenomenon that lacks a concise agreed-upon definition. [cit] recognized more than 100 definitions of the term terrorism in both the official and the academic fields.
"in addition, the results of mapping terrorist groups using network analysis were similar to the results of cluster analysis, which gives validation to the proposed algorithm. moreover, network analysis outperforms cluster analysis because it promotes the analyst with much more details than just classification. for instance, it enables the analyst to visualize the linkages between different terrorist groups; it calculates several measures of centrality or prestige, or detect the most important groups in the network; it weights the linkages between different groups by the similarity measure, which enables more in-depth analysis."
"sampling also avoids the necessity of precise representations. it is implausible that people would make deterministic choices based on values that are almost exactly equal, but this is what the local map algorithm assumes. nearly indiscriminable choice probabilities arise in fitting the local map algorithm to learning data under a plausible set of parameters. in contrast, the particle filter algorithm samples, so that choices between representations that have nearly equal probability are chosen nearly equally often. this algorithm, or one that interpolates between pure sampling and pure maximization makes for a more psychologically plausible alternative to the local map. these results, combined with recent work that has successfully applied particle filters to a range of problems [cit], lead us to believe that particle filters have the potential to be a powerful tool for producing rational process models."
"corollary 2: there are two basic communities of terrorist groups in egypt. each group has its own pattern of attacks. the first community tends to make armed assault using firearms attacking mainly police officers. however, the second community tends to make explosions using explosives attacking both police and military officers. corollary 3: the egyptian terrorism network reveals three cliques, one of them constitute a community in itself, which is the first community in corollary 2. however, the second two cliques are different in the pattern of attacks, but in the same time, the two cliques are connected through a link between \"ajnad misr\" and \"ansar bayt al-maqdis (ansar jerusalem).\" cliques 2 and 3 share the same pattern of using explosives but differ in their targets, where clique 2 mainly attacks military forces, while clique 3 mainly attacks police forces. they also differ in the inclusion criteria and peak years."
using the c# tailored application to extract data from a sql database including the original gtd data; the program extracted 22 [cit] . table i shows the names of these 22 terrorist groups sorted alphabetically.
"however, other studies rejected this idea, claiming that terrorism is a group activity, and therefore, psychopathology or a single personality type cannot solely result in terrorism. the proponents of this idea claim that shared values and ideological commitment and group solidarity are much more important than psychological factors in understanding terrorism [cit] ."
"differently from criminal activities, terrorism is defined by a clear political orientation and/or relevance. such a political dimension reminds us that terrorists usually (although not always) have a political goal [cit] ."
"map algorithm was outperformed by the particle filter, especially by the single-particle particle filter. for the order effects of stimuli presentation, the local map algorithm predicts order effects that are stronger than those displayed by human participants. a particle filter with m ϭ 100 particles predicted almost no order effects, but for the single-particle particle filter, the size of the order effect was similar to the empirical average. further advantages of the particle filter were found for newer experiments with categories that differed in linear separability (j. d. [cit] ). the statistical model underlying the rmc predicts that outlier stimuli will initially be categorized incorrectly, but over blocks, will eventually be categorized correctly. the local map algorithm did not predict the crossover in individual runs with its best fitting parameters and did imitate it in the average data by averaging over different trial orders. in contrast, the single-particle particle filter predicts both the crossover in average data, as well as individual variability in how quickly the outlier is learned to be classified correctly."
"one of the most fundamental challenges in performing probabilistic inference concerns the situation in which the number of hypotheses is very large. this is typically encountered when each hypothesis corresponds to a statement about a number of different variables. the number of hypotheses then suffers from a combinatoric explosion. for example, in many theories of category learning, it is assumed that people assign objects to clusters. if so, then each hypothesis is composed of many assignment variables, one per object. likewise, in causal learning, hypotheses about causal structure can often be expressed in terms of all of the individual causal relations that make up a given structure, thus requiring multiple variables. reasoning about hypotheses comprising large numbers of variables poses a particular challenge because of the combinatorial nature of the hypothesis space: the number of hypotheses to be considered can increase exponentially in the number of relevant variables. the number of possible clusterings of n objects, for example, is given by the nth bell number, with the first 10 values being 1, 2, 5, 15, 52, 203, 877, 4,140, 21,147, and 115,975 . in such cases, brute force enumeration of all hypotheses will be extremely computationally expensive and scale badly with the number of variables under consideration."
"though the algorithm will eventually sample from the desired distribution, it starts at a particular, often random, set of values. the early iterations show the algorithm converging on the desired distribution but are not yet samples from this distribution. these iterations are known as the burn-in and are thrown away. an additional difficulty is that iterations following the burn-in iterations often show strong dependency from one iteration to the next. these iterations are then thinned, which means keeping every nth iteration and discarding the rest. the remaining iterations after burn-in and thinning are used as samples from the desired distribution. this process provides a way to generate samples from probability distributions defined over large numbers of variables without ever having to enumerate the entire hypothesis space, providing a tractable way to perform probabilistic inference in these cases."
"the main research question of this study is whether there exist patterns in terrorism incidents. in other words, can we classify terrorist groups based on the pattern they share in perpetuating their incidents?"
"using the generated network data file, the researcher uses gephi software to visualize the network, and calculate the following measures: number of components (communities) and cliques in the network. degree and weighted degree centrality of each group. betweenness centrality of each group. eigenvector centrality of each group. clustering coefficient of each group."
"to make some of these ideas more concrete, figure 4 presents a visual depiction of the relation between the partitioning implied by the dp, the distribution over parameters that is sampled from the dp, and the resulting mixture distribution over stimuli that results in the dpmm. the partitioning implied by the dpmm shows that items are divided into discrete clusters. each of these clusters is given a parameter drawn from the prior distribution over parameters. a large number of parameter draws are shown in figure 4b . each spike is a new parameter value, and the height of the bars depends on the number of clusters that use that parameter. finally, combining the parameter values with a continuous likelihood function, such as a gaussian distribution, gives the mixture distribution shown in figure 4c ."
"when making probabilistic inferences, people rarely have all the information they need to definitively evaluate a hypothesis. as a result, when a learner observes a piece of data and uses this to form beliefs, he or she generally remains somewhat uncertain about which hypothesis is really the correct one. when a new piece of information arrives, this distribution needs to be updated to the new beliefs. the consequence is that an ideal learner needs to constantly update a probability distribution over hypotheses as more data are observed."
"the rational model is not able to predict these order effects, but approximations to the rational model can. approximations only assign mass to a small portion of the posterior space over partitions, in effect embodying only a small number of hypotheses table 2 transfer stimuli ordered by category 1 [cit] transfer stimuli 1111 0101 1010 1101 0111 0001 1110 1000 0010 1011 0100 0000 about how the stimuli should be clustered. when a new trial is added to the representation, the possible new representations are extensions of the previous representations. so, if a particular partition of the existing stimuli is not present among the particles, then it will never appear when the representation has been updated. in this way, the approximation to the dpmm can be led down a garden path by presenting many early trials that point toward a particular type of representation."
"prototype models predict that these outlier stimuli will always be classified in the incorrect category, whereas exemplar models predict that they will be classified fairly accurately. j. d. [cit] ran a series of experiments in which they examined the time course of learning central and outlier members of categories. initially outlier items were classified as belonging to the incorrect category, but performance improved over blocks of training trials. figure 9 displays these average results as well as the results of individual participants. the data of the individual participants were noisy, so the training blocks are grouped into three bins that are summarized in bar graphs. the outlier stimuli could be both classified incorrectly (labeled opposite categories), both classified in one category or another, or both classified correctly. the decrease in the number of individuals who classify both outliers incorrectly and the increase in the number who classify both outliers correctly over blocks mirror the average results."
"in contrast, the stochastic assignment of the single-particle particle filter allows for individual runs of the rmc to display crossovers. [cit] . the line plots show the proportion of trials on which category a was chosen for each stimulus. stimuli belonging to category a are marked with an x, whereas stimuli belonging to category b are marked with a circle. the darker lines highlight the stimuli that are unusual in each category. the bar plots show how the unusual stimuli were classified in early, middle, and late blocks. when the two stimuli were both more often given the incorrect response, they were classified as opposite. both a and both b mean that both stimuli were more often classified in one of the two categories than the other, and correct means that both unusual stimuli were classified correctly on average. relative probability of the new cluster. one way in which sampling can cause crossing over is if an outlier is initially assigned to a cluster containing the central stimuli from the other category. this outlier will initially be categorized incorrectly. but in later blocks, the outlier has the possibility of being assigned to a new cluster that contains only that outlier. once the outlier is assigned to a new cluster, the outlier in later blocks tends to be assigned to the same cluster because the new cluster contains only the outlier and thus is a very good likelihood match. prediction of the outlier's category label will become more accurate because the cluster containing only the outlier will have a stronger influence over blocks, and it predicts the correct category label. as the assignments are stochastic, the block on which the crossover occurs will vary over runs of the algorithm. the prediction of individual crossovers at variable blocks in training matches the human data."
"in the equations for the rmc above, all clusters are used in each category, but a generalization of the rmc allows for other assumptions about the structure of categories. this generalization casts the exemplar, prototype, and rmc as restricted versions of hierarchical dps [cit] . other models in this framework allow for probabilistic sharing of clusters between categories or even completely independent sets of clusters for different categories."
"we obtain equation 7 for the probability of the resulting partition. this distribution has a number of nice properties, with one of the most important being exchangeability: the prior probability of a partition is unaffected by the order in which the observations are received [cit] . intuitively, exchangeability is similar to independence, but slightly weaker."
"the global terrorism database (gtd) [cit] . the national consortium for the study of terrorism and responses to terrorism (start) makes gtd available online, with systematic data on domestic, as well as transnational and international terrorist incidents. gtd is currently considered as the most comprehensive unclassified database on terrorist attacks in the world."
"one advantage to describing categorization in terms of the density estimation problem is that both exemplar models and prototype models can be described as different methods for determining the probabilities described by equation 1. [cit] observed that if the learner uses a simple form of nonparametric density estimation known as kernel density estimation (e.g., [cit] ) in order to compute the probability p(x n ͉ y n ϭ j, x n ϫ 1, y n ϫ 1 ), then an exemplar model of categorization is the result. on the other hand, they note that the learner could use a form of parametric density estimation (e.g., [cit] ), in which the category distribution is assumed to have some known form, and the learner's goal is to estimate the unknown parameters of that distribution. if the learner uses this approach, then the result is a prototype model, with the centroid being an appropriate estimate for distributions whose parameters characterize their mean. to illustrate the point, in figure 2, we show a prototype model on the left, in which the category distri- bution is assumed to be normal distribution centered on the prototype, and an exemplar model on the right, in which a separate normal distribution (the kernel) is placed over each exemplar, and the resulting category distribution is a mixture model."
"we compared order effects produced by the range of approximation algorithms with the human data. for all algorithms, c ϭ .5 and ␤ ϭ 1, the values used for the local map by anderson and matessa [cit] . the adjusted rand index [cit], a standard measure of distance between partitions, was used to find the similarity of the output of the local map and particle filter to each of the four partitions that split the stimuli along a single feature. the single-feature-based partition that had the highest adjusted rand index was selected as the partition for that sample. if there was a tie, one of the best was selected with equal probability."
"some of the challenges of probabilistic inference can be addressed by approximating optimal solutions with algorithms based on the monte carlo principle. this principle is one of the most basic ideas in statistical computing: rather than performing computations using a probability distribution, we perform those computations using a set of samples from that distribution. the resulting approximation becomes increasingly accurate as the number of samples grows and the relative costs of computing time and errors in approximation can be used to determine how many samples should be generated. this principle forms the foundation of an entire class of approximation algorithms [cit] . monte carlo methods provide a way to efficiently approximate probabilistic inference. however, generating samples from posterior distributions is typically not straightforward: generating samples from a distribution requires knowing the form that distribution takes, which is a large part of the challenge of probabilistic inference in the first place. consequently, sophisticated algorithms need to be used in order to generate samples. here, we introduce two such algorithms at an intuitive level: gibbs sampling and particle filters. a parallel mathematical development of the general algorithms is given in the appendix and toy examples of these algorithms applied to categorization and a discussion of their psychological plausibility are given later."
"given the cluster, the value on each feature is assumed to have a bernoulli distribution. integrating out the parameter of this distribution with respect to a symmetric beta (␤, ␤) prior, we obtain"
"arduino is an open-source platform which can be programed with personal computer (pc) to control different types of electronic devices such as sensors, lights, loudspeakers, motors [cit] . the arduino board used in the system is shown in figure 1."
"t he rapid increase of connected mobile terminals in the past few years has been pushing data rate requirements of 5g systems to new levels [cit] . as the sub-6 ghz spectrum is congested, moving towards other ranges, such as millimeter wave (mmwave) regime, has been one of the main ideas for achieving these requirements [cit] . the mmwave band is little regulated and is mostly available, allowing for mobile communication systems to operate on large bandwidths. however, the propagation characteristics on this elevated frequency range poses many engineering challenges. for instance, the path loss is very strong as it increases with the inverse of the wavelength squared. highly directional propagation with massive antenna arrays is then required to compensate for this large path loss. such technology has been referred to in literature as massive multiple-input multiple-output (mimo) [cit] ."
"in our experiments, two image datasets are used: corel 10000 and the subset of catltech 101 object categories. the corel dataset commonly used, contains 10,000 images of various contents, such as flowers, food, wave, pills, sunset, beach, car, horses, fish and door, etc. it contains 100 categories and each category contains 100 images in jpeg format. for the second dataset, 1,500 images are chosen from catltech 101 object categories. we choose 30 objects of sunflower, dollar, headphone and faces, etc. and each object contains 50 images. in each category we randomly choose 5 images, so 500 queries are drawn from the corel10000 and 150 queries from catltech 101 object categories, respectively. for both datasets, image feature vectors are extracted using mth, as described in section 4.1."
"the rest of this paper is organized as follows. section 2 shows an overview of related work. section 3 presents the framework of lrfir. section 4 describes the index construction service and the query processing service. section 5 evaluates the performance of lrfir. finally, section 6 concludes our discussion."
"abstract-with the congestion of the sub-6 ghz spectrum, the interest in massive multiple-input multiple-output (mimo) systems operating on millimeter wave spectrum grows. in order to reduce the power consumption of such massive mimo systems, hybrid analog/digital transceivers and application of low-resolution digital-to-analog/analog-to-digital converters have been recently proposed. in this work, we investigate the energy efficiency of quantized hybrid transmitters equipped with a fully/partially-connected phase-shifting network composed of active/passive phase-shifters and compare it to that of quantized digital precoders. we introduce a quantized mimo model based on an additive quantization noise model considering a realistic power consumption and loss model to evaluate the spectral and energy efficiencies of the transmit precoding methods. simulation results show that partially-connected hybrid precoders can be more energy-efficient compared to digital precoders, while fully-connected hybrid precoders perform poorly due to their severe insertion losses. also, the topology of the phase-shifting components offers an energy-spectral efficiency trade-off: active phase-shifters provide higher data rates, while passive phaseshifters maintain better energy efficiency."
"once receiving a query message from the dht layer, the node checks the local index storage to find if there exists the same resid as it receives. to reduce the network transmission cost, it only returns the top t indexes sorted in terms of the distance from the query, defined in (4)."
"on the other side, in order to fully utilize the chord id space and keep the load balanced, the consistent hash function sha-1 is employed to distribute indexes as symmetrically as possible."
"to satisfy these requirements, lrfir constructs indexes based on the image feature vectors and further improves the results by using the relevance feedback. to meet the first requirement, a set of lsh functions are employed to produce the image indexes. at the time of the system start, these hash functions are generated and used in all nodes. due to the locality sensitive property of lsh, it is more likely that the feature vectors of content similar images have the same hash values. then these hash values denoted by the integer vectors are mapped to resource ids. therefore, if two images are similar, they may share the same resource id. on the other hand, considering load balance, these indexes are distributed as evenly as possible. to satisfy the second requirement and improve the retrieval performance, lrfir employs the relevance feedback by leveraging the information retrieval technique, which can narrow the gap between low-level concepts and high-level features. in addition, the human can interact with lrfir to help refine the query. lrfir can learn the semantics of query images through iterative feedback and query refinement."
"relevance feedback technique, originated from the information retrieval, is employed in lrfir to narrow the gap between low-level features and high-level ones. the features extracted from an image are low-level features, such as color, shape and texture, while high-level features are human perception of images, i.e., semantics. however, low-level features cannot fully represent the high-level semantic concepts. moreover, there is no direct link between these two level features [cit] ."
"pir sensor is an electronic device that is used to detect an object's motion. this came for the fact that energy is emitted from all objects, which have temperature higher than absolute zero, in the form of radiation. pir sensor is basically designed to detect such radiation. the object's motion is captured by this sensor when the distance between them is equal or less than 6 meters. the setup time of this sensor is between 10-60 second [cit] ."
"cloud-based services rely on the datacenters which store massive data [cit] . therefore, it is important to choose appropriate topology to establish the high-performance datacenters which can satisfy the requirements of searching and analyzing large dispersive datasets. the datacenter architecture is categorized into two models: centralized and decentralized. most commercial cloud offerings are centralized. in such a model, resources in large datacenters are centrally managed and the management nodes become bottlenecks. moreover, since datacenters scale exponentially with the rapid growth of data volume, a single point of failure may occur due to fires, power outages, natural disasters, etc . to address this problem, decentralized datacenters are designed according to the peerto-peer (p2p) paradigm, which provide better scalability and adaptability. the p2p based datacenters can be built by connecting many individual peers, without any central monitoring or coordination components. each peer takes charge of part of data and replicas to improve the clouds' reliability. p2p techniques are very likely to be adopted in clouds [cit] )."
"in this paper, we present a novel cbir system called lshbased and relevance feedback for image retrieval (lrfir) for the large scale p2p datacenter. lrfir supports both content similarity search and semantic similarity search, which is different from the keyword search used in existing systems. the efficient index construction service and query processing service are proposed for lrfir."
"in the underlying dht layer of lrfir, the participating nodes are organized into a structured p2p network, chord [cit], without loss of generality, which also natively offers node join and leave mechanisms. therefore, lrfir can support efficient routing, due to the dht layer. the index publishing and query routing are automatically accomplished by chord. for each image, lrfir constructs a few index messages, each of which contains the resource id, the image feature vector and the ip address of the data owner. since only feature vectors whose dimensionalities are not very high are added to the indexes, each index will not cause significant storage overhead to the nodes. given a query message, it is only forwarded to a few particular nodes which are likely to store indexes of similar images. to achieve the objective of efficiently searching similar images for the query, the index construction and query processing should satisfy the following requirements: (1) the index of content similar images should be stored at the same peer; (2) the semantic search should be supported with the help of users."
"is always smaller than one, low-resolution quantization decreases the total channel power. another interesting feature of model (7) is that n ′ comprises the awgn plus an attenuated channel-filtered quantization noise term. therefore, n ′ is not guaranteed to be gaussian, as nothing is assumed about the distribution of e. the covariance matrix of the total additive noise is"
"in practice, several iterations of feedback are needed to improve the search accuracy. however, more iterations would bring about more network hops. in chord, searching from the source node to the destination node needs o (log n) network hops in a network of n nodes. so in our relevance feedback search, the lookup requires ro (log n) hops in r iterations. if r is large enough, the accuracy of the query can be improved while the network cost is also increased. in contrast, if r is too small, the network cost is reduced while the accuracy might also be decreased. so we have to make a tradeoff between the network cost and the search accuracy."
"bluetooth electronic app for switching electrical devices the buttons shown on the front page of the app (i.e. buttons inside the blue rectangle) are used to turn the relays on or off just like we have done with ir remote control. the button label (i.e. whether it is number1 or number2 etc. compared to the remote control) starts from left to right. for example, if we want to turn on the appliance connected to relay1 via this app, we need to press the first button on the left (this button is just like pressing number1 on the remote control). after pressing the button of the app (i.e. bluetooth electronic app), the smartphone sends a signal to the bluetooth module connected to the arduino. when arduino received the signal from bluetooth module, it turns on or off the device (via relay) based on the pressed button on the app."
"after image features are extracted, the question is how to construct resource ids for images and answer the query effi ciently. to improve the search efficiency, the content-based similar images measured in euclidean space should share the images. sobel operator is applied as the gradient operator, which returns two gradient images, i.e., the image along horizontal and the one along vertical. sobel operator is used to the red, green and blue channel. and then texture orienta tion image θ (x, y) is obtained. secondly, in the rgb color space, r, g and b are respec tively quantized into 64 colors, for simplification. and the quantized image is denoted by c (x, y) ."
"to the best of our knowledge, extensive performance evaluation of quantized hybrid transmitters with fully-and partiallyconnected psn under realistic rf modeling has not been considered yet. we thus aim at filling this gap by this paper. our main contributions in this direction can be summarized as follows:"
"to test this approach, we have sent a message contained 1 1 to the system. consequently, the devices connected to those relays have been turned on. similarly, when we have sent 0 0, the devices connected to those relays have been turned off. one of the important aspects of our system is that there is a good correlation between all the ways of controlling appliances in the receiving mode. that is, when a home appliance turned on, for example, by smartphone app, it can be turned off by gsm message or ir remote control and vice versa."
"as for future work, we plan to investigate the following issues. firstly, some sensitive information is being centralized into the cloud, so we may search over encrypted cloud data. secondly, image quality may not be very high, which can be improved by using the image pretreatment technique before processing the query. thirdly, we also plan to compare the performance of lrfir against other existing systems."
this piece of hardware is used to receive signal from the remote control and transfers the received signal into an understandable form by the arduino. the arduino then performs a specific task based on the received data from the ir receiver.
"let us first discuss the static power consumption of hybrid and digital transmitters. the transmitter rf front-end is formed by dacs for each i/q channel, rf chains, psn (hybrid systems), and pas. the considered direct conversion rf chain consists of two low-pass filters, two mixers, a local oscillator (lo) shared among all chains, and a 90"
"energy efficiency is an important concern in the design of mmwave massive mimo systems. large antenna arrays are employed to compensate for the increased path loss of mmwave channels and to achieve high spectral efficiency. however, several electronic components such as dacs, phaseshifters, and power amplifiers become inefficient when operating in the millimeter wave range over large bandwidth, making radio systems equipped with such arrays expensive due to large power consumption and losses of their rf front-end. therefore, there has been important research efforts on designing systems that not only maximize its spectral efficiency, but also its energy efficiency, which can be defined as the ratio between the system capacity and its power consumption [cit] ."
", where p lo, and p dac (b dac, f s ) denote power consumption of a single local oscillator, and dac with b dac bits of resolution sampling at f s hertz, respectively. p pa stands for the power consumed by all pas. a hybrid transmitter with fully-connected psn, by contrast, employs l t dac/rf chain pairs, and n t power amplifiers. a total of n t l t phase-shifters are used in this architecture, thus the total power consumption is"
"the vector quantizer simply consists of applying scalar quantization to each input vector entry. unlike the lloyd-max quantizer [cit], the considered vector quantization approach is not optimal in the mse sense."
"let us define the quantization operation in this work to model dac and introduce its linear approximation. we define q b (·) as the uniform scalar quantizer that operates independently on both real and imaginary components of the input. are listed in [23, table ii ]. such coding suits our modeling since the baseband precoded signals are still gaussian. for vector inputs, we define"
"in the index construction service, lrfir leverages an image feature extraction algorithm called multi-texton histogram (mth) [cit], which combines the texture and content providers fig. 1 image retrieval in the p2p datacenter color feature. thus, each image's content can be represented as a feature vector from which the content similarity can be measured quantitatively. accordingly, we employ a set of locality sensitive hashing (lsh) functions [cit], which convert feature vectors of similar images to the same hash value with high probability, owing to the locality-preserving property. the hash value denoted by an integer vector is mapped to a resource id without destroying the locality-preserving property. in this way, the indexes of similar images are more likely published into the same node in the dht layer with high probability. when a query is issued, the query processing service produces a set of resource ids from the query's feature vector by using the same lsh functions. query messages are then forwarded to the nodes responsible for the ids. to support semantic search, the relevance feedback technique [cit], originated from information retrieval technique, is adopted to overcome the gap between low-level features and high-level features. it allows users to iteratively refine the result by marking a set of relevant and non-relevant images so that lrfir can learn the semantics of the query image. finally, we implement a prototype system based on next generation service overlay network (ngson) [cit], in which different functional overlays can systematically be coordinated with each other. it is used to evaluate the performance of our algorithm in two image datasets, i.e., corel 10000 [cit] ) and the subset of catltech 101 object categories [cit] ). the experiments show that our algorithm achieves high recall rate and precision with only a small number of lookup hops. moreover, query results can be further improved by using the relevance feedback technique."
"in this section, we introduce the quantized hybrid precoding problem, and present precoding strategies to solve it. analog precoding methods for fpsn and ppsn are presented in sec. iii-a, baseband precoding is defined in sec. iii-b, computational complexity analysis is conducted in sec. iii-c, and achievable rate lower bounds are derived in sec. iii-d."
"the number of hash tables, m, is a system parameter. for ics, it also represents the number of indexes for an image and has an impact on the query efficiency and communication cost. as above discussed, more hash tables can provide better chance of finding the images that are similar to the query. however, more hash tables means more index messages to be published and requires more storage. so we should make a tradeoff between m and the query efficiency."
"sim900a is the dual band gsm/ general packet radio service (gprs) system and its operation frequencies are 850 mhz, 900mhz, 1800 [cit] mhz. the connection between this module and pc or microcontroller is made with the help of rs232 and serial cable. internet access, voice call, and sms are the functions performed by this module [cit] . figure 3 shows the gsm module used in our system."
"where p ps denotes power consumption of a single phase-shift element with b ps bits of resolution. in general, the power consumption of dividers and combiners is negligible. transmitters with partially-connected psn have n a phase-shifters per subarray, i.e., there are n a l t phase-shifters in total, hence their power consumption is given by"
"in this section, we present an overview of lrfir framework. it is expected that the novel framework should support cbir in p2p datacenters storing a large number of data. under such an environment, the simple solution of traversing all the participating nodes for an image query is impractical, due to the high communication cost. similarly, establishing and maintaining a central index of all the shared images can lead to scalability and reliability concerns [cit] ). hence we propose a scalable scheme that distributes the indexes of content similar images to the same node and returns approximate answers by only visiting a small number of nodes. besides, relevance feedback technique is also adopted to support semantic search."
"the final results chosen by the user can be cached in local database for future query reuse. ics can also construct indexes for these result images and publish them to chord. when a query is issued, lrfir first checks the caches. if there are no suitable answers in the caches, the qps is invoked to publish query messages to chord. on the other hand, the feedback process can be terminated by the user, when the results are not improved. in this way, the expensive cost of query processing can be saved."
"for controlling the electrical devices through bluetooth, a smartphone is connected to the system and a special app so-called bluetooth electronic is used as shown in figure below."
"in lrfir, texture descriptors are used to extract the visual feature of an image. the texture describes the granularity and repetitive patterns of surfaces within an image [cit] . a novel and efficient method called mth [cit] ) is adopted to represent the texture feature. mth explores the spatial correlation between neighboring colors and the one between neighboring texture orientations. then it takes advan tage of the histogram and co-occurrence matrix to improve the texture features. compared with mth, other approaches such as machine learning techniques [cit] generally train some examples to learn a classifier which should be kept consistency in all the nodes. so these approaches are impractical in the distributed environment."
"after receiving the signal, arduino sends out instruction to the electrical device through selected relay. finally, the electrical devices will be turned on or off depending on the pressed number on the ir remote control."
"most current work about image retrieval in the p2p paradigm assumes that images are described in text by users [cit] . the searching of images is only based on their names and mainly relies on keywords matching. however, since it is difficult to annotate images very exactly, identifying of images in this way is inaccurate and cannot satisfy users' requirement in some cases. in some applications, users may request inexact queries such as \"find the top-a: images which are most similar to a given sample\". however, it is difficult for humans to describe how an image is similar to the given sample with keywords [cit] . the content based image retrieval (cbir) can find similar images through sample images instead of keywords. but most work in cbir needs the global information in a centralized fashion, which does not scale well in the distributed situation [cit] . therefore, our objective is to design a system which can process smart queries and improve search performance in terms of precision and recall rate, without any global information."
"the corresponding recall rates are evaluated with different number of hash functions and top images, as shown in fig. 4 . the x-axis represents the number of top images, varying from 4 to 24 for both datasets. the y-axis denotes the recall rates measured under different number of top im ages. m and k respectively represents the number of hash tables and buckets. the recall rates increase as the number of top images increases for both datasets. but not many images are returned, because only the very similar images are needed to return. this facilitates convenience for people in browsing the results for the feedback. in lrfir, 12 top images are defined as the default returned number."
"hybrid analog/digital (a/d) transceiver architectures have been proposed to enable mmwave massive mimo systems [cit] . they employ digital filtering (precoding/decoding) at baseband, and perform beamforming in the radio-frequency (rf) domain by analog components. the most popular implementation of this rf beamformer consists of an active phaseshifting network (psn) connecting the outputs of the baseband filter to the antennas, which is known as fully-connected psn. this implementation, however, is associated with a large power consumption as a considerable number of active phase-shift elements is required. as an alternative, one can employ subarray beamforming, reducing the number of phase-shifters, and, consequently, power consumption. it has been claimed that hybrid precoding provides a throughput close to that of fully-digital systems [cit] . however, insertion losses of rf hardware are usually disregarded in the analysis of such hybrid systems. if these losses are not properly compensated for, then their spectral efficiency might be much smaller in practice than what is expected."
"conducting performance analysis of a mimo system in terms of the exact non-linear quantization model presented above would be challenging. among many reasons, we can point to the difficulty of calculating the statistics of the transmitted signals. as alternative, we resort to a linear approximation of the scalar quantizer output applying the aqn model [cit]"
"in this section, the query processing service (qps) is discussed, supposing that all the image indexes are published into the dht layer. when a node issues a query, qps is invoked and the content-based similar images are retrieved. the objective of this service is to answer a query effectively. search effectiveness is measured by the quality of search result, i.e., recall rate and precision. furthermore, once the query iteration is completed, the user interacts with lrfir to help improve the search result through the relevance feedback technique. after several iterations, the search accuracy can be significantly improved."
"the effect of relevance feedback on the search performance is measured, as shown in fig. 7 . the horizontal axis represents the number of hash tables, m, which varies from 1 to 15 in fig. 7a and from 5 to 20 in fig.7b, respectively. the vertical axis is the average recall rate and the average precisions measured for both datasets. in this experiment, 100 nodes are randomly selected to issue queries. and each node needs to issue all the queries. for both datasets, the recall rate and the precision grows slowly as m increases, regardless of the feedback iteration. the recall rate grows slightly when m is greater than 5 for the catltech 101 object. and the precision increases slightly as m is greater than 10 for the corel. this is because when m is large enough, the images with their feature vectors similar to that of the query have already been gathered into the same cluster. as a result, simply increasing m cannot improve the recall and the precision."
"in the following, we present analog precoding methods for the fully-and partially-connected psn topologies. since f rf might have very large dimensions, the computational efforts of precoding design can be significant, and, thus, we focus on low computational complexity instead of high spectral efficiency."
"hybrid precoding with fully-connected psn exhibits important power losses mostly due to the large number of phaseshifters, power dividers and combiners used in the massive mimo setup. such power loss is not easily compensated with present mmwave pa technology, causing severe spectral and energy efficiency degradation. as an alternative, hybrid precoders equipped with partially-connected psn are shown to be energy-efficient in high snr regimes. our results revealed that phase-shifting topology offers an energy-spectral efficiency trade-off. active phase-shifters favor higher throughput, while passive elements aim at energy efficiency. finally, the proposed power consumption model suggests that computational power spending should be considered in the power budget of very-large array systems. the present work considered a pointto-point mimo system for simplicity reasons. introducing interfering users, performing transceiver optimization, and designing efficient hybrid schemes are envisioned as future work."
"when a node wants to share an image, the feature vectors of the image are automatically extracted. lrfir adopts motion picture experts group-7 (mpeg-7) descriptors, which represent visual contents with feature values. the mpeg-7 standard provides the multimedia content description interface, which includes a set of descriptors, such as color, shape and texture descriptors, to support image retrieval [cit] . these visual descriptors represent human visual perception as feature vectors to evaluate the similarity of two fig. 2 interactions between key components of lrfir images in appearance. the index construction is based on the visual feature space."
"in lrfir, relevance feedback is incorporated to further improve the retrieval performance. to simplify the procedure, non-relevant images are ignored. in each iteration, after the user marks a set of relevant images that are semantically close to the query, the query refinement is executed according to the feature vectors of relevant images. based on the user's feedback, eq. 9 is used to modify q. after the new query vector q' is generated, the qps is invoked again. qps produces the query messages for q' as above described. then the new query message is sent to the dht layer."
"when the number of nodes is 1,000, the percentage of indexes kept in each node is shown in fig. 6 . obviously, some nodes store more indexes than others. that is the reason why the percentage of indexes in fig. 5 does not grow very steadily as the percentage of nodes increases. the maximum percentage values for both datasets are 7% and 3.5%, respectively. besides, the percentage of indexes kept in many nodes is very low or close to zero."
"similar to ics, the number of query messages also depends on the number of hash tables m. if m increases, more query messages need to be published, which causes high query cost. but increasing m also increases the probabilities of finding the images similar to the query. in contrast, if m is too small, the query cost is reduced while the accuracy might also be decreased."
"the quantized hybrid precoding problem is based on system model (9) assuming channel state information at the transmitter (csit). it consists of finding the precoding matrices f rf and f bb that maximize the achievable instantaneous rate r [cit], i.e.,"
"unfortunately, finding the digital precoder f bb that maximizes (13) for fixed f rf is not straightforward. the svd precoding with water-filling power allocation is not guaranteed anymore to decompose the mimo system into orthogonal subchannels due to the structure of the total noise vector n ′ . furthermore, in order to form h ′, one needs knowledge of the diagonal elements of r uu, which depends on f bb ."
"in this mode two relays play an essential role. these relays are used to switch on and off the home appliances through the low-power arduino output. in terms of distance, this mode is categorized as local and global remote control. in the former case, infrared and bluetooth communication links are used to control the home appliances via ir remote control and smartphone app respectively. based on our program, the ir remote control works as follows:"
"is formed by the quantization distortion factors ρ b,m associated with the m-th quantizer input-output. from the definition of ρ b, it follows that the covariance matrix of the quantization error vector is given by:"
"cloud computing enables users to flexibly access reconfigurable computing resources without the burden of managing and maintaining the resources [cit] . the paradigm has brought about the essential characteristics including reliable and infinite storage capacity, data access independent of locations and time, and dynamical resources provision in a multi-tenant way to avoid costly wasting [cit] ). due to these features, cloud computing becomes prevalent in the distributed storage and retrieval field. on the other hand, constructing a robust storage model is also a driving force for the development of cloud computing. in fact, the datacenter provides a substrate for high capacity storage models and modern internet applications. from cloud providers' viewpoint, datacenters provide the illusion of unlimited and powerful information storage, with the purpose of offering on-demand high quality applications, e.g. amazon's s3 and microsoft's skydrive and live mesh. from cloud consumers' viewpoint, large datacenters can provide an online service running on the cloud, which permits fast data access."
"in addition, although the resids of the new query q' may be equal to these of the previous query q, query messages containing the same resids are still sent. and the query processing is the same as above described. after the query vector is refined, the image feature vectors close to the new query vectors are retrieved. they are semantically closer to the query image."
"nowadays, security is one of the crucial issues in the world. home security system has altered significantly over the last decade and it is expected that the change will be continuing in the coming years. burglary and other household thefts have been increasing noticeably. besides, safety from gas leakage, fire and controlling appliances remotely have become the essential requirements for the modern homes, offices and buildings. therefore, currently used security systems, which mainly rely on video monitoring, need to be enhanced to meet the acceptable level of security [cit] . to solve the above-mentioned limitations, a smart home system has been proposed. the proposed system allows the user to obtain both security and safety. in security section, unauthorized person is detected using passive infrared (pir) sensor and arduino uno. then, the property owner will be informed by short message service (sms) through global system for mobile communication (gsm) networks. this system aims to prevent accessing unapproved people to the property while the landlord is not at home. in the safety section, gas and smoke sensors are used to ensure the property is safe enough. if any of the abovementioned sensors is activated, the landowner is notified directly with the help of gsm module by sending sms to the authorized party. additionally, the home appliances can be switched on and off remotely through the system in three ways, namely sending sms from the registered number to the system, using smartphone app, and infrared (ir) remote control."
". inequality (17) shows that quantization noise dominates over awgn in high snr regime, leading to saturation of the system capacity. the obtained achievable rate expression at low snr shows that quantized system capacity is always smaller than that of unquantized systems due to the factor (1 − ρ b ) inside the logarithm."
"it is worth mentioning that the sensors, bluetooth module, and ir module are connected to the input pins of the arduino since they take signals from outside into arduino. while the relays, buzzers and laser are connected to its output pins since these components are derived by the signals coming out from the arduino."
"now we conduct an asymptotic performance assessment of the quantized hybrid precoder. let us consider the received signal model with gaussian noise approximation and hybrid precoding: y ≈ 1 lrf h ′ f bb s + n g, then we employ svd combining (with respect to h eq ) at the receiver, and finally decorrelate the gaussian noise vector n g to obtain an achievable rate lower-bound. from equation (15), it follows that the rotated received signal can be written as"
"three types of sensors are used in this mode these are pir sensor, gas sensor and smoke sensor. to make the system more realistic, two lasers and one piezo buzzer have been used in the system. in the first step we have tested the pir sensor as shown in figure 12 . when an object's motion has fallen into the working range of the sensor, the sensor has been activated and has sent a signal to the arduino. then the arduino has sent relevant instruction to the gms module, which has an active sim card in it, to inform the authorized party by sending sms to the registered number. meanwhile, the two lasers have been flashed and the piezo buzzer has turned on. similarly, the two other sensors, gas sensor and smoke sensor, have been tested by methane gas and smoke respectively. consequently, the lasers and the buzzer have turned on and the mobile phone has received the notification messages at each case as shown in the following figures. the figure above is a complete result of the transmitting mode of the system when all the sensors (i.e. motion, gas, and smoke) have been activated. in such case the arduino receives the signal from the all sensors. after manipulating the received signals, it sends instruction to the gsm module to notify the homeowner about each case. as a result, the mobile phone has received a message that shows all the sensors have been activated instantaneously."
"the number of buckets in each hash table, k, is another system parameter. it impacts not only the query efficiency but also the load of nodes. fewer k means that more images are clustered to the same hash value, i.e., the same resid. this can lead to fewer clusters and accordingly each cluster has more images. once a resid is located, more relevant images can be obtained. on the other side, the node in charge of the resid stores more indexes, if k is too small."
"regarding information retrieval in the decentralized structured p2p paradigm, there are many studies in this issue, such as mcan [cit], m-chord [cit], psearch [cit], prism [cit] ) and idisque [cit] b ). mcan using can as the underlying structure adopts a pivot technique to map data objects to n-dimensional vectors . but the chosen pivots are preprocessed in a centralized fashion, and then distributed to peers. m-chord takes the advantage of the idistance which maps objects into one-dimensional space. but its data clustering and mapping are still completed in a centralized model. in psearch, the latent semantic indexing (lsi) is used to generate a semantic space. then, this space is mapped to a multi-dimensional can which has the same dimension as the data space. however, different overlays may have different dimensionalities, since the dimensionality of can depends on the dimensionalities of various datasets. and lsi still works in a centralized fashion. in prism, it stores multiple indexes for one object in many chord peers based on the distances between the object's vector and the reference vectors, so that the indexes of similar object are clustered to the same peer. but reference vectors are still chosen in a centralized fashion, which is not well suited for large datasets. [cit] generate the same index for semantically close files by using lsh and vector space model (vsm), with the purpose of answering queries by only visiting a small number of nodes. but these hash values are directly used as resource keys, which destroy the load balance of chord. in idisque framework, the data on each peer is clustered, and then lsh functions only map cluster centers to chord resource keys. the key of a cluster center represents the data in the cluster. however, the hash values of queries may be not equal to these of cluster centers."
"all the images of the local database in the node are reevaluated periodically, e.g., once a week or a month. if an fig. 3 publishing the indexes and the query image image is added or deleted, its indexes are constructed or removed. depending on the similarity between the modified image and the original one, we can determine whether or not the indexes should be reconstructed. if the similarity between these two versions is less than the threshold, the indexes remain unchanged. otherwise, the ics is invoked to re construct the indexes."
"we propose an effective framework to support cbir in the distributed cloud datacenter. lrfir supports both contentbased similarity search and semantic search. the ics constructs indexes based on p-stable hash functions, where the content similar images are mapped into the same resource id and distributed to the same chord node, with high probability. the qps not only publishes the query message, but also employs the relevance feedback to refine the initially query vector. therefore, the gap between low-level features and high-level features are overcome. the experiments show that our approach achieves high recall rate and good load balance, and it only needs a small number of network hops."
"relay is an electronically operated switch which is used to open and close another switch which has ability to carry lager amount of current [cit] . in this paper we use relay, shown in figure 4, when we want to control high power appliances, such as tv and air condition via low-power arduino output signal. the relays are connected to the arduino output pins while the home applicants are connected to the output of the relays."
"to make our model consistent with power constraints present in real-world implementations, we incorporate nonlinearities due to the dac stages and rf losses caused by analog beamforming. the non-linearity introduced by dacs is modeled as a quantization stage. this is because the continuous-valued dac output signal has its amplitude wellrepresented by a finite set of values generated by the dac hold circuits. the rf losses are accounted by considering a power loss factor"
"in this paper a cost-efficient security, safety, and home automation system have been proposed, designed and implemented. the proposed system allows property security/protection by using pir sensor, arduino and gms module. the system also provides remotely controlling electrical devices at home through text messaging, smartphone app, and ir control. therefore, the system meets comparatively higher level of user satisfactions since they can protect their homes and control their electrical devices regardless where they are. the most important aspect of our system is that it informs the property owner about any events, such as intrusion, fire or gas leakage, in real time which is not the case for the traditionally used video-streaming based system. the current system can be further improved by adding ip camera to it. in that case if any unauthorized movement or gas leakage or fire is detected, the system will capture the event(s) then sends the picture to the authorized parties through a predefined way such as email."
"nowadays, since smart phones, tablet computers and many lightweight devices have been penetrating into our lives, millions of files including images, videos and plain texts are transferred into datacenters. figure 1 shows an application scenario of image retrieval in the p2p datacenter. content providers can be cloud providers or cloud customers. the content providers upload their significant resources to p2p cloud datacenters. in addition, cloud customers can also upload interesting images to their facebooks or microblogs deployed in cloud datacenters. when an authorized cloud customer issues a query request, it is sent to the datacenter, which takes charge of search processing. afterwards, query results are sent back to the cloud customer. however, it is challenging to locate files in such a distributed datacenter storing a large amount of files. as a case of study, we present this image retrieval application implemented on the p2p datacenter. although throughout this paper we focus on image retrieval, our methods are applicable to multimedia retrieval domain where similarity search is performed in a p2p paradigm."
"after the query node receives all the results, it merges the results before showing them to the user. the merging processing contains two steps. first, it eliminates the duplicates. if two indexes have the same f v, we consider that they might represent the same image and one of them will be randomly selected. second, all the results are sorted in terms of the distance defined in (4), and the top t indexes are chosen. then connections are established between the query node and data owners, and images can be transmitted to the query node. finally, the top t most similar images are showed to the user."
", and l c (l t ) denote the static power loss introduced by each phase-shifter (active or passive), the loss of all n t -port power dividers, and l t -port combiners, respectively [cit] . partially-connected psns, however, use only"
"in this paper, our main contributions are as follows: (1) the image feature vector is integrated into dht for implementing an efficient indexing and locating approach. the index construction is based on the image content represented by the feature vector instead of keywords; (2) the characteristic of lsh is exploited to place the similar images to the same node without any global information, and the query is only sent to the nodes which are more likely to answer it. in this way, the communication cost is reduced while the result accuracy is guaranteed; (3) we introduce the relevance feedback to the p2p model with the purpose of supporting semantically close image search. this approach allows users to interact with lrfir to refine the query vector; (4) we evaluate our approach using two real-world image datasets and demonstrate that lrfir is very effective."
"the query processing service adopts the similar way to generate the resource id and the query message. then the query message is routed through the underlying dht layer using the resource id as the destination. once receiving the query message, the destination node invokes the local search method which checks the local index storage according to the resource id and then returns the top t indexes. after the user surface of the query node merges all the results obtained from the participating nodes, the final results are shown to the user. furthermore, the query refinement method is used for relevance feedback. the query refinement re-computes the query vector according to the relevant images the user chooses. and the new query message is generated and routed through the dht layer again. the relevance feedback iteration stops when the user ends it."
"the architecture of the proposed system is shown in figure 8 . as can be seen that the core part of the system is an arduino board which receives the signals from all the sensors connected to its inputs then sends the instructions to the gsm module and turns on the lasers and the buzzer. when the system works as home automation system, the gsm module tells the arduino what to do based on the content of the received sms from the registered number. in our system the home appliances can be controlled not only through gsm network, but also mobile app and remote control. the electrical devices connected to the relays can be turned on and off in either way."
"in figure 4, the performance of the hybrid precoders is shown considering rf hardware losses. in this scenario, hpf performs worse than hpp because its insertion loss is larger than that of the other architecture, which causes smaller effective transmit power, and spectral efficiency. when rf losses are taken into account, the phase-shifting implementation becomes important. as shown in figure 4, precoders with active phase-shifters are more spectral-efficient since these components present smaller insertion loss compared to passive phase-shifters, however they come with increased power consumption. snr losses of 25 db observed in figure 4 have also been reported in other works [cit] . such large losses occur due to the very large number of lossy rf components employed in the massive mimo setup."
"the need of efficient mmwave systems fueled the development of new electronic components. one can find in the literature different parameters for the electronic devices considered in our models. we adopt an optimistic parameter selection approach, in which we choose the most efficient implementation reported. results obtained with this approach ought to provide an idea of what to expect for future mmwave massive mimo systems. regarding the power consumption of the rf chain components, we consider values reported in [32, chapter 5] for the 90"
"another energy-efficient approach to mmwave massive mimo consists of using low-resolution digital-toanalog/analog-to-digital converters (dacs/adcs) [cit] . at the receive side, the high-resolution adc chains are the most power hungry part, motivating the application of lowresolution devices to reduce their power consumption. at the transmit side, however, power expenditure is dominated by power amplifiers (pas), which are usually required to operate within the high linearity regime to avoid distortion of the signal constellation. employing low-resolution dacs relaxes the linearity requirement, allowing the amplifiers to operate closer to saturation, thus increasing their efficiency."
"in this section, the effectiveness of load balancing is investi gated in two datasets. figure 5 shows the index distributions for different cases. the x-axis values are the percentage of nodes, whose ids are along chord ring from small to large. and the number of nodes n varies from 10 to 5,000. the yaxis values are the percentage of indexes assigned to these nodes. note that in both datasets, curves show much less skew as the number of nodes increases. that means that the load is more balanced with the increase of the number of nodes. this is because when the number of node increases, the interval between node ids becomes smaller and more nodes are assigned to store these indexes. therefore, there are fewer indexes in each node. in fig. 5, when the number of node is 1,000 and 5,000, the load is more balanced than other cases for both datasets. however, for the corel10000, when n is 10, the load is skewed and 40% of nodes stores around 60% of indexes. the reason is that the number of nodes is so small that the interval between node ids becomes large. therefore, some nodes store much more indexes than others."
"where a is a j-dimensional vector whose elements are chosen independently from the 2-stable distribution. b, a real number, is randomly selected from the range [0, w]"
"regarding the effect of relevance feedback, we take the corel for an example. catltech 101 object has the similar conclusion. the retrieval rates can be improved by increasing the number of feedback iteration. \"0-iteration\" curve represents the average precision without any relevance feedback. \"1-iteration\" curve represents the average precision when one feedback iteration is applied to improve \"0-iteration\" result. \"2-iteration\" curve shows the average precision when another feedback iteration is applied to refine \"1-iteration\" result. and so on. obviously, we can see the improvement of \"3-iteration\" in terms of precision compared with \"0-iteration\" for the corel. however, in catltech 101 object, \"3-iteration\" has almost similar recall rate as \"2-iteration\". in the corel, \"3-iteration\" is a little better than \"2-iteration\" in terms of precision. in short, we can see that the required number of feedback iteration is small."
"most of recent works on massive mimo focus on analyzing either the performance of energy-efficient full-resolution hybrid or low-resolution fully-digital transceivers. in the following, we discuss their contributions."
"the interactions among key components of lrfir are illustrated by fig. 2 . lrfir is located between the user and the dht layer, which contains two services of index construction and query processing. we first discuss the index construction service. each node has a local image database, where images are shared with others. in addition, each node also has an image feature extractor which consists of a set of feature extractors specific to different image formats. the image feature extractor accesses each image in the local database, which adopts mth algorithm to analyze the image's texture feature and compute its feature vector. for the feature vector, a set of lsh functions are adopted to compute the hash values and determine the number of indexes for each image. for each hash value, the index construction generates the resource id and publishes the index message to the dht layer. once receiving an index message from the dht layer, the node inserts it in the index storage according to the resource id. in addition, the image indexes in the local database are refreshed after a period of time to ensure the validity of resources."
"the capacity expression in (13) provides some insights on the performance of the quantized hybrid precoder. in order to discuss them, consider the actual transmited power"
"t ) flops are necessary to compute the fully-connected analog precoder, where i denotes the number of iterations of the alternating minimization method. the total cost is then given by the baseband and analog precoders computation cost n (4n t n a +2n a +1), where j represents the number of iterations of the power method. the total computation cost of the partially-connected hybrid precoder accounts for the calculation of h eq and its svd. therefore, it is given by n, we observe that the latter is less complex than the former because the partially-connected structure is exploited to reduce the analog precoding computational complexity."
"in the latter case (i.e. global remote control), the sms is sent from the mobile phone to the system through gsm network and the system response (whether turns the electrical devices on or off) is based on the message content. the message should have the following structure:"
"the implemented system shown in figure 11, has been tested thoroughly and the obtained results have been presented and discussed in the following steps: as we mentioned above, the system works in two main modes namely, transmitting mode and receiving mode."
"the p2p network is categorized into three models: unstructured, hybrid and structured. the organizing structures and routing mechanisms for information retrieval in the p2p network are also applied to the image retrieval."
"the purpose of this service is to generate the same resids for the similar images with high probability, and then publish the indexes to particular nodes through the dht layer. that is different from the traditional location approach, where dhts access an image through the hash key of the image name annotated by the human. thus, the indexes of the similar images are randomly distributed across the dht. as a result, it is difficult to guarantee the search accuracy. in this paper, we propose the index construction service (ics) which adopts pstable lsh to preserve the locality sensitive property and distributes the indexes to the chord as evenly as possible."
"the proposed system in this paper is generally divided into two parts. the first part concerns with security and safety in which the system works as a transmitter. in the second part, the system can perform home automation functionalities and works as a receiver. then, both parts are integrated to create a reliable smart-home system which has security, safety and home automation functionalities. the primary components used in the system are explained below."
"in the considered power consumption model, a simple implementation for a 1-bit adc is presented, leading to negligible power consumption. the considered analog combining strategy consists of beam scanning, while baseband combining is based on svd processing with water-filling power allocation, as in the previously mentioned works. results suggest that digital combining is more efficient than hybrid combining especially in the low snr regime. however, the contributions mentioned above impose low-resolution only at the receiver side, assuming fully-digital or hybrid transmitters with highresolution dacs."
"the aqn model approximates a non-linear deterministic operation as a linear stochastic process by assuming zero correlation between the quantizer input and the quantization noise. similar results are obtained by using the bussgang theorem [cit], which states that the cross-correlation function between two gaussian signals taken after one of them has been non-linearly distorted is proportional to the cross-correlation function before distortion. if a linear model, e.g. (3), is used to approximate the non-linear distortion, this theorem implies that the approximation error is uncorrelated with the input, i.e., e[ue"
"bluetooth module is a bluetooth serial port protocol (spp) module which has 3mbps data rate and operates on 2.4ghz. this module is designed to provide short range serial communications; therefore, it can be easily used to interface with arduino [cit] . we use this module when we want to control electrical devices through smartphone app."
"relevance feedback intends to model the high-level image semantics through iterative feedback and query refinement. humans are engaged in this alternative search process and help lrfir to learn the semantics of query images. when one search iteration is completed, the user picks up relevant images and non-relevant images to update the previous query vector. in lrfir, we choose rocchio's formula [cit] shown in (9), where the image's semantics is captured by a set of weights. the latest query vector q can be refined by assigning higher weights to the relevant terms and lower weights to the non-relevant terms. it provides a query point movement approach, which moves the query point towards positive samples and away from negative ones in the vector space."
"despite the fact that harvesting the energy by the surrounding rf transmissions can increase the lifetime in wireless devices [cit], it is not able to provide enough power to counterbalance the consumed energy in realistic scenarios. this is mainly due to the path loss between the transmitters and the receiver and the losses from the rf-to-dc conversion. however, with the use of dedicated power transmitters or power beacons (pb) distributed in the hospital, it is possible to solve the aforementioned problem and provide the wearable devices with sufficient energy to sense, process and transmit. to ensure that the devices will always have enough energy to operate, they employ the harvest-then-transmit protocol with which the wearables harvest energy for a certain period of time and then consume all of it for measurement and communication [cit] ."
"in fig. 4, we demonstrate the probability c n versus the number of pbs per sector. as it can be observed, by increasing the number of beacons the probability c n is also increasing up to a saturation point (i.e., here at ∼ 60 beacons per sector. this happes due to the fact that as the beacons increase, the ability of the wearables to have enough energy to transmit during the cp is also increasing. however, after a certain point, all wearables have enough power to transmit, therefore adding more pbs at the sector will not benefit the network performance, which is an important insight from a system design perspective."
"to begin with, in order to achieve a correct notification, there are three prerequisites that should be satisfied: i) the wearable should have collected enough energy during the hp to surpass the energy threshold θ, ii) the gateway should decode successfully the transmitted message from the wearable, and iii) the gateway should successfully deliver the wearable's message to the nearest medical personnel device. in the following, we will provide each prerequisite and, then, we will combine them into the probability c n ."
"therefore, there are no works in the literature on clustered networks with weh in the wireless devices, which is an interesting combination given the realistic modeling and the convenience of weh in health applications. in our work, we attempt to cover this area by studying the communication performance of wearables in a hospital environment, where the patients are distributed according a poisson cluster process. each wearable transmits its messages to the cluster center, where a gateway is located that notifies the nearest medical personnel. moreover, the wearables are able to harvest energy from pb transmissions to charge their batteries. our contribution is twofold: i) we derive the probability of correct notification for a clustered network with weh-enabled wearables, and ii) we provide a performance evaluation of the network and insights for the network performance. we believe that our results can act as a guide for the network design and the choice of the appropriate network parameters, according to the needs of each hospital."
"wearables in medical environments can provide an unobtrusive, scalable and relatively low-cost way to monitor patients in hospitals. however, they still need to guarantee a reliable communication and high lifetime. as typically multiple patients occupy each hospital room, we considered that wearables form clusters and that harvest rf energy via power beacons to increase their lifetime. we analytically derived the probability that weh-enabled wearables forming clusters in a hospital environment will successfully notify the medical personnel via a gateway at the cluster center. we validated our analysis through extensive simulations and showed that wearable transmission power over 10 dbm degrades the network performance, while adding more than 60 [cit] m 2 sector does not enhance the notification probability. in the future, we plan to extend this work by assuming a more realistic topology that considers peculiarities of indoor environments. in this way, the interference from the other clusters will be minimized, but the ability of the gateway to deliver successfully its messages will deteriorate."
"since our multiscale vector representation decomposes a volumetric object in a semantically meaningful way, this decomposition provides a natural guidance to the workflow one needs to follow when creating a digital volumetric object in our representation. the content creation process can be divided into a planning stage and a creation stage. during the planning stage, a user needs to rely on his/her prior knowledge of the object to be modeled to conceptually decompose the object in a hierarchical way. the following questions need to be answered."
"nevertheless, constructing high-quality digital models of natural organisms and materials with complex volumetric properties is an extremely challenging task. first, how can we compactly represent volumetric structures and patterns spanning a wide range of scales? taking the human body as an example, it has a skeleton, organs and tissues at the macroscopic scale, cellular structures and neuronal fibers at an intermediate scale as well as proteins and genes at the molecular scale. second, how can we represent high-frequency features in a resolution-independent way? physical and appearance properties (e.g. color) change abruptly across different materials or volumetric components. such discontinuities typically form thin surface sheets, which may join or split following an irregular pattern, giving rise to a non-manifold structure (figure 2 ). to prevent visual artifacts when zooming into these high-frequency structures, a resolution-independent vector representation is desired. a volumetric object representation not only needs to depict complex multiscale structures, but also should be easy to use, which means it should be easy to create novel objects and easy to view existing objects in this representation. thus, a volumetric object representation should have the following desired properties:"
"as far as the second issue is concerned, it is imperative to guarantee that the energy requirements of the wireless wearables will not impose any limitations to their operation. to avoid the inconvenient and time consuming battery replacing or cable charging, a new method has been proposed lately that exploits the energy of radio frequency (rf) transmissions to increase the lifetime of devices [cit] . this method, called wireless energy harvesting (weh), can be an effective solution as rf signals are nowadays in abundance. with weh, the rf signals are converted to direct current (dc) electricity using rectifying antennas and, if the amount of received energy at a temporary storage unit, e.g., a capacitor, is at the same level as the consumed energy, it is even possible to achieve a self-sustainable operation."
"comparison with vector solid textures. our work was partially inspired by vector solid textures [cit] . nevertheless, there exist significant differences between sdf trees and vsts."
"an important limitation of a single signed distance function is that it can only divide a volume into two-colorable regions [cit] . this limitation can be overcome by increasing the number of sdfs. for example, two sdfs can already divide a volume into four-colorable regions. thus, multiple sdfs have the capability of representing non-manifold structures, where more than two regions are simultaneously adjacent to each other."
"lately, wearable sensors have emerged as a comfortable and affordable way to monitor vital signs and activities in a nonintrusive way [cit] . many applications in fitness and wellness motivate people to improve their health and log their daily performance by tracking their activities, exercise and sleep. even though commercial wearables were mainly employed for such welfare applications, they are nowadays starting to undertake more critical tasks for health monitoring, such as measuring blood pressure, oxygen saturation, blood glucose level and fatigue level, periodically and without any human intervention. these current advances in wearable sensors technology enable their use in medical environments, providing an unobtrusive, scalable and relatively low-cost way to monitor patients in hospitals or elders in nursing homes and notify instantly the medical personnel in urgent situations."
"the key idea is to decompose a complex object into individual volumetric components with respect to its multiscale structure, and preserve the boundary shape of each component in a resolution-independent way. this is achieved by hierarchically dividing an entire volumetric object into multiple nonoverlapping regions using multiple continuous signed distance functions (sdfs). these continuous sdfs collectively deliver a powerful resolution-independent vector representation for complex region boundary surfaces that contain non-manifold structures and non-differentiable features such as creases and corners. the hierarchical decomposition organizes all resulting regions as leaf nodes of a binary tree, which we call an sdf tree. sdf trees support fast random access because signed distance functions can determine region memberships efficiently. we further introduce object embedding and instancing in our vector representation to compactly represent internal structures with a wide range of scales. volumetric objects at smaller scales can be embedded into the regions of those at larger scales by linking together their respective sdf trees."
"also, the gap size of 32 occasionally caused issues for pairing in part i. as an example, table 5 shows that the first cluster includes the strength of 100 mg and the frequency of bid, the second cluster includes the first dose amount of 2 and the frequency of daily, and the third cluster includes the second dose amount of 2, so the output has three rows. however, because there were two dose amounts and two frequencies, these entities were paired in order in the gold standard i."
"have also used multiple sdfs to define the layers. however, their method essentially organizes sdfs in a csg tree using boolean operators to create the boundary of a single solid object, while we organize the sdfs in a so-called sdf tree that allows for simultaneous modeling of multiple regions and a wide range of scales. [cit] present diffusion surfaces as a primitive to model featured colors in a volume. the color of any location is then computed by diffusion from nearby surfaces. together with a sketch-based gui, they have demonstrated that various volumetric objects can be created rapidly. by duplicating a diffusion surface into two slightly apart sheets, their method can also achieve the effect of blurred color transitions, which is an advantage over our method. however, diffusion surfaces are meshes, not a true vector representation, and it is also unclear how to add region-based highfrequency and multiscale details. in addition, their representation does not support fast random access as the color of every vertex on a cross section needs to be calculated in a separate rendering pass whenever the cross section is changed."
we have discussed that sdfs can be used together to represent complex volumetric objects by partitioning the volume into regions that can be defined independently and recursively. we therefore introduce a descriptive language called volumetric object markup language (voml) based on xml that allows a user to design the region configuration of a volumetric object.
"efficient rendering is critical for interactive content creation. furthermore, rendering arbitrary cross sections of a volumetric object is an important way to visualize its internal content. given a 3d location p, the rendering process traverses the sdf tree from the root until it reaches a region node. the color of p is then decided by evaluating the color function defined in that region. appendix a shows simplified pseudo code for this process."
"detailed medication dose information is often required to perform medicationbased population studies. medication dose data can be obtained from a structured data source or an unstructured data source such as clinical notes in ehrs. to extract medication dose information from unstructured text, a specialized algorithm such as a natural language processing system is commonly used. however, the output of nlp systems is often not in a form that is useful for analysis. we developed a post-processing algorithm to address this issue. our algorithm consists of two parts to parse raw nlp output, connect medication names and attributes, and eliminate redundant information."
"to represent a volumetric object or component with a single scale, an intuitive way is to decompose it into multiple semantically meaningful but simpler regions, and represent the region boundaries and contents separately. we decide to use signed distance functions (sdfs) to represent region boundaries because they are implicit functions suitable for representing volumetric regions."
2. a region can be further divided by replacing the original region node in the sdf tree by a subtree containing more regions and sdf nodes.
"for relatively simple volumetric objects, the user can construct sdf trees and perform object instancing by directly editing a voml file. for more complex objects, we provide a gui that allows the user to organize an intricate sdf tree by drag-and-dropping different nodes intuitively on the screen."
"with the help of a recursively defined spatial indexing structure, we demonstrate that antialiased visualizations of arbitrary cross sections of a complex volumetric object can be generated in real time."
"in our current implementation, a polygonal mesh is used to define the outmost boundary surface where the color of a volumetric object should be evaluated. an intriguing idea is to get rid of any mesh and decide the evaluation locations by directly ray-casting ] the implicit region boundaries in our representation."
"there exist various techniques to generate a spatial distribution of instances as well as the affine transformations associated with the instances. we have adopted three strategies for different scenarios: physical simulation for densely packed objects, adaptive blue-noise sampling [cit] ] for elements whose density and local properties vary according to a user-provided control map, and interactive placement of individual instances via the gui we provide."
"proof. as we have already mentioned, to achieve a correct notification, three independent events should be satisfied: i) the wearable should be charged, ii) the gateway should decode the message from the wearable, and iii) the medical personnel should receive this message. these three events are given by the probabilities described in the aforementioned lemmas. to that end, c n is given by"
"the medication extraction and normalization (medxn) system was designed to extract medication information from clinical notes and convert it into an rxnorm concept unique identifier (rxcui). [cit] this system identifies medication names and attributes, such as dosage, strength, and frequency, in clinical notes using the rxnorm dictionary and regular expressions. the attributes associated with a medication name are combined together in the rxnorm standard order and normalized to a specific rxcui. medextractr medextractr is a medication extraction algorithm built using the r programming language, and is more targeted than other more general purpose nlp systems. [cit] given a list of drug names to search for, medextractr creates a search window around each identified drug mention within a clinical note in which to search for related drug entities."
"we generated training and test sets for each medication (i.e., tacrolimus and lamotrigine) from medication entities extracted by each nlp system (i.e., medxn and medextractr). each dataset included approximately 300 observations from 10 patients."
"part i processes the raw nlp output and then pairs the parsed entities, as outlined along with some illustrative examples in figure 1 . the algorithm begins with a single file that is the raw output from a nlp system, from which a drug name and its entities are isolated and converted to a standardized form [ step 1]. the entities include strength, dose amount, route, frequency, duration, dose change, and dose given intake (\"dose\"). the dose change and dose entities are specific to medextractr. dose change includes key words such as \"increase\" that indicate that the dose isn't a current dose. dose is an aggregate total dose given intake when dose amount information is not found. note that if strength, frequency, or route is missing and there is a unique strength, frequency, or route within the same note, that strength, frequency, or route is borrowed. if there is not a unique strength within the same note, the closest preceding strength is borrowed. if there is no preceding strength, the closest strength after the missing strength is borrowed. if there is not a unique frequency or route within the same note, the most common frequency or route within the note is borrowed. duration is only borrowed within a dose sequence. for example, a row with an intake time of \"am\" and no duration could borrow the duration in the row after it if that row has an intake time of \"pm\". any records that are still missing strength are removed, since dose cannot be calculated in these cases [step 6]. if records are still missing frequency or route, the most common frequency or route across all observations for that drug is imputed [step 7]. next, the dose given intake is computed by multiplying strength by dose amount. if there is not an intake time, the daily dose is computed by multiplying dose given intake by frequency. if there is an intake time (e.g., am, noon, or pm), the daily dose is calculated by adding the dose given intakes at each of the intake times [step 8] . finally, any redundancies are removed at date level and note level separately, yielding two datasets [step 9]."
"in table 1 we compare our volumetric object representation against existing ones, such as voxel grids, procedural methods, vector solid textures (vst) [cit], and diffusion surfaces (dss) [cit], with respect to four criteria, including expressiveness, ease of editing, random-access performance and compactness. these criteria are consistent with the aforementioned desired properties of volumetric representations. in summary, our representation compares favorably in all of these criteria."
"note that we allow the same sdf to appear in multiple sdf nodes as long as these nodes do not lie on the same tree path. as a result, the same set of sdfs can be organized differently in an sdf tree, giving rise to different region layouts. figure 3 shows a 2d illustration. to determine the region membership of a 3d location p, one simply traverses the sdf tree by taking the child branch corresponding to the sign of the sdf associated with the current sdf node until a region node is reached."
"procedural noises are capable of generating solid textures as well as object boundaries [cit] in a very compact form. however they are domain specific and it can be difficult to control the generation process precisely, and thus not sufficient to create generic volumetric objects with deterministic and semantically meaningful structures."
"we define the color attribute inside each region as a 3d function. once we know which region a 3d location p belongs to through sdf tree traversal, we can use the coordinates of p to evaluate the specific color function associated with that region to yield the final color at p. in our current implementation, the color function can be defined in one of the following three modes: solid color, solid textures, and radial basis functions. note that a region can be declared as empty. empty regions do not have color functions and can be used to define the exterior of a volumetric object."
"without loss of generality, we assume that all vector volumetric objects are initially defined in a unit cuboid, [cit] 3 . similar to sdf instances, we define an object instance by associating a volumetric object with an affine transformation. to embed an instance of object a into region b of another object b, we connect the sdf tree for a with the region node for b in the sdf tree for b. when an sdf tree traversal reaches region b, the 3d location p is first"
"when making the gold standard i, we sometimes paired entities based on their positions in the original note. this only happened for complicated cases when the number of each entity was not equal (e.g., three strengths and two dose amounts), which caused difficulty on our judgement for pairing; hence, we looked at the position in the original note to make the pairing decision. since part i only used position to make the clusters but did not use information about the position of the entities when pairing, rather it used order, this sometimes resulted in disagreement between the gold standard and the algorithm. table 3 presents an example of this case. in this example, there are three strengths and four frequencies in the medxn output. the algorithm pairs them in order, but the gold standard i paired them based on position."
"in this section, we present the analytical derivations of the probability of correct notification c n, which is the probability that a wearable will manage to deliver successfully a message to the gateway and, then, the gateway will deliver this message to the nearest medical personnel available."
"the results for part ii can be found in table 2 . here we present the recall, precision, and f1-measure for both medications at the note and date level, and we consider the dose intake and daily dose. the note level collapsing performed very well with all f1-measures equal to 1 for dose intake for both medications and both nlp systems, and f1-measures ranging from 0.96 to 1 for daily dose. date level collapsing also performed well with f1-measures ranging from 0.95 to 1 for dose intake and 0.91 to 1 for daily dose."
the f1-measures were the same for both medications (0.94). the algorithm performed better on the medextractr output than the medxn output. the tacrolimus recall/precision/f1-measure was 1.00/1.00/1.00 while lamotrigine was 0.98/0.98/0.98.
"which concludes the proof. as we can see, (22) does not depend on the intensity of the medical personnel, because as the intensity increases or decreases, the received signal and the interference change proportionally and, thus, the sir remains unaffected."
"such an explicit definition of individual volumetric components offers a few advantages. first, each region in a volumetric object can define its own color function. changes made to one region do not affect others and the boundaries between regions remain sharp during magnification. second, it allows a user to define a complex volumetric object by identifying representative components at each scale, modeling such components as standalone objects, and finally linking them together through embedding and/or instancing. it also greatly improves the reusability of existing volumetric components."
"defining sdf tree structure. listing 1 shows part of the voml source code for the volumetric object in figure 6 . basically, in a voml source file, the region configuration is defined by organizing sdf nodes and region nodes into a binary tree. each sdf node has two children nodes, corresponding to the subvolumes where the sdf is positive and negative, respectively. either of the two nodes can map to a region by adding a region node as its child, or can be further subdivided by adding another sdf node as its child. a region node must be a leaf node in the tree and cannot have any children nodes."
"however, we know most of these combinations are incorrect, and hence we developed a more sophisticated post-processing algorithm to give us more useful data for analysis."
"converting the output from medxn and medextractr into a form that can be used for analysis requires post-processing. a simple post-processing method we considered formed all possible combinations of entities. for example, using this simple method on a drug mention with two strengths, two dose amounts, and two frequencies would have resulted in eight combinations of strengths, dose amounts, and frequencies."
"because our vector representation clearly defines the individual volumetric components of an object and organizes them into a hierarchical structure, the user is able to choose a highly flexible and iterative workflow during content creation. for example, a user could follow a top-down approach by first defining large-scale regions and then adding more details by subdivision or object embedding, or a bottom-up approach by first creating all low-level components and then organizing them into a complete multiscale object. at any time during the process, the user can also iteratively modify a single component without affecting other parts of the object. a live demo is given in the supplemental video."
"constructive solid geometry. an sdf tree is a more powerful structure than constructive solid geometry (csg) for implicit surfaces [cit] ] even though the latter also has a hierarchical tree structure. in a csg tree, individual implicit surfaces are associated with leaf nodes, which are merged bottom-up using boolean operators to represent the single solid object associated with the root. in an sdf tree, individual implicit surfaces are associated with intermediate tree nodes, which recursively subdivide an inhomogeneous volume associated with the root into multiple simpler regions defined as the leaf nodes. therefore, a csg tree essentially defines one single solid, while an sdf tree is capable of defining multiple mutually adjacent solid regions which are not two-colorable. in fact, each region defined by an sdf tree can also be represented by a standalone csg tree whose leaf nodes correspond to a subset of the signed distance functions in the sdf tree ( figure 5 ). moreover, the time complexity for determining whether a point belongs to a certain region represented by a csg tree is usually between o(n) and o(n 2 ) [cit], while in an sdf tree, the complexity is only o(log n), with n being the number of sdfs in both cases."
"in this paper, we have introduced a compact multiscale vector representation for volumetric objects with complex internal structures spanning a wide range of scales. our representation relies on a binary sdf tree to hierarchically decompose an entire volumetric object into multiple regions while maximally aligning sharp features with region boundaries. by linking together multiple sdf trees, a complex volumetric object can be constructed from simpler volumetric objects at different scales. sdf and object instancing further utilize the substantial repetition of similar elements or volumetric features in an object and reduce the storage cost significantly. to facilitate authoring and editing, we have also provided both a scripting language and a gui prototype. results show that volumetric objects with highly complex structures and resolution-independent features can be easily created from scratch using our method. these vector volumetric objects have compact storage and, with the help of a recursively defined spatial index, can be rendered in real time with antialiasing. figure 10 . note the ubiquitous non-manifold structures."
"csg tree 1 csg tree 2 csg tree 3 figure 5 : a standalone csg tree (or adf) is needed to represent each region defined in figure 4 (a), while a single sdf tree is capable of representing all of them simultaneously."
"spatial indexing for embedded objects. embedded sdfs and objects have compact oriented bounding boxes to define their spatial extent. but these bounding boxes negatively impact random access performance since an additional step is required to decide which sdf/object a position belongs to. to overcome this problem, we build a spatial indexing structure for each region that contains embedded volumetric components or sdfs. the indexing structure simply divides the region into a uniform grid and records pointers to the objects and sdfs that overlap with each grid cell. note that the indexing structure itself can be embedded and instanced along with its parent object, effectively making the entire volumetric object adaptively indexed."
which essentially means that the shape defined by the composite sdf is the union of the shapes defined by the primitive sdf instances. note that multiple instances can point to the same sdf.
"figure 6: object embedding. object a is constructed by recursive embedding of object b, c, and d. the sdf trees of the four objects are shown in (a)-(d) respectively. the bold arrows illustrate the traversal path to evaluate the color at location pa, whose coordinates are transformed to pb, pc, pd in each embedded instance's local frame respectively during the process. a crossed square indicates an empty region."
"in order to create sdfs for highly structured region boundaries, we have built a tool to convert a polygonal mesh to an sdf [cit] . to facilitate a continuous workflow, we have integrated this tool as a plug-in for the open-source 3d modeling software, blender. in addition to sdfs converted from meshes, we have also implemented several procedural algorithms for efficiently synthesizing stochastic sdfs [cit] or adding randomness to existing sdfs [cit] ."
"most natural organisms and materials, such as the human body, fruits and sedimentary rocks, have rich and complex volumetric structures, patterns and color variations. constructing high-quality digital models for such natural organisms and materials is of vital importance because they exist everywhere, and literally everything we see is either a living being, a natural material or something made from these two. since volumetric models allow us to visualize the internal structure of an object, they are valuable graphical contents that can be used in biomedical research, scientific visualization as well as educational and training activities."
"the \"instances\" property points to an external text file containing the transformations of each instance. more details about voml can be found in the supplemental materials."
"region content. region content can be directly defined in the voml source. for example, the following code defines two regions, filled by a solid color and a solid texture respectively."
"however, although the sensing abilities of wearables become better, there are some challenges that must be overcome to ensure the suitability of wearables in the context of medical care [cit] : i) transmission problems, ii) low battery life, iii) ergonomics, and iv) non-intuitive software. although the ergonomics and software issues are a subjective matter of product design, successful message delivery and high lifetime are fundamental requirements for the correct communication between the wearables and the medical personnel. thus, there is a need to guarantee that all wearables will be able to: i) deliver reliably their messages to a final destination and ii) provide uninterrupted and stable services without intermissions for battery recovery. regarding the first issue, the communication of wearables should be carefully studied taking into account the node deployment, the interference and the channel randomness. for instance, in a typical hospital scenario, each room is hosting multiple patients. from a communication perspective, the transmitters of the wearables are forming small clusters that should be considered in the calculation of the received interference from the other clusters and, thus, the wearables' ability to communicate correctly."
"many volumetric objects contain multiple similar but smaller-scale elements, such as cells in an organ. modeling every individual element separately would be a tedious task and also requires a significant amount of storage. by embedding multiple instances that point to the same object but with different transformations, we considerably save both storage and modeling effort."
"in this paper, we introduce multiscale vector volumes, a compact vector representation for volumetric objects with complex internal structures spanning a wide range of scales."
"the coordinates p is transformed whenever the traversal enters a new embedded object (i.e. another sdf tree associated with an affine transformation), so that once the final region is reached, p may have been transformed multiple times, as shown in figure 7 ."
"we have generated several volumetric objects of different types using our method, as shown in figure 1 and 10. the overall authoring time ranges from a couple of minutes to several hours, most of which is spent on modeling the polygonal meshes for the sdfs. in addition to creating sdfs manually, we can also utilize existing data, such as scanned medical data. in figure 10 (d), a medical scan of a human brain is segmented into regions, from which we can build an sdf tree with multiple sdfs automatically. once all the sdfs and instancing transformations are known, it usually takes only minutes to design and construct a corresponding sdf tree."
"3. a region can contain instances of other volumetric components. for example, the hypodermis region of skin contains many fat cells. a user could embed a volumetric object for the fat cell into the skin's hypodermis by object instancing."
"once a volumetric object has been loaded from a file, a parser analyzes the region configuration defined in voml and generates all necessary shader resources for rendering."
"in this section, we present the simulation setup, we validate the analytical derivations obtained in section iii via monte carlo simulations and we discuss the network performance."
"it should be noticed that the number of interferers in (15) and (16) is multiplied by the probability that these interferers will have enough power to be active during the cp. in this way, if some interferers have not received enough energy during the hp, they will not contribute at the total interference. substituting (15) and (16) in (14), yields the result of lemma 2."
"the algorithms were evaluated using recall, precision, and f1-measure. recall is the proportion of the gold standard entities that were correctly identified by the algorithm."
"the system model is described in section ii. the mathematical analysis is presented in section iii and the numerical results in section iv. finally, section v concludes the paper."
"the regions in (a) are two-colorable as they can be distinguished by a single binary mask (b). in (c) there are more than two regions adjacent to each other, therefore the regions are not twocolorable. the boundaries between regions contain non-manifold features (e.g. t-junctions as highlighted in yellow)."
"a multiscale vector volume has an explicit definition of the individual volumetric components within. here a component may refer to either a physical material such as the stone chips inside a terrazzo, or a semantically meaningful concept such as a cell of an organ. since the internal structures and contents of the objects we intend to model span a wide range of scales, we have developed two methods to describe them. in the following, we first introduce a novel data structure called sdf tree that hierarchically divides a volumetric object into non-overlapping regions with arbitrarily complex boundary shapes and defines the attributes of each region independently. then we discuss how to represent a complete multiscale object by linking together multiple vector volumetric objects."
"figure 4: the same shape (shaded) represented by (a) an sdf tree, (b) a csg tree, and (c) an adf respectively. note that in addition to the shape boundary, our sdf tree also defines three internal regions."
"the goal of the post-processing algorithm was to convert all the extracted information from an nlp system into a usable format. however, this may result in conflicting doses on the same day, which will need to be resolved before the data can be used for medication-based studies such as pharmacokinetic or pharmacodynamic studies."
"the system shortens the search window when a medication name which is not of interest appears, to avoid extracting incorrect or irrelevant information. by default, the list of unrelated drug names is based on the rxnorm library supplemented with common abbreviations. some drug entities are identified and extracted based on matching expressions in manually curated entity-specific dictionaries, including frequency, intake time, and dose change (keywords to indicate that a regimen may not be current). for the remaining entities, including strength, dose amount, dose (i.e., dose given intake), and time of last dose, regular expressions are used to identify common patterns for how these entities are written. function arguments can be used to optimize drug entity extraction for a given set of clinical notes. examples include specifying the maximum edit distance for approximate drug name matching or the length of the search window."
"implicit surfaces are an important geometric modeling tool. ricci [cit] organizes multiple implicit surfaces into a csg tree to construct complex solid geometries from simpler primitives. [cit] develops adaptively-sampled distance fields (adfs) which encode an sdf using an adaptive space partition scheme, such as an octree, so that geometric features with varying scales can be represented efficiently. perry and frisken [cit] extended adfs to support multiple sdfs combined using boolean operators within each octree cell to represent non-differentiable features such as corners. however, the above methods are designed for representing solids with a single region or multiple two-colorable regions. they are unable to represent objects with non-manifold structures. differences among csg, adfs and our method will be further elaborated in section 3.1.3."
"binary space partitioning. in the sense of region partitioning, an sdf tree can be considered as a generalization of a binary space partitioning (bsp) tree. a partition surface in a bsp tree is typically a plane while a partition surface in an sdf tree is the zero isosurface of an implicit function capable of defining an arbitrary shape."
"during the creation stage, the user needs to actually create the \"building-blocks\" of an object, such as sdf instances and region definitions, and then assemble them together into linked sdf trees."
"adaptively-sampled distance fields. in a common implementation of adfs, a volume is recursively subdivided by an octree where each cell contains a set of distance samples from which a"
"for each of the training and test sets, we generated three sets of gold standard datasets to test each part of the post-processing algorithm. each gold standard dataset was manually generated to make the intended output for each part of the algorithm. that means that we generated \"gold standard i\" as the output of part i if the five steps of the algorithm were correctly performed. then, we generated \"gold standard ii-date\" and \"gold standard ii-note\" as the output data of part ii applied to the gold standard i at date level and at note level, respectively, if the nine steps of the algorithm were correctly performed. these were generated for each of the training and test sets and each of the medications (i.e., tacrolimus and lamotrigine), yielding a total of 12 sets of gold standard datasets for each nlp system."
"however, to the best of our knowledge, the validation of post-processing algorithms for these nlp systems has not been reported separately from the overall evaluation of the nlp systems in the literature, although they may be unofficially validated during the development phase. correctly connecting entities with both drug name and with each other to obtain medication dose (e.g., dose given intake or daily dose) can be challenging and error prone if the prescription pattern is complex."
"moreover, in fig. 5, we depict the performance of the network as the number of clusters is growing. from this figure, we notice that as the number of clusters increases, more interference is generated, resulting in performance degradation. it is worth noting that the higher number of clusters deteriorates both the link between wearables and gateways and the link between gateways and medical personnel. in a hospital scenario with a fixed number of clusters, the first link can be enhanced by decreasing the radius of each cluster, while the second link can be enhanced by adopting a more sophisticated technique for transmission and, thus, reducing the interference."
"developing and validating a post-processing algorithm separately from the main nlp system would allow for easier identification of error sources and more efficient improvement of relevant parts of the system. thus, we developed algorithms that process the raw output from two medication extraction systems, medxn and medextractr, which performed best out of 4 nlp systems previously tested. [cit] the post-processing algorithm we developed is divided into two parts. part i processes the raw output from the nlp system and connects entities together. part ii removes redundant data entries anchored at each note or date identifier and calculates dose given intake and daily dose. these measurements are crucial for medication-based studies such as population pharmacokinetic and pharmacodynamic or pharmacogenomic studies."
"patients with complex data were over sampled. complexity was determined by the presence of multiple clusters or clusters containing entities with conflicting values. these cases are often difficult to process and are likely to produce discrepant daily dose. for example, when two different strengths of 100 mg and 200 mg are associated with a lamotrigine mention, they are more difficult to process compared to only a unique strength of 100 mg associated with that drug mention since each strength must be paired with the correct dose amount and frequency. thus, for each of the training and test sets, we randomly selected a greater number of complicated cases; six of the tacrolimus patients were chosen from patients with known complications as well as eight of the lamotrigine patients."
"in part ii, challenges occurred when the morning dose came after the evening dose. the algorithm only treated dose sequences correctly when the morning dose came before the evening dose."
"given n overlapping sdfs, their zero isosurfaces divide the volume into multiple physical regions, each of which can be uniquely identified by the signs of the n sdfs. we organize the sdfs into a binary tree called an sdf tree. each intermediate node of this tree is associated with an sdf and thus denoted as an sdf node. since the zero isosurface of an sdf partitions a space into two half spaces, an sdf node can also have two children. having another sdf node as a child means one of the half spaces is further partitioned by the zero isosurface of that sdf. a leaf node is also called a region node as it corresponds to one of the physical regions in the volumetric object. in practice, we may need to merge multiple physical regions to define a semantically meaningful logical region."
"partial implicit surface can be reconstructed. in order to represent sharp features more efficiently without excessive subdivision, perry and frisken [cit] allows an octree cell to contain more than one partial implicit surfaces whose boolean combination results in one partial surface with sharp features. nevertheless, similar to csg, an adf as a whole still defines one single solid that might consist of multiple two-colorable regions. it is unclear how adfs can be extended to represent non-manifold structures, where at least three regions are mutually adjacent, which on the other hand can be conveniently represented with an sdf tree. figure 4 shows the same shape represented by csg, adf, and our sdf tree respectively."
"the recall was slightly higher for tacrolimus than lamotrigine when using the medxn output (0.96 vs. 0.94), but the precision was slightly lower for tacrolimus (0.93 vs. 0.94)."
"real-world volumetric objects often contain many volumetric components with dramatically different scales. each component can also be considered as a standalone volumetric object and contain smaller-scale components recursively. we extend our volumetric representation to include object embedding, which allows one volumetric object to be embedded into a region of another object."
"our post-processing algorithm was developed using the output from two nlp systems, medxn and medextractr. our algorithm performed reasonably well to process the output from both medxn (f-measures part i: ³ 0.94; part ii: ³ 0.98) and medextractr (f-measures part i: ³ 0.98; part ii: ³ 0.91). we tested the algorithm using two medications that have widely different prescribing patterns, but it should be tested using other medications. we also have part i written for two other nlp systems, medex and clamp, but we have not yet tested the algorithm using a gold standard for these systems. we are in the process of incorporating a few changes to the algorithm, such as pairing entities based on distance in part i in order to improve the performance."
"we can also embed multiple object instances into the same region. since multiple instances may overlap in space, we use the embedding order to resolve the ambiguity in region membership. an illustration of object embedding and the corresponding traversal procedure is shown in figure 6 ."
"sdfs have been widely used in volumetric modeling. in this section we compare our vector representation with several sdf-related methods, especially those also involving a tree structure."
"where r is the euclidean distance between the two nodes, i intra denotes the interference from the remaining wearables of the cluster and i inter denotes the interference received from the clusters."
"limitations currently each instance is only associated with an affine transformation and our representation does not support nonlinear transformations of object instances. in addition to transformations, it should be useful in certain scenarios to add other perinstance data such as color variations. supporting blurred region boundaries would be challenging since the neighborhood information of a region is not directly accessible."
"transformed into the local frame for the instance of object a and then used to traverse the sdf tree of a to decide which region p belongs to. if however p falls into an empty region of a, its final color is determined by the color function of background region b ."
"another challenge occurred when the nlp extracted incorrect information. this could have occurred because of a misspelling, missing spaces, or an uncommon abbreviation of a drug name, which caused the nlp to extract entities anchored to the wrong drug mention. this results in several extra strengths, dose amounts, or frequencies, making for a very complicated cluster structure. table 4 illustrates this example. here, all of the information that was extracted by medxn was incorrect because of the missing spaces between the frequencies and the next drug name."
"in an sdf tree, this is achieved by mapping multiple leaf nodes to the same logical region. figure 3 : by organizing two sdfs differently in an sdf tree, various region layouts can be achieved."
"the coefficients of a 0 and a 1 have to be dynamically determined through steps 1-3. if only the fundamental component exists, (29) provides only one unique real solution. if both of the fundamental and interharmonics are present, it gives two unique solutions."
"as the proximity based clustering protocol is based on a suggested threshold, it is worth investigating the optimal latency distance threshold that can speed up information propagation. to this purpose, we experiment with bcbpt based on several suggested distance thresholds dt. a comparison among three variances of delays which were measured based on three different suggested thresholds 30ms, 50ms, 100ms, is shown in fig.4 . results reveal that less distance threshold performs less variance of delays. judging from that, propagation delay negatively corresponds with the latency threshold, as the total duration of subsequent announcements of the transaction by the remaining nodes increases with a larger latency threshold. the key reason of variances of delays have been declined when the threshold value is reduced is that the number of nodes at each cluster is minimised due to the limited coverage physical topology which are offered dt.. there are some security implications that might be raised while selecting peers confined to closest proximity. in particular, it would seem possible for an attacker to more easily launch eclipse attacks by concentrating its bad peers within a small cluster. though, a good peer from the same area joining the bitcoin network might have a higher probability of selecting from these bad peers. this would achieve a completely malicious cluster. in our view, an eclipse attack is a bit challenging as the proposed protocol aims to have clusters based on countries. similarly, partition attacks seem to have a great potential. as undertaking clustering in the bitcoin network is a fundamentally different proposition to clustering within other classes of network due to the strict requirements on security. therefore, we plan to evaluate some possible classes of attacks with regards to our proximity protocol. so our future work will include evaluation of partition attacks as well as eclipse attacks."
"in order to avoid singularity of the instantaneous autocovariance matrix r(t) (i.e., rank 1 at any time instant). we apply to both sides of (16) a simple second-order low-pass filter that is described by transfer function representation"
"the second test emulates the sudden occurrence of the resonance of the lcl filters between parallel pv systems. the power source is programmed initially with the fundamental component of 50 hz only. then, the interharmonic component is added to the fundamental component. the measured input voltage signal provided by the power source is shown with the analog version of the fundamental frequency and interharmonic frequency in the kernel signal is used to provide fast detection of the interharmonic. it is the eigenvalue of the matrix signal r f (t) that is composed of processed versions (filtered) of the incoming signal y(t). so the kernel signal is an inherent sinewave because of the sinusoidal nature of y(t). when the inharmonic is absent, the kernel signal is zero. when the interharmonic appears, the kernel signal increases from zero. therefore, when the kernel signal increases beyond a threshold δv 1, it can trigger the signal of interharmonic occurrence from 0 to 1. the kernel signal will then rise until it settles down to a steady-state level (with a sinusoidal ac ripple). a moving window method is used to locate the minimum kernel value for each period of the kernel signal (i.e., half period of the mains voltage). when the interharmonic disappears, the kernel signal will decay to zero. thus, when the kernel signal is less than the minimum kernel value by another threshold δv 2, it can trigger the signal of the interharmonic occurrence from 1 to 0. as long as the thresholds δv 1 and δv 2 are slightly larger than the noise level, the detection times can be minimized. in the practical tests, δv 1 and δv 2 are set at 0.098 v and 0.188 v, respectively."
"bitcoin protocol achieves the distributed validation based on a replicated ledger that is collectively implemented by network voluntaries. this ledger tracks the address balances of all users. an arbitrary number of addresses can be created by each user to send and receive bitcoins. an ecdsa key pair is used to prove the ownership of bitcoins associated with that address. each entry in the public ledger represents a transaction which is a signed data structure that is created by a bitcoin user who intends to send a specific bitcoin to one or more destination accounts [cit] . transactions are responsible for claiming some bitcoins that are associated with the address of the sending party and reassigning them to the address of receiving part/parties. transactions are represented by a hash of the previous transaction that has been sent before as well as the public key of the future owner. each transaction includes input and output. for combining or splitting bitcoins, transactions can handle multiple inputs and outputs. inputs reference the funds from other previous transactions, whereas outputs indicate the transferred bitcoins. a transaction output also indicates the new owner of the transferred bitcoins when it is referenced as an input in a future transaction. though, the balance of an account is the sum of all values of all unspent outputs owned by that account. the sum of all outputs should be equal or less than the sum of all inputs [cit] ."
"in this section, an estimation scheme is designed for estimating n arbitrary frequencies in a harmonic signal. the fundamental theory behind this method is based on rather complex mathematics, but the suitably designed algorithm can be simply implemented via linear state-space equations. next, we introduce some basic theories that contribute the further analysis, while the detailed derivation is reported in the appendix."
"based on our previous work [cit], we proved that grouping bitcoin nodes that are geographically close would improve the transaction propagation delay. however, nodes that are geographically close might actually be quite far from each other in the physical internet. this actual, physical internet distance may lead to different results, leading to different conclusions too. taking that into account, in this work we examine a new protocol that groups the bitcoin nodes based on ping latencies. specifically, we introduce a novel clustering protocol that uses the proximity based ping latencies in the neighbour selection in order to incorporate proximityawareness into the existing bitcoin protocol. the proposed protocol, which is called bitcoin clustering based ping time (bcbpt), aims to convert the bitcoin network topology from normal randomised neighbour selection to proximity based latency selection. peers in bcbpt are self-cluster based proximity, thus every peer must know whether other peers are close in the topological term (physical internet) in order to connect to those peers and form a cluster. therefore, peers within each cluster are highly connected via short link latencies. giving the visibility into the available information from the outside cluster, each node maintains a few long distance links to the outside cluster. this protocol is implemented in two phases, cluster generation and cluster maintenance. both phases will be discussed in detail in the following subsections."
"turning now to the comparison between bcbpt protocol and locality based protocol lbc. the lbc protocol was proposed in our prior work [cit] as a mechanism to improve the transaction propagation delay in the bitcoin network. the lbc protocol aims to convert the bitcoin network topology from normal randomised neighbour (connected nodes) selection to location based neighbour selection. clusters in lbc protocol are formulated by referring an extra function to each node in the bitcoin network. by this function, each node is responsible for recommending proximity nodes to its neighbours. the proximity is defined based on the physical geographical location. fig. 3 shows the variances of delay both the bcbpt protocol and lbc protocol. before proceeding to discuss the difference between both protocols, it is important to mention that these variances of delays in both protocols have been measured using the same methodology. results show that the bcbpt protocol maintains an improvement in variances of delays over the lbc protocol. the most likely cause of the higher variances of delays in the lbc protocol is that two geographically close nodes may be actually quite far from each other in the physical internet, as in any other p2p network. therefore, physical distance may lead to better results, leading to a different conclusion that proximity awareness in the physical internet improves the delivery latency with a higher probability due to offering fewer hops as well as shorter links. furthermore, dynamics of internet routing, as caused by bgp (boarder gateway protocol) peering agreements, can also result in surprising situations that closest differs between geographical and topological terms."
applying the linear integral operator to both side of (23) with a kernel function in the form of (6) with the order of the multiplier reduced from 2n + 1 to 4 and ρ is designed as 1000
"remark 1: note that the algebra algorithm (19) is valid only when the filtered auto-covariance matrix r f (t) is invertible. the invertibility of r f (t) characterizes a sufficiently informative output signal at time t. in view of (14), the parameter ρ determines the poles of g that in turn determines the cutoff frequency of the overall low-pass filtering structure. a larger ρ results in a poorer noise immunity, while a smaller ρ may result in a less informative r f (t) due to the excessive attenuation of y(t). to this end, the choice of ρ depends on a priori information of the frequency band of the interharmonic."
"regarding issues that are mentioned above, this paper presents bitcoin clustering based ping time protocol (bcbpt), an efficient solution for tackling the problem of the transaction propagation delay. bcbpt aims to increase the proximity of connectivity among nodes in the bitcoin network based on round-trip ping latencies. currently in the bitcoin network, a node connects with nodes regardless of any proximity criteria. in contrast, the core of our solution is to get nodes to gain more information about the proximity of other nodes, thus, enabling them make a better decision on which nodes to connect with. based on the simulation model that was presented in our previous work [cit], bcbpt evaluation results are presented in this work. the evaluation of bcbpt is done based on whether or not the bcbpt protocol is able to speed up the transaction propagation delay. in addition, we perform a comparison based on the transaction propagation delay between the bcbpt protocol and lbc protocol that has been presented in our previous work [cit] . however, finding out the best mechanism which is able to speed up the propagation delay in the bitcoin network is deemed to be the main intended impact of this research."
"fig .3 compares the distributions of ǻ‫ݐ‬m,n for the simulated bcbpt protocol against the same distributions that have been measured in the simulated bitcoin protocol and lbc protocol. results reveal that the bcbpt protocol offers an improvement in propagation delay compared to the bitcoin protocol as well as lbc protocol. regarding the comparison between the bcbpt and bitcoin protocol, the bitcoin protocol performs variances of delays, which have been collected in our prior work [cit], that grow linearly with the number of connected nodes, whereas bcbpt maintains lower variances of delays regardless of the number of connected nodes. this suggests a strong link may exist between these results and connections based on the proximity in the physical internet topology that is maintained in the bcbpt protocol. precisely, the reduction of the transaction propagation time variances in the bcbpt protocol has to do with the fact that the connections based proximity between nodes implies faster transmissions due to short distance links among nodes."
"with the aim of improving the convergence speed of a timedomain method without significantly increasing the computation burden, a novel kind of kernel-based algorithms has been recently proposed for estimating n sinusoidal components with arbitrary frequencies [cit] . this class of algorithms has been adopted for fast detection of fundamental and harmonics for grid-connected power electronics equipment [cit] . the basic theory is based on a rather complex mathematical framework, which is beyond the scope of this paper. while the original algorithm can estimate multiple frequency components including the fundamental, harmonics, and interharmonics, the objective of this paper is to adopt this theory in a specific form to explore the use of this estimator for fast and accurate tracking of the fundamental and a single interharmonic within specified frequency band in the ac mains. practical results are included to verify the feasibility and fast response of the proposed method."
"it is worth noting that the frequencies ω k act as the zeros of p (s). based on (4), the incoming signal y(t) defined in (1) can be thought as generated by the following autonomous system the eigenvalue of which is identical to the zeros of the characteristic polynomial p (s) [cit] :"
"the original algorithm [cit] was designed to detect n unknown frequencies. previously, it has been used to detect harmonics for active power filter applications. in this project, we demonstrate its use for the detection of the fundamental and interharmonics. since the frequency estimates tend to converge to the frequencies with greater amplitude, the introduction of a band-pass filter is needed to identify the interharmonic (which typically has a much smaller amplitude than the fundamental frequency). but the cutoff frequency of the band-pass filter is hardly dependent on the interharmonic. in this application, it is simply set at a value higher than the fundamental frequency. if the information of the interharmonic is unknown, a higher order estimation scheme of the general algorithm (that takes more frequencies into consideration) has to be adopted."
"in this paper, brief backgrounds of bitcoin system as well as analysing the information propagation in the real bitcoin network were presented. in addition, how propagation delay in the bitcoin network could affect security by offering an opportunity to double spend the same coins; thereby abusing the consistency of the public ledger was discussed in this paper. the bcbpt, a novel clustering protocol that incorporates proximity-awareness into the existing bitcoin protocol, was presented in this paper. by conducting extensive simulations, bcbpt evaluation results indicate an improvement in the transaction propagation delay over the bitcoin network protocol. furthermore, experiments with different distance threshold values have been conducted to identify the distance threshold that would give better improvement in the transaction propagation delay. we discovered that the providing less distance threshold would improve the transaction propagation delay with high proportion. based on the transaction propagation delay, we compared between the bcbpt protocol and lbc protocol. comparison results showed that the bcbpt protocol minimises variances of delay"
"the time in which the transaction is propagated by our measuring node and reached each node of our measuring node connections was calculated by running the measuring node m. in order to get accurate measurements, the latency is determined by an average of approximately 1000 runs as errors such as loss of connection and data corruption are expected to happen while dealing with the network. the distribution of these measured time differences ǻ‫ݐ‬m,n represents the exact transaction propagation delay as measurements are indicated when peers receive transactions."
"where p (jω) is parametrized by a i as given in (4). a structural block diagram of the proposed algorithm is shown in fig. 1, where the n + 1 identical system [see (14) ] is enhanced by dotted rectangles and y(t) is the incoming signal."
"the third test focuses on the sudden disappearance of the interharmonic. this is to emulate the situation that the resonance between the parallel lcl filters has stopped. the corresponding measured signals are displayed in fig. 6 . fig. 5 shows that the kernel signal will reach its steady waveform within about one mains cycle. such waveform is used again for detecting the disappearance of the interharmonic. when the kernel value falls to a value less than the minimum value by a certain threshold, it signifies the absence of the interharmonic. in this case, it takes about 9 ms to detect the disappearance of the interharmonic. this detection time is less than half of a mains cycle. in fig. 6, the kernel signal at the high state has some ripple. the trigger signal has to wait for the kernel signal to drop below the minimum kernel signal in the moving window by δv 2 before changing from one to zero. this is why the detection time for the disappearance of the interharmonic is longer than that for the occurrence of the interharmonic."
"the paper is organised as follows: in section ii, related work in measuring and analysing bitcoin information propagation and in modelling approaches to avoid double spending attacks will be outlined. section iii focuses on giving an overview of the bitcoin system and briefly describing the bitcoin networking aspects. in addition, we discuss in details the information propagation in the bitcoin network and analyse the double spending attack which is caused by the transaction propagation delay. section iv details the proposed clustering protocol (bcbpt) with reference to the clusters generation and clusters maintenance. in section v, bcbpt's protocol evaluation results regarding speeding up the transaction propagation delay is performed. furthermore, a comparison between the bcbpt protocol and lbc protocol is provided based on the transaction propagation delay. in section vi, we conclude the paper and discuss the future work."
we introduce a utility function that could calculate the distance between two nodes in the bitcoin network measured by latency. this function would dramatically change the behavior of the overlay and help enriching nodes with proximity knowledge. the new utility function is shown in (2):
"comparing to the available methods that mainly deal with the fundamental and harmonic frequencies, the proposed multifrequency strategy offers the flexibility of being able to address more than one sinusoid. this important feature offers a solution to fast online interharmonic detection. as clearly shown in (5), the order of the signal generator model depends on the number of frequencies to be estimated. the residual sinusoidal components (that are not taken into account) are implicitly treated as a disturbance which may result in a degradation of accuracy. however, higher computing load is required by increased dynamic order. in this connection, the computation burden and the precision need to be compromised. without loss of generality, a generic framework that copes with n harmonics and a possible offset [see (1) ] is dealt with in section iii."
"in this section, the generic algorithm for fast detection of signals of n (arbitrary) frequencies is introduced. equations (1)- (20) are used to explain the general concept of multiple frequency detection in sections ii and iii. then, the estimator will be adopted for the specific application and the implementation of tracking the fundamental and an interharmonic (which is far from the fundamental frequency and is not an integral multiple of the fundamental frequency) in section iv. this estimator is, therefore, especially designed for detecting the emerging problem of oscillations between parallel-connected pv inverters with lcl filters."
"on the domestic level, rooftop photovoltaic (pv) systems have been increasingly installed in the distribution networks. unlike traditional fossil-fuel-based power generation, distributed renewable generation systems are usually bidirectional. for example, solar power generated by the pv systems can be consumed by domestic loads, while surplus power can be injected into the power supply side (i.e., the distribution networks)."
"the fourth test considers the possibility of mains frequency variation due to the increasing use of intermittent renewable energy penetration in power grid, especially microgrid using small electric generators. normally, large-scale power grids do not have rate of change of frequency (rocof) higher than 0.1 hz/s. for u.k., the rocof is limited at 0.2 hz/s [cit] . for micro- grid, a higher rocof of 0.5 hz/s is adopted in this test. the rocof is programmed in a linear manner so that the frequency value ramps up and down between 48 and 52 hz. for a rocof of 0.5 hz/s, the mains frequency changes from 48 to 52 hz in 8 s. fig. 7 shows the input signal comprising the fundamental frequency superimposed with an interharmonic of 1.94 khz, with the fundamental frequency varying between 48 and 52 hz. it can be seen that the analog version of the fundamental frequency varies within 48 and 52 hz linearly as expected, while the analog version of the interharmonic frequency remains constant. these results indicate that even if the mains frequency varies, the successful detection of interharmonic remains intact."
"as distances measurements are subject to network congestion and therefore dynamic, within some variance, multiple messages between pairs of nodes, repeatedly are sent over the time in order to determine variance. in terms of a discovered node close to a node, the node establishes a connection with the discovered node by sending a version message as a handshake. in contrast, these two nodes would have a very little chance to get directly connected and stay in the same cluster if they are so far away from each other. therefore, clusters in the overlay network become more proximity-aware and nodes make a better decision in terms of limiting the cost of communication. however, to measure the distance between nodes in \"ping latency\" requires every pair of nodes to interact, which added an extra overhead to the network. this overhead will be evaluated in our future work."
"where t is the time variable, v 1 (t) is the fundamental voltage, v m (t) is the interharmonic that needs to be identified, and"
"due to the purpose of keeping the chronological order of the valid transactions across nodes, every valid transaction should be included in a block that forms a part of the ledger. each block is connected with an earlier block through the earlier block's hash which must be included in the block's header. as an exception, only one block, known as the genesis block, cannot reference an earlier block. in order to acknowledge a group of transactions in a block, computational effort must be spent. therefore, a transaction is added to the ledger once sufficient work is done to acknowledge the block that contains it. this computational effort is provided by special nodes that are known as miners [cit] . blocks and transactions are broadcasted in the entire network in order to synchronize the replicas of the public ledger across all nodes. on receiving a new transaction, a peer checks whether the bitcoin has been previously spent in the block chain, and whether the transaction is correctly formed. once a transaction has been verified by a peer, as shown in fig.1, it announces the transaction availability to nodes by propagating an inv(inventory) message that contains the hash of the transaction. this propagating scenario is followed in order to avoid sending a transaction to a node that already receives it from other nodes. on receiving an inv message, a node would request a transaction if it has not seen before. requesting a transaction is fulfilled by sending a getdata message. responding to the received getdata message, a node sends the transaction's data. alas, the transaction broadcasting scenario causes a delay in transaction dissemination which, on the other hand, affects the bitcoin network scalability. this affection is represented by making the public ledger inconsistent. however, inconsistency of the public ledger would encourage attackers to successfully perform double spending attacks [cit] ."
"the term d(m) denotes the distance between two nodes i and j. d(m) can be calculated using the geographical distance calculation methodology that has been used in our previous work [cit] . s is the speed of the signal which is equal to 3*10 8 ms when dealing with wi-fi internet, while it is equal to 2/3 *3*10 8 ms in terms of copper cable. q´ represents the queuing time(average). queuing time can be calculated as:"
"the condition (a1-a) is met by the factor (1 − e −ρτ ) n up to n th order, while the condition (a2-b) is met by the third factor (1 − e −ρ(t−τ ) ) n ."
"in this phase, a protocol for bcbpt clustering maintenance is designed. while joining the network for first time, a node n learns about the available bitcoin nodes from a list of dns services. however, the node discovery service should also make a ranking on which node to select and which not as the initial dns seed service might return sub optimal peers. therefore, dns service nodes should recommend available nodes to the node n based on the proximity in the physical geographical location as the geographic distance in the internet is many times a good indication of topologic distance. dns service follows the geographical distance calculation methodology that has been used in our previous work [cit] in order to recommend closest available nodes to node n. the node n calculates the distance to each discovered node in order to get its proximity ordering based on a latency threshold. this ordering would help the node n to be directed to a specific cluster. the role of dns service stops once the node n joins a cluster. after that, the node n sends a join request destined for the closest node k of the discovered nodes. once the node n connects to the node k, it receives a list of ips' of nodes that belong to the same cluster of the node k in order to allow the node n connects to the nodes that belong to k's cluster only. periodically, node n discovers other nodes using the normal bitcoin network nodes discovery mechanism [cit] . then, node n finds out whether the discovered nodes are physically close by following the distance calculation mechanism that has been mentioned above. when the node n wants to leave the network, in this case no further action is required."
"consider a power electrical signal comprising a fundamental signal y 1 (t) of nominal frequency ω 1, plus other high-frequency components and a dc offset (a 0 ) as"
"the first test is to generate a fundamental signal at 50 hz with a superimposed high-frequency signal at 1.94 khz (i.e., representing the interharmonic). this signal is sampled by the dspace a/d system, and digitized for use in the proposed algorithm. the sampling rate of the dspace system is 10 khz, meaning that the adopted algorithm can be executed within 100 μs. the computational time of the algorithm in the dspace system is about 55 μs. the actual and computed fundamental frequency and interharmonic frequency are displayed in fig. 4 and tabulated in table i . these results show that the practical and computed frequencies are in good agreement. adding extra frequency detection functions will inevitably increase the computational burden of the microcontroller of the power inverter. if such computational needs exceed the computational capabilities (e.g., speed) of the microcontroller, additional processor may be needed to implement the frequency detection functions."
this paper presents the first practical implementation of an observer-based algorithm for fast detection of fundamental signal and interharmonic component. the theory behind the algorithm has been summarized and explained. the algorithm is adopted in a form for monitoring the presence of the mains frequency and the interharmonic and for estimating their values. a novel method of using the kernel signal for fast interharmonic detection has been proposed and practically demonstrated. experimental results have confirmed that the detection times of the fundamental and interharmonic can be achieved within half of a mains cycle. this method is suitable for applications such as monitoring the occurrence of resonance between parallel lcl filters of grid-connected inverters.
"it is, however, important to note that the harmonic detection time from 0 to 1 is usually very fast because the initial kernel signal is zero and has no ripple. this feature is important and advantageous because the purpose of the algorithm is to detect the occurrence of the interharmonic in this specific application as the signal for the occurrence of resonance between parallel grid-connected power inverters. in fig. 5, the detection time for the occurrence of the interharmonic is only 3 ms. this detection time can be reduced if δv 1 is further reduced, as long as δv 1 is larger than the noise level in the practical implementation."
note that (13) represents a standard linear expression for identification. conventional augmentation tools used in system identification can be employed to form a well-posed algebraic system based on (13) . let us first multiply both sides of (13) by z(t)
step 4: the frequencies ω 1 and ω m of the fundamental and the interharmonic are indirectly estimated as the roots of the following equation:
we have also shown [cit] that no forwarding strategy can be designed that works correctly in the presence of interest aggregation and uses nonces and the names of ndos as the basis of interest-loop detection. section iii shows an example of the occurrence of undetected interest loops in ndn and ccnx.
"ternary content-addressable memory (tcam) compares an input word with its entire stored data in parallel, and outputs the matched word's address. tcam stores data in three states: 0, 1, and x (don't care). traditional tcams are built in application-specific integrated circuit (asic), and offer highspeed search operations in a deterministic time."
"when a router receives an interest, it checks whether there is a match for the content requested in the interest in its cs. the interest matching mechanisms differ in ndn [cit] and ccnx [cit], with the latter supporting exact interest matching only. if a match to the interest is found, the router sends back an ndo over the reverse path traversed by the interest. if no match is found in the cs, the router determines whether the pit stores an entry for the same content. in ndn, if the interest states a nonce that differs from those stored in the pit entry for the requested content, then the router \"aggregates\" the interest by adding the incoming interface from which the interest was received and the nonce to the pit entry without forwarding the interest. on the other hand, if the same nonce in the interest is already listed in the pit entry for the requested content, the router sends a nack over the reverse path traversed by the interest. in ccnx, aggregation is done if the interest is received from an interface that is not listed in the pit entry for the requested content. a retransmitted interest received from the same interface is forwarded [cit] ."
"in set (p it i n(j) ) denotes the set of neighbors from which router i has received an interest for ndo n(j); ou t set (p it i n(j) ) denotes the set of neighbors to which router i has sent an interest for ndo n(j); and rt (p it i n(j) ) denotes the lifetime of the pit entry. the maximum interest life-time (mil) assumed by a router before it deletes an interest from its pit is large enough to preclude unnecessary retransmissions, and not too large to cause the pits to store too many interests for which no ndo messages or nacks can be sent due to failures or transmission errors."
"to verify our proposed design we implemented it on a xilinx virtex-6 fpga device (xc6vlx760). the proposed design was implemented using the xilinx ise 14.7 design tool, and verified through behavioral and post-route simulations using an isim simulator."
"re-configurable hardware fpgas emulate tcam functionality using sram memory. existing sram-based solutions of tcam on fpgas achieve inefficient memory usage and offer lower operational frequencies. we have presented a memory-efficient design of tcam, based on multipumpingenabled multiported sram, by operating the sram blocks in the design at a frequency that is multiple times higher than that of the overall system. this allows reading from its sub-blocks to take place within one system cycle. the fpga implementation results show that the performance per memory of our proposed design is up to 2.85 times higher than for existing sram-based tcam solutions on fpga."
"if a router does not find a match in its cs and pit, the router forwards the interest along a route listed in its fib for the best prefix match. in ndn, a router can select an interface to forward an interest if it is known that it can bring content and its performance is ranked higher than other interfaces that can also bring content. the ranking of interfaces is done by a router independently of other routers. figure 1(a) shows the case of a long-term interest loop caused by multi-paths implied in fibs not being loop-free, even though all routing tables are consistent. in this case, the ranking of interfaces in a fib can be such that a path with a larger hop count may be ranked higher than a path with a smaller hop count, because of the perceived performance of the interfaces or paths towards prefixes. figure 1(b) shows the case of a temporary interest loop when single-path routing is used and fibs are inconsistent due to a topology change at time t 1 . in both cases, router a aggregates the interest from x and router x aggregates the interest from y, and the combined steps preclude the detection of any interest looping. in this example, it would appear that the looping problems could be avoided by forcing router b to use q rather than x for interests regarding prefixes for which router j is an origin. however, the same looping problems would exist even if link (b, q) were removed in the example, and the ways in which fibs are populated and interfaces are ranked are independent of updates made to pits."
"excessive usage of brams in the design of tcam can result in a lack of brams for other parts of the system on fpga. furthermore, the limited amount of bram resources on fpga can compel designers to implement tcams in distributed ram using slicem, resulting in the consumption of many slices, and a limitation on the maximum clock frequency of the design. this problem becomes more severe for the design of large storage capacity tcams. the efficient utilization of sram memory is imperative for the design of tcams on fpgas."
"our proposed tcam design exploits the efficient utilization of sram memory by mapping tcam divisions to shallow sub-blocks of brams on fpga. furthermore, it operates high-speed brams in the design as multipumping-enabled multiported sram, maintaining a high system throughput."
"algorithm 1 describes a simple forwarding strategy for interests in which router i simply selects the first neighbor v in the ranked list of neighbors stored in the fib for prefix n(j) * that satisfies the first condition in the elf rule (line 10 of the algorithm). more sophisticated strategies can be devised that attain load balancing among multiple available routes towards content and can be close to optimum (e.g., [cit] ). in addition, the same interest could be forwarded over multiple paths concurrently, in which case content is sent back over each path that the interest traversed successfully. to be effective, however, these approaches must require the adoption of a loopfree multi-path routing protocol in the control plane (e.g., [cit] . in this context, the control plane establishes valid multi-paths to prefixes using long-term performance measures, and the data plane exploits those paths using the elf rule and short-term performance measurements, without risking the long delays associated with backtracking due to looping."
"the basic architecture of our proposed tcam memory design is shown in figure 4 . it is operated by two fully synchronized clocks, a system clock clk s and internal clock clk p, such that clk p is p times faster than clk s . an incoming tcam word is registered in a w -bit shift register using the system clock clk s . the log 2 p-bit counter generates a sequence of log 2 p-bit numbers in p internal clock cycles. it is initialized to zero upon reset and it rolls over after every p internal clock cycles. the log 2 p-bits from the counter are concatenated with the log 2 (r d /p) bits from the shift register to make the log 2 r d -bit address space of the sram. at the positive edge of the internal clock clk p, the sram address is executed such that log 2 p-bits from the counter constitute its most significant bits, and points to the start of the corresponding sub-block in sram and the lower log 2 (r d /p) bits from the shift register selects an sram word in the sub-block."
"the name of ndo j is denoted by n(j). the terms neighbor and interface are used interchangeably, and the set of neighbor routers of router i is denoted by (j) and, in addition to the information maintained in ndn or ccnx, it stores the distance assumed by router i to name prefix n(j) * when it forwarded i i [n(j)]. this distance is denoted by d(i, n(j)). a point worth mentioning is that the nonces used in ndn are not needed for interest-loop detection."
the first condition ensures that router i accepts an interest from neighbor k only if router i determines that it can forward the interest for n(j) through a neighbor that is closer to prefix n(j) * than neighbor k is. the second condition ensures that router i accepts an interest from neighbor k only if router i was closer to n(j) * when it sent its interest for n(j) than neighbor k is when the interest from k is received.
"proof: consider an interest for n(j) being issued by consumer s at time t 1 . the forwarding of interests assumed in ccn-elf is based on the best match of the requested ndo name with the prefixes advertised in the network. a router sends back an ndo message to a neighbor that sent an interest for ndo n(j) only if has an exact match of the name n(j) in its content store, and a router that receives an ndo message in response to an interest it forwarded must forward the same ndo message. hence, the wrong ndo message cannot be sent in response to an interest. there are three cases to consider next: (a) there are no routes to the name prefix n(j) * of the requested ndo, (b) the interest traverses a routing loop, or (c) the interest traverses a simple path towards a router d that can reply to the interest."
"our proposed solution is general, and can be applied to many applications. our future work will include the application of the proposed design to various applications. she has authored and co-authored over 100 reviewed journal and conference papers. her research activities cover high performance computer architectures, memory architecture, approximate computing, selfaware computing, and reliable computing. she is a member of the national academy of engineering in south korea. volume 6, 2018"
"we present the first solution to the interest looping problems in ndn and ccnx that works correctly in the presence of aggregation and does not require any modifications to the interest packet formats used in ndn and ccnx. we call this new approach ccn-elf (ccn with expanded look-up of fib), because the detection of interest loops relies on a simple look-up of an expanded fib that stores the distances to name prefixes reported by neighbors of a content router, rather than just the set of next hops to name prefixes."
"several information-centric networking (icn) architectures have been proposed as an alternative to today's internet, and the leading icn approach can be characterized as interestbased. this approach consists of: populating forwarding information bases (fib) maintained by routers with routes to name prefixes denoting content, sending content requests (called interests) for specific named data objects (ndo) over paths implied by the fibs, and delivering content along the reverse paths traversed by interests. the original content-centric networking proposal was the first example of an interest-based icn architecture in which interests need not be flooded and do not state the identity of the sender. today, named data networking (ndn) [cit] and ccnx [cit] are the leading interestbased icn approaches."
"compared to ndn, the additional storage needed to maintain distances to prefixes through each neighbor and the distance assumed to a name prefix when an interest is forwarded is more than compensated by the storage savings derived from not having to store the nonces included in interests. the mechanisms needed for ccn-elf can be adopted in ndn and ccnx, because it does not change any of the packet formats, and the additional storage needed to implement the elf rule is proportional to the number of fib entries."
"algorithms 1 describes the steps taken by routers to process interests. our description does not take into account such issues as load balancing of available paths to name prefixes, congestion-control, or the forwarding of an interest over multiple paths concurrently. for simplicity, it is assumed that all interest retransmissions are carried out on an end-to-end basis (i.e., by the consumers of content) rather than relaying routers. hence, routers do not attempt to provide any \"local repair\" when a neighbor fails or a nack to an interest is received."
ccn-elf operates in much the same way as ndn and ccnx do. the difference is in the way in which interests are forwarded according to the elf rule using the expanded fibs and the distance information stored in pits.
"the operation of ccn-elf differs from the current specifications of ndn and ccnx only in the way in which interests are forwarded and the modifications needed in fibs and pits. accordingly, we only describe those aspects of ccn-elf that differ from ndn and ccnx. in our description, we assume that interests are retransmitted only by the consumers that originated them, rather than routers that relay interests. routers are assumed to know which interfaces are neighbor routers and which are local consumers, and forward interests on a best-effort basis. furthermore, given that no interest matching policy has been shown to work better than simple exact matching of interests, we assume that routers use exact interest matching as in ccnx [cit] to forward interests."
equation (2) describes that the sram memory usage of our proposed design is our proposed design achieves a considerable reduction in the sram memory usage by a factor of
"field-programmable gate arrays (fpgas) emulate tcam using static random-access memory (sram), by addressing sram with tcam contents. each sram word corresponds to a specific tcam pattern, and stores information on its existence for all possible data of the tcam table. the increase in the number of tcam pattern bits results in an exponential growth in memory usage. this exponential growth in memory usage has been reduced to linear growth by cascading multiple sram blocks in the design of tcam on fpga in previous work [cit] ."
interest loops resulting from inconsistencies in fib entries maintained at different routers are avoided or detected if they occur using the following rule. elf rule: router i accepts interest i k [n(j)] from router k if one of the following two conditions is satisfied:
"the design objective of ccn-elf is to ensure that no interest loops can go undetected even when interests are aggregated, and without requiring any changes to the packet formats used in ndn and ccnx. the design rationale in ccn-elf is twofold. first multi-path content routing protocols [cit] are a much more attractive alternative than single-path routing in content-centric networks. second, the fib of each content router is a readily-available tool to enforce the needed ordering in the forwarding strategy operating in the data plane."
"algorithm 1 implements the elf rule to ensure that no interest looping goes undetected. router i forwards a new interest when condition 1 in the elf rule is satisfied (line 10 of algorithm 1), or aggregates an interest when condition 2 of the elf rule is satisfied (line 18 of algorithm 1). for simplicity, we assume that content requests from local content consumers are sent to the router in the form of interests, and each router knows which neighbors are remote and which are local."
"in ndn and ccnx in ndn and ccnx, a given router uses three primary data structures: a forwarding information base (fib), a pending interest table (pit), and a content store (cs). the forwarding plane uses these three tables to forward interests towards routers advertising having copies of requested content, and send named data objects (ndo) or other responses back to consumers over reverse paths traversed by interests."
the multipumping technique multiplies the ports of a dual ported sram block by internally clocking it at an integral multiple of the external system clock [cit] . the addresses and data are registered and provided access to the sram block in a circular order by using mod p counter bits as shown in figure 1 . several designs utilize multipumping for the implementation of efficient multiported memory [cit] .
"the elf rule is independent of the specific metric used to measure distances from routers to name prefixes, or the specific way in which a router selects next hops towards a name prefix. section vi proves that using the elf rule is sufficient to ensure that an interest loop cannot occur in ccn-elf, without a router in the loop detecting that the interest has been forwarded incorrectly."
"a router uses its fib to route interests towards the desired content producer advertising a content-name prefix . a fib is populated using content routing protocols or static routes. the fib entry for a given name prefix lists the interfaces that can be used to reach the prefix. in ndn [cit], the fib entry for a name prefix also contains a stale time after which the entry could be deleted; the round-trip time through the interface; a rate limit; and status information stating whether it is known or unknown that the interface can bring data back, or is known that the interface cannot bring data back. a cs is a cache for content objects. with on-path caching, routers cache the content they receive in response to interests they forward."
"we have recently proposed an approach [cit] that remedies the interest-loop-detection problems in ndn and ccnx. this approach requires an interest to state a hop count to the intended name prefix. we have proven that this approach prevents interests from looping, independently of the state of fibs. however, a limitation of this approach is that the routing protocol operating in the control plane of the network must maintain hop counts to name prefixes in addition to any other type of distance information that may be used in the network (e.g., congestion-or delay-based distances)."
the demand for efficient utilization of sram memory in the design of tcam and the speed provided by existing fpga-based tcam solutions make the use of multipumping based multiported sram more practical for designing tcam memory on fpga. our proposed tcam design aims to achieve efficient memory utilization with a high throughput.
the update of a tcam word is performed in each tcam memory unit of the design in parallel. the worst-case update latency of the proposed design comprises r d /p system cycles.
"sections iv describes ccn-elf, which ensures that interest loops are detected if they occur, even if interests from different consumers are aggregated. sections vi proves that ccn-elf ensures that no interest loop can go undetected and that any interest must receive a response within a finite time. section vii addresses performance implications of ccn-elf."
"pits are used in ndn and ccnx to keep track of the neighbors to which ndo messages or nacks should be sent back in response to interests, allow interests to not disclose their sources, and enable interest aggregation. a pit entry in ndn lists the name of a requested ndo, one or multiple tuples stating a nonce received in an interest for the ndo and the incoming interface where it was received, and a list of the outgoing interfaces over which the interest was forwarded. a pit entry in ccnx is similar, but no nonces are used."
"[% interest can be aggregated: ] the pair of numbers next to a router in figure 2 (a) indicate the distance from that router to prefix n(j) * through a neighbor and the ranking of the neighbor according to the fib of the router. let the triplet (v, h, r) denote a neighbor, a distance to the prefix through that neighbor, and its ranking in the fib of a router. assuming that all neighbors of a router are listed in the fib entry for n(j) * in figure 2 within a finite time, the fibs of all routers are updated to reflect the new shortest paths that take into account the changes to links (a, p) and (b, q). once fibs are consistent, interests regarding objects in the name prefix n(j) * are forwarded along shortest paths towards n(j) * . the elf rule is only a sufficient condition to avoid interest looping, and it is possible for a router to assume that an interest is traversing a loop when this is not the case. in the example in figure 2(d), router b could forward the interest to router q without causing a loop. however, the elf rule is not satisfied by router q and b cannot select it."
"given the speed with which fibs are updated to reflect correct distances computed in the control plane, false loop detections are rare, and their occurrence is better than having to store pit entries for interests that cannot receive responses, until their lifetimes expire after many seconds."
"lt i stores the cost of the link from router i to each of its neighbor routers. it can be updated based on the congestion perceived over the link. the cost of the link from router i to neighbor v is denoted by c(i, v)."
"case 1: if there is no route to n(j) *, then it follows from the operation of ccn-elf that a router issues a nack stating that there is no route. that nack is either forwarded successfully back to s or is lost due to errors or faults. in the latter case, a router must send a nack back towards s stating that the interest expired or the route failed."
"multipumping divides the achievable internal clock frequency of the design by the multipumping factor, to obtain the operating frequency of the overall system [cit] . although an increase in the multipumping factor p results in a higher memory efficiency for the design of tcam, only the use of small multipumping factors is practical in order to avoid a significant drop in the operating frequency of the overall system. overall multipumping factor p controls a tradeoff between the sram memory efficiency and speed of the proposed design."
"an interest forwarding strategy must ensure that either an ndo message or a nack is received within a finite time by the consumer who issues an interest. the following theorem shows that this is the case for ccn-elf, independently of the state of the topology or the fate of messages."
"our proposed design provides increased design flexibility in terms of the speed vs memory usage tradeoff. the designer must consider the important design factors such as the required storage capacity, relative availability of brams on the target fpga, and required throughput for the selection of the multipumping factor in our proposed design."
"it follows from the above argument that, for l to exist when each router in the loop follows the elf rule to send interests asking for n(j), it must be true that"
"tcam is widely employed to design high-speed search engines and has applications in networking, artificialintelligence, data compression, radar signal tracking, pattern matching in virus-detection, gene pattern searching in bioinformatics, image processing, and to accelerate various database search primitives [cit] . the internet-ofthings and big-data processing devices employ tcam as a filter when storing signature patterns, and achieve a substantial reduction in energy consumption by reducing wireless data transmissions of invalid data to cloud servers [cit] ."
theorem 6.2: ccn-elf ensures that a consumer that issues an interest for a valid ndo with name n(j) receives an ndo message for name n(j) or a nack within a finite time.
"the information used to enable correct forwarding of interests, ndo messages, and nacks are the name of ndos, distance information stored in fibs, and link-cost information to each neighbor. interests, ndo messages and nacks (called interest return in ccnx [cit] ) are assumed to specify the same information used in ndn or ccnx."
"the following theorems show that no interest loops can occur and be undetected if ccn-elf is used, and that every interest must receive a response (an ndo message or a nack) within a finite time. these results are independent of whether the network is static or dynamic, the specific caching strategy used in the network (e.g., at the edge or along paths traversed by ndo messages [cit] ), the retransmission strategy used by content consumers or relay routers after experiencing a timeout or receiving a nack, or how many paths are used to forward an interest."
"undetected interest loops have been shown to occur in ndn and ccnx, which causes interests to timeout without content or negative acknowledgments being received in response. we introduced ccn-elf, the first approach to content-centric networking that eliminates the possibility of undetected interest loops without requiring packet formats to be modified in ccnx or ndn."
"section ii summarizes the operation of the ndn and ccnx forwarding planes. since the introduction of the original content-centric networking proposal [cit], the research community (e.g., [cit] has assumed that the forwarding planes of ndn and ccnx are such that they can recover from resource failures and congestion problems, because packets containing data are sent back in response to interests. however, we have shown [cit] that this is not the case in general. more specifically, we have proven that interest loops may go undetected when interests from different consumers requesting the same content are aggregated and interests are forwarded along routing loops, which may occur due to rankings of routes, failures, mobility, or congestion."
"contemporary fpgas implement block-ram (bram) in the silicon substrate, and offer a high speed. for example, xilinx virtex-6 xc6vlx760 fpga contains 720 brams of size 36 kb [cit], and provide operating frequencies of greater than 500 mhz [cit] . designers utilize these high-speed sram blocks to design sram-based tcams on fpga."
the elf rule also points out a way in which sifah [cit] can be modified to use distance values other than minimumhop counts. the end result would be the ability to eliminate interest looping by having an interest state the distance to the requested content and fibs maintain distances reported by neighbors considered to be next hops to prefixes.
"the design of memory-efficient tcams requires shallow sram blocks on fpgas. multipumping-based multiported sram emulates the sub-blocks of a dual port sram block as multiple shallow sram blocks, by operating sram with a higher frequency clock, allowing access to its sub-blocks in one system cycle. researchers have designed efficient multiported memories using brams on fpga [cit] ."
"with respect to wearability, the proposed system should satisfy two requirements; (1) the sliding structure should be fixed on the finger phalanges during the hand motion, and (2) users with various finger sizes should be able to wear the proposed system. we have considered the characteristics of the lower part of the finger joint during finger motion. figure 10a shows the lower part of the finger when the finger is flexed. as the finger is flexed, the lower part of the finger joint becomes folded, and the flesh under the phalanges is concentrated in the middle of the phalanges. in particular, we consider that the lower part of the finger joint could be a stationary point for fixing the sliding structure, since the lower part has an empty space during hand motion. after trial and error, as shown in figure 10b, the maximum flexion of the finger could be achieved by fixing a 3 mm diameter bar at the lower part of the finger joint. the bar under the finger joint is referred to as the \"underjoint structure\". this underjoint structure is connected to the sliding structure with rubber bands, allowing the device to fit various finger sizes; the rubber band can fit different thickness of finger and the sliding structure can fit different finger lengths. additionally, the sliding structure remains in place during hand motion, as shown in figure 10b ."
"compiler generator tools, such as eli [cit], elan [cit], stratego/xt [cit], asf+sdf [cit], txl [cit], jastadd [cit], and silver [cit] may all be used for source-to-target language transformation. they all have wider ambitions than our work, supporting specifications of full-scale compilers, many including static and dynamic semantics as well as turing complete computation on asts of the source language which obviously precludes our level of safety guarantees."
"ternary and and ternary or gates are known as the minimum and maximum gates. these two gates can be generated by adding one ternary not gate after t-nand and t-nor, respectively. therefore, the two-input t-and gate needs nine fets (six for t-nand and three for not), and the t-or gate also needs nine fets. similar design methods can be used to implement the multi-input t-and and t-or logic gates."
"in this paper, we propose a 3d hand motion measurement system that addresses three challenges of existing systems: elimination of the calibration process, universal wearing of a single system, and proper system design to measure the 3d cmc joint motion of the thumb. an absolute rotation measurement structure was developed in a compact configuration using linear hall sensors to exclude the calibration process. from observations of the lower part of the finger joint as the fixing point, the new wearing method using an underjoint structure and a rubber band provided an adaptable fit to the hand bones for users with various hand sizes."
"the tool may be used by: 1) programmers to extend existing languages with their own macros; 2) developers to embed dsls in host languages; 3) compiler writers to implement only a small core language (and specify the rest externally as extensions); and 4) developers and teachers to build multilayered languages. the banana algebra tool is available-as 3,600 lines of o'caml code-along with examples from its homepage:"
"in order to permit modular language development and separate each of the ingredients in a transformation, we added local definition mechanism via the standard let-in functional programming local binder construction. thus, we add to the syntax of both languages and transformations; variables (figure 2, rules l2 and x2) and local definitions (figure 2, rules l7, and x7)."
"adder is the most important arithmetic logical unit in a microprocessor, because all the mathematic operators and computations can be finally reduced into addition. ternary logic decreases the requirement of components and interconnections by realizing more data transmission over an interconnection wire. therefore, it is promising in high speed and high efficient data processing."
"the two links of the rotation measurement structure must be parallel to the phalanges during hand motion. the easiest way to accomplish this is to attach the structures to the finger phalanges; however, as it turns out, the two links of the rotation measurement structure should slide over the finger phalanges instead. the necessity of a sliding structure is demonstrated in figure 9 . the upper side of figure 9 shows a simplified representation of the phalanges and the rotation measurement structure and the bottom side shows the corresponding structure design. in this setup, the finger joint is assumed to be a circular disk, and the phalanges rotate about the disk's center. the rotation measurement structure is constrained to be parallel to each finger phalanx. in figure 9a, the rotation axis of the rotation measurement structure is located directly above the center of the finger joint; a dotted line connects the center of the finger joint and the center of the rotation measurement structure. as the finger joint flexes, as shown in figure 9b, the axis of the rotation measurement structure should move forward to remain parallel to each phalanx. this requires the rotation measurement structure to slide over the finger phalanges. therefore, a hook-type sliding structure is incorporated into our design. the rotation measurement structure has grooves corresponding to the hook shape of the sliding structure to facilitate sliding motion."
"in summary, we perform a systematical study on the 2d-materials-based ternary logic from individual ternary logic gates to large scale integrated circuits. we design and fabricate various ternary logic gates with different logic functions, which show good ternary performance with simplified circuital structure compared to traditional silicon ternary and cnt ternary. then by developing the ternary cycling gate and the improved ternary adder algorithm, we propose a 19-trit ternary adder design with great circuitry simplicity. the design shows about a 50% reduction in the required number of transistors compared to the existing cnt ternary and silicon ternary, and it also shows itself competitive to the classic cmos binary design with potential reduction in the number of required transistors and computation steps at the same data throughput. this work shows the potential for ternary logic in future integrated circuits application with higher data density, smaller chip area, faster computation speed and less latency."
"similarly, the banana algebra tool can be used to extend java with lots of syntactic constructions which can be desugared into java itself; e.g., for-each control structures, enumeration declarations, design patterns templates, and so on. here, we will give only one simple example of a java extension; the repeat-until of figure 3 (c):"
"step 2, the ab_sum and the input carry of c in are summed up in the half-adder 2 (red dashed box). the generated result of sum will be the final summation of a, b and c in . step 3, the two generated carry trits, ab_carry from the half-adder 1 and abc_carry from the half-adder 2, will be sent into an or gate, and will generate the final carry, which will be a new c in to the next series of full-adder."
"there exists a body of work on catamorphisms in a category theoretical setting [cit] . however, these are theoretical frameworks that have not been turned into practical tool implementations supporting the notion of addition on languages and transformations which plays a crucial role in the extension pattern of figure 1 and many of the examples."
"because the bp flakes were easily deteriorated in air conditions, great attention should be taken to protect the samples during the whole fabrication. the samples should always be kept in the glove box, and the time of exposure to air in-between process steps should be minimized."
"where θ is the rotation angle, l 1, l 2 are the constant lengths between the rotation axis to axis 1, 2, respectively, and d is the distance between axis 1 and 2, which is measured by the linear hall sensor and the magnet. the change in length was measured using the magnet and the linear hall sensor; the distance between them was calculated by measuring the strength of the magnetic field. the distance relationship between the linear hall sensor and the magnet was obtained experimentally, using a potentiometer. generally, the strength of the magnetic field is inversely proportional to the square of the distance. the fitting result depicting this relationship is close to the measured value with a root mean square error (rmse) of 0.02 v. the distance between the sensor and the magnet is as follows:"
"also, many wearable systems have not been taken into account measurement of a thumb motion in their design. they measured the thumb motion by properly rotating and positioning the sensing structure of the index finger on the thumb [cit] . therefore, frequently, the sensor alignment did not match the actual position of the thumb joint in real applications, resulting in low measurement accuracy [cit] . moreover, the thumb has additional supination/pronation (s/p) motion at the carpometacarpal (cmc) joint if flexion/extension (f/e) and abduction/adduction (ab/ad) axes are considered not to match the anatomical ones, which are non-orthogonal and non-intersecting [cit] . however, this has been rarely addressed."
"finally, the two as-designed half-adders can be combined into a 1-trit ternary full-adder, as shown in figure 7a . the operating principle can be explained as follows: step 1, the input a and b are summed up in the half-adder 1 (blue dashed box) and they then generate the result of ab_sum and ab_carry. step 2, the ab_sum and the input carry of cin are summed up in the half-adder 2 (red dashed box). the generated result of sum will be the final summation of a, b and cin."
"so far, we have designed and fabricated four types of inverters (sti/nti/pti/dci) by using 2d materials. compared to the existing silicon ternary [cit] and cnt ternary [cit], our design has largely simplified the ternary logic design complexity. there are only two or three fets involved in one ternary logic gate, which is only about half of that in silicon ternary or cnt ternary. therefore, it is possible to accomplish simplicity in modern digital design by using ternary logic due to the reduced circuit overhead and chip area."
"we propose an algebra of 16 operators on languages and transformations as a simple, incremental, and modular way of specifying safe and efficient syntactic language extensions through algebraic composition of previously defined languages and transformations."
the characteristics of the hand and overview of the proposed system are explained in section 2. detailed descriptions of each component are discussed in sections 3 and 4. the measurement performance verification of the thumb and index finger is presented in section 5. conclusions and future works are summarized in section 6.
"adder is the most important arithmetic logical unit in a microprocessor, because all the mathematic operators and computations can be finally reduced into addition. ternary logic decreases the requirement of components and interconnections by realizing more data transmission over an interconnection wire. therefore, it is promising in high speed and high efficient data processing."
"everything presented in the paper has been implemented in the form of the banana algebra tool which, as argument, takes a transformation term of the algebra which is then analyzed for safety and reduced to a constant catamorphism which may subsequently be run to transform an input program."
"the approach captures the niche where full-scale compiler generators as outlined in section 7 are too cumbersome and where simpler techniques for syntactic transformation are not expressive or safe enough, or do not have sufficient support for incremental development."
"we conducted another experiment on circumduction motion, which is the combination of all joint angles of the cmc joint. experimental setup was similar to that of figure 12a . the vector of the red arrow on the thumb was measured directly by the motion capture system and was compared with the calculated vector of the proposed system. figure 13 shows the comparison of the orientation vector itself during the circumduction motion. as this motion is the combination of the f/e, ab/ad, and s/p motions, the rmse is bigger than each rmse of those motions in figure 12 . however, the performance is better than that of previous studies [cit] ."
"syntax macros [cit] provide a means to unidirectionally extend a \"host language\" on top of which the macro system is hard-wired. extension by syntactic macros corresponds to having control over only \"step iii)\" of figure 1 (some systems also permit limited control over what corresponds to \"step ii)\"). by contrast, our algebraic approach can be used to extend the syntax of any language or transformation; and not just in one direction-extensions may be achieved through addition, composition, or otherwise modular assembly of other previously defined languages or transformations. uni-directional extension is just one form of incremental definition in our algebraic approach. the work on extensible syntax [cit] improves on the definition flexibility in providing a way of defining grammars incrementally. however, it supports only three general language operations: extension, restriction, and update."
"in the experiments of the index finger in figure 11a, two markers were directly attached on the hand: left sides of finger and the dorsum. in the case of the metacarpal of the thumb, it was difficult to attach markers on the hand because the structure covered the upper side of the metacarpal, and much flesh was concentrated on the left side of the metacarpal such that it was highly flexible. therefore, the thumb experiment was performed by attaching a planar structure consisting of three markers, a square black structure with three markers in figure 12 next to the sliding structure on the dorsum and the metacarpal."
"in this paper, we propose an exoskeleton system with linear hall sensors that overcomes the limitations of the existing systems described above. the goals of the proposed system are to (1) develop a structure that can measure hand motion without a calibration process; (2) propose a new wearing method to accommodate users of various hand sizes, while the system structures remain tightly fixed to the hand; (3) develop a new measuring structure for complex three-dimensional (3d) motions of the thumb, including s/p motion."
"the proposed system also can measure s/p motion; the details are illustrated in figure 8 . figure 8a shows the initial position of the cmc joint structure, and figure 8b presents the situation when the relative positions change. as shown in the left side of figure 8c, there are rotary hall sensors attached to the ends of the slider that connect the dorsum of the hand and the metacarpal of the thumb to measure rotation of the dorsum and the metacarpal; these rotations are referred to as θ thumb and θ dorsum, respectively. the reference lines of θ thumb and θ dorsum are indicated by dotted red lines and are set to be positive in the counterclockwise direction."
"diverse hand motion measurement systems have been actively investigated for the applications of virtual reality (vr), tele-operation, biomechanics, etc. many methods have been proposed to measure the hand motion; the most common systems are optical marker-based motion capture systems, which use infrared signals to measure the three-dimensional position of markers [cit] . however, stable and robust measurement of the hand is still challenging due to occluded markers, ghost markers and dense marker sets caused by a high degree of articulation, self-similarity, and the small scale of the hand. therefore, wearable systems have been developed in order to overcome those problems when measuring the hand motion."
"sub2 similar results are shown for various subjects of different hand sizes in table 3 . the results are reasonable because the proposed system was designed to adaptable to various users; (1) measurement principle is not dependent on different hand sizes because it directly measures absolute rotation angle, and (2) the system can be properly worn to users of different hand sizes; the rubber band fits the thickness of the finger and the sliding structure fits the length of the finger. also, the rmse of f/e of the pip joint was larger than those of the mcp joint in each subject; inaccuracy of the pip joint measurement shows consistency for multiple subjects."
"although very simple, capable of trivial recursion only, we claim that this kind of constructive catamorphisms provide a basis for programming language extension. we shall investigate this claim in the following section."
"integrated circuits (ic) are the cornerstone of modern information society and are widely used in almost all of the electronic systems. binary digital integrated circuits are the most prevalent computing technology today, because the binary signals exist naturally in semiconductor electronic devices, and binary boolean algebra can be easily implemented by using binary logic gates. however, the performance of binary logic is fundamentally limited by its low density of logic states, that is, only two logic levels (0, 1) can be transmitted over a given set of lines [cit] . therefore, it needs a large number of logic gates and transistors to reach the required data size. besides, even more interconnected wirings between the system components are required in integrated circuits. for example, in a very large scale integrated (vlsi) circuit, approximately 70 percent of the area is devoted to interconnection, 20 percent to insulation, and only 10 percent to device [cit], which exceedingly increases the complexity, both in design and manufacture."
"where p i,x, p i,y, p i,z represent the i th position in the direction of x, y and z -axes, respectively. other parameters can be found in figure 7 . table 1 . dh parameters of p 1 . the vector direction of the metacarpal of the thumb is given by"
"the algebraic approach offers via 16 operators a simple, incremental, and modular means for specifying syntactic language extensions through algebraic composition of previously defined languages and transformations. the algebra comes \"for free\" in that any algebraic transformation term can be statically reduced to a constant transformation without compromising the strong safety and efficiency properties offered by catamorphisms."
"to satisfy these goals, (1) we introduce a structure that can measure absolute rotation angles, with a compact design that does not require any calibration process; (2) a new wearing method that uses the lower parts of the hand joints and rubber bands instead of a glove; and (3) a new measuring structure to calculate the direction of the vector parallel to the metacarpal of the thumb directly, rather than to measure the finger joint angles of the cmc joint separately. the prototype of the proposed system was manufactured for the thumb and index finger, and the performance was verified using an optical motion capture system."
"appl. sci. 2019, 9, x for peer review 4 of 13 voltage and on-off ratio, as well as a high mobility, were chosen for the second electron-beam lithography and electron-beam evaporation to fabricate the interconnections. because the bp flakes were easily deteriorated in air conditions, great attention should be taken to protect the samples during the whole fabrication. the samples should always be kept in the glove box, and the time of exposure to air in-between process steps should be minimized."
"they have been developed with various mechanisms: inertia measurement units (imu) [cit], infrared-based [cit], bending sensors [cit], soft sensors [cit], etc. perception neuron [cit] can measure whole body motion using sensing modules based on imus; hand motion can also be measured if the sensing modules are connected to the finger. leap motion estimates hand gestures using infrared information [cit] . however, infrared-based systems are significantly influenced by hand gestures. for example, it is difficult to estimate the gesture of an index finger when the index finger is covered by the middle finger."
"in practice, it turns out to be useful to also be able to define (local) transformations while specifying languages; and, orthogonally, to define (local) languages while specifying transformations. hence, we add the local definitions l8 and x8 to figure 2"
"where k can take logic values of \"0\", \"1\" or \"2\". note that the outputs of the decoder have only two logic values (namely \"2\" and \"0\"), corresponding to highest logic state (\"2\") and lowest logic state (\"0\") in binary logic, therefore, binary or gates can be used here. a ternary buffer is also presented in figure 4a . the ternary buffer contains one dci and one sti. it is easy to deduce that the response of the buffer is given by for the t-nor, it also contains two-step of logic operations: it maximizes the two inputs of a and b, and then outputs the result after a not transformation. if the maximum input is \"2\", the output is \"0\". if the maximum input is \"1\", the output is \"1\". if the two inputs are both \"0\", the output is \"2\". the designed circuits for the t-nor are shown in figure 3d, where four bp transistors and two mos 2 transistors are involved. the transistors' parameters are similar to that of a t-nand. when the input of a is \"2\", the bp2 fet is fully turned off, so the output is \"0\", no matter what b is; when the input of a is \"1\", the bp2 fet is half turned on, so the output will not be \"2\" (yet it will be \"0\" or \"1\", dependent on b); when the input of a is \"0\", the bp2 fet is turned on as a p-type, so the output is dependent on b (figure 3e )."
"ternary and and ternary or gates are known as the minimum and maximum gates. these two gates can be generated by adding one ternary not gate after t-nand and t-nor, respectively. therefore, the two-input t-and gate needs nine fets (six for t-nand and three for not), and the t-or gate also needs nine fets. similar design methods can be used to implement the multi-input t-and and t-or logic gates."
"finally, the two as-designed half-adders can be combined into a 1-trit ternary full-adder, as shown in figure 7a . the operating principle can be explained as follows: step 1, the input a and b are summed up in the half-adder 1 (blue dashed box) and they then generate the result of ab_sum and ab_carry."
"extension is simple because we base ourselves on a well-proven and easy-touse formalism for well-typed syntax-directed transformations known as constructive catamorphisms. these transformations are specified relative to a source and a target language which are defined via context-free grammars (cfgs). catamorphisms have previously been studied and proven sufficiently expressive as a means for extending a large variety of programming languages via transformation [cit] . hence, the main focus of this paper lies not so much in addressing the expressiveness and which transformations can be achieved as on showing how algebraic combination of languages and transformations results in highly modular and incremental language extension. incremental and modular means that any previously defined languages or transformations may be composed algebraically to form new languages and transformations. safety means that the tool statically guarantees that the transformations always terminate and only map syntactically legal input terms into syntactically legal output terms; efficiency means that any transformation is guaranteed to run in linear time (in the size of input and generated output)."
"we found it convenient to permit lexical structure to be specified using regular expressions, as often encountered in parser/scanner tools. however, the tool currently considers this an atomic terminal layer that cannot be transformed."
"where p is the vector direction parallel to the metacarpal, and p 1, p 2 are the position vectors from the reference point on the dorsum to the positions of p 1, p 2, respectively. once the vector direction is determined, the f/e and ab/ad angles can be calculated according to a predetermined thumb joint model. in this paper, it is assumed that the f/e and ab/ad axes are orthogonal and are matched to the xand yaxes of the 0 th frame in figure 7b . the f/e and ab/ad angles are calculated by rotating the unit vector of (0, 0, −1), using euler angles in the direction of the xand yaxes with respect to the 0 th frame, to become the p vector of equation 9 . in terms of the coordination of the cmc joint in figure 7b, we just use simple rotation axes for validation [cit] because our proposed method does not consider the anatomical rotation axes to measure the motion of the cmc joint; the orientation of the cmc joint is directly measured as a vector form."
"the tool can be used for any syntax-directed transformation that can be expressed as catamorphisms (which includes all the transformations of metafront [cit] and xsugar [cit] ). this includes translation between different languages, transformations on a language, and format conversion, but here we will focus on language extension from each of the \"four scenarios\" from the introduction. before that, however, we would like to show a concrete example program."
"step 3, the two generated carry trits, ab_carry from the half-adder 1 and abc_carry from the halfadder 2, will be sent into an or gate, and will generate the final carry, which will be a new cin to the next series of full-adder. next, note that the logic state of carry in the ternary adder ( figure 6c ) can only be \"0\" or \"1\". for example, the carry of 0 + 0, 0 + 1, 0 + 2 and 1 + 1 is \"0\", and only the carry of 1 + 2 or 2 + 2 is \"1\". therefore, it is not necessary to use the full ternary decoder. the improved ternary half adder is presented in figure 6b . when c is \"0\", the sum is a; when c is \"1\", the sum is a + 1. while the carry is always c1 + a2."
"in figure 12a,b, the maximum errors appear near the maximum f/e and ab/ad angles. possible reasons include static friction of the system, or not enough fluidity in the slider due to manufacturing issues. because the maximum error only occurred at the end of the hand motion, not the beginning, the static friction may not be the main reason. as the maximum errors occurred at similar angles during the hand motion, we conclude that the fluidity in the slider between the dorsum and the thumb due to poor manufacturing quality may be a plausible reason for the measurement error."
"the design of the rotation measurement structure is shown in figure 2b . this structure was designed to measure the absolute angle of rotation instead of the change in the length of the finger, to exclude the calibration process. a linear hall effect sensor (hereafter referred to as the linear hall sensor) was used to design a compact rotation measurement structure. the sliding structure was required for the rotation measurement structure to slide over the finger phalanx as shown in figure 2b . it had a hooked structure and the rotation measurement structure had a groove to combine with the sliding structure and moved on the sliding structure according to the rotation of the finger joint."
"for measurement of the motion of the cmc joint, the proposed rotation measurement structure cannot be applied for the following reasons. first, the thumb has additional s/p motion, which makes it difficult to measure using the proposed rotation measurement structure. second, it is difficult to fix the rotation measurement structure to the cmc joint without disturbing wrist movement, which has the rotational motion of two dofs."
"the design of the rotation measurement structure is shown in figure 3 . although there are commercial sensors that can measure the absolute rotation angle, a structure using a linear hall sensor and a magnet offers a more compact design than conventional systems, which typically include an optical encoder and a rotary hall sensor. the optical encoder requires a disk in which the passage of light depends on the rotation angle; the minimum width of the disk is 10 mm. the rotary hall sensor measures the absolute rotation angle by distinguishing the rotation of the sine-wave magnetic field made by the magnet which has s and n poles on the left and right sides. the rotary hall sensor and the magnet itself are extremely small. however, a electronic board is required to house the sensor circuit, which includes capacitors and resistors. therefore, the height of the absolute rotation angle measurement structure of conventional sensors is more than 10 mm. in the proposed design configuration, the rotation measurement structure was about 6-7 mm in height, as shown in figure 3a, which is more compact than other commercially available systems. this is because the sensor and housing structure did not need to be located on the rotation axis in the proposed system. as shown in figure 3a, the linear hall sensor and magnet were installed at positions other than on the rotation axis."
"those systems have the advantage that the sensor signal is not affected by external disturbances. however, they require a cumbersome calibration process. the lower sensor value when the user wears the system is different individually due to various sizes of the hand and how tightly the system is worn. the upper value is also different because of various radiuses of the finger joint. therefore, the calibration process which maps the sensor signal to actual finger joint angle is required. as an extreme case, cyberglove may require up to 44 different poses or guided movements [cit] ."
"once defined, languages and transformations can all be added, composed, or otherwise put together. thus, a programmer can use the tool to essentially tailor his own macro-extended language; e.g., \"(java \\ while) + sql\"."
"in this paper, we propose a new method to measure the cmc joint. the proposed wearing method that will be explained in next section can fix the sliding structure on the metacarpal without any restrictions on the cmc joint. then, the vector direction, which is a combination of the f/e and ab/ad motions by the cmc joint of the thumb, is measured directly to calculate the positions of two points on the sliding structure. figure 6 shows the sliding structure placed on the metacarpal of the thumb using the proposed wearing method and its movement with 3d thumb motion. here, the user can fix the sliding structure on the metacarpal under various movements of the thumb without any restrictions on the cmc joint. also, the vector direction of the metacarpal can be obtained from the position of the sliding structure, as the sliding structure is always kept parallel to the metacarpal."
"the prototype of the proposed system is shown in figure 2a . figure 2b-d show the major components of the system: a rotation measurement structure, a sliding structure, wearing configuration by an underjoint structure and rubber bands, and a cmc joint measurement structure."
"of course, catamorphisms may be mimicked by disciplined style of functional programming, possibly aided by traversal functions automatically synthesized from datatypes [cit], or by libraries of combinators [cit] . however, since within a general purpose context, it cannot provide our level of safety guarantees and would not be able to compile-time factorize composition (although the functional techniques deforestation/fusion [cit] may-in some instances-be used to achieve similar effects)."
"constructive catamorphisms have a lot of interesting properties; they can be statically verified for syntactic safety, are guaranteed to terminate, and to run in linear time."
"the first three rules just trivially recurse through the input structure producing an identical output structure. zero becomes the identity function, successor adds a \"lambda s\" in front of the encoding of the argument, and predecessor peels off one lambda by applying it to the identity function (note that the predecessor of zero is thus consequently defined as zero). this will, for instance, map succ zero to its encoding lam s (lam z (var z))."
"notably, the equation for θ s/p does not change, even when f/e and ab/ad motions of the cmc joint occur. the left side of figure 8c shows the position with ab/ad motion. the s/p angle is not affected, because the rotation structures next to the structures of θ thumb and θ dorsum are rotated. in the case of f/e motion (right side of figure 8c ), the s/p angle is also not affected as the rotation axis of the slider is rotated. therefore, the f/e and ab/ad motions of the cmc joint are not related to θ s/p measurements, because the different axes from the rotation of θ thumb and θ dorsum are rotated instead; the formula for calculating the s/p angle in actual motion is the same as equation 11 ."
"a catamorphism (aka., banana [cit] ) is a generalization of the list folding higher-order function known from functional programming languages which processes a list and builds up a return value. however, instead of working on lists, it works on any inductively defined datatype. catamorphisms have a strong category theoretical foundation [cit] which we will not explore in this paper. a catamorphism associates with each constructor of the datatype a replacement evaluation function which is used in a transformation. given an input term of the datatype, a catamorphism then performs a recursive descent on the input structure, effectively deconstructing it, and applies the replacement evaluation functions in a bottom-up fashion recombining intermediate results to obtain the final output result. many computations may be expressed as catamorphisms. as an example, let us consider an inductively defined datatype, list, defining non-empty lists of numbers:"
"our work shares many commonalities and goals with that of syntax macros, source transformation systems, and catamorphisms (from a category theory perspective) the relation to which will be outlined below."
"in order to issue strong safety guarantees, in particular termination, we clearly sacrifice expressibility in that the catamorphisms are not able to perform turing complete transformations. however, previous work using constructive catamorphisms for syntactic transformations (e.g., metafront [cit] and xsugar [cit] ) indicate that they are sufficiently expressive and useful for a wide range of applications."
"in figure 7b, we present the design block diagram of the 19-trit ripple-carry adder, in which a18a17…a1a0 is a ternary number of a input, b18b17…b1b0 is a ternary number of b input, s18s17…s1s0 is the output summation result, c18c17…c1 is the carry, c0 is always 0 and cout is the overflow bit. a18a17…a1a0 and b18b17…b1b0 is ternary number of a and b, respectively. s18s17…s1s0 is the output summation result, c18c17…c1 is the carry and the first input carry of c0 is 0."
the truth table of ternary not-and (nand) and ternary not-or (nor) gates are given in figure 3a . the two-input ternary logic functions of t-nand and t-nor gates [cit] are defined by the following two equations:
"where k can take logic values of \"0\", \"1\" or \"2\". note that the outputs of the decoder have only two logic values (namely \"2\" and \"0\"), corresponding to highest logic state (\"2\") and lowest logic state (\"0\") in binary logic, therefore, binary or gates can be used here. a ternary buffer is also presented in figure 4a . the ternary buffer contains one dci and one sti. it is easy to deduce that the response of the buffer is given by"
"lastly, magnetic disturbances such as smartphones and motors might affect the sensor signal of the linear hall sensor. however, the linear hall sensor are not affected by those disturbances because it can only measure large magnetic field. the measurement range of the linear hall sensor (wsh135-xpan2) used in this system can measure around ±1000 g (gauss)."
"we handle whitespace via permitting a special whitespace terminal named \"$\" to be defined (it defaults to the empty regular expression). the semantics is that the whitespace is interspersed between all terminal and nonterminals on the right-hand-side of all productions. for embedded languages, it might be interesting to have finer grained control over this, but that is currently not supported by our tool."
"the performance of the proposed hand motion measurement system was verified using an infrared motion capture system. figure 11a shows the experimental setup of motion measurement performance of the index finger, using the prime 13 model by optitrack [cit] . two markers were attached to one finger phalanx around the finger joint to be measured to make a vector, and the actual finger joint angles were compared using the angles between the two vectors rotating around the finger joint."
"in the future, it would be interesting to also add a means for alpha conversion and static semantics checks on top of the syntactic specifications"
"in order to validate the algebraic approach, we have implemented everything in the form of the banana algebra tool which we have used to experiment with different forms of language extensions."
our contributions include the design of an algebra of languages and trans-formations for incremental and modular syntactic language extension built on top of catamorphisms; a proof-of-concept tool and implementation capable of working with concrete syntax; and an evaluation of the algebraic approach.
"the direction of future works includes easy wearability and additional experiments with various users. it is quite cumbersome to match the underjoint structure correctly. a cradle-based or fabric-based structure may allow for easier wear. in future experiments, we plan to recruit users with various hand sizes to verify the measurement performance by the improved system in terms of wearability and precise manufacturing. also, repeatability of the proposed system will be conducted."
"no structure on cmc joint the structure design to calculate the vector direction of the metacarpal and the coordinates of the denavit-hartenberg (dh) parameter are shown in figure 7, and tables 1 and 2. the proposed system measures the absolute positions of two points of the sliding structure (p 1, p 2 ) with respect to the dorsum, and obtains the direction of the vector connected by the two points. forward kinematics by the dh parameter is used to calculate the positions of p 1 and p 2 . the positions of p 1 and p 2 using forward kinematics are as follows:"
"an important property of the algebra which is built on top of catamorphisms is that it is \"self-contained\" in the sense that any term of the algebra may be reduced to a constant catamorphism, at compile-time. this means that all high-level constructions offered by the algebra (including composition of languages and transformations) may be dealt with at compile-time, before the transformations are run, without sacrificing the strong safety and efficiency guarantees."
"the tool may be used for many different transformation purposes, such as transformation between different languages (e.g., for translating java programs into html documentation in the style of javadoc or for prototyping lightweight domain-specific language compilers), transforming a given language (e.g., the cps transformation), format conversion (e.g., converting bibtex to bibtexml). however, in this paper we will focus on language extension for which we have the following usage scenarios in mind: 1) programmers may extend existing languages with their own macros; 2) developers may embed domain-specific languages (dsls) in host languages; 3) compiler writers may implement only a small core and specify the rest externally; and 4) developers or teachers may define languages incrementally by stacking abstractions on top of each other. we will substantiate these usage claims in section 6."
"as shown in figure 2c, a method was required to hold the sliding structure in place on the finger phalanx even when the finger was in motion, and to apply this for users of various hand sizes. the proposed system could fix the structure and fit various hand sizes via an underjoint structure, located under the lower part of the finger joint, and rubber bands."
"the developed rotation measurement structure was also applied to measure f/e motion of the mcp joint of the thumb; although the mcp joint of the thumb has two dofs rotations, we only applied one rotation measurement structure of f/e motion because the ab/ad motion of mcp joint is dependent on the motion of cmc joint."
note that none of the operators go beyond the expressivity of constructive catamorphisms in that any language term can be statically reduced to a context-free grammar; and any transformation term to a catamorphism.
"for measuring the thumb motion, we propose a new mechanism to measure cmc joint motion as shown in figure 2d . measurement of the motion of the cmc joint is complicated. first, the cmc joint has various rotations: f/e, ab/ad, and s/p motions. second, the cmc joint is attached at the wrist, such that it is difficult to place the rotation measurement structure on the cmc joint itself. in this paper, the orientation of a vector that is parallel to the metacarpal of the thumb is measured directly with respect to a frame on the dorsum rather than measuring f/e and ab/ad motions separately. also, the proposed structure can measure s/p motion."
"with respect to measurement of thumb motion, it was very difficult to design the system to decouple f/e and ab/ad motions of the cmc joint due to its complex anatomical features. additionally, the wearing method that fixed the structure on cmc joint while not interfering with the wrist motion was also difficult. however, the proposed system solved these challenges in a different way. as it was difficult to decouple f/e and ab/ad motions, instead, we calculated the orientation of the metacarpal itself, which was a combination of f/e and ab/ad motions. the obtained result (i.e., the orientation of the phalanx) could be used to realize thumb motion directly in various applications, or f/e and ab/ad angles could be calculated from the predetermined kinematic model of the thumb. lastly, there was no structure on the cmc joint itself; instead, the structure was attached to the metacarpal near the cmc joint using the proposed wearing method, allowing the orientation of the cmc joint to be calculated by the positions of the metacarpal. thus, the cmc joint was not restricted even in the wrist motion. although the proposed system overcame some challenges of existing systems, it still has some limitations. first, precision manufacturing is required to improve the measurement performance. the system itself is designed to guarantee the performance when it is manufactured precisely; the tolerance of the slider between the rotation measurement structure and the sliding structure, and the length change measurement structure including the linear hall sensor and the magnet, should be accurate. second, the device has limited wearability, despite being able to fit users of various hand sizes. the tension of the rubber band might make the user feel uncomfortable. users with very small hands cannot use the system, as the rotation measurement structure has a length of 17-20 mm. third, the rotation measurement structure at mcp joint of the thumb might not be in a proper alignment because the mcp joint has additional dof of ab/ad motion. lastly, we did not measure the dip joint angle because the rotation measurement structures of the pip and dip joints were collided on the middle phalanx."
"to design a hand motion measurement system, an understanding of hand features and anatomical structures is required to determine the targeted measurement range, degrees of freedom (dofs), and accuracy of the hand motion measurement system."
"it is interesting to observe that, in the case of free reading, memory is enhanced by congruity: when the user can select a topic of his or her interest, the banner is better remembered if its subject is similar to the topic. this condition of free choice and congruity is in a sense the most \"natural\" one, being closer to real navigation scenarios. another result worthy of note is that, in the case of incongruity, memory is better with imposed reading. a possible interpretation of such behaviour is that since the tester reads something he or she is not really interested in, he or she gets \"distracted\" by the banner, which is therefore more noticed."
"generally, the qualitative comparisons in fig.7-fig.11 show that the proposed can effectively remove fog from various types of fog images and obtain more detail information of defogged images. moreover, our method has also achieved good results for underwater image enhancement."
"a clear image is a key prerequisite for understanding realworld scenarios in the field of digital imaging. in the outdoor environment, visibility and contrast of a photograph will seriously reduce due to bad weather such as light, fog and haze [cit] . the main reason is that the quality of the photo is highly susceptible to scattering, refraction, and reflection of a large amount of small particles in the air before the light reaches the camera lens. in order to effectively remove dense fog and highlight the details of the image, image restoration and enhancement are commonly used methods [cit] . fig. 1 shows examples of dense fog images and their corresponding defogged images. as shown in the top row of fig. 1, low-quality images greatly affect the perceptions and recognition capabilities of the human eyes. it can"
"in order to quantify the impact of inefficiency in the production process on the performance of a decoupled inventory system, we compare the decoupling stock with poisson arrivals to corresponding decoupled inventory system with interrupted poisson arrivals. the arrival interruptions account for inefficiency in the production process. we then use the following parameters to characterise the interrupted poisson process,"
"h2 states that when the tester is asked to read an online newspaper article whose topic they have chosen from a set (i.e. free reading), the ad banner receives more fixations (sub-hypothesis h2 1 ), with a higher total duration (sub-hypothesis h2 2 ), if the subject of the banner is congruent with the content of the article. wilcoxon signed ranks tests showed that:"
"where i is a channel of the image. equation (8) is actually a ssr algorithm, in order to compensate for shortcomings of the ssr, the msr [cit] improves the color image by linear weighting fusion of multiple ssrs with different scales, which is expressed as follows:"
"in summary, defogging method based on image enhancement can be detached from the dependence on physical devices and has good application value. in order to take full advantage of the enhancements with the retinex and solve the problem of missing image detail information, a defogging model based on multi-channel convolutional msrcr (mc_msrcr) is proposed, which is combined by guided filtering and msrcr, as well as introduces multi-channel convolution and linear weighted fusion. the main contributions of this paper include four aspects: 1) for the enhanced image of msrcr, the smoothing constraints of both illumination component and reflected component are considered together by using guided filter twice, thus the enhanced image satisfies the smoothing constraint and the noise in the enhanced image is reduced."
"h4. when the tester is asked to read an online newspaper article whose topic is chosen by him or herself (from a set), the ad banner is better remembered (sub-hypothesis h4 1 ) and recognised (sub-hypothesis h4 2 ) if the subject of the banner is congruent with the content of the article, rather than if the subject of the banner is incongruent."
"as regards the average duration of fixations on the banner area and the time intervening between page load and the first fixation -if any -on the banner, both these secondary dependent variables did not exhibit any relation either with congruity/incongruity between article and banner or with imposed/free reading; this was true both considering all testers and excluding total banner blindness cases. therefore such variables will not be taken into account in the following, apart from some considerations about the correlation between number of fixations and time to first fixation."
(1) free surfing the sites; (2) reading a piece of news presented either in textual or multimedia form; and (3) choosing and using an example of multimedia content within a proposed list.
"our investigation exploits eye tracking, a technology that allows the user's gaze to be monitored while interacting with a computer [cit] . eye movements occur as sudden, almost instantaneous (, 100 ms) \"saccades\", followed by fixation periods (, 100-600 ms) during which the eye is almost still. fixations are the basis for the analysis of user activity and performance, because they can be considered directly connected to the concept of \"attention\". in fact, eye tracking studies of user behaviour while performing a task are essentially based on the \"eye-mind hypothesis\" [cit], 1980), which claims the existence of a direct correspondence between the user's gaze and his or her point of attention. some experiments [cit] have shown that while it is possible to move one's attention without shifting the gaze, it is difficult to move one's gaze without shifting the attention, which can thus be considered closely correlated to eye behaviour. as will be discussed later, however, this theory cannot be considered separately from \"inattentional blindness\" [cit], according to which sometimes we look at things without being fully aware of them. oir 37,3"
"h1. when the tester is explicitly asked to read a specific article from an online newspaper, the ad banner receives more fixations (sub-hypothesis h1 1 ), with a higher total duration (sum of all fixation durations, sub-hypothesis h1 2 ), if oir 37,3 the subject of the banner is congruent with the content of the article, rather than if the subject of the banner is incongruent."
"the proposed method is implemented on a pc-windows 10 platform with an intel (r) core (tm) i9-9900k cpu @ 3.6 ghz processor and 16gb [cit] . in this paper, natural images in foggy conditions are randomly selected for testing, and the images are derived from nasa's open image, dataset and a previous standard dataset [cit] . the analysis of the experiment results mainly includes implementation details and overall performance analysis of this method including qualitative and quantitative comparisons, while outputting a defogged image with a better natural appearance."
"eye tracking has rarely been used to study thematic congruity in online content. [cit] carried out an experiment in which they combined an ad with congruent and incongruent webpages, measuring the impact by using attitude towards the ad and click intention. the consequences of displaying the ad before, along with, or after a webpage were examined. eye tracking results, although very preliminary, suggested that the way the medium context and the ad are watched may affect the impact that the ad has on people. moreover congruity effects seem to be more marked when the number of switches between context (webpage) and ad is low, while they become less relevant as the number of switches increases. [cit] found that, although congruity between ads and webpage content had no appreciable effects on fixation duration on banners, congruent ads were better memorised."
"memory: with congruent banner and article seven banners were remembered, while with incongruent banner and article only one banner was remembered: hence only sub-hypothesis h4 1 is verified."
"moreover outside the congruity matter, there are few cases of eye tracking studies conducted on print and online newspapers. among these important investigations have been performed by the poynter institute [cit] . in one experiment some testers were asked to surf different online newspapers designed according to five different \"styles\". users were assigned three kinds of tasks:"
". globally 32 per cent of banners were not remembered even if there were at least two fixations on them, and 17 per cent were not recognised. with no fixations at all, no banner was remembered, while 6 per cent were recognised."
"results from past research are inconsistent, and suggest that congruity may have a positive or negative effect depending on the context and other factors. an advantage of our investigation is that, besides analysing typical quality measures such as free and helped recall, eye tracking is used to gain insights about users' eye behaviour. knowing that a congruent banner attracts more fixations than an incongruent one has important consequences, even if in our experiments memory and recall do not seem to be greatly affected. the first and most significant implication is that banner blindness can be partially overcome (if a banner is not seen at all, it will hardly have any effect on the user's mind). second our findings prompt researchers to continue studies on congruity, which might seem of little interest because of the divergent results produced to date. web advertising agencies and online newspapers are encouraged to collaborate more closely to design banner campaigns where the subject of the ad is connected with the topic of its article, in order to potentially increase their success. we think that in a period like the present, in which several newspapers are debating whether to continue offering free access to their online content, the problem is particularly relevant."
"our study has provided a significant contribution to our understanding of the effects of banner/article congruity. in particular our findings highlight that congruity increases the number of fixations that a banner receives, as well as their total duration."
"in this part, in order to verify the performance of the white balance, the dense fog images a and b of two different scenes are selected as test sample, as shown in the first column of fig. 6 . we have described the effect of secondary guided filter in detail in section c, this section mainly introduces the effect of white balance. in the third column of the a scene, we can clearly see that the defogged image processed by white balance is better eliminate red color and improves the visual effect of the defogged image. in the third column of the b scene, we can clearly see that the defogged image has better visual effects and sharpness."
"where mean and var are the functions of mean and variance, respectively. in the gimp source code, the researchers point out that enhanced images have a better dynamic compression range d with is 2∼3. we verify through experiments that when d is set to 2 msrcr enhanced images can better retain detail and restore the color of the image. for equation (12), an overflow judgment is added to ensure that all pixel values are between [cit], that is:"
"h3 states that when the tester is explicitly asked to read a specific article from an online newspaper (imposed reading), the ad banner is better remembered (free recall, sub-hypothesis h3 1 ) and recognised (helped recall, sub-hypothesis h3 2 ) if the subject of the banner is congruent with the content of the article. \"remembered\" means that the tester correctly recalled the theme/brand of the banner when answering question 3 (see experiment design section). \"recognised\" means that the tester correctly identified the banner among the 12 presented. cochran's tests showed that:"
"experiment design each tester was asked to read four articles on four different pages, which were the result of a reworking of real pages from the online version of corriere della sera (figure 1), one of the most well-known italian newspapers. everything was left unchanged, apart from the ad banner placed at the upper right of the article and few other details. this position for banners is common not only in italy, but in many online newspapers worldwide as well. the article also contained a picture in the upper left corner, within the text. the material for the experiments was prepared as follows. two articles about cinema were used to test hypotheses h1 and h3 (i.e. imposed reading). the subjects of the corresponding banners were cinema (congruity between banner subject and article content) and cars (incongruity). five further articles about books, cooking, music, the web and sport were used to test hypotheses h2 and h4 (free reading). in five versions of these articles the banner subjects were the same as the articles' topics (i.e. books, cooking, music, the web and sport); in another five versions, the banner subjects were different, namely radio, pubs, theatre, travel and telephones). a total of 12 different pages/articles were therefore created and employed to form the various test combinations. all articles were about the same length (approximately 2,600 characters, including spaces)."
"where ∇xf (i, j) and ∇yf (i, j) are the difference of f (i, j) along the x and y directions, m and n represent the width and height of the input image, respectively. therefore, the larger the value of the ag that more detail information is obtained for the defogged image."
"different convolution kernels can obtain different feature maps of input image in cnn [cit], and these features maps are a representation of feature information. however, the complexity of the algorithm increases as the number of convolution kernel increases. therefore, it is necessary to determine the number of convolution kernel by weighing the amount of feature information and the time complexity of the algorithm. the estimation of illumination components in retinex is convolution operation on the input image by gaussian kernels of different scales. these images obtained by convolution of the input image by gaussian kernels of different scales are like feature maps in cnn. however, the traditional msrcr method uses gaussian kernel functions with three different scales to convolution r, g and b channels. based on this consideration, the idea of multiple kernel convolutions is introduced into msrcr method by referring to the ability of multiple kernel convolutions to extract precise features in cnn [cit] . firstly, six gaussian convolution kernels of different scales were used for convolution of r, g, and b channels in order to extract precise features to estimate illumination components. then, in order to enhance the detail information and global contrast of the image by a multi-scale linear weighted retinex operation is performed on the illumination component. the framework of multi-channel multi-scale convolution is shown in the second phase of fig. 2, the estimation formula of the illumination component is expressed as follows:"
"the first guided filtering only takes into account the smooth constraint on the illumination component. the result of the final defogged image preserves the noise of the original image and enhances the estimation error of the illumination component. however, the secondary guided filtering takes volume 7, 2019 into account both the smoothing constraints of the illumination component and the reflected image together, so that the image after secondary guided filtering processing satisfies the smoothing constraint condition and the noise in the enhanced image is reduced. the secondary guided filter of r msrcr, g msrcr and b msrcr channels is expressed as follows:"
"in most manufacturing systems, there is a trade-off between achieving a high service level and minimising the inventory costs. therefore, it is of main importance to understand the effects of the system parameters on both the average lead time (characterising the service level) and the average stock level (determining the inventory costs). table 1 indicates the relation between the parameters of a decoupled inventory system and the two performance measures e q p and lt. intuitively, a higher arrival rate and stock capacity for the semi-finished products lead to a higher average stock and a lower average lead time while an increase in the order arrival rate decreases the average stock and increases the average lead time. finally, a decrease in completion time on average reduces both the average stock of semi-finished products and the average lead time. table 1 . effects of the system parameters on e q p and lt"
"step 2: according to subsection d, we can know that r 2 gf, g 2 gf, and b 2 gf are detailed images by secondary guided filter."
"the study we present in this paper is primarily focused on the possible influence of congruity (between banner subject and article content) on the way readers relate to ad banners, both in terms of \"how\" they look at them and with regard to the memorability of the message. moreover our work introduces a further aspect that represents a novelty in this kind of investigation, namely the \"reading condition\": imposed or free. usually, experiments aimed at exploring user behaviour while interacting with web content involve specific pages, prepared on purpose. the tester is asked to perform some tasks on some pages, which may vary among testers, but are typically imposed on the single participant in the experiment. considering our online newspaper setting, such a situation does not correspond to what happens in a real scenario, where it is the reader who chooses what to read. in our study we have therefore considered both the common experimental case in which the tester is asked to read a specific article, and the more \"natural\" condition in which the tester can choose among a set of topics, according to his or her preference. our assumption is that free reading can better reflect what occurs in a real usage scenario, possibly producing different results compared to imposed reading. [cit], who argued that differences in navigation style may influence the way users relate to banners."
a majority (27) of the testers remembered having seen some advertising in the pages and 22 remembered the right position of the banner (upper right).
"where i is the pixel value, p(i) is the probability of the occurrence of pixels with a pixel value of i in the entire picture. hence the larger the ie value, the richer the color information contained in the image, that is, the better the visual effect of the image. epi reflects the change in the gradient at the edge, which can quantitatively describe the sharpness of the edge of the image. epi represents the ability of an enhanced image to maintain the horizontal or vertical edges of the original image. therefore, the higher the value of epi, table 2. ag, ie, and epi of the proposed algorithm with six other algorithms in fig. 7, 8, 9, 10 and 11. the better the edge preservation capacity. epi is defined as follows. (24) where m is the number of pixels of the image, f 1 and f 2 are the grayscale values of the left and right or up and down adjacent cells, respectively."
"the remainder of this paper is organised as follows. section 2 describes the decoupled inventory system at hand. in section 3, the steady-state probabilities are derived and relevant performance measures are determined. to illustrate our approach, section 4 considers some numerical examples. finally, conclusions are drawn in section 5."
"where µ r is used to estimate the color of the illumination color, and α is employed to adjust µ r . although it is simple, this white balance method effectively eliminates the color casts and also restores the white and grays shades of the defogged image."
"the decoupling stock is modelled as a queueing model with two queues, as depicted in figure 1 . the first queue -the product queue -has finite capacity c p and stores the semi-finished products prior to being processed to finished products. the second queue -the order queue -keeps track of the orders that have not yet been delivered and has infinite capacity. arriving orders are served in accordance with a first-come-first-served queueing discipline. each order takes a semi-finished product from the product queue and completes the product in accordance with order specifications. note that the two queues in the model at hand are tightly coupled. departures from the product queue are only possible when there are orders. similarly, departures from the order queue are only possible if there are semi-finished products in the product queue."
"with imposed reading congruity increases the number of fixations on the banner, although the total fixation duration is not markedly affected. moreover congruity affected neither memory nor recognition."
"when at least two fixations (single fixations may be due to unintentional gaze shifts to the banner) were detected on the banner, the banner was not remembered seven times in the case of imposed reading and congruity, six times with imposed reading and incongruity, 11 times with free reading and congruity and 14 times with free reading and incongruity. analogously, still with at least two fixations in its area, the banner was not recognised four times in the case of imposed reading and congruity, five times with imposed reading and incongruity, three times with free reading and congruity and eight times with free reading and incongruity. when no fixations were detected on the banner, it was not remembered, while it was recognised twice in the case of imposed reading and congruity, once with imposed reading and incongruity, one time with free reading and congruity and three times with free reading and incongruity."
"step 3: enhanced images and detailed images are obtained according to step 1 and step 2 respectively, then r, g and b channels are fused. from the linear weighted fusion formula [cit], it can be concluded that:"
"among other results, the study provided some findings about advertising. an important lesson learned was that the position of ad banners affects the way users look at them (the best location being the left column). [cit], both focused on the way online newspapers are read. [cit] examined the impact of verbal emotional cues in capturing attention on advertising regions of a news portal."
"the first row is the original image, and the defogged images of msrcr when the number of kernels is 3, 6, 9 and 12, respectively. the second row is the local amplification effect of the red box area in the first column."
"most research on make-to-order systems focusses on mto/mts decisions and hybrid mto/mts systems. in order to arrive at a mto/ [cit] defined the customer order decoupling point (codp) concept. these authors consider market, product and production related factors as well as the desired service level and associated inventory costs to locate the optimal decoupling point. [cit] investigated socalled hybrid mto/mts systems. they studied a onestation production system dealing with two types of random demands: ordinary demand and specific demand. in this hybrid system, both types of demand arrive according to a poisson process and production times of the workstation are exponentially distributed. specific demand has a higher priority with respect to ordinary demand and the performance of this system is studied by means of matrixgeometric methods. the decoupling inventory problem has also been studied as a two-stage production process in which the output of stage one, and the demand at stage two, are generated by independent stochastic processes. the stages are decoupled by storing intermediate products. [cit] sets limits on the available storage capacity and the rates of flow production into and out of the decoupling inventory. he formulated a model which enables the firm to determine the optimum capacities for the storage facility, and to determine the value of an additional supply of intermediate product."
"ad banners are vital for online newspapers, which rely on them as the main source of revenue. the banner blindness phenomenon, however, strongly reduces their efficacy, and advertisers are striving to find effective strategies for online marketing. understanding more about banners and the way users relate to them is therefore an essential requirement for website designers. oir 37,3"
"we first analyze the qualitative results of the proposed defogging method. the defogged images can be divided into five categories based on different scenarios, including airport, factory, highway, billboard and underwater. we compare the performance of the proposed method with state-of-the-art methods: zhuet al. [cit], chenet al. [cit], heet al. [cit], renet al. [cit], caiet al. [cit], and wanget al. [cit], the results are shown in fig.7-fig.10 . it can be clearly seen that all of these defogging methods get good defogged results, and these fog-free images can achieve better defogging effects, and the fog-free images achieve contrast enhancement with more detail information. the following is a comparison and analysis of fig.7-fig.11, from two aspects of overall vision and local details, respectively."
"and where i andî denote the identity matrix and the diagonal matrix with first diagonal element equal to zero and the others equal to one, respectively."
"our study stems from three main considerations: (1) results obtained to date from previous research on congruity between online content and ad banners are inconsistent, and further investigations are thus necessary; (2) in the context of online newspapers, congruity has been considered in extremely few cases; and (3) to our knowledge no work has really studied congruity in online newspapers using eye tracking technology."
"in this paper, we investigate the impact of inefficiency in a make-to-order system (with a decoupling stock) in a markovian setting. here, decoupling means that the completion of the semi-finished product is only possible when there is an order. these orders are backlogged and can be satisfied only when the semi-finished products are available. therefore, the studied make-to-order system is modelled as a homogeneous birth-and-death process (qbd) and solved with matrix analytic methods."
"in this section, we will describe in detail the proposed defogging model in detail, as shown in fig. 2 . in the following, we describe the process and analyze the meaning of each step in detail. firstly, the guided filtering processing of the original image preserves the edge information and overcomes the noise. secondly, in order to extract more precise features to estimate the illumination component, the r, g and b channels after the guided filtering are convolution by six gaussian convolution kernels of different scales, respectively, and then the corresponding six feature maps of the same size are obtained. thirdly, the six feature maps corresponding to each channel are enhanced by msrcr and merged with the linear weighting to improve the enhanced quality, and the quantization operation is introduced to ensure that the defogging image with good color fidelity. meanwhile, the overflow judgment is introduced to ensure that the pixel-value of the defogged image is between 0 ∼ 255. fourthly, since the first guided filter only considers the smoothing constraints on the illumination component, the final defogged image not only preserves the noise of the original image, but also enhances the estimation error of the illumination component. however, the smoothing constraints of both illumination component and reflected image are considered by using twice guided filter, so that the enhanced image satisfies the smoothing constraint and the noise in the enhanced image is reduced. fifthly, the image enhanced by msrcr and the image processed by secondary guided filter are fused with linear weighting to reconstruct the final fog-free image. finally, in order to enhance the appearance of the defogged image by white balance processing."
"h4 states that when the tester is asked to read an online newspaper article whose topic they have chosen from a set (i.e. free reading), the ad banner is better remembered (free recall, sub-hypothesis h4 1 ) and recognised (helped recall, sub-hypothesis h4 2 ) if the subject of the banner is congruent with the content of the article. cochran's tests showed that:"
"in order to avoid the bias caused by qualitative analysis, we qualitatively evaluate analyze our method and six other methods. in terms of quantitative evaluation, we compare different methods from three objective metrics: ag, ie, and epi. ag reflects small changes in the details of the image, and the richness of the image information. for an input image f (i, j), ag is defined as follows."
"h5 states that h1 and h3 (imposed reading) are confirmed or not confirmed to a different extent with respect to h2 and h4 (free reading). in other words, h5 aims at finding possible effects of the imposed/free-choice article conditions on the way the banner is watched, remembered and recognised. of course, h5 could be indirectly tested through h1-h4, but we prefer to clearly explicate its focal points. considering only articles congruent with the banner, wilcoxon signed rank tests indicated that:"
"ie reflects the average amount of information, which can quantitatively describe the richness of image color. when an image is not uniform, the probability of any gray-scale value in the image is equal, that is to say, the dynamic range of the image is broader. therefore, the value of ie is the maximum, but in areas where the fog and grayscale are consistent, the value of ie is minimal. ie is defined as follows."
"a study on goal-directed navigation and web advertising [cit] showed that highly task-oriented consumers exhibit more ad avoidance, and that ad avoidance is related negatively to ad recall. [cit] focused on the importance of congruity in moderating the relationship between forced ad exposure (e.g. pop-up ads) and memory effects, with the relationship being intensified by congruity and reduced by incongruity. [cit], considered the connection between ad recall, congruity and task orientation, showing that task-oriented navigation leads to a more marked avoidance of ads, but that the effect can be mitigated by congruity."
"free-choice articles were preceded by a \"topic selection page\" displaying five buttons that listed five available topics (figure 3 ): by clicking on them, the corresponding articles were loaded. the second time the topic selection page was shown, a warning advised the tester not to choose the topic they picked the first time. after reading the four articles the tester was presented with an online questionnaire composed of three questions:"
"as mentioned above, each tester read four articles in sequence. the 24 combinations given by the conditions \"imposed/free-choice article\" and \"congruent/incongruent banner\", as well as by the presentation order of these four cases, were randomly assigned to 24 testers. for the remaining 11 participants, 11 combinations (still randomly chosen from the available 24) were replicated. after reading the articles the testers answered questions 1, 2 and 3 and selected the banners they remembered seeing in the visited pages."
"where n is the number of scales r msr i (x, y) is the result of msr processing on the i th channel, g n (x, y) is the gaussian kernel function corresponding to the n th scale, and w n is the weight corresponding to the n th scale."
"2) in order to extract more precise features to estimate the illumination component, six gaussian convolution kernels of different scales are used for multi-channel convolution. meanwhile, the retinex operation is carried out, and the quantization operation is introduced to ensure that the defogging image has good color fidelity."
"h5. hypotheses h1 and h3 (\"imposed\" article) are confirmed or not confirmed to a different extent with respect to hypotheses h2 and h4 (\"free choice\" article)."
"online newspapers are increasingly becoming one of the major sources of news for many people. as the number of readers increases -also thanks to the rising popularity of tablets -companies invest considerable amounts of money to advertise their products within information webpages, primarily in the form of ad banners. despite intensive research arguing that this kind of promotion is hampered by several issues that limit its effectiveness, banners are widely exploited in practically all electronic newspapers, and come in a variety of styles and formats."
"last, the tester was presented with a page containing all the 12 banners created for the experiments and was asked to select the possible banners he or she had noticed while reading the four articles (although the tester could have seen only four banners out of the 12, as only one banner was displayed along with each article). of course a participant's interest in the specific content of an incongruent banner may have influenced results. we have tried to reduce this problem, affecting all studies on congruity, by choosing very common banner subjects, uniformly distributed among testers. all pages employed in the experiments were published on a web server and were accessible online, with server-side data management implemented in php."
"in the basic setting, the markov process is a homogeneous quasi-birth-and-death process (qbd), [cit] . in the present setting, the so-called level or block-row number, indicates the number of waiting orders while the phase, i.e. the index within a block element, indicates the number of semi-finished products. the one-step transitions are restricted to states in the same level (from state (n, * ) to state (n, * )) or in two adjacent levels (from state (n, * ) to state (n + 1, * ) or state (n − 1, * )). in particular, the transition rates between the different states of the markov chain are summarised in the table below."
"the rest of the paper is organized as follows. the proposed defogging method and its detail operation are shown in section ii. along with experimental results and analysis in section iii, the performance of the proposed method in different applications is evaluated. the conclusion and the exploration of the future work are shown in section iv."
"h3. when the tester is explicitly asked to read a specific article from an online newspaper, the ad banner is better remembered (sub-hypothesis h3 1 ) and recognised (sub-hypothesis h3 2 ) if the subject of the banner is congruent with the content of the article, rather than if the subject of the banner is incongruent."
"the present study of the decoupling stock also closely relates to literature on two-part assembly systems or kitting processes. for such systems, there are two queues, each storing a specific part, and production only starts when both part buffers are non-empty. in the present setting, one part-buffer corresponds to the decoupling stock, while the other corresponds to the list of backlogged orders. also in the current setting, production only starts when both buffers are non-empty. indeed, each delivery of a finished product requires both the order specifications and a semi-finished product and can only be satisfied if both are present. [cit] was the first to prove that, assuming no arrival control strategy, this queueing system is inherently unstable. in particular, he studied the multipleinput extension of the gi/g/1 queue in which arrivals in each stream are described by an independent renewal process and service times are independent and identically distributed. he showed that part waiting times converge to non-defective limiting distributions only if the buffer capacities are bounded. [cit] who termed the two-part assembly system as waiting lines with paired customers. he considered a fig. 1 . decoupling stock in a make-to-order system system of infinite capacity queues with poisson arrivals for both parts and exponential services. the steady state is attained, i.e. the system is stable, if the arrival rates depend on the difference between queue lengths. [cit] extended latouche's research by considering two exponential distributions, one for the part processing distribution, i.e. the synchronisation phase, and the other for the assembly operation distribution. approximations for the throughput rate and average queue length were given. [cit] also extended the work of latouche by considering multiple poisson input streams arriving in buffers with a finite capacity. they derived bounds and approximations for poisson arrivals with the same rate and an arrival is blocked and lost if the buffer is full. [cit] studied a two-queue system in which each part is processed according to an exponential distribution and the assembly operation times are generally distributed. they followed a matrix-geometric approach to determine numerically the marginal distributions of both kit and end-product inventory positions. [cit] by means of the generalized minimal residual method."
"step 1: according to subsection c, we can know that r msrcr, g msrcr, and b msrcr are enhanced images by multichannel convolutional msrcr."
"other research has been aimed at examining the effects of banner ads and website congruity on users' attitudes towards the website of a brand, discovering that high congruity is positively correlated with attitude [cit] ."
"according to our results congruity does have a positive effect on the number of fixations on the banner and on their total duration. free reading seems to favour this, since both the number of fixations and their duration increased. in this respect it is important to observe that even if no substantial differences were found between imposed and free reading in relation to the number of fixations and to the total fixation duration on the banner, our assumption is that free reading can better reflect what occurs in a real usage scenario. thus results obtained for the free reading condition should be regarded as a better representation of \"reality\". while for imposed reading only sub-hypothesis h1 1 (more fixations with congruity) was verified, both sub-hypotheses h2 1 (more fixations with congruity) and h2 2 (higher fixation total duration with congruity) were confirmed for free reading, as well as sub-hypothesis h4 1 (better memory with congruity). the fact that the banner is seen more does not necessarily imply that it is also more remembered and recognised. in our experiments memory was improved by congruity in the case of free reading, while there were no clear effects on recognition. a partial explanation of this may be that testers answered the questionnaires only after reading all the four articles, which was the only way for participants not to discover the trials' purpose in advance in a within-subjects design. another aspect to be considered is \"inattentional blindness\" [cit], which states that sometimes we do not pay attention to what we are looking at. therefore it may be that the tester's gaze was captured by the banner, but it did not hold his or her attention."
"to model the burstiness in the arrival process of the semifinished products, we replace the poisson processes by a markovian arrival process [cit] . we here limit the presentation to interrupted poisson processes, but the methodology easily extends to general markovian arrival processes. in either case, the qbd structure of the markov chain is retained, the phase now describing both the content of the decoupling stock and the state of the markovian environment."
"concerning the two-dimensional markov process (with poisson arrivals), the throughput of the product queue equals the outgoing rate of a m/m/1/c p queue and can explicitly be calculated as,"
"as mentioned above, for state (n, m, k), the level is still defined by the state value n, indicating the number of waiting orders, while the phases are defined by m (the number of semi-finished products) and k (the state of the markovian environment). it is assumed that when the state k equals one, the process is in an active state while when it equals zero the process is inactive. clearly, the generator matrix of the markov model still has the block matrix representation (1). the blocks are now defined as follows. we have"
"the associate editor coordinating the review of this manuscript and approving it for publication was qiangqiang yuan. be seen from the bottom row of fig. 1 that the defogged images have better visual effects and clearer details, which are more suitable for applications in expanding areas such as space, transportation, meteorology, underwater detection, and military technology. therefore, image defogging has become an important research direction, which has attracted more and more attention of researchers [cit] ."
3) the final defogging image is a linear weighted fusion of the image after secondary guided filtering processing and the msrcr-based enhanced. 4) the final defogged image is processed by white balance to improve the visual effect of defogged image.
"our results can be used to drive further research in the field, for example taking into account additional independent variables such as banner position, style and size."
"in this paper, a single image defogging method based on multi-channel convolutional msrcr proposed. our method mainly consists of four parts: illumination component estimation, guided filter operation, reconstruction of fog-free images, and write balance operation. the proposed method not only ensures the quality of the illumination component, but also the noise in the enhanced image is reduced. in particular, our method improves both qualitative and quantitative performances when compared with the six state-of-the-art methods. however, our method also has two shortcomings. 1) the complexity of the method is increased due to the introduction of multi-channel convolution and guided filter. 2) since the colors of the fog and the sky are similar, it is difficult to remove fog effectively in the sky area. these issues will be our future work."
"h2. when the tester is asked to read an online newspaper article whose topic is chosen by him or herself (from a set), the ad banner receives more fixations (sub-hypothesis h2 1 ), with a higher total duration, if the subject of the banner is congruent with the content of the article, rather than if the subject of the banner is incongruent."
"we used a within-subjects design, in which each tester dealt with both values of the two conditions set by independent variables. therefore each participant read (not necessarily in this order):"
"it is interesting to note that the banner blindness phenomenon was definitely evident in our experiments too, affecting all the four test cases but not showing significant differences among them. in particular there were 12 occurrences of banner blindness (with not even one fixation on the banner area) in the case of imposed reading and congruity between banner and article, 14 occurrences in the case of imposed reading and incongruity, 11 occurrences in the case of free reading and congruity, and 13 occurrences in the case of free reading and incongruity. in the following statistical analysis we will consider both the whole group of 30 testers and the subsets of participants for whom at least one fixation was detected on the banner area in at least one of the two cases being compared (therefore excluding total banner blindness instances). the analysis was carried out using non-parametric statistics, since data populations did not satisfy the normality assumption (assessed both by looking at histograms and, more formally, through the kolmogorov-smirnov test). the significance level was 5 per cent."
"our experiments were aimed at studying congruity in online newspapers and finding possible relationships between banner subjects and article content. in particular, considering the typical case in which a single main banner is displayed along with the article, we identified five main hypotheses (h1-h5):"
". with free reading congruity increases both the number of fixations on the banner and the total fixation duration. congruity also enhances memory, but seems not to noticeably affect recognition."
"prior to each test session, participants were instructed to read the articles they would be presented with. testers were also explicitly told that, although reading the articles was a mandatory task, they would not be questioned about their contents. this clarification was necessary -suggested by some preliminary trials -in order to prevent participants from paying too much attention to the articles' content, thinking that it was their memory that was being investigated. such behaviour was to be avoided, as it would have affected the testers' attitude towards the trial's tasks: in fact oir 37,3 figure 2 . banners used in the experiments online newspapers and ad banners in normal situations users read online newspapers for their \"pleasure\", without putting any excessive effort into the activity."
"the term \"banner blindness\" refers to the well-known phenomenon in which web surfers tend to ignore page elements that look like ad banners (e.g. drèze [cit], thus making most of the efforts of advertisers and graphic designers pointless. to improve the likelihood of banners being noticed, several solutions have been proposed and tested, with variations on message components (e.g. with or without graphic elements, static, animated, etc.), dominant colours, size, position and other factors. in this context the thematic congruity between banner subject and webpage content has also been considered, as a potential way to overcome users' distrust of banner-like elements."
". figure 4 shows the boxplots for number of fixations and fixation duration, referring to cases without total banner blindness only. the bottom and top of the box are the 25th and 75th percentiles, while the inner band indicates the 50th percentile; the ends of the whiskers represent the smallest and largest non-outlier values; circles denote outliers standing more than 1.5 box-lengths above the box; and stars indicate extreme values, standing more than three box-lengths above the box."
"h1 states that when the tester is explicitly asked to read a specific article from an online newspaper (i.e. imposed reading), the ad banner receives more fixations (sub-hypothesis h1 1 ), with a higher total duration (sub-hypothesis h1 2 ), if the subject of the banner is congruent with the content of the article."
"(1) did you notice any advertising in the pages you visited? (available answers: \"yes\", \"no\", \"i don't remember\"); (2) (only if the answer to the previous question was \"yes\") do you remember where the advertising element was placed with respect to the article? (available answers: \"upper left\", \"upper right\", \"lower left\", \"lower right\"); and (3) (only if the answer to the first question was \"yes\") do you remember the theme/brand of any ad banners you noticed in the page? (the tester had to write his or her answers within a text area in the page)."
"after the experiments we explicitly checked the presence of fixations throughout the rows of all articles read by testers, and had to reject three participants who did not fulfil the \"whole reading\" requirement. two further testers had to be excluded because of poor eye tracking data. therefore we considered 30 valid tests out of 35 (18 males and 12 females)."
". one free-choice article with a congruent banner; and . one free-choice article with an incongruent banner. considering the presentation order of these four cases, 24 combinations were obtained. at the end of each page a link to the next page in the sequence was added, in order for the experiment to be carried out without interruptions. the other links present in the original pages from corriere della sera were disabled (but not removed), to prevent testers from loading pages extraneous to the study. a total of 12 banners, moderately animated, were created using adobe flash cs4. to avoid any kind of influence from known brands, the banners advertised imaginary products or services. apart from the specific textual and visual content, all banners had a similar structure (figure 2), and were based on the typical ad model which starts with a question (e.g. \"are you looking for a different night out?\") and ends with an answer (e.g. \"don't miss this occasion\") and with an invented product/brand (e.g. \"dream café, more than 10 cocktails to try\"). figure 2 shows these final parts of banner display."
"where s(x, y) is an image observed in the real word or generated by other imaging devices, and l(x, y) is an illumination component and r(x, y) is a reflected image. then, taking logarithm of both sides of equation (5) log"
"production facilities are often characterised as make-toorder (mto) or make-to-stock (mts) systems. in mts production, products are stocked in advance, while in mto production, a product only starts to be manufactured when a customer order is received. nowadays, as a mean to respond quickly to growing variety, shorter product life cycles and intense competition, industries are moving towards mto production [cit] . indeed, to ensure short delivery times, most production systems have a decoupling point at the inventory of their semi-finished products. in such systems, the decoupling stock is an accumulated inventory of semi-finished products waiting for an order to arrive. this allows for diminishing delivery times of customised products as only the final completion step still needs to be done. research on the performance of the decoupling stock in a make-toorder system is therefore of main importance. this is the subject of the present paper."
"as our numerical examples show, system performance is highly sensitive to the arrival process parameters. inefficiency in the production process causes on average a longer lead time and a higher number of waiting orders, even with a higher stock capacity for the semi-finished products. still, it is of main importance to determine the total cost of a decoupled inventory system. this will remain the focus of future work."
"combining the eqs. (29), (30), and (31) and considering sðx \" for more information on this or any other computing topic, please visit our digital library at www.computer.org/publications/dlib."
"a hetero-manifold is an ensemble of uni-and cross-modal sub-manifolds. uni-modal sub-manifolds are the manifolds whose elements corresponding to different objects share a common modality. for example, x u is a dataset in which all samples are on the uth uni-modal sub-manifold. it is clear that uni-modal sub-manifolds are used to represent the intrastructure of uni-modal data. in contrast, cross-modal submanifolds serve as bridges to connect different uni-modal data. ideally, any pair of data points on different uni-modal sub-manifolds could be connected via a path on the crossmodal manifolds and the distance of the path could be used to represent the similarity between the cross-modal data."
"in this paper, by integrating the supervision information and the local structure of heterogeneous data, a novel method termed hetero-manifold regularisation (hmr) is proposed to learn hash functions for efficient cross-modal search. three significant advantages are illustrated in the schematic diagram of a hetero-manifold shown in fig. 1 . first, a heteromanifold well describes the local information by representing homogeneous data on the sub-manifolds. in fig. 1, the data in three different modalities are represented by three sub-manifolds which well model the relationship between homogeneous data. second, the hetero-manifold emphasises the global information of multi-modal data as well, by modelling the information propagation across modalities with three-order random walks. it is clear in fig. 1 that any pair of points could be connected via two steps on homogeneous sub-manifolds and one step crossing two different sub-manifolds. thus, the samples across modalities could be compared by integrating the information from all related homogeneous sub-manifolds."
"where w t k w l defines the linear correlation between w k and w l . by minimising the term w t k q ð1þ k w k, the learned projection direction w ð1þ k will be approximatively orthogonal to all of the other learned projection directions. similar to formula (22), the optimisation problem (23) could also be resolved by using the lagrange dual method"
"the cumulative relative frequencies of rdm and lnmag are displayed in figure 8 . due to the rough approximation of the smooth surfaces, all models consisting of regular hexahedra (especially at the mesh width of 2 mm) lead to relatively high topography and magnitude errors when compared to the surface-based tetrahedral reference model. comparing the results in the 2 mm model with regard to the rdm ( figure 8, top), the projected mixed-fem performs best with roughly with regard to the lnmag, the differences between the results obtained using the mesh resolutions of 1 and 2 mm and also between mixed-, dg-and the two cg-fem approaches are larger than for the rdm (figure 8 t his study introduced the mixed-fem approach for the eeg forward problem. two approaches to model the dipole source were derived, the direct and the projected approach. numerical results for sphere and realistic head models were presented and compared to different established numerical methods."
"given a training set, the inherent similarity of multiple modalities on the hetero-manifold is represented by the hetero-laplacian matrix. thus, by minimising the regularisation item via the graph hetero-laplacian, a set of cross-modal hash functions which are smooth on the hetero-graph can be learned to embed original data points into a hamming space. in other words, the learned hash functions will preserve the geometrical structure and global supervision information of the hetero-manifold. meanwhile, a novel weighted cumulative distance inequality on hetero-graph is introduced to cross the gap between hamming distance and euclidean distance. by using this novel distance inequality, the problem of learning hash functions is transformed into training a hetero-manifold regularised support vector machine."
"definition 4 (hinge loss constraint). for a function f u k in the uth modality, if any point x u i captured in this modality and its corresponding hash code defined in eq. (7) satisfies"
"also with regard to the lnmag ( figure 6, bottom row), the influence of the skull leakages is apparent. in models seg 2 res 2 r82 and seg 2 res 2 r83, the lnmag increases up to an eccentricity of 0.964, and only decreases for higher eccentricities. this effect is clearly stronger for the whitney cg-fem than for the mixed-fem. in contrast, the lnmag for the whitney cg-fem decreases clearly stronger than for the mixed-fem in model seg 2 res 2 r84 with increasing eccentricity, leading to a switch from about 0.2 for eccentricities below 0.964 to values lower than 0.2 at an eccentricity of 0.993. especially in model seg 2 res 2 r83 the whitney cg-fem also leads to a higher variance of the lnmag, while this is less distinct in the other models."
s earching is dramatically changed by the amount and the appearance of multi-modal data. multi-modal data are heterogeneous and large-scale because of the advancement of digital technologies and the internet. both of these fundamental characteristics of multi-modal data require measuring the cross-modal similarity when developing any searching algorithms by hashing.
". thus, the optimal solution can be obtained by a representation theory in appendix d. therefore, all of these steps of optimising the original optimisation problem (19)"
"because it has proved to be robust enough to obviate the modeling of the forces in the wheel-soil interaction in the presence of slippage, a sliding-mode controller is applied to a skid-steering mobile robot. this scheme ensures the control of the hending velocity and the yaw angle."
"in summary, our contributions are four-fold: (1) a novel hetero-manifold is first proposed as a well-defined platform to capture both local information of sub-manifolds corresponding to homogeneous data and global information of hetero-manifold corresponding to multi-modality data. (2) a weighted cumulative distance inequality on the heteromanifold is provided to theoretically guarantee the reasonability of replacing hamming distance by euclidean distance during supervised learning. (3) a novel heteromanifold regularised support vector machine, taking advantages of the hetero-manifold in representing the information of multi-modality data and the support vector machine in generalisation, is proposed based on the proposed weighted cumulative distance inequality for generating more efficient hash functions for cross-modality searching. (4) extensive experiments on the multi-modality data with six modalities are reported for showing the flexibility of the hetero-manifold regularised support vector machine as more than two modalities are considered."
"in general, learning to hash tries to minimise a cumulative hamming distance with some constraints. if the distance is defined on a manifold, then a weighted cumulative hamming distance l . actually, the weights between the samples embody the intrinsic structures and useful information including local neighbourhood, prior semantic cues and affinities. by considering these weights, the original structure and information can be preserved in a new learned space. in this paper, the weights reflect the information contained in the hetero-manifold."
"evaluation metrics: on the one hand, for identification systems, the cumulated matching characteristics (cmc) [cit] are commonly used for performance evaluation and measuring how well an identification system ranks the identities in the 2. without confusion, the subscript x y ð0þ 1 will be simplified as x ð0þ y 1 ."
"obtaining a numerical solution for (6)/(8) necessitates choosing suitable discrete approximations for the test function spaces h(div; ω) and l 2 (ω). utilizing a galerkin approach, these are also the spaces in which the discrete solution (u h, j h ) lies."
"cross-camera person re-identification is a very challenging task because of the variation of camera views and the environment. given a probe image containing a person, the most popular method of recognising the person is to rank the similarities between the probe image and the images in the gallery (captured by other cameras). in this experiment, the similarity is calculated in the learned hamming space across the cameras and the maximum rank of auc is 85."
"to further simplify the optimisation problem (17), the last two constraint conditions are slightly relaxed and transferred into the objective function by using the lagrangian principle. the constraint condition (iii) will be considered when the projections are learned using a sequential strategy. as for constraint condition (ii), the total number of pairs"
"with p 21 and p 22 being two entries of the positive-definite and symmetric matrix p occurring in the expression of the candidate lyapunov function. here, this hyperplane is a straight line."
"referring to eq.(3) of the wheel dynamics, we can consider that the addition of a force differential in a wheel is equivalent to a differential in its angular acceleration, i.e.,"
"in this article, a mixed finite element method (mixed-fem) to solve the eeg forward problem is introduced. compared to the cg-fem it has the advantage that the current source can be represented in a direct way, while either an approximation using electrical monopoles has to be derived or the subtraction approach has to be applied when using the cg-fem. furthermore, the mixed-fem is current preserving and thereby prevents the effects of the (local) current leakages through the skull that might occur for the cg-fem [cit] . this is also shown in simulations in section iv-c. an accurate simulation of the currents penetrating the skull is important, as the influence of an accurate representation of the skull for accurate forward simulations was shown [cit] . the accuracy of the mixed-fem in comparison to cg-fem approaches is evaluated in both sphere and realistic head models. it is shown that the mixed-fem achieves higher accuracies in solving the eeg forward problem than the cg-fem for highly eccentric sources in sphere models and also in realistic head models."
"realizing this directly leads to the three main sources of error that exist in these evaluations -besides the already discussed leakage effects, these are inaccurate representation of the geometry and numerical inaccuracies. a major source of error is the representation of the geometry. since regular hexahedral meshes were used here, the influence of geometry errors is significant, especially for coarse meshes with resolutions of 2 mm or higher. no explicit convergence study comparing the results in models with increasing mesh resolution but a constant representation of the geometry was performed. though, it can be assumed from the results of further studies that the geometry error dominates the numerical errors due to lower mesh resolutions [cit] ."
"using the sliding-mode theory, we suggest here a new approach to control a fast skid-steering mobile robot with wheel torques as inputs, based on its dynamics model. the objective is to force the mobile robot to follow a desired path at rel-atively high speed, by controlling its yaw angle and its longitudinal velocity. the ground considered is nominally horizontal and relatively smooth as compared with the wheel dimensions. if most of the control laws consider that the conditions of movement without slippage are satisfied, this hypothesis is not valid at high speed, where wheel slippage becomes significant, thus reducing the robot stability. the implemented controller will have to be robust with respect to these phenomena in order to ensure an efficient path-following."
"the robot has to follow the path, the reference point p being all the time the projection of its centre of mass g on this one. to take into account the lateral error, we add to θ d a term limited between − π 2 and π 2 excluded, increasing with the lateral error d, the function defining this term being also odd to permit a similar behavior on both sides of the path. we thus define the modified desired steering angleθ d such as:θ"
"in sections iv-a and iv-b we found that the projected mixed-fem is superior to the direct mixed-fem. to keep the presentation concise, we therefore from now on only compare the projected mixed-fem with the whitney cg-fem. the results in model seg 2 res 2 r84 (table iii), which does not contain any skull leakages yet, mainly resemble those in model seg 2 res 2 for both rdm and lnmag ( figure 6 )."
"to estimate the value of the lateral forces f y, pacejka theory [cit] could be used, by taking into account the slip angle. because of the robustness of the sliding-mode control, however we can consider that f y is a perturbation to be rejected, and we do not include it in the control law. a slip-angle measure being in practice not very efficient, this solution is better."
"using with the hinge loss constraint, the problem of hash function learning on hetero-manifold (8) could be approximated by minimising its upper bound (16) with some constraint conditions"
"consequently, we derive from this solution that to correct the error, it is necessary to increase the value of p 21 and to decrease the value of p 22 . according to expression (15) for p, we know these two parameters. hence, to eliminate quickly the position error, it is necessary to increase the value of a and to decrease that of b. as far as the sliding straight line is concerned, this modification of the various coefficients increases the straight line slope."
"in order to reduce the geometry error, the use of geometryadapted meshes was considered for the cg-fem and it was shown in a variety of studies that this already clearly improves the representation of the geometry [cit] . however, while the use of non-degenerated parallelepipeds is uncritical for the mixed-fem, \"some complications may arise for general elements\" [cit] . at least for the two-dimensional case, error estimates for general quadrilateral grids can be obtained when modifying the lowest-order raviart-thomas elements [cit] and for convex quadrilaterals even superconvergence could be shown [cit] . the use of geometry-adapted hexahedral meshes in combination with the mixed-fem should therefore be evaluated in future studies."
two probe samples with first 3 matches are shown in fig. 10 . the two persons have images from the 0 à 4 modality to the 15 à 19 modality. the left probe comes from the 5 à 9 modality while the right one comes from the 0 à 4 modality. we can see that several images with a same person have been successfully matched in different age stages by cross-age retrieval.
"in this paper, the concept of hetero-manifold was introduced for integrating the uni-and cross-modal similarities of multi-modal data in a global view. both types of similarity are represented in the laplacian matrix l corresponding to the hetero-manifold. the laplacian matrix l appears smoothly when the hamming distance in eq. (8) is replaced by the euclidean distance in eq. (17), which hints that no hash functions could be learned without all uni-and crossmodal similarities being defined on the hetero-manifold. therefore, the proposed framework of hetero-manifold regularised hash function learning (eq. (17)) could benefit from the view of treating multi-modal data as a whole. the experimental results demonstrate that the proposed hmr outperforms the state-of-the-art methods on four popular datasets."
"to further compare with other hashing methods, binary codes of shorter lengths (32, 64 and 128) are learned on the cuhk01 dataset. the results are shown in fig. 5 and table 2 . we can observe that, as the code length increases, the performance of eigenvalue decomposition based methods such as cvh decreases since the first few projection directions occupy most of variances. however, it is reasonable that our hmr can achieve better when the code length increases. more information can be kept because hmr considers both the orthogonality and the cross-modal intrinsic structure. we can see that hmr achieves best results at all code lengths. specifically, the advantages of hmr are more obvious, when the length of learned codes increases. the rank 1 scores of the five methods are also shown in the legend of fig. 5 and hmr obtains at least 0.024 higher scores than other methods."
"to compare the performance with the state-of-the-art person re-identification methods, we evaluate the proposed hmr and the recently published algorithms on the viper dataset including: sdalf [cit], cps [cit], kissme [cit], esdc [cit], salmatch [cit], mlf [cit] and ladf [cit] . for the proposed hmr, two lengths of binary codes 512 and 800 have been learned and the experimental results corresponded to both code lengths are denoted as hmr-512 and hmr-800, respectively. the comparison results are shown in fig. 4 . first, we can see that, except for ladf, hmr (hmr-512 and -800) significantly outperforms other methods and the advantages are more obvious especially at higher ranks (from 5 to 60). it is worth to point out that hmr is the only hashing method among the compared ones and still achieves comparative results to a non-hashing metric learning method ladf. in fact, due to quantisation loss, the performance of hashing methods is normally lower than that of non-hashing methods in many applications. second, hmr-512 achieves similar results as hmr-800 and this demonstrates that the performance keeps stable when the code length is above a certain threshold. finally, we also compare with other hashing methods on the viper dataset when the binary code length is fixed at 512 4 and the comparison results are illustrated in table 1 . we can see that, both from the perspectives of ranks 1, 5, 10, 15 and 20 and the overall performance auc, hmr achieves much better results than state-of-the-art hashing methods."
"for the scalar-valued equation (2b), one can simply choose the space of square-integrable functions l 2 (ω) as the test space. now, we can introduce the weak formulation of (2)"
"due to the vector-valued equation (2a), it is necessary to introduce a space of vector-valued test functions. a natural function space for the current in the mixed formulation is h(div; ω):"
"as stated previously, c u is the control law and m (u,u) is a function of uncertainties on u andu in the equations of the system dynamics. we have the following relationship:"
"to correct the velocity errors, higher torque values are applied with the slidingmode controller (torque curve for the velocity regulation of the fig. 10 ), stabilized around 30 nm. on the torque curve for the yaw regulation, we observe few peaks for three bends (7 s, 11 s et 15 s), with higher values without sliding-mode because fig. 9 torques without the sliding-mode controller (nm) fig. 10 torques with the sliding-mode controller (nm) of larger errors to correct in order to reach the path."
"2) the capturing conditions of images are quite diverse in different places and years. 3) as far as we know, the cross-age face retrieval is the first multi-modal experiment, in which six modalities are considered. intuitively, the ages of faces can be considered as modalities in our setting, in which faces of different persons with the same age range share similar characteristics including smoothness, wrinkles and hair. fg-net. some examples of an ageing dataset [cit], which contains 82 people with age ranges from 0 to 69, are shown in fig. 8 . the images of a same person distribute unevenly and most of the images are captured in the early ages. thus, we divide the ages into six stages including 0 à 4, 5 à 9, 10 à 14, 15 à 19, 20 à 30 and 31 à 69 which correspond to six modalities in our method. in this experiment, the parameters c 1 and c 2 of algorithm 1 are set to 10 and 0.1, respectively. 10-fold cross validation is used and, in each fold, 90 percent persons will be chosen as training and the remaining as for testing. in this experiment, the maximum value for auc is set to 50."
"gallery with respect to a probe sample. moreover, the area under curve (auc) corresponding to the cmc curves is also reported to show the overall performance at ranks from 1 to a fixed maximum. a larger auc score means the corresponding method is more robust. on the other hand, for the ranking cases of multiple feedbacks, the precision and recall are normally calculated"
"where sðx fig. 2 . on the one hand, for the first and third steps, the neighbours of the end must be represented in a common modality. thus, the similarity between x u i and its neighbour x u i 0 is generally measured by a gaussian kernel, such as"
"in the first study, for each model, the source positions were projected to the closest face center and the source directions were chosen according to the face normals, so that only one basis function contributes to the right-hand side. thereby, the results are not influenced by the interpolation that is needed for arbitrary source directions and positions. for the whitney approach it was shown that it has the highest accuracy of all cg-fem approaches in this scenario [cit] . next, the three approaches were compared in the same models using the initially generated random source positions and radial source directions, so that neither position nor direction were adjusted to the mesh. we limit our investigations to radial sources, as eccentric radial sources were shown to lead to higher numerical errors than tangential sources in previous studies [cit] . finally, the projected mixed-fem and whitney cg-fem were evaluated in combination with the models seg 2 res 2 r82, seg 2 res 2 r83, and seg 2 res 2 r84 generated out of model seg 2 res 2 but with an especially thin skull layer, again with random positions and radial source directions. table iii indicates the outer skull radii of the different models and the thereby generated number of leakages, i.e., the number of nodes in which elements of skin and csf compartment touch."
"the correction of the vehicle steering does not permit the system to converge to the desired trajectory. it is also necessary to correct the lateral error; otherwise, the system will aim towards a movement parallel to the reference path, not necessarily reaching it. this is why we are going to modify the desired yaw angle, as proposed in other works [cit] ."
"with τ u and τ θ re-computed with a change of variable, from the inverse of the robot dynamics model-equations (17) and (4) with the accelerationsu andθ replaced respectively by the control laws (19) and (14)-namely,"
"the delay of the actuators is due to the inertia of the vehicle slipping on a wet ground. there is almost no yaw-angle error during this period, as we can see at around a time of 14 s on the sliding mode controller curve fig. 7 . there is thus only a lateral error to be corrected, which explains a slower convergence. the convergence time can be tuned with parameter d 0 of eq.(5), but too high a value will bring about the risk of yaw instability, which could occur during bend. on the lateral error curves displayed on fig. 6, we notice a good following until the there is no significant difference between the yaw error with and without the sliding-mode control law; however, after the last bend (15 s) the robot has some difficulties to reach the path without the sliding-mode controller. the longitudinal velocity error is globally well reduced with the sliding-mode controller, as we can see in fig. 8 . we observe the chattering phenomenon during the second bend (11 s), with the strong oscillations of the velocity curve."
"consequently, with the help of the inequality in the corollary 1, a relaxed optimisation problem which will be introduced in the following section can be generated. in this paper, we will consider to learn linear hash functions via minimising the upper bound l"
"however, despite the progress made by existing methods considering certain aspects of the problem, cross-modal search remains a very challenging task because of the integration complexity and heterogeneity of the multi-modal data. in fact, the nature of multi-modal data is a combination of heterogeneity and the homogeneity. thus, in cross-modal search, the cross-modal and within-modal similarity information should be simultaneously considered. on the one hand, the methods developed based on supervision information mainly focus on the similarity information of heterogeneity without considering the homogeneous information, but it is obvious that the within-modal similarity benefits to capture the intrinsic geometric structure. on the other hand, the methods generated by emphasising within-modal similarity decompose multi-modal data into a set of uni-modal data, which means multi-modal similarity learning cannot be treated as a whole because more than one manifold are needed to represent both crossmodal and within-modal similarities. therefore, it is necessary to connect and integrate all information from data in different modalities to describe the diversity of the world. to achieve this, the key of cross-modal search is to overcome the obstacle of multiple modalities by considering both the local geometric and global supervision information."
"the proposed hmr is validated on four recent public datasets: the viper [cit] and cuhk01 [cit] datasets for cross-camera person re-identification, the wiki dataset [cit] for crossmodal retrieval and the fg-net ageing dataset [cit] for crossage face image retrieval where the number of modalities is 6. four state-of-the-art cross-modal binary code learning methods, including pdh [cit], cvh [cit], cmssh [cit] and cmfh [cit], are mainly compared with and some other area-specific methods are also used for comparative analysis in our experiments."
"as mentioned, the mixed-fem guarantees the conservation of charge by construction and especially in models with thin insulating compartments and at highest eccentricities it still leads to high accuracies. these results encourage the use of the mixed-fem also in related applications that depend on an accurate simulation of the electric current, such as the magnetoencephalography (meg) forward problem, transcranial direct current stimulation (tdcs), or deep brain stimulation (dbs) simulations."
"in this section, we validate the proposed hmr on a more challenging task: cross-age face retrieval. given a probe face image, we need to search for the face images of the same person but captured in different age stages. this task is derived from age estimation [cit] but it is more difficult and novel because: 1) the principal characteristics of the face appearance of a same person vary hugely along with the variation of his or her age."
"it is worth to point out that f u k is a hinge loss constraintsatisfied function only when all the samples in modality u satisfy condition (10) . and eq. (11) can be proved, when a condition 8k; then, based on the condition (10), we can extend the inequality (11) to a weighted cumulative distance inequality on a graph."
"for the implementation of the controller, we proceed to a temporary change of the control variable by replacing the generalized torque τ θ . to this end, we introduce c dθ, which represents the control law to be applied and n (θ, r,ṙ) the uncertainty function on θ, r andṙ in the dynamics equations. we thus have the relation:"
"we suggest a strategy based on the sliding-mode theory. the sliding-mode control law-or, more precisely, controller with variable structure generating a sliding regime-aims to obtain, by feedback, a dynamics widely independent from that of the process and its possible variations. hence, the controller can be considered as belonging to the class of robust controllers. the sliding-mode control appears attractive for the handling of nonlinear and linear systems, multivariable and singlevariable systems, as well as model or trajectory pursuit problems and problems of regulation."
"the optimisation problem (21) is derived from the problem (19) where the orthogonal constraint condition becomes zero because it is assumed that w (21) is convex. meanwhile, the lagrange dual of the optimisation problem (21) is a problem of quadratic programming. therefore, the optimal w ð1þ 1 could be defined as"
"in order to solve the problem in eq. (19), we first divide it into sub-problems, in each of which only one projection for the kth code is considered. thus, in eq. (15), the kth row vector y k of y is a binary vector which corresponds the kth bits of all samples in all modalities while the corresponding kth column vector of w is denoted as w k . then, we have"
"t he eeg forward problem is to simulate the electric potential on the head surface that is generated by a minimal patch of active brain tissue. its accurate solution is fundamental for precise eeg source analysis. this can be achieved via numerical methods that allow one to take the realistic head geometry into account. here, finite element methods (fem) achieve high accuracies and enable one to realistically model complicatedly shaped tissue boundaries, such as the gray matter/csf interface, and to incorporate tissue conductivity anisotropy. the importance of incorporating these model details for the computation of accurate forward solutions and in consequence also for precise source analysis was shown in multiple studies [cit] ."
"for eeg forward modeling, the mixed-fem shares this current preserving property with the recently proposed dg-fem [cit] . both approaches were evaluated against cg-fem approaches in the realistic six-compartment head model. in this head model, both mixed-and dg-fem were advantageous in comparison to the cg-fem (figure 8) . the projected mixed-fem clearly outperforms both whitney and st. venant cg-fem in this model and achieves a slightly higher accuracy than the partial integration dg-fem. since only few skull leakages occured in this model and as these were concentrated in the area of the temporal bone, leakage effects do not suffice to explain the higher accuracy of mixed-and dg-fem. an overall higher accuracy of these approaches in this kind of model, i.e., regular hexahedral with a mesh resolution of 2 mm, can be assumed. the relatively high level of errors is a consequence of the coarse regular hexahedral meshes that were used, while the reference solution was computed in a highly-resolved tetrahedral model. the result for the st. venant cg-fem in the 1 mm model helps to estimate the relation between the influence of the different numerical approaches and the accuracy of the approximation of the geometry. it is shown that the difference between projected mixed-fem and whitney and venant cg-fem in the 2 mm model is nearly as big as the difference between using the 1 mm and the 2 mm model for the venant cg-fem."
"the rest of this paper is organised as follows. the related work is introduced in section 2. in section 3, constructing a hetero-manifold for the multi-modal data is detailed. next, based on this hereo-manifold, learning a set of hash functions for cross-modal retrieval is presented in section 4. then, section 5 provides a sequential strategy to solve a complicated objective function. section 6 illustrates comprehensive experimental results for four datasets. section 7 draws our conclusions."
"therefore, the original optimisation problem (17) is transformed by replacing the constraint conditions ðiiþ with the relaxed constraint conditions (18) and using the lagrangian principle into"
"the sliding-mode controller introduced here was tested in real-life conditions with a torque-controlled four-wheel skid-steering robot. it was proven to be robust on sinusoidal paths, and despite wheel slippage. the chattering noticed during the experiments led to a higher energy consumption. in order to reduce it, we defined the gain variables according to some criteria such as the velocity or the path curvature. if the energy consumption becomes a concern, a higher-order sliding-mode control law should be considered."
"lastly, the hetero-manifold is flexible and can be extended to model any number of modalities. as far as we know, most of existing cross-modal searching algorithms either are limited to two modalities [cit] or strive to cope with more than two modalities but are still evaluated on the datasets with only two modalities [cit] ."
"images and texts are the two popular modalities for testing cross-modal retrieval methods. there are several datasets available but wiki is the most popular one. thus, in this experiment, the wiki [cit] dataset is used for our evaluations."
"to bridge the gap between modalities, various straightforward strategies have been developed to learn the cross-modal similarity. some methods focus on the supervision information including correspondences [cit], semantic correlation [cit], pairwise sets [cit] and semantic affinities [cit] between heterogeneous data, while others including composite multiple information sources [cit], a-average technique [cit], markov random field [cit] and deep neural networks [cit] emphasise the value of homogeneous manifold in the problem of multimodal similarity learning in a common space."
"with regard to the lnmag (figure 4, bottom row), only minor differences are recognizable for model seg 1 res 1. in model seg 2 res 2, it is notable that the direct mixed-fem leads to very high maximal errors for eccentricities of 0.987, while whitney cg-fem and projected mixed-fem perform similar with a tendency of the whitney cg-fem towards lower errors."
"the position is plotted on figure 4 in m, the gains being tuned for optimum pathfollowing: without sliding-mode, we see on the position curve that the vehicle follows the desired path with a small position offset. after a bend, the robot takes time to converge to the path. the succession of bends maintains this position error. with slidingmode, the position curve converges much better to the desired path with however a small lateral error of about the length of a vehicle track, between the second and the third bend."
"for a single, exemplary dipole the distribution of the volume currents in skull and skin in model seg 2 res 2 r82 simulated with the whitney cg-and projected mixed-fem is visualized in figure 7 . the leakage effect for the cg-fem (figure 7, left) is obvious. while the mixed-fem (figure 7, right) leads to a smooth current distribution and highest current strengths in the skull compartment (up to ≈ 13 µa/mm 2 ), the current strength peaks in the skin compartment for the whitney cg-fem (maximum ≈ 144 µa/mm 2 ) and the value is increased by a factor of more than 11 compared to the mixed-fem (note the different scaling of the colorbars). in comparison, the current strength in the skull is very low here, showing the leakage of the volume currents through the nodes shared between csf and skin."
"the results for the lnmag ( figure 5, bottom row) do not show remarkable differences for all models up to an eccentricity of 0.964. in model seg 1 res 1, the projected mixed-fem leads to the lowest spread for the three highest eccentricities. however, the lnmag decreases from positive values for all source positions at low eccentricities to completely negative values at the highest eccentricity. this effect is even stronger for the whitney cg-fem. in contrast, the median of the direct mixed-fem remains close to constant up to the highest eccentricity, however, with higher spread. the same behaviour of the three approaches, just at a generally higher error level is found for model seg 2 res 2."
"the controller is implemented in two steps: first, a proportional derivative controller is settled for path following; then, the sliding-mode controller is added and its gains tuned. this sliding-mode controller being a torque controller, a difficulty is that the robufast a robot inputs are its wheel velocities. it is thus necessary to convert the amplitude of the torques generated by the controller."
"normally, during the matching stage, the hamming distance is far less computationally expensive than the euclidean distance. however, despite the simplicity in eq. (8), minimisation of the hamming distance is generally intractable, because it is a concrete quantity. thus, we seek to minimise an alternative item, which guarantees that the hamming distance will be minimised simultaneously."
"finally, during several trial days, we noticed an increase of the energy consumption of 20 % to 30 % with the sliding-mode controller, the robot batteries emptying faster because of a higher frequency request of the actuators. this last point is a constraint, the supply of energy being an important problem in mobile robotics. this control law could be used at intervals, when it turns out to be necessary."
"in this notation, the saddle point structure of problem (8) and thus also (6) is recognizable. as a consequence, existence and uniqueness of a solution cannot be shown using the lemma of lax-milgram. indeed, while the therefore necessary boundedness is satisfied for operator a as defined in (7a), ellipticity is not given. further analysis shows that to prove existence and uniqueness of a solution to (8), it is necessary that the operator a is h(div; ω)-elliptic and b fulfills an inf-sup condition, which is in this case also called lbb condition, named after the mathematicians ladyzhenskaya, babuska, and brezzi. at this point, we shall restrict to noting that these conditions are fulfilled by a and b defined as in (7) and thereby existence and uniqueness of a solution"
"the results suggest that the mixed-fem achieves an appropriate accuracy for common sphere models, especially the projected approach. the comparison with the whitney cg-fem approach with optimized positions and orientations shows that the mixed-fem leads to comparable accuracies ( figure 4 ). both for optimized and arbitrary source positions, the projected approach achieved a superior accuracy compared to the direct approach. previous publications concentrated on evaluating the whitney cg-fem in tetrahedral models [cit] . in these studies, the accuracy of the whitney approach deteriorated when using arbitrary source positions and orientations, most possibly due to interpolation. this effect is not found in the hexahedral models used here and a high accuracy is achieved (figure 4) . these results should be deeper investigated in further studies. in the leaky models seg 2 res 2 r82, and seg 2 res 2 r83 the mixed-fem performs better than the whitney cg-fem ( figure 6 ). this is an expected result, since the mixed-fem is by construction charge preserving, which should prevent current leakages [cit] ."
"overall, the presented mixed-fem is an interesting new approach that can at least complement and in some scenarios even outperform standard continuous galerkin fem approaches for simulation studies in bioelectromagnetism."
"because of the structure of the heterograph, and each u ik exists in mn constraint conditions. thus all of these constraint conditions can be summed up and the conditions will be relaxed as"
"in this paper, a new finite element method to solve the eeg forward problem is introduced. it is expected that it should be preferrable compared to the commonly used cg-fem approaches especially in leakage and realistic scenarios. the goal of sections iv-a and iv-b is to show that this new method performs appropriately when compared to the established cg-fem in common sphere models, while in sections iv-c and iv-d the advantages in leakage and realistic scenarios are evaluated."
"a variety of different fem approaches has been proposed, e.g., the st. venant, partial integration, whitney, or subtraction approach [cit] . while these approaches differ in the way the dipole source is modelled, the underlying discretization of the continuous partial differential equation (pde) is the same; a conforming galerkin-fem with most often linear ansatz-functions (cg-fem). the necessary discretization of the head volume can be achieved either using tetrahedral or hexahedral head models. hexahedral models have the advantage that they can be directly generated from voxel-based magnetic resonance images (mri), while the generation of tetrahedral meshes can be complicated. therefore, hexahedral meshes are more and more frequently used in praxis [cit] and were furthermore recently positively validated in an animal study [cit] ."
"in order to evaluate the accuracy of the mixed-fem, different comparisons both in hexahedral four-layer sphere and realistic head models were performed. as it is common for the evaluation of eeg forward approaches, the error measures rdm (minimal error 0, maximal error 2) and lnmag (minimal error 0, maximal error ±∞) were used [cit] ."
"the matrix k has a large 0-block. thus, directly solving (10) is not recommendable [cit] . krylov subspace algorithms, such as variants of the conugate gradient (cg) or generalized minmal residual (gmres) method, are not as efficient as for many other problems, since the commonly used methods for preconditioning fail. nevertheless, much research has been performed trying to find preconditioning techniques that make a solution using cg-solvers possible [cit] . we apply a different solution strategy that makes use of the fact that a is -unlike k -positive definite. if we write (10) as a system of two equations,"
"the robot moves at a velocity of 3 m/s along a sinusoidal road. a derivativeproportional controller is applied to the robot, then the sliding-mode controller is implemented."
"to investigate the details of cross-age retrieval, the performance at ranks 1, 10 and 20 between any modalities is shown in tables 4, 5, and 6, respectively. on the one hand, we can see that, in general, the performance of cross-age retrieval between two adjacent modalities is higher than that of nonadjacent modalities. in essence, the appearance changes between adjacent modalities will be smaller than those between large age gaps. on the other hand, it is interesting that the retrieval performance when the probe image comes from older age stages and the gallery consists of images from earlier ages normally will be better than the opposite conditions. we think this is because the appearance variation trend in the later age stages becomes smaller and some important identification characteristics remain as age increases."
"qureshi and colleagues described the jaroka tele-health system, which employed lady health workers in rural pakistan to use sms/mms to register patients, report symptoms, acquire prescriptions and connect to specialists. the resulting data was then used to track disease spread and the authors have been able to use the visualised data via crude numbers or rates and identify clustering; the authors found higher rates of disease during times of migration and with internally displaced persons, which could be explained by poor sanitation and overpopulation. they also found increased rates of hypertension in women in these populations [cit] ."
"developed and piloted a web-based crowdsourcing software to enable junior doctors to upload cases and receive feedback from expert physicians, with an element of gamification using reward points."
"uses amazon mechanical turk for classifying fundus photos of diabetic retinopathy. using a 19% threshold, 90% of melanomas were identified and 72% of non-melanomas, and with a 65% threshold, 67% of melanomas were identified and 100% of non-melanomas. authors recommend the 19% threshold. [cit] diagnosis diagnosing medical images"
"using results from personal genomics (my quantified self), and through tracking personal behavior, the authors explore the relation between the oxtr gene and social intelligence using personality testing"
"the paper discusses some difficulties with the app, including verifying images due to clarity and receiving multiple images/submissions of the same breeding site. it does not report on the impact of the app on dengue outbreaks."
"when 'live' for one month, app had already been downloaded over 1000 times and 600 disease notifications had been added. data was treated as most trusted depending on information provided by submitter (ie, if email, contact details submitted). [cit] surveillance disease outbreak monitoring"
"the docchirp (crowdsourcing health information retrieval protocol for doctors) is a mobile application that helps clinician' s problem solving at the point-of-care. sims and colleagues presented the experiences of clinicians using the application. 78% of clinicians using the application reported benefit on routine patient care, medical education and accurate referrals, as well as diagnosing unusual cases. some concerns reported by clinicians included lowered productivity, due to responding to the application, and interference with 'off the clock' time, though the latter was only reported among non-users [cit] . mccomb and bond also reported on an application that assists clinicians in making diagnoses. their application, called codiagnose, has junior doctors upload case information and receive feedback from a crowd of expert clinicians and it features a built-in e-learning component. the junior doctors' diagnostic accuracy improved 14% with the use of the application; however, the authors reported a lack of enthusiasm on part of the expert clinicians about participating [cit] ."
"used amazon mechanical turk to rank severity of adverse drug reactions (adrs), which were retrieved from the sider2 database. turkers were provided with 10 pairwise comparisons of adrs and were asked to select which is worse."
"single-trial and oscillatory datasets (sets 7-9) single-trial datasets reflecting functional connectivity in a working memory task were created with and without oscillatory activity and are suitable for most types of analyses (i.e., ica, time-frequency analysis, granger causality, etc.). in each case, sources embedded within 128 single trials of noise were jittered about their mean latency and amplitude. set 7 is similar to set 6 (vsm-ctf meg system) only now each of the 128 single trials is available. again, the four cortical sites were: 1) primary visual cortex (v1); 2) inferior lateral occipital gyrus (i.log); 3) intraparietal sulcus (ips); and 4) dorsolateral prefrontal cortex (dlpfc). the cortical patch current strengths were initially assigned values similar to those we observe in our visual working memory studies (30-50 nam peaks) using the mriview forward simulator [cit] and were then randomly jittered about those values by up to +/− 50% across the single trials. peak latencies were also jittered across each trial by a randomly selected value up to +/− the full width at half maximum (fwhm) divided by 2. to allow for traditional source analysis of averaged evoked responses, the 128 single trials were then averaged together and written out to the netcdf file format."
"kido and swan reported on using crowdsourced data from myquantifiedself, which is a personal genomics company, in order to test their novel hypothesis, that some genetic profiles would exhibit a natural capacity for social intelligence. the authors combined citizen science as a form of crowdsourcing and the daily tracking of their \"myfinder\" application in order to explore the role of genomics (oxtr gene mutations) on personalities. the authors found that their hypothesis did not appear to be valid, that the increase in the frequency of the g allele did not lead to increased optimism; however, an increase in the frequency of the a allele seemed to lead to decreased optimism. the authors stated that further analyses with larger sample sizes needed to be conducted in order to confirm their hypotheses [cit] ."
"dumitrache [cit] general medicine/ other extracting annotation from medical text \"dr. detective\" is a game that uses medical experts as a crowd, and is designed to extract annotation and solve disagreements in medical text."
"a recent study using global burden of disease (gbd) data examined creating a game to scan the gbd database for errors, as algorithms currently used are imperfect. while participants were staff from the institute for health metrics and evaluation at the university of washington, and thus not laypersons with no background knowledge, the author found that using gamification increased the accuracy of results by 1.7 times [cit] . as only 4% of infectious diseases have been comprehensively mapped, innovative solutions such as amt and other crowdsourcing applications detailed above may be useful in providing infectious disease mapping and surveillance [cit] ."
"954 reports were obtained through the website. these were compared to prices provided through law enforcements and through the dark web. the prices were highly correlated between the three. authors propose a distribution method using the mobility of the local population, and using information gained by cell towers. participants would exchange packages at a point they normally visit, at a time they normally visit it."
"the authors divided players into 'experts' and 'inexperienced' and found that the expert group and considering both groups together significantly enriched knowledge for cancer related diseases, while the inexperienced group' s results did not. [cit] general medicine/ other"
"crowdsourcing has been used in public health for research in the areas of tobacco control, physical activity and built environment, environmental health, to shape messaging and for public health related contests [cit] ."
"the third study explored the potential for crowdsourcing to adequately respond to discussions in an autism support group by outsourcing the help questions to amt and having the responses rated against the in-group answers. the amt answers were rated as more helpful and amt was seen as a quick way to provide direct and informal emotional support and to broaden perspectives of the autistic community [cit] . the final study combined crowdsourcing and data mining, using amt to generate alternative life quality statements to enable comprehensive data mining for these statements from twitter [cit] ."
"the second way crowdsourcing can be used in diagnostics in by helping clinicians make diagnoses through applications such as docchirp and codiagnose, which had promising results. however, both applications appeared to have some push-back from their 'crowd' of experts [cit], which questions their sustainability."
"uses amazon mechanical turk to study viability of crowdsourcing to detect glaucomatous optic neuropathy. turkers were asked to classify images as normal or abnormal, with each image being classified 20 times."
"authors gained information of distribution of malaria species in india, and estimated burden, which coincided with official public health reports. freeman [cit] identifying erroneous global burden of disease estimates surveillance a crowdsourcing platform was designed, comparing the effect of gamification, to identify erroneous estimates in the global burden of disease database."
"three articles reported on physical instances of crowdsourcing. maki and cohnstaedt reported on using crowdsourcing to collect physical samples of mosquitos from a mostly trained crowd. through crowdsourcing, the authors were able to collect mosquitos in \"geographically vital, hard-to-access locations\" and achieved a 91% response rate [cit] . mcinerney and colleagues reported on a pilot to employ crowdsourcing and bayesian modelling to deliver items in low-and middle-income countries. the authors described using cell towers during texts and calls to predict temporality, assessing the number of people to achieve geographic coverage and the feasibility to deliver to rural locations and briefly mention a pilot."
"somatosensory study an s88 dual channel stimulator with psisu7 optical stimulus isolation units (grass instruments, west warwick, ri, usa) was used for the somatosensory study. electrical stimuli were delivered to the median nerve and to the index finger of the right and left hands with an isi varying between 1.5 and 2 s. the intensity of median nerve stimulation was adjusted to produce a mild thumb twitch and the intensity was kept the same for index fingers and median nerves. [cit] ."
"the authors found that turkers were younger, more educated, white, and more likely to be middle-class compared to the general population. the frequency of trauma exposure and depression. a high proportion of turkers had clinically relevant anxiety symptoms, but this mirrors previous studies of active internet users. the data were deemed reliable, but authors recommend similar studies in other countries. de [cit] psychology measuring depression in populations via social media uses amazon mechanical turk to obtain a survey on depression, and a self-report of history of depression. the turkers could opt-in to share their social media handles."
"the field of crowdsourcing has developed in information technology or business, but crowdsourcing can be a promising tool in health, and in global health in particular. it is rapid, low cost, and can collect a huge amount of information from a large number of people [3, [cit] . it also is a flexible method that has the potential to cover a variety of research, including quickly evolving epidemiological research and traditional behavioural research. it can cover unpredictable events, produce novel discoveries, and can also be used to raise public awareness [3, [cit] . research in crowdsourcing has also been shown to be at least as accurate as traditional research methods [cit] ."
"multidipole, spatiotemporal source localization was conducted for subject #2 (m072) using the csst algorithm for simulated data sets 8 and 9 (ctf and neuromag systems, respectively). table 4 shows the results from these analyses. once again csst determines the locations of the active cortical areas with a good degree of accuracy. we do find obvious differences between the results for the csst dipole fits for the two different subjects. this was not surprising since the simulations were 1) created using each subjects' mri, therefore, the exact location of the cortical patch differs somewhat between subjects which will result in different waveform distributions at the sensor level for the different meg systems; and 2) the v1 source was given a smaller initial amplitude (30 nam versus 50 nam) in subject #2 (m072), making it more difficult to identify. furthermore, there is also a slight variation in how accurately each source is located depending on the meg system used to collect the empirical data from which the noise trials were taken to create the simulated data."
"remaining examples of crowdsourcing were in areas of oncology, medical text, various aspects of drugs, including curation, severity of reactions and even black market prices, examples of physical crowdsourcing and other miscellaneous examples [cit] 93 ]."
"the web interface improved diagnostic ability of junior clinicians, but senior clinicians were less actively involved, due to workload, time, availability and reluctance to embrace the new technology. [cit] surveillance review of participatory epidemiology"
"it is important for future work using crowdsourcing to consider the appropriateness of the crowd being used, to ensure the crowd has the capability and the adequate knowledge and also, to design the task and the method of analysis effectively. freeman found that using gamification (ie, having crowdsourcing activities linked to a game with rewards, scoreboards or some sort of competition) improved accuracy and, examples provided that have used gamification have been quite successful [cit] . other modes of analysis that have been successful include introducing thresholds and degrees of trustworthiness in order for an individuals' answers to be included into the crowd' s or for the crowds' answer to be used [cit] . it is important to note that not all the research, nor all the successful research, in crowdsourcing involved the internet. some of the crowdsourcing studies were done in person or involved sending in physical samples [cit] . previous definitions of crowdsourcing necessitated using the internet [cit], but use of the internet is not compulsory and this is important to stress, especially in the context of global health where use of the internet may not be accessible to all."
"used poor yelp reviews, specifically those using the words sick, vomit, diarrhea, or food poisoning, to identify food poisoning in new york city restaurants users in the eatery app post a photo of food, asking other users how healthy it is, and receive crowdsourcing ratings. the goal is to modify diets based on the feedback."
"somatosensory and auditory datasets simulating median nerve stimulation (fig. 8) provides one of the simplest cases. this activity consists of contralateral primary somatosensory (si contra ), contralateral secondary somatosensory (sii contra ), and ipsilateral secondary somatosensory cortex activity (sii ipsi ). and finally, an auditory dataset (fig. 9) provides a simple example of initial synchronous, bilateral activity in auditory cortex. this set also includes asynchronous activation of the temporo-parietal junction and cingulate cortex (4 cortical sources)."
"in set 4, six sources of activity are modeled where one pair of sources is synchronous (one small and one large cortical patch with consequent differences in intensity). this type of profile builds off of set 3 by making it more consistent with our working memory studies where initial asynchronous activity is followed by late activity in several disparate brain regions (including dorsolateral prefrontal cortex, anterior cingulate and superior lateral occipital gyrus) that become synchronous over time [cit], along with many others, suggests that synchronous neuronal firing provides one mechanism for binding the different features/attributes of stimuli across widespread cortical areas. table 2 shows actual source locations, csst estimated source locations, and errors when either noise was absent (no-noise) or real noise was present. average error across the 6 sources was 0.1 mm for the nonoise condition and 6.8 mm for the real noise condition. this fig. 4 sample source locations and time-courses for 3 of the simulated cases. note the subtle differences in temporal dynamics between these test cases table demonstrates that the presence of real noise does significantly affect source localization accuracy; however, our csst solution for the real noise condition was still good for this complicated dataset and inconsistent with previous critiques of dipole modeling approaches that state dipole methods cannot accurately localize more than a few point sources of activity. set 5 is the same as set 4 with the addition of a source in right hippocampus (7 sources of activity). set 6 (not shown in table 1 ) is a case where late activity (e.g., 400-600 ms) was synchronous across four cortical sites (v1, i. log, ips, and dlpfc), also seen in working memory studies. the upper left panel of fig. 5 displays the locations of the cortical patches (cortical patches are located at the crosshairs) while the time-courses assigned to the cortical patches are shown beneath the mris. the averaged waveforms (128 trials with signals embedded in real spontaneous noise) seen across 275 channels are shown in the middle left column. csst source locations are shown in the upper right panel (see tabled values). the table shows the coordinates of the actual sources, the estimated source locations, and the errors using euclidean distance. net source orientation errors were 42.0°for v1, 58.2°for i. log, 20.9°for ips and 48.0°for the dlpf sources. the middle right panel shows the estimated time-courses and source locations. the average error across all 4 sources was 6.7 mm with the greatest error for the i. log source. the cross-correlations between time-courses are shown in the bottom row of this figure. we examined early activity first (200-350 ms-bottom left panel) which shows that v1 activity correlated highly with i. log (dark blue tracing), regions showing the initial spike-like activity (~280 ms). ips and dlpf cross-correlations were also highly correlated and near the zero-lag (orange tracing). the maximal correlation coefficients of the other pairs of sources were lower in value and were not near the zero-lag. in contrast, the late activity (350-600 ms-bottom right panel) shows higher zero-lag correlation coefficients for activity between the 4 brain regions (i.e., late activity was synchronous across brain regions) with ips and dlpfc revealing the highest correlation coefficient (orange tracing). these data are also suitable for examining basic coherence between sensors."
"overall, peer and expert ratings were highly correlated across the us and europe. several food categories led to higher healthiness scores among peers (fruit, vegetables, whole grains, legumes, nuts and seeds) and lower healthiness scores among peers (fast food, refined grains, red meat, cheese, savory snacks, desserts, and sugar-sweetened beverages) [cit] nutrition identifying calories in meals using a smartphone a mobile application was used to pilot the feasibility of a smartphone app for crowdsourcing with non-experts to identify calories. training was provided to non-experts, who were asked a month later to estimate calories of food using a photo. a crowd of experts and non-experts was investigated."
"authors were able to reach a more diverse population than with traditional data collection methods, more quickly, and for less cost. they were able to gain more nuanced suggestions to tailor their materials to different populations. [cit] public health & environment including youth perspectives in hiv/aids messaging a website, crowdoutaids.org, was created to enable youth to be involved in shaping policies, to set priorities and influence actions at unaids. the website intended to connect a community of young people and collect their experiences and ideas and to provide a means to synthesize the information collected from youth globally. questions were asked in community and online forums in order to ensure youth without internet access were able to participate."
"in 9 mo, 6, 941 unique gene-disease assertions were generated from dizeez; 2137 were not found in any gene-disease databases (omim, pharmgkb, or gene wiki). 17 of these associations occurred more than 7 times; these were statistically significant. authors examined these through a manual literature search and found evidence 14. [cit] genetics gene mutation relations"
"barriers addressed by creating realistic simulated datasets one barrier encountered in the area of software development for electromagnetic measures is the lack of an extensive, realistic simulated testbed for determining the success of the algorithms and for comparing algorithmic performance with others (i.e., standardization). often developers test their algorithms using one or a few test cases that may or may not closely emulate real brain activity and often these test cases are not readily amenable for other developers to use. for example, white noise is often added to the data to simulate noise normally contained in data. the addition of real brain noise (e.g., ongoing background rhythms not related to the task) is more appropriate since real brain noise can have a dramatic effect on the localization ability of algorithms. furthermore, users of algorithms would like to know how various analysis methods work in the modalities they are investigating (e.g., auditory, visual, somatosensory) or in the specific areas of their research interests (e.g., sensory or cognitive studies). our realistic simulated datasets also show tremendous differences in snr across participants for similar source locations, due to signal cancellation and summation associated with differences in cortical geometry [e.g., [cit], b) ]. an algorithm may work well in one of the scenarios listed above but may be less than optimal in others. the creation of realistic datasets ranging from sensory to cognitive studies in auditory, somatosensory, and visual modalities and from a number of participants can help developers tremendously in understanding behaviors of their algorithms."
dizeez is a crowdsourcing game where players match genes with a clue of the disease; players receive points for selecting the correct disease-gene match. players can select a specific disease area or protein family.
"comprehensibility scores were calculated, which ranged from 45% to 98%, and correlated strongly to those in another study uses oral responses with the same pictograms."
biogames has achieved diagnostic accuracy comparable to that of experts when scores from individual non-experts are aggregated and the crowd size is large. [cit] diagnosis malaria diagnosis a crowdsourcing game (malariaspot) was designed using malaria-positive blood films.
"overall, the classifications were matched to a gbd expert 86% of the time. adding gamification increased accuracy significantly, with gamified users identifying 1.7 times more trends than those using a standard (non-gamified) interface. [cit] nutrition restaurant reviews to identify foodborne illness"
"unaids was able to collect information across the globe, highlight similarities and differences from youth both globally and within regions, and to enable youth to influence their policy."
"empirical meg/mri datasets have been acquired for 5 participants under a partnership formed between the mrn, massachusetts general hospital, university of minnesota/ veterans affairs in minneapolis, university of new mexico, and los alamos national laboratory. data were acquired using 3 different meg systems (vsm medtech 275, elekta-neuromag 306, 4-d neuroimaging 3600) and 3 different sensory paradigms (visual, auditory and somatosensory) for each participant. these empirical data will be made available via the meg-sim portal and will be discussed more completely later. a grant from nimh (r21mh080141) allowed us to then create realistic fig. 1 a freesurfer-segmented gray matter/white matter boundary for the simulations (shown in red) was imported into mriview from which patches (a) of simulated activity (b) were generated. 100 passes of spontaneous activity or noise (c) were identified using ctf software (data editor) and averaged together using megan. the simulated activity was embedded within the averaged noise file (d) and saved in netcdf format (i.e., a netmeg file in megan) simulated data derived from the real noise contained in the collected data and to establish a web portal for others to access these simulated datasets. we refer to the testbed as 'realistic' simulated data because: 1) colored noise is used in most examples (i.e., spontaneous data containing correlated noise); 2) the time-courses and source locations simulated are based on findings from empirical data; 3) different-sized cortical patches are created from mris of individual participants (i.e., the snr and orientation of sources differ across participants); and 4) in some cases each of the unique single trials, mimicking actual data acquisition, are provided."
"received 248 submissions from 21 teams over approximately 6 mo. the leaderboard enabled teams to improve submissions once they had an initial performance estimate. finally, authors aggregated submissions of best-performing teams. [cit] genetics"
"in genetics, crowdsourcing has been used for challenges in genetic research, for matching genes to mutations, to identify novel hypotheses through crowdsourced data, and it was proposed as a solution for incidental findings in genomics studies [cit] ."
"finally, as gwas studies are becoming more prevalent and the probability of incidental findings becomes more likely, krantz and berg suggest crowdsourcing as a solution to managing incidental findings. the authors proposed a 'binning system' in genetic studies that employs crowdsourcing, such as through amt, to separate incidental findings into bins based on their current risk to the individual. for example, clinically actionable results would be placed in \"bin 1\", while results with a high clinical validity with no actionability would be in \"bin 2\" and those with no clinical significance would be in \"bin 3.\" the results in bin 2 would need to be re-scored as new advances in medicine are made [cit] ."
"the authors' results were not statistically significant and need more power; however, the authors' initial results were in a different direction than hypothesized. individuals with the ag genotype have lower eq/iri values than those with aa, and that an increase in the a allele' s frequency corresponds to decreased optimism. krantz & berg [cit] genetics incidental findings in gwas studies"
"created a game called \"the cure\" which trained players prior to entering the main gaming area. once in this area, gamers play against an automated opponent, selecting genes in a decision tree classifier, with the aim of surviving."
"used crowdflower, and uploaded snomed ct relationships and a definition; the crowd was asked whether this was true or false. experts were also asked to evaluate relationships."
"3000 hits were posted from 706 drug labels in 8 h. the aggregated accuracy was 96%, and the total cost was us$ 1.75 per drug label, which is substantially less expensive that traditional alternatives. [cit] general medicine/ other black market prices for prescription opioids uses streetrx (a crowdsourcing website) to obtain prices of prescription opioids. visitors to the website anonymously post the price they paid for prescription opioids and where they were purchased (and are able to see similar purchases and prices)."
"the program was able to display regional patterns for diseases, as well as a disease outbreak that was due to a mass migration of internally displaced persons. the authors reported that the program helps identify whether an epidemic is imminent. [cit] surveillance dengue surveillance"
"web forums were ineffective. turkers diagnosed easy to diagnose cases. o'desk workers were able to diagnose easy cases, but were more likely to express caution when providing diagnoses for any complicated cases, and some refused to provide diagnoses. [cit] diagnosis point-of-care problem solving for clinicians authors reports clinicians' experiences using a crowdsourcing application for point-of-care problem solving, where clinicians post problems via an app and these are answered by verified users and viewable by users in that user' s provider group over 80% of respondents felt that app could have a positive impact on patient care, medical education, referrals, and difficult diagnoses. both non-users and users were surveyed, and non-users were more concerned about potential to disrupt workflow. mccomb & bond [cit] diagnosis increasing diagnostic accuracy among junior physicians"
"the average accuracy was 39.54%, with the best student only making the correct diagnosis 50% of the time. using a machine learning algorithm with majority voting, combined with crowdsourcing, which learns the students' mistakes, accuracy can increase to 80%. [cit] diagnosis diagnosing medical illnesses investigated the feasibility of using crowdsourcing for diagnosing medical conditions with case descriptions of varying difficulty, posted on amazon mechanical turk, o'desk, and web forums."
"bow and colleagues reported using crowdsourcing with their pre-clinical medicine students at johns hopkins university to create flashcards to improve studying. the professors had questions and respective answers after lectures available on googledrive, which was shared with the class. students were able to add to questions, add new questions and add to answers. the questions were changed to flashcards using java, to assist students as study aids. the students' grades improved in comparison with students from the previous year [cit] . another study stated that crowdsourcing could help educators grade students' assignments more fairly, but did not elaborate [cit] ."
"the creation of single-trial simulated datasets permit a wider variety of meg analysis tools to be compared. construction of single trials that mimic the differences between epochs of real data allow the use of analysis techniques such as ica to be used in conjunction with various source modeling techniques to identify functional networks. these results can then be compared with traditional source analysis conducted on averaged data. with the addition of oscillations to the simulated datasets, analyses of the accuracy of functional connectivity measures between various brain areas can also be investigated. this is important since it is incomplete to know, for example, that certain brain locations are active without information about which areas onset first, their durations, and whether cross-frequency oscillatory activity is evident across multiple disparate brain regions or not. furthermore, the simulated datasets described here are available in a variety of file formats, including netcdf, neuromag .fif, ctf .ds, and curry (compumedics, neuroscan). hopefully, the creation of these new datasets and formats, including novel single trial simulations, will foster algorithm performance comparisons and facilitate cross-site collaborations. all datasets and single trials discussed herein are currently available at the web portal (http://cobre.mrn.org/megsim/)."
"crowdsourcing is a new field that has been widely used and is innovative and adaptable. with the exception of surveillance applications that are used in emergency and disaster situations, most uses of crowdsourcing have only been used as pilots. these exceptions demonstrate that it is possible to take crowdsourcing applications to scale. crowdsourcing has the potential to provide more accessible health care to more communities and individuals rapidly and to lower costs of care."
"crowdsourcing as a field is still nascent, with the term having only been coined a decade ago [cit] . despite this, it has been used across numerous disciplines in medicine, from diagnosis and surveillance to nutrition, psychology, and even to crowdsourcing minimally invasive surgery. the wideness of uses demonstrates that crowdsourcing applications have been innovative and adaptable. however, many of the crowdsourcing applications have not been used past pilot phases, with the exception of surveillance applications that are used in disaster and emergency relief. these exceptions demonstrate that it is possible to use crowdsourcing at scale; further efforts are needed to take promising crowdsourcing applications to scale in order to provide accessible health care to more communities and individuals rapidly at a low cost."
"one goal of this effort is to offer developers of meg methods, and hopefully eeg methods in the future, an opportunity to directly compare results from their analysis routines with others by using this extensive realistic simulated and empirical testbed of data established for the purpose of quantifying strengths and limitations of each method (standardization). this will aid in the refinement and further development of algorithms. second, we are all aware that some analysis procedures are better-suited for certain types of studies while other analysis procedures are better-suited for other studies. the extensive testbed of realistic simulated data provided at the web portal (http:// cobre.mrn.org/megsim/) includes sample datasets emulating sensory through working memory-related processes across visual, auditory, and somatosensory modalities. users of meg analysis procedures should be able to make informed decisions as to which analysis tools are best-suited for their data by working with these datasets. the direct comparison of different analysis techniques is necessary for moving the meg (and eeg) field forward in the neuroimaging arena."
"misinterpretations were judged to be based on errors within the pictograms themselves, not with the turkers' abilities. [cit] general medicine/ other fact extraction from scientific literature authors present a conceptual framework for scientific fact extraction from literature in different disciplines, to assist researchers who are conducting cross-disciplinary research."
"there were four articles published in psychology [cit] . the first explored the viability of using crowdsourcing, specifically via amt, for studying mental health issues. the authors assessed misrepresentation, inconsistencies in basic demographic information and clinical symptom reporting. the authors found that amt workers' mental health mirrors that of the general population, other than social anxiety and satisfaction of life scales. amt workers have an increased social anxiety, which mirrors other internet-based studies, and lower satisfaction with life scales [cit] . a second study used amt to diagnose major depressive disorder (mdd). it asked users for the twitter account and data-mined their accounts for one-year prior in order to measure user engagement, egocentric social graph, linguistic style, depressive language use, and their mention of using antidepressants. the control was a standard user class. the authors found lower social activity, greater negative emotion, higher self-attentional focus, increased relational and medicinal concerns and heightened expression of religious thoughts among the mdd group [cit] ."
"in the area of nutrition, articles employing crowdsourcing focused on food safety, food labelling, assessing how healthy the meals were, and identifying predictors of obesity [cit] . two articles used yelp reviews to assess food safety in restaurants, one in new york city and the other in seattle [cit] . in new york city, the department of health and mental hygiene received data from yelp and used computer algorithms to scan the data for probable food poisoning cases. these cases were then reviewed by a food-borne epidemiologist, and probable cases were requested for interview. three outbreaks were discovered by the study [cit] . the second article, which uses yelp reviews for seattle, tested whether yelp reviews would be able to predict whether a restaurant would fail its health inspection. the authors found that the yelp reviews were 82% accurate predictors for restaurants that would fail health inspections [cit] ."
"an additional barrier for meg investigators is the fact that meg systems made by different manufacturers have different pickup coils, sensor arrays, noise cancellation methods, as well as different software packages for data analysis. many of the software implementations are specific to one particular data storage format or noise cancellation method as well. these factors make it extremely difficult to compare results or pool data across laboratories. if one developer creates a simulated data set using the sensor geometry of the elekta neuromag 306 system, for example, then investigators using the vsm medtech ctf 275 system usually cannot use it because of file formatting barriers. we have been working on software that is machine-independent (e. best and d. ranken) that can convert from one meg system format to another. a standard testbed that can be used by all developers and users would be of great help to the meg community, and hopefully to the eeg community in the future."
"because there is a lack of high-level experts in rural china, the authors investigated whether crowdsourcing could be used to diagnose medical images. 2 nd_ or 3 rd_ year graduate students with a medical imaging major participated."
"another way crowdsourcing has been used in public health is through contests to draw attention to important causes and promote public engagement. in philadelphia, a large crowdsourced competition was deployed to bring attention to heart disease through mapping automated external defibrillators (aeds) [cit] . through this contest, 1429 aeds were located through 313 submissions. data were validated through gps, door-to-door and photo verifications. the authors were pleasantly surprised that, despite being a social-media based exercise, many older participants contributed. another exercise that used a crowdsourcing contest to draw attention to a public health concern asked participants to design and develop videos to promote hiv testing in china. seven eligible videos were submitted in an eight-week time period [cit] ."
"michael & geleta [cit] surveillance global disease surveillance describes a smartphone app, click clinica, to increase the identification of infectious diseases globally. app contains clinical guidelines, and questions to confirm diagnosis and resistance information."
"some of the research that has taken advantage of crowdsourcing in the public health and environmental health would not have been possible without crowdsourcing. for example, hipp' s characterisation of changes in the built environment' s effect on physical activity, using web cam footage, would not have been possible without crowdsourcing; it would have been impossible to have the hours and hours of webcam footage annotated. the citi-sense and citi-sense-mob project, which uses mobile phones to obtain individual level data that are gps tagged to map air pollution, would also be impossible without crowdsourcing."
"clickclinica is an application that was developed to provide general practitioners (gps) and medical doctors (mds) indexed guidelines for diseases. currently, gps and mds use the application to diagnose diseases by asking questions about the condition of the patient. the submitted data are graded by the quality of the user information, ie, if the submitter has a medical id or an institutional email. over 1000 mds across the world have been using the application, despite it not being marketed. the application can also be used to increase recruitment for clinical studies through gis notification of nearby, relevant studies. the authors suggested that this application could be developed into real-time global surveillance based on usage [cit] ."
"used amazon mechanical turk to curate drugs indications. hits were simplified by asking turkers to make a judgement of whether a drug label is indicated for a disease, which is highlighted."
"both the crowd of experts and the crowd of non-experts outperformed individual experts or individual non-experts. the expert group estimated the calories significantly more accurately than the non-expert group. once annotated, study found changes in behaviour after changes in built environment."
"uses lady health workers in rural areas of pakistan to report via sms health information to an electronic disease monitoring system (jaroka telehealthcare system), which provides geospatial location of patients for doctors, medical experts and health officials."
"eight areas of importance were identified: (i) diagnosis; (ii) surveillance; (iii) nutrition; (iv) public health and environment; (v) education; (vi) genetics; (vii) psychology; and, (viii) general medicine/other. table 1 provides an in-depth description of the individual studies."
"turner and colleagues employed amt to test multilingual promotional dental materials (in english and spanish). they were able to receive feedback from almost 400 turkers in less than 2 weeks and received especially valuable feedback from the spanish-speaking turkers regarding the cultural appropriateness and dialects of their messaging [cit] . an additional public health messaging project that used crowdsourcing was crowdoutaids, which was a large campaign by the unaids secretariat that had included youth, both online and offline, from 79 countries in shaping unaids messaging and their priorities for sexual health [cit] ."
"love and colleagues used crowdsourcing to explore women' s questions regarding collateral damage from breast cancer treatment. advocacy organisations collected responses to the questions posed. many of the women who submitted questions complained of fatigue, memory loss, numbness, anxiety or depression [cit] . another study used crowdsourcing to survey the knowledge of the population about ovarian cancer, using breast cancer as a control. the authors used amt as a platform and found that 56% of those surveyed reported no knowledge of ovarian cancer [cit] . a third study employed a crowdsourcing game, 'the cure,' to predict breast cancer survival in order to improve prognostic indicators of breast cancer. approximately 60% of players were not knowledgeable about breast cancer. the authors reported that both the expert and the all (expert and non-expert combined) set 'significantly enriched' knowledge, but that the non-expert group alone did not. the responses of the expert group performed well in a sage contest as well [cit] ."
"freifeld and colleagues reviewed a number of crowdsourcing platforms that have been used for community surveillance and participatory epidemiology. frontline sms, also called frontline sms medic, enables users to request needs via sms. it has been used in malawi, burundi, bangladesh and honduras. ushahidi is an open source crowdsourcing application that collects individual reports via web, sms, and email. it can classify, translate and geotag results. ushahidi was initially created in response to election violence in kenya, but it has been used most famously in the aftermath of the haitian earthquake. it has also been deployed in uganda, malawi, afghanistan, and zambia. ushahidi also has a feature for collecting voice reports, which is essential for people who are not literate. geochat is another crowdsourcing application. it aims to aid in faster and more coordinated responses to disease outbreaks and natural disasters. team members use the application to communicate their location through sms, email and web. this information synchronises on all users' devices. the application has been launched in thailand and cambodia [cit] ."
"authors describe two projects where sensors are provided to citizens and linked to smartphones, measuring air pollution. the hope is that the sensors will change behaviour patterns, causing citizens to avoid polluted areas, while also producing a map of pollution for cities in which sensors are in use."
"moorhead also used photos to have a crowd estimate the calories in food, but also developed personalised messages for prevention and management of obesity. they piloted their application with a crowd of 12 non-experts and 12 experts. in both cases, the group estimates were more accurate than any individual estimate [cit] ."
"diagnosis was the most common usage of crowdsourcing in health. crowdsourcing has been used multiple times for diagnosing malaria, specifically, and then for grading images in order to diagnose various conditions and diseases [cit] 88] . it has also been used to assist physicians in diagnosing conditions [cit] ."
"authors propose using crowdsourcing to solve the problem of reporting incidental findings to populations who have participated in gwas studies, given that new knowledge of genetic diseases is being discovered. amazon mechanical turk was used, restricted to us residents, to explore whether an amt population would be a viable research tool for mental health studies. participants were followed up one week later. fabrication of mental health symptoms was investigated."
"authors used amazon mechanical turk to judge associations between genes and mutations. genes were taken from the gennorm system, and mutations from the extractor of mutations system. genes and mutations mentioned in pubmed were included, and turkers were provided with the abstract(s) mentioning the gene and mutations and had to judge if they were related."
"as mentioned previously, visual, somatosensory and auditory data have already been acquired from 5 participants. details regarding these data are described below. visual study small visual patterns of two sizes (1.0°and 5.0°o f visual angle) were presented at 3.8°eccentricity in the left and right visual fields. the small stimulus was designed to activate~4 mm 2 of tissue in primary visual cortex (at 3.8°e ccentricity) [cit] while the large stimulus was designed to activate~20 mm 2 of cortex (focal vs. extended sources). the background matched the mean luminance of the bullseye patterns. participants passively viewed a small fixation point at the center of the screen while the stimuli were randomly presented to the left and right visual fields for a duration of 500 ms and at a rate of 800-1,300 ms (slightly randomized to avoid expectation). two hundred individual responses for each of two stimulus conditions were averaged together. visual stimuli were projected onto a backprojection screen using a dlp projector (projection design fx1+) outside the magnetically shielded room. the same projector and notebook computer were flown to each site. stimulus sequences were generated using presentation software (neurobehavioral systems, http:// www.neurobs.com/). eccentricities and visual angle subtended by the stimuli were kept constant across sites by adjusting the stimulus size based on the path lengths measured at each site."
"auditory study in this empirical study, three pure tones of different frequencies (500 hz, 2,000 hz, and 4,000 hz) were presented to obtain a tonotopic map. in addition white noise was also presented intermixed with the tones (focal vs. extended source conditions). white noise contains spectral energy over a wide frequency range in contrast to pure tones, and thus increases the size of the activated cortical patch [cit] ). the cochlea is organized tonotopically and this organization is propagated to primary auditory cortex, where low frequencies are represented rostrally and high frequencies caudally [cit] ). white noise should stimulate extended tissue covering a range of frequencies. the tones and white noise (200 trials for each stimulus) were randomly presented at an average inter-stimulus interval of 1,000 ms. auditory stimuli were generated using presentation software and were presented via a creative labs soundblaster audio card. the sound was delivered to the subject's ear canal using sound transducers connected with plastic tubing to ergonomically designed earplugs. a db attenuator was used to adjust the intensity of the tones."
"citizens use the app or social media to report breeding sites, symptoms and mosquito bites. using this information, tailored health messages are delivered to individuals living in hot spots. predictive surveillance predicts outbreaks using this information, combined with weather and other data."
"the authors characterize differences between depressed and non-depressed individuals, including time of posting, emotion, linguistic style, engagement and ego-network. these are used to create a social media depression index that could be used to predict risk of depression based on social media posts for other users. 1191 responses were collected. while many issues reported were known side effects, some issues were not commonly reported."
"reported that their crowdsourcing exercise identified predictors of obesity that were not found in the literature and that should be explored in future research [cit] . this method to identify predictors for statistical models for both well studied and less commonly researched diseases could be especially beneficial before beginning a study, while deciding which data will be collected."
"all data necessary to reproduce our analyses are located on the web portal available to all interested parties (http:// cobre.mrn.org/megsim/). mri (mprage), segmented mri, cortical surfaces, bem surfaces and ground truth source distributions are available. where appropriate, idl utilities will be supplied to write data to text formats. a free matlab utility exists that permits reading netcdf format into matlab. the web portal has links to download mriview and megan as well as functions that import data from netmeg files into c or idl so that additional simulations can be constructed by others."
"authors describe a challenge by dream and sage, which is a crowdsourcing competition, to develop genetic predictors of response to immunosuppressive therapy in rheumatoid arthritis."
"another very common purpose to which crowdsourcing is used in health is surveillance, both in the context of research and in emergency situations for programming. a number of articles described pilots or projects that employed crowdsourcing for health surveillance [cit], 70% of the world' s population carried a mobile phone [cit], making surveillance through mhealth a promising avenue."
"gamer diagnoses had an accuracy of 99%, sensitivity of 95.1% and specificity of 99.4%. authors suggests that gaming could be a viable option for telepathology."
"authors had two groups, one did not restrict and the other restricted to high-performing turkers. sensitivity was high across both, ranging from 83%-88%, but specificity was poor, ranging between 35%-43%. [cit] diagnosis grading of diabetic retinopathy"
"crowdsourcing has been used to predict survival of cancer, using both laypersons who were not knowledgeable about cancer and experts (though the expert group performed better), which demonstrated that having some knowledge is important when the subject matter is advanced [cit] . however, laypeople have been able to perform to expert-level accuracy in other tasks, such as annotation [cit] . diagnostics is an area where crowdsourcing is especially promising, as shown by the malaria studies. interestingly, the most promising studies in diagnostics employed gamification, which was shown to improve accuracy in an unrelated study [cit] 42] . authors have explored whether crowds of amt workers display similar characteristics to the general population and concluded that they are appropriate for organisational research and for psychological research, as they differed minimally [cit] ."
"high agreement between crowd' s annotations for medication names and types, correction of previous annotations and linking medications with their attributes. the authors found that simple voting provided the best form of aggregation. [cit] general medicine/ other"
"200 snomed ct relationships were evaluated (each by 25 workers). the experts and crowd responses were nearly indistinguishable. errors were identified, which is concerning regarding the biomedical ontologies within snomed ct. [cit] general medicine/ other natural language processing using crowdflower, crowdsourcing medication names, types and linked attributes of clinical trials that were randomly selected from clinicaltrials.gov"
"many studies focused on using crowdsourcing for diagnostics or for surveillance. indeed, it appears that crowdsourcing can be uniquely positioned to improve diagnostics and surveillance of illness. strategies to improve accuracy included employment of machine-learning algorithms [cit], gamification [cit] 42], and establishing thresholds for trustworthiness or questions to weed out malicious workers [cit] ."
"dizeez is a human gene annotation game where players guess which gene causes which disease, out of four options. the game aims to identify gene-disease associations that are known but not present in structured annotation databases. authors took the genes selected to be associated with a particular disease by many players for further investigation. the authors note that, unfortunately, when players suggest potentially novel associations, they are 'punished' by the game. however, the game was able to successfully identify gene-disease associations [cit] . another gene matching game, entrezgene, used amt to match genes from papers and abstracts to their entrezgene identifier. turkers were asked to judge whether the gene is associated with a mutation and 20% of the tasks were controls. in the authors' report, there were problems with the study giving false information to the turkers but after adjusting for this, the turkers achieved 82.3% precision [cit] ."
"the authors explored quality control methods, including repeating experiments on the same hit and aggregating those results, and eliminating any turker whose performance fell below 50% accuracy on control items. when these were implemented, accuracy of 89.9% was achieved for a cost of us$ 0.76"
"for a number of reasons, discussed later, results from the varied inverse procedures have never been tested and compared using a standard testbed of realistic simulated data. it is standard practice in the meg field to create computer-simulated data in order to test a newly developed or revised inverse procedure, using data where the ground truth (i.e., the solution) is known before testing it on empirical data where the ground truth cannot be known. since each of the meg analysis methods is known to have its own set of strengths and weaknesses [for some examples see [cit] ], it would be beneficial to the community to qualify and quantify them. therefore, we are announcing the establishment of a website containing a series of realistic simulated data for testing purposes (http:// cobre.mrn.org/megsim/). if an algorithm provides reasonable solutions to simulations then it is standard practice to use the algorithms in simple sensory empirical data where the literature provides information on the expected locations and time-courses of sources (e.g., non-human primate studies) before attempting analysis of cognitive datasets where it is impossible to know the ground truth. therefore, we acquired simple somatosensory, auditory, and visual sensory data on several participants for this purpose since it is best if the same empirical datasets are shared across the community for comparison. if an algorithm fails to identify the simulated sources and time-courses under realistic conditions (e.g., similar signal-to-noise ratio or snr as empirical data and real artifacts occurring at random intervals), then one cannot expect to obtain correct or reasonable results in empirical data. the rationale and description of the testbed created along with sample simulated cases emphasizing functional connectivity and oscillatory activity are presented here. the single-trial datasets described are suitable for a wide assortment of analyses including independent component analysis (ica), granger causality/directed transfer function, and single-trial analysis."
"curnet and the pcsf application accelerated with curnet are freely available on https://bitbucket.org/curnet/ curnet. figure 1 shows an overview of the full curnet stack, by which r data is passed, as input data, to the gpu environment for parallel computation. the input network is represented, in r, through a standard r data frame, where every edge between two vertices is stored with the corresponding weight. by exploiting the rcpp library of r, an r-c++ wrapper has been developed to automatically translate the network from the standard r representation to a c++ data structure, and to link the algorithm invocation from the r to the c++ environment."
"we evaluated the curnet performance by comparing its execution time with the corresponding sequential implementations provided in the igraph r package (http:// igraph.org/r/). the curnet software requires a gpu device with compute capabilities at least 3.0. we performed tests on two different gpu devices running on a machine equipped with an amd phenom ii x6 (3ghz) host processor, 64 gb ram, ubuntu 14.04 os, and cuda toolkit v 8.0. the first device is an nvidia maxwell geforce gtx 980 gpu having 16 sms (2048 cuda cores) and 8 gb of gddr5 memory, and it is capable of concurrently executing 32,768 threads. the second device is an nvidia tesla k40 comprised of 12 gb of gddr5 memory and 15 sms (2880 cuda cores), and it is able of concurrently executing 30,720 threads the two gpu devices have equal memory technology but they differ in the number of threads that they can concurrently execute and in the internal architecture. the technology of the maxwell architecture is more recent than the tesla one. for these reasons, the first device is expected to show better performances, compared with the second device, in many applications. in what follows, we show the main results we obtained by running tests on the maxwell device, while we run a subset of the benchmarks on the tesla device to show a comparison of performance between the two architectures."
"the method is designed to estimate an unknown matrix, while row and column totals are known. the approach builds on bi-proportional scaling methods, or what is typically called a ras procedure in input-output economics. in this field, the ras approach is routinely applied for matrix balancing. typically, the procedure starts from an initial guess: the seed or prior matrix. in the count-seed ras, the prior matrix is constructed by counting the number of items that simultaneously contribute to a given pair of source-and target-classification in a fully disaggregated mapping between two classifications. in our case, this correspondence table comes from eurostat's metadata centre and matches more than 30 0 0 cpa categories with more than 100 coicop categories. using this as a starting point for the matrix structure, the approach then applies bi-proportional scaling methods until convergence is reached when row and column totals match the given input data."
"this work presents curnet, an r package that provides a wrap of parallel graph algorithms to the r environment. as an initial proof of concept, curnet includes basic data structures for representing graphs, a parallel implementation of breadth-first search (bfs) [cit], single source shortest paths (sssp) [cit], and strongly connected components (scc) [cit] . the package makes available gpu solutions to r end-users in a transparent way, such that gpu modules are invoked by r functions."
"parallel implementation of strongly-connected components for gpu curnet implements a multi-step approach that applies different gpu-accelerated algorithms for scc decomposition [cit] . the algorithm is reported in additional file 1: section 1. the multi-step approach consists of 3 phases. in the first phase it iterates a trimming procedure to identify and delete vertices of g that form trivial sccs (i.e., vertices with no active successors or predecessors). in the second phase it iterates a forward-backward algorithm to identify the main components. the first step is related to the choice of the pivot for each set, where heuristics can be applied to maximize vertices coverage within a single iteration. forward and backward closure is then computed from this vertex, and up to four subgraphs are generated. the first one is the component which the pivot belongs to, and it is calculated as the intersection of the forward and backward closure. the other three sets are scc-closed subgraphs that can be processed in parallel at the next iteration. they correspond to the non-visited vertices in the current set, to the forward closure but not to the backward one, and to the backward-reachable vertices, respectively. in the third phase the approach runs a coloring algorithm to decompose the rest of the graph. a unique color is firstly assigned to each vertex. the max color is then propagated to the successor non-eliminated vertices until no more updates are possible. pivots are chosen as the vertices which color is unchanged. running the backward closure from these vertices on the corresponding set, curnet detects the components labelled with that color."
"this dataset represents bridging matrices between two different data classification systems: consumption by purpose (coicop) and products by activity (cpa). while the former classification is used in household budget and expenditure surveys, the latter represents the industry sector dimension that is typically adopted in national accounts and inputoutput tables. we collect input data from eurostat on total household consumption for 35 coicop and 63 [cit] . based on these data, we construct bridging or concordance tables for 30 european countries using recently developed matrix balancing techniques. the resulting tables enable data conversion between consumption-and production-based statistics, facilitating research that integrates macroeconomics, multi-sectoral international trade and heterogeneous agents in household-level expenditure micro-data. although they are a necessary input in several types of research, they are often constructed on an ad hoc and region-specific basis and not shared publicly. as such, making this dataset available will be useful for computable general equilibrium and input-output models and for carbon footprint and life cycle analyses that incorporate rich consumption micro-data, for instance to shed light on distributional aspects of climate and energy policies. furthermore, by eliminating a barrier raised by differences in statistical classifications, this dataset may foster collaboration between different research teams and may facilitate soft-linking between complementary analytical tools used for policy support."
"curnet has been compared with the bfs, sssp, and scc implementation of the igraph r package (http://igraph. org/r/). tests were run over on annotated undirected protein interaction networks and on directed homology networks provided by the stringdb [cit] ."
"we also created a set of directed unlabelled networks (see additional file 1: figure s6 ) as follows. we used the complete set of 115 archaea species to create homology networks having incremental amount of involved organisms. the homology information between proteins is measured by sequence blast alignments. for each protein, string reports the best blast hits [cit], w.r.t. the given species. horizontal gene transfer is a frequent phenomenon in microbes [cit], and homology networks are used to search for gene families shared by several organisms [cit] ."
"the network representation in the c++ environment relies on the coordinate list (coo) data structure, which is a mandatory step to generate the compressed sparse row (csr) data structure for the gpu computation. csr is a well-known storage format to efficiently represent graphs, and it allows reaching high performance during the graph traversal on the gpu."
"these instances have been flagged accordingly in the supplementary data file (in the worksheets containing the input data, databycoicop and databycpa ). furthermore, input data from coicop and cpa classifications were not consistent for some countries, when the aggregates across categories did not match. inconsistencies are limited and range within ± 3% for all countries. we address this issue by rescaling the values by cpa categories by country to match coicop totals."
"the dataset represents bridging matrices between two classifications: consumption by purpose (coicop) and product by activity (cpa). it covers 30 countries, including all eu member states, united kingdom, norway and serbia. [cit], with proxy years in case of missing numbers in the input data from eurostat. [cit] (or proxy year)."
"the bridging matrices presented here are entirely based on input data from publicly available sources. two types of missing data had to be addressed. [cit] for certain countries. [cit] (bulgaria and ireland) [cit] (malta). the second type of missing data relates to certain categories that are not available in the input data, for instance due to confidentiality. we overcome this lack of data by calculating or imputing values, in some instances splitting the difference between the total and the missing categories according to shares from proxy countries."
"curnet has been developed to be easy to use both as a stand-alone analysis application and as a core primitive to be incorporated in more complex algorithmic frameworks. curnet has been structured to modularly include, as current and future work, a wide collection of algorithms for biological network analysis."
"we used the string dataset [cit], which mainly contains protein-protein interaction (ppi) networks of several organisms, varying from microbes to eukaryotes. we used the r package stringdb to download the data. we refer the reader to additional file 1: section 2 for details on the data."
"the c++ interface allows handling the interaction with the gpu device. it generates the host (cpu) representation of the graph starting from the rows in the data frame, it initializes the gpu kernel, it handles the host (cpu)-device (gpu) data exchanging, and, finally, it runs the kernel for the parallel computation. the computation result is retrieved from the device and passed back to r through the rcpp/c++ layers."
the curnet scc computation results in a vector of associations between vertices and strongly component ids. it is retrieved from the gpu device to r through the rcpp/c++ layer and obtained by invoking the following curnet function:
the views expressed are purely those of the authors and may not in any circumstances be regarded as stating an official position of the european commission.
"the parallel graph traversal through bfs [cit], which is listed and analyzed in additional file 1: section 1 -algorithm 1 and figure s1, respectively, explores the reachable vertices, level-by-level, starting from a source s. curnet implements the concept of frontier [cit] to achieve work efficiency. a frontier holds all and only the vertices visited at each level. the algorithm checks every neighbour of a frontier vertex to see whether it has been already visited. if not, the neighbour is added into a new frontier. curnet implements a frontier propagation step through two data structures, f 1 and f 2 . f 1 represents the actual frontier, which is read by the parallel threads to start the propagation step. f 2 is written by the threads to generate the frontier for the next bfs step. at each step, f 2 is filtered and swapped into f 1 for the next iteration. when a thread visits an already visited neighbour, that neighbour is eliminated from the frontier. when more threads visit the same neighbour in the same propagation step, they generate duplicate vertices in the frontier. curnet implements efficient duplicate detection and correction strategies based on hash tables, advanced strategies for coalesced memory accesses, and warp shuffle instructions. moreover, it implements different strategies to deal with the potential workload imbalance and thread divergence caused by any actual biological network non-homogeneity. these include prefix-sum procedures to efficiently handle frontiers, dynamic virtual warps, dynamic parallelism, multiple cuda kernels, and techniques for coalesced memory accesses."
"curnet allows users to quickly retrieve ncrna-pathway associations and individual genes contributing to them. to evaluate the curnet performance in making highly confident ncrna function predictions, we analysed a case study with the well-known lncrna involved in cancer called malat1. noncoding rnas (ncrnas) are emerging as key molecules in human cancer but only a small number of them has been functionally annotated [cit] . using the guilt-by-association principle is possible to infer functions of lncrnas on a genome-wide scale [cit] . this approach identifies protein coding genes significantly correlated with a given lncrna using gene-expression analysis. in combination with enrichment strategies, it projects functional protein coding gene sets onto mrnas correlated with the lncrna of interest, generating hypotheses for functions and potential regulators of the candidate lncrna. we used a public rna sequencing dataset of 21 prostate cancer cell lines sequenced on the illumina genome analyzer and gaii (geo accession number gse25183) and built up a large-scale gene association network using curnet scc (pearson method as pairwise correlations). we extracted the sub-networks where malat1 is present and calculated single-source shortest paths, mean distance of shortest paths within this subnetwork, and mean distance of shortest paths over the whole big graph. gene set enrichment analysis (gsea) was carried out to identify associated biological processes and signalling pathways [cit] . we computed overlaps of genes in the malat1 sub-networks with gene sets in msigdb c2 cp (canonical pathways) and hallmark gene sets [cit] . several cancer related pathways such as epithelial mesenchymal transition (emt) and dna replication were enriched, which implies that malat1 sub-networks might be involved in the metastasis related pathways [cit] . in addition, we identified an over-representation of gene sets that corresponds to the validated malat1 functionality reported in the literature: cell cycle, e2ftargets, proliferation, b-myb-related, and g2m checkpoint [cit] ."
"we created a benchmark of undirected label networks by using the pvalues of differential expression values regarding the treatment of a549 lung cancer cells by means of resveratrol, a natural phytoestrogen found in red wine and a variety of plants shown to have protective effects against the disease [cit] (see additional file 1: figure s5 ). we used such values to label the above networks."
"we tested curnet bfs on undirected unlabeled networks and sssp on undirected labeled networks related to homo sapiens, danio rerio and zea mais by varying the number of sources ranging from just to few vertices to a 20% of vertices. figures 2 and 3 (see also additional file 1: figures s9 and s10) show the execution time of the bfs and sssp, as well as the corresponding speedup w.r.t. the sequential counterpart. running times were evaluated as an average of 10 runs."
"the result is a double numeric matrix (i.e., distances and predecessors), which are retrieved from the gpu device to r through the rcpp/c++ layer. they are obtained by invoking the curnet functions cur-net_sssp and curnet_sssp_dists for the matrix of shortest paths (returned as lists of predecessor vertices) and the corresponding source-destination distances:"
"the curnet cuda implementation of the sssp algorithm is based on the bellman-ford's approach [cit] . the parallel algorithm is reported in additional file 1: section 1. curnet sssp visits the graph and finds the shortest path d to reach every vertex of v from source s. also in this case, curnet exploits the concept of frontier to deal with the most expensive step of the algorithm (i.e., the relax procedure). at each iteration i, the algorithm extracts, in parallel, the vertices from one frontier and inserts the active neighbours in the second frontier for the next iteration step. each iteration concludes by swapping the contents of the second frontier (which will be the actual frontier at the next iteration) into the first one. indeed, the frontiers allow working only on active vertices, i.e., all and only vertices whose tentative distance has been modified and, thus, that must be considered for the relax procedure at the next iteration."
"the running time to create graph data structures in curnet and igraph from the above datasets is reported in additional file 1: figures s7 and s8 . in general, curnet requires half the time of igraph to perform such a task."
"we retrieved the undirected unlabeled networks related to homo sapiens, danio rerio and zea mais (see additional file 1: figures s2, s3 and s4 for a description of the network characteristics). those species were chosen among the organisms having the largest networks stored in string, to cover the biological diversity that can be encountered in performing analysis of biological networks. for each network, we varied the threshold on the assigned edge scores to obtain sparse as well as dense networks."
"our previous studies [cit] revealed that playing collaborative games during scrum meetings improves participants' communication, commitment, and creativity. in this study, we go one step further and make it easier for agile teams to adopt collaborative games. we developed a web portal (http://153.19.52.168) which provides online versions of 8 collaborative games. in these games, a team or a group of stakeholders participates in a collocated session and plays a game to discover requirements, prioritize requirements, or provide feedback related to the development process or the software system being implemented."
"collaborative games refer to several structured techniques inspired by game play, but designed for the purpose of solving practical problems [cit] . they involve strong visual or tactile elements that help the participants leverage multiple dimensions of communication, resulting in richer, deeper, and more meaningful exchanges of information [cit] . at the same time, they make use of the concepts of teamwork and collaboration, which lead to a variety of measurable societal outcomes."
"the game handles both the positive and negative aspects of the sprint, but also brings forth the continuous improvement [cit] . the game board contains four columns:"
"a meaning map provides the conceptual analog of a saliency map by representing the spatial distribution of semantic features (rather than image features) across a scene. meaning maps therefore provide a basis for directly comparing the influences of meaning and salience on attentional guidance, using the same methods that have been used to test the goodness of fit of predictions from saliency theory 31 . in our first study comparing the roles of meaning and image salience, subjects were engaged in a viewing task in which they freely viewed scenes to prepare for a task (memory or aesthetic judgement) that was presented after the scene was removed from view 31 . it could be that under such conditions, viewers do not guide attention as quickly or in as controlled a manner as they might in an on-line task requiring responses in real time during viewing. here we set out to determine how well meaning and image salience predict attentional guidance in free viewing of scenes when the viewer is actively engaged with and responding to each scene on-line, and attention is directly relevant to the on-line task."
"the great majority of participants state that each evaluated game produces better results than the standard approach ( fig. 3) and is easy to understand and play (fig. 4) . however, as for whole product, mood++, and 4l's the opinions on whether these games should be permanently adopted by the team, are divided almost equally between supporters, opponents and undecided (fig. 5) . the opponents complain that playing a retrospective game is much more time-consuming than running a traditional retrospective. in turn, the final result of whole product was unreadable, because most of the identified features fell into the first category. the great majority also consider that the games foster participants' creativity ( fig. 6 ) and improve communication among participants (fig. 7) . especially, communication between the team and its customer has been improved. all games except 4l's are claimed to foster participants' motivation and involvement with only single opposite voices (fig. 8) . as for 4l's, the opinions are divided equally between supporters, opponents and undecided. when it comes to the impact of the games on the willingness to attend the meeting, the responses are dominated by those who purport that it is difficult to unequivocally determine the impact (fig. 9 ). although these respondents see the value in the games, they are afraid that playing a game at each sprint may be tiring. the key question for this study is whether the online versions outperform the non-digital ones (fig. 10) . although the online versions do not perform worse, only the online version of speedboat, whole product, and mood++ perform significantly better than their non-digital counterparts. as for whole product, its digital game board is considered more apparent than the original one (we changed the original game board [cit] due to implementation difficulties)."
"making corrections (e.g. moving cards/notes between different areas or updating the content) is easier and more flexible in the online versions. thereby, outcomes generated from the online versions are more readable. moreover, the non-digital versions require physical game accessories to play a game. even though most of the games use only simple accessories such as posters, colorful sticky notes and coloring markers, the team encountered situations where there were not enough colors of sticky notes. as for the online versions, there are no problems with missing artifacts."
"in experiment 1, subjects described actions that could take place in the scenes. in experiment 2, they simply described the scenes. in both experiments, the main results were that meaning accounted for more of the overall variance and more of the unique variance in attention compared to image salience. the spatial distribution of meaning and image salience were correlated across the images, but importantly, when this correlation was statistically controlled, meaning accounted for substantial additional variance in attention, whereas salience did not. furthermore, the advantage for meaning started with the very first subject-determined fixation. together these results support cognitive over image salience theories of attentional guidance."
"joining an existing game session is cumbersome. it would be better if there is a drop-down list of all available game sessions that users can join. furthermore, the rules of a game should be accessible when the game is running."
the game helps the team discover new features that can make the product distinct and prioritize the product backlog [cit] . the game board comprises four stairs levels that represent four kinds of features:
"visual saliency theory has provided an influential approach to attentional guidance in real world scenes. the theory proposes that attention is strongly controlled by contrasts in low-level image features such as luminance, color, and edge orientation 9, 10 . in visual saliency theory, saliency maps are generated by pooling contrasts in these semantically uninterpreted features, and attention is then assumed to be captured or \"pulled\" to visually salient scene regions [cit] . in this view, the scene image in a sense prioritizes its own regions for attentional selection. the appeal of image guidance theory is that visual salience is both neurobiologically inspired in that the visual system is known to compute these features, and computationally tractable in that working models have been implemented that can generate saliency from these known neurobiological processes 4 . in contrast to saliency theory, cognitive guidance theory emphasizes the strong role of meaning in directing attention in scenes. in this view, attention is \"pushed\" by the cognitive system to scene regions that are semantically informative and cognitively relevant in the current situation 17 . cognitive guidance theory places explanatory primacy on scene semantics interacting with viewer goals 6, [cit] . for example, in the cognitive relevance model 26, 27, the priority of a scene entity or region for attention is determined by its meaning in the context of the scene and the current goals of the viewer. in this view, meaning determines attentional priority, with image properties used only to generate the set of potential attentional targets but not their priority for selection. attentional priority is then assigned to those potential targets based on knowledge representations generated from scene analysis and memory. the visual stimulus remains important in that it is used to parse the scene into a map of potential targets, and processes related to image differences (i.e., those captured by saliency models) likely play a role in which targets are encoded in the map. but the critical hypothesis is that this parse generates a flat (that is, unranked) landscape of potential attentional targets and not a landscape ranked by salience. knowledge representations then provide the attentional priority ranking for attentional targets based on their meaning 3, 26, 27 . to investigate and directly compare the influences of image salience and meaning on attention, it is necessary to represent both in a manner that can generate comparable quantitative predictions. contrasting predictions can then be tested against attention. implemented computational saliency models output a spatial map of saliency values over a scene, providing such quantitative predictions 11, 12, [cit] . how can we similarly generate a spatial distribution of meaning over a scene for comparison to saliency? unfortunately, it is far more difficult to create a computational model of meaning than it is to create a computational model of image salience, a likely reason that saliency models have been so popular 4 . to address this issue, we have recently developed a meaning map representation of the spatial distribution of meaning across scenes 31 . the key idea of a meaning map is that it captures the spatial distribution of semantic content in the same format that a saliency map captures the spatial distribution of image salience. to create meaning maps, we used crowd-sourced responses given by naïve subjects who rated the meaningfulness of a large number of context-free scene patches. photographs of real-world scenes were divided into dense arrays of objectively defined circular overlapping patches at two spatial scales. these patches were then presented to raters on mechanical turk independently of the scenes from which they were taken. the raters indicated how meaningful they judged each patch to be. we constructed smoothed maps for each scene by interpolating these ratings over a large number of patches and raters for each scene. like image salience, meaning is spatially distributed non-uniformly across scenes, with some scene regions relatively rich in semantic content and others relatively sparse (see fig. 1 )."
"the visual world presents us with far more information than we can process at any given moment. intelligent analysis of a scene therefore requires selection of relevant scene regions for prioritized processing. how does the brain select those aspects of the world that should receive priority for visual and cognitive analysis? a key mechanism of selection in natural vision is overt visual attention via saccadic eye movements [cit] . what we see, comprehend, and remember about the visual world is a direct result of this prioritization and selection. a central topic in visual cognition, therefore, is understanding how overt attention is guided through scenes."
"how is attention guided through real-world scenes? the concept of image salience, typically represented by computationally derived saliency maps, has provided an important theoretical framework for thinking about the guidance of attention in scenes [cit] . at the same time, it is well established that people are highly sensitive to the semantic content of the visual world they perceive 5, 6, suggesting that attention is directed by the semantic content of a scene rather than by visually salient image features 26, 27 . until recently it has been difficult to directly contrast image salience and meaning so that their relative influences on attention can be compared. to address this challenge, we developed a method for identifying and representing the spatial distribution of meaning in real world scene images 31 . the resulting meaning maps quantify the spatial distribution of semantic content across scenes in the same format that saliency maps quantify the spatial distribution of image salience. meaning maps therefore provide a method for directly comparing influences of the distributions of meaning and image salience. in the present study, we used meaning maps to test the relative importance of meaning and salience by assessing theoretically motivated meaning maps and saliency maps against observed attention maps generated from the eye movements of viewers during two scene description experiments."
"in this study we investigated the relative importance of meaning and image salience on the guidance of attention when viewers engage in on-line free-viewing tasks involving real-world scene description. to assess meaning, we used meaning maps, a recently developed method for measuring and representing the distribution of semantic content over scene images 31 . we assessed the relative influences of meaning and image salience in two language production tasks, one in which subjects simply described the scenes, and a second in which they described scene functions. we found that the spatial distribution of meaning was better able than image salience to account for the guidance of attention in both tasks. the pattern of results is consistent with cognitive control theories of scene viewing in which attentional priority is assigned to scene regions based on semantic information value rather than image salience. in addition to the important theoretical implications of these findings for understanding attention, the present results also have important implications for current artificial intelligence and machine learning approaches to detecting and labeling real-world images. these systems often use image salience to locate regions likely to be attended by people, and the present results suggest that such an approach may be limited."
"as an example of how cognitive guidance might work, the cognitive relevance model 26, 27 proposes that a representation of scene entities is generated based on an initial image parse, with attentional priority assigned to entities (e.g., perceptual objects) based on their meaning and relevance in the context of the current task. in this view, contrasts in image features are not used to assign attentional priority, but instead serve as the basis of the scene parse. it is important to note that cognitive guidance does not require that meaning be assigned simultaneously to all perceptually mapped objects across the entire scene. that is, cognitive guidance does not require a strong late-selection view of scene perception. indeed, there is significant evidence against semantic analysis of objects in the visual periphery of scenes [cit] . instead, when a scene is initially encountered, its gist can be quickly apprehended [cit] . apprehending the gist allows access to schema representations that provide constraints on what objects are likely to be present and where those objects are likely to be located within the scene 3, 7, and can guide attention at the very earliest points of scene viewing 7, [cit] . information retrieved from memory schemas can be combined with low-quality visual information from the periphery to assign tentative meaning to perceptual objects and other scene regions. these initial representations provide a rich set of priors and can be used to generate predictions for guiding attention to regions that have not yet been identified 4, 30 . in addition, many saccades during scene viewing are relatively short in amplitude, such that attention is frequently guided from one location to the next using parafoveal and perifoveal information relatively close to the fovea where at least partial identity and meaning can be determined."
"it has sometimes been proposed that attention might initially be guided by image salience, but that as the scene is viewed over time, meaning begins to play a greater role in attentional guidance 7, [cit] . to test this hypothesis, we conducted time-step analyses. linear correlation and semi-partial correlations were generated based on a series of attention maps, with each map representing sequential eye fixations (i.e., 1st, 2nd, 3rd fixation) in each scene. we then compared these time-step attention maps to the meaning and saliency maps to test whether the relative importance of meaning and salience in predicting attention changed over time."
"subjects. thirty university of california, davis, undergraduate students with the characteristics listed in experiment 1 participated in this experiment. none of the subjects participated in experiment 1. an initial group of 38 subjects was recruited, and data from an a priori target number of 30 subjects who met the inclusion criteria were included in the analyses. excluded subjects failed to calibrate (one), did not follow instructions to speak out loud (one), or could not be accurately eye-tracked (six)."
"subjects. university of california, davis, undergraduate students with normal or corrected-to-normal vision participated in the experiment. all subjects were naive concerning the purposes of the experiment and provided informed consent. an initial group of 32 subjects was recruited, and data from an a priori target number of 30 subjects were included in the analyses. the target of 30 was chosen by examining in several independent data sets the number of subjects needed to establish stable attention maps. the two excluded subjects could not be accurately eye-tracked."
"the game is a strategic planning technique used to help an organization identify the strengths, weaknesses, opportunities, and threats related to a project. strengths and weaknesses are internal to the business, while opportunities and threats arise externally. this game can be also employed to discover requirements for a software system [cit] ."
"the game explicitly asks customer representatives to say what they do not like about the product. nonetheless, it lets the facilitator stay in control of how the complaints are stated. the game starts by drawing a speedboat. the speedboat represents the software system. everyone wants the speedboat to move fast. unfortunately, the speedboat has a few anchors holding it back [cit] . customer representatives write what they do not like on sticky notes and place them under the speedboat as anchors. the lower an anchor is placed, the more significant the issue is. customer representatives may also add engines to the speedboat. the engines represent features that can \"overpower\" the anchors and enable the speedboat to move faster [cit] ."
"the first decision to be made was the selection of collaborative games to be implemented. our main objective when developing the portal was to offer at least one game for each scrum meeting except the daily scrum, which is too short and too well-structured to take advantage of collaborative games. since there are several games that may be utilized during each scrum meeting, we chose those that had received the most positive feedback in our previous studies and were easy to implement. ultimately, our portal provides 8 collaborative games. the assignment of the games to the scrum meeting in which the game is applicable is as follows:"
"given the dominance of meaning over image salience we have observed previously 31 and in the present study, how do we account for prior results suggesting that image salience can account for attention? one answer is that in some earlier studies, meaningless images (e.g., fractals) were used as stimuli. in these cases, the only meaning to be found in the image may be feature contrasts. that is, salience may be meaningful when no other type of meaning is available. in the case of studies supporting an effect of image salience in meaningful real-world scenes, it is likely that salience accounts for attention to some degree because of its strong correlation with meaning 31, 57 . as we have shown here, shared variance between meaning and salience was about .70 overall and .45 when the down-weighted map peripheries were removed from the analysis. given this relationship, the only way to unambiguously demonstrate an influence of salience over meaning is to de-correlate them. and as we have shown, when this is done statistically, there is little evidence for an influence of image salience independent of meaning both in the present task and in scene memorization and aesthetic judgement tasks 31, 58 ."
"the type of scene meaning we have investigated here is based on ratings of scene patches that were shown to raters independently of their scenes and independently of any task or goal other than the rating itself. these experiments therefore investigated the role of what we call scene-intrinsic context-free meaning on attention. we can contrast this with other types of meaning based on other types of cognitive representations. as one example, we might consider scene-intrinsic contextualized meaning, in which the degree of meaning associated with a scene region is based on its relationship to the meaning of the scene as a whole. for instance, an octopus might be meaningful floating in an undersea scene, but it might be even more meaningful in a farmyard scene or floating in the sky 7, 40, 42, [cit] . note that in all cases, a scene parse will lead to a representation of an octopus object, but its meaning value may differ substantially. similarly, we might consider task-related and goal-related contextualized meaning, in which the meaning of a scene region is based on its relationship to the viewer's task and goals rather than intrinsic to the scene itself. for example, during a tennis serve, the bounce point of the ball might not be especially intrinsically meaningful, but might become meaningful right after the ball leaves the server's racket given the goal of determining where the ball is going to end up 1, 2, 4, 25, 55, 56 . how cognitive representations that encode these types of meaning interact with each other to guide attention is currently unknown. the meaning map approach provides a basis for pursuing these questions, and the present results suggest that such a pursuit would be worthwhile. note that given these considerations, the present study using context-free meaning potentially employed the least potent version of meaning maps, and we may therefore be underestimating the degree to which meaning predicts attention."
we conducted a focus group with the team to analyze and discuss the results presented in the previous subsection. the discussion was structured around four questions:
"real-world scenes contain an enormous amount of information, but human vision and visual cognition are capacity-limited: only a small amount of the available information can be processed at any given moment. efficient visual analysis therefore requires properly selecting the information that is most relevant given the current needs of the viewer."
"in summary, the goal of this study was to test current theoretical approaches to attentional guidance in real-world scenes across two free-viewing experiments using two scene description tasks. we applied a recently developed method, meaning maps, to generate the spatial distribution of semantic content across the scenes, and used a popular computational model to generate image salience. we then tested cognitive and image guidance theories by comparing the relative success of meaning maps and saliency maps to predict the distribution of attention as assessed by eye fixations in the scene description tasks."
"each subject rated 300 random patches extracted from the scenes. subjects were instructed to assess the meaningfulness of each patch based on how informative or recognizable they thought it was. subjects were first given examples of two low-meaning and two high-meaning scene patches to make sure they understood the rating task, and then rated the meaningfulness of scene patches on a 6-point likert scale ('very low','low','somewhat low','somewhat high','high','very high'). patches were presented in random order and without scene context, so ratings were based on context-free judgments. each unique patch was rated three times by three independent raters for a total of 48960 ratings. due to the high degree of overlap across patches, each fine-scale patch contained rating information from 27 independent raters and each coarse-scale patch contained rating information from 63 independent raters. meaning maps were generated from the ratings by averaging, smoothing, and then combining scale maps from the corresponding patch ratings. the ratings for each pixel at each scale in each scene were averaged, producing an average fine-scale and coarse-scale rating map for each scene. the average rating maps were then smoothed using thin-plate spline interpolation (matlab'fit' using the'thinplateinterp' method). finally, the smoothed maps were combined using a simple average. this procedure resulted in a meaning map for each scene. the final maps were blurred using a gaussian kernel followed by a multiplicative center bias operation that down-weighted the scores in the periphery to account for the central fixation bias commonly observed in scene viewing in which subjects concentrate their fixations more centrally and rarely fixate the outside border of a scene 14, 27, 59 . image-based saliency maps were computed for each scene using the graph-based visual saliency (gbvs) toolbox with default settings 15 . gbvs is a prominent saliency model that combines maps of neurobiologically inspired low-level image features. a center bias operation is intrinsic to the gbvs saliency maps."
"the game is a way of choosing the right set of features to be developed in the next sprint. in this game, customer representatives collaborate to purchase their most desired features using game money (fig. 1) . strictly speaking, they jointly prioritize their desires as a group [cit] . each features has a price related to its development cost. some features may be priced so high that no single player can buy them individually. this motivates negotiations among players because they have to pool their money to buy the feature."
"in experiment 1, subjects were asked to describe a set of actions that could be performed in the currently viewed scene. each scene was presented for 30 s, and both eye movements and speech were recorded throughout viewing."
"the game aims at identifying \"needed\" and \"desired\" features of the system to be developed. the final result should be a mind map demonstrating the size of the project [cit] . unfortunately, due to implementation difficulties our version of this game only allows for categorizing features without the possibility of creating a mind map."
"procedure. the procedure was the same as in experiment 1, with the exception that the instructions were changed such that subjects were instructed simply to describe each scene: \"in this experiment, you will see a series of scenes. you will have 30 seconds to describe the scene out loud."
"procedure. each trial began with a five-point array of fixation crosses used to check calibration. the subject then fixated the center cross until the experimenter pressed a key to begin the trial. the scene was then presented for 30 s while eye movements and speech were recorded. a unique pseudo-random scene order was generated for each subject with the restriction that two scenes of the same category (e.g., kitchen) did not occur consecutively. subjects were instructed to describe a set of actions that could be performed in the scene in the following way: \"in this experiment, you will see a series of scenes. in each scene, think of the average person. describe what the average person would be inclined to do in the scene. you will have 30 seconds to respond. \" after the 30 second response window terminated, subjects proceeded to the next trial by pressing any button on a response box. prior to the experimental trials, subjects completed three practice trials to familiarize themselves with the task and the duration of the response period. a calibration procedure was performed at the start of each session to map eye position to screen coordinates. successful calibration required an average error of less than 0.49° and a maximum error of less than 0.99°. fixations and saccades were segmented with eyelink's standard algorithm using velocity and acceleration thresholds (30°/s and 9500°/s 2 ). eye movement data were imported offline into matlab using the edfconverter tool. the first fixation, determined by the fixation cross and always located at the center of the display, was eliminated from analysis."
"meaning and saliency maps were normalized to a common scale using image histogram matching, with the fixation map for each scene serving as the reference image for the corresponding meaning and saliency maps. histogram matching of the meaning and saliency maps was accomplished using the matlab function 'imhistmatch' in the image processing toolbox. further details about map generation can be found in previous work 31 ."
"we report the results of two experiments using two different free-viewing scene description tasks. most theories of language production assume that production is at least moderately incremental 32, 33, so that speakers dynamically interleave planning and speaking rather than preparing the entire description in advance of any articulation. because production is incremental, small phrasal-sized units of speech are planned in response to fixations made to different scene regions. scene description therefore allows us to assess the influence of image salience versus meaning in a continuous, on-line task in which attention to specific scene regions is functional and necessary. in experiment 1, subjects were asked to describe what someone might do in each scene. in experiment 2, subjects were simply asked to describe each scene. in both experiments, subjects were instructed to begin their description as soon as each scene appeared and to continue speaking for 30 seconds. eye movements over the scenes were tracked throughout the trial."
"lemma. the sum rate c in what follows, we show that the strict hierarchy for channel capacities according to the resources in network coding, namely"
"in this work, we establish a bell scenario as a framework of non-local network coding that applies nonsignaling correlations beyond shared randomness to the preparation of codewords for reliable network communication. we in particular consider two-input and twooutput interference channels, to show that non-signaling (quantum) correlations are in general more useful for higher channel capacities than quantum (local) correlations. it is also shown that more non-local correlations do not necessarily imply to a higher channel capacity, i.e., the non-locality is not a general resource that enhances a network protocol. the framework is also useful to construct non-local polytopes containing the set of quantum correlations. our results can be generally applied to other network channels when non-local resources are available in network coding."
"the sum capacity with local network coding c (l) (n ) can be obtained by exploring all local deterministic vertices characterized by v 1 in eq. (11) . applying all local reversible relabelings, there are 256 local deterministic vertices. the details are shown in appendix. the sum capacity is found as follows,"
"and π a 2 x 2 . this means two parties prepare codewords by sharing quantum states and measurements on them. then, non-local network coding is referred when two senders have access to all non-signaling probabilities in the encoding,"
this shows that the no-signaling conditions are not fulfilled in an incompatible network channel. correlations of any kind can be generated between the parties communicating via the interference channel.
"note that the simple quantum protocol is derived by considering an initial probability distribution which gives maximum bell-chsh violation (i.e., tirelson's point for chsh scenario), followed by the stated deterministic mappings applied to input and output bits from this distribution to generate inputs to the channel. we tried other approach like choosing an initial probability distribution for which: (i) quantum boundary points giving maximal violation of tilted-chsh inequalities [cit], and (ii) points on the tirelson-landau-masanes (tlm) boundary of the quantum set in chsh scenario [cit] . it turns out that though for some values of the channel parameters p and q the lower bound can be slightly improved, the lower bounds from the protocol that we consider is sufficient to show a classicalquantum gap for a wide range of channel parameters p and q. moreover, regions where the current protocol fails to show a classical-quantum gap, the other approach like (i) and (ii) fail to reveal any classical quantum gap. many other numerical tests performed with various other quantum points gives similar results."
"finally for obtaining an upper bound on quantum capacity, similar to the method described for computing the no-signaling capacity, we maximize the sum-rate over all points in v p out, i.e., over all the vertices of the polytope p out . first, with the help of mathemat-ica, we compute the sum-rate c (q ub ) e over the set of all vertices v p out of the polytope p out, these are basically different functions of p and q. then by noticing that many of the vertices give same sum-rate functions, we delete all the duplicates to reduce to very few number of distinct functions. since for finding an expression for capacity we need to take maximum over all these functions, we filtered and eliminated all the functions which are always less than or equal to a smaller subset of the set of distinct functions. we find that such smallest subset contains only four different functions of p and q."
"a multipartite bell scenario presents a natural network framework that maps input bits to outputs bits. inputs are chosen by the parties randomly and independently and a bell scenario may generate correlations among outcomes, classified into local and non-local ones. the parties with shared randomness only are compatible with local correlations but non-local ones."
network coding in a two-input and two-output interference channel works as follows. let a 1 and a 2 denote random variables of two senders and b 1 and by a mapping e :
"a network generates correlations. the parties that aim to establish a communication protocol via a network have to deal with interventions due to the correlations. network coding presents a framework to devise codewords for reliable communication in a network via cooperation of the parties [cit] . messages are chosen by senders, mapped to codewords by network coding, and then transmitted to receivers through a network channel."
"to compare the non-locality and the channel capacity, let us consider three vertices denoted by v, v 1 4 are compared. the non-locality is measured in the first row, and the sum rate in the second. the vertex v 1 correlation is the least useful for network coding, and also that the least non-local one is the most useful for a network communication. this shows that non-local correlations are not a general resource that enhances a network protocol."
"the sets of local (l), quantum (q), and non-signaling (n s) probabilities are convex and strict hierarchical l q n s. that is, non-signaling (quantum) correlations contain quantum (local) ones."
"to this end, we are going to exploit the cglmp scenario. the convex geometry of the sets l, q, and n s has been analyzed for the cglmp scenario [cit] . the local polytope is identified by the convex hull of all local deterministic points, which are also finite. it suffices to explore the finite vertices to compute the sum capacity. the non-signaling polytope is given by the convex hull of all local and nonlocal points. note that the nonlocal properties of the vertices are invariant under local reversible relabeling of inputs and outputs of individual parties in a bell scenario [cit], see also appendix. the set of vertices up to local reversible relabelings gives the full characterization of the non-signaling polytope."
"the maximization in eq. (2) runs over encoding schemes e with an available resource r. the sum rate c (r) e (n ) has been defined in eq. (3), where the mutual information with an encoding scheme e is denoted by i e . note that a decoding does not increase the mutual information since it would correspond to a mapping between sets of alphabets of an equal size. w.l.o.g., it suffices to optimize an encoding e to find the sum capacity."
"we in particular consider a class of two-sender and two-receiver interference channels, where fig. 1 . the channels are characterized by joint probabilities with two parameters as follows,"
"in conclusion, we have established a framework of network coding with a bell scenario. local, quantum, and non-local network coding schemes are characterized accordingly. we have constructed equivalent bell inequalities by exploiting the technique of local reversible relabelings to solve the optimization in network coding. on the technical side, our method can be used to construct non-local polytopes that contain the quantum set."
where h denotes the binary entropy. the sum capacity with non-local network coding c (n s) (n ) can be obtained by applying all reversible local relabelings to the three sets of the non-local verticies in eq. (12) . the non-signaling polytope
"for deriving the no-signaling capacity, we first generate the sets of nonlocal vertices v1 2, v1 3, and v1 4 by applying all reversible local relabelings to the three respective nonlocal vertex given by eqs. (20), (21), (22) . the total number of vertices in the three sets v1 2, v1 3, and v1 4 are respectively 10368, 110592, and 82944 . thus we could get the set of all no-signaling vertices"
"this shows that the sum rate are hierarchical according to the resources in network coding e. this in fact leads to a simplification in the computation of the sum capacity: consequently, it suffices to consider all extreme encodings in the optimization in eq. (3)."
"the quantum set of correlations q is a convex set but not a polytope which makes computation of quantum capacity a hard problem. in fact the exact boundary of the set q is not known: it has infinitely many and unknown extreme points. therefore with our current knowledge, in general, one can derive only some lower and upper bounds on quantum capacity by approximating the quantum set from inside and outside with the help of some simple well defined geometries, for instance some polytopes. in what follows, we adopt this approach for deriving suitable lower and upper bounds on quantum capacities."
"it is shown that non-local resources are strictly more useful than quantum resources, which are strictly more useful than local resources. we have also shown that the non-locality is not a general resource for enhancing network communications. more non-locality does not necessarily leads to a higher rate in network communication."
". together with an encoding scheme e to an interference channel n, the transmission from two senders (a 1, a 2 ) to two outputs (y 1, y 2 ) is characterized by a joint probability in the following,"
"our results shed a new light to understand network information theory. the results find that non-local correlations are generally useful in network communication. the framework we presented here with a bell scenario can be applied to other network channels in general. to compute a channel capacity over the quantum set, it is asked to develop an efficient method of constructing non-local polytopes for optimizations in network coding. we leave it an open question to seek theoretical tools for the purpose. in general, network coding can be found by characterizing constrained non-local polytopes in the probability space. finally, our work paves a way to develop multipartite bell scenarios for network communication. in future investigations, it would be interesting to apply multipartite bell scenarios to multiple-input multiple-output (mimo) network channels. we assume a bell scenario of two parties where each one applies n inputs and have m outputs. bell inequalities correspond to hyperplanes in the probability space such that all local probabilities are manifested. bell inequalities that can be related with each other by local reversible relabelings are equivalent. this means that a number of equivalent bell inequalities can be generated by a relabeling technique. moreover, as a consequence, any point in the probability space induces an equivalence class of a set of points such that any point in the set can be transformed to any other point of that set by a suitable local reversible relabeling."
"where m is the total sample number of one class, k is the number of categories, and c(i, i) is the current classified sample by the algorithm."
"in the first part of ng-apc module, the convolution depth is n, atrous pyramid convolution depth isn − 1. formula (5) gives the desirable maximum value of atrous rate under non-gridding conditions, which can be reduced according to the actual situation."
"the goal of this algorithm is to obtain the material properties of a single-pixel for the identification of different kinds of ground objects in hyperspectral images. in the three public available hyperspectral datasets [cit], the indian pines datasets and salinas datasets target the farmland of different crops and the natural topography. they are continuous large areas of the same category, and the classification effect will be better by using the method of spatial spectrum combination. since our algorithm aims at small target recognition of different features, we select the pavia university dataset to verify the effectiveness of the algorithm. this dataset is one of the common scenes in the city, which is in line with the goal of this algorithm."
"the simulated power line noise allowed us to measure and compare the effectiveness of the different approaches to remove non-stationary power line noise (with fluctuating amplitude and with abrupt on-/offsets), since it was added to meg data practically free of line noise, which therefore represented the \"ground truth\" that is otherwise missing in experimental datasets. spectrum interpolation effectively removed the simulated power line noise (both for abrupt on-/offsets and fluctuating line noise), resulting in a smaller nrmse (compared to the line-noise-free meg dataset) than the signal processed with cleanline, which showed residual power line noise artifacts that was clearly visible in single trials and in the erf. for the meg data that incorporated simulated line noise with fluctuating amplitude, spectrum interpolation in addition outperformed the dft filter, which was not able to remove line noise interference completely."
"the erfs reveal that the few extreme outlier trials (containing onand offsets of line noise) of the dft filtered signal corrupted the averaged erf (fig. 3a) . the same applies to the erf after the application of cleanline, though restricted to a time window of approximately 200 ms, revealing an artificially enhanced erf component. spectrum interpolation and the notch filter show the most faithful reproduction of the original erf (fig. 3a) ."
"in this paper, combining these two methods of solving the gridding problem and introducing inception model, we propose a non-gridding multi-level concatenated atrous pyramid convolution module (ng-apc). ng-apc module is divided into two parts. the first part is atrous pyramid convolution, while the second part is divided into several branches. each branch uses a different atrous rate convolution and then merges together, which can achieve the maximum reception field of the nongridding atrous convolution."
"init is the estimated precision matrix after the initialization step of mixmpln. in mixmpln þ glasso(iterative tuning parameter), we used the same formulation to initialize the q value, but then updated it in each iteration based on the new estimated precision matrix in that iteration."
"the diabetes dataset is pre-processed using hybrid genetic algorithms so as to eradicate the inconsistent data from the dataset. as temporal fuzzy rule based classifier is built by the generalization of fuzzy rough sets, it's done by the lower and higher approximation operators of similarity relations; the number of human involvement is being decreased and also the newly designed classifier is simple as compared to alternative classifiers. within the rule primarily based classifier, the allen's temporal algebra is used to induce fuzzy if -then rules. in the rule based classifier, the allen's temporal algebra is utilized to induce fuzzy if -then rules. from the induced rules forecasting is performed to confirm whether the patient is suffering from diabetes or not and to diagnosis the severity of the disease. the analysis of temporal data in medicine is done using the various modules described above. it has begun with the formation of temporal clinical databases and the efforts to use representations of medical facts for evidential reasoning, which is vital, for developing unswerving medical diagnoses. there is also possibility of using medical models to prophesy a patient's physical condition or devise medical treatment."
"in the fourier domain, fluctuations in the amplitude of line noise interference effectively translates to spreading of the noise component to neighboring frequencies, thereby escaping the dft filter."
"limitations of the current study are related to the application of the four methods to the eeg datasets, since the \"ground truth\" of a clean signal without power line noise was unknown here. hence the rmse compared to a noise free signal cannot be computed and the evaluation of the performance of the different approaches is limited to the visualization of the erps and the computation of the rmse is restricted to a relative comparison of the notch-and dft-filtered signal to the spectruminterpolated signal. the quantification of the superior performance of spectrum interpolation over the dft filter was only possible by applying it to artificially simulated power line noise added to line-noise-free meg data."
"the code for the spectrum interpolation approach introduced here has been made available in fieldtrip, an open source meg/eeg analysis toolbox (http://fieldtriptoolbox.org; [cit] ) ."
"one of the possible causes for ringing artifacts is the gibbs effect [cit], which can be especially pronounced when there are sharp discontinuities in the filter's frequency response [cit] . this is especially the case with notch filters, due to the sharp and abbreviations: mrl, middle latency response; sws, slow wave sleep; nrmse, normalized root mean squared error.narrow stopband. spectrum interpolation results in a smoother fourier spectrum, which has been shown to reduce the gibbs phenomenon [cit] ."
we can solve this equation using matrix derivative rules to compute an estimate for the covariance matrix r l in iteration ðt þ 1þ as:
"the data was segmented into epochs including the flash stimulus, with a prestimulus and a poststimulus time period of 500 ms, resulting in 40 non-overlapping trials of 1 s length. a baseline correction was performed by demeaning single trials according to the prestimulus period of à200 ms to 0 ms."
"to construct the temporal decision fuzzy table for classification, data pre-processing is first performed. we have taken diabetic dataset which is defined with 17 attributes [cit] . we propose a genetic algorithm based algorithm for attribute subset selection in order to build a better temporal fuzzy approximation space to remove the obsolete data from the dataset. in order to build temporal rule based classifier using generalization of temporal fuzzy rough set approach with the set of knowledge base is build from clinical temporal databases. the parameters such as threshold α and noise percentage β are taken into consideration in turn for building an efficient rule based classifier to classify the records effectively. the fuzzification process is performed on the crisp values obtained from the previous module. discernibility vector is designed to describe the consistency degree between the two clinical records. the rules (patterns) are generated by transforming the rule based classifier to fuzzy inference system using the reduced temporal fuzzy decision table which consists of attribute values at a particular time interval have been interpreted as patterns, stated as temporal rules by allen's temporal algebra to define relations between the temporal intervals. from the rules induced prediction process is performed to conclude the patient's condition pertaining to the diabetes disease. it asserts whether the patient been diagnosed with suffering from diabetes or not, if so to analyze the severity of the disease and plan the medical treatment for the disease."
"spectrum interpolation was repeated for the respective harmonics up to 500 hz for the eeg data, since the erp analysis incorporated a band signal width up to 500 hz for the sws study (see above: analysis of eeg sws data)."
"statistical-based classification methods first convert image information into discrete digital matrices and then use strict mathematical derivation algorithms to distinguish different features. examples of such methods are support vector machine (svm) [cit], pca-based classification method [cit], and classification methods based on sparse matrix [cit] . statistical-based classification methods usually require the processed data to meet certain conditions, such as conforming to normal distribution or supporting normalization. thus, the high requirement of the digital matrices and the previous data processing will inevitably reduce the accuracy of classification."
"where w t il is calculated using equation (10). considering the part of the l function that is dependent on k and h, we have the following term to maximize:"
we used a coordinate ascent approach in conjunction with a block update strategy to optimize l. we present the details of parameter initialization and subsequent iterative updates below.
"where ϑ is the residuated implicator, r describes a fuzzy similarity relation and a t (u) corresponds to activation function. the t start and t end defines the starting time and the ending time of the temporal concept. the lower approximation operation using generalization of fuzzy rough sets is"
"the rest of this paper is organized as follows. in section 2, we introduce the atrous convolution. section 3 briefly introduces gridding and non-gridding problems. section 4 gives the algorithm of ng-apc module. in section 5, we experimentally compare the performance of our method with svm and some other cnn-based algorithms. section 6 draws some conclusions."
"in this paper, we compare ng-apc model with the classical svm, the new 1d-cnn [cit] 2d-cnn [cit], 3d-cnn algorithms [cit], and rnn [cit] . we refer some of these algorithms by nshaud/deephyperx on github. we set training sample 80%, test sample 20%, and keep some default parameter settings, such as patch size is 7 and epoch is 50."
"similar problems occur in clinical applications of the eeg, including monitoring of sleep disorders and epilepsy, which typically occur in unshielded environments subject to electrical noise from both medical equipment and ordinary infrastructure. intracranial eeg, often employed prior to neurosurgical resection for epilepsy or brain tumors, can also be subject to line noise."
"gridding and non-gridding sketch map is shown in fig. 2 . it can be seen from fig. 2a, after three-layer atrous convolution where atrous rate is set to 3, the reception field can be expanded to 12. but the effective connection point is 5, and the adjacent points of the red pixels are not connected."
"in this case, the efficiency of the receptive field (erf) is 5/12, and the gridding problem is obvious. since the convolution kernel has holes, after several superimposed atrous convolutions, there will be a problem that the features of the data in the receptive field are incomplete. hence, it is also the essential cause of the gridding problem. for the gridding problem, the researchers have proposed two solutions:"
"a widely used notch filter in this context is the butterworth filter. this type of filter is an infinite impulse response filter (iir), causing a delay in the time domain and a nonlinear phase response that can be corrected (zero-phase delay) by reverse filtering, also referred to as two-pass filtering [cit] . in consequence, signal components in the time domain as, e.g., event-related potentials might be shifted back in time after non-causal low-pass filtering, which hinders the correct estimation of onset latencies or peak amplitudes [cit] . low-pass filtering can also cause artificial smearing of oscillations to preceding samples, which might lead to misinterpretations of phase or connectivity effects, as, e.g., spurious prestimulus phase effects [cit] or it might cause detrimental effects on granger causality [cit] ."
"in the field of image semantic segmentation, chen proposed the deeplab model [cit], which introduced the atrous convolution. [cit], chen proposed the atrous spatial pyramid pooling [cit], which was applied to the up-sampling part of the encode-decode architecture to expand the convolution receptive field to acquire long-distance features effectively. it performed good in rgb image semantic segmentation. the core idea is to expand the receptive field of different scales by changing the size of the atrous while maintaining the size of the convolution kernel and to quickly obtain a larger range of feature maps."
"input: diabetic dataset which consists of patients' medical records. output: reduced dataset with condition attributes. begin 1. load the patients' dataset from the database that fits in the memory. 2. identify the temporal attributes from the datasets. 3. the dataset is converted to relational database. 4. chromosomes are selected using roulette wheel method and rank it using fitness value. 5. eliminate the redundant attributes by applying the hybrid genetic algorithms operations such as crossover and mutation. 6. selection of attributes is done to measure the consistence degree between the two attributes, those are called as condition attributes. end"
"in table 2, we compare the classification results of the proposed ng-apc with other classification algorithms (svm, 1d cnn, 2d cnn, 3d cnn, and rnn). ng-apc module belongs to 1d cnn. the classification results of ng-apc algorithm are better than the 1d and 2d cnn classification methods in gravel, meadows, and bitumen feature classification. even though ng-apc algorithm has less accuracy than 3d cnn [cit] classification results in bitumen and bricks features, it has much higher than the average level (0.821 and 0.900). table 3 shows confusion matrices for the pavia university dataset. it can be seen from the detailed classification accuracy of all the classes, which is calculated from one arbitrary train/test partition. the cell of ith row and jth column means the probability that the ith class samples is classified as the jthclass. the percentages on diagonal line are just the classification accuracy of corresponding class. the proposed algorithm performs under 95% in only two classes (bitumen and bricks) among the nine classes. these two class samples are wrongly classified as asphalt with 3.38% and 3.26% separately. as shown in the table 1, the two classes are the ones with smaller numbers of samples. the more similar two classes of spectral domain, the higher probability they are wrongly classified to each other."
"there are several approaches to remove artifacts stemming from power line noise, with different strengths and weaknesses. the most prevalent method might be to filter the data in the respective frequency band with a notch filter (a bandstop filter with a narrow stopband). this attenuates the power for frequencies in the respective stopband (centered at either 50 or 60 hz). however, filtering comes with the risk of causing distortions in the passband and in the resulting time domain signal, producing artifacts like ringing [cit] ."
"to quantify the robustness of the methods against non-stationary line noise interference, power line noise with fluctuating amplitude and with abrupt on-and offsets was simulated and added to an meg dataset that was practically free of line noise."
"real data: we also applied our mixture modeling framework to a sample-taxa count matrix produced by a recent microbiome study [cit] that explored connections between gut microbiome composition and the risk of plasmodium falciparum infection. in this study, stool samples from a cohort composed of 195 malian adults and children were collected and analyzed. the samples were assayed by sequencing the 16s ribosomal rna gene to determine the bacterial communities they contained. this generated a sample-taxa count matrix with 195 samples and 221 bacterial genera which we analyzed in this paper."
"in order to simulate line noise interference with fluctuating amplitude, gaussian white noise with a length of 109.2 s and a sampling rate of 1017.2 hz (parameters matched to the real meg dataset), was filtered with a 0.4 hz low-pass filter (fir filter, zero-phase forward and reverse filter). low-pass filtering included mirror-padding the data to a length of 250 s, to avoid filter edge artifacts. the maximum of the absolute values of the lowpass-filtered white noise signal was added to the signal to ensure only positive values for the amplitude modulation. the resulting filtered white noise signal was multiplied with a 50 hz sinusoid (zero phase) to create an amplitude-modulated line noise component. the simulated line noise signal was added to the power line-noise-free meg dataset (described above) such that the resulting signal had a root mean square (rms) amplitude approximately twice as high as the original meg signal, yielding a signal-to-noise ratio (snr) of à6.0 db."
"here, a sliding window of 4 s and a 1 s step size (eeglab default) was used for the meg dataset mixed with fluctuating line noise and a sliding window of 0.3 s and a 0.1 step size was applied for the abrupt on-/offsets of line noise. a sliding window of 1 s and a 0.1 s step size was used for the eeg dataset, because this improved the performance. a taper bandwidth of 4 hz was used for the meg data with fluctuating amplitude (centered around 50 hz), a bandwidth of 24 hz was used for the abrupt on-/offsets and a taper bandwidth of 6 hz was used for the eeg data, to achieve comparable results to the other methods (see bandwidths for the other methods above). default values from the eeglab implementation were used for all other parameters."
"data pre-processing is generally performed to find out if there is much extraneous and redundant information present on the dataset. this includes various steps such as cleaning normalization, transformation, feature extraction and selection, etc. the outcome of data pre-processing is the ultimate data set with reduced attributes. feature selection is the technique of selecting a subset of relevant features for constructing best learning models. genetic algorithms are used for feature selection. genetic algorithm is a search algorithm which provides a way for solving problems by imitating the same process that mother nature uses. we use the concept of hybrid genetic algorithm to determine the reduced attributes. they use the same combination of selection, crossover and mutation to develop a solution to a problem."
"in fig. 3, n is the number of convolution layers, and the default value is 3, which can be modified. the number of branches of the module is i, and r i n is atrous rate. according to formulas (5) and (6), r i n is not greater than r i n max, and f i n is reception field, which can obtain the global farthest distance feature. figure 2d is an example of ng-apc module, which has three convolution layers. the first layer atrous rate is 1, the second layer atrous rate is 3, and the third layer has two branches. the atrous rate of branch 1 is 9 and reception field is 27. the atrous rate of branch 2 is 18 and reception field is 45. the holes in branch 2 can be filled by branch 1, so the whole is non-gridding atrous pyramid."
"likewise, spectrum interpolation performed considerably better than the dft filter and cleanline with respect to the eeg sleep data, that was subject to large line noise artifacts. the results of the application of the dft filter and cleanline to the eeg data revealed that residual line noise artifacts might also be clearly evident in the erp. the developers of cleanline have already demonstrated the respective difficulties this regression based method might encounter with large non-stationary spectral artifacts [cit] ."
"from the discernibility vector, the core values are defined to find the reducts. the reducts in combination with the association rule mining are used to define the frequent temporal patterns. from these temporal patterns the rules are defined in accordance to the reduct values. the reducts are the reduced attribute values, and then the covering degree [cit] of the rule is developed in consideration with the time slots, if the reduct values has the highest covering degree value then those rules are considered as reduced ruleset. in view that the fuzzy if then rules induced from the preceding part using the allen's temporal algebra the patient's insulin levels are delineated. based on these values forecasting is done. the defuzzification process is done to convert the fuzzy values to crisp values. by constructing a rule based classifier the prediction process is performed to conclude the patient's condition pertaining to the diabetes disease. based upon allen's temporal interval algebra, the temporal patterns are written as temporal rules for predicting the severity of the disease."
"layer n of ng-apc module is the second part of model. it is a convolution layer composed of multiple branches. the number of branches can be set, according to the requirement of receptive field in practical application. the rate of each branch is according to formula (6):"
"the notch filter and spectrum interpolation substantially reduced line noise, as evident from the frequency spectra ( fig. 4b and c) . cleanline was not able to attenuate this non-stationary line noise interference, and introduced seemingly inappropriate components corresponding to integer frequencies (fig. 4d) . the frequency spectra also reveal how the signal is slightly changed in the passband frequencies after notch filtering, while it is preserved with spectrum interpolation (fig. 4a, b and c) ."
"the original eeg signal showed high amounts of power line noise artifacts for the 50 hz component and the respective harmonics, which is visualized in the power spectrum of an example sws segment (fig. 6a ). in addition, the power line noise appears to be non-stationary with amplitude fluctuations. this amplitude modulation of the 50 hz line noise is mirrored by the broadness of the 50 hz component, which is not confined to 50 hz but extends to the neighboring frequencies of 47 hz and up to 53 hz. the frequency spectra demonstrate that spectrum interpolation and notch filtering attenuated power in the 50 hz frequency and in the harmonics up to 500 hz to a large extent as intended (fig. 6b and c) . but applying the cleanline method results in substantial residual energy in those frequency ranges and additional artificial frequency components (fig. 6d) ."
"we have worked and tested our experiments with lower approximation operations alone. in future, upper approximations can be used to build the rule based classifier with effective decisions, which can reduce the number of rule sets when compared with the lower approximations. this helps to build efficient rule based classifier with small number of rule sets."
"in the dataset, when we randomly select the similar labeled pixels in different spatial locations, the features of shadows and painted metal are more consistent. however, the spectral features of bitumen, gravel, etc., are quite different. during the experiment, we randomly divide the labeled samples into training sets and test sets according to the ratio of 80% and 20%. the unlabeled sample pixels are background pixels. actually, there should be different material labels. because they are not labeled, they are not used for training. otherwise, it will affect the accuracy of the identification classification. figure 5 shows the false color composite image and ground truth with different land cover classes."
"however, the application of a notch filter might cause unintended a) the erfs of the original noisy eeg signal (black), after the application of the notch filter (red, second panel), the dft filter (green, third panel), cleanline (light blue, fourth panel) and for the spectrum interpolated signal (blue, all panels) reveal residual line noise interference after the application of the dft filter and cleanline. b) the boxplot of the rmse relative to the spectrum interpolated data reveals that over all single trials cleanline shows the lowest performance, followed by the dft filter. the notch filter (butterworth) and spectrum interpolation show the best performance, with almost no differences. the magnified view of the boxplot is shown on the right."
"such signal distortions can be evaluated by examining the time and frequency domain responses of the filter to artificial signals like an impulse or step signal. an ideal frequency domain response would have unity passband and zero stop magnitude. a real-world filter necessarily deviates from this ideal, with potential ripples in the passband and stopband as well as a certain transition bandwidth. the impulse and step responses reveal filter behavior in the time domain, visualizing the amplitude and extent of distortions."
"the mm principle is a general technique [cit] that has proven to be useful for tackling function optimization problems (mm stands for minorizationmaximization in maximization problems and for majorizationminimization in minimization problems). for our scenario, let hðhþ denote a function to be maximized. the mm principle proposes to maximize the minorizer function gðhjh t þ instead of maximizing hðhþ directly; here, h t denotes a fixed value of the parameter h. the function gðhjh t þ is said to be a minorizer of hðhþ if:"
"among the variety of artifacts affecting the quality or signal to noise ratio of meg and eeg (meg/eeg) measurements, the 50 or 60 hz interference from power lines is probably the most pervasive one. shielded rooms can help reduce the influence of power line interference on meg/eeg recordings, but often not completely. furthermore, the mobility of eeg systems makes them a good candidate for \"field\" or mobile measurements [cit], where shielding is usually impractical and proximity to everyday electrical appliances is more likely."
"in this paper, we consider the following computational inference problem: for a sample-taxa matrix x (containing absolute counts of taxa) that is generated by a mixture model consisting of k mpln component distributions, estimate the mixing coefficients and the distribution parameters of the k components. we note that the precision matrices of the k components define the k different microbial networks. we formulate this inference problem using a maximum likelihood framework, and estimate the various parameters of the constructed likelihood function directly using a numerical optimization method based on the minorization-maximization (mm) principle [cit] . we extend this formulation based on an l 1 -penalty model and provide algorithms to infer k sparse networks. we evaluate the performance of our algorithms using both synthetic and real datasets. we also evaluate the performance of our method on compositional data obtained by subsampling from the true counts of the taxa. this evaluation models the real-world scenario, where the sample-taxa matrices that we have access to, contain only relative abundances of the observed taxa."
"the comparison of the single trials after line noise removal reveals substantial residual 50 hz noise for the dft-filtered signal and the signal after applying cleanline, but only small deviations from the original signal for the spectrum-interpolated and notch-filtered signal (see fig. 5b for an example trial). this is visualized by the difference curves between the filtered (dft filter, notch filter, cleanline or spectrum interpolation) signal and the original noise free meg signal for the example trial (fig. 5c) ."
"the study of microbial communities has been greatly enabled with the advent of high-throughput next-generation dna sequencing technologies [cit] . the taxonomic composition of a microbial community can be obtained by sequencing the dna extracted from a biological sample that has been collected from the environment of interest. this is achieved either using a targeted approach, involving the sequencing of a taxonomic marker gene [for instance, the 16s ribosomal rna gene, which is found in all bacteria [cit] ] or using a whole-genome shotgun sequencing approach [cit] . both approaches generate taxa counts that are compositional in nature, and that enable the estimation of the relative abundances of the constituent members of the community."
"power line noise is a pervasive artifact in meg and eeg data that cannot always be satisfactorily reduced by shielding the recording environment. typical filter approaches to remove this type of artifact come at the risk of introducing ringing or other signal distortions that can lead to spurious effects or may not be effective for longer data segments. in addition, some of these approaches, such as the dft filter and the cleanline method, are not capable of removing non-stationary line noise interference of high magnitude. here we demonstrate how spectrum interpolation can be used as an alternative approach to also effectively reduce non-stationary power line noise in meg and eeg data, while undesired signal distortions in the time domain are reduced compared to the butterworth notch and the dft filter."
"the gaussian-shaped signal served to test the potentially detrimental effects of line noise removal on the analysis of erp/erf signals. again, it revealed the strongest signal distortions for the notch filter, followed by the dft filter. cleanline, which again almost did not affect the signal, showed the best performance with almost no deviations from the original test signal. aside from cleanline, spectrum interpolation performed considerably better than the other approaches, with only slight signal distortions in the time domain, visible as very small artificial oscillations near the line noise frequency. fig. 6 . example segment for eeg sleep data with massive power line noise due to acquisition in unshielded settings. a) the power spectra (psd, log scale) of the original eeg signal (dark blue) and b) after the application of spectrum interpolation (blue), c) a notch filter (red) and d) the regression based method cleanline (light blue) reveal that spectrum interpolation and the notch filter attenuate power line noise to a sufficient extent. cleanline does not attenuate the line noise interference sufficiently while even adding artificial components in the frequency domain. all magnified views of the foi are shown on the right side."
"since hsi is different from an rgb image, the classical dcnn model cannot be directly applied to hsi classification. indeed, such a classification raises three main problems: a) multiple spectral features. hsi has hundreds of dimensions of spectral features. if directly input dcnn model, the kernels of each layer should have several times the number of dimensions of the original, which causes the number of parameters to increase exponentially. this phenomenon is called dimension disaster. moreover, a further dimensional reduction reduces accuracy, such as svm [cit] . b) small spatial size of hyperspectra dataset. usually, the hsi dataset has little spatial information, even only a picture of tens of thousands of pixels. this could be not enough for a dcnn model to achieve effectively classification by extracting spatial features, and it is easy to cause overfitting. therefore, the common hyperspectral cnn model [cit] has only two convolution layers; thus, it is difficult to learn the combination features over a long distance. c) the number of parameters has to be kept as small as possible to achieve fast computational speed. this is necessary because in the future, it envisioned the possibility to run a dcnn model on mobile devices and provide real-time feedback. for example, real-time land information can be provided to farmers through his."
"in this paper, we propose a 15-layer dcnn model with ng-apc module to classify single-pixel of hsi. this model has solved three problems in hsi classification. first, the single-pixel classification of hsi learns the whole spectral information of each pixel, which not only solves the problem of large computational complexity of high-dimensional data, but also solves the problem of insufficient samples in dcnn training. second, aiming at the reasonable combination of 1d atrous convolution, we propose ng-apc module, which solves the gridding problem and enlarges the receptive field from 7 to 45. moreover, the classification accuracy is improved by learning the long-distance feature combination. the oa reaches 98% and the kappa reaches 0.974 on the pavia university dataset, whose are superior to many kinds of 1d, 2d, and 3d cnn models. third, replacing the 1d convolution with the full connection layer, although the model is a 15-layer dcnn, the parameters are similar to those of the same type of 1d-cnn of five layers, which meets the requirements of small parameter models. in conclusion, ng-apc model is an excellent dcnn model for hsi classification."
"the main issue concentrated in this paper is that the lower approximation operation -threshold values have been given more significance in order to intensify the temporal rule based classifier, thus obtaining firm rule set and from those rule set, the rules are induced by means of the allen's temporal algebra and prediction is performed to achieve the desired results."
"the objective of this study was to evaluate and compare the performance of the spectrum interpolation approach to a notch filter, a dft filter and the cleanline method, three widely used methods to remove power line interference in eeg and meg data. the three methods were applied to (i) synthetic test signals, as an impulse, step and a gaussianshaped signal, (ii) abrupt on-and offsets of power line noise added to line-noise-free real meg data, (iii) simulated power line noise data exhibiting non-stationarities in amplitude and frequency, that was added to line-noise-free real meg data and (iv) unshielded eeg measurements in a typical home environment, presumably containing non-stationary power line noise components."
"hyperspectral remote sensing is a novel technology that can simultaneously acquire spectral and spatial information at the nanometer scale while maintaining the advantages of the previous wide-band remote sensing technology. moreover, hyperspectral remote sensing can cover tens or even hundreds of continuous narrow-band spectrums formed from chromatic dispersion, such as ultraviolet, visible, nearinfrared, and far-infrared bands. therefore, hyperspectral images (hsi) are often used for fine classification of features [cit], such as distinguishing different types of crops or ground materials [cit] . in recent years, the spatial resolution of hyperspectral image sensors has been greatly improved. using aviris sensors, the 20 [cit] has been improved by the 3.7-m resolution on the salinas dataset. then, the reflective optics spectrographic imaging system (rosis-03) has allowed to further improve to a 1.3-m resolution on the pavia university dataset. with such an improvement and an abundance of spectral features, the number of mixed pixels has been significantly reduced. hence, the material properties of a single-pixel have become clearer, enabling to make qualitative detection out of small targets more feasible."
"kappa is used to calculate the similarity between the classification results of ground and the real distribution of ground objects with disperse analysis method, the formula is as follows (8):"
microbial networks are typically constructed from sample-taxa count matrices. a sample-taxa count matrix is generated by sequencing multiple biological samples (n samples) collected from the environment of interest and identifying the counts of the observed taxa (d taxa) in each sample.
"atrous convolution, also known as dilated convolution, combines different cavity-sized atrous convolutions called pyramid dilated convolutions. atrous convolution is to inject holes into the standard convolution map to expand the reception field without increasing the parameter amount and convolution depth. moreover, it obtains larger scale feature information. 1d atrous convolution is defined as:"
"synthetic test signals. in order to evaluate the impulse and step response of the different filter approaches and the distortions in the time domain, the mean root mean squared error (rmse) was calculated between the filtered signal (time domain response) and the synthetic test signals, leaving out time periods with an amplitude of one (time point zero for the impulse and in addition all time points for the step signal that were larger than zero). this was done to evaluate time domain distortions with respect to the signal period that had an amplitude of zero before line noise removal. in order to evaluate the effects of the different approaches on a gaussian-shaped test signal, the mean rmse was calculated between the processed signal (after line noise removal) and the original gaussian-shaped signal."
"synthetic sample-taxa count matrices were generated in order to assess the performance of the various mixmpln algorithms. we evaluated the convergence properties of the algorithms as a function of increasing sample sizes. since, in practice, sample-taxa count matrices generated from biological samples are compositional in nature, we also evaluated the effect of sampling from the true (or absolute) counts, and the subsequent application of data normalization, on network recovery and convergence. in addition, we also evaluated the accuracy in recovering sparse networks. finally, we applied our mixture model framework to analyze a real dataset."
"to solve the above three problems, in this paper, we combine the atrous pyramid convolution to obtain hsi's spectral features more effectively by solving the gridding problem in the atrous convolution. we acquire the feature maps of the spectral information in all dimensions and replace the fully connected layer with two convolutional layers. it significantly reduces the number of parameters and provides as output the material label of a single hyperspectral pixel."
"an alternative approach to remove power line noise -termed spectrum interpolation -has been developed to remove line noise from the electromyogram [cit] ), but has not yet been applied to eeg or meg data. this method is based on the simple concept of: (i) transforming the time domain signal into the frequency domain via a discrete fourier transform (dft), (ii) removing the line noise component in the amplitude spectrum by interpolating the curve at interference frequency according to neighboring frequencies, and (iii) transforming the data back into the time domain via an inverse discrete fourier transform (idft). the undesired signal distortions in the time and frequency domain mentioned above may be reduced by the application of spectrum interpolation."
"microbial interactions can be modeled using a weighted graph (or network), where each node in the graph represents a taxon (or taxonomic group) and an (undirected) edge exists between two nodes if the corresponding taxa interact with, or influence, each other. the edge weight captures the strength of the interaction, with its sign reflecting whether the interaction is positive or negative. this framework can be used to model a variety of microbial interactions, including competition and co-operation. while we do not consider it here, a directed graph can also be used to represent interactions, where the edge direction indicates the direction of influence (or causality)."
"the properties of a filter are by convention tested by a sharp impulse and step signal. the unit impulse signal had a length of 2 s, ranging from à1 to 1 s, with a sampling rate of 500 hz and consisted of zeros with a single element of unit magnitude (value of one) at time point zero, constituting a single sharp pulse (delta function). the same length and sampling rate applies to the step signal created, but the signal consists of zeros for negative time points and of ones for time point zero and the following positive time points."
"here we demonstrate that spectrum interpolation might be a preferable alternative to the dft filter and the cleanline method in case the power line noise exhibits severe non-stationarities, as e.g., large fluctuations in amplitude over time. in this case, it is as effective as a butterworth notch filter in removing the artifact, but likely introduces less distortion in the time domain."
"for the investigation of the effects of filtering non-stationary power line noise with fluctuating amplitude, simulated line noise (50 hz) with an amplitude modulation was added to the original meg data (free of line noise). the power spectrum of the mixed signal shows a strong 50 hz component, leaking into neighboring frequencies (48-52 hz) due to the amplitude modulation (fig. 4a) . hence this was the target frequency range to be removed by all approaches. the amplitude of the simulated line noise changed on a fast time scale, randomly between 0 and 0.4 hz and the rms amplitude of the simulated line noise signal was scaled to be a factor of 2 of the rms amplitude of the original meg signal. the parameters were chosen in a way to simulate a severe case of non-stationary line noise interference, but with a magnitude comparable to what we have observed \"in the field\" as with our sleep eeg data."
"as we discuss briefly below, microbial networks can be constructed using a variety of different approaches. to our knowledge, all of these methods assume that the sample-taxa matrix is associated with a single underlying stochastic process (i.e. there is one underlying network topology and set of edge weights). however, this need not always be the case. in this paper, we consider an important extension to the network inference problem, whereby we develop a mixture modeling framework for inferring k microbial networks when the observed sample-taxa matrix is associated with k underlying distributions. we are motivated by large-scale humanassociated and other environmental microbial community projects that are now possible due to cost-effective sequencing. for instance, human gut microbiome studies now routinely analyze large cohorts of individuals and generate microbial community data from several hundreds (to even thousands) of samples. an important research question in this area involves the definition of a 'core' microbiome associated with a particular host phenotype [cit] . it is well understood that the gut microbiome composition is greatly influenced by many factors including diet and age, and thus it is not unreasonable to expect the associated microbial network interactions to also be different when these factors vary (e.g. the gut microbial community interaction network in vegetarian hosts can be expected to be different compared to the network in non-vegetarian hosts). a similar situation also occurs in environmental studies where the microbial interactions are influenced greatly by the physical and chemical gradients of the environment. often the collected metadata in these studies may not be comprehensive enough to discern these interactions in a supervised manner. our proposed mixture framework offers a principled approach to identifying these multiple microbial interaction networks from a sample-taxa matrix."
"the reason for the failure of the dft filter is the non-stationary nature of the power line noise, fluctuating in amplitude (at 50 hz in the current study). this amplitude modulation of the 50 hz sinusoid resulted in a 50 hz component with a width of 4 hz in the frequency domain, spreading the artifact into the neighboring frequencies."
"in mixmpln þ huge(stars), we used the stability approach to regularization selection (stars) method [cit] . this method selects the coefficient which results in the most edge stability in the final graph) [cit] . the tuning parameter selection method in mixmpln þ huge(fixed tuning parameter) and mixmpln þ huge(fixed tuning parameter) was the same as the corresponding implementations using glasso."
"meg data: visual evoked fields. the meg dataset was recorded for a study investigating visual perception and information processing of light flashes. full-field white flashes were presented to the right eye, while the left eye was occluded. the flashes had a duration of 16.67 ms with an isi of 2.41 s. the experimental task required no behavioral response. brain activity was recorded via meg with a 148-magnetometer whole head meg (magnes 2500 wh, 4d neuroimaging, san diego, usa) in a magnetically shielded room. meg was recorded with a sampling rate of 1017.2 hz and a 0.1 hz high pass hardware filter was applied prior to digitization. a sample dataset with a length of 109.2 s of one participant (27 years old, male) was chosen for the current investigation of power line noise artifact removal methods. global noise, incorporating power line noise, was removed by subtracting external noise recorded via 11 meg reference channels. these reference channels were multiplied with individually calculated fixed weight factors, before their signal was subtracted from meg data. hence the meg dataset exhibited no discernible power line noise at 50 hz and its respective harmonics ( fig. 2a) ."
"in table 4, we compare five metrics (i.e., oa, kappa, the parameters quantities, memory usage, and the runtime) with the other approaches. compared with the 1d cnn models, whose parameters and runtime are equivalent, the ng-apc model gets better results on oa and kappa. oa increases from 84.619% and 92.778% to 97.966%, while kappa increases from 0.792 and 0.904 to 0.973. when comparing with the best 3d cnn [cit], whose number of parameters and memory are 5.8 times and 4.68 times larger than ng-apc model. at the same time, oa of ng-apc increases 2.54% and kappa of ng-apc increases 0.032, respectively. therefore, the ng-apc model is the best one in those convolution models. it not only has high accuracy classification results, but also has smaller parameters and faster runtime."
suyun zhao et. al [cit] put forward a rule-based classifier which is constructed through generalization of fuzzy rough set (frs) by adhering to a new notion called as consistence degree it is employed as the decisive value to retain the discernibility information similarly as it is in the procedure of rule induction. the main concern is that the strictures such as threshold value and noise percentage in gfrs are not elucidated in depth.
"the performance of the notch filter (butterworth) was comparable to the spectrum interpolation approach and removed power line noise equally well in the meg and eeg datasets. there was almost no difference in rmse between both approaches, except for the simulated amplitude modulated line noise and the abrupt on-/offsets of line noise added to meg data. in both cases spectrum interpolation had slightly less deviations from the original signal, but this difference was very small and not as clear as the comparison to the other approaches."
"meg data with simulated line noise. the original meg signal was free of discernible line noise, and therefore represents the \"ground truth\". the normalized rmse (nrmse) between the signal after line noise removal and the original meg signal was calculated for each single trial and then averaged. in order to normalize the results, the single trial rmses were divided by the difference of the maximum and the minimum of the respective original single trials (line-noise-free) they were compared to."
"where r is atrous rate, d is the number of layers in the serial convolution, max is the maximum value that can be obtained at non-gridding, f is receptive field, and the calculation of the first layer of convolution f can be gotten in formula (2) . in this paper, we only discuss and analyze the case of 1d convolution and of 3 kernels. in summary, both of these two methods can improve the efficiency of convolution, expand the reception, and reduce the gridding problem."
"we set the experiment parameters as follows: learning rate is 0.9, the activation function is relu, batch is 100, and the optimization strategy uses sgd. the model train and test are on a single nvidia gtx 980 4 gb gpus and with 8 g memory."
"in order to visualize the performance of the different filter methods removing line noise interference, auditory erps (in this case the mlr component) were calculated for the sws periods of one dataset of the eeg sleep study. fig. 3 . meg signal with added simulated abrupt on-and offsets of line noise interference. a) erf (here the mlr) after the application of spectrum interpolation (blue, first panel), the notch filter (red, second panel), the dft filter (green, third panel), cleanline (light blue, fourth panel) and for the original noise-free meg signal (black, all panels). there is residual line noise for the dft filter and cleanline. b) single trial difference curves (relative to the original meg signal) after the application of all four methods reveal signal distortions in the time domain with different magnitude. cleanline shows the largest deviation from the original signal during time segments of line noise and around the abrupt onset, while c) the notch filter and spectrum interpolation show larger deviations during line-noise-free time segments (magnified view of the first trial). d) the boxplot of the nrmse relative to the original data reveals that over all single trials of the mlr component cleanline shows the largest signal distortion, followed by the notch filter (butterworth), spectrum interpolation and the dft filter. the magnified view of the boxplot is shown on the right."
"this way simultaneously sound impulses were send to the earphones and to the trigger recording box. recording sessions lasted between 5.5 and 8.5 h. for the present analysis, the dataset of one participant (29 years old, male) was used to compare the performance of the different approaches. the signal from electrode tp10 was selected for the respective analysis."
"we use the function on the right-hand side of equation (9) as the minorizer function for our problem. thus, we will define the new objective function (l) to be maximized as follows:"
"as shown in fig. 1, each small square represents a pixel. the red pixel is the center pixel of the convolution; the blue pixels are associated with the center pixel and covered by the atrous convolution. that is to say, the feature of blue pixels can be sampled at that location. the gray ones are the holes from which the convolution cannot learn the feature of the location. through one layer of atrous convolution, it is possible to associate far apart features, that is, to obtain a combined feature of a longer distance dimension. the 1d reception field equation is as follows:"
gaussian waveforms are suitable test signals to investigate the effects of filtering on erp/erf results [cit] . the gaussian-shaped test signal had a length of 500 ms with a sampling rate of 500 hz. the signal was a probability density function of a normal distribution with a standard deviation of 5 ms.
"if power line noise interference is considered as an additional component -with a peak at 50 hz -superimposed on the continuous power spectrum curve of meg/eeg data, it can be removed by interpolating the curve of the power spectrum at the respective frequency samples [cit] . if this interpolation is calculated in complex fourier space, the inverse fourier transformation can be used to retain the time domain signal in which the line noise is now reduced."
"microbes are found almost everywhere on earth, including in environments deemed too extreme for other life forms, and they play critical roles in many biogeochemical processes [cit] . microbial communities are also found in association with higher life forms, including plants and animals; for instance, trillions of microbes live in or on the human body (almost as many human cells as there are in the body) [cit] and ongoing research continues to reveal the important roles that many of these microbes play in human health [cit] . microbial communities are typically structured and composed of members of different species. the microbes in a community do not exist in isolation, but interact with each other and also compete for the available carbon and energy sources. these interactions, along with resource availability and environmental parameters (like temperature, ph and salinity) [cit], determine the taxonomic composition of the microbial community and the abundance levels of its constituents. knowledge of these interactions is crucial for understanding the overall behavior of the microbial community, and can be used to elucidate the biological mechanisms underlying microbe-associated disease progression and microbemediated processes (like biofilm formation)."
"therefore, the first step in our mm approach is to find a minorizer function which has the required property. for this, we use the following observation that follows from the concavity property of the log function [cit]"
"simulated power line noise with abrupt on-/offsets and with fluctuating amplitude. in order to create abrupt on-and offsets of power line noise, a 50 hz sinusoid (zero phase) was multiplied with a unity-height rectangular pulse with a width of 4.93 s centered at a signal with a length of 16.4740 s. six of these rectangular pulse signals were concatenated, multiplied with the 50 hz sinusoid and added to line-noise-free meg data of 109.2 s length, such that the resulting line noise segments had a root mean square (rms) amplitude approximately four times as high as the original line-noise-free meg data and a width of 4.93 s. the simulated six abrupt on-and offsets of 50 hz line noise were centered at the epochs that were used later for single trial and erf analysis in order to reveal possibly detrimental filter effects."
"the frequency spectra reveal that the application of spectrum interpolation and the notch filter resulted in a reduction of line noise interference (fig. 2c, d and e), whereas cleanline was not able to reduce line noise to a satisfying extent [cit] for demonstrating this issue with large non-stationary power line noise). fig. 3b shows three example trials for single trial difference curves between the filtered signals and the original clean signal, with a linenoise-free trial, a trial with an abrupt onset of line noise at the center and a trial with line noise over the whole time period. the single trial difference curves reveal that all methods lead to slight signal distortions even in time periods that were free of line noise before filtering except for cleanline, showing no visible distortions ( fig. 3b and c, first sample trial). especially notch filtering leads to artificial oscillations (around 50 hz) in these time ranges, but also spectrum interpolation shows these distortions to a slightly lesser extent ( fig. 3c and d) ."
"we implemented the mixmpln algorithms in r, and assessed their performance using the synthetic datasets. for our assessments, we generated sample-taxa count matrices x (and their corresponding xs and ts matrices) for the following four sets of parameters:"
"eeg sws data. the mean rmse between the original, notch-or dftfiltered single trials and the spectrum-interpolated single trials was calculated. results were not normalized since the ground truth (linenoise-free signal) is unknown."
"there are three ways to get the same receptive field, if we use the standard convolution. first, using the big kernel of 7, the parameters are also expanded by 7/3 times; second, using the four-layer convolution structure kernel of 3, the parameters are expanded by 4 times; third, using two-layer convolution kernel of 3, the connection is expanded by max pooling or average pooling in the middle, and the parameter quantity is expanded by 2 times. however, the boundary is blurred, details are lost, and errors are introduced when pooling. it can be seen that the atrous convolution has great advantages in expanding the receptive field. at the same time, it also exists as the problem of not learning all features, which is called gridding problems."
"prior to describing our mixture modeling framework, we describe a single mpln distribution. in our discussions, we denote matrices using upper-case letters, column vectors using bold letters (upper-or lower-case) and scalar values using normal lower-case letters."
"in order to calculate auditory erps, these two long data segments were bandpass-filtered with a butterworth filter (filter order of 4) between 15 and 500 hz for the so-called middle latency response (mlr) component of auditory brain responses [cit] . then the data was segmented into epochs, with a pre-and poststimulus period of 500 ms relative to the auditory stimulus, resulting in 4197 trials of 1 s length."
"the impulse and step responses of a filter are practical methods to evaluate filter behavior and distortions that might occur when broadband signals are filtered [cit] . the impulse response revealed the largest undesired distortions in the time domain for the butterworth notch filter, indicated by artificial oscillations near the notch frequency. likewise, the dft filter also resulted in signal distortions of the same nature, but smaller in magnitude. the cleanline method showed no deviation from the original signal and did not affect the signal at all, since it incorporates a statistical test to identify significant line noise interference, which was not present in the impulse signal. aside from cleanline, spectrum interpolation showed the best performance with neglectable deviations from the impulse signal. the step response on the other hand revealed signal distortions in the time domain for both the notch filter and spectrum interpolation. here cleanline and the dft filter performed better than the other approaches, with no or almost no deviations."
"by visualizing erps of real eeg data, we were able to demonstrate cases where the application of cleanline and the dft filter clearly fails to remove power line noise, while spectrum interpolation is capable of removing this artifact and performs as well as a notch filter. at the same time the synthetic test signals revealed that undesired filter distortions as they occur with a notch filter are reduced with spectrum interpolation, considering the impulse signal and the gaussian-shaped test signal. since the gaussian-shaped test signal mimics the erp/erf components occurring in almost all neurophysiological signals it is especially notable that spectrum interpolation performs better than the butterworth notch filter.. a further advantage of spectrum interpolation over notch filtering is that it does not affect frequencies in the passband, preserving spectral energy of frequencies outside the stopband."
"since the dft filter is realized by fitting a sine and cosine (here at 50 hz) at the respective interference frequency to each data segment, the width of the frequencies removed cannot be controlled directly, but only indirectly via the width of the time window of the trial, which in turn determines the frequency resolution of the dft. in addition the estimated amplitude of the power line noise is a mean across time. this is only feasible if the amplitude of the power line noise is stable over time. if the amplitude fluctuates, the subtraction is not removing the artifact but can even add artifacts in this frequency range. even though the dft filter was applied in addition to the neighboring frequencies (48-52 hz) to account for the amplitude modulation and respective width of the line noise component, the dft filter was not able to attenuate power line noise to a sufficient extent. the failure of the dft filter to remove a non-stationary power line noise artifact has already been described and therefore advised to be used with caution by the developers of fieldtrip (http:// www.fieldtriptoolbox.org/faq/why_is_there_a_residual_50hz_line-noise_ component_after_applying_a_dft_filter)."
"over the time course there were six sudden on-and offsets of power line noise (width of 4.93 s), centered at the respective epochs, with an rms amplitude four times as high as the original meg signal (fig. 2b), which mimics an extreme case of power line noise pulses. the power spectrum of the mixed signal reveals that the abrupt on-and offsets of line noise resulted in a pronounced 50 hz component incorporating a wide frequency range between 38 and 62 hz (fig. 2b) . spectrum interpolation was used to reduce line noise interference by replacing values for all fourier coefficients between 38 and 62 hz by the mean of the neighboring frequencies (26-38 hz and 62-74 hz) in the amplitude spectrum (see method section and fig. 2c for the resulting power spectrum). the same frequency range (38-62 hz) was used for the two other approaches, as the notch filter and the cleanline method (see method section), except for the dft filter. here fitting and removing a single 50 hz sinusoid from each trial resulted in better performance than the removal of all integer values between 38 and 62 hz."
"an alternate approach to constructing a microbial network, and that which we adopt in this paper, is to model the vector of observed taxa counts (in samples) using a multivariate distribution and to infer the parameters of this distribution from the observed data using a maximum likelihood framework. any candidate multivariate distribution for this approach will have to be flexible enough to capture the underlying covariance structure to model the network interactions (i.e. allow for both positive and negative covariances); this rules out distributions like the multinomial or the dirichletmultinomial, which are popular choices for modeling microbial count data in certain situations [cit], but which cannot capture both types of interactions. the multivariate poisson log-normal (mpln) distribution [cit] can be used for modeling multivariate count data and its covariance structure can capture both positive and negative interactions. this distribution was used recently [cit] to model counts in a sample-taxa matrix and infer an underlying microbial network using an assumption of sparsity."
"in this context, wireless and dry electrode eeg systems facilitate measurements in natural environments outside the lab, which is also especially desirable in the context of brain-computer interfacing [cit] . this expands opportunities for research, but also increases the exposure of the eeg system to power line noise, since shielding against this interference is impractical in these scenarios."
"a further limitation of the study is that the application of cleanline involves the complex interplay of many parameters that can be adjusted by the user. we attempted to select parameters that improved performance as much as possible, but it may be the case that different parameters could have yielded better performance. since the developers already demonstrated that one shortcoming of cleanline is that it might fail with large non-stationary artifacts, we believe that there is no substantial improvement possible with this specific type of high amplitude power line noise [cit] ."
"in the present study, we suggest spectrum interpolation as an alternative approach to remove power line noise artifacts in meg/eeg data and compare its performance to three other widely used approaches: the notch filter (butterworth), the dft filter and cleanline. [cit] to remove power line noise in emg data. here we applied spectrum interpolation to synthetic test signals, a meg data set that included simulated abrupt on-/offsets of 50 hz and nonstationary power line noise of fluctuating amplitude and to eeg data from an overnight sleep study that was subject to substantial power line noise due to unshielded measurement conditions. with respect to the meg dataset that included non-stationary power line noise and the eeg dataset, spectrum interpolation outperformed the dft filter [cit] and the cleanline method and performed equally well to the notch filter. compared to notch filtering with a butterworth filter, spectrum interpolation introduced less signal distortion in the time domain, as indicated by the synthetic test signals used in the current study. a) the erfs (here the mlr) after the application of spectrum interpolation (blue, first panel), the notch filter (red, second panel), the dft filter (green, third panel), cleanline (light blue, fourth panel) and for the original noise-free meg signal (black, all panels) reveal residual line noise interference after the application of cleanline. b) an example trial reveals also substantial residual line noise after dft filtering (in addition to cleanline), only visible on a single trial level. c) the single trial difference curves (relative to the original meg signal) for the same trial show the signal distortions in the time domain more clearly. d) the boxplot of the nrmse (arbitrary units) relative to the original data reveals that over all single trials of the mlr component, cleanline shows the largest signal distortion followed by the dft filter. spectrum interpolation shows a slightly better performance than the notch filter (butterworth). the magnified view of the boxplot is shown on the right."
"however, cleanline showed the best performance with respect to signal distortions in the time domain in synthetic test signals. in many cases time domain regression-based methods as, e.g., cleanline might be preferable over a notch filter or spectrum interpolation, since they only remove deterministic line components, inducing almost no time domain signal distortions [cit] . since cleanline incorporates a sliding window estimation, it also allows for non-stationarities in the phase and amplitude of the line noise component to a certain extent [cit] . but if non-stationarities are especially strong, as, e.g., with amplitude fluctuations of line noise as large as in the presented eeg and meg data, cleanline may fail to remove line noise interference. in these scenarios, the line noise removal of spectrum interpolation is superior to the cleanline approach and the dft filter, while the risk of signal distortions in the time domain seems to be relatively reduced in comparison to a notch filter. the artifact removal approach should be carefully selected in each case according to the measures of interest and the nature of the power line noise."
"the lower approximation hypothesis and fuzzy decision table are used to obtain fuzzy decision classes for constructing the classifier. the lower approximations are used to define the decision classes. by taking a subset of attributes into consideration, the lower approximations are designed. the elementary sets will be the outcome of this stage from which the objects are classified into the decision classes. the decision classes are fixed as 0 and 1. the value 0 and 1 indicates that a patient belongs to category that he/she is and not suffering from diabetes. the fuzzification process is performed for the values on the crisp values to obtain the fuzzy values using the gaussian membership function, which is defined as"
"nowadays, hsi classification algorithms are mainly divided into two categories: spectral information matching methods and statistical methods. the former methods directly utilize known spectral information in the spectral library to identify the types of features in the image. these methods can be used to compare and match the whole band spectral information, or to select some spectral bands information of interest, so as to achieve the purpose of classification. examples of spectral information matching methods are minimum distance measure [cit], binary code matching [cit], spectral angle mapping, and spectral information divergence [cit] . such algorithms are mainly used in some hsi processing software, and the application scope and classification accuracy are limited."
"the artifact removal methods were also tested by applying them to an eeg dataset that we recorded for an overnight sleep study. eeg measurements took place in the participants' own homes without the benefit of a shielded eeg booth, so these measurements were subject to line noise interference presumably exhibiting different types of nonstationarities. the all-night eeg recordings (including electrooculogram and electromyogram recordings) were conducted with a portable brainamp dc eeg system and a brainproducts acticap electrode system (brainproducts gmbh, gilching, germany). the electrode placement was according to the standard international 10-20 system layout [cit] . the ground electrode of the brainproduct acticap electrode system is afz and the reference electrode is fcz. the electrode impedances were below 20 kω, as recommended by the manufacturer (brainproducts gmbh, gilching, germany). this was checked to the extent possible, at the beginning and at the end of the recording in the morning. for online preprocessing hardware filter settings were set to a bandwidth of 0.016-1000 hz, with a sampling rate of 2500 hz. during the night, a continuous auditory stimulation of condensation-rarefaction clicks of 100 ms length was performed. the stimuli were saved as a lossless wave-format file, with an inter-stimulus-interval (isi) of 500 ae 150 ms. the acoustic stimuli were presented via a battery-powered ipod nano 2g (apple inc., cupertino, usa) and delivered via in-ear earphones (hf5, etymotic research inc., elk grove village, il, usa) at a volume level of 40 db sound pressure level. in order to ensure trigger recording with high temporal precision, the stimulus signal from the playback device was split into two channels."
"if we use the atrous convolution at the same rate or multiplying rate, gridding and non-gridding problems will become worse. for example, if the rate is 2, 2, and 2 or 2, 4, and 8, this problem will show up [cit] . in fig. 2, the atrous convolution of different atrous rates is selected to convolute from top to bottom. the red pixel in the figure is the central pixel of the convolution kernel, which is combined with the features of other locations by different atrous rate to construct a feature map, regardless of the size of rate and the number of convolution layers. according to the principle of convolution, although the data of other positions will be introduced after the convolution, the data of the center position (red) will always exist. blue pixels are positions where features can be obtained by atrous convolution and the more the number of combinations. the higher redundancy, the deeper of the blue color."
"these time domain signal distortions are also referred to as ringing artifacts, and can occur if only part of the frequency range of a broadband signal is attenuated. since all synthetic test signals had a broadband spectrum, ringing artifacts occurred in all approaches presented here, except for cleanline, which had no impact on the impulse or step signal."
we use oa (overall accuracy) and kappa (kappa coefficient) as the criteria for classification of hsi. oa refers to the proportion of correctly classified samples to all classified samples. the formula is as follows (7):
"where c(i, +) is the total number of pixels, in which a ground object is divided into a particular category, c(+, i) is the total number of pixels, in which the ground object actually belongs to a particular category."
where k ijl is the j-th entry in k il . we solve for the parameters separately using the partial derivation method. calculation of the partial derivative of l 2 with respect to k ijl gives us:
"one major advantage over the notch filter is that the dft filter avoids potential corruption of frequencies away from the powerline frequency. the best performance can be achieved by applying the dft filter to relatively short segments of data (e.g., 1 s or less), ensuring the closest fit of the estimated components, in case the line noise interference exhibits certain dynamics. this filter approach assumes a constant amplitude of the line noise component. hence it may fail if the amplitude of the power line noise fluctuates over the input data segment, as is likely to occur with longer trials or continuous data. in consequence, the mean estimated amplitude does not apply to the actual amplitude of the line noise, resulting in additional line noise artifacts after subtraction (for a schematic illustration, see http://www.fieldtriptoolbox.org/faq/why_is_ there_a_residual_50hz_line-noise_component_after_applying_a_dft_filter)."
"there are several studies reporting severe signal distortions in the temporal domain, such as filter or ringing artifacts and artificial components after the application of filters [cit] . this might cause unwanted effects in the meg/eeg parameters of interest, e.g., a distortion of the peak amplitude [cit] or even artificial oscillations with a frequency near the cutoff frequency of the filter [cit] ."
"the relevance of time domain signal distortions (demonstrated here for synthetic test signals) might differ strongly between studies, depending on the measures of interest and many other parameters. therefore it should be evaluated individually to what extent the time domain signal distortions presented here are of relevance for the different types of analysis (as, e.g., erp/erf or phase and connectivity measures). for the current study, we applied the butterworth filter that is commonly applied when sharp cutoffs in the frequency response are required, but other filter types might show less signal distortions in the time domain than the butterworth filter. therefore the demonstrated superior performance of spectrum interpolation with respect to time domain signal distortions is limited to the comparison with the butterworth filter."
"consequently, other alternatives to reduce line noise artifacts have been developed. one method widely applied [cit] to meg/eeg data containing line noise interference is the discrete fourier transform (dft) filter, an example of which is implemented in fieldtrip, an open source meg/eeg analysis toolbox [cit] . the dft filter is realized by fitting a sine and cosine at interference frequency to the signal. the estimated components are subtracted from the data."
"the code for spectrum interpolation is made available in fieldtrip, an open source meg/eeg analysis toolbox (http://fieldtriptoolbox.org; [cit] ) . we implemented spectrum interpolation in fieldtrip [cit], involving the following processing steps. first the data is transformed into fourier space by calculating a dft. the absolute value of the complex fourier coefficients is calculated to retain the amplitude. the interpolation of the interference frequencies is realized by calculating the mean of the amplitude of the neighboring frequencies and replacing the amplitude of the line noise fourier coefficients with the respective value. the number of neighboring frequencies to be included can be determined by the user. after interpolation, the amplitude spectrum is combined with the original phase information via eulers formula. it is not known a priori how the phases of neighboring frequencies relate to each other, hence phases of the fourier spectrum were not interpolated, but the original phase information was retained. the interpolated complex fourier signal is then transformed back into time domain by calculating the inverse dft. to retain a real-valued signal after the inverse transform, fourier coefficients were treated as conjugate symmetric, ensuring that the positive and negative frequency components are mirror images of each other [cit] ."
"the original meg data did not show artifacts stemming from power line noise (50 hz) in the power spectrum of the continuous data segments ( fig. 2a) . in order to evaluate the performance of the different approaches, simulated power line noise (50 hz sinusoid) with abrupt onand offsets was added to the line-noise-free meg data."
"eq.(11) can be regarded as the classical signal recovery problem in estimation theory. in the next section, a map deconvolution algorithm is proposed based on the property of the sea clutter and target."
"the performance of the proposed map algorithm also relies on the parameters µ and η from the prior function. the smaller the η is, the closer is the estimate to the smooth imaging, and the larger the η is, the less the result is influenced by the prior [cit] . in practice, we need to select suitable parameters to balance clutter amplification and angular superresolution. the proposed map algorithm was listed in table 1, where k is the iterations."
"more levels of security/sensitivity could be added, if necessary, to add more or less fine-grained possibilities. e cut values of 1.6 and 2.2 are just examples; they can be adjusted according to data sensitivity and the weight that journal of healthcare engineering needs to be put into each of the values of the scale (low, medium, and high)."
"new components that distinguish this model from the other access control models include devices, user activity profile (uap), locations, connections, adaptable visualization module (avm) [cit], and adaptable access control policy (aacp). succinctly, sotraace aims to automatically learn from individuals' interactions and from live data collected from every interaction a user makes comprising human, social, and technical context at that moment (e.g., time, location, previous interactions, and type of connection/device) and decides what is the most transparent, secure, and usable way (avm) to both ask and retrieve the results of each request, to and from the application at hand. sotraace performs a quantitative and qualitative risk assessment analysis supporting decision-making (aacp) on the most secure, private, and usable way to access and display information. more details about these components and their integration are available in previous research [cit] . ere are two types of risk assessment: quantitative and qualitative. e former uses numbers to quantify mostly the loss of tangible assets (e.g., replace a defective server) while the latter assesses the probability that a certain level of loss of confidentiality, integrity, or availability (e.g., low, moderate, and high) may occur and the impact it can cause (e.g., patient records are breached via hacking) [cit] . besides the international standards on risk management and risk assessment [cit], some examples of specific risk assessment frameworks for access control include (a) fuzzy multi-level security (mls) access control model [cit] that quantifies the risk associated with an access with basis on a value of information and probability of unauthorized disclosure, (b) a framework for threat assessment approaches for subjectobject accesses, which can be selected based on the context of applications or on the preference of organizations [cit], (c) radac [cit] (already introduced), or (d) dread (damage potential, reproducibility, exploitability, affected users, and discoverability) [cit], which rates risk by answering five questions related with those five categories. e last two methods are reused by sotraace and included in its own risk assessment mechanism."
is type of study can help bringing the knowledge of security experts closer to real-life situations and their associated risks. e applied questionnaire and detailed rounds of questions are presented in the annex and its results are presented in section 4.1.1.
"one limitation of this work concerns the lack of existing models or hybrid risk assessment procedures and related categorized data in terms of security and sensitivity levels in healthcare, to perform better and more adapted access control decisions."
"so far, the angular superresolution methods can be divided into three categories. one is the traditional spectral estimation the associate editor coordinating the review of this manuscript and approving it for publication was ravibabu mulaveesala . method [cit], represented by multiple signal classification (music) method. however, how to collect enough snapshots to realize the high estimation precision of the covariance matrix is difficult for the mechanical scanning radar system [cit] . in recent years, an iterative adaptive approach (iaa) was presented, which can significantly decrease the requirement of snapshots in traditional method. some effects have been gained by applying this approach to the real-aperture angular superresolution problem [cit] . besides, there are two main categories of deconvolution techniques based on regularization theory and bayes criterion, respectively. the core of regularization theory is to model the unknowns as a deterministic function, where the errors are gaussian noise. the pseudo-inverse solution is regarded as an optimal solution. in the references [cit], the authors proposed a truncated singular value decomposition (tsvd) method with the intention of suppressing noise amplification. it is able to reduce the influence of noise in the solving process and suitble for low noise environment. another class of methods with additional regular terms to balance the effect of noise suppression and target prior, such as l 1 − norm and l 2 − norm, are widely studied by researchers [cit] . however, the constraint of single regular term only can be applied to the specific imaging scene, so the adaptability on different scenes is poor."
"to further verify the superresolution performance, the profiles of two adjacent targets marked by red circle are shown in fig.7 . from the profiles,we can find that the tsvd algorithm and rayleigh-based ml algorithm only can sharpen the real beams lightly with a high saddle. the iaa algorithms can achieve higher resolution than above two algorithms, but the resolution improvement is limited compared with the gsmap algorithm and proposed rlgmap algorithm. obviously, the gsmap algorithm and proposed rlgmap algorithm can achieve higher resolution with lower saddle than other algorithms, but their performance was not the same. we can find that the profile of gsmap algorithm generates the unexpected false target, and the proposed rlgmap algorithm distinguishes the adjacent targets and has better noise suppression ability than others algorithms."
"where rf i is the risk ranking (e.g., low � 1, medium � 2, or high � 3) associated to the respective risk factor i, in figure 2, and w i is the weight agreed by the experts in the delphi study, for rf i . for instance, if the rf 1 -identified attribute is wep (rc4) (figure 2 ), its ranking would be high; so, for the purpose of calculating the risk, rf 1 � 3, while the corresponding w 1 � 4.25 ( figure 3 ). e weighted mean ensures a dynamic variation of the number of risk factor collected and can never be null as some of the risk factors are always applicable, such as the ssid, type of wireless connection, data sensitivity, or type of device."
"in this section, we mainly describe the angular superresolution model which emphasis on the echo formulation. figure 1 shows the geometric model of the real aperture scanning radar. in the scanning imaging model, the airborne platform moves on with a uniform velocity v, and the radar antenna counterclockwise sweeps across forward-looking imaging region containing targets l 1 ∼ l k . h and ω repre- sent the flight height of airborne platform and the scanning speed of the antenna, respectively. r 0 is initial range history between the antenna and l 1 . after a time interval t, the range history r(t) can be expressed as"
"the reconnaissance and detection of sea-surface target are of crucial importance, which are applied to marine environmental monitoring, marine vessel detection and maritime rescue. in particular, it plays an indispensable role in the situational awareness of the sea battlefield. real-aperture scanning radar imaging is a significant technology to achieve the reconnaissance and detection of sea-surface target because it is suitable for most geometry situation [cit] . however, the angular resolution of this radar system is greatly limited by the antenna aperture size, which seriously influences the searching ability and location accuracy [cit] . over the years, the proposed angular superresolution approaches are applied to the ground scenarios mostly. therefore, it is urgent for us to propose effective methods to improve the angular resolution of realaperture radar for sea-surface target."
"where µ, η represent the distribution parameter of the lognormal, which restrict the mean and variance. f j is the j-th element of f . combining eq. (14) and eq. (15), the posteriori probability function with negative-logarithm operation becomes"
"each idp contains an authentication layer (to manage authentication of users and control their identity), an access control layer (based on sotraace), and an sql database to store the access-control list (acl) permissions and user profile (with all past requests and attributes), as well as to assist the layers of authentication and access control. it also integrates a log system to enable audit."
"sps use sql databases to store health data and logs. ose databases also store acl permissions that are synchronized with the main service, the idp. in each sp, the respective logs for posterior auditing are also stored. it is important to store information to a better version of control over the health data (e.g., who, when, and where the data was changed). e acl exists in the idp, but it must also exist in the federated sps. for instance, when the internet connection fails in an institution, the internal local area network (lan) of the institution checks the internal acl database for permissions, without the need to connect to an outside idp. e database in the idp is the one that contains the main acls. e health data are stored in institutional databases. e local sp acl is synchronized with the main acl in the idp. for the system and services to remain available to authorized parties, a set of idps must exist. if one fails, another takes its place. e system can have many geographically fragmented federated institutions (sps) that can share data between them, if the user consents. e user should be obligated to perform a login each time he/she requires a phr from a different institution. as such, by using sso, the users can move between services securely and uninterruptedly without specifying their credentials every time. also, multifactor authentication must be present in the system to protect stolen devices and access from new locations in cases of stolen accounts. e patient should have access to all clinical documents, history, and logs. questions such as who, when, and where his/her documents were accessed and who changed them must always be recorded and available to the patients. ere are some cases in which one cannot waste time setting permissions on the mobile phone. in cases of extreme emergency, such as an unconscious patient in an ambulance unable to give access to resources, the btg mechanism needs to allow emergency access to the health professionals."
"prior information expressed as probability density function represents the statistical characteristics of target backscatter coefficient, which is employed for high angular resolution and image quality [cit] . the sparse distribution is usually considered as the prior information in real aperture imaging because in most cases, the distribution of strong scattering target is sparse relative to the imaging area [cit] . it can greatly improve the azimuth resolution of target imaging, but the drawbacks of this constraint are losing imaging quality and the false target will appear when the signal to noise ratio (snr) is low. in addition, the smoothing property of gaussian distribution is in favor of the enhancement of imaging quality, but the improvement in azimuth resolution is limited [cit] ."
"the essence of angular superresolution imaging on seasurface target is to improve angular resolution under the condition of suppressing sea clutter. sea clutter is complicated and changeable under the interference of natural climate, and the previous assumptions are no longer suitable for seasurface environment. therefore, the sea-surface background and the target prior need to be modeled again in the seasurface environment. bayesian approach need to choose the reasonable likelihood function and prior information."
"connected to this necessity, to the best of our knowledge, there are only two previous works tackling the need to include contextual elements in the risk assessment of mhealth apps: a previous work on risk-adaptable access control [cit] and a proposal of a risk framework to support clinical use of medical apps [cit] . e former is an access control model (sotraace, socio-technical risk-adaptable access control) that integrates the described needed features, but it has only been presented at a theoretical level [cit] . e latter strives to include only external elements directly related to the app in the risk assessment, such as inadequate training of the users or the usage factor of the app [cit] . nonetheless, it does not include elements such as network connection type (e.g., public wi-fi or protected isp), version of operating system (os) used, or if the user has already made that same request and, if so, what was the associated risk at that time (e.g., user's risk profiling). more work has been done since to developing a sotraace prototype into a mhealth app and include a hybrid risk assessment feature. e aim of this paper is to present a risk assessment feature integration into the sotraace model, as well as the operationalization of the related mobile health decision policies. a delphi study with security experts was performed and is presented to integrate the categorization of the data regarding the impact of security and privacy loss into the prototype, to reflect security experts' knowledge and to be closer to real-life situations and their associated risks. a simple prototype is discussed and validated on patient access control scenarios, using a fictitious mhealth application. e next section presents background work and section 3 describes the methods used to develop risk assessment into the sotraace prototype, while section 4 presents the results from the delphi study as well as the implemented prototype with risk assessment in patient's access scenarios. section 5 discusses obtained results that go beyond the state of the art, together with the work limitations, while section 6 concludes the paper."
"in the healthcare domain, smartphones can bring many advantages to tackle diverse needs of stakeholders. health professionals can use smartphones to access and manage patient records, to view exam results, to share and ask for second-opinion diagnosis, and to prescribe medications [cit] . on the other hand, patients can use smartphones to manage, update, and control access to their medical records, monitor their health statistics, and view their prescriptions [cit] ."
"in this section, we convert the deconvolution problem into an equivalent maximum a posterior (map) estimation task. the map algorithm is a typical superresolution approach which is based on the bayesian framework"
"with the definition of the quantitative and qualitative risk assessment feature, a set of patient-access use cases was defined to implement and validate the proof of concept of the sotraace prototype."
"where τ denotes the fast time, and t r is the pause width of the lfm. f 0 and k r mean the carrier frequency and frequency modulation slope, separately. the window function can be expressed as"
"above simulation results have verified angular superresolution of the proposed rlgmap algorithm in rayleigh distribution. in this section, the real data was processed by the rlgmap and other traditional algorithms to verify imaging performance in practical sea-surface targets. the original scene is shown in fig.5 with some lands and the superresolution targets of two ships in the red circle, and the picture is from google earth. the real data was recorded by an x-band radar whose main system parameters were listed in table 3. the superresolution results of measured data processed by different algorithms are shown in fig.6 . from the fig.6.(a), we can see that real beam echo has low resolution and many targets are merged by clutters. fig.6.(b) and fig.6.(c) show the results processed by the tsvd algorithm and rayleighbased ml algorithm, respectively. the tsvd algorithm has poor resolution improvement, thus adjacent ships marked in red circle are almost undistinguishable in azimuth, and the imaging result produces more obvious parasitic ripples. the rayleigh-based ml algorithm has improved resolution performance and the ability of clutter suppression, but the sharpening ability is limited. the imaging results of the iaa algorithm and gsmap algorithm are shown in fig.6.(d) and fig.6.(e), respectively. the two methods can basically distinguish adjacent ships, but they amplify the clutter, resulting in parasitic ripples that cannot be ignored. compared with the imaging results of gsmap algorithm, the proposed rlgmap algorithm shown in fig.6.(f) has greatly improved the resolution of targets estimation under sea clutter interference. that is to say, not only the adjacent ships are distinguished clearly, but also the noise level is lower than others. we can see that the proposed rlgmap algorithm can obtain better superresolution of angular and noise suppression ability than other methods."
"e developed system is divided in three major components: mobile applications, web services (including identity providers, idps), and service providers (sps) with databases. e patient mobile component comes with an application, with web-based central authentication and authorization idp, to secure access and share health data stored in databases of geographically fragmented sps. in figure 5, the generic architecture is graphically schematized, with the respective representation of the different types of communications that are used. e proof of concept was implemented on the operating system ubuntu 16.04 lts, and for the mobile application, android was used. e native programming language used in android development is java. e integrated development environment (ide) used was android studio, the official ide for google's android os development. e local data records on the mobile device were stored under sqlite database, and the android layouts were designed using xml. to handle the asynchronous android client requests, the android application uses loopj android asynchronous http client library."
"health information systems can empower the performance and maintenance of health services, but the processing and storage of highly sensitive data raises serious concerns regarding privacy and safety of patients [cit] . e healthcare industry is a prime target for medical information theft due to the systematic unpreparedness in dealing with cyber threats menacing vital data [cit] . ere is the need to increase the awareness and understanding that, in healthcare, the risk associated with patient data is not just about such data, but about patient care delivery, and potentially, even about the mental and physical health of the patient [cit] ."
"where the elements of eq.(11) are the weighed value of the antenna pattern to the corresponding target at different sampling times, and l denotes the discrete number of antenna pattern. the goal of radar imaging with high azimuth resolution is to accurately infer f from the echo signal g. based on the convolution model of g and f, this task is called deconvolution problem in this paper. however, direct division in the frequency domain is unstable and impracticable for radar system due to the band-limited character of antenna pattern in the frequency domain and the influence of echo noise, which is called ill-posed nature for direct deconvolution [cit] ."
"another limitation is the fact that the delphi study was performed only with security experts and did not integrate multidisciplinary expertise, such as the one from healthcare professionals. however, this expertise was integrated within this study, when defining what risk factors to evaluate. another limitation was the number of analysed risk factors, which was small and needs to include more contextual as well as clinical and health-related data aspects. e presented prototype screenshots are very simple. at this stage, there was no need to show more complex application functionalities as the main goal of this work was focused on risk assessment procedures for mhealth applications."
"we resort to use case diagrams to provide an overview on how a user interacts with the features of the system and the functionality provided by the system in terms of actors. a patient, with the sotraace prototype application installed in the android mobile device, can try the various options described in figure 6 . for instance, the login action includes a previously registered account, and if the location and device imei are new, the login can be extended to include a multifactor authentication protocol. e uml sequence diagram in figure 7 shows the sequence of a patient's login functionality to access the mobile application. as a prerequirement, the patient needs to have a registered account. in the first step, the patient requests to login, sending his/her login credentials, also the mobile application collects the gps location (if available) and device imei (to identify the device). next, the idp validates the login credentials and checks the patient's profile to see whether the device and location are already known or were previously used. if not, multifactor authentication will be used instead (figure 7, additional bracket part on the right). after the login stage, the patient can choose between the options: (i) add new device or location, (ii) read messages, (iii) create phr, (iv) create new relationships, and (v) view available phrs/ehrs."
"in this paper, it is assumed that the noise in each observed cell obeys independent rayleigh distribution [cit] so that the likelihood function can be written as"
"in this subsection, the simulation results are two parts that they are point target and iterative error curve in different signal to clutter ratio (scr). in the first part, the point target simulations by ten times' monte carlo experiments are processed under different algorithms, which include the tsvd algorithm, rayleigh-based ml algorithm, iaa algorithm, guassian-based sparse map (gsmap) algorithm and proposed algorithm(rlgmap). in the second part, iterative error curve simulation results of different algorithms are presented. the angular superresolution and stability of the rlgmap are verified. the detailed simulation parameters are given in table 2 . because the noise is invertable in practice, the rayleigh noise is added into the echo signal."
"with all this in mind, the authors propose a more adaptable/adequate means for risk assessment on the fly integrated into existing access control models but with novel functionalities to perform a more complete risk assessment for each mhealth interaction. e authors could not find such models for other applications, but certainly not for mhealth, which could journal of healthcare engineering integrate all the necessary requirements both for flexibility and adaptability, as well as qualitative and quantitative risk analysis, specifically for personal health data. furthermore, no guidelines or standards for this specific domain were found to help define data sensitivity for mobile healthcare use and associated visualization/presentation. is work presents sotraace, an access control decision model which integrates more features for flexibility and better adaptable security, not only for calculating hybrid risk assessment for each users' contextual interaction and subsequent communications but also to improve and adapt visualization with end users. sotraace, associated with the delphi study presented in this work, provides a first effort to achieve the categorization of personal health data resorting to security experts to both reflect that experts' knowledge and to be closer to real-life situations and their associated risks."
"quantitative risk analysis is complemented with more qualitative measures that can integrate the operational urgency (how urgent is it to access that object at that moment?) and other external situational factors (unusual distant locations where the access was performed) to provide a more accurate, secure, and adapted access decision. for instance, a scenario illustrating a qualitative risk evaluation could be the following: if a nurse is trying to access a medical record at a different time from her normal working hours, using a different device and connection, the calculated quantitative risk will be higher than usual. however, a more qualitative analysis may attenuate that risk if it confirms that the nurse is accessing data that are customary and from a secure location (e.g., secure service provider wi-fi at home). in this case, auditing can register some warnings and visual security restrictions can be applied as a preventive measure."
"finally, the risk level output is a number between 1 and 3 and can be mapped to new security measures and access restrictions as in the following examples:"
"following quantitative and qualitative calculations of risk, as defined by standards such as iso/iec 27005:2011 [cit] and nist sp800-30 rev.1 [cit], the delphi study results provide, this way, means to use a hybrid risk assessment method, which combines characteristics from both quantitative and qualitative risk assessments (figure 4 ). is helps integrating benefits from both worlds, thus leading to the adoption of more accurate and prioritized mitigation measures for the analysed domain. is can also be helpful in order to perform stronger statements to the organization's management and governance departments, as numbers sometimes can be clearer than descriptions and adjectives. moreover, this still leaves space for stricter qualitative assessments, which may, for instance, include situations where specific values cannot easily be associated to risks (please see section 4.1.2)."
"bayesian deconvolution approaches, relying on the statistical characteristics of noise and the prior knowledge of target scatters, can convert the solving problem of angular superresolution into maximum a posterior probability (map) estimation [cit] . in these references [cit], the map method was presented based on the assumption that the statistic of noise obeys gaussian or poisson distribution, which is effective in the process of recovering land target. however, the imaging results of these methods above are undesired facing the problem of sea-surface environment."
"building adaptable and resilient access control models into the most generalized technology used nowadays (e.g., smartphones) is crucial to fulfil both users' goals as well as security and privacy requirements for healthcare data. is work is an alert for the research community to put more efforts into these areas in order to better integrate and personalize security into every patient or, for that matter, any type of user's life."
"access control (often encompassing identification, authentication, authorization, and accountability) is the first and one of the most crucial interactions between users and mobile devices [cit] . when a user requests data from an app, authorization is being constantly checked and so should risk assessment be constantly verified to adapt to the changes of the ubiquitous characteristics of mobile devices and their location. to do this, risk assessment must comprise technical, contextual, environmental, and user's profiling data to identify, at each access request of the user, what is the probability of a negative impact to occur when making that request available, within the identified conditions, at that specific moment."
"but risk, as the by-product of the likelihood of a vulnerability being exploited by a threat and the negative impact this can cause [cit], is very difficult to calculate and maintain, especially in such a heterogeneous and high turnover environment. e risks can increase considerably when personal health-related data can be collected, processed, and stored by many types of different devices (e.g., smartphones, smartwatches, or other iot sensors) and associated vulnerabilities, anytime and anywhere [cit] ."
"where e i means the iteration error after i iterations and f i is the estimation of imaging scene amplitude values after i iterations. fig.4(a) shows a plot of the normalized iterative error curves in rayleigh distribution when the scr is 20 db. the rayleigh-based ml algorithm has stable convergence performance, but the iterative error is higher than other methods. oppositely, the convergence speed of the other algorithms is lower than rayleigh-based ml algorithm, whereas the iterative error is low. the iaa algorithm and gsmap both have stable convergence after 7 iterations, but the proposed rlgmap algorithm can converge well in less than 7 iteratins with lower iteration errors, which shows that the proposed algorithm has the advantage of envergence property over other methods. fig.4(b) shows a plot of the normalized iterative error curves in rayleigh distribution when the scr is 10 db. in this low scr condition, the rayleigh-based ml algorithm, iaa algorithm, gsmap algorithm and proposed method's convergence speed slows down. the rayleigh-based ml algorithm also has higher iterative error. in particular, the iterative error curve of the gsmap algorithm produces a fluctuation and iteration number of convergence is greater than 15. however, the proposed rlgmap algorithm has a good convergence speed and low normalized iterative error."
"for the web service, the technology used was restful api, which is a flexible way to provide different kinds of applications with data formatted in a standardized way and very important for ehealth, as it helps to meet integration requirements that are critical to building systems where data can be quickly combined and extended. also, it can facilitate its use as it provides json format. ide eclipse neon enterprise edition was used to build the java web service and configured to use apache tomcat servlet container (often referred to as tomcat server). in addition, the web service uses jersey libraries and tools. jersey restful web service framework is open source, made in java, which provides support for jax-rs apis and serves as a jax-rs (jsr311 and jsr 339) reference implementation."
"ω cos θ 0 is the doppler shift, which can usually be ignored in practice. in other words, the doppler shift is zero for static platform and has small effect to the echo signal for the motion plat form with low speed moving or high speed scanning [cit] . the received echo can be written as the following two-dimensional convolution model"
"to develop the sotraace prototype into a mobile application, an analysis of global smartphone os market was performed and it was decided that the initial os target for the implementation and test of this research is android [cit] ."
"e delphi study performed in the scope of this work allowed the quantification of the relative importance of the risk associated with several technical, environmental, and contextual risk factors (rf). e selection of the eleven risk factors ( figure 2 ) was obtained from the following sources: (1) previous research work on the definition of sotraace model (figure 1 ) with new components (e.g., devices, locations, and connections) that integrate specific requirements, which can be associated with specific vulnerabilities (e.g., rf1, rf2, rf3, rf4, and rf7), (2) rfc documents on cryptographic and authentication protocols (https://www.ietf.org/rfc/ [cit] .txt and https://www.ietf.org/rfc/rfc5246.txt) (e.g., rf1 and rf3), and (3) debates among research experts in the domains of healthcare and information system security, during the course of the work, where attributes closer to the healthcare domain-rf6 and rf8-and human interaction-rf5, rf9, rf10, and rf11-were also defined. some of the risk factors reflect vulnerabilities associated to android mobile operating system because the actual android market share is roughly in the neighbourhood of 75 to 85 percent, and it is expected that the gap towards ios keeps increasing in the next years. however, similar attributes for other technologies, or types of devices, can also be adopted to calculate the necessary risk in different platforms and with different characteristics."
"for the multifactor authentication, the patient receives a random secret pin in his/her email; each time, the multifactor authentication is required. when the patient requests login, the idp verifies if he/she is using a new, nonregistered device or location (i.e., not registered in their profile, from past accesses) and requests a multifactor authentication to the patient. e patient checks his/her email and sends the secret pin to the idp, which verifies this secret authenticity. if everything matches, idp stores the new device imei or/ and location in the user profile and notifies the patient. now, the patient can login using a different mobile device."
"for the risk-adaptable decisions in the access control layer, the android application needs to manage data about users' locations and connections. also, location is used in the authentication layer to identify the user. e alert system warns the user about some important aspects of the interactions. it can be used to (a) warn about the risk of access to specific data in a dangerous context, (b) release alerts to teach the patient to get better decisions and security, (c) warn about the existence of a new phr (or ehr, electronic health record) or a change in one, (d) inform the user of new access requests, and so on."
"a comprehensive search on subjects such as access control and risk evaluation (both quantitative and qualitative), specifically applied to mobile apps, was performed so as to understand what types of risk assessment are used and how these can be improved (more details in sections 1 and 2)."
"is btg access must be well defined and always recorded in the logs. secure backups must also be performed with short intervals of time. e management of health data can be critical, so all the end points and communications must be protected. security mechanisms must assure confidentiality, integrity, and availability of patients' personal and health data, empowering this way patients' privacy. besides patient data, attributes such as location, types of devices, and sensitive profile information must also be protected. to assure this, the aacp is embedded at the idps and all attributes are aggregated there, never reaching the sps. ese attributes help sotraace to perform a risk evaluation and perform the best access decision at the moment of each request."
"regarding mhealth applications (apps), i.e., software applications used on a mobile device for medical or other health-related purposes, the risk of disclosure and breach can be higher as no adequate security measures are yet available or these are not properly used for these devices [cit], and a medical record has a significant higher financial value compared with, for instance, credit card data [cit] . but, even if proper safeguards were available, they would still be difficult to verify and control in the hands of different millions of users around the world [cit] . ere is some work in the literature that focuses on using mobile apps to assess risks (a) while managing a disease [cit] and much less to (b) detect source threats to the quality and integrity of medical data of patients that circulate in mobile applications [cit] . however, this second type of risk assessment targets only part of the problem. ere are currently no standard means to assess risk in every interaction between a user (e.g., a patient) and the mhealth app, both dependently and independently of the goal and content of the app."
is situation is bound to be more and more frequent regarding not only the pressure put by the constant increase of aged population worldwide in need of health-related ambient assisted living products [cit] but also the empowerment that current legislation and regulation on personal data protection offers individuals [cit] .
is provides an asynchronous call back-based http and https client for android built on top of apache httpclient libraries. e mobile apps were tested in the android versions marshmallow and nougat and with a physical device huawei p9 lite.
"since a main objective of this prototype system is to ensure patients' privacy and empowerment, sotraace collects available user data request/interaction (e.g., location and connection) to perform risk evaluation and agree on the most adequate access decision. however, to assure privacy, personal data cannot be stored on the federated institutions' (sp) side but must always be stored at the idp. moreover, each request generates a log to help build the patient profile, the user activity profile (uap), and to enable audit. figure 8 shows a sequence concerning a patient choosing an ehr from the initial menu list. is request goes with an authorization token (at) directly to the federated institution. e patient's device sends the necessary data to the idp for sotraace to do the work. e federated institution validates the at with the idp. if the at is valid, a record is created in the log system. after this, sotraace evaluates the risk for that request and adopts the query (e.g., if the risk is high, some parts of the ehr are omitted). en, the ehr is sent to the idp, and the idp determines the best protection and accesses decision based on sotraace and sends it to the patient. e patient views the ehr and changes the access permissions. ose permission changes are updated in the idp sotraace acl and at the federated institution acl. finally, the patient is notified about the success/failure of his/her alterations. figure 9 shows a screenshot of the main page of the patient's mobile sotraace prototype with the main menu activity and available functionalities. figure 10 shows the displays that a patient sees when the functionality \"show my ehr's\" is selected. on the left is an ehr for the internment of the patient within a hospital and related actions. ere is the possibility to verify the type of sensitivity level associated to this record. e same is true for the image on the right, which shows similar data but for allergy exams."
"the traditional spectral estimation method and regularization method are mainly applied to ground target. it is deemed that the noise satisfies the gaussian distribution, and the sea clutter characteristics are not considered. therefore, the imaging performance of the above two methods is poor when they are applied to the sea target."
"in practical applications, the time to sweep through a target is very short. in addition, forward-looking scanning radar generally has a long range history. thus, the influence of the quadratic term on the range history is often ignored and eq.(2) can be further simplified as"
"where g(r, θ) is the received two-dimensional signal, r and θ represent the range and azimuth variable of the two-dimensional signal, respectively. h(r, θ) is the twodimensional convolution kernel function and f (r, θ) is the target scattering distribution. the high resolution can be easily achieved by transmitting a large bandwidth lfm signal in range dimension, and the range resolution is decided by the 3-db width of the convolution kernel sinc function, which can be calculated by c/2b. in azimuth dimension, the kernel function is the antenna pattern, and the azimuth resolution is limited by the size of real aperture which leads to the unmatched two-dimensional resolution. this paper presents a new deconvolution method for high azimuth resolution. we first rearrange eq.(9) as the following matrix form"
"e results, including means and standard deviations of each question of the two rounds, are presented in figure 3 . as expected, the standard deviation in most cases was reduced from the first to the second round, converging to a consensus of opinion among the experts. e risk was ranked using a number and color scale of low � 1 (green), medium � 2 (yellow), and high � 3 (red). is ranking was chosen by the authors as the simplest example scale for the purpose of using them as input in the sotraace model to test its risk assessment features. e respective final results extracted from the delphi final round are converted to weights and are reallocated in the following arithmetic mean to calculate the risk:"
"in this section, we present results from the architecture, requirements, and implementation of risk assessment into the sotraace prototype, as well as the description of patient use cases and how this can reflect into an mhealth application."
"where k represents the iteration index, p (k) is calculated by f (k), and the initial value of iteration f (1) is simply represented by the received data g."
"prior knowledge plays an important role in bayesian method. it is helpful to approximate the optimal solution during the iterations. the introduction of reasonable prior knowledge for the target scene could significantly improve the imaging performance. this paper introduces lognormal distribution as the prior distribution of sea-surface target, for the reason that lognormal distribution [cit] as a heavytailed distribution can catch the strong targets easily, and it can be approximatively regarded as a combined constraint term. it possesses better smooth features comparing with the common sparse distribution. and the lognormal distribution function can be written as"
"for the repository, a relational database management system (rdbms) mysql was used together with the phpmyadmin administration tools. to connect the javabased web services (idp and sps) to the mysql database, the official driver connector java database connectivity (jdbc) was used. to test the restful web services (idp and sps), advanced rest client was used as it makes a connection directly to the socket giving full control over the connection and url request/response headers. is way, it is possible to analyze and test all headers before inserting them in the mobile applications."
"finally, past decisions and respective parameters provided by the aacp are recorded and used to help decide each subsequent decision. e main goal is to analyse user-profiling information to securely improve and optimize similar future interactions. is knowledge can enhance algorithms that determine the risk, operational need, and the rate of positive/negative access control decisions, to build more accurate user activity profiles (uap) and object logs and also, therefore, improve and monitor security measures in place."
"each question answered by the security experts in the delphi study was related with each of those rfs, which are rf1 (wireless and encryption), rf2 (ssid), rf3 (security in connection), rf4 (location), rf5 (number of networks available nearby), rf6 (resource sensitivity), rf7 (device threat level), rf8 (role), rf9 (registered mobile devices), rf10 (global situational factors), and rf11 (behavioral differences). each rf comprises several attributes for the security experts to weight in terms of their risk criticality (e.g., attributes for rf3 are vpn, https, or http), see figure 2 ."
"this paper focused on the problem of low azimuth resolution in forward-looking radar imaging of sea-surface target and proposed a deconvolution rlgmap algorithm. the proposed rlgmap algorithm establishes the signal model of forward-looking scanning radar as a convolution of the antenna pattern and the target scattering coefficient. then, relying on the statistical characteristics of sea clutter and the prior knowledge of target scatters, we can obtain the iterative estimated solution based on the optimization theory to achieve azimuth superresolution. at length, the simulation and real data processing results show that the proposed algorithm not only can effectively improve the azimuth resolution and suppress sea clutter, but also can maintain stable convergence characteristics compared with other algorithms."
"on the subject of health data, associated risks are commonly focused on people's diagnosis, treatments' outcomes, and to help in medical decisions. however, risks need to be associated and constantly evaluated in relation to additional health data that are accessed and processed by any means (e.g., paper, web, and mobile) as well as for each type of interaction and associated contextual variables/characteristics. is assumes a higher criticality due to the fact that those data can also greatly influence people's health, security, and privacy. to achieve this, risk assessment should always be considered, which is not happening at the moment. to make matters worse, although the use of mhealth can improve treatment and outcomes and change the paradigm of healthcare to anytime/anywhere, it can also exponentially increase the available vulnerabilities and threats and, again, the risk."
"for this work, the delphi study comprised a total of twelve (12) experts. e group comprised 3 (25%) females and 9 (75%) males with a background in computer science (n � 11; 92%) and electrical engineering (n � 1; 8%), with the expertise in computer and information security (n � 8; 66%), cryptography (n � 1; 8.5%), standards and modelling (n � 1; 8.5%), software development (n � 1; 8.5%), and information systems and computer engineering (n � 1; 8.5%). half of the experts had experience between five to ten years (n � 6), while four of them had less than five years' experience and two had more than ten years' experience, in their fields of research/work. more experts work in academy/education (n � 9) than in the industry (n � 3). e expert group provided health data security categorization for a number of patient-related data, as well as the definition of the impact of those data security and privacy loss. e questionnaire comprised twelve questions, eleven of which use a five-point likert scale, from negligible (1) to critical (5), and the last question was an open question that allowed the experts to provide or suggest more information regarding evaluated factors or others that were not included in the study. at questionnaire was answered twice in twotime separated rounds, by the same group of experts, and at the end of the first round, an anonymized summary of the experts' results was provided to them. with the provided summary, the experts were encouraged to revise their earlier answers in the light of other members' replies so that, during this process, the range of the answers will decrease and the group will converge towards the most consensual answer."
"finally, the performed delphi study shows that much more work needs to be done. ere is the need for the whole community of security and healthcare experts to join in the definition of security/sensitivity levels of relevant variables for the decision process. in this case, it was only an initial effort and the experts could only focus on very specific (mostly technology related) aspects of security (e.g., wireless connections, communication protocols, and users' roles). is is where the main research efforts need to be focused. once this is clearly defined, its consensual implementation in the clinical practice will not take major resources, as technology is readily available to model the identified needs. e presented prototype corroborates the previous statement since it was implemented with existing technology and security protocols. moreover, if given an initial strong base to perform adaptable decisions, the access control model will then learn and optimize with its use time and provide personal and customized secure access control for all mhealth users."
"it is clear that integrating a series of security and sensitivity level classification to a decision process is not difficult; the difficulty lies on the definition of those levels for each type of data and how accurate and adaptable they can be. to do this, it is important to compare and discuss results from other works. in this case, we found a work where some similarities can be drawn, which also helps validating our own results. in work [cit], interviews were applied to sixteen participants, the majority having expertise in ict, where questions focused on the threats, criticality, and frequency of those threats, in healthcare information systems. although the questions were generic and were not rated in several rounds, with the aim to provide risk calculation, as in our delphi study, the main outcome was a list of the most common identified threats, by those participants. e obtained results are closely associated to contextual and environmental attributes as well as to the specific technologies that are used to assess those threats, as most critical identified threats for that study comprise power, internet, and air conditioning failures. nonetheless, the most five critical threats identified in that study overlap in some degree with some of the most significant risk factors ranked by our experts. ese overlaps are highlighted in table 1 ."
"this paper is organized as follows. the imaging model of the real aperture scanning radar is described in section ii. in section iii, the map method based on the hypothesis that the sea clutter and targets obey the rayleigh distribution and log-normal distribution is derived in detail. section iv shows the processed results of simulation and measured data as well as the analysis of the handled image. conclusions are discussed in section v."
"ere are no means to compare with proposed work, and therefore, this constitutes an initial step in that direction. being such a first attempt, this work does not yet integrate the prototype testing with real users and in real scenarios, though this is planned, as future work, once the prototype is enriched with all the required use cases."
